From leventov.ru at gmail.com  Sun Jan  5 14:22:43 2020
From: leventov.ru at gmail.com (Roman Leventov)
Date: Sun, 5 Jan 2020 22:22:43 +0300
Subject: [concurrency-interest] java.util.Timer is good for wall time
 scheduling?
Message-ID: <CAAMLo=azs5MFQRwEjx_9hfyM5Q3Mc=ACUyGw4mWsCM04fiUarQ@mail.gmail.com>

There is an interesting question on StackOverflow with an evidence of
rather extreme time drift when a task is scheduled periodically using
ScheduledThreadPoolExecutor:
https://stackoverflow.com/questions/56571647/why-does-the-java-scheduler-exhibit-significant-time-drift-on-windows.
java.util.Timer appears to be a workaround for that problem. It looks to me
that java.util.Timer is an only tool offered by JDK suitable for wall-clock
time (cron-style) scheduling.

In this context, I would like to recall the discussion of hibernation and
ScheduledThreadPoolExecutor behavior (
https://bugs.openjdk.java.net/browse/JDK-8146527; the discussion thread in
this list:
http://cs.oswego.edu/pipermail/concurrency-interest/2016-January/014817.html).
It seems actually that the desired case of "sending an e-mail each hour"
could be pretty easily coded with Timer:

new Timer().scheduleAtFixedRate(new TimerTask() {
  long minutes = 0;
  @Override public void run() {
      if (minutes % 60 == 0 &&
          System.currentTimeMillis() - scheduledExecutionTime() <
MINUTES.toMillis(1)) {
       sendEmail();
     }
     minutes++;
   }
}, nextRoundMinuteDate(), MINUTES.toMillis(1));

With this, I have a few questions:

 1) Should the Javadoc for Timer, which currently says:

"Java 5.0 introduced the java.util.concurrent package and one of the
concurrency utilities therein is the ScheduledThreadPoolExecutor which is a
thread pool for repeatedly executing tasks at a given rate or delay. It is
effectively a more versatile replacement for the Timer/TimerTask
combination, as it allows multiple service threads, accepts various time
units, and doesn't require subclassing TimerTask (just implement Runnable).
Configuring ScheduledThreadPoolExecutor with one thread makes it equivalent
to Timer."

be amended, perhaps, with the following passage: ", except that Timer may
be more appropriate for scheduling recurring activities that are sensitive
to absolute time because Timer is more robust to the clock drift
than ScheduledThreadPoolExecutor."

 2) In https://bugs.openjdk.java.net/browse/JDK-8146527, David wrote

"Also note that we (JSR166 expert group) explicitly killed the
absolute-time schedule methods due to their problematic nature (as
evidenced by bugs reports on java.util.Timer functionality) - and that
wasn't even considering PC sleep/suspend/hibernate issues."

Could somebody please kindly point to these bugs on Timer functionality in
JDK issue tracker, or note whether they were fixed?

3) Quite unrelated to the previous, but since Timer is discussed rarely:
would it make sense to replace Timer's threadReaper field which is
currently an Object with a finalize() method overridden with a Cleaner?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200105/c7bd41c3/attachment.html>

From davidcholmes at aapt.net.au  Sun Jan  5 16:54:48 2020
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 6 Jan 2020 07:54:48 +1000
Subject: [concurrency-interest] java.util.Timer is good for wall time
 scheduling?
In-Reply-To: <CAAMLo=azs5MFQRwEjx_9hfyM5Q3Mc=ACUyGw4mWsCM04fiUarQ@mail.gmail.com>
References: <CAAMLo=azs5MFQRwEjx_9hfyM5Q3Mc=ACUyGw4mWsCM04fiUarQ@mail.gmail.com>
Message-ID: <020a01d5c412$c04720c0$40d56240$@aapt.net.au>

Hi Roman,

 

Regarding:

 

“Could somebody please kindly point to these bugs on Timer functionality in JDK issue tracker, or note whether they were fixed?”

 

See for example

 

https://bugs.openjdk.java.net/browse/JDK-4290274

 

and linked bugs.

 

Cheers,

David

 

From: Roman Leventov 
Sent: Monday, January 6, 2020 5:23 AM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>; markus at headcrashing.eu; davidcholmes at aapt.net.au
Subject: java.util.Timer is good for wall time scheduling?

 

There is an interesting question on StackOverflow with an evidence of rather extreme time drift when a task is scheduled periodically using ScheduledThreadPoolExecutor: https://stackoverflow.com/questions/56571647/why-does-the-java-scheduler-exhibit-significant-time-drift-on-windows. java.util.Timer appears to be a workaround for that problem. It looks to me that java.util.Timer is an only tool offered by JDK suitable for wall-clock time (cron-style) scheduling.

 

In this context, I would like to recall the discussion of hibernation and ScheduledThreadPoolExecutor behavior (https://bugs.openjdk.java.net/browse/JDK-8146527; the discussion thread in this list: http://cs.oswego.edu/pipermail/concurrency-interest/2016-January/014817.html). It seems actually that the desired case of "sending an e-mail each hour" could be pretty easily coded with Timer:

 

new Timer().scheduleAtFixedRate(new TimerTask() {

  long minutes = 0;

  @Override public void run() {

      if (minutes % 60 == 0 &&

          System.currentTimeMillis() - scheduledExecutionTime() < MINUTES.toMillis(1)) {

       sendEmail();

     }

     minutes++;

   }

}, nextRoundMinuteDate(), MINUTES.toMillis(1));

 

With this, I have a few questions:

 

 1) Should the Javadoc for Timer, which currently says:

 

"Java 5.0 introduced the java.util.concurrent package and one of the concurrency utilities therein is the ScheduledThreadPoolExecutor which is a thread pool for repeatedly executing tasks at a given rate or delay. It is effectively a more versatile replacement for the Timer/TimerTask combination, as it allows multiple service threads, accepts various time units, and doesn't require subclassing TimerTask (just implement Runnable). Configuring ScheduledThreadPoolExecutor with one thread makes it equivalent to Timer."

 

be amended, perhaps, with the following passage: ", except that Timer may be more appropriate for scheduling recurring activities that are sensitive to absolute time because Timer is more robust to the clock drift than ScheduledThreadPoolExecutor."

 

 2) In https://bugs.openjdk.java.net/browse/JDK-8146527, David wrote

 

"Also note that we (JSR166 expert group) explicitly killed the absolute-time schedule methods due to their problematic nature (as evidenced by bugs reports on java.util.Timer functionality) - and that wasn't even considering PC sleep/suspend/hibernate issues."

 

Could somebody please kindly point to these bugs on Timer functionality in JDK issue tracker, or note whether they were fixed?

 

3) Quite unrelated to the previous, but since Timer is discussed rarely: would it make sense to replace Timer's threadReaper field which is currently an Object with a finalize() method overridden with a Cleaner?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200106/61f870ec/attachment.html>

From leventov.ru at gmail.com  Mon Jan  6 04:33:09 2020
From: leventov.ru at gmail.com (Roman Leventov)
Date: Mon, 6 Jan 2020 12:33:09 +0300
Subject: [concurrency-interest] java.util.Timer is good for wall time
 scheduling?
In-Reply-To: <020a01d5c412$c04720c0$40d56240$@aapt.net.au>
References: <CAAMLo=azs5MFQRwEjx_9hfyM5Q3Mc=ACUyGw4mWsCM04fiUarQ@mail.gmail.com>
 <020a01d5c412$c04720c0$40d56240$@aapt.net.au>
Message-ID: <CAAMLo=bJYVExt-GAxKTkVSc3jYWKuVX8xMeTYmTCsFnriOObjg@mail.gmail.com>

Thank you.

Re: https://bugs.openjdk.java.net/browse/JDK-8209462

"The software should continue printing numbers. This is a real problem when
the daylight savings time arrives (summer/winter)."

May there be a bug in JVM/macOS? Daylight savings should not affect
currentTimeMillis(). Assuming the reporter actually experienced this
problem, rather than observed the delay from a manual time reset and then
extrapolated to the daylight saving case.

Re: https://bugs.openjdk.java.net/browse/JDK-4290274

I think there is something that definitely could be improved: a reference
to "countdown timer that ticks once every second for ten seconds" use case
removed from Timer Javadocs, because it definitely looks like the case
where ScheduledThreadPoolExecutor is a superior alternative all around.

Regarding a timer freeze when time is set backward, a possible solution
might be that TimerThread.mainLoop() detects the time going backward, and
in this case, walks through the queue and examines each TimerTask scheduled
at a fixed rate, "unwinding" it appropriately  (this may result in
rebuilding the whole queue, but that shouldn't be practically a performance
concern). The only semantic problem with this is that
TimerTask.scheduledExecutionTime() may now go backward in successive
executions, which might be unexpected by some clients.

On Mon, 6 Jan 2020 at 00:54, David Holmes <davidcholmes at aapt.net.au> wrote:

> Hi Roman,
>
>
>
> Regarding:
>
>
>
> “Could somebody please kindly point to these bugs on Timer functionality
> in JDK issue tracker, or note whether they were fixed?”
>
>
>
> See for example
>
>
>
> https://bugs.openjdk.java.net/browse/JDK-4290274
>
>
>
> and linked bugs.
>
>
>
> Cheers,
>
> David
>
>
>
> *From:* Roman Leventov
> *Sent:* Monday, January 6, 2020 5:23 AM
> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>;
> markus at headcrashing.eu; davidcholmes at aapt.net.au
> *Subject:* java.util.Timer is good for wall time scheduling?
>
>
>
> There is an interesting question on StackOverflow with an evidence of
> rather extreme time drift when a task is scheduled periodically using
> ScheduledThreadPoolExecutor:
> https://stackoverflow.com/questions/56571647/why-does-the-java-scheduler-exhibit-significant-time-drift-on-windows.
> java.util.Timer appears to be a workaround for that problem. It looks to me
> that java.util.Timer is an only tool offered by JDK suitable for wall-clock
> time (cron-style) scheduling.
>
>
>
> In this context, I would like to recall the discussion of hibernation and
> ScheduledThreadPoolExecutor behavior (
> https://bugs.openjdk.java.net/browse/JDK-8146527; the discussion thread
> in this list:
> http://cs.oswego.edu/pipermail/concurrency-interest/2016-January/014817.html).
> It seems actually that the desired case of "sending an e-mail each hour"
> could be pretty easily coded with Timer:
>
>
>
> new Timer().scheduleAtFixedRate(new TimerTask() {
>
>   long minutes = 0;
>
>   @Override public void run() {
>
>       if (minutes % 60 == 0 &&
>
>           System.currentTimeMillis() - scheduledExecutionTime() <
> MINUTES.toMillis(1)) {
>
>        sendEmail();
>
>      }
>
>      minutes++;
>
>    }
>
> }, nextRoundMinuteDate(), MINUTES.toMillis(1));
>
>
>
> With this, I have a few questions:
>
>
>
>  1) Should the Javadoc for Timer, which currently says:
>
>
>
> "Java 5.0 introduced the java.util.concurrent package and one of the
> concurrency utilities therein is the ScheduledThreadPoolExecutor which is a
> thread pool for repeatedly executing tasks at a given rate or delay. It is
> effectively a more versatile replacement for the Timer/TimerTask
> combination, as it allows multiple service threads, accepts various time
> units, and doesn't require subclassing TimerTask (just implement Runnable).
> Configuring ScheduledThreadPoolExecutor with one thread makes it equivalent
> to Timer."
>
>
>
> be amended, perhaps, with the following passage: ", except that Timer may
> be more appropriate for scheduling recurring activities that are sensitive
> to absolute time because Timer is more robust to the clock drift
> than ScheduledThreadPoolExecutor."
>
>
>
>  2) In https://bugs.openjdk.java.net/browse/JDK-8146527, David wrote
>
>
>
> "Also note that we (JSR166 expert group) explicitly killed the
> absolute-time schedule methods due to their problematic nature (as
> evidenced by bugs reports on java.util.Timer functionality) - and that
> wasn't even considering PC sleep/suspend/hibernate issues."
>
>
>
> Could somebody please kindly point to these bugs on Timer functionality in
> JDK issue tracker, or note whether they were fixed?
>
>
>
> 3) Quite unrelated to the previous, but since Timer is discussed rarely:
> would it make sense to replace Timer's threadReaper field which is
> currently an Object with a finalize() method overridden with a Cleaner?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200106/f90c6039/attachment-0001.html>

From mads at renxo.com  Thu Jan 16 14:43:27 2020
From: mads at renxo.com (Manuel Dominguez Sarmiento)
Date: Thu, 16 Jan 2020 16:43:27 -0300
Subject: [concurrency-interest] AtomicReferenceArray.get() and intrinsics
 method inlining
Message-ID: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>

Hi group,

For the past few weeks, we've been attempting to diagnose a caching 
performance issue. Profiling with YourKit it became evident that the 
caching layer (EhCache 2.10.x) was the culprit. This caching library 
uses a fork of ConcurrentHashMapV8 (pre-Java8) behind the scenes.

The ConcurrentHashMapV8 fork has been customized for internal EhCache 
usage, however looking at the source, it does not differ that much from 
the original ConcurrentHashMapV8. Sources for EhCache 2.10.5 can be 
downloaded from https://www.ehcache.org/downloads/
The ConcurrentHashMapV8 fork is 
net.sf.ehcache.util.concurrent.ConcurrentHashMap

Further profiling revealed that the hotspot was not within the 
ConcurrentHashMapV8 fork, but rather at 
j.u.c.atomic.AtomicReferenceArray.get()
Apparently, the EhCache team decided at some point to replace 
Unsafe.getObjectVolatile() with AtomicReferenceArray, presumably to 
avoid compiler warnings regarding the unsupported use of Unsafe (this 
was way before Java 9, the module system, Unsafe deprecation, etc.)

The logic seems straightforward and there are no obvious errors in their 
use of AtomicReferenceArray. The switch from Unsafe.getObjectVolatile() 
to AtomicReferenceArray.get() works fine.
However, we performed the following micro-benchmark: we inserted a 
single entry [key = Integer.valueOf(1); value = new Object()] and then 
performed map.get() a billion times for the same key.

The ConcurrentHashMapV8 took about 40 seconds on average to complete. 
However, stock Java 8 ConcurrentHashMap took only about 800 ms to 
complete the same task. Profiling showed that the ConcurrentHashMapV8 
fork hotspot was at AtomicReferenceArray.get(), which matched the issue 
we found in our production systems with very "hot" cache keys.

We produced a simple test class to exclude everything EhCache-related 
and we were able to reproduce this same behaviour with direct 
AtomicReferenceArray usage. This benchmark does not attempt to measure 
anything useful in the real world, but it's simple enough so that this 
performance difference became easily reproducible:

///////////////////////////////////////////////////////////////////////////////////////////////////////////////
public class Test {
     private static final Unsafe UNSAFE;
     private static final int KEY = 1;
     static {
         try {
             Field f = Unsafe.class.getDeclaredField("theUnsafe");
             f.setAccessible(true);
             UNSAFE = (Unsafe) f.get(null);
         } catch (Exception e) {
             throw new Error(e);
         }
     }

     public static void main(String[] args) {
         final long repetitions = 1_000_000_000L;
         System.out.println("ara=" + ara(repetitions) + " ms");
     }

     private static final long ara(final long repetitions) {
         final AtomicReferenceArray<Object> ara = new 
AtomicReferenceArray<Object>(100);
         ara.set(KEY, new Object());
         long start = System.nanoTime();
         for (long i = 0; i < repetitions; i++) {
             ara.get(KEY);
         }
         long end = System.nanoTime();
         return (end - start) / (1000 * 1000);
     }
}
///////////////////////////////////////////////////////////////////////////////////////////////////////////////

This test took about 40 seconds on the exact same hardware as previous 
CHM tests. So we concluded that AtomicReferenceArray.get() usage instead 
of Unsafe.getObjectVolatile() was the cause behind the 
ConcurrentHashMapV8 EhCache fork being so much slower than Java8 stock 
ConcurrentHashMap.

The difference is so huge that we suspected this could be a result of 
compiler optimizations resulting in very different bytecode, and 
possibly method inlining not working as expected with Unsafe instricts. 
This was dead on: we enabled the following JVM diagnosing options:
-server -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions 
-XX:+PrintInlining -XX:-TieredCompilation -XX:MaxInlineLevel=15

And the test yielded the following on standard output (only the relevant 
part is reproduced below):

  com.renxo.cms.Test::ara @ 32 (65 bytes)
                             @ 34 
java.util.concurrent.atomic.AtomicReferenceArray::get (10 bytes) inline 
(hot)
                               @ 3 
java.util.concurrent.atomic.AtomicReferenceArray::checkedByteOffset (45 
bytes)   inline (hot)
                                 @ 41 
java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 
bytes)   inline (hot)
                               @ 6 
java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)   
inline (hot)
                                 @ 8 sun.misc.Unsafe::getObjectVolatile 
(0 bytes)   failed to inline (intrinsic)
                                 @ 8 sun.misc.Unsafe::getObjectVolatile 
(0 bytes)   native method

So this is the interesting bit:
sun.misc.Unsafe::getObjectVolatile (0 bytes)   failed to inline (intrinsic)

Running a similar benchmark against stock Java8 ConcurrentHashMap.get() 
showed that sun.misc.Unsafe::getObjectVolatile was being inlined just 
fine. So something at AtomicReferenceArray.get() is preventing 
getObjectVolatile() to be inlined. Since this is an intrinsic function, 
performance difference should be huge.

After careful studying of stock Java8 ConcurrentHashMap.get(), we found 
that the reason why that method was being successfully inlined is the 
(tab = table) != null check before tabAt() is invoked. Apparently, the 
HotSpot compiler is unable to inline getObjectVolatile() unless it can 
verify that its main argument will always be non-null.

So, we created an AtomicReferenceArray fork with an alternative getRaw() 
implementation:

     private E getRaw(long offset) {
         Object[] array = this.array;
         if (array == null) {
             throw new IllegalStateException();
         }
         return (E) unsafe.getObjectVolatile(array, offset);
     }

This resulted in the test taking less than 500 ms instead of 40 seconds 
(that's an 80-fold difference!). And the compiler was now successfully 
inlining:

com.renxo.cms.Test::ara @ 32 (65 bytes)
                             @ 34 
com.renxo.cms.AtomicReferenceArray::get (10 bytes)   inline (hot)
                               @ 3 
com.renxo.cms.AtomicReferenceArray::checkedByteOffset (42 bytes) inline 
(hot)
                                 @ 38 
com.renxo.cms.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)
                               @ 6 
com.renxo.cms.AtomicReferenceArray::getRaw (26 bytes)   inline (hot)
                                 @ 22 sun.misc.Unsafe::getObjectVolatile 
(0 bytes)   (intrinsic)

We were extremely surprised that AtomicReferenceArray being a core 
library could benefit so much from such a small change. It is likely 
that our particular test is not representative of real-world workloads, 
but still, is there any reason why this fix should not be introduced in 
OpenJDK? We see that the current trunk AtomicReferenceArray no longer 
uses Unsafe.getObjectVolatile() and instead uses VarHandles. We're still 
on Java8 so we did not test this on Java9 or later. But it is possible 
that the same issue is still there.

Also, we just focused on AtomicReferenceArray.get(). It is likely that 
other methods using Unsafe having similar inlining issues. Also, other 
j.u.c.atomic classes might exhibit similar inlining behaviour.

Concurrency experts, we'd appreciate your feedback on our findings.
Thanks,

*Manuel Dominguez Sarmiento*

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/e8f961f7/attachment.htm>

From shade at redhat.com  Thu Jan 16 15:15:23 2020
From: shade at redhat.com (Aleksey Shipilev)
Date: Thu, 16 Jan 2020 21:15:23 +0100
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
Message-ID: <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>

On 1/16/20 8:43 PM, Manuel Dominguez Sarmiento via Concurrency-interest wrote:
>     private static final long ara(final long repetitions) {
>         final AtomicReferenceArray<Object> ara = new AtomicReferenceArray<Object>(100);
>         ara.set(KEY, new Object());
>         long start = System.nanoTime();
>         for (long i = 0; i < repetitions; i++) {
>             ara.get(KEY);
>         }
>         long end = System.nanoTime();
>         return (end - start) / (1000 * 1000);
>     }
> }

For the love of all that is holy, use JMH:
  https://openjdk.java.net/projects/code-tools/jmh/

> This test took about 40 seconds on the exact same hardware as previous CHM tests. So we concluded
> that AtomicReferenceArray.get() usage instead of Unsafe.getObjectVolatile() was the cause behind the
> ConcurrentHashMapV8 EhCache fork being so much slower than Java8 stock ConcurrentHashMap.

> So this is the interesting bit:
> sun.misc.Unsafe::getObjectVolatile (0 bytes)   failed to inline (intrinsic)

Cannot reproduce:

$ java -server -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining
-XX:-TieredCompilation -XX:MaxInlineLevel=15 Test
java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)

   @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)

java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)

   @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)

   @ 3   java.util.concurrent.atomic.AtomicReferenceArray::checkedByteOffset (45 bytes)   inline
(hot)
     @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)

   @ 6   java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)   inline (hot)

     @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)

Test::ara @ 29 (65 bytes)

   @ 38   java.util.concurrent.atomic.AtomicReferenceArray::get (10 bytes)   inline (hot)

     @ 3   java.util.concurrent.atomic.AtomicReferenceArray::checkedByteOffset (45 bytes)   inline
(hot)
       @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)

     @ 6   java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)   inline (hot)

       @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)

Test::ara @ -2 (65 bytes)   made not entrant

ara=429 ms


$ java -version
openjdk version "1.8.0_232"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)


> After careful studying of stock Java8 ConcurrentHashMap.get(), we found that the reason why that
>  method was being successfully inlined is the (tab = table) != null check before tabAt() is 
> invoked. Apparently, the HotSpot compiler is unable to inline getObjectVolatile() unless it can 
> verify that its main argument will always be non-null.
If true, that would qualify as performance bug. Hotspot should be able to inline Unsafe accessors,
and it would emit runtime null-checks if it is not sure about nullity.

Please try with more up to date JDK binary.

> The ConcurrentHashMapV8 took about 40 seconds on average to complete. However, stock Java 8 
> ConcurrentHashMap took only about 800 ms to complete the same task. Profiling showed that the 
> ConcurrentHashMapV8 fork hotspot was at AtomicReferenceArray.get(), which matched the issue we 
> found in our production systems with very "hot" cache keys.
The more likely causes would be:
 a) additional memory dereference every time "table" is accessed. In that frankenstein-monster of
CHM it would be additional dereference to reach the backing array in ARA itself;
 b) profiling skew that misattributed the bottleneck to ARA.get();
 c) some subtle difference between the fork and the "stock" CHM version;

The trouble here is that you have the minimized test that does not show the problem :/ Please
provide MCVE for the actual problem you are chasing (pull the exact CHM sources into there, if you
have to), and full details on the environment you run the test in.

-- 
Thanks,
-Aleksey


From mads at renxo.com  Thu Jan 16 15:28:56 2020
From: mads at renxo.com (Manuel Dominguez Sarmiento)
Date: Thu, 16 Jan 2020 17:28:56 -0300
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
Message-ID: <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>

Thanks Aleksey for your feedback. We know about JHM but we have no 
experience with it. We'll definitely use it in the future.
We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported 
results. Update 212 is from April 2019 so it's not that old anyway.
We'll re-test on the latest 1.8.x JDK and report back.

*Manuel Dominguez Sarmiento*

On 16/01/2020 17:15, Aleksey Shipilev wrote:
> On 1/16/20 8:43 PM, Manuel Dominguez Sarmiento via Concurrency-interest wrote:
>>      private static final long ara(final long repetitions) {
>>          final AtomicReferenceArray<Object> ara = new AtomicReferenceArray<Object>(100);
>>          ara.set(KEY, new Object());
>>          long start = System.nanoTime();
>>          for (long i = 0; i < repetitions; i++) {
>>              ara.get(KEY);
>>          }
>>          long end = System.nanoTime();
>>          return (end - start) / (1000 * 1000);
>>      }
>> }
> For the love of all that is holy, use JMH:
>    https://openjdk.java.net/projects/code-tools/jmh/
>
>> This test took about 40 seconds on the exact same hardware as previous CHM tests. So we concluded
>> that AtomicReferenceArray.get() usage instead of Unsafe.getObjectVolatile() was the cause behind the
>> ConcurrentHashMapV8 EhCache fork being so much slower than Java8 stock ConcurrentHashMap.
>> So this is the interesting bit:
>> sun.misc.Unsafe::getObjectVolatile (0 bytes)   failed to inline (intrinsic)
> Cannot reproduce:
>
> $ java -server -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining
> -XX:-TieredCompilation -XX:MaxInlineLevel=15 Test
> java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)
>
>     @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)
>
> java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)
>
>     @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)
>
>     @ 3   java.util.concurrent.atomic.AtomicReferenceArray::checkedByteOffset (45 bytes)   inline
> (hot)
>       @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)
>
>     @ 6   java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)   inline (hot)
>
>       @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)
>
> Test::ara @ 29 (65 bytes)
>
>     @ 38   java.util.concurrent.atomic.AtomicReferenceArray::get (10 bytes)   inline (hot)
>
>       @ 3   java.util.concurrent.atomic.AtomicReferenceArray::checkedByteOffset (45 bytes)   inline
> (hot)
>         @ 41   java.util.concurrent.atomic.AtomicReferenceArray::byteOffset (12 bytes)   inline (hot)
>
>       @ 6   java.util.concurrent.atomic.AtomicReferenceArray::getRaw (12 bytes)   inline (hot)
>
>         @ 8   sun.misc.Unsafe::getObjectVolatile (0 bytes)   (intrinsic)
>
> Test::ara @ -2 (65 bytes)   made not entrant
>
> ara=429 ms
>
>
> $ java -version
> openjdk version "1.8.0_232"
> OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_232-b09)
> OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.232-b09, mixed mode)
>
>
>> After careful studying of stock Java8 ConcurrentHashMap.get(), we found that the reason why that
>>   method was being successfully inlined is the (tab = table) != null check before tabAt() is
>> invoked. Apparently, the HotSpot compiler is unable to inline getObjectVolatile() unless it can
>> verify that its main argument will always be non-null.
> If true, that would qualify as performance bug. Hotspot should be able to inline Unsafe accessors,
> and it would emit runtime null-checks if it is not sure about nullity.
>
> Please try with more up to date JDK binary.
>
>> The ConcurrentHashMapV8 took about 40 seconds on average to complete. However, stock Java 8
>> ConcurrentHashMap took only about 800 ms to complete the same task. Profiling showed that the
>> ConcurrentHashMapV8 fork hotspot was at AtomicReferenceArray.get(), which matched the issue we
>> found in our production systems with very "hot" cache keys.
> The more likely causes would be:
>   a) additional memory dereference every time "table" is accessed. In that frankenstein-monster of
> CHM it would be additional dereference to reach the backing array in ARA itself;
>   b) profiling skew that misattributed the bottleneck to ARA.get();
>   c) some subtle difference between the fork and the "stock" CHM version;
>
> The trouble here is that you have the minimized test that does not show the problem :/ Please
> provide MCVE for the actual problem you are chasing (pull the exact CHM sources into there, if you
> have to), and full details on the environment you run the test in.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/465bf1c3/attachment.htm>

From shade at redhat.com  Thu Jan 16 15:35:32 2020
From: shade at redhat.com (Aleksey Shipilev)
Date: Thu, 16 Jan 2020 21:35:32 +0100
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
Message-ID: <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>

On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
> We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported results. Update 212 is from April
> 2019 so it's not that old anyway.

Wait, now *that* sounds familiar.

Plus the original observation:

> After careful studying of stock Java8 ConcurrentHashMap.get(), we found that the reason why that
>  method was being successfully inlined is the (tab = table) != null check before tabAt() is 
> invoked. Apparently, the HotSpot compiler is unable to inline getObjectVolatile() unless it can 
> verify thatits main argument will always be non-null.
Suggests this:
  https://bugs.openjdk.java.net/browse/JDK-8221355

You should really try up-to-date JDK.

-- 
Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/93d1cf39/attachment.sig>

From ben.manes at gmail.com  Thu Jan 16 15:38:39 2020
From: ben.manes at gmail.com (Benjamin Manes)
Date: Thu, 16 Jan 2020 12:38:39 -0800
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
 <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
Message-ID: <CAGu0=MOAxnzXemK2QaDJ56Rx9EWPSt57Nf4R=psxsz6eqXhyiw@mail.gmail.com>

I have not investigated Ehcache 2.x for a while, but it used to
use SelectableConcurrentHashMap [1] which is a fork of Java 5's map. In
that fork, the lock-free reads is replaced by a per-segment read lock.

At the time of 2.10.4, this was the default implementation when creating a
cache in a benchmark [2] as,
    CacheConfiguration config = new CacheConfiguration("benchmark",
maximumSize);
    config.setMemoryStoreEvictionPolicyFromObject(evictionPolicy);
    cache = new Cache(config);

That JMH benchmark ran with a Zipfian distribution (so hot keys are
accessed more frequently) and I observed ~20M gets per second at 16 cores,
using the LRU policy.

The benchmark might be a good starting point. I had to remove v2 when it
became incompatible with v3 due to [3].

This was applicable around 2014-15 timeframe and I have not looked at it
since, so beware of my possible misunderstandings.

[1]
http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/store/chm/SelectableConcurrentHashMap.java
[2]
https://github.com/ben-manes/caffeine/blob/master/caffeine/src/jmh/java/com/github/benmanes/caffeine/cache/GetPutBenchmark.java
[3] https://github.com/ehcache/ehcache3/issues/2334

On Thu, Jan 16, 2020 at 12:36 PM Aleksey Shipilev via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
> > We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported
> results. Update 212 is from April
> > 2019 so it's not that old anyway.
>
> Wait, now *that* sounds familiar.
>
> Plus the original observation:
>
> > After careful studying of stock Java8 ConcurrentHashMap.get(), we found
> that the reason why that
> >  method was being successfully inlined is the (tab = table) != null
> check before tabAt() is
> > invoked. Apparently, the HotSpot compiler is unable to inline
> getObjectVolatile() unless it can
> > verify thatits main argument will always be non-null.
> Suggests this:
>   https://bugs.openjdk.java.net/browse/JDK-8221355
>
> You should really try up-to-date JDK.
>
> --
> Thanks,
> -Aleksey
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/993b62d0/attachment-0001.htm>

From mads at renxo.com  Thu Jan 16 15:57:29 2020
From: mads at renxo.com (Manuel Dominguez Sarmiento)
Date: Thu, 16 Jan 2020 17:57:29 -0300
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
 <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
Message-ID: <e515af93-b2ae-960d-a7e2-04dc853ab168@renxo.com>

Repeated the test on 8u241 ... the issue is GONE
Also, testing the forked ConcurrentHashMapV8, performance is now 
up-to-par with stock Java8 ConcurrentHashMap.

Seems JDK-8221355 was the culprit.
Thanks Aleksey!

> On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
>> We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported results. Update 212 is from April
>> 2019 so it's not that old anyway.
> Wait, now *that* sounds familiar.
>
> Plus the original observation:
>
>> After careful studying of stock Java8 ConcurrentHashMap.get(), we found that the reason why that
>>   method was being successfully inlined is the (tab = table) != null check before tabAt() is
>> invoked. Apparently, the HotSpot compiler is unable to inline getObjectVolatile() unless it can
>> verify thatits main argument will always be non-null.
> Suggests this:
>    https://bugs.openjdk.java.net/browse/JDK-8221355
>
> You should really try up-to-date JDK.
>


From mads at renxo.com  Thu Jan 16 16:09:03 2020
From: mads at renxo.com (Manuel Dominguez Sarmiento)
Date: Thu, 16 Jan 2020 18:09:03 -0300
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <CAGu0=MOAxnzXemK2QaDJ56Rx9EWPSt57Nf4R=psxsz6eqXhyiw@mail.gmail.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
 <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
 <CAGu0=MOAxnzXemK2QaDJ56Rx9EWPSt57Nf4R=psxsz6eqXhyiw@mail.gmail.com>
Message-ID: <8a709767-1643-8312-d55b-6861a53a2e6e@renxo.com>

Hi Ben, thanks to your comment, we found an issue in some critical 
interning and reflection-related classes in our codebase: they were 
using net.sf.ehcache.util.concurrent.ConcurrentHashMap instead of 
java.util.concurrent.ConcurrentHashMap

It seems Eclipse's auto-import feature picked the "wrong" 
ConcurrentHashMap ... so that's why we're seeing the 
net.sf.ehcache.util.concurrent.ConcurrentHashMap in our profiler even if 
it's not the default map EhCache might be using for the actual caches.

Lessons learned: always use an up-to-date JDK and never trust Eclipse's 
auto-import feature.

> I have not investigated Ehcache 2.x for a while, but it used to 
> use SelectableConcurrentHashMap [1] which is a fork of Java 5's map. 
> In that fork, the lock-free reads is replaced by a per-segment read lock.
>
> At the time of 2.10.4, this was the default implementation when 
> creating a cache in a benchmark [2] as,
>     CacheConfiguration config = new CacheConfiguration("benchmark", 
> maximumSize);
> config.setMemoryStoreEvictionPolicyFromObject(evictionPolicy);
>     cache = new Cache(config);
>
> That JMH benchmark ran with a Zipfian distribution (so hot keys are 
> accessed more frequently) and I observed ~20M gets per second at 16 
> cores, using the LRU policy.
>
> The benchmark might be a good starting point. I had to remove v2 when 
> it became incompatible with v3 due to [3].
>
> This was applicable around 2014-15 timeframe and I have not looked at 
> it since, so beware of my possible misunderstandings.
>
> [1] 
> http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/store/chm/SelectableConcurrentHashMap.java
> [2] 
> https://github.com/ben-manes/caffeine/blob/master/caffeine/src/jmh/java/com/github/benmanes/caffeine/cache/GetPutBenchmark.java
> [3] https://github.com/ehcache/ehcache3/issues/2334
>
> On Thu, Jan 16, 2020 at 12:36 PM Aleksey Shipilev via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>
>     On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
>     > We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported
>     results. Update 212 is from April
>     > 2019 so it's not that old anyway.
>
>     Wait, now *that* sounds familiar.
>
>     Plus the original observation:
>
>     > After careful studying of stock Java8 ConcurrentHashMap.get(),
>     we found that the reason why that
>     >  method was being successfully inlined is the (tab = table) !=
>     null check before tabAt() is
>     > invoked. Apparently, the HotSpot compiler is unable to inline
>     getObjectVolatile() unless it can
>     > verify thatits main argument will always be non-null.
>     Suggests this:
>     https://bugs.openjdk.java.net/browse/JDK-8221355
>
>     You should really try up-to-date JDK.
>
>     -- 
>     Thanks,
>     -Aleksey
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/3c34ff17/attachment.htm>

From nigro.fra at gmail.com  Thu Jan 16 16:52:50 2020
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Thu, 16 Jan 2020 22:52:50 +0100
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <8a709767-1643-8312-d55b-6861a53a2e6e@renxo.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
 <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
 <CAGu0=MOAxnzXemK2QaDJ56Rx9EWPSt57Nf4R=psxsz6eqXhyiw@mail.gmail.com>
 <8a709767-1643-8312-d55b-6861a53a2e6e@renxo.com>
Message-ID: <CAKxGtTXOWyg0XtNkt-9spshS41Ox5cww1J-qn62qowq+XbtKeA@mail.gmail.com>

@manuel and attempts to use JMH too! :P

Il gio 16 gen 2020, 22:09 Manuel Dominguez Sarmiento via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> Hi Ben, thanks to your comment, we found an issue in some critical
> interning and reflection-related classes in our codebase: they were using
> net.sf.ehcache.util.concurrent.ConcurrentHashMap instead of
> java.util.concurrent.ConcurrentHashMap
>
> It seems Eclipse's auto-import feature picked the "wrong"
> ConcurrentHashMap ... so that's why we're seeing the
> net.sf.ehcache.util.concurrent.ConcurrentHashMap in our profiler even if
> it's not the default map EhCache might be using for the actual caches.
>
> Lessons learned: always use an up-to-date JDK and never trust Eclipse's
> auto-import feature.
>
> I have not investigated Ehcache 2.x for a while, but it used to
> use SelectableConcurrentHashMap [1] which is a fork of Java 5's map. In
> that fork, the lock-free reads is replaced by a per-segment read lock.
>
> At the time of 2.10.4, this was the default implementation when creating a
> cache in a benchmark [2] as,
>     CacheConfiguration config = new CacheConfiguration("benchmark",
> maximumSize);
>     config.setMemoryStoreEvictionPolicyFromObject(evictionPolicy);
>     cache = new Cache(config);
>
> That JMH benchmark ran with a Zipfian distribution (so hot keys are
> accessed more frequently) and I observed ~20M gets per second at 16 cores,
> using the LRU policy.
>
> The benchmark might be a good starting point. I had to remove v2 when it
> became incompatible with v3 due to [3].
>
> This was applicable around 2014-15 timeframe and I have not looked at it
> since, so beware of my possible misunderstandings.
>
> [1]
> http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/store/chm/SelectableConcurrentHashMap.java
> [2]
> https://github.com/ben-manes/caffeine/blob/master/caffeine/src/jmh/java/com/github/benmanes/caffeine/cache/GetPutBenchmark.java
> [3] https://github.com/ehcache/ehcache3/issues/2334
>
> On Thu, Jan 16, 2020 at 12:36 PM Aleksey Shipilev via Concurrency-interest
> <concurrency-interest at cs.oswego.edu> wrote:
>
>> On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
>> > We used Oracle JDK 1.8.0_212 on Mac OS X to produce the reported
>> results. Update 212 is from April
>> > 2019 so it's not that old anyway.
>>
>> Wait, now *that* sounds familiar.
>>
>> Plus the original observation:
>>
>> > After careful studying of stock Java8 ConcurrentHashMap.get(), we found
>> that the reason why that
>> >  method was being successfully inlined is the (tab = table) != null
>> check before tabAt() is
>> > invoked. Apparently, the HotSpot compiler is unable to inline
>> getObjectVolatile() unless it can
>> > verify thatits main argument will always be non-null.
>> Suggests this:
>>   https://bugs.openjdk.java.net/browse/JDK-8221355
>>
>> You should really try up-to-date JDK.
>>
>> --
>> Thanks,
>> -Aleksey
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/74312025/attachment-0001.htm>

From mads at renxo.com  Thu Jan 16 16:55:54 2020
From: mads at renxo.com (Manuel Dominguez Sarmiento)
Date: Thu, 16 Jan 2020 18:55:54 -0300
Subject: [concurrency-interest] AtomicReferenceArray.get() and
 intrinsics method inlining
In-Reply-To: <CAKxGtTXOWyg0XtNkt-9spshS41Ox5cww1J-qn62qowq+XbtKeA@mail.gmail.com>
References: <4318844c-b435-1419-6b86-efb2826a2ad0@renxo.com>
 <e98bd675-01d8-3d2b-3601-a5eebc2e6ddc@redhat.com>
 <05eec9a6-cf28-77c9-3f17-48814995c86c@renxo.com>
 <c7ef66ac-4eeb-cd47-a20e-f4228b01d5ba@redhat.com>
 <CAGu0=MOAxnzXemK2QaDJ56Rx9EWPSt57Nf4R=psxsz6eqXhyiw@mail.gmail.com>
 <8a709767-1643-8312-d55b-6861a53a2e6e@renxo.com>
 <CAKxGtTXOWyg0XtNkt-9spshS41Ox5cww1J-qn62qowq+XbtKeA@mail.gmail.com>
Message-ID: <688b76da-bef7-8075-01c1-a0ca2ddc21f4@renxo.com>

Sure :-)

On 16/01/2020 18:52, Francesco Nigro wrote:
> @manuel and attempts to use JMH too! :P
>
> Il gio 16 gen 2020, 22:09 Manuel Dominguez Sarmiento via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
>
>     Hi Ben, thanks to your comment, we found an issue in some critical
>     interning and reflection-related classes in our codebase: they
>     were using net.sf.ehcache.util.concurrent.ConcurrentHashMap
>     instead of java.util.concurrent.ConcurrentHashMap
>
>     It seems Eclipse's auto-import feature picked the "wrong"
>     ConcurrentHashMap ... so that's why we're seeing the
>     net.sf.ehcache.util.concurrent.ConcurrentHashMap in our profiler
>     even if it's not the default map EhCache might be using for the
>     actual caches.
>
>     Lessons learned: always use an up-to-date JDK and never trust
>     Eclipse's auto-import feature.
>
>>     I have not investigated Ehcache 2.x for a while, but it used to
>>     use SelectableConcurrentHashMap [1] which is a fork of Java 5's
>>     map. In that fork, the lock-free reads is replaced by a
>>     per-segment read lock.
>>
>>     At the time of 2.10.4, this was the default implementation when
>>     creating a cache in a benchmark [2] as,
>>         CacheConfiguration config = new
>>     CacheConfiguration("benchmark", maximumSize);
>>     config.setMemoryStoreEvictionPolicyFromObject(evictionPolicy);
>>         cache = new Cache(config);
>>
>>     That JMH benchmark ran with a Zipfian distribution (so hot keys
>>     are accessed more frequently) and I observed ~20M gets per second
>>     at 16 cores, using the LRU policy.
>>
>>     The benchmark might be a good starting point. I had to remove v2
>>     when it became incompatible with v3 due to [3].
>>
>>     This was applicable around 2014-15 timeframe and I have not
>>     looked at it since, so beware of my possible misunderstandings.
>>
>>     [1]
>>     http://svn.terracotta.org/svn/ehcache/trunk/ehcache/ehcache-core/src/main/java/net/sf/ehcache/store/chm/SelectableConcurrentHashMap.java
>>     [2]
>>     https://github.com/ben-manes/caffeine/blob/master/caffeine/src/jmh/java/com/github/benmanes/caffeine/cache/GetPutBenchmark.java
>>     [3] https://github.com/ehcache/ehcache3/issues/2334
>>
>>     On Thu, Jan 16, 2020 at 12:36 PM Aleksey Shipilev via
>>     Concurrency-interest <concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>>
>>         On 1/16/20 9:28 PM, Manuel Dominguez Sarmiento wrote:
>>         > We used Oracle JDK 1.8.0_212 on Mac OS X to produce the
>>         reported results. Update 212 is from April
>>         > 2019 so it's not that old anyway.
>>
>>         Wait, now *that* sounds familiar.
>>
>>         Plus the original observation:
>>
>>         > After careful studying of stock Java8
>>         ConcurrentHashMap.get(), we found that the reason why that
>>         >  method was being successfully inlined is the (tab = table)
>>         != null check before tabAt() is
>>         > invoked. Apparently, the HotSpot compiler is unable to
>>         inline getObjectVolatile() unless it can
>>         > verify thatits main argument will always be non-null.
>>         Suggests this:
>>         https://bugs.openjdk.java.net/browse/JDK-8221355
>>
>>         You should really try up-to-date JDK.
>>
>>         -- 
>>         Thanks,
>>         -Aleksey
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200116/e81b1256/attachment.htm>

From dl at cs.oswego.edu  Fri Jan 17 16:39:00 2020
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 17 Jan 2020 16:39:00 -0500
Subject: [concurrency-interest] ForkJoin refresh
Message-ID: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>


Now committed in jsr166 are extensions of the AQS refresh of last fall
to also cover ForkJoin; among other things removing reliance on builtin
monitors  The extra few months were mainly a result of noticing that the
initial way I did this increased performance differences across garbage
collectors. These are now reduced (not increased), but still worth
noting: FJ programs are relatively sensitive to GC choice because they
entail work-stealing duels between program and collector. On hotspot,
usually the best throughput for divide-and-conquer style usages
(including parallelStreams) is with -XX:+UseParallelGC. For unstructured
computations with lots of little tasks (for example little
CompletableFutures), UseShenandoahGC (and/or if needing more than 32GB
heap, UseZGC) are often better choices.  UseParallelGC meshes well with
highly generational structured parallelism, but is prone to
card-mark-contention with small tasks (even with -XX:+UseCondCardMark)
and long pauses. The default UseG1GC tends to be somewhere in the
middle. Also, on linux, using -XX:+UseTransparentHugePages seems to
always be a good idea for programs using FJ. (Other switches seem to
have less consistent positive effects.)

These days, it is almost impossible to quantify exactly how much better
performance is, but it's likely that your usages are faster. For
example, about half of the programs in the recent Renaissance Suite
(https://renaissance.dev/) somehow use FJ, and most of these have
improved scores on most machines, most JVMs (including OpenJ9 and
Graal), most collectors, most phases of the moon....

It would be great to get feedback from other usages before integrating
into OpenJDK. As usual, you can try it with any JDK11+ JVM by grabbing
http://gee.cs.oswego.edu/dl/concurrent/dist/jsr166.jar and running "java
--patch-module java.base="$DIR/jsr166.jar", where DIR is the full file
prefix. (Although beware that using --patch-module slows down startup,
and occasionally entire test runs. For other options, see
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html).

-Doug


From heinz at javaspecialists.eu  Fri Jan 17 16:54:07 2020
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Fri, 17 Jan 2020 16:54:07 -0500
Subject: [concurrency-interest] ForkJoin refresh
In-Reply-To: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
References: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
Message-ID: <27b7bfa3-c3d4-8f8c-2c0b-88e5b1d952e0@javaspecialists.eu>

Thanks Doug - how different is this to the Java 14-EA version?  I 
noticed the ManagedBlocker implementation today whilst showing the 
Condition.await() implementation to a class I was teaching and even 
tweeted about it: https://twitter.com/heinzkabutz/status/1218204292626243585

Funny coincidence :-)

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
Java Champion - www.javachampions.org
JavaOne Rock Star Speaker
Tel: +30 69 75 595 262
Skype: kabutz

On 2020/01/17 16:39, Doug Lea via Concurrency-interest wrote:
> Now committed in jsr166 are extensions of the AQS refresh of last fall
> to also cover ForkJoin; among other things removing reliance on builtin
> monitors  The extra few months were mainly a result of noticing that the
> initial way I did this increased performance differences across garbage
> collectors. These are now reduced (not increased), but still worth
> noting: FJ programs are relatively sensitive to GC choice because they
> entail work-stealing duels between program and collector. On hotspot,
> usually the best throughput for divide-and-conquer style usages
> (including parallelStreams) is with -XX:+UseParallelGC. For unstructured
> computations with lots of little tasks (for example little
> CompletableFutures), UseShenandoahGC (and/or if needing more than 32GB
> heap, UseZGC) are often better choices.  UseParallelGC meshes well with
> highly generational structured parallelism, but is prone to
> card-mark-contention with small tasks (even with -XX:+UseCondCardMark)
> and long pauses. The default UseG1GC tends to be somewhere in the
> middle. Also, on linux, using -XX:+UseTransparentHugePages seems to
> always be a good idea for programs using FJ. (Other switches seem to
> have less consistent positive effects.)
>
> These days, it is almost impossible to quantify exactly how much better
> performance is, but it's likely that your usages are faster. For
> example, about half of the programs in the recent Renaissance Suite
> (https://renaissance.dev/) somehow use FJ, and most of these have
> improved scores on most machines, most JVMs (including OpenJ9 and
> Graal), most collectors, most phases of the moon....
>
> It would be great to get feedback from other usages before integrating
> into OpenJDK. As usual, you can try it with any JDK11+ JVM by grabbing
> http://gee.cs.oswego.edu/dl/concurrent/dist/jsr166.jar and running "java
> --patch-module java.base="$DIR/jsr166.jar", where DIR is the full file
> prefix. (Although beware that using --patch-module slows down startup,
> and occasionally entire test runs. For other options, see
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html).
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From nigro.fra at gmail.com  Fri Jan 17 17:11:28 2020
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Fri, 17 Jan 2020 23:11:28 +0100
Subject: [concurrency-interest] ForkJoin refresh
In-Reply-To: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
References: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
Message-ID: <CAKxGtTU6njsSUwoTXPKbjkViNV4PZQ3-UwuLz_41qGqqR1nX7A@mail.gmail.com>

Thanks for sharing!

Probably a naive question...
How these results are been obtained?
There is any Open-Source repository that shows the source of the tests
performed?


Thanks,
Francesco

Il ven 17 gen 2020, 22:40 Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

>
> Now committed in jsr166 are extensions of the AQS refresh of last fall
> to also cover ForkJoin; among other things removing reliance on builtin
> monitors  The extra few months were mainly a result of noticing that the
> initial way I did this increased performance differences across garbage
> collectors. These are now reduced (not increased), but still worth
> noting: FJ programs are relatively sensitive to GC choice because they
> entail work-stealing duels between program and collector. On hotspot,
> usually the best throughput for divide-and-conquer style usages
> (including parallelStreams) is with -XX:+UseParallelGC. For unstructured
> computations with lots of little tasks (for example little
> CompletableFutures), UseShenandoahGC (and/or if needing more than 32GB
> heap, UseZGC) are often better choices.  UseParallelGC meshes well with
> highly generational structured parallelism, but is prone to
> card-mark-contention with small tasks (even with -XX:+UseCondCardMark)
> and long pauses. The default UseG1GC tends to be somewhere in the
> middle. Also, on linux, using -XX:+UseTransparentHugePages seems to
> always be a good idea for programs using FJ. (Other switches seem to
> have less consistent positive effects.)
>
> These days, it is almost impossible to quantify exactly how much better
> performance is, but it's likely that your usages are faster. For
> example, about half of the programs in the recent Renaissance Suite
> (https://renaissance.dev/) somehow use FJ, and most of these have
> improved scores on most machines, most JVMs (including OpenJ9 and
> Graal), most collectors, most phases of the moon....
>
> It would be great to get feedback from other usages before integrating
> into OpenJDK. As usual, you can try it with any JDK11+ JVM by grabbing
> http://gee.cs.oswego.edu/dl/concurrent/dist/jsr166.jar and running "java
> --patch-module java.base="$DIR/jsr166.jar", where DIR is the full file
> prefix. (Although beware that using --patch-module slows down startup,
> and occasionally entire test runs. For other options, see
> http://gee.cs.oswego.edu/dl/concurrency-interest/index.html).
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200117/9d7d5e97/attachment.htm>

From dl at cs.oswego.edu  Sat Jan 18 08:05:32 2020
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 18 Jan 2020 08:05:32 -0500
Subject: [concurrency-interest] ForkJoin refresh
In-Reply-To: <CAKxGtTU6njsSUwoTXPKbjkViNV4PZQ3-UwuLz_41qGqqR1nX7A@mail.gmail.com>
References: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
 <CAKxGtTU6njsSUwoTXPKbjkViNV4PZQ3-UwuLz_41qGqqR1nX7A@mail.gmail.com>
Message-ID: <cf4c4c86-b8ca-f1c5-e4c7-864eecb1a7c9@cs.oswego.edu>

On 1/17/20 5:11 PM, Francesco Nigro wrote:

> How these results are been obtained?

I'm not set up to report performance on a published benchmark suite, so
just informally characterized results. Checking the Renaissance suite is
helpful in avoiding over-reliance on microbenchmarks that might not
capture typical uses. But coming up with results people could readily
compare with others is a lot of work. It would be great if anyone wants
to take this on.

Also currently, some programs in the suite (version 0.10) don't work
with JDK12+. To run the ones that appear to work (although in two cases
with warnings that might bear on results), and use FJ (in a few cases
not all that much) try:

java --patch-module java.base=jsr166.jar [... gc settings ...] \
  -jar renaissance-gpl-0.10.0.jar \

fj-kmeans,future-genetic,scrabble,reactors,par-mnemonics,neo4j-analytics,finagle-http,finagle-chirper

With --csv option for results, and/or use their instructions to run
under JMH at https://github.com/renaissance-benchmarks/renaissance/
> 
> Thanks,
> Francesco
> 
> Il ven 17 gen 2020, 22:40 Doug Lea via Concurrency-interest
> <concurrency-interest at cs.oswego.edu
> <mailto:concurrency-interest at cs.oswego.edu>> ha scritto:
> 
> 
>     Now committed in jsr166 are extensions of the AQS refresh of last fall
>     to also cover ForkJoin; among other things removing reliance on builtin
>     monitors  The extra few months were mainly a result of noticing that the
>     initial way I did this increased performance differences across garbage
>     collectors. These are now reduced (not increased), but still worth
>     noting: FJ programs are relatively sensitive to GC choice because they
>     entail work-stealing duels between program and collector. On hotspot,
>     usually the best throughput for divide-and-conquer style usages
>     (including parallelStreams) is with -XX:+UseParallelGC. For unstructured
>     computations with lots of little tasks (for example little
>     CompletableFutures), UseShenandoahGC (and/or if needing more than 32GB
>     heap, UseZGC) are often better choices.  UseParallelGC meshes well with
>     highly generational structured parallelism, but is prone to
>     card-mark-contention with small tasks (even with -XX:+UseCondCardMark)
>     and long pauses. The default UseG1GC tends to be somewhere in the
>     middle. Also, on linux, using -XX:+UseTransparentHugePages seems to
>     always be a good idea for programs using FJ. (Other switches seem to
>     have less consistent positive effects.)
> 
>     These days, it is almost impossible to quantify exactly how much better
>     performance is, but it's likely that your usages are faster. For
>     example, about half of the programs in the recent Renaissance Suite
>     (https://renaissance.dev/) somehow use FJ, and most of these have
>     improved scores on most machines, most JVMs (including OpenJ9 and
>     Graal), most collectors, most phases of the moon....
> 
>     It would be great to get feedback from other usages before integrating
>     into OpenJDK. As usual, you can try it with any JDK11+ JVM by grabbing
>     http://gee.cs.oswego.edu/dl/concurrent/dist/jsr166.jar and running "java
>     --patch-module java.base="$DIR/jsr166.jar", where DIR is the full file
>     prefix. (Although beware that using --patch-module slows down startup,
>     and occasionally entire test runs. For other options, see
>     http://gee.cs.oswego.edu/dl/concurrency-interest/index.html).
> 
>     -Doug
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From dl at cs.oswego.edu  Sat Jan 18 09:07:08 2020
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 18 Jan 2020 09:07:08 -0500
Subject: [concurrency-interest] ForkJoin refresh
In-Reply-To: <27b7bfa3-c3d4-8f8c-2c0b-88e5b1d952e0@javaspecialists.eu>
References: <565925d8-dd20-ebd6-0959-7de1bfb4b662@cs.oswego.edu>
 <27b7bfa3-c3d4-8f8c-2c0b-88e5b1d952e0@javaspecialists.eu>
Message-ID: <ece0e10f-ef2a-73e0-067d-8d4a3b1e9fa4@cs.oswego.edu>

On 1/17/20 4:54 PM, Dr Heinz M. Kabutz wrote:
> Thanks Doug - how different is this to the Java 14-EA version?  I 

One thing you might notice is the lack of performance oddities when
tasks unblock but GC hasn't yet unbiased locks. Also better behaved when
a bunch of exceptions are thrown by tasks. Also less flailing with
relatively slow streams of incoming tasks. Plus the other improvements I
mentioned

> noticed the ManagedBlocker implementation today whilst showing the 
> Condition.await() implementation to a class I was teaching and even 
> tweeted about it: https://twitter.com/heinzkabutz/status/1218204292626243585
> 

Right. I had forgotten that because few people use non-LTS JDKs, most
haven't seen that blocking on BlockingQueues etc, along with most other
j.u.c locking/sync work better especially in FJ programs.

-Doug



From leventov.ru at gmail.com  Mon Jan 20 15:44:11 2020
From: leventov.ru at gmail.com (Roman Leventov)
Date: Mon, 20 Jan 2020 22:44:11 +0200
Subject: [concurrency-interest] Nested ManagedBlockers are benign?
Message-ID: <CAAMLo=YYcHGJyHgxymkoVWEkpopQWLkD4H4pnp3XHDBJL4vWfA@mail.gmail.com>

Does calling FJP.managedBlock() within another FJP.managedBlock() call
usually or always not cause an additional thread to spin up the second
compensation thread needlessly?

I ask because I proposed a widely used HTTP client library to wrap blocking
network calls transparently for users:
https://github.com/square/okhttp/issues/5655, but now I'm not sure it is a
good idea if there are some users who already wrap the calls to the library
themselves with FJP.managedBlock(), who might then be penalized for their
own prudence if nested ManagedBlockers start extra threads.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200120/b1a626c3/attachment.htm>

From dl at cs.oswego.edu  Wed Jan 22 07:27:32 2020
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 22 Jan 2020 07:27:32 -0500
Subject: [concurrency-interest] Nested ManagedBlockers are benign?
In-Reply-To: <CAAMLo=YYcHGJyHgxymkoVWEkpopQWLkD4H4pnp3XHDBJL4vWfA@mail.gmail.com>
References: <CAAMLo=YYcHGJyHgxymkoVWEkpopQWLkD4H4pnp3XHDBJL4vWfA@mail.gmail.com>
Message-ID: <64cc64cf-dff6-d1e5-918a-c002aef2929b@cs.oswego.edu>

On 1/20/20 3:44 PM, Roman Leventov via Concurrency-interest wrote:
> Does calling FJP.managedBlock() within another FJP.managedBlock() call
> usually or always not cause an additional thread to spin up the second
> compensation thread needlessly?

There is nothing built into FJ that remembers if you've called
managedBlock, so if implementations of block() invoke another
ManagedBlocker.block, then FJ may reserve or create multiple worker
threads. Wrapping a large amount of code, most of which does not block,
inside ManagedBlock.block is not recommended. On the other hand, it may
be better than some alternatives in practice.

The ManagedBlocker API was designed for tight wrapping of blocking code.
The reason for "isReleasable" is to reduce false-alarms when blocking is
not necessary: internally, we take snapshot of pool state, then check
isReleasable, then, in effect, compareAndSet state, as a check that
releasability status coincides with state.

And it is a good time for a reminder that when programs rely on
non-blocking sync/IO, less infrastructure of any kind is needed to
obtain acceptable performance, although sometimes at the expense of
other kinds of overhead.

-Doug



From mark.falco at gmail.com  Wed Jan 22 08:34:43 2020
From: mark.falco at gmail.com (Mark Falco)
Date: Wed, 22 Jan 2020 08:34:43 -0500
Subject: [concurrency-interest] open up ForkJoinPool.managedBlock to other
 pool implementations
Message-ID: <CADw9S5yCA=TH8dXHPGF0JibqxNt+B1k+oGa30OBtZn0jMO63ZQ@mail.gmail.com>

With the updates being made to the ForkJoinPool and the wider adoption of
ManagedBlocker across the j.u.c blocking classes I'm curious if there would
be interest/willingness to open this up for use by other thread pool
implementations.  Specifically ForkJoinPool.managedBlock currently only
supports notifying ForkJoinWorkerThreads that they will be block, but it.
would seem fairly trivial to support this more generically.  For instance
there could be a BlockingAware interface which ForkJoinWorkerThread would
implement:

interface BlockingAware {
    void managedBlock(ManagedBlocker blocker)
        throws InterruptedException;
}

And then ForkJoinPool.managedBlock could just be:

public static void managedBlock(ManagedBlocker blocker)
    throws InterruptedException {
    Thread t = Thread.currentThread();
    if (t instanceof BlockingAware)
        ((BlockingAware) t).managedBlock(blocker);
    else
        unmanagedBlock(blocker);
}

With ForkJoinWorkerThread's managedBlock implementation being:

void managedBlock(ManagedBlocker blocker) {
    if (pool == null) {
        ForkJoinPool.unmanagedBlock(blocker);
    }
    else {
        ForkJoinPool.compensateBlock(blocker);
    }
}

This would then allow other thread pool implementations to implement
BlockingAware and to be notified when their threads block on a
ManagedBlocker.

I suppose taking things a bit further ManagedBlocker should exist at j.u.c
with FJP.ManagedBlocker extending that, and similarly the FJP.managedBlock
static method would ideally also be exposed on class other then FJP,
perhaps as a static method on ManagedBlocker itself.

thanks,

Mark
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200122/bfbb5e40/attachment.htm>

From leventov.ru at gmail.com  Wed Jan 22 11:01:43 2020
From: leventov.ru at gmail.com (Roman Leventov)
Date: Wed, 22 Jan 2020 18:01:43 +0200
Subject: [concurrency-interest] Nested ManagedBlockers are benign?
In-Reply-To: <64cc64cf-dff6-d1e5-918a-c002aef2929b@cs.oswego.edu>
References: <CAAMLo=YYcHGJyHgxymkoVWEkpopQWLkD4H4pnp3XHDBJL4vWfA@mail.gmail.com>
 <64cc64cf-dff6-d1e5-918a-c002aef2929b@cs.oswego.edu>
Message-ID: <CAAMLo=YstWS7Sti2S++TcuhqywA8x=TW6iz3RRq=ahhW+XTVbg@mail.gmail.com>

Thanks for the clarification.

Do you think it makes sense to prevent nested ManagedBlockers from creating
many workers? Seems to me it might be done with a single flag
in ForkJoinWorkerThread.

On Wed, 22 Jan 2020 at 14:29, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 1/20/20 3:44 PM, Roman Leventov via Concurrency-interest wrote:
> > Does calling FJP.managedBlock() within another FJP.managedBlock() call
> > usually or always not cause an additional thread to spin up the second
> > compensation thread needlessly?
>
> There is nothing built into FJ that remembers if you've called
> managedBlock, so if implementations of block() invoke another
> ManagedBlocker.block, then FJ may reserve or create multiple worker
> threads. Wrapping a large amount of code, most of which does not block,
> inside ManagedBlock.block is not recommended. On the other hand, it may
> be better than some alternatives in practice.
>
> The ManagedBlocker API was designed for tight wrapping of blocking code.
> The reason for "isReleasable" is to reduce false-alarms when blocking is
> not necessary: internally, we take snapshot of pool state, then check
> isReleasable, then, in effect, compareAndSet state, as a check that
> releasability status coincides with state.
>
> And it is a good time for a reminder that when programs rely on
> non-blocking sync/IO, less infrastructure of any kind is needed to
> obtain acceptable performance, although sometimes at the expense of
> other kinds of overhead.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200122/de549a9c/attachment.htm>

From viktor.klang at gmail.com  Wed Jan 22 11:20:21 2020
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 22 Jan 2020 16:20:21 +0000
Subject: [concurrency-interest] Nested ManagedBlockers are benign?
In-Reply-To: <CAAMLo=YstWS7Sti2S++TcuhqywA8x=TW6iz3RRq=ahhW+XTVbg@mail.gmail.com>
References: <CAAMLo=YYcHGJyHgxymkoVWEkpopQWLkD4H4pnp3XHDBJL4vWfA@mail.gmail.com>
 <64cc64cf-dff6-d1e5-918a-c002aef2929b@cs.oswego.edu>
 <CAAMLo=YstWS7Sti2S++TcuhqywA8x=TW6iz3RRq=ahhW+XTVbg@mail.gmail.com>
Message-ID: <CANPzfU_icc9YCaeyHkStYhqZq=g3mBE9eP5F-NH7mzKiFg0rTQ@mail.gmail.com>

Yes, it would seem to be possible to have a local flag in the FJWT which is
set to on before the call to block() and then is set to off after it.

On Wed, Jan 22, 2020 at 4:04 PM Roman Leventov via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Thanks for the clarification.
>
> Do you think it makes sense to prevent nested ManagedBlockers from
> creating many workers? Seems to me it might be done with a single flag
> in ForkJoinWorkerThread.
>
> On Wed, 22 Jan 2020 at 14:29, Doug Lea via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> On 1/20/20 3:44 PM, Roman Leventov via Concurrency-interest wrote:
>> > Does calling FJP.managedBlock() within another FJP.managedBlock() call
>> > usually or always not cause an additional thread to spin up the second
>> > compensation thread needlessly?
>>
>> There is nothing built into FJ that remembers if you've called
>> managedBlock, so if implementations of block() invoke another
>> ManagedBlocker.block, then FJ may reserve or create multiple worker
>> threads. Wrapping a large amount of code, most of which does not block,
>> inside ManagedBlock.block is not recommended. On the other hand, it may
>> be better than some alternatives in practice.
>>
>> The ManagedBlocker API was designed for tight wrapping of blocking code.
>> The reason for "isReleasable" is to reduce false-alarms when blocking is
>> not necessary: internally, we take snapshot of pool state, then check
>> isReleasable, then, in effect, compareAndSet state, as a check that
>> releasability status coincides with state.
>>
>> And it is a good time for a reminder that when programs rely on
>> non-blocking sync/IO, less infrastructure of any kind is needed to
>> obtain acceptable performance, although sometimes at the expense of
>> other kinds of overhead.
>>
>> -Doug
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200122/76a4364b/attachment.htm>

From dl at cs.oswego.edu  Wed Jan 22 19:09:43 2020
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 22 Jan 2020 19:09:43 -0500
Subject: [concurrency-interest] open up ForkJoinPool.managedBlock to
 other pool implementations
In-Reply-To: <CADw9S5yCA=TH8dXHPGF0JibqxNt+B1k+oGa30OBtZn0jMO63ZQ@mail.gmail.com>
References: <CADw9S5yCA=TH8dXHPGF0JibqxNt+B1k+oGa30OBtZn0jMO63ZQ@mail.gmail.com>
Message-ID: <f7da8ab8-bcda-8926-60c1-303727081cf7@cs.oswego.edu>

On 1/22/20 8:34 AM, Mark Falco via Concurrency-interest wrote:
> With the updates being made to the ForkJoinPool and the wider adoption
> of ManagedBlocker across the j.u.c blocking classes I'm curious if there
> would be interest/willingness to open this up for use by other thread
> pool implementations.  

I had considered this, but it's not clear how useful it would be.  For
ThreadPoolExecutors (TPE, the basis of most Executors class factories)
the action would need to conform to the other tunable settings (core
threads etc), which invites unexpected problems. The least problematic
cases are TPEs corresponding to Executors.newFixedThreadPool(n). But
here, it's usually better to replace these pools with "new
ForkJoinPool(n)". At one point, we considered replacing the Executors
factory method to do this. But noticed that are a couple of corner-case
incompatibilities across TPE and FJP involving interactions of timeouts
and interrupts. These are not commonly encountered, but we have a lot of
tests for them, so we know them well. I'll re-investigate whether these
differences can be removed. If so, we might reconsider redefining
newFixedThreadPool, or at least adding a usage note that people should
consider FJP as an alternative.

-Doug





From mark.falco at gmail.com  Wed Jan 22 21:27:07 2020
From: mark.falco at gmail.com (Mark Falco)
Date: Wed, 22 Jan 2020 21:27:07 -0500
Subject: [concurrency-interest] open up ForkJoinPool.managedBlock to
 other pool implementations
In-Reply-To: <f7da8ab8-bcda-8926-60c1-303727081cf7@cs.oswego.edu>
References: <f7da8ab8-bcda-8926-60c1-303727081cf7@cs.oswego.edu>
Message-ID: <722A4056-B2EC-409E-A4ED-17F3EF7E6B3D@gmail.com>

I wasn’t so much thing about their potential use in TPE, but in non-JDK pools. The ability to know that your thread is blocked is such a critically useful piece of information and I’d love to have access to it for my own custom pool implementations and I imagine other may as well.

thanks again,

Mark

> On Jan 22, 2020, at 7:12 PM, Doug Lea via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> ﻿On 1/22/20 8:34 AM, Mark Falco via Concurrency-interest wrote:
>> With the updates being made to the ForkJoinPool and the wider adoption
>> of ManagedBlocker across the j.u.c blocking classes I'm curious if there
>> would be interest/willingness to open this up for use by other thread
>> pool implementations.  
> 
> I had considered this, but it's not clear how useful it would be.  For
> ThreadPoolExecutors (TPE, the basis of most Executors class factories)
> the action would need to conform to the other tunable settings (core
> threads etc), which invites unexpected problems. The least problematic
> cases are TPEs corresponding to Executors.newFixedThreadPool(n). But
> here, it's usually better to replace these pools with "new
> ForkJoinPool(n)". At one point, we considered replacing the Executors
> factory method to do this. But noticed that are a couple of corner-case
> incompatibilities across TPE and FJP involving interactions of timeouts
> and interrupts. These are not commonly encountered, but we have a lot of
> tests for them, so we know them well. I'll re-investigate whether these
> differences can be removed. If so, we might reconsider redefining
> newFixedThreadPool, or at least adding a usage note that people should
> consider FJP as an alternative.
> 
> -Doug
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From leventov.ru at gmail.com  Wed Jan 29 12:11:41 2020
From: leventov.ru at gmail.com (Roman Leventov)
Date: Wed, 29 Jan 2020 20:11:41 +0300
Subject: [concurrency-interest] java.util.Timer is good for wall time
 scheduling?
In-Reply-To: <CAAMLo=bJYVExt-GAxKTkVSc3jYWKuVX8xMeTYmTCsFnriOObjg@mail.gmail.com>
References: <CAAMLo=azs5MFQRwEjx_9hfyM5Q3Mc=ACUyGw4mWsCM04fiUarQ@mail.gmail.com>
 <020a01d5c412$c04720c0$40d56240$@aapt.net.au>
 <CAAMLo=bJYVExt-GAxKTkVSc3jYWKuVX8xMeTYmTCsFnriOObjg@mail.gmail.com>
Message-ID: <CAAMLo=b=e5MQLFXuFxJfAsEGauwe2MbVxL0xxMss96Pt-XM=YA@mail.gmail.com>

I've created a project CronScheduler that aims to solve time/clock drift,
system time resets, and suspend mode issues with
ScheduledThreadPoolExecutor and Timer:
https://medium.com/@leventov/cronscheduler-a-reliable-java-scheduler-for-external-interactions-cb7ce4a4f2cd.
It uses similar idea as mentioned by Josh Bloch in this comment:
https://bugs.openjdk.java.net/browse/JDK-4290274?focusedCommentId=12563675&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-12563675

The code of CronScheduler is based on public domain JSR-166 code for TPE
and SchTPE. Here are some minor issues that I've noticed with the code:

 - SchTPE.shutdownNow() Javadoc says that it returns "a list of task that
never commenced execution". But I think it also returns periodic tasks. So
this may sound confusing.
 - It may make sense to add toString() impl to ScheduledFutureTask,
FutureTask, or RunnableAdapter, because AbortPolicy adds
runnable.toString() to the error message, which is an instance
of ScheduledFutureTask in case of SchTPE, which may make debugging harder
because it will not allow to see from where the rejected task originates
(of course the final runnable also doesn't have toString(), but it may be
of the form OriginatingClass$1...)
 - CompletableFuture.toString() Javadoc says the messages are "Completed
Normally" and "Completed Exceptionally", but actual messages in code have
different case: "Completed normally" and "Completed Exceptionally"
 - Possible improvement: cancelled tasks may be opportunistically purged
during the queue restructuring process in DelayedWorkQueue.offer().

On Mon, 6 Jan 2020 at 12:33, Roman Leventov <leventov.ru at gmail.com> wrote:

> Thank you.
>
> Re: https://bugs.openjdk.java.net/browse/JDK-8209462
>
> "The software should continue printing numbers. This is a real problem
> when the daylight savings time arrives (summer/winter)."
>
> May there be a bug in JVM/macOS? Daylight savings should not affect
> currentTimeMillis(). Assuming the reporter actually experienced this
> problem, rather than observed the delay from a manual time reset and then
> extrapolated to the daylight saving case.
>
> Re: https://bugs.openjdk.java.net/browse/JDK-4290274
>
> I think there is something that definitely could be improved: a reference
> to "countdown timer that ticks once every second for ten seconds" use case
> removed from Timer Javadocs, because it definitely looks like the case
> where ScheduledThreadPoolExecutor is a superior alternative all around.
>
> Regarding a timer freeze when time is set backward, a possible solution
> might be that TimerThread.mainLoop() detects the time going backward, and
> in this case, walks through the queue and examines each TimerTask scheduled
> at a fixed rate, "unwinding" it appropriately  (this may result in
> rebuilding the whole queue, but that shouldn't be practically a performance
> concern). The only semantic problem with this is that
> TimerTask.scheduledExecutionTime() may now go backward in successive
> executions, which might be unexpected by some clients.
>
> On Mon, 6 Jan 2020 at 00:54, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>> Hi Roman,
>>
>>
>>
>> Regarding:
>>
>>
>>
>> “Could somebody please kindly point to these bugs on Timer functionality
>> in JDK issue tracker, or note whether they were fixed?”
>>
>>
>>
>> See for example
>>
>>
>>
>> https://bugs.openjdk.java.net/browse/JDK-4290274
>>
>>
>>
>> and linked bugs.
>>
>>
>>
>> Cheers,
>>
>> David
>>
>>
>>
>> *From:* Roman Leventov
>> *Sent:* Monday, January 6, 2020 5:23 AM
>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>;
>> markus at headcrashing.eu; davidcholmes at aapt.net.au
>> *Subject:* java.util.Timer is good for wall time scheduling?
>>
>>
>>
>> There is an interesting question on StackOverflow with an evidence of
>> rather extreme time drift when a task is scheduled periodically using
>> ScheduledThreadPoolExecutor:
>> https://stackoverflow.com/questions/56571647/why-does-the-java-scheduler-exhibit-significant-time-drift-on-windows.
>> java.util.Timer appears to be a workaround for that problem. It looks to me
>> that java.util.Timer is an only tool offered by JDK suitable for wall-clock
>> time (cron-style) scheduling.
>>
>>
>>
>> In this context, I would like to recall the discussion of hibernation and
>> ScheduledThreadPoolExecutor behavior (
>> https://bugs.openjdk.java.net/browse/JDK-8146527; the discussion thread
>> in this list:
>> http://cs.oswego.edu/pipermail/concurrency-interest/2016-January/014817.html).
>> It seems actually that the desired case of "sending an e-mail each hour"
>> could be pretty easily coded with Timer:
>>
>>
>>
>> new Timer().scheduleAtFixedRate(new TimerTask() {
>>
>>   long minutes = 0;
>>
>>   @Override public void run() {
>>
>>       if (minutes % 60 == 0 &&
>>
>>           System.currentTimeMillis() - scheduledExecutionTime() <
>> MINUTES.toMillis(1)) {
>>
>>        sendEmail();
>>
>>      }
>>
>>      minutes++;
>>
>>    }
>>
>> }, nextRoundMinuteDate(), MINUTES.toMillis(1));
>>
>>
>>
>> With this, I have a few questions:
>>
>>
>>
>>  1) Should the Javadoc for Timer, which currently says:
>>
>>
>>
>> "Java 5.0 introduced the java.util.concurrent package and one of the
>> concurrency utilities therein is the ScheduledThreadPoolExecutor which is a
>> thread pool for repeatedly executing tasks at a given rate or delay. It is
>> effectively a more versatile replacement for the Timer/TimerTask
>> combination, as it allows multiple service threads, accepts various time
>> units, and doesn't require subclassing TimerTask (just implement Runnable).
>> Configuring ScheduledThreadPoolExecutor with one thread makes it equivalent
>> to Timer."
>>
>>
>>
>> be amended, perhaps, with the following passage: ", except that Timer may
>> be more appropriate for scheduling recurring activities that are sensitive
>> to absolute time because Timer is more robust to the clock drift
>> than ScheduledThreadPoolExecutor."
>>
>>
>>
>>  2) In https://bugs.openjdk.java.net/browse/JDK-8146527, David wrote
>>
>>
>>
>> "Also note that we (JSR166 expert group) explicitly killed the
>> absolute-time schedule methods due to their problematic nature (as
>> evidenced by bugs reports on java.util.Timer functionality) - and that
>> wasn't even considering PC sleep/suspend/hibernate issues."
>>
>>
>>
>> Could somebody please kindly point to these bugs on Timer functionality
>> in JDK issue tracker, or note whether they were fixed?
>>
>>
>>
>> 3) Quite unrelated to the previous, but since Timer is discussed rarely:
>> would it make sense to replace Timer's threadReaper field which is
>> currently an Object with a finalize() method overridden with a Cleaner?
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20200129/9f58637c/attachment.htm>

