From sandeep.bansal85 at gmail.com  Fri Oct  1 02:20:37 2010
From: sandeep.bansal85 at gmail.com (sandeep bansal)
Date: Fri, 1 Oct 2010 11:50:37 +0530
Subject: [concurrency-interest] Final field set null on thread
	termination
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEIBIIAA.davidcholmes@aapt.net.au>
References: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEIBIIAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTimG-sYJKdm_Hmx=7FzKFWKPmuLZpJXZa1QHvVy8@mail.gmail.com>

The application is a webapp deployed in tomcat. Whenever the context is
reloaded all the logging threads are shutdown.
As soon as i add a local reference to the class instance and use that
reference everything starts working fine.

On Fri, Oct 1, 2010 at 3:36 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Can you show us the actual stacktrace for the NullPointerException? I
> just want to verify it is coming from where you think.
>

The stacktrace for the application as as follows:

Exception in thread "Logging Thread for: router"
java.lang.NullPointerException
    at com.spice.common.logging.LogThread.run(LogThread.java:69)

The location is exactly the line of statsLogger.shutdown and this is always
the case no matter how many times i run the application.


>
> Can you try this on a version of JDK7?
>

Downloading the jdk right now. Will post the result soon.


>  Note that the null check in run() is superfluous as statsLogger can never
> be null.
>

I snipped a large part of the code. There are a lot of different logger in
my application. If i don't want to log anything i just don't start the
thread and drop the data. I forgot to remove that part.


>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *sandeep bansal
> *Sent:* Thursday, 30 September 2010 11:58 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Final field set null on thread
> termination
>
> Hi Everyone,
>
> I have a thread which is something like this:
>
> public class LogThread extends Thread {
>         ...
>     private static volatile boolean stopped;
>     private final StatisticsLogger statsLogger;
>     ...
>     public LogThread(String loggerName, Config config) {
>         statsLogger = new FileLogger(loggerName, config);
>     }
>
>     public void run() {
>         if (statsLogger == null) {
>             return;
>         }
>         while(!LogThread.stopped) {
>             try {
>                             ...
>                             statsLogger.log(logData);
>                             ...
>             } catch (Exception ex) {
>                 logger.error("Exception when logging data", ex);
>             }
>         }
>         statsLogger.shutdown();
>     }
>
>     public static final void shutdown() {
>         stopped = true;
>     }
> }
>
> http://pastie.org/1191267
>
> The problem is that whenever i shutdown the thread statsLogger.shutdown()
> throws a null pointer exception. I have no idea why this is happening. My
> java version is 1.6.0.18. Can anyone please give me a clue.
>
> --
> Regards,
> Sandeep
>
>


-- 
Regards,
Sandeep
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101001/dff20f04/attachment.html>

From sandeep.bansal85 at gmail.com  Fri Oct  1 04:25:24 2010
From: sandeep.bansal85 at gmail.com (sandeep bansal)
Date: Fri, 1 Oct 2010 13:55:24 +0530
Subject: [concurrency-interest] Final field set null on thread
	termination
In-Reply-To: <AANLkTimG-sYJKdm_Hmx=7FzKFWKPmuLZpJXZa1QHvVy8@mail.gmail.com>
References: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEIBIIAA.davidcholmes@aapt.net.au>
	<AANLkTimG-sYJKdm_Hmx=7FzKFWKPmuLZpJXZa1QHvVy8@mail.gmail.com>
Message-ID: <AANLkTikgB2M7JF4NNQom97FGVSbvW8-q5GOfH=F27vUQ@mail.gmail.com>

I tested it with jdk 7 and still found this to be happening.

On Fri, Oct 1, 2010 at 11:50 AM, sandeep bansal
<sandeep.bansal85 at gmail.com>wrote:

> The application is a webapp deployed in tomcat. Whenever the context is
> reloaded all the logging threads are shutdown.
> As soon as i add a local reference to the class instance and use that
> reference everything starts working fine.
>
> On Fri, Oct 1, 2010 at 3:36 AM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Can you show us the actual stacktrace for the NullPointerException? I
>> just want to verify it is coming from where you think.
>>
>
> The stacktrace for the application as as follows:
>
> Exception in thread "Logging Thread for: router"
> java.lang.NullPointerException
>     at com.spice.common.logging.LogThread.run(LogThread.java:69)
>
> The location is exactly the line of statsLogger.shutdown and this is always
> the case no matter how many times i run the application.
>
>
>>
>> Can you try this on a version of JDK7?
>>
>
> Downloading the jdk right now. Will post the result soon.
>
>
>>  Note that the null check in run() is superfluous as statsLogger can
>> never be null.
>>
>
> I snipped a large part of the code. There are a lot of different logger in
> my application. If i don't want to log anything i just don't start the
> thread and drop the data. I forgot to remove that part.
>
>
>>
>> David Holmes
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *sandeep bansal
>> *Sent:* Thursday, 30 September 2010 11:58 PM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Final field set null on thread
>> termination
>>
>> Hi Everyone,
>>
>> I have a thread which is something like this:
>>
>> public class LogThread extends Thread {
>>         ...
>>     private static volatile boolean stopped;
>>     private final StatisticsLogger statsLogger;
>>     ...
>>     public LogThread(String loggerName, Config config) {
>>         statsLogger = new FileLogger(loggerName, config);
>>     }
>>
>>     public void run() {
>>         if (statsLogger == null) {
>>             return;
>>         }
>>         while(!LogThread.stopped) {
>>             try {
>>                             ...
>>                             statsLogger.log(logData);
>>                             ...
>>             } catch (Exception ex) {
>>                 logger.error("Exception when logging data", ex);
>>             }
>>         }
>>         statsLogger.shutdown();
>>     }
>>
>>     public static final void shutdown() {
>>         stopped = true;
>>     }
>> }
>>
>> http://pastie.org/1191267
>>
>> The problem is that whenever i shutdown the thread statsLogger.shutdown()
>> throws a null pointer exception. I have no idea why this is happening. My
>> java version is 1.6.0.18. Can anyone please give me a clue.
>>
>> --
>> Regards,
>> Sandeep
>>
>>
>
>
> --
> Regards,
> Sandeep
>



-- 
With warm regards,
Sandeep
9868615579
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101001/3dcf5c15/attachment-0001.html>

From teck at terracottatech.com  Fri Oct  1 05:38:11 2010
From: teck at terracottatech.com (Tim Eck)
Date: Fri, 1 Oct 2010 02:38:11 -0700 (PDT)
Subject: [concurrency-interest] Final field set null on thread
	termination
Message-ID: <01a501cb614c$630c2460$29246d20$@com>

> The application is a webapp deployed in tomcat. Whenever the context is
> reloaded all the logging threads are shutdown.

I wonder if tomcat's reference clearing stuff is the cause of this?

Setting
-Dorg.apache.catalina.loader.WebappClassLoader.ENABLE_CLEAR_REFERENCES=fal
se would be a quick test to see if it stops the NullPointerException. 

For source minded:
http://svn.apache.org/repos/asf/tomcat/tc6.0.x/tags/TOMCAT_6_0_29/java/org
/apache/catalina/loader/WebappClassLoader.java



From ach at quartetfs.com  Fri Oct  1 07:57:28 2010
From: ach at quartetfs.com (Antoine CHAMBILLE)
Date: Fri, 1 Oct 2010 13:57:28 +0200
Subject: [concurrency-interest] ForkJoinPool spawning much more workers than
	its target parallelism
Message-ID: <007501cb615f$d69b4b00$83d1e100$@quartetfs.com>

Dear all,

 

We at QuartetFS have been using the ForkJoinPool (JDK6 + jsr166y) for a
while. We write a high throughput aggregation engine that receives tuples
and aggregates them following some group-by clauses. The ForkJoinPool is a
fantastic tool to do that in parallel, because it remains efficient for very
small tasks, and because tasks can resubmit subtasks without any risk of
dead-locking.

 

We test for performance on an Intel Xeon Nehalem platform (2 sockets,
4-cores CPUs, hyperthreading, so 16 hardware threads). We dimension the
target parallelism of the fork join pool to 16.

 

We have always seen the fork join pool spawning one or two more workers than
the target parallelism. It has been explained that this is the way the fork
join pool keeps close to its target parallelism. On our test platform we
usually see 18 running workers for our target parallelism of 16.

 

But the last time we updated from the jsr166y trunk (just after the recent
openJDK synchronization) we notice a strong change in this behaviour: The
fork join pool now creates tons of worker, up to 3 times the target
parallelism. On our platform we see up to 50 running workers with a target
parallelism of 16. And the overall performance is reduced a little (-5%)
compared to our previous jsr166y snapshot (July 2010).

 

 

Is that an expected behaviour? Do you have an idea of the code change that
creates it?

 

 

Thank you very much,

-Antoine CHAMBILLE

QuartetFS

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101001/f5d7a2f4/attachment.html>

From dl at cs.oswego.edu  Fri Oct  1 08:21:05 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 01 Oct 2010 08:21:05 -0400
Subject: [concurrency-interest] ForkJoinPool spawning much more workers
 than	its target parallelism
In-Reply-To: <007501cb615f$d69b4b00$83d1e100$@quartetfs.com>
References: <007501cb615f$d69b4b00$83d1e100$@quartetfs.com>
Message-ID: <4CA5D231.6060704@cs.oswego.edu>

On 10/01/10 07:57, Antoine CHAMBILLE wrote:
> We test for performance on an Intel Xeon Nehalem platform (2 sockets, 4-cores
> CPUs, hyperthreading, so 16 hardware threads).

Could you tell me exactly what OS platform and JDK version?

> We have always seen the fork join pool spawning one or two more workers than
> the target parallelism. It has been explained that this is the way the fork
> join pool keeps close to its target parallelism. On our test platform we
> usually see 18 running workers for our target parallelism of 16.

Yes. Creating or restarting enough internal spares to maintain
target parallelism and avoid starvation is intrinsically heuristic
(it is impossible to know for sure whether lack of progress is due
to insufficient threads vs momentary unrelated stalls), so
will often transiently overshoot.

>
> But the last time we updated from the jsr166y trunk (just after the recent
> openJDK synchronization) we notice a strong change in this behaviour: The
> fork join pool now creates tons of worker, up to 3 times the target
> parallelism.
>
> Is that an expected behaviour? Do you have an idea of the code change that
> creates it?
>

This is surprising; thanks for reporting it!
It would be helpful if you could send me (off-list)
a test case showing this. The changes last month
mainly entail using a backoff/timeout to smooth over false
alarms due to transient loads, so should (and does for our tests)
generally result in fewer spare threads, not more. However, it does
introduce new dependencies on JVM timed wait mechanics, that might
account for this.

-Doug

From sandeep.bansal85 at gmail.com  Fri Oct  1 14:52:50 2010
From: sandeep.bansal85 at gmail.com (sandeep bansal)
Date: Sat, 2 Oct 2010 00:22:50 +0530
Subject: [concurrency-interest] Final field set null on thread
	termination
In-Reply-To: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
References: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
Message-ID: <AANLkTik0rA0cQaVu777vN1VkQWZ=+ArSeSPdeqzTH0xi@mail.gmail.com>

>
> Message: 1
> Date: Fri, 1 Oct 2010 02:38:11 -0700 (PDT)
> From: "Tim Eck" <teck at terracottatech.com>
> Subject: Re: [concurrency-interest] Final field set null on thread
>        termination
> To: <concurrency-interest at cs.oswego.edu>
> Message-ID: <01a501cb614c$630c2460$
> 29246d20$@com>
> Content-Type: text/plain;       charset="us-ascii"
>
> > The application is a webapp deployed in tomcat. Whenever the context is
> > reloaded all the logging threads are shutdown.
>
> I wonder if tomcat's reference clearing stuff is the cause of this?
>
> Setting
> -Dorg.apache.catalina.loader.WebappClassLoader.ENABLE_CLEAR_REFERENCES=fal
> se would be a quick test to see if it stops the NullPointerException.
>
> For source minded:
> http://svn.apache.org/repos/asf/tomcat/tc6.0.x/tags/TOMCAT_6_0_29/java/org
> /apache/catalina/loader/WebappClassLoader.java<http://svn.apache.org/repos/asf/tomcat/tc6.0.x/tags/TOMCAT_6_0_29/java/org/apache/catalina/loader/WebappClassLoader.java>
>
>
I think this is the case and i probably should have asked at the tomcat list
first :). I did spend a complete day trying to write all sorts of tests for
this error and trying to debug it. I will test it and reply back on Monday
as to this solves the problem or not. Thanks for the direction Tim.

-- 
Regards,
Sandeep
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101002/ab2da818/attachment.html>

From davidcholmes at aapt.net.au  Sat Oct  2 05:16:43 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 2 Oct 2010 19:16:43 +1000
Subject: [concurrency-interest] Final field set null on threadtermination
In-Reply-To: <01a501cb614c$630c2460$29246d20$@com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEIHIIAA.davidcholmes@aapt.net.au>

Tim Eck writes:
> > The application is a webapp deployed in tomcat. Whenever the context is
> > reloaded all the logging threads are shutdown.
>
> I wonder if tomcat's reference clearing stuff is the cause of this?
>
> Setting
> -Dorg.apache.catalina.loader.WebappClassLoader.ENABLE_CLEAR_REFERENCES=fal
> se would be a quick test to see if it stops the NullPointerException.

Wow! That's nasty. You don't want to do that without also fully
understanding the thread lifecycles involved. Ouch! I can understand the
need to workaround past GC/retention issues, but I would have thought that
defaulting to false would have been much more prudent.

David Holmes

> For source minded:
> http://svn.apache.org/repos/asf/tomcat/tc6.0.x/tags/TOMCAT_6_0_29/java/org
> /apache/catalina/loader/WebappClassLoader.java
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From sandeep.bansal85 at gmail.com  Mon Oct  4 02:13:47 2010
From: sandeep.bansal85 at gmail.com (sandeep bansal)
Date: Mon, 4 Oct 2010 11:43:47 +0530
Subject: [concurrency-interest] Final field set null on thread
	termination
In-Reply-To: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
References: <AANLkTi=a6zMkA4uVH47VFqkf6-trG0iNB=POmZasNbpY@mail.gmail.com>
Message-ID: <AANLkTinkCcG8wNKSkE2Gv4ukO9e6eAYV_=tN6O3z03kS@mail.gmail.com>

Hi Everyone,

Setting the
-Dorg.apache.catalina.loader.WebappClassLoader.ENABLE_CLEAR_REFERENCES=false
solved the problem for me in tomcat 6. Thanks again Tim.

On Thu, Sep 30, 2010 at 7:28 PM, sandeep bansal
<sandeep.bansal85 at gmail.com>wrote:

> Hi Everyone,
>
> I have a thread which is something like this:
>
> public class LogThread extends Thread {
>         ...
>     private static volatile boolean stopped;
>     private final StatisticsLogger statsLogger;
>     ...
>     public LogThread(String loggerName, Config config) {
>         statsLogger = new FileLogger(loggerName, config);
>     }
>
>     public void run() {
>         if (statsLogger == null) {
>             return;
>         }
>         while(!LogThread.stopped) {
>             try {
>                             ...
>                             statsLogger.log(logData);
>                             ...
>             } catch (Exception ex) {
>                 logger.error("Exception when logging data", ex);
>             }
>         }
>         statsLogger.shutdown();
>     }
>
>     public static final void shutdown() {
>         stopped = true;
>     }
> }
>
> http://pastie.org/1191267
>
> The problem is that whenever i shutdown the thread statsLogger.shutdown()
> throws a null pointer exception. I have no idea why this is happening. My
> java version is 1.6.0.18. Can anyone please give me a clue.
>
> --
> Regards,
> Sandeep
>



-- 
With warm regards,
Sandeep
9868615579
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101004/ca7a5771/attachment.html>

From ach at quartetfs.com  Mon Oct  4 03:27:47 2010
From: ach at quartetfs.com (Antoine CHAMBILLE)
Date: Mon, 4 Oct 2010 09:27:47 +0200
Subject: [concurrency-interest] ForkJoinPool spawning much more workers
	than its target parallelism
Message-ID: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>

Hello Doug,

Thank you for looking into that issue.

The test platform is a Dell Inspiron T7500 workstation:

2x Intel Xeon 5560 CPU (Nehalem architecture, quad-core, hyperthreading, 16
hardware threads)
48GB or DDR3 memory (1333MHz)
Windows Seven Professional
JDK 1.6.0_21 x64

I will contact you directly to follow up on the troubleshooting.

While I am still in the concurrency mailing list, does anyone else notice
this new behaviour where a lot of spare worker threads are created by the
fork join pool?

-Antoine CHAMBILLE
QuartetFS




-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
Sent: 01 October 2010 14:21
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers
than its target parallelism

On 10/01/10 07:57, Antoine CHAMBILLE wrote:
> We test for performance on an Intel Xeon Nehalem platform (2 sockets, 
> 4-cores CPUs, hyperthreading, so 16 hardware threads).

Could you tell me exactly what OS platform and JDK version?

> We have always seen the fork join pool spawning one or two more 
> workers than the target parallelism. It has been explained that this 
> is the way the fork join pool keeps close to its target parallelism. 
> On our test platform we usually see 18 running workers for our target
parallelism of 16.

Yes. Creating or restarting enough internal spares to maintain target
parallelism and avoid starvation is intrinsically heuristic (it is
impossible to know for sure whether lack of progress is due to insufficient
threads vs momentary unrelated stalls), so will often transiently overshoot.

>
> But the last time we updated from the jsr166y trunk (just after the 
> recent openJDK synchronization) we notice a strong change in this 
> behaviour: The fork join pool now creates tons of worker, up to 3 
> times the target parallelism.
>
> Is that an expected behaviour? Do you have an idea of the code change 
> that creates it?
>

This is surprising; thanks for reporting it!
It would be helpful if you could send me (off-list) a test case showing
this. The changes last month mainly entail using a backoff/timeout to smooth
over false alarms due to transient loads, so should (and does for our tests)
generally result in fewer spare threads, not more. However, it does
introduce new dependencies on JVM timed wait mechanics, that might account
for this.

-Doug
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gdenys at yahoo.com  Mon Oct  4 09:01:19 2010
From: gdenys at yahoo.com (Geert Denys)
Date: Mon, 4 Oct 2010 06:01:19 -0700 (PDT)
Subject: [concurrency-interest] ForkJoinPool spawning much more workers
	than its target parallelism
In-Reply-To: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>
References: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>
Message-ID: <410761.87946.qm@web51405.mail.re2.yahoo.com>

I've just completed a few tests with the latest version of jsr166y. We're 
currently still using the July version in production. 


I have not seen the problems to the extent you describe (Windows 7 Pro, JDK 
1.6.0-20 x64).

On a quad-core, most of the times there is only 1 or 2 spares. Once I've seen 8 
FJ threads, of which 4 active, which is a little worrisome.

Regards,
Geert.

 



----- Original Message ----
From: Antoine CHAMBILLE <ach at quartetfs.com>
To: concurrency-interest at cs.oswego.edu
Sent: Mon, October 4, 2010 9:27:47 AM
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers than 
its target parallelism

Hello Doug,

Thank you for looking into that issue.

The test platform is a Dell Inspiron T7500 workstation:

2x Intel Xeon 5560 CPU (Nehalem architecture, quad-core, hyperthreading, 16
hardware threads)
48GB or DDR3 memory (1333MHz)
Windows Seven Professional
JDK 1.6.0_21 x64

I will contact you directly to follow up on the troubleshooting.

While I am still in the concurrency mailing list, does anyone else notice
this new behaviour where a lot of spare worker threads are created by the
fork join pool?

-Antoine CHAMBILLE
QuartetFS




-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
Sent: 01 October 2010 14:21
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers
than its target parallelism

On 10/01/10 07:57, Antoine CHAMBILLE wrote:
> We test for performance on an Intel Xeon Nehalem platform (2 sockets, 
> 4-cores CPUs, hyperthreading, so 16 hardware threads).

Could you tell me exactly what OS platform and JDK version?

> We have always seen the fork join pool spawning one or two more 
> workers than the target parallelism. It has been explained that this 
> is the way the fork join pool keeps close to its target parallelism. 
> On our test platform we usually see 18 running workers for our target
parallelism of 16.

Yes. Creating or restarting enough internal spares to maintain target
parallelism and avoid starvation is intrinsically heuristic (it is
impossible to know for sure whether lack of progress is due to insufficient
threads vs momentary unrelated stalls), so will often transiently overshoot.

>
> But the last time we updated from the jsr166y trunk (just after the 
> recent openJDK synchronization) we notice a strong change in this 
> behaviour: The fork join pool now creates tons of worker, up to 3 
> times the target parallelism.
>
> Is that an expected behaviour? Do you have an idea of the code change 
> that creates it?
>

This is surprising; thanks for reporting it!
It would be helpful if you could send me (off-list) a test case showing
this. The changes last month mainly entail using a backoff/timeout to smooth
over false alarms due to transient loads, so should (and does for our tests)
generally result in fewer spare threads, not more. However, it does
introduce new dependencies on JVM timed wait mechanics, that might account
for this.

-Doug
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      

From jseigh_cp00 at xemaps.com  Wed Oct  6 06:57:51 2010
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Wed, 06 Oct 2010 06:57:51 -0400
Subject: [concurrency-interest] Unsafe publication of new objects question
In-Reply-To: <410761.87946.qm@web51405.mail.re2.yahoo.com>
References: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>
	<410761.87946.qm@web51405.mail.re2.yahoo.com>
Message-ID: <4CAC562F.4020301@xemaps.com>

  How did the JVM get around to fixing the unsafe publication problem of new object state,  i.e. basic type safety for primative types?   Exploiting load dependency perhaps?

Joseph Seigh

From davidcholmes at aapt.net.au  Wed Oct  6 07:06:36 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 6 Oct 2010 21:06:36 +1000
Subject: [concurrency-interest] Unsafe publication of new objects
	question
In-Reply-To: <4CAC562F.4020301@xemaps.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEJIIIAA.davidcholmes@aapt.net.au>

Joseph Seigh writes:
>
>   How did the JVM get around to fixing the unsafe publication
> problem of new object state,  i.e. basic type safety for
> primative types?   Exploiting load dependency perhaps?

Primitive types are type safe by definition. The only guarantee you have
regarding unsafe publication is that you can never see uninitialized state -
at a minimum you must see the default initialization values (which is easily
achieved by allocating out of pre-zeroed memory).

David Holmes


From ach at quartetfs.com  Thu Oct  7 05:55:07 2010
From: ach at quartetfs.com (Antoine CHAMBILLE)
Date: Thu, 7 Oct 2010 11:55:07 +0200
Subject: [concurrency-interest] ForkJoinPool spawning much more workers
	than its target parallelism
In-Reply-To: <410761.87946.qm@web51405.mail.re2.yahoo.com>
References: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>
	<410761.87946.qm@web51405.mail.re2.yahoo.com>
Message-ID: <01c601cb6605$bc4f62e0$34ee28a0$@quartetfs.com>

I wanted to close this thread, like 95% of the time the issue was in the
code of our application:

One of the fork join tasks was submitting its child tasks directly to the
fork join pool, instead of properly forking them. The fork join pool in the
latest jsr166 cut reacts to that by allocating a lot of workers, possibly
much more than the target parallelism. Interestingly that behaviour has
changed since the July cut. The previous version was not allocating that
many spare workers.

Sorry for the "false alert" and thank you for your support.

-Antoine CHAMBILLE
QuartetFS



-----Original Message-----
From: Geert Denys [mailto:gdenys at yahoo.com] 
Sent: 04 October 2010 15:01
To: Antoine CHAMBILLE; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers
than its target parallelism

I've just completed a few tests with the latest version of jsr166y. We're
currently still using the July version in production. 


I have not seen the problems to the extent you describe (Windows 7 Pro, JDK 
1.6.0-20 x64).

On a quad-core, most of the times there is only 1 or 2 spares. Once I've
seen 8 
FJ threads, of which 4 active, which is a little worrisome.

Regards,
Geert.

 



----- Original Message ----
From: Antoine CHAMBILLE <ach at quartetfs.com>
To: concurrency-interest at cs.oswego.edu
Sent: Mon, October 4, 2010 9:27:47 AM
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers
than 
its target parallelism

Hello Doug,

Thank you for looking into that issue.

The test platform is a Dell Inspiron T7500 workstation:

2x Intel Xeon 5560 CPU (Nehalem architecture, quad-core, hyperthreading, 16
hardware threads)
48GB or DDR3 memory (1333MHz)
Windows Seven Professional
JDK 1.6.0_21 x64

I will contact you directly to follow up on the troubleshooting.

While I am still in the concurrency mailing list, does anyone else notice
this new behaviour where a lot of spare worker threads are created by the
fork join pool?

-Antoine CHAMBILLE
QuartetFS




-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
Sent: 01 October 2010 14:21
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ForkJoinPool spawning much more workers
than its target parallelism

On 10/01/10 07:57, Antoine CHAMBILLE wrote:
> We test for performance on an Intel Xeon Nehalem platform (2 sockets, 
> 4-cores CPUs, hyperthreading, so 16 hardware threads).

Could you tell me exactly what OS platform and JDK version?

> We have always seen the fork join pool spawning one or two more 
> workers than the target parallelism. It has been explained that this 
> is the way the fork join pool keeps close to its target parallelism. 
> On our test platform we usually see 18 running workers for our target
parallelism of 16.

Yes. Creating or restarting enough internal spares to maintain target
parallelism and avoid starvation is intrinsically heuristic (it is
impossible to know for sure whether lack of progress is due to insufficient
threads vs momentary unrelated stalls), so will often transiently overshoot.

>
> But the last time we updated from the jsr166y trunk (just after the 
> recent openJDK synchronization) we notice a strong change in this 
> behaviour: The fork join pool now creates tons of worker, up to 3 
> times the target parallelism.
>
> Is that an expected behaviour? Do you have an idea of the code change 
> that creates it?
>

This is surprising; thanks for reporting it!
It would be helpful if you could send me (off-list) a test case showing
this. The changes last month mainly entail using a backoff/timeout to smooth
over false alarms due to transient loads, so should (and does for our tests)
generally result in fewer spare threads, not more. However, it does
introduce new dependencies on JVM timed wait mechanics, that might account
for this.

-Doug
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      


From dl at cs.oswego.edu  Thu Oct  7 06:28:23 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 07 Oct 2010 06:28:23 -0400
Subject: [concurrency-interest] ForkJoinPool spawning much more workers
 than its target parallelism
In-Reply-To: <01c601cb6605$bc4f62e0$34ee28a0$@quartetfs.com>
References: <00b101cb6395$a60bf6f0$f223e4d0$@quartetfs.com>	<410761.87946.qm@web51405.mail.re2.yahoo.com>
	<01c601cb6605$bc4f62e0$34ee28a0$@quartetfs.com>
Message-ID: <4CADA0C7.1080506@cs.oswego.edu>

On 10/07/10 05:55, Antoine CHAMBILLE wrote:
> I wanted to close this thread, like 95% of the time the issue was in the
> code of our application:

But thanks for reporting this anyway, because this...

> One of the fork join tasks was submitting its child tasks directly to the
> fork join pool, instead of properly forking them.

... confirms that the differences between pool.invoke(task)
and task.invoke() (or related forms like submit) are still subtle
and error-prone. (See also the postings a few months ago
by Dan Grossman about student experiences.) I had announced
plans to change this by trapping self-pool invokes, but
we backed most of this out after noticing that doing so
led to a different form of surprise, about behavior on
shutdown. But I now see that backing it out was a mistake.
Instead, I'll look into a more careful form of bypass that
preserves conformance to both specs and expectations about
both forms of invocation.

-Doug



From robert.nicholson at gmail.com  Thu Oct  7 20:13:43 2010
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Thu, 7 Oct 2010 19:13:43 -0500
Subject: [concurrency-interest] Replacement for new Thread and a run loop?
Message-ID: <7AB24787-41EA-49C6-9AD2-D08304F9A23A@gmail.com>

If you're concerned about your thread dying or disappearing whenever you use new Thread with  run forever/running loop what is the "concurrent" equivalent? 

Is it a ScheduledExecutor where because there's a pool you're always guaranteed to have a Thread
available to process your tasks?

Also, if you use a LinkedBlockingQueue is it important to code defensively to not keep putting this in your queue if there's no consumer taking things from the queue? ie. Do you code for this possibility by abandoning your loop where you are doing your "offer" if it fails too many times?

Cheers.




From davidcholmes at aapt.net.au  Thu Oct  7 21:09:09 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 8 Oct 2010 11:09:09 +1000
Subject: [concurrency-interest] Replacement for new Thread and a run
	loop?
In-Reply-To: <7AB24787-41EA-49C6-9AD2-D08304F9A23A@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEKBIIAA.davidcholmes@aapt.net.au>

Robert Nicholson writes:
>
> If you're concerned about your thread dying or disappearing
> whenever you use new Thread with  run forever/running loop what
> is the "concurrent" equivalent?

If an exception is encountered, and not handled, then the thread will
terminate, regardless of whether it is your own thread or one in a pool.
Either way your loop is terminated.

> Is it a ScheduledExecutor where because there's a pool you're
> always guaranteed to have a Thread available to process your tasks?

If you wanted each iteration of the loop to be independent then you'd have
to submit each iteration as a separate task to the executor (a
singleThreadExecutor if the tasks must be executed serially).

If the trigger for each iteration was time-based then you might submit a
periodic tasks to a ScheduledExecutor, but again the need to serialize the
tasks would complicate things.

Further, it may be very difficult to perform the next iteration if the
previous one failed due to an unknown error - there may be corrupt state
shared across iterations. If the iterations are completely independent then
each iteration is really an independent task that would be best submitted to
an executor. An executor is likely a better tool for the job than your own
thread that pulls work from a work-queue.

> Also, if you use a LinkedBlockingQueue is it important to code
> defensively to not keep putting this in your queue if there's no
> consumer taking things from the queue? ie. Do you code for this
> possibility by abandoning your loop where you are doing your
> "offer" if it fails too many times?

Well you could. It all depends on the overal context and semantics. A more
typical defensive strategy would be to use a blocking put with a timeout,
and if you timeout you assume something is wrong with the consumer (or
vice-versa for the consumer doing a take) and "do something".

HTH

David Holmes

> Cheers.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ach at quartetfs.com  Fri Oct  8 04:08:59 2010
From: ach at quartetfs.com (Antoine CHAMBILLE)
Date: Fri, 8 Oct 2010 10:08:59 +0200
Subject: [concurrency-interest] Efficient wait during the exection of a fork
	join task
Message-ID: <004d01cb66c0$10a16e00$31e44a00$@quartetfs.com>

According to its documentation, the fork join pool targets workloads where
each task works independently on a data section that is isolated from
concurrent tasks. The only kind of acceptable "waiting" is to join
sub-tasks. One of the great features of the fork join pool is that when a
task waits for the completion of a child task, it releases the worker thread
and allows some other tasks to complete, instead of wasting the current
thread.

 

But I think the fork join pool is too great to be restricted to that kind of
workload. I am sure a lot of developers use it already more generally with
tasks that contribute to shared structures and do a bit of locking. But then
when the task waits to acquire a lock, the worker thread is wasted, although
it could theoretically execute another pending task like during a join.

 

 

Would it be possible to efficiently use the worker thread also in that case?

-          Maybe with some ForkJoinTask.activeWait() method one could call
after failing acquiring a lock.

-          Or something like ForkJoinTask.acquire(Lock lock).

-          Or with a special lock implementation ( Lock
ForkJoinPool.createLock() ) that would consume pending tasks until acquired.

 

 

-Antoine CHAMBILLE

QuartetFS

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101008/0707336f/attachment.html>

From concurrency-interest at stefan-marr.de  Mon Oct 11 18:59:32 2010
From: concurrency-interest at stefan-marr.de (Stefan Marr)
Date: Tue, 12 Oct 2010 00:59:32 +0200
Subject: [concurrency-interest] [CfPart] BoF: Non-deterministic Programming
	for Manycore Systems at SPLASH'10
Message-ID: <ABA2E77E-72EF-48AB-8238-58AE7F75AB14@stefan-marr.de>

============================================================================
                           Call for Participation

             Non-deterministic Programming for Manycore Systems

                             Birds of a Feather
                    Tuesday, October 19, starting 5:30pm
                          Reno/Tahoe, Nevada, 2010.
                         Co-located with SPLASH 2010

     http://soft.vub.ac.be/~smarr/bof_non-determenistic_programming.html
============================================================================



    Non-deterministic Programming: A Solution for the Manycore Challenge?
            Informal Workshop on the Opportunities of Embracing
                Non-determinism in a Highly Parallel World

Goal
----

This BoF is meant to bring together people who are interested in solving the
challenges that the fine-grained parallelism of manycore and exascale systems
present.

Today, non-determinism is usually considered a problem researchers have to
solve when they confronted with parallel programming. However,
non-deterministic programming and its potential has been the subject of
research for decades past. With the increasing and unavoidable degree of
parallelism, it may be the time to discuss it from a new perspective. Thus,
the main question of the workshop is, what if the downsides of non-determinism
are outweighed by its benefits?

We like to discuss the question of how a programming model for
non-deterministic parallel programming could be designed. Thus, we want to
think about what challenges need to be overcome and what research questions
need to be formulated. Of interest are also which classic problems, for
instance sorting, could be solved in a non-deterministic manner, and what kind
of computational problems could benefit from speedups provided by additional
parallelism even when trading off precision for speed.

Attendance
----------

This Birds of a Feather is supposed to propose and discus new ideas, without
any restriction of the creativity of its attendees. Every attendee is welcome
to give a brief presentation, show a demo, or just engage into a discussion
with the audience.

Renaissance Project
-------------------

This BoF will also be used as a venue to present the Renaissance Virtual
Machine (RVM) of IBM Research[3]. On this occasion, the RVM will be
opensourced and attendees will have the opportunity to get introduced to the
technical details of it. The RVM is meant to provide a common foundation for
research regarding non-deterministic parallel programming for manycore
systems. It is a Squeak/Pharo compatible Smalltalk VM supporting a classic
shared-memory programming model. Thus, the programmer has the illusion of a
single object heap on which Smalltalk Processes can work in parallel.

On top of the RVM, Ly was developed to embrace non-determinism and to harness
emergence.

The first results will be reported in an Onward! presentation on Thursday
afternoon [1]:

  Harnessing Emergence for Manycore Programming: Early Experience
  Integrating Ensembles, Adverbs, and Object-based Inheritance
  by David Ungar, Sam S. Adams


Confirmed Participants
----------------------

 David Ungar, Doug Kimelman, IBM Research
   Brief introduction into the Renaissance project and demo of the current
   status of their Ly/Sly language and manycore virtual machine.

 Max OrHai, Andrew Black, Portland State University
   Giving an overview of an example application to test the Renaissance
   ensemble code as well as an outlook to a scalable non-deterministic
   parallel sorting algorithm.
   
 Theo D'Hondt, Vrije Universiteit Brussel
   Lessons from the past: Recapitulating the relations of non-deterministic 
   programming, AI, and highly parallel systems like Connection Machine.

Organizers
----------

Theo D'Hondt, Software Languages Lab, Vrije Universiteit Brussel
Stefan Marr,  Software Languages Lab, Vrije Universiteit Brussel


[1] http://splashcon.org/program/onward-short-papers/154-onward-short-papers-b
[2] http://splashcon.org/schedule/tuesday-oct-19-/235
[3] https://researcher.ibm.com/researcher/view_project.php?id=245


-- 
Stefan Marr
Software Languages Lab
Vrije Universiteit Brussel
Pleinlaan 2 / B-1050 Brussels / Belgium
http://soft.vub.ac.be/~smarr
Phone: +32 2 629 2974
Fax:   +32 2 629 3525



From alarmnummer at gmail.com  Sun Oct 17 17:22:13 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 17 Oct 2010 23:22:13 +0200
Subject: [concurrency-interest] sequential consistency between volatile
	write and volatile read
Message-ID: <AANLkTimn9wixOMceinbLa9Yt=o3SX87GyuRVC36Nc4Yk@mail.gmail.com>

I'm struggling with the following problem (part of the commit of an stm).

        ___value = tranlocal.value;
        ___version = tranlocal.version+1;

        //JMM problem here, the volatile read of ___listeners could jump in
front of the volatile write of
        //version, meaning that it could lead to not picking up the
listeners that is done after the write. And
        //this could lead to a deadlock.
        Listeners listenersAfterWrite = ___listeners;

        if(listenersAfterWrite != null){
           listenersAfterWrite = ___removeListenersAfterWrite();
        }

In this example ___value, ___version and ___listeners are volatile and the
rest is a normal variable. The problem here is that the read of ___listeners
could jump in front of the volatile write of ___version or even ___value.
The consequence is that it could happen that listeners that should been
picked up after writing the value, don't need to be picked up (because the
read happened too early) and this could lead to a deadlock since listeners
that should have been notified, are not notified.

Is my assumption correct that this problem could happen?

I'm currently trying to figure out why transaction enter a deadlock and this
deadlock can be explained by this bug in the stm code.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101017/7b381d16/attachment.html>

From hans.boehm at hp.com  Sun Oct 17 19:03:42 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Sun, 17 Oct 2010 23:03:42 +0000
Subject: [concurrency-interest] sequential consistency between
	volatile	write and volatile read
In-Reply-To: <AANLkTimn9wixOMceinbLa9Yt=o3SX87GyuRVC36Nc4Yk@mail.gmail.com>
References: <AANLkTimn9wixOMceinbLa9Yt=o3SX87GyuRVC36Nc4Yk@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0426F23CF2E84@GVW0436EXB.americas.hpqcorp.net>

So long as your code doesn't allow simultaneous conflicting accesses to a non-volatile variable, the Java memory model guarantees sequential consistency (with the exception of a few j.u.c. methods that explicitly don't, like lazySet).  If I understand your code correctly, that applies here, and your assumption is incorrect.

I'm not sure that all implementations get this 100% right, but the specification does not allow the kind of reordering you describe.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Peter Veentjer
Sent: Sunday, October 17, 2010 2:22 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] sequential consistency between volatile write and volatile read

I'm struggling with the following problem (part of the commit of an stm).

        ___value = tranlocal.value;
        ___version = tranlocal.version+1;

        //JMM problem here, the volatile read of ___listeners could jump in front of the volatile write of
        //version, meaning that it could lead to not picking up the listeners that is done after the write. And
        //this could lead to a deadlock.
        Listeners listenersAfterWrite = ___listeners;

        if(listenersAfterWrite != null){
           listenersAfterWrite = ___removeListenersAfterWrite();
        }

In this example ___value, ___version and ___listeners are volatile and the rest is a normal variable. The problem here is that the read of ___listeners could jump in front of the volatile write of ___version or even ___value. The consequence is that it could happen that listeners that should been picked up after writing the value, don't need to be picked up (because the read happened too early) and this could lead to a deadlock since listeners that should have been notified, are not notified.

Is my assumption correct that this problem could happen?

I'm currently trying to figure out why transaction enter a deadlock and this deadlock can be explained by this bug in the stm code.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20101017/d12f0081/attachment.html>

