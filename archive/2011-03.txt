From ashwin.jayaprakash at gmail.com  Wed Mar  2 19:20:51 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 2 Mar 2011 16:20:51 -0800
Subject: [concurrency-interest] Atomic byte[] operations?
Message-ID: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>

Is there a way to atomically write to portions of a large byte array
concurrently from multiple threads? Something like AtomicIntegerArray or
atomic ops on ByteBuffer and its slices?

The reason I'm asking is because the HBase guys have a "home grown" memory
manager using byte[ ]s. And I was under the impression that atomic ops were
needed to do what they are doing correctly (See comments at end:
http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-2/
).

Or is it sufficient to just use System.arraycopy() from multiple threads
into different regions on the array simultaneously and hope it's all safe -
for reading and writing?

Thanks,
Ashwin (http://www.ashwinjayaprakash.com).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110302/20f622d5/attachment.html>

From ashwin.jayaprakash at gmail.com  Wed Mar  2 19:27:09 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 2 Mar 2011 16:27:09 -0800
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
References: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
Message-ID: <AANLkTikak206f4ccTFBkFmO-Dcj8CXAnG9k9--r93dVu@mail.gmail.com>

One more thing:  Is there something in the sun.misc.Unsafe class that can do
this?

On Wed, Mar 2, 2011 at 4:20 PM, Ashwin Jayaprakash <
ashwin.jayaprakash at gmail.com> wrote:

> Is there a way to atomically write to portions of a large byte array
> concurrently from multiple threads? Something like AtomicIntegerArray or
> atomic ops on ByteBuffer and its slices?
>
> The reason I'm asking is because the HBase guys have a "home grown" memory
> manager using byte[ ]s. And I was under the impression that atomic ops were
> needed to do what they are doing correctly (See comments at end:
> http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-2/
> ).
>
> Or is it sufficient to just use System.arraycopy() from multiple threads
> into different regions on the array simultaneously and hope it's all safe -
> for reading and writing?
>
> Thanks,
> Ashwin (http://www.ashwinjayaprakash.com).
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110302/fdd5a706/attachment.html>

From davidcholmes at aapt.net.au  Wed Mar  2 20:09:05 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 3 Mar 2011 11:09:05 +1000
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>

Ashwin,

What exactly do you mean by "atomically" here?

I can't tell from the blog what HBase is doing. Each individual access to a
byte[] is atomic and word-tearing is not permitted (ie you can't read
32-bits, update 8-bits and write back the 32-bit value)**. If threads
operate on different sections of the array then that is safe. However, if
multiple threads access the same sections, ie one writes and other read,
then you have the usual possibility of seeing stale values.

** This is true for individual accesses. I'm not sure it is true for
System.arraycopy. I seem to recall a word-tearing issue with memcpy but now
I can't locate any reference to it :(

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Thursday, 3 March 2011 10:21 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Atomic byte[] operations?


  Is there a way to atomically write to portions of a large byte array
concurrently from multiple threads? Something like AtomicIntegerArray or
atomic ops on ByteBuffer and its slices?

  The reason I'm asking is because the HBase guys have a "home grown" memory
manager using byte[ ]s. And I was under the impression that atomic ops were
needed to do what they are doing correctly (See comments at end:
http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstor
e-local-allocation-buffers-part-2/).

  Or is it sufficient to just use System.arraycopy() from multiple threads
into different regions on the array simultaneously and hope it's all safe -
for reading and writing?

  Thanks,
  Ashwin (http://www.ashwinjayaprakash.com).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110303/838556cc/attachment.html>

From ashwin.jayaprakash at gmail.com  Wed Mar  2 20:28:30 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 2 Mar 2011 17:28:30 -0800
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>
References: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTim_5XuvaWGScGdbFjHRm6m26OSVDYLF8x3OBnb_@mail.gmail.com>

By "atomic" I meant a thread writing to the first 128 byte positions of a
large 1024 byte array, while another thread is writing to 128-512 positions
of the same array at the same time.

Assuming those offsets are not word aligned or page aligned - just some odd
number like position 0-37, another writing position 38-510 bytes
concurrently - will this work for 2 or more such concurrent writers without
corrupting at those "adjacent" (37-38) byte regions? I guess this is the
word-tearing issue you are talking about. Will the writes have to pad to
make sure they always write at word boundaries?

So, if they are reading from completely different regions, then is it safe?
No need to think of cache line size etc?

Thanks,
Ashwin.


On Wed, Mar 2, 2011 at 5:09 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

>  Ashwin,
>
> What exactly do you mean by "atomically" here?
>
> I can't tell from the blog what HBase is doing. Each individual access to a
> byte[] is atomic and word-tearing is not permitted (ie you can't read
> 32-bits, update 8-bits and write back the 32-bit value)**. If threads
> operate on different sections of the array then that is safe. However, if
> multiple threads access the same sections, ie one writes and other read,
> then you have the usual possibility of seeing stale values.
>
> ** This is true for individual accesses. I'm not sure it is true for
> System.arraycopy. I seem to recall a word-tearing issue with memcpy but now
> I can't locate any reference to it :(
>
> David
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110302/a914de02/attachment.html>

From mlists at juma.me.uk  Wed Mar  2 20:28:44 2011
From: mlists at juma.me.uk (Ismael Juma)
Date: Thu, 3 Mar 2011 01:28:44 +0000
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>
References: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTinuFYPCe__63TsRNLkvP-eA6SNP7biFHkKrx-zY@mail.gmail.com>

On Thu, Mar 3, 2011 at 1:09 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> ** This is true for individual accesses. I'm not sure it is true for
> System.arraycopy. I seem to recall a word-tearing issue with memcpy but now
> I can't locate any reference to it :(

Perhaps http://blogs.sun.com/dave/entry/memcpy_concurrency_curiosities ?

Best,
Ismael

From davidcholmes at aapt.net.au  Wed Mar  2 20:56:50 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 3 Mar 2011 11:56:50 +1000
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <AANLkTim_5XuvaWGScGdbFjHRm6m26OSVDYLF8x3OBnb_@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEELEILAA.davidcholmes@aapt.net.au>

Reading/writing different regions of the array should act like accessing
independent arrays. This is what "no word-tearing" guarantees. No matter how
the array is implemented by the VM the VM has to make sure this holds for
basic array accesses.

As I said it is less clear if a native version of arraycopy gets involved.

David
  -----Original Message-----
  From: Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com]
  Sent: Thursday, 3 March 2011 11:29 AM
  To: dholmes at ieee.org
  Cc: David Holmes; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Atomic byte[] operations?


  By "atomic" I meant a thread writing to the first 128 byte positions of a
large 1024 byte array, while another thread is writing to 128-512 positions
of the same array at the same time.

  Assuming those offsets are not word aligned or page aligned - just some
odd number like position 0-37, another writing position 38-510 bytes
concurrently - will this work for 2 or more such concurrent writers without
corrupting at those "adjacent" (37-38) byte regions? I guess this is the
word-tearing issue you are talking about. Will the writes have to pad to
make sure they always write at word boundaries?

  So, if they are reading from completely different regions, then is it
safe? No need to think of cache line size etc?

  Thanks,
  Ashwin.



  On Wed, Mar 2, 2011 at 5:09 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Ashwin,

    What exactly do you mean by "atomically" here?

    I can't tell from the blog what HBase is doing. Each individual access
to a byte[] is atomic and word-tearing is not permitted (ie you can't read
32-bits, update 8-bits and write back the 32-bit value)**. If threads
operate on different sections of the array then that is safe. However, if
multiple threads access the same sections, ie one writes and other read,
then you have the usual possibility of seeing stale values.

    ** This is true for individual accesses. I'm not sure it is true for
System.arraycopy. I seem to recall a word-tearing issue with memcpy but now
I can't locate any reference to it :(

    David

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110303/21ccf652/attachment-0001.html>

From mohit.riverstone at gmail.com  Thu Mar  3 02:57:23 2011
From: mohit.riverstone at gmail.com (Mohit Kumar)
Date: Thu, 3 Mar 2011 13:27:23 +0530
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 73,
	Issue 15
In-Reply-To: <mailman.3.1298739601.10889.concurrency-interest@cs.oswego.edu>
References: <mailman.3.1298739601.10889.concurrency-interest@cs.oswego.edu>
Message-ID: <AANLkTiksZqxrVorE295-BAFVb-PTboRKzZTSTPfahCXR@mail.gmail.com>

Hi Doug,

I need another clarification.

My understanding is that safe publication guarantees(ordering and
visibility) of j.u.c.atomic classes are given because the internal variables
are
declared volatile. Well! in the Hotspot additional barriers are applied in
the unsafe native implementation.
This means(theoretically) in a NON Hotspot jvm the guarantees provided by
the atomic classes wont break
even if the NON Hotspot JVM doesn't apply those additional barriers because
the variable is declared volatile.
(I understand that x86/sparc CAS has the required ordering but what about
Power platforms)

if this were so then doing a relaxed write on a volatile variable wouldn't
make sense because volatile would guarantee
safe publication guarantees(ordering and visibility) as defined in spec.OR
it has to be a special case processing.

Thanks in advance
Mohit


On Sat, Feb 26, 2011 at 10:30 PM, <
concurrency-interest-request at cs.oswego.edu> wrote:

> Send Concurrency-interest mailing list submissions to
>        concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>        concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>        concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>   1. Re: Numa and ReentrantLock (Mohit Kumar)
>   2. Re: Numa and ReentrantLock (Doug Lea)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Sat, 26 Feb 2011 21:09:03 +0530
> From: Mohit Kumar <mohit.riverstone at gmail.com>
> Subject: Re: [concurrency-interest] Numa and ReentrantLock
> To: concurrency-interest at cs.oswego.edu, dl at cs.oswego.edu
> Message-ID:
>        <AANLkTimUauV1znGmfaoKieKZmNWth3WCS4RafSV5-cS3 at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Hi Doug,
>
> In the ConcurrentHashMap the get method does not require
> "readValueUnderLock" because a racing remove does not make the value null.
> The value never becomes null on the from the removing thread. this means it
> is possible for get to return a value for key even if the removing thread
> (on the same key) has progressed till the point of cloning the preceding
> parts of the list.
> This is fine so long as it is the desired effect.
>
> But this means "readValueUnderLock" is not required for NEW memory model.
>
> However for the OLD memory model a put may see the value null due to
> reordering(Rare but possible).
>
> Is my understanding correct.
>
> Thanks in advance
> Mohit
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110226/399d0259/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Sat, 26 Feb 2011 11:16:26 -0500
> From: Doug Lea <dl at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Numa and ReentrantLock
> To: concurrency-interest at cs.oswego.edu
> Message-ID: <4D69275A.8010009 at cs.oswego.edu>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> On 02/26/11 10:39, Mohit Kumar wrote:
> > Hi Doug,
> >
> > In the ConcurrentHashMap the get method does not require
> "readValueUnderLock"
> > because a racing remove does not make the value null.
> >
> > Is my understanding correct.
> >
>
> Not quite. You are right that it should never be called.
> However, the JLS/JMM can be read as not absolutely
> forbidding it from being called because of weaknesses
> in required ordering relationships among finals
> vs volatiles set in constructors (key is final, value is
> volatile), wrt the reads by threads using the
> entry objects. (In JMM-ese, ordering constraints for
> finals fall outside of the synchronizes-with relation.)
> That's the issue the doc comment (pasted below) refers to.
> No one has ever thought of any practical loophole that a
> processor/compiler might find to produce a null value read,
> and it may be provable that none exist (and perhaps someday
> a JLS/JMM revision will fill in gaps to clarify this),
> but Bill Pugh once suggested we put this in anyway just
> for the sake of being conservatively pedantically correct.
> In retrospect, I'm not so sure this was a good idea, since
> it leads people to come up with exotic theories.
>
> ...
>
>      * Because the value field is volatile, not final, it is legal wrt
>      * the Java Memory Model for an unsynchronized reader to see null
>      * instead of initial value when read via a data race.  Although a
>      * reordering leading to this is not likely to ever actually
>      * occur, the Segment.readValueUnderLock method is used as a
>      * backup in case a null (pre-initialized) value is ever seen in
>      * an unsynchronized access method.
>      */
>     static final class HashEntry<K,V> {
>         final K key;
>         final int hash;
>         volatile V value;
>         final HashEntry<K,V> next;
>
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 73, Issue 15
> ****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110303/860019b5/attachment.html>

From dl at cs.oswego.edu  Thu Mar  3 07:01:28 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 03 Mar 2011 07:01:28 -0500
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 73,
 Issue 15
In-Reply-To: <AANLkTiksZqxrVorE295-BAFVb-PTboRKzZTSTPfahCXR@mail.gmail.com>
References: <mailman.3.1298739601.10889.concurrency-interest@cs.oswego.edu>
	<AANLkTiksZqxrVorE295-BAFVb-PTboRKzZTSTPfahCXR@mail.gmail.com>
Message-ID: <4D6F8318.2060906@cs.oswego.edu>

On 03/03/11 02:57, Mohit Kumar wrote:
> This means(theoretically) in a NON Hotspot jvm the guarantees provided by the
> atomic classes wont break
> even if the NON Hotspot JVM doesn't apply those additional barriers because the
> variable is declared volatile.
> (I understand that x86/sparc CAS has the required ordering but what about Power
> platforms)

Mappings of compareAndSet etc on all platforms must obey the same specs.
So for example, on Power and ARM, the compareAndSet method issues
barriers in addition to an LL/SC. We are in regular contact with VM
and processor spec folks to help ensure that these are done correctly.

-Doug

From kasper at kav.dk  Thu Mar  3 07:43:54 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 03 Mar 2011 13:43:54 +0100
Subject: [concurrency-interest] Numa and ReentrantLock
In-Reply-To: <4D69275A.8010009@cs.oswego.edu>
References: <ActFmuAlWJKDMsJWT2qQSlkcxGBftw==>
	<4c7735e4.10c98e0a.47d4.ffffe2fe@mx.google.com>
	<AANLkTimUauV1znGmfaoKieKZmNWth3WCS4RafSV5-cS3@mail.gmail.com>
	<4D69275A.8010009@cs.oswego.edu>
Message-ID: <4D6F8D0A.3080004@kav.dk>

On 26-02-2011 17:16, Doug Lea wrote:
> On 02/26/11 10:39, Mohit Kumar wrote:
> In retrospect, I'm not so sure this was a good idea, since
> it leads people to come up with exotic theories.

Originally, my biggest trouble with understanding the code was why the 
value iterator of CHM didn't make use of readValueUnderLock().
Now I just assume it is because nobody has been (or will be) bitten by 
that particular reordering.

Cheers
   Kasper

From kasper at kav.dk  Thu Mar  3 08:43:38 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 03 Mar 2011 14:43:38 +0100
Subject: [concurrency-interest] Atomic byte[] operations?
In-Reply-To: <AANLkTim_5XuvaWGScGdbFjHRm6m26OSVDYLF8x3OBnb_@mail.gmail.com>
References: <AANLkTikZ4D8YfUELdJw2MrC3wiX_RB53bjJC59BRBFxg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAELCILAA.davidcholmes@aapt.net.au>
	<AANLkTim_5XuvaWGScGdbFjHRm6m26OSVDYLF8x3OBnb_@mail.gmail.com>
Message-ID: <4D6F9B0A.90006@kav.dk>

Look no further than extra166y.ParallelArray.map().
Which uses multiple threads to reassign mapped values into the same 
array without regards to word alignment.

Cheers
  Kasper

On 03-03-2011 02:28, Ashwin Jayaprakash wrote:
> By "atomic" I meant a thread writing to the first 128 byte positions of
> a large 1024 byte array, while another thread is writing to 128-512
> positions of the same array at the same time.
>
> Assuming those offsets are not word aligned or page aligned - just some
> odd number like position 0-37, another writing position 38-510 bytes
> concurrently - will this work for 2 or more such concurrent writers
> without corrupting at those "adjacent" (37-38) byte regions? I guess
> this is the word-tearing issue you are talking about. Will the writes
> have to pad to make sure they always write at word boundaries?
>
> So, if they are reading from completely different regions, then is it
> safe? No need to think of cache line size etc?
>
> Thanks,
> Ashwin.
>
>
> On Wed, Mar 2, 2011 at 5:09 PM, David Holmes <davidcholmes at aapt.net.au
> <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     Ashwin,
>     What exactly do you mean by "atomically" here?
>     I can't tell from the blog what HBase is doing. Each individual
>     access to a byte[] is atomic and word-tearing is not permitted (ie
>     you can't read 32-bits, update 8-bits and write back the 32-bit
>     value)**. If threads operate on different sections of the array then
>     that is safe. However, if multiple threads access the same sections,
>     ie one writes and other read, then you have the usual possibility of
>     seeing stale values.
>     ** This is true for individual accesses. I'm not sure it is true for
>     System.arraycopy. I seem to recall a word-tearing issue with memcpy
>     but now I can't locate any reference to it :(
>     David
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From mohit.riverstone at gmail.com  Fri Mar  4 01:11:16 2011
From: mohit.riverstone at gmail.com (Mohit Kumar)
Date: Fri, 4 Mar 2011 11:41:16 +0530
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 74,
	Issue 2
In-Reply-To: <mailman.1.1299171600.10828.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1299171600.10828.concurrency-interest@cs.oswego.edu>
Message-ID: <AANLkTimcNagjT0j8zhtPYHgf3Rty53aMs9npBpW2M_ZT@mail.gmail.com>

>
> hi doug,
>

The answer isn't clear and maybe my question wasn't worded well.

My question is what the point of relaxed write(idea is to pay the cost of
ordering but not visibility) when the variable that you are writing to is
declared volatile forcing ordering and visibility regardless of the
VM/Hardware in question because the VM has to respect the volatile
semantics.


Thanks and Regards
Mohit

>
>
> On 03/03/11 02:57, Mohit Kumar wrote:
> > This means(theoretically) in a NON Hotspot jvm the guarantees provided by
> the
> > atomic classes wont break
> > even if the NON Hotspot JVM doesn't apply those additional barriers
> because the
> > variable is declared volatile.
> > (I understand that x86/sparc CAS has the required ordering but what about
> Power
> > platforms)
>
> Mappings of compareAndSet etc on all platforms must obey the same specs.
> So for example, on Power and ARM, the compareAndSet method issues
> barriers in addition to an LL/SC. We are in regular contact with VM
> and processor spec folks to help ensure that these are done correctly.
>
> -Doug
>
> Hi Doug,
>
>


> I need another clarification.
>
> My understanding is that safe publication guarantees(ordering and
> visibility) of j.u.c.atomic classes are given because the internal
> variables
> are
> declared volatile. Well! in the Hotspot additional barriers are applied in
> the unsafe native implementation.
> This means(theoretically) in a NON Hotspot jvm the guarantees provided by
> the atomic classes wont break
> even if the NON Hotspot JVM doesn't apply those additional barriers because
> the variable is declared volatile.
> (I understand that x86/sparc CAS has the required ordering but what about
> Power platforms)
>
> if this were so then doing a relaxed write on a volatile variable wouldn't
> make sense because volatile would guarantee
> safe publication guarantees(ordering and visibility) as defined in spec.OR
> it has to be a special case processing.
>
> Thanks in advance
> Mohit
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110304/8e60555a/attachment.html>

From davidcholmes at aapt.net.au  Fri Mar  4 01:17:02 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 4 Mar 2011 16:17:02 +1000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 74,
	Issue 2
In-Reply-To: <AANLkTimcNagjT0j8zhtPYHgf3Rty53aMs9npBpW2M_ZT@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGELOILAA.davidcholmes@aapt.net.au>

Mohit,

If the relaxed write is done via Unsafe that overrides the default volatile
handling.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohit Kumar
  Sent: Friday, 4 March 2011 4:11 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
74,Issue 2


    hi doug,



  The answer isn't clear and maybe my question wasn't worded well.


  My question is what the point of relaxed write(idea is to pay the cost of
ordering but not visibility) when the variable that you are writing to is
declared volatile forcing ordering and visibility regardless of the
VM/Hardware in question because the VM has to respect the volatile
semantics.




  Thanks and Regards
  Mohit


    On 03/03/11 02:57, Mohit Kumar wrote:
    > This means(theoretically) in a NON Hotspot jvm the guarantees provided
by the
    > atomic classes wont break
    > even if the NON Hotspot JVM doesn't apply those additional barriers
because the
    > variable is declared volatile.
    > (I understand that x86/sparc CAS has the required ordering but what
about Power
    > platforms)

    Mappings of compareAndSet etc on all platforms must obey the same specs.
    So for example, on Power and ARM, the compareAndSet method issues
    barriers in addition to an LL/SC. We are in regular contact with VM
    and processor spec folks to help ensure that these are done correctly.

    -Doug

    Hi Doug,





    I need another clarification.

    My understanding is that safe publication guarantees(ordering and
    visibility) of j.u.c.atomic classes are given because the internal
variables
    are
    declared volatile. Well! in the Hotspot additional barriers are applied
in
    the unsafe native implementation.
    This means(theoretically) in a NON Hotspot jvm the guarantees provided
by
    the atomic classes wont break
    even if the NON Hotspot JVM doesn't apply those additional barriers
because
    the variable is declared volatile.
    (I understand that x86/sparc CAS has the required ordering but what
about
    Power platforms)

    if this were so then doing a relaxed write on a volatile variable
wouldn't
    make sense because volatile would guarantee
    safe publication guarantees(ordering and visibility) as defined in
spec.OR
    it has to be a special case processing.

    Thanks in advance
    Mohit
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110304/2a6cf380/attachment.html>

From navin.jha at FXALL.com  Wed Mar  9 09:58:34 2011
From: navin.jha at FXALL.com (Navin Jha)
Date: Wed, 9 Mar 2011 09:58:34 -0500
Subject: [concurrency-interest] use of skiplist for sorted ConcurrentMap
	instead of red-black tree
Message-ID: <EA6FFD6D7496E940BA1A5A5C1CEFAF76135BF40B62@NYEXCH02.fxall.com>

Hi,

The TreeMap uses Red-Black tree but ConcurrentSkipListMap uses  skipList. What is the reason for using skipList for sorted ConcurrentMap implementation instead of Red-Black tree? Does skipList implementation have some advantage when used by multiple threads?

Thanks,
Navin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110309/38c4b16c/attachment.html>

From holger.hoffstaette at googlemail.com  Wed Mar  9 10:24:30 2011
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Wed, 09 Mar 2011 16:24:30 +0100
Subject: [concurrency-interest] use of skiplist for sorted ConcurrentMap
 instead of red-black tree
In-Reply-To: <EA6FFD6D7496E940BA1A5A5C1CEFAF76135BF40B62@NYEXCH02.fxall.com>
References: <EA6FFD6D7496E940BA1A5A5C1CEFAF76135BF40B62@NYEXCH02.fxall.com>
Message-ID: <4D779BAE.7090508@googlemail.com>

On 09.03.2011 15:58, Navin Jha wrote:
> The TreeMap uses Red-Black tree but ConcurrentSkipListMap uses
>  skipList. What is the reason for using skipList for sorted
> ConcurrentMap implementation instead of Red-Black tree? Does skipList
> implementation have some advantage when used by multiple threads?

http://stackoverflow.com/questions/256511/skip-list-vs-binary-tree

hope this helps,
Holger

From kasper at kav.dk  Wed Mar  9 10:42:16 2011
From: kasper at kav.dk (Kasper Nielsen)
Date: Wed, 09 Mar 2011 16:42:16 +0100
Subject: [concurrency-interest] use of skiplist for sorted ConcurrentMap
 instead of red-black tree
In-Reply-To: <EA6FFD6D7496E940BA1A5A5C1CEFAF76135BF40B62@NYEXCH02.fxall.com>
References: <EA6FFD6D7496E940BA1A5A5C1CEFAF76135BF40B62@NYEXCH02.fxall.com>
Message-ID: <4D779FD8.9090307@kav.dk>

On 09-03-2011 15:58, Navin Jha wrote:
> Hi,
>
> The TreeMap uses Red-Black tree but ConcurrentSkipListMap uses skipList.
> What is the reason for using skipList for sorted ConcurrentMap
> implementation instead of Red-Black tree? Does skipList implementation
> have some advantage when used by multiple threads?
>
> Thanks,
>
> Navin
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

>From ConcurrentSkipListMap
---
Given the use of tree-like index nodes, you might wonder why
this doesn't use some kind of search tree instead, which would
support somewhat faster search operations. The reason is that
there are no known efficient lock-free insertion and deletion
algorithms for search trees.
---

There is recent paper by Bronson on a concurrent binary search tree.

link : http://ppl.stanford.edu/papers/ppopp207-bronson.pdf
src : https://github.com/nbronson/snaptree

I haven't had any chance to try it out though, maybe others have 
experience, or can comment on the paper?

Cheers
   Kasper

From viktor.klang at gmail.com  Wed Mar  9 15:38:49 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3IgS2xhbmc=?=)
Date: Wed, 9 Mar 2011 21:38:49 +0100
Subject: [concurrency-interest] BoundedPriorityBlockingQueue
Message-ID: <AANLkTimyuYJF8excF+W-K__qX6-m9hLgW1bOipzJQ9rc@mail.gmail.com>

Hey,

anyone have an implementation of a bounded PriorityBlockingQueue lying
around?
Thinking about writing it myself but I'd rather not reinvent the wheel if
possible,

I want it to obey the contract of BlockingQueue, so block puts, offer with
timeout etc...

Suggestions?

Cheers,

-- 
Viktor Klang,
Code Connoisseur
Work:   Scalable Solutions <http://www.scalablesolutions.se>
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110309/417a7d4e/attachment.html>

From david at walend.net  Thu Mar 10 22:31:05 2011
From: david at walend.net (David Walend)
Date: Thu, 10 Mar 2011 22:31:05 -0500
Subject: [concurrency-interest]  BoundedPriorityBlockingQueue
In-Reply-To: <mailman.1.1299776400.5316.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1299776400.5316.concurrency-interest@cs.oswego.edu>
Message-ID: <E5AF6BEA-1B20-49E1-8A52-B49C7F128038@walend.net>

Hi Viktor,

I haven't got one in pocket, but it shouldn't be hard to build. Start  
with a PriorityQueue and this example: http://javaconcurrencyinpractice.com/listings/ConditionBoundedBuffer.java 
  .

Wrap the code in the example around the PriorityQueue, then fill in  
the missing pieces with various await()s to get the timeouts to do all  
of BlockingQueue. It should be fun.

Dave

On Mar 10, 2011, at 12:00 PM, concurrency-interest- 
request at cs.oswego.edu wrote:

> Date: Wed, 9 Mar 2011 21:38:49 +0100
> From: ?iktor Klang <viktor.klang at gmail.com>
> Subject: [concurrency-interest] BoundedPriorityBlockingQueue
> To: concurrency-interest at cs.oswego.edu
> Message-ID:
> 	<AANLkTimyuYJF8excF+W-K__qX6-m9hLgW1bOipzJQ9rc at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Hey,
>
> anyone have an implementation of a bounded PriorityBlockingQueue lying
> around?
> Thinking about writing it myself but I'd rather not reinvent the  
> wheel if
> possible,
>
> I want it to obey the contract of BlockingQueue, so block puts,  
> offer with
> timeout etc...
>
> Suggestions?
>
> Cheers,
>
> -- 
> Viktor Klang,
> Code Connoisseur
> Work:   Scalable Solutions <http://www.scalablesolutions.se>
> Code:   github.com/viktorklang
> Follow: twitter.com/viktorklang
> Read:   klangism.tumblr.com


From viktor.klang at gmail.com  Fri Mar 11 04:04:02 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3IgS2xhbmc=?=)
Date: Fri, 11 Mar 2011 10:04:02 +0100
Subject: [concurrency-interest] BoundedPriorityBlockingQueue
In-Reply-To: <F2FCDF2E-6089-46B7-9D80-9C6512A0574A@atlassian.com>
References: <AANLkTimyuYJF8excF+W-K__qX6-m9hLgW1bOipzJQ9rc@mail.gmail.com>
	<F2FCDF2E-6089-46B7-9D80-9C6512A0574A@atlassian.com>
Message-ID: <AANLkTi=Rf87s+9w0QR-xjrAvX1MRXUYntDnqsX5fFGgA@mail.gmail.com>

On Fri, Mar 11, 2011 at 4:53 AM, Jed Wesley-Smith <
jwesleysmith at atlassian.com> wrote:

> I'm still trying to work out why you'd need something like this and some of
> the semantics when full. It doesn't for instance seem to make much sense to
> have a high-priority item that is blocked waiting for possibly lower
> priority items to be removed.
>

It's intended for use as a mailbox for our Actor implementation, the
blocking and capacity feature is intended for putting a backpressure to
avoid OOMEs at high loads.

I just want the exact same blocking behavior as, let's say,
LinkedBlockingQueue, only difference is that I want it seeded with a
comparator and hav it order the queue after that comparator.

Makes sense?

Cheers,


>
> cheers,
> jed.
>
> On 10/03/2011, at 7:38 AM, ?iktor Klang wrote:
>
> Hey,
>
> anyone have an implementation of a bounded PriorityBlockingQueue lying
> around?
> Thinking about writing it myself but I'd rather not reinvent the wheel if
> possible,
>
> I want it to obey the contract of BlockingQueue, so block puts, offer with
> timeout etc...
>
> Suggestions?
>
> Cheers,
>
> --
> Viktor Klang,
> Code Connoisseur
> Work:   Scalable Solutions <http://www.scalablesolutions.se/>
> Code:   github.com/viktorklang
> Follow: twitter.com/viktorklang
> Read:   klangism.tumblr.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>


-- 
Viktor Klang,
Code Connoisseur
Work:   Scalable Solutions <http://www.scalablesolutions.se>
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110311/f0d2e880/attachment.html>

From viktor.klang at gmail.com  Sat Mar 12 04:37:21 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3IgS2xhbmc=?=)
Date: Sat, 12 Mar 2011 10:37:21 +0100
Subject: [concurrency-interest] BoundedPriorityBlockingQueue
In-Reply-To: <A4522326-1A3D-4ACB-B472-DDEF0AD67529@atlassian.com>
References: <AANLkTimyuYJF8excF+W-K__qX6-m9hLgW1bOipzJQ9rc@mail.gmail.com>
	<F2FCDF2E-6089-46B7-9D80-9C6512A0574A@atlassian.com>
	<AANLkTi=Rf87s+9w0QR-xjrAvX1MRXUYntDnqsX5fFGgA@mail.gmail.com>
	<A4522326-1A3D-4ACB-B472-DDEF0AD67529@atlassian.com>
Message-ID: <AANLkTi=3z=8dm7dyK7Ys-g8=j_GgkTNrwTNTn3Th7kYf@mail.gmail.com>

On Sat, Mar 12, 2011 at 5:04 AM, Jed Wesley-Smith <
jwesleysmith at atlassian.com> wrote:

> The problem though for a Priority based queue is that you always
>

Nope, not always, not when the queue is full.


> want higher priority items to jump up the queue. That means that you could
> be constantly pre-empting the removal of lower priority items. Eventually,
> you'll have to process the lower items _before_* you can process the higher
> ones if you limit the size of the container. Otherwise you need to start
> dropping lower priority items.
>

No, I just want to put a backpressure on the producer when the consumer
cannot keep up.


>
> What sort of priorities do you use? Do you have a small set of priorities
> or is it a more complex comparable type? I ask because, if you have a small
> number of priorities you can stitch together a number of fixed capacity
> BlockingQueues (one for each priority) into a composite BQ quite easily.
>

Priorities will be set with a Comparator, so it's unknown by design.

Whipped up a BoundedBlockingQueue that can wrap any Queue and add bounds +
blocking queue behavior.
That means for the unbounded case I can use the PriorityBlockingQueue, and
for the bounded case I can just wrap a PriorityQueue.

Cheers,


>
> cheers,
> jed.
>
> On 11/03/2011, at 8:04 PM, ?iktor Klang wrote:
>
>
>
> On Fri, Mar 11, 2011 at 4:53 AM, Jed Wesley-Smith <
> jwesleysmith at atlassian.com> wrote:
>
>> I'm still trying to work out why you'd need something like this and some
>> of the semantics when full. It doesn't for instance seem to make much sense
>> to have a high-priority item that is blocked waiting for possibly lower
>> priority items to be removed.
>>
>
> It's intended for use as a mailbox for our Actor implementation, the
> blocking and capacity feature is intended for putting a backpressure to
> avoid OOMEs at high loads.
>
> I just want the exact same blocking behavior as, let's say,
> LinkedBlockingQueue, only difference is that I want it seeded with a
> comparator and hav it order the queue after that comparator.
>
> Makes sense?
>
> Cheers,
>
>
>>
>> cheers,
>> jed.
>>
>> On 10/03/2011, at 7:38 AM, ?iktor Klang wrote:
>>
>> Hey,
>>
>> anyone have an implementation of a bounded PriorityBlockingQueue lying
>> around?
>> Thinking about writing it myself but I'd rather not reinvent the wheel if
>> possible,
>>
>> I want it to obey the contract of BlockingQueue, so block puts, offer with
>> timeout etc...
>>
>> Suggestions?
>>
>> Cheers,
>>
>> --
>> Viktor Klang,
>> Code Connoisseur
>> Work:   Scalable Solutions <http://www.scalablesolutions.se/>
>> Code:   github.com/viktorklang
>> Follow: twitter.com/viktorklang
>> Read:   klangism.tumblr.com
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
> --
> Viktor Klang,
> Code Connoisseur
> Work:   Scalable Solutions <http://www.scalablesolutions.se/>
> Code:   github.com/viktorklang
> Follow: twitter.com/viktorklang
> Read:   klangism.tumblr.com
>
>
>


-- 
Viktor Klang,
Code Connoisseur
Work:   Scalable Solutions <http://www.scalablesolutions.se>
Code:   github.com/viktorklang
Follow: twitter.com/viktorklang
Read:   klangism.tumblr.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110312/2364c5d6/attachment.html>

From lrengan at us.ibm.com  Sat Mar 12 16:01:49 2011
From: lrengan at us.ibm.com (Lakshminaraya Renganarayana)
Date: Sat, 12 Mar 2011 16:01:49 -0500
Subject: [concurrency-interest] AUTO: Lakshminaraya Renganarayana is out of
	the office (returning 03/27/2011)
Message-ID: <OFF8C976A8.7F2597CD-ON85257851.00738635-85257851.00738635@us.ibm.com>



I am out of the office until 03/27/2011.

I will not have access to email during the next (first) week (march 14-21).
After that I will have limited access to my emails.


Note: This is an automated response to your message  "Concurrency-interest
Digest, Vol 74, Issue 7" sent on 3/12/2011 12:00:00.

This is the only notification you will receive while this person is away.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110312/bd7c0243/attachment.html>

From mohanr at fss.co.in  Mon Mar 14 03:36:49 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Mon, 14 Mar 2011 13:06:49 +0530
Subject: [concurrency-interest] Utility of final fields
Message-ID: <99072A942C66C14280133ADAC5B03E3204C86D@fssbemail.fss.india>

Hi,

 

This section from the JLS seems to mean that final fields interact with
threads in a more predictable way than non-finals. How does this rule
interact with the JMM guarantees ?

 

I am just trying to understand this from a novice's perspective.

 

JLS 17.5

 

The example below illustrates how final fields compare to normal fields.

class FinalFieldExample {

final int x;

int y;

static FinalFieldExample f;

public FinalFieldExample() {

x = 3;

y = 4;

}

static void writer() {

f = new FinalFieldExample();

}

static void reader() {

if (f != null) {

int i = f.x; // guaranteed to see 3

int j = f.y; // could see 0

}

}

}

The class FinalFieldExample has a final int field x and a non-final int
field y. One

thread might execute the method writer(), and another might execute the
method

reader().

Because writer() writes f after the object's constructor finishes, the
reader() will be

guaranteed to see the properly initialized value for f.x: it will read
the value 3. However,

f.y is not final; the reader() method is therefore not guaranteed to see
the value 4 for it.

 

 

Thanks,

Mohan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110314/40191997/attachment.html>

From davidcholmes at aapt.net.au  Mon Mar 14 03:45:45 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 14 Mar 2011 17:45:45 +1000
Subject: [concurrency-interest] Utility of final fields
In-Reply-To: <99072A942C66C14280133ADAC5B03E3204C86D@fssbemail.fss.india>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEPAILAA.davidcholmes@aapt.net.au>

I'm not sure I understand your question. This section is is giving an
example of how the JMM rules for final fields work. JLS 17 contains the
definition of the JMM.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohan
Radhakrishnan
  Sent: Monday, 14 March 2011 5:37 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Utility of final fields


  Hi,



  This section from the JLS seems to mean that final fields interact with
threads in a more predictable way than non-finals. How does this rule
interact with the JMM guarantees ?



  I am just trying to understand this from a novice's perspective.



  JLS 17.5



  The example below illustrates how final fields compare to normal fields.

  class FinalFieldExample {

  final int x;

  int y;

  static FinalFieldExample f;

  public FinalFieldExample() {

  x = 3;

  y = 4;

  }

  static void writer() {

  f = new FinalFieldExample();

  }

  static void reader() {

  if (f != null) {

  int i = f.x; // guaranteed to see 3

  int j = f.y; // could see 0

  }

  }

  }

  The class FinalFieldExample has a final int field x and a non-final int
field y. One

  thread might execute the method writer(), and another might execute the
method

  reader().

  Because writer() writes f after the object's constructor finishes, the
reader() will be

  guaranteed to see the properly initialized value for f.x: it will read the
value 3. However,

  f.y is not final; the reader() method is therefore not guaranteed to see
the value 4 for it.





  Thanks,

  Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110314/87d17599/attachment.html>

From aleksey.shipilev at gmail.com  Mon Mar 14 03:53:49 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Mon, 14 Mar 2011 10:53:49 +0300
Subject: [concurrency-interest] Utility of final fields
In-Reply-To: <99072A942C66C14280133ADAC5B03E3204C86D@fssbemail.fss.india>
References: <99072A942C66C14280133ADAC5B03E3204C86D@fssbemail.fss.india>
Message-ID: <AANLkTimzZuAb0i-a-K5xcf_Ho5EG9vOjr2bNcddVmZCK@mail.gmail.com>

Hi Mohan,

JMM is already a part of JLS (entire chapter 17). If you like to read
up extended technical background on final fields semantics, you would
probably like to read "Final fields" section from JSR-133 Cookbook
[1].

Referenced JLS paragraph says "An object is considered to be
completely initialized when its constructor finishes. A thread that
can only see a reference to an object after that object has been
completely initialized is guaranteed to see the correctly initialized
values for that object's final fields."

In that particular example, store to non-final field "y" can be
reordered with store to "f", and hence reordered with "y" read, which
can result in reading "y == 0". Store to final field "x" can not be
reordered with store to "f", hence with reading "x".

-Aleksey.

[1] http://g.oswego.edu/dl/jmm/cookbook.html

On Mon, Mar 14, 2011 at 10:36 AM, Mohan Radhakrishnan <mohanr at fss.co.in> wrote:
> Hi,
>
>
>
> This section from the JLS seems to mean that final fields interact with
> threads in a more predictable way than non-finals. How does this rule
> interact with the JMM guarantees ?
>
>
>
> I am just trying to understand this from a novice?s perspective.
>
>
>
> JLS 17.5
>
>
>
> The example below illustrates how final fields compare to normal fields.
>
> class FinalFieldExample {
>
> final int x;
>
> int y;
>
> static FinalFieldExample f;
>
> public FinalFieldExample() {
>
> x = 3;
>
> y = 4;
>
> }
>
> static void writer() {
>
> f = new FinalFieldExample();
>
> }
>
> static void reader() {
>
> if (f != null) {
>
> int i = f.x; // guaranteed to see 3
>
> int j = f.y; // could see 0
>
> }
>
> }
>
> }
>
> The class FinalFieldExample has a final int field x and a non-final int
> field y. One
>
> thread might execute the method writer(), and another might execute the
> method
>
> reader().
>
> Because writer() writes f after the object's constructor finishes, the
> reader() will be
>
> guaranteed to see the properly initialized value for f.x: it will read the
> value 3. However,
>
> f.y is not final; the reader() method is therefore not guaranteed to see the
> value 4 for it.
>
>
>
>
>
> Thanks,
>
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From mohanr at fss.co.in  Mon Mar 14 08:00:31 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Mon, 14 Mar 2011 17:30:31 +0530
Subject: [concurrency-interest] Utility of final fields
In-Reply-To: <AANLkTimzZuAb0i-a-K5xcf_Ho5EG9vOjr2bNcddVmZCK@mail.gmail.com>
Message-ID: <99072A942C66C14280133ADAC5B03E3207F382@fssbemail.fss.india>

Hi,
    Ok. JMM is part of JLS. I didn't know that final fields had this
type of semantics.

Thanks,
Mohan

-----Original Message-----
From: Aleksey Shipilev [mailto:aleksey.shipilev at gmail.com] 
Sent: Monday, March 14, 2011 1:24 PM
To: Mohan Radhakrishnan
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Utility of final fields

Hi Mohan,

JMM is already a part of JLS (entire chapter 17). If you like to read
up extended technical background on final fields semantics, you would
probably like to read "Final fields" section from JSR-133 Cookbook
[1].

Referenced JLS paragraph says "An object is considered to be
completely initialized when its constructor finishes. A thread that
can only see a reference to an object after that object has been
completely initialized is guaranteed to see the correctly initialized
values for that object's final fields."

In that particular example, store to non-final field "y" can be
reordered with store to "f", and hence reordered with "y" read, which
can result in reading "y == 0". Store to final field "x" can not be
reordered with store to "f", hence with reading "x".

-Aleksey.

[1] http://g.oswego.edu/dl/jmm/cookbook.html

On Mon, Mar 14, 2011 at 10:36 AM, Mohan Radhakrishnan <mohanr at fss.co.in>
wrote:
> Hi,
>
>
>
> This section from the JLS seems to mean that final fields interact
with
> threads in a more predictable way than non-finals. How does this rule
> interact with the JMM guarantees ?
>
>
>
> I am just trying to understand this from a novice's perspective.
>
>
>
> JLS 17.5
>
>
>
> The example below illustrates how final fields compare to normal
fields.
>
> class FinalFieldExample {
>
> final int x;
>
> int y;
>
> static FinalFieldExample f;
>
> public FinalFieldExample() {
>
> x = 3;
>
> y = 4;
>
> }
>
> static void writer() {
>
> f = new FinalFieldExample();
>
> }
>
> static void reader() {
>
> if (f != null) {
>
> int i = f.x; // guaranteed to see 3
>
> int j = f.y; // could see 0
>
> }
>
> }
>
> }
>
> The class FinalFieldExample has a final int field x and a non-final
int
> field y. One
>
> thread might execute the method writer(), and another might execute
the
> method
>
> reader().
>
> Because writer() writes f after the object's constructor finishes, the
> reader() will be
>
> guaranteed to see the properly initialized value for f.x: it will read
the
> value 3. However,
>
> f.y is not final; the reader() method is therefore not guaranteed to
see the
> value 4 for it.
>
>
>
>
>
> Thanks,
>
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From mohit.riverstone at gmail.com  Tue Mar 22 02:07:46 2011
From: mohit.riverstone at gmail.com (Mohit Kumar)
Date: Tue, 22 Mar 2011 11:37:46 +0530
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 74,
	Issue 2
In-Reply-To: <AANLkTimcNagjT0j8zhtPYHgf3Rty53aMs9npBpW2M_ZT@mail.gmail.com>
References: <mailman.1.1299171600.10828.concurrency-interest@cs.oswego.edu>
	<AANLkTimcNagjT0j8zhtPYHgf3Rty53aMs9npBpW2M_ZT@mail.gmail.com>
Message-ID: <AANLkTim2Lz2Yx4jA1B76Sz-wvRHeOU6qtT=YMP_8Ss2e@mail.gmail.com>

Hi Doug,

There are two versions of ConcurrentLinkedDeque floating. One in which tail
and head pointers lag like LTQ and CLQ.
And the other in which removal used boxing to mark the next pointer and, the
head and tail dont lag.
Which is a better one and why? Should be the second one(marking).

Also, in the second one the doc says the prev pointers are optimistically
linked. Did we have other choice?? Not very clear.

Thanks and regards
Mohit
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110322/503b6f55/attachment.html>

From martinrb at google.com  Tue Mar 22 17:10:24 2011
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 22 Mar 2011 14:10:24 -0700
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 74,
	Issue 2
In-Reply-To: <AANLkTim2Lz2Yx4jA1B76Sz-wvRHeOU6qtT=YMP_8Ss2e@mail.gmail.com>
References: <mailman.1.1299171600.10828.concurrency-interest@cs.oswego.edu>
	<AANLkTimcNagjT0j8zhtPYHgf3Rty53aMs9npBpW2M_ZT@mail.gmail.com>
	<AANLkTim2Lz2Yx4jA1B76Sz-wvRHeOU6qtT=YMP_8Ss2e@mail.gmail.com>
Message-ID: <AANLkTimtaVqK4mz6ikPQzdsW+4QS7Pu6i0QqsLSnMFkK@mail.gmail.com>

On Mon, Mar 21, 2011 at 23:07, Mohit Kumar <mohit.riverstone at gmail.com>wrote:

> Hi Doug,
>
> There are two versions of ConcurrentLinkedDeque floating. One in which tail
> and head pointers lag like LTQ and CLQ.
> And the other in which removal used boxing to mark the next pointer and,
> the head and tail dont lag.
> Which is a better one and why? Should be the second one(marking).
>
> Also, in the second one the doc says the prev pointers are optimistically
> linked. Did we have other choice?? Not very clear.
>
>
Naturally, we like the one that ended up in java.util.concurrent !

Do you have suggestions for improving the comments?

Martin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110322/f6320f53/attachment.html>

From mohanr at fss.co.in  Thu Mar 24 01:50:25 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Thu, 24 Mar 2011 11:20:25 +0530
Subject: [concurrency-interest] Visualize parallelism
Message-ID: <99072A942C66C14280133ADAC5B03E320BC9E7@fssbemail.fss.india>

Hi,

 

        I came across several tools like the Linux Trace Toolkit(
http://lttng.org/ )

 in the recent issue of ACM's 'Software' magazine. This issue is devoted
to parallel processing.

 

Hope I am not going overboard with these types of tools.

 

Thanks,

Mohan

 

 

Are you looking for something that would show you the task graph that
gets
created? I don't know of any such tools, though it would be interesting
to
know a few basic facts about the graph when tuning your algorithms.
 
The ability to use different cores is just a consequence of
multi-threading.
There's nothing specific to FJ about that - it will use as many cores as
it
has threads with work to do.
 
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
<http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
[mailto:concurrency-interest-bounces at cs.oswego.edu
<http://cs.oswego.edu/mailman/listinfo/concurrency-interest> ]On Behalf
Of Mohan
Radhakrishnan
  Sent: Wednesday, 16 February 2011 4:52 PM
  To: concurrency-interest at cs.oswego.edu
<http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
  Subject: [concurrency-interest] Visualize parallelism
 
 
  Hi,
 
 
 
  This is a newbie question.
 
 
 
          What is the recommended way to visualize how the FJ API's
parallelism is able to use different cores ? Is there a Windows tool for
visualizing it at that level ?
 
 
 
  Is any tool like this used by the team to test the API ?
 
 
 
  Thanks,
 
  Mohan

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110324/47eee4bd/attachment.html>

From jim.andreou at gmail.com  Thu Mar 24 02:07:28 2011
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Wed, 23 Mar 2011 23:07:28 -0700
Subject: [concurrency-interest] Visualize parallelism
In-Reply-To: <99072A942C66C14280133ADAC5B03E320BC9E7@fssbemail.fss.india>
References: <99072A942C66C14280133ADAC5B03E320BC9E7@fssbemail.fss.india>
Message-ID: <AANLkTik9QK+SR+=WAstkBiAcMcTm0d4_R_8=dyTNKm_Q@mail.gmail.com>

I'm not closely following the graph drawing community, but what is needed
here is a good visualization of so called Series-Parallel graphs (which I
think is exactly what FJ computations amounts to). If anyone has access to
interested students, that could make a fun project.

Dimitris

On Wed, Mar 23, 2011 at 10:50 PM, Mohan Radhakrishnan <mohanr at fss.co.in>wrote:

>  Hi,
>
>
>
>         I came across several tools like the Linux Trace Toolkit(
> http://lttng.org/ )
>
>  in the recent issue of ACM?s ?Software? magazine. This issue is devoted to
> parallel processing.
>
>
>
> Hope I am not going overboard with these types of tools.
>
>
>
> Thanks,
>
> Mohan
>
>
>
>
>
> Are you looking for something that would show you the task graph that gets
>
> created? I don't know of any such tools, though it would be interesting to
>
> know a few basic facts about the graph when tuning your algorithms.
>
>
>
> The ability to use different cores is just a consequence of multi-threading.
>
> There's nothing specific to FJ about that - it will use as many cores as it
>
> has threads with work to do.
>
>
>
> David Holmes
>
>   -----Original Message-----
>
>   From: concurrency-interest-bounces at cs.oswego.edu <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
> [mailto:concurrency-interest-bounces at cs.oswego.edu <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>]On Behalf Of Mohan
>
> Radhakrishnan
>
>   Sent: Wednesday, 16 February 2011 4:52 PM
>
>   To: concurrency-interest at cs.oswego.edu <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>   Subject: [concurrency-interest] Visualize parallelism
>
>
>
>
>
>   Hi,
>
>
>
>
>
>
>
>   This is a newbie question.
>
>
>
>
>
>
>
>           What is the recommended way to visualize how the FJ API's
>
> parallelism is able to use different cores ? Is there a Windows tool for
>
> visualizing it at that level ?
>
>
>
>
>
>
>
>   Is any tool like this used by the team to test the API ?
>
>
>
>
>
>
>
>   Thanks,
>
>
>
>   Mohan
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110323/fa6616d9/attachment.html>

From mettafort at gmail.com  Thu Mar 24 08:50:34 2011
From: mettafort at gmail.com (Insane Membrane)
Date: Thu, 24 Mar 2011 07:50:34 -0500
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
Message-ID: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>

Hi,

I am writing with the hope of finding some helpful advice on where to look
deeper into the theory and practice of building concurrent, object models
that involve large numbers of updates.  In front office finance, there is
the need to handle multiple data feeds, and then apply them to some object
model which is to be ultimately rendered in a trading UI.

A common approach is to use immutability, but that often involves copying
objects and then applying an update, or some other mechanism....I am curious
to find out other approaches.  Copying at the data feed/communications level
can result in large numbers of objects being created, causing a GC issue.  I
have built designs that allow object state to be stored in arrays, which
allows the copying to avoid creating new objects, but these designs can be
complicated, possibly prematurely.

Could anyone offer advice/papers/concepts on this topic? I have Concurrency
in Practice and Concurrent Programming in Java on my desk now, but looking
for other sources if there are any.

Many thanks,

MF.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110324/8bc15022/attachment-0001.html>

From carfield at carfield.com.hk  Thu Mar 24 12:31:01 2011
From: carfield at carfield.com.hk (Carfield Yim)
Date: Fri, 25 Mar 2011 00:31:01 +0800
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
Message-ID: <AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>

I think update immutability have to clone the order. You may consider
model the amendment as change set ( for me I usually name it as delta
object ) and maintain in a collection, maybe a list.

Although just that list maybe a mutable list, or maybe
copyonwritelist, but it likely save you from a lot of GC and enjoy
most benefit of immutable object

On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com> wrote:
> Hi,
> I am writing with the hope of finding some helpful advice on where to look
> deeper into the theory and practice of building concurrent, object models
> that involve large numbers of updates. ?In front office finance, there is
> the need to handle multiple data feeds, and then apply them to some object
> model which is to be ultimately rendered in a trading UI.
> A common approach is to use immutability, but that often involves copying
> objects and then applying an update, or some other mechanism....I am curious
> to find out other approaches. ?Copying at the data feed/communications level
> can result in large numbers of objects being created, causing a GC issue. ?I
> have built designs that allow object state to be stored in arrays, which
> allows the copying to avoid creating new objects, but these designs can be
> complicated, possibly prematurely.
> Could anyone offer advice/papers/concepts on this topic? I have Concurrency
> in Practice and Concurrent Programming in Java on my desk now, but looking
> for other sources if there are any.
> Many thanks,
> MF.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From ben_manes at yahoo.com  Thu Mar 24 15:39:36 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 24 Mar 2011 12:39:36 -0700 (PDT)
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
Message-ID: <337855.64253.qm@web38804.mail.mud.yahoo.com>

It is often acceptable to weaken the consistency guarantee to improve the write 
performance. If writes are more frequent then reads, then a write-back buffer 
may be used to batch the updates operations under the lock. A writer no longer 
suffers the penalty of acquiring the lock since their work is scheduled on a 
queue, which is a much cheaper operation. When a consistent read is required, 
the thread can take the penalty of catching up the object's state by applying 
the pending operations. The draining of the buffer(s) can be performed on either 
a dedicated thread or amortized on user-threads, the latter by using a tryLock 
so that concurrent operations just append to the buffer without blocking. If 
applicable, a fuller description of this approach in a real-world example is 
available here [1].


[1] http://code.google.com/p/concurrentlinkedhashmap/wiki/Design



________________________________
From: Carfield Yim <carfield at carfield.com.hk>
To: Insane Membrane <mettafort at gmail.com>
Cc: concurrency-interest at cs.oswego.edu
Sent: Thu, March 24, 2011 9:31:01 AM
Subject: Re: [concurrency-interest] High Volume Scenarios and Concurrency

I think update immutability have to clone the order. You may consider
model the amendment as change set ( for me I usually name it as delta
object ) and maintain in a collection, maybe a list.

Although just that list maybe a mutable list, or maybe
copyonwritelist, but it likely save you from a lot of GC and enjoy
most benefit of immutable object

On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com> wrote:
> Hi,
> I am writing with the hope of finding some helpful advice on where to look
> deeper into the theory and practice of building concurrent, object models
> that involve large numbers of updates.  In front office finance, there is
> the need to handle multiple data feeds, and then apply them to some object
> model which is to be ultimately rendered in a trading UI.
> A common approach is to use immutability, but that often involves copying
> objects and then applying an update, or some other mechanism....I am curious
> to find out other approaches.  Copying at the data feed/communications level
> can result in large numbers of objects being created, causing a GC issue.  I
> have built designs that allow object state to be stored in arrays, which
> allows the copying to avoid creating new objects, but these designs can be
> complicated, possibly prematurely.
> Could anyone offer advice/papers/concepts on this topic? I have Concurrency
> in Practice and Concurrent Programming in Java on my desk now, but looking
> for other sources if there are any.
> Many thanks,
> MF.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110324/8d49cee2/attachment.html>

From mettafort at gmail.com  Thu Mar 24 22:00:28 2011
From: mettafort at gmail.com (Insane Membrane)
Date: Thu, 24 Mar 2011 21:00:28 -0500
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <AANLkTikeCvDOPiiue14oRx+2QHwj7zfOj1-jvDDSZSG7@mail.gmail.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
	<AANLkTimMunfo8LOGtQFivGKAQFiMbkDz3k8DxKau+p2c@mail.gmail.com>
	<AANLkTinbGqUE3S1XnxwX4k6Z66_7QQLs7xHYWeoq_jja@mail.gmail.com>
	<AANLkTikeCvDOPiiue14oRx+2QHwj7zfOj1-jvDDSZSG7@mail.gmail.com>
Message-ID: <AANLkTin5qbbswUDc+3S6ED6eHPW98ARbze2p3eOAguPK@mail.gmail.com>

Reposting for the benefit of the group...my mailer just did a direct
reply to the author...

On Thursday, March 24, 2011, Insane Membrane <mettafort at gmail.com> wrote:
> I am not exactly sure how this would work, the parts aren't coming
> together in my mind. ?Could you explain a bit further?
>
> I have implemented something in the past were I track what fields are
> modified, maintaining the values that are applied. ?When a subsequent
> update comes in on the writer side, it uses the latest values in the
> change set, makes modifications, and makes the new update ready for
> the reader. ?The reader then pulls over and applies the changes, in
> essence, to see the latest version of the object. ?However this works
> really only with simple objects, though I suspect it could work with
> more canonical object model designs. ?The other issue is it only
> supports one writer and one reader, or multiple readers (reader is a
> thread) but requires tricky synchronization.
>
> Interested to know more about your approach, if you would be kind
> enough to explain in more detail?
>
> On Thursday, March 24, 2011, Carfield Yim <carfield at carfield.com.hk> wrote:
>> There will be no clone of the original data, the list contain original
>> data, then with series of delta object.
>>
>> When system read the attributes, it do something like
>>
>> String getName() {
>> ?? for ( int i = list.size() - ?1 ; i >= 0 ; i-- ) {
>> ?? ? ?if(data[i].name().exist) return data[i].name().get();
>> ?? }
>> }
>> On Fri, Mar 25, 2011 at 3:49 AM, Insane Membrane <mettafort at gmail.com> wrote:
>>> How do you avoid impacting the GC? Arent you still creating that delta
>>> object, a clone of the original order? And then what happens if more changes
>>> come through? Do you simply apply each individual change event to a new
>>> clone of the previous? This would result in many objects again wouldnt it?
>>> Or maybe I am misunderstanding something.
>>> And then the question is, what if Order has off of it a non-trivial object
>>> model? Say a list of objects, each in turn containing other lists for other
>>> data. ?When the object model from a root is complex, is the whole model
>>> copied?
>>> Thanks for your response.
>>> On Thu, Mar 24, 2011 at 11:31 AM, Carfield Yim <carfield at carfield.com.hk>
>>> wrote:
>>>>
>>>> I think update immutability have to clone the order. You may consider
>>>> model the amendment as change set ( for me I usually name it as delta
>>>> object ) and maintain in a collection, maybe a list.
>>>>
>>>> Although just that list maybe a mutable list, or maybe
>>>> copyonwritelist, but it likely save you from a lot of GC and enjoy
>>>> most benefit of immutable object
>>>>
>>>> On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com>
>>>> wrote:
>>>> > Hi,
>>>> > I am writing with the hope of finding some helpful advice on where to
>>>> > look
>>>> > deeper into the theory and practice of building concurrent, object
>>>> > models
>>>> > that involve large numbers of updates. ?In front office finance, there
>>>> > is
>>>> > the need to handle multiple data feeds, and then apply them to some
>>>> > object
>>>> > model which is to be ultimately rendered in a trading UI.
>>>> > A common approach is to use immutability, but that often involves
>>>> > copying
>>>> > objects and then applying an update, or some other mechanism....I am
>>>> > curious
>>>> > to find out other approaches. ?Copying at the data feed/communications
>>>> > level
>>>> > can result in large numbers of objects being created, causing a GC
>>>> > issue. ?I
>>>> > have built designs that allow object state to be stored in arrays, which
>>>> > allows the copying to avoid creating new objects, but these designs can
>>>> > be
>>>> > complicated, possibly prematurely.
>>>> > Could anyone offer advice/papers/concepts on this topic? I have
>>>> > Concurrency
>>>> > in Practice and Concurrent Programming in Java on my desk now, but
>>>> > looking
>>>> > for other sources if there are any.
>>>> > Many thanks,
>>>> > MF.
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> >
>>>> >
>>>
>>>
>>
>


From mettafort at gmail.com  Thu Mar 24 22:14:42 2011
From: mettafort at gmail.com (Insane Membrane)
Date: Thu, 24 Mar 2011 21:14:42 -0500
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <337855.64253.qm@web38804.mail.mud.yahoo.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
	<337855.64253.qm@web38804.mail.mud.yahoo.com>
Message-ID: <AANLkTin1=GhaMqBQRni-CLb2_igSs1LiHrstJjw6KwQx@mail.gmail.com>

Ben,

This is very interesting.  I am currently reading all of the PDFs from
the web link you provided. I need to digest the information, but I am
curious to know how, IF, this would work with an object model.  Assume
an Order and it has a list of OrderItem objects, for each line item.
As order fulfillment takes place, the remaining items to be completed
for the line item is updated.  So a line item may include multiple
things (contrived, stick with me!) concurrently the fulfillment engine
finds the line item and indicates how many items have been filled.
The order needs to hold return the number of items
complete/incomplete.  Let's also say the order can have hundreds of
line items.

The UI could be looking at say 200 line items, and there is a total of
completed items at the top. The UI periodically refreshes, every
2seconds say.  For concurrency, it would need to lock the whole order
before updating the visuals of all the line items and the total.  If
the UI needs to wait for the lock, say due to a large volume of
updates on the fulfillment side, the UI is blocked.  It could do a
tryLock, but let's assume it doesn't.

The other solution is to copy on the fulfillment side and when the UI
wants the new version, it takes the some lock to allow it to process
those items.  We have copies of the order, but we also have copies of
line items that were modified during the 2 sec period.  Even with one
writer, if the desire is to be lock free on the UI side, we want a cas
style pointer swap on the UI side to get the view of the changes, but
this is still a problem when we have updates being applied to two
different objects.  There's solutions but they become somewhat complex
as the object model associations increase.

Even with this simple contrived problem I am interested to see if the
approach you presented Ben will be a workable solution.  Thank you for
sharing.

On Thursday, March 24, 2011, Ben Manes <ben_manes at yahoo.com> wrote:
> It is often acceptable to weaken the consistency guarantee to improve the write performance. If writes are more frequent then reads, then a write-back buffer may be used to batch the updates operations under the lock. A writer no longer suffers the penalty of acquiring the lock since their work is scheduled on a queue, which is a much cheaper operation. When a consistent read is required, the thread can take the penalty of catching up the object's state by applying the pending operations. The draining of the buffer(s) can be performed on either a dedicated thread or amortized on user-threads, the latter by using a tryLock so that concurrent operations just append to the buffer without blocking. If applicable, a fuller description of this approach in a real-world example is available
>  here [1].
>
> [1] http://code.google.com/p/concurrentlinkedhashmap/wiki/Design
> From: Carfield Yim <carfield at carfield.com.hk>
> To: Insane Membrane <mettafort at gmail.com>
> Cc: concurrency-interest at cs.oswego.edu
> Sent: Thu, March 24, 2011 9:31:01 AM
> Subject: Re: [concurrency-interest] High Volume Scenarios and Concurrency
>
> I think update immutability have to clone the
>  order. You may consider
> model the amendment as change set ( for me I usually name it as delta
> object ) and maintain in a collection, maybe a list.
>
> Although just that list maybe a mutable list, or maybe
> copyonwritelist, but it likely save you from a lot of GC and enjoy
> most benefit of immutable object
>
> On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com> wrote:
>> Hi,
>> I am writing with the hope of finding some helpful advice on where to look
>> deeper into the theory and practice of building concurrent, object models
>> that involve large numbers of updates. ?In front office finance, there is
>> the need to handle multiple data feeds, and then apply them to some object
>> model which is to be ultimately rendered in a trading UI.
>> A common approach is to use immutability, but that often
>  involves copying
>> objects and then applying an update, or some other mechanism....I am curious
>> to find out other approaches. ?Copying at the data feed/communications level
>> can result in large numbers of objects being created, causing a GC issue. ?I
>> have built designs that allow object state to be stored in arrays, which
>> allows the copying to avoid creating new objects, but these designs can be
>> complicated, possibly prematurely.
>> Could anyone offer advice/papers/concepts on this topic? I have Concurrency
>> in Practice and Concurrent Programming in Java on my desk now, but looking
>> for other sources if there are any.
>> Many thanks,
>> MF.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>


From ktlam at cs.hku.hk  Thu Mar 24 23:28:55 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Fri, 25 Mar 2011 11:28:55 +0800
Subject: [concurrency-interest] ConcurrentHashMap, volatile field access,
	sequential consistency in distributed shared memory
Message-ID: <00bb01cbea9c$c7695d40$f8b10893@ktlampc>

Hi all, 

I have some questions about transparent support of ConcurrentHashMap (CHM)
or other concurrent utility classes in a distributed runtime environment.
Hope you would find it interesting to discuss on this topic.

Specifically, it's a software distributed shared memory (DSM) - a set of
interconnected JVMs on a cluster of computing nodes - which implements some
memory consistency models like Home-based Lazy Release Consistency (HLRC)
and Sequential Consistency (SC) at the heap level.

In such a system, threads are distributed; no multiprocessor shared memory
and memory fences. So the Java Memory Model (JMM) has to be fulfilled by
extra message passing operations; the semantics of locks and volatiles are
extended through modifying the JVM to guarantee "cluster-wide consistency".

However, I am afraid some of my concepts are still not clear enough.

>From many readings, I have been told that Java volatile fields follow the
Sequential Consistency (SC).

In my learnt concept about SC (in software DSM), a read of a variable X at a
processor is not guaranteed to return the most recent write by another
processor to X. Otherwise, the model is already a strict consistency model.
So it can happen that the read returns an old value in the local cache
(heap) because the update is not yet propagated over the network.

What SC really guarantees is some global order: that is, in a sequentially
consistent world, suppose two processors perform two write operations on a
variable x, a fourth processor could not see the two writes in a different
order than what the third processor observed, i.e. below situation is not
valid:

P1:  W(x)1
-----------------------
P2:        R(x)1 R(x)2
-----------------------
P3:        R(x)2 R(x)1
-----------------------
P4:  W(x)2

Please see below (the above e.g. is taken there):  
http://www.cs.nmsu.edu/~pfeiffer/classes/573/notes/consistency.html


But when I go through some web posts like this:
http://stackoverflow.com/questions/3603157/volatile-keyword-in-java-clarific
ation

I get confused at seeing below sayings:
----------------------------------------------------------------------------
volatile guarantees that reads from the variable always reflects the most up
to update value. The runtime can achieve this in various ways, including not
caching or refreshing the cache when the value has changed.

Before Java 1.5 was released the volatile keyword declared that the field
will get the most recent value of the object by hitting main memory each
time for reads and flushing for writes.

Today's volatile keyword says two very important things:

1. Don't worry about how but know that when reading a volatile field you
will always have the most up to date value.
2. A compiler cannot reorder a volatile read/write as to maintain program
order.
----------------------------------------------------------------------------

If the above sayings are totally correct, then what Java volatile semantics
have ensured is in fact already strict consistency - much stronger than SC.
And these can't be implemented efficiently in a distributed memory
environment. May I ask if the above sayings are true?

I think these are true for shared-memory multiprocessors because the JIT
compiler inserts memory barriers/fences around volatile field accesses that
invalidate and flush processor cache memory to main memory. Memory bus
transactions would be serialized, so a read will return the most recent
write on the main memory. (Is this correct?)

But in a distributed shared memory, such barriers do not exist. Emulating
such barriers via software means of message passing is an expensive
operation that kills scalability. 

I believe the above guarantee is stricter than necessary. Indeed, if Java
volatiles are following SC, then it should be valid to just ensure a
consistent global memory order, but no need for a volatile read to return
the most up-to-date value of a volatile write, which is still on the way of
propagation. Am I correct? If yes, then a cluster-wide SC protocol can be
implemented quite easily yet efficient enough, say with multicasting (See
p.2 of http://deneb.cs.kent.edu/~mikhail/classes/aos.f03/l14dsm.PDF).

The most important question is: would ConcurrentHashMap (CHM) rely on the
property of "when reading a volatile field you will always have the most up
to date value" in order to behave correctly? If CHM runs atop a software
Java DSM implementing the SC model I mentioned, would it be running
correctly?

Note: put() and remove() operations in CHM are synchronized with locks. So
if these write operations are made serialized cluster-wide w.r.t. the
object's home node, I believe this has ensured that every thread will see
the same sequence of the writes, so SC has been observed. Is this also
right?

Somehow, I notice that the new JMM could have complicated the matter, now
the visibility guarantee of a volatile field access has been similar to that
of a lock/unlock operation. So updates made before a volatile write happen
before a volatile read and must be sent to the volatile field reading thread
as well.

Last, I am also astonished and confused at seeing this:
"Volatile memory operations are not sequentially consistent."
from this lecture note page:
http://bluehawk.monmouth.edu/rclayton/web-pages/u03-598/netmemcon.html

----------------------------------------------------------------------------
Sequential Consistency and Volatile Memory
- The sliding fence trick tells us one other thing. 
  - Volatile memory operations are not sequentially consistent. 
  - Two threads can view two volatile memory operations in different orders.

- Java requires that thread-order be preserved for memory operations on
volatile fields.
----------------------------------------------------------------------------

Indeed, I don't understand what the author means by "sliding fence trick".
Anyway, I am quite sure Java volatile accesses must have observe SC or even
strict consistency due to the way the JMM is implemented by most JVMs
through memory fencing.

Your comments are most appreciated.
Thank you so much.

Best regards,
Tin
 



From ben_manes at yahoo.com  Thu Mar 24 23:49:58 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 24 Mar 2011 20:49:58 -0700 (PDT)
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <AANLkTin1=GhaMqBQRni-CLb2_igSs1LiHrstJjw6KwQx@mail.gmail.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
	<337855.64253.qm@web38804.mail.mud.yahoo.com>
	<AANLkTin1=GhaMqBQRni-CLb2_igSs1LiHrstJjw6KwQx@mail.gmail.com>
Message-ID: <146577.80697.qm@web38807.mail.mud.yahoo.com>

I can't offer a satisfactory answer, because each problem is best served by a 
different technique. For user interfaces, I try to follow the BASE [1] 
performance model to minimize the user-perceived latencies and design the data 
model to allow write conflicts to be resolved asynchronously. In the data 
structure example, a portion is inherently single-threaded but a strict 
consistency isn't required. Instead the operations can be recorded and replayed 
later to catch-up with the expected state when necessary (e.g. cache eviction). 
The bottleneck wasn't the execution of each operation but rather the thrashing 
of the exclusive lock, so buffering / batching the work under the lock enabled 
better concurrency behavior. The usage of the lock is primarily to maintain the 
much simpler sequential programming model. The penalty of applying those 
operations is kept small by amortizing it, such as performing only 64 of those 
tasks. When a consistent read is required then only a few operations should be 
pending, so there shouldn't be a significant penalty. Perhaps the simplest way 
to think of this technique is in terms of a journaled file-system, except that 
transactions being replayed are on a custom data structure.

In a general purpose object model you tend to lose the ability to make design 
assumptions since there is too much flexibility. Instead of focusing on 
concurrency the practical solutions usually involve some form of versioning 
(e.g. transactions), the scoping of data to allow parallelism, and reads from a 
slightly stale snapshot.

[1] http://www.addsimplicity.com/downloads/ArchitectingForLatency.pdf



________________________________
From: Insane Membrane <mettafort at gmail.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: Carfield Yim <carfield at carfield.com.hk>; 
"concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Sent: Thu, March 24, 2011 7:14:42 PM
Subject: Re: High Volume Scenarios and Concurrency

Ben,

This is very interesting.  I am currently reading all of the PDFs from
the web link you provided. I need to digest the information, but I am
curious to know how, IF, this would work with an object model.  Assume
an Order and it has a list of OrderItem objects, for each line item.
As order fulfillment takes place, the remaining items to be completed
for the line item is updated.  So a line item may include multiple
things (contrived, stick with me!) concurrently the fulfillment engine
finds the line item and indicates how many items have been filled.
The order needs to hold return the number of items
complete/incomplete.  Let's also say the order can have hundreds of
line items.

The UI could be looking at say 200 line items, and there is a total of
completed items at the top. The UI periodically refreshes, every
2seconds say.  For concurrency, it would need to lock the whole order
before updating the visuals of all the line items and the total.  If
the UI needs to wait for the lock, say due to a large volume of
updates on the fulfillment side, the UI is blocked.  It could do a
tryLock, but let's assume it doesn't.

The other solution is to copy on the fulfillment side and when the UI
wants the new version, it takes the some lock to allow it to process
those items.  We have copies of the order, but we also have copies of
line items that were modified during the 2 sec period.  Even with one
writer, if the desire is to be lock free on the UI side, we want a cas
style pointer swap on the UI side to get the view of the changes, but
this is still a problem when we have updates being applied to two
different objects.  There's solutions but they become somewhat complex
as the object model associations increase.

Even with this simple contrived problem I am interested to see if the
approach you presented Ben will be a workable solution.  Thank you for
sharing.

On Thursday, March 24, 2011, Ben Manes <ben_manes at yahoo.com> wrote:
> It is often acceptable to weaken the consistency guarantee to improve the write 
>performance. If writes are more frequent then reads, then a write-back buffer 
>may be used to batch the updates operations under the lock. A writer no longer 
>suffers the penalty of acquiring the lock since their work is scheduled on a 
>queue, which is a much cheaper operation. When a consistent read is required, 
>the thread can take the penalty of catching up the object's state by applying 
>the pending operations. The draining of the buffer(s) can be performed on either 
>a dedicated thread or amortized on user-threads, the latter by using a tryLock 
>so that concurrent operations just append to the buffer without blocking. If 
>applicable, a fuller description of this approach in a real-world example is 
>available
>  here [1].
>
> [1] http://code.google.com/p/concurrentlinkedhashmap/wiki/Design
> From: Carfield Yim <carfield at carfield.com.hk>
> To: Insane Membrane <mettafort at gmail.com>
> Cc: concurrency-interest at cs.oswego.edu
> Sent: Thu, March 24, 2011 9:31:01 AM
> Subject: Re: [concurrency-interest] High Volume Scenarios and Concurrency
>
> I think update immutability have to clone the
>  order. You may consider
> model the amendment as change set ( for me I usually name it as delta
> object ) and maintain in a collection, maybe a list.
>
> Although just that list maybe a mutable list, or maybe
> copyonwritelist, but it likely save you from a lot of GC and enjoy
> most benefit of immutable object
>
> On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com> wrote:
>> Hi,
>> I am writing with the hope of finding some helpful advice on where to look
>> deeper into the theory and practice of building concurrent, object models
>> that involve large numbers of updates.  In front office finance, there is
>> the need to handle multiple data feeds, and then apply them to some object
>> model which is to be ultimately rendered in a trading UI.
>> A common approach is to use immutability, but that often
>  involves copying
>> objects and then applying an update, or some other mechanism....I am curious
>> to find out other approaches.  Copying at the data feed/communications level
>> can result in large numbers of objects being created, causing a GC issue.  I
>> have built designs that allow object state to be stored in arrays, which
>> allows the copying to avoid creating new objects, but these designs can be
>> complicated, possibly prematurely.
>> Could anyone offer advice/papers/concepts on this topic? I have Concurrency
>> in Practice and Concurrent Programming in Java on my desk now, but looking
>> for other sources if there are any.
>> Many thanks,
>> MF.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110324/e547fab8/attachment.html>

From davidcholmes at aapt.net.au  Fri Mar 25 00:20:32 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 25 Mar 2011 14:20:32 +1000
Subject: [concurrency-interest] ConcurrentHashMap, volatile field access,
	sequential consistency in distributed shared memory
In-Reply-To: <00bb01cbea9c$c7695d40$f8b10893@ktlampc>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBNIMAA.davidcholmes@aapt.net.au>

Tin,

To get a thorough and accurate response to this I would suggest posting on
the JMM discussion list instead:

https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion

At the risk of being inaccurate I'll make a couple of responses to some of
your questions.

First, yes the JIT and runtime introduce whatever barriers/fences are needed
to enforce the Java Memory Model semantics.

Second, it is not required that a read of a volatile variable must see the
most recent write - as you say that write might still be awaiting
propogation, for example sitting in a store buffer. It is only required that
the happens-before orderings are preserved. Without a global observer the
only way a thread can detect that a read returned an earlier value is by
examining a second variable that indicates what the original should be. As
long as the happens-before ordering is preserved then a thread can not tell
whether a read of X that yields 0 occurred just before or just after a write
to X that set 1.

Third, CHM relies on the happens-before properties that volatiles provide -
no more, no less.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of King Tin
> Lam
> Sent: Friday, 25 March 2011 1:29 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ConcurrentHashMap, volatile field
> access,sequential consistency in distributed shared memory
>
>
> Hi all,
>
> I have some questions about transparent support of ConcurrentHashMap (CHM)
> or other concurrent utility classes in a distributed runtime environment.
> Hope you would find it interesting to discuss on this topic.
>
> Specifically, it's a software distributed shared memory (DSM) - a set of
> interconnected JVMs on a cluster of computing nodes - which
> implements some
> memory consistency models like Home-based Lazy Release Consistency (HLRC)
> and Sequential Consistency (SC) at the heap level.
>
> In such a system, threads are distributed; no multiprocessor shared memory
> and memory fences. So the Java Memory Model (JMM) has to be fulfilled by
> extra message passing operations; the semantics of locks and volatiles are
> extended through modifying the JVM to guarantee "cluster-wide
> consistency".
>
> However, I am afraid some of my concepts are still not clear enough.
>
> >From many readings, I have been told that Java volatile fields follow the
> Sequential Consistency (SC).
>
> In my learnt concept about SC (in software DSM), a read of a
> variable X at a
> processor is not guaranteed to return the most recent write by another
> processor to X. Otherwise, the model is already a strict
> consistency model.
> So it can happen that the read returns an old value in the local cache
> (heap) because the update is not yet propagated over the network.
>
> What SC really guarantees is some global order: that is, in a sequentially
> consistent world, suppose two processors perform two write operations on a
> variable x, a fourth processor could not see the two writes in a different
> order than what the third processor observed, i.e. below situation is not
> valid:
>
> P1:  W(x)1
> -----------------------
> P2:        R(x)1 R(x)2
> -----------------------
> P3:        R(x)2 R(x)1
> -----------------------
> P4:  W(x)2
>
> Please see below (the above e.g. is taken there):
> http://www.cs.nmsu.edu/~pfeiffer/classes/573/notes/consistency.html
>
>
> But when I go through some web posts like this:
> http://stackoverflow.com/questions/3603157/volatile-keyword-in-jav
> a-clarific
> ation
>
> I get confused at seeing below sayings:
> ------------------------------------------------------------------
> ----------
> volatile guarantees that reads from the variable always reflects
> the most up
> to update value. The runtime can achieve this in various ways,
> including not
> caching or refreshing the cache when the value has changed.
>
> Before Java 1.5 was released the volatile keyword declared that the field
> will get the most recent value of the object by hitting main memory each
> time for reads and flushing for writes.
>
> Today's volatile keyword says two very important things:
>
> 1. Don't worry about how but know that when reading a volatile field you
> will always have the most up to date value.
> 2. A compiler cannot reorder a volatile read/write as to maintain program
> order.
> ------------------------------------------------------------------
> ----------
>
> If the above sayings are totally correct, then what Java volatile
> semantics
> have ensured is in fact already strict consistency - much
> stronger than SC.
> And these can't be implemented efficiently in a distributed memory
> environment. May I ask if the above sayings are true?
>
> I think these are true for shared-memory multiprocessors because the JIT
> compiler inserts memory barriers/fences around volatile field
> accesses that
> invalidate and flush processor cache memory to main memory. Memory bus
> transactions would be serialized, so a read will return the most recent
> write on the main memory. (Is this correct?)
>
> But in a distributed shared memory, such barriers do not exist. Emulating
> such barriers via software means of message passing is an expensive
> operation that kills scalability.
>
> I believe the above guarantee is stricter than necessary. Indeed, if Java
> volatiles are following SC, then it should be valid to just ensure a
> consistent global memory order, but no need for a volatile read to return
> the most up-to-date value of a volatile write, which is still on
> the way of
> propagation. Am I correct? If yes, then a cluster-wide SC protocol can be
> implemented quite easily yet efficient enough, say with multicasting (See
> p.2 of http://deneb.cs.kent.edu/~mikhail/classes/aos.f03/l14dsm.PDF).
>
> The most important question is: would ConcurrentHashMap (CHM) rely on the
> property of "when reading a volatile field you will always have
> the most up
> to date value" in order to behave correctly? If CHM runs atop a software
> Java DSM implementing the SC model I mentioned, would it be running
> correctly?
>
> Note: put() and remove() operations in CHM are synchronized with locks. So
> if these write operations are made serialized cluster-wide w.r.t. the
> object's home node, I believe this has ensured that every thread will see
> the same sequence of the writes, so SC has been observed. Is this also
> right?
>
> Somehow, I notice that the new JMM could have complicated the matter, now
> the visibility guarantee of a volatile field access has been
> similar to that
> of a lock/unlock operation. So updates made before a volatile write happen
> before a volatile read and must be sent to the volatile field
> reading thread
> as well.
>
> Last, I am also astonished and confused at seeing this:
> "Volatile memory operations are not sequentially consistent."
> from this lecture note page:
> http://bluehawk.monmouth.edu/rclayton/web-pages/u03-598/netmemcon.html
>
> ------------------------------------------------------------------
> ----------
> Sequential Consistency and Volatile Memory
> - The sliding fence trick tells us one other thing.
>   - Volatile memory operations are not sequentially consistent.
>   - Two threads can view two volatile memory operations in
> different orders.
>
> - Java requires that thread-order be preserved for memory operations on
> volatile fields.
> ------------------------------------------------------------------
> ----------
>
> Indeed, I don't understand what the author means by "sliding fence trick".
> Anyway, I am quite sure Java volatile accesses must have observe
> SC or even
> strict consistency due to the way the JMM is implemented by most JVMs
> through memory fencing.
>
> Your comments are most appreciated.
> Thank you so much.
>
> Best regards,
> Tin
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ktlam at cs.hku.hk  Fri Mar 25 07:47:29 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Fri, 25 Mar 2011 19:47:29 +0800
Subject: [concurrency-interest] ConcurrentHashMap, volatile field access,
	sequential consistency in distributed shared memory
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBNIMAA.davidcholmes@aapt.net.au>
References: <00bb01cbea9c$c7695d40$f8b10893@ktlampc>
	<NFBBKALFDCPFIDBNKAPCKEBNIMAA.davidcholmes@aapt.net.au>
Message-ID: <00d601cbeae2$6f1e5b40$f8b10893@ktlampc>

David,

Thank you so much for your suggested JMM discussion list and the prompt
replies to my questions. I have just posted my questions there.

I totally agree with your answers.

Except one thing I still don't understand:

> As long as the happens-before ordering is preserved then a thread can not
tell whether a read of X that yields 0 occurred just before or just after a
write to X that set 1.


In my concept, if HB ordering is already preserved, then the thread should
be able to tell whether a read of X that yields 0 occurred just before or
just after a write to X that set 1.

For example, x = y = 0 initially,

P1:  W(y)2 W(x)1
---------------------------------
P2:             R(x)0 R(x)1 R(y)2

Suppose the above R/W operations on x are volatile field accesses while
accesses to y are not.

Based on the new JMM semantics, R(y) can surely return 2 because of volatile
W(x) happened before volatile R(x). So if P2 can see x=1, then it must also
see y=2. This is what we know.
 
For the R(x) that returns 0, it is valid in SC. Here, yes, we can't tell
which of W(x) on P1 and R(x) on P2 happens first. 

But the R(x) that returns 1, we observe a flip of x from 0 to 1. This
implies there was a write on x happened before this read, this updated value
has just been received before x is being read. So here, we can tell W(x) on
P1 happens before R(x) on P2.  

Am I correct?

Thanks a lot again. 

Cheers,
Tin
 

> -----Original Message-----
> From: David Holmes [mailto:davidcholmes at aapt.net.au]
> Sent: Friday, March 25, 2011 12:21 PM
> To: King Tin Lam; concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] ConcurrentHashMap, volatile field
> access,sequential consistency in distributed shared memory
> 
> Tin,
> 
> To get a thorough and accurate response to this I would suggest posting on
> the JMM discussion list instead:
> 
> https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion
> 
> At the risk of being inaccurate I'll make a couple of responses to some of
> your questions.
> 
> First, yes the JIT and runtime introduce whatever barriers/fences are
> needed
> to enforce the Java Memory Model semantics.
> 
> Second, it is not required that a read of a volatile variable must see the
> most recent write - as you say that write might still be awaiting
> propogation, for example sitting in a store buffer. It is only required
> that
> the happens-before orderings are preserved. Without a global observer the
> only way a thread can detect that a read returned an earlier value is by
> examining a second variable that indicates what the original should be. As
> long as the happens-before ordering is preserved then a thread can not
> tell
> whether a read of X that yields 0 occurred just before or just after a
> write
> to X that set 1.
> 
> Third, CHM relies on the happens-before properties that volatiles provide
> -
> no more, no less.
> 
> Cheers,
> David Holmes
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of King Tin
> > Lam
> > Sent: Friday, 25 March 2011 1:29 PM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] ConcurrentHashMap, volatile field
> > access,sequential consistency in distributed shared memory
> >
> >
> > Hi all,
> >
> > I have some questions about transparent support of ConcurrentHashMap
> (CHM)
> > or other concurrent utility classes in a distributed runtime
environment.
> > Hope you would find it interesting to discuss on this topic.
> >
> > Specifically, it's a software distributed shared memory (DSM) - a set of
> > interconnected JVMs on a cluster of computing nodes - which
> > implements some
> > memory consistency models like Home-based Lazy Release Consistency
(HLRC)
> > and Sequential Consistency (SC) at the heap level.
> >
> > In such a system, threads are distributed; no multiprocessor shared
> memory
> > and memory fences. So the Java Memory Model (JMM) has to be fulfilled by
> > extra message passing operations; the semantics of locks and volatiles
> are
> > extended through modifying the JVM to guarantee "cluster-wide
> > consistency".
> >
> > However, I am afraid some of my concepts are still not clear enough.
> >
> > >From many readings, I have been told that Java volatile fields follow
> the
> > Sequential Consistency (SC).
> >
> > In my learnt concept about SC (in software DSM), a read of a
> > variable X at a
> > processor is not guaranteed to return the most recent write by another
> > processor to X. Otherwise, the model is already a strict
> > consistency model.
> > So it can happen that the read returns an old value in the local cache
> > (heap) because the update is not yet propagated over the network.
> >
> > What SC really guarantees is some global order: that is, in a
> sequentially
> > consistent world, suppose two processors perform two write operations on
> a
> > variable x, a fourth processor could not see the two writes in a
> different
> > order than what the third processor observed, i.e. below situation is
> not
> > valid:
> >
> > P1:  W(x)1
> > -----------------------
> > P2:        R(x)1 R(x)2
> > -----------------------
> > P3:        R(x)2 R(x)1
> > -----------------------
> > P4:  W(x)2
> >
> > Please see below (the above e.g. is taken there):
> > http://www.cs.nmsu.edu/~pfeiffer/classes/573/notes/consistency.html
> >
> >
> > But when I go through some web posts like this:
> > http://stackoverflow.com/questions/3603157/volatile-keyword-in-jav
> > a-clarific
> > ation
> >
> > I get confused at seeing below sayings:
> > ------------------------------------------------------------------
> > ----------
> > volatile guarantees that reads from the variable always reflects
> > the most up
> > to update value. The runtime can achieve this in various ways,
> > including not
> > caching or refreshing the cache when the value has changed.
> >
> > Before Java 1.5 was released the volatile keyword declared that the
> field
> > will get the most recent value of the object by hitting main memory each
> > time for reads and flushing for writes.
> >
> > Today's volatile keyword says two very important things:
> >
> > 1. Don't worry about how but know that when reading a volatile field you
> > will always have the most up to date value.
> > 2. A compiler cannot reorder a volatile read/write as to maintain
> program
> > order.
> > ------------------------------------------------------------------
> > ----------
> >
> > If the above sayings are totally correct, then what Java volatile
> > semantics
> > have ensured is in fact already strict consistency - much
> > stronger than SC.
> > And these can't be implemented efficiently in a distributed memory
> > environment. May I ask if the above sayings are true?
> >
> > I think these are true for shared-memory multiprocessors because the JIT
> > compiler inserts memory barriers/fences around volatile field
> > accesses that
> > invalidate and flush processor cache memory to main memory. Memory bus
> > transactions would be serialized, so a read will return the most recent
> > write on the main memory. (Is this correct?)
> >
> > But in a distributed shared memory, such barriers do not exist.
> Emulating
> > such barriers via software means of message passing is an expensive
> > operation that kills scalability.
> >
> > I believe the above guarantee is stricter than necessary. Indeed, if
> Java
> > volatiles are following SC, then it should be valid to just ensure a
> > consistent global memory order, but no need for a volatile read to
> return
> > the most up-to-date value of a volatile write, which is still on
> > the way of
> > propagation. Am I correct? If yes, then a cluster-wide SC protocol can
> be
> > implemented quite easily yet efficient enough, say with multicasting
> (See
> > p.2 of http://deneb.cs.kent.edu/~mikhail/classes/aos.f03/l14dsm.PDF).
> >
> > The most important question is: would ConcurrentHashMap (CHM) rely on
> the
> > property of "when reading a volatile field you will always have
> > the most up
> > to date value" in order to behave correctly? If CHM runs atop a software
> > Java DSM implementing the SC model I mentioned, would it be running
> > correctly?
> >
> > Note: put() and remove() operations in CHM are synchronized with locks.
> So
> > if these write operations are made serialized cluster-wide w.r.t. the
> > object's home node, I believe this has ensured that every thread will
> see
> > the same sequence of the writes, so SC has been observed. Is this also
> > right?
> >
> > Somehow, I notice that the new JMM could have complicated the matter,
> now
> > the visibility guarantee of a volatile field access has been
> > similar to that
> > of a lock/unlock operation. So updates made before a volatile write
> happen
> > before a volatile read and must be sent to the volatile field
> > reading thread
> > as well.
> >
> > Last, I am also astonished and confused at seeing this:
> > "Volatile memory operations are not sequentially consistent."
> > from this lecture note page:
> > http://bluehawk.monmouth.edu/rclayton/web-pages/u03-598/netmemcon.html
> >
> > ------------------------------------------------------------------
> > ----------
> > Sequential Consistency and Volatile Memory
> > - The sliding fence trick tells us one other thing.
> >   - Volatile memory operations are not sequentially consistent.
> >   - Two threads can view two volatile memory operations in
> > different orders.
> >
> > - Java requires that thread-order be preserved for memory operations on
> > volatile fields.
> > ------------------------------------------------------------------
> > ----------
> >
> > Indeed, I don't understand what the author means by "sliding fence
> trick".
> > Anyway, I am quite sure Java volatile accesses must have observe
> > SC or even
> > strict consistency due to the way the JMM is implemented by most JVMs
> > through memory fencing.
> >
> > Your comments are most appreciated.
> > Thank you so much.
> >
> > Best regards,
> > Tin
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >


From mettafort at gmail.com  Fri Mar 25 09:06:11 2011
From: mettafort at gmail.com (Insane Membrane)
Date: Fri, 25 Mar 2011 08:06:11 -0500
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <146577.80697.qm@web38807.mail.mud.yahoo.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
	<337855.64253.qm@web38804.mail.mud.yahoo.com>
	<AANLkTin1=GhaMqBQRni-CLb2_igSs1LiHrstJjw6KwQx@mail.gmail.com>
	<146577.80697.qm@web38807.mail.mud.yahoo.com>
Message-ID: <AANLkTimLLptDGjJ07=ueWZ82jNaEUUvHAfPsjU_WE17n@mail.gmail.com>

Thanks again Ben. I take your point and would agree.  I guess my motives
behind this whole thread is more to try to learn more about the idioms and
patterns that seem to unfold from overlapping problem sets; the
understanding being that a pattern must always be shaped to the individual
problem at hand.

In the designs that I have built, the approach you mention of recording the
operations to be later applied, is essentially what I have done.  The
difference, perhaps, is that a single thread is the writer, and a key aspect
of the writer view of the "data model" is that no matter what happens on the
reader side, the writer has the very latest view of the model.  This is
essential, because one update could apply new values to some object, which
subsequently cause further mutations within the model.  All mutations for
this write event must be memento'd some how, such that when the writer comes
in again, before the reader still, it is looking at the very latest because
a new update might be affected by the secondary mutations of the previous
write.

On the reader side, it might be such that it requires seeing each change
event as such events, perhaps when keeping some UI state machine, which I
have seen.  In this fashion, it cannot simply drop the individual change
event deltas that the writer has created and just apply the last item as the
current and final state.  However there are also scenarios were this is
acceptable, such as when handling top of book market data events.

The similarity here is indeed the fact that the above is essentially
journal-ling the mutations into some form of change batch, applied writer
side for up-to-date state on the writer side, and a catch up on the reader
side, at its own convenience.

Thanks for your time on this subject Ben, much appreciated.  I will check
out the link you passed over.

On Thu, Mar 24, 2011 at 10:49 PM, Ben Manes <ben_manes at yahoo.com> wrote:

> I can't offer a satisfactory answer, because each problem is best served by
> a different technique. For user interfaces, I try to follow the BASE [1]
> performance model to minimize the user-perceived latencies and design the
> data model to allow write conflicts to be resolved asynchronously. In the
> data structure example, a portion is inherently single-threaded but a strict
> consistency isn't required. Instead the operations can be recorded and
> replayed later to catch-up with the expected state when necessary (e.g.
> cache eviction). The bottleneck wasn't the execution of each operation but
> rather the thrashing of the exclusive lock, so buffering / batching the work
> under the lock enabled better concurrency behavior. The usage of the lock is
> primarily to maintain the much simpler sequential programming model. The
> penalty of applying those operations is kept small by amortizing it, such as
> performing only 64 of those tasks. When a consistent read is required then
> only a few operations should be pending, so there shouldn't be a significant
> penalty. Perhaps the simplest way to think of this technique is in terms of
> a journaled file-system, except that transactions being replayed are on a
> custom data structure.
>
> In a general purpose object model you tend to lose the ability to make
> design assumptions since there is too much flexibility. Instead of focusing
> on concurrency the practical solutions usually involve some form of
> versioning (e.g. transactions), the scoping of data to allow parallelism,
> and reads from a slightly stale snapshot.
>
> [1] http://www.addsimplicity.com/downloads/ArchitectingForLatency.pdf
>
> ------------------------------
> *From:* Insane Membrane <mettafort at gmail.com>
> *To:* Ben Manes <ben_manes at yahoo.com>
> *Cc:* Carfield Yim <carfield at carfield.com.hk>; "
> concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
> *Sent:* Thu, March 24, 2011 7:14:42 PM
> *Subject:* Re: High Volume Scenarios and Concurrency
>
> Ben,
>
> This is very interesting.  I am currently reading all of the PDFs from
> the web link you provided. I need to digest the information, but I am
> curious to know how, IF, this would work with an object model.  Assume
> an Order and it has a list of OrderItem objects, for each line item.
> As order fulfillment takes place, the remaining items to be completed
> for the line item is updated.  So a line item may include multiple
> things (contrived, stick with me!) concurrently the fulfillment engine
> finds the line item and indicates how many items have been filled.
> The order needs to hold return the number of items
> complete/incomplete.  Let's also say the order can have hundreds of
> line items.
>
> The UI could be looking at say 200 line items, and there is a total of
> completed items at the top. The UI periodically refreshes, every
> 2seconds say.  For concurrency, it would need to lock the whole order
> before updating the visuals of all the line items and the total.  If
> the UI needs to wait for the lock, say due to a large volume of
> updates on the fulfillment side, the UI is blocked.  It could do a
> tryLock, but let's assume it doesn't.
>
> The other solution is to copy on the fulfillment side and when the UI
> wants the new version, it takes the some lock to allow it to process
> those items.  We have copies of the order, but we also have copies of
> line items that were modified during the 2 sec period.  Even with one
> writer, if the desire is to be lock free on the UI side, we want a cas
> style pointer swap on the UI side to get the view of the changes, but
> this is still a problem when we have updates being applied to two
> different objects.  There's solutions but they become somewhat complex
> as the object model associations increase.
>
> Even with this simple contrived problem I am interested to see if the
> approach you presented Ben will be a workable solution.  Thank you for
> sharing.
>
> On Thursday, March 24, 2011, Ben Manes <ben_manes at yahoo.com> wrote:
> > It is often acceptable to weaken the consistency guarantee to improve the
> write performance. If writes are more frequent then reads, then a write-back
> buffer may be used to batch the updates operations under the lock. A writer
> no longer suffers the penalty of acquiring the lock since their work is
> scheduled on a queue, which is a much cheaper operation. When a consistent
> read is required, the thread can take the penalty of catching up the
> object's state by applying the pending operations. The draining of the
> buffer(s) can be performed on either a dedicated thread or amortized on
> user-threads, the latter by using a tryLock so that concurrent operations
> just append to the buffer without blocking. If applicable, a fuller
> description of this approach in a real-world example is available
> >  here [1].
> >
> > [1] http://code.google.com/p/concurrentlinkedhashmap/wiki/Design
> > From: Carfield Yim <carfield at carfield.com.hk>
> > To: Insane Membrane <mettafort at gmail.com>
> > Cc: concurrency-interest at cs.oswego.edu
> > Sent: Thu, March 24, 2011 9:31:01 AM
> > Subject: Re: [concurrency-interest] High Volume Scenarios and Concurrency
> >
> > I think update immutability have to clone the
> >  order. You may consider
> > model the amendment as change set ( for me I usually name it as delta
> > object ) and maintain in a collection, maybe a list.
> >
> > Although just that list maybe a mutable list, or maybe
> > copyonwritelist, but it likely save you from a lot of GC and enjoy
> > most benefit of immutable object
> >
> > On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com>
> wrote:
> >> Hi,
> >> I am writing with the hope of finding some helpful advice on where to
> look
> >> deeper into the theory and practice of building concurrent, object
> models
> >> that involve large numbers of updates.  In front office finance, there
> is
> >> the need to handle multiple data feeds, and then apply them to some
> object
> >> model which is to be ultimately rendered in a trading UI.
> >> A common approach is to use immutability, but that often
> >  involves copying
> >> objects and then applying an update, or some other mechanism....I am
> curious
> >> to find out other approaches.  Copying at the data feed/communications
> level
> >> can result in large numbers of objects being created, causing a GC
> issue.  I
> >> have built designs that allow object state to be stored in arrays, which
> >> allows the copying to avoid creating new objects, but these designs can
> be
> >> complicated, possibly prematurely.
> >> Could anyone offer advice/papers/concepts on this topic? I have
> Concurrency
> >> in Practice and Concurrent Programming in Java on my desk now, but
> looking
> >> for other sources if there are any.
> >> Many thanks,
> >> MF.
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110325/a761bfac/attachment.html>

From ben_manes at yahoo.com  Sat Mar 26 04:52:55 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Sat, 26 Mar 2011 01:52:55 -0700 (PDT)
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
Message-ID: <471238.66909.qm@web38802.mail.mud.yahoo.com>

I suspect the closest papers of interest, in regards to your scenario, are the ones on "flat combining". This is a blocking form of lock amortization, whereas I try to design around non-blocking solutions when possible. This technique might help scale an STM, which are traditionally governed by locks. The amortization approach would avoid lock thrashing when attempting to commit a transaction. That said, I am not up to speed on the latest STM implementations.

On Fri Mar 25th, 2011 6:06 AM PDT Insane Membrane wrote:

>Thanks again Ben. I take your point and would agree.  I guess my motives
>behind this whole thread is more to try to learn more about the idioms and
>patterns that seem to unfold from overlapping problem sets; the
>understanding being that a pattern must always be shaped to the individual
>problem at hand.
>
>In the designs that I have built, the approach you mention of recording the
>operations to be later applied, is essentially what I have done.  The
>difference, perhaps, is that a single thread is the writer, and a key aspect
>of the writer view of the "data model" is that no matter what happens on the
>reader side, the writer has the very latest view of the model.  This is
>essential, because one update could apply new values to some object, which
>subsequently cause further mutations within the model.  All mutations for
>this write event must be memento'd some how, such that when the writer comes
>in again, before the reader still, it is looking at the very latest because
>a new update might be affected by the secondary mutations of the previous
>write.
>
>On the reader side, it might be such that it requires seeing each change
>event as such events, perhaps when keeping some UI state machine, which I
>have seen.  In this fashion, it cannot simply drop the individual change
>event deltas that the writer has created and just apply the last item as the
>current and final state.  However there are also scenarios were this is
>acceptable, such as when handling top of book market data events.
>
>The similarity here is indeed the fact that the above is essentially
>journal-ling the mutations into some form of change batch, applied writer
>side for up-to-date state on the writer side, and a catch up on the reader
>side, at its own convenience.
>
>Thanks for your time on this subject Ben, much appreciated.  I will check
>out the link you passed over.
>
>On Thu, Mar 24, 2011 at 10:49 PM, Ben Manes <ben_manes at yahoo.com> wrote:
>
>> I can't offer a satisfactory answer, because each problem is best served by
>> a different technique. For user interfaces, I try to follow the BASE [1]
>> performance model to minimize the user-perceived latencies and design the
>> data model to allow write conflicts to be resolved asynchronously. In the
>> data structure example, a portion is inherently single-threaded but a strict
>> consistency isn't required. Instead the operations can be recorded and
>> replayed later to catch-up with the expected state when necessary (e.g.
>> cache eviction). The bottleneck wasn't the execution of each operation but
>> rather the thrashing of the exclusive lock, so buffering / batching the work
>> under the lock enabled better concurrency behavior. The usage of the lock is
>> primarily to maintain the much simpler sequential programming model. The
>> penalty of applying those operations is kept small by amortizing it, such as
>> performing only 64 of those tasks. When a consistent read is required then
>> only a few operations should be pending, so there shouldn't be a significant
>> penalty. Perhaps the simplest way to think of this technique is in terms of
>> a journaled file-system, except that transactions being replayed are on a
>> custom data structure.
>>
>> In a general purpose object model you tend to lose the ability to make
>> design assumptions since there is too much flexibility. Instead of focusing
>> on concurrency the practical solutions usually involve some form of
>> versioning (e.g. transactions), the scoping of data to allow parallelism,
>> and reads from a slightly stale snapshot.
>>
>> [1] http://www.addsimplicity.com/downloads/ArchitectingForLatency.pdf
>>
>> ------------------------------
>> *From:* Insane Membrane <mettafort at gmail.com>
>> *To:* Ben Manes <ben_manes at yahoo.com>
>> *Cc:* Carfield Yim <carfield at carfield.com.hk>; "
>> concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
>> *Sent:* Thu, March 24, 2011 7:14:42 PM
>> *Subject:* Re: High Volume Scenarios and Concurrency
>>
>> Ben,
>>
>> This is very interesting.  I am currently reading all of the PDFs from
>> the web link you provided. I need to digest the information, but I am
>> curious to know how, IF, this would work with an object model.  Assume
>> an Order and it has a list of OrderItem objects, for each line item.
>> As order fulfillment takes place, the remaining items to be completed
>> for the line item is updated.  So a line item may include multiple
>> things (contrived, stick with me!) concurrently the fulfillment engine
>> finds the line item and indicates how many items have been filled.
>> The order needs to hold return the number of items
>> complete/incomplete.  Let's also say the order can have hundreds of
>> line items.
>>
>> The UI could be looking at say 200 line items, and there is a total of
>> completed items at the top. The UI periodically refreshes, every
>> 2seconds say.  For concurrency, it would need to lock the whole order
>> before updating the visuals of all the line items and the total.  If
>> the UI needs to wait for the lock, say due to a large volume of
>> updates on the fulfillment side, the UI is blocked.  It could do a
>> tryLock, but let's assume it doesn't.
>>
>> The other solution is to copy on the fulfillment side and when the UI
>> wants the new version, it takes the some lock to allow it to process
>> those items.  We have copies of the order, but we also have copies of
>> line items that were modified during the 2 sec period.  Even with one
>> writer, if the desire is to be lock free on the UI side, we want a cas
>> style pointer swap on the UI side to get the view of the changes, but
>> this is still a problem when we have updates being applied to two
>> different objects.  There's solutions but they become somewhat complex
>> as the object model associations increase.
>>
>> Even with this simple contrived problem I am interested to see if the
>> approach you presented Ben will be a workable solution.  Thank you for
>> sharing.
>>
>> On Thursday, March 24, 2011, Ben Manes <ben_manes at yahoo.com> wrote:
>> > It is often acceptable to weaken the consistency guarantee to improve the
>> write performance. If writes are more frequent then reads, then a write-back
>> buffer may be used to batch the updates operations under the lock. A writer
>> no longer suffers the penalty of acquiring the lock since their work is
>> scheduled on a queue, which is a much cheaper operation. When a consistent
>> read is required, the thread can take the penalty of catching up the
>> object's state by applying the pending operations. The draining of the
>> buffer(s) can be performed on either a dedicated thread or amortized on
>> user-threads, the latter by using a tryLock so that concurrent operations
>> just append to the buffer without blocking. If applicable, a fuller
>> description of this approach in a real-world example is available
>> >  here [1].
>> >
>> > [1] http://code.google.com/p/concurrentlinkedhashmap/wiki/Design
>> > From: Carfield Yim <carfield at carfield.com.hk>
>> > To: Insane Membrane <mettafort at gmail.com>
>> > Cc: concurrency-interest at cs.oswego.edu
>> > Sent: Thu, March 24, 2011 9:31:01 AM
>> > Subject: Re: [concurrency-interest] High Volume Scenarios and Concurrency
>> >
>> > I think update immutability have to clone the
>> >  order. You may consider
>> > model the amendment as change set ( for me I usually name it as delta
>> > object ) and maintain in a collection, maybe a list.
>> >
>> > Although just that list maybe a mutable list, or maybe
>> > copyonwritelist, but it likely save you from a lot of GC and enjoy
>> > most benefit of immutable object
>> >
>> > On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com>
>> wrote:
>> >> Hi,
>> >> I am writing with the hope of finding some helpful advice on where to
>> look
>> >> deeper into the theory and practice of building concurrent, object
>> models
>> >> that involve large numbers of updates.  In front office finance, there
>> is
>> >> the need to handle multiple data feeds, and then apply them to some
>> object
>> >> model which is to be ultimately rendered in a trading UI.
>> >> A common approach is to use immutability, but that often
>> >  involves copying
>> >> objects and then applying an update, or some other mechanism....I am
>> curious
>> >> to find out other approaches.  Copying at the data feed/communications
>> level
>> >> can result in large numbers of objects being created, causing a GC
>> issue.  I
>> >> have built designs that allow object state to be stored in arrays, which
>> >> allows the copying to avoid creating new objects, but these designs can
>> be
>> >> complicated, possibly prematurely.
>> >> Could anyone offer advice/papers/concepts on this topic? I have
>> Concurrency
>> >> in Practice and Concurrent Programming in Java on my desk now, but
>> looking
>> >> for other sources if there are any.
>> >> Many thanks,
>> >> MF.
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> >
>> >
>>
>>



      

From carfield at carfield.com.hk  Sun Mar 27 13:13:47 2011
From: carfield at carfield.com.hk (Carfield Yim)
Date: Mon, 28 Mar 2011 01:13:47 +0800
Subject: [concurrency-interest] High Volume Scenarios and Concurrency
In-Reply-To: <AANLkTin5qbbswUDc+3S6ED6eHPW98ARbze2p3eOAguPK@mail.gmail.com>
References: <AANLkTi=8jrZ=8_sM14KXpJT2i_183=YO8468ZW-tgdZT@mail.gmail.com>
	<AANLkTi=3ZJKNNJCQEb910MFpM4zaRFK0-BUc_y5N=0VF@mail.gmail.com>
	<AANLkTimMunfo8LOGtQFivGKAQFiMbkDz3k8DxKau+p2c@mail.gmail.com>
	<AANLkTinbGqUE3S1XnxwX4k6Z66_7QQLs7xHYWeoq_jja@mail.gmail.com>
	<AANLkTikeCvDOPiiue14oRx+2QHwj7zfOj1-jvDDSZSG7@mail.gmail.com>
	<AANLkTin5qbbswUDc+3S6ED6eHPW98ARbze2p3eOAguPK@mail.gmail.com>
Message-ID: <AANLkTikj15oDs5Lh7wA_EOJef++u3A+7UX4RYBC8T-PD@mail.gmail.com>

HI Insane, I am working on a finance system which using FIX as
communication protocol (
http://fixwiki.fixprotocol.org/fixwiki/FIXwiki ) , and I just try to
model the object according to the specification which easier for me to
handle the complication for various corner case. Here is the code
snippet about the idea, you may take a look and suggest if there any
problem of doing that.

For fields those are mandatory for each amendment, we can just get
from the latest amendment for the value and it should be quick; there
is some field that may not exist for each amend and I need to loop
through the amendment to get the values, which may cost performance
problem of reading those fields, but that only rarely operations.


import java.util.Iterator;
import java.util.List;
import java.util.concurrent.CopyOnWriteArrayList;

public class Order {

	private final Order.NewOrder newOrder;
	private final List<Order.Delta> amendments = new
CopyOnWriteArrayList<Order.Delta>();
	private final List<Order.Delta> pendingAmendments = new
CopyOnWriteArrayList<Order.Delta>();

	private Order(NewOrder newOrder) {
		this.newOrder = newOrder;
	}

	public static Order create(String fixMsg) {
		return new Order(new Order.NewOrder(fixMsg));
	}

	public void amendRequest(String fixMsg) {
		pendingAmendments.add(new Delta(fixMsg));
	}

	public void executionReport(String fixMsg) {
		Delta delta = new Delta(fixMsg);
		amendments.add(delta);
		for (Iterator<Delta> iter = pendingAmendments.iterator(); iter.hasNext();) {
			Delta thisDelta = iter.next();
			if(thisDelta.clorderid.equals(delta.clorderid))
				iter.remove();
		}
	}

	static class Delta {
		private final String clorderid;
		private final double price;
		private final OrderType orderType;
		private final TimeInForce tif;
		private final double orderQty;
		private final double cumQty;
		private final double leaveQty;
		private final double avgPx;
		private final double leavePx;

		private Delta(String fixMsg) {
			// parse message to order Delta
			if(orderQty != cumQty + leaveQty) {
				throw new IllegalStateException("orderQty != cumQty + leaveQty ,
plz check");
			}
		}
	}

	static class NewOrder {
		private final String orderid;
		private final String clorderid;
		private final double price;
		private final OrderType orderType;
		private final TimeInForce tif;
		private final double orderQty;
		private final String comment;

		private NewOrder(String fixMsg) {
			// parse message to create order
		}
	}

	public String getOrderid() {
		return newOrder.orderid;
	}

	public String getComment() {
		return newOrder.comment;
	}

	public String getOrdClorderid() {
		if (amendments.size() == 0) {
			return null;
		} else if (amendments.size() == 1) {
			return newOrder.clorderid;
		} else {
			return amendments.get(amendments.size() - 2).clorderid;
		}
	}

	public String getClorderid() {
		if (amendments.size() == 0) {
			return newOrder.clorderid;
		} else {
			return amendments.get(amendments.size() - 1).clorderid;
		}
	}

	public double getOpenQty() {
		return getOrderQty() - getFilledQty();
	}

	public double getFilledQty() {
		if (amendments.size() > 0)
			return amendments.get(amendments.size() - 1).cumQty;
		else
			return 0;
	}

	public double getOrderQty() {
		if (amendments.size() > 0)
			return amendments.get(amendments.size() - 1).orderQty;
		else
			return newOrder.orderQty;
	}

	public OrderType getOrderType() {
		if (amendments.size() > 0)
			for (int i = amendments.size() - 1; i >= 0; i--) {
				if (amendments.get(i).orderType != null) {
					return amendments.get(i).orderType;
				}
			}
		return newOrder.orderType;
	}

	// Similar to other attibutes
}


On Fri, Mar 25, 2011 at 10:00 AM, Insane Membrane <mettafort at gmail.com> wrote:
> Reposting for the benefit of the group...my mailer just did a direct
> reply to the author...
>
> On Thursday, March 24, 2011, Insane Membrane <mettafort at gmail.com> wrote:
>> I am not exactly sure how this would work, the parts aren't coming
>> together in my mind. ?Could you explain a bit further?
>>
>> I have implemented something in the past were I track what fields are
>> modified, maintaining the values that are applied. ?When a subsequent
>> update comes in on the writer side, it uses the latest values in the
>> change set, makes modifications, and makes the new update ready for
>> the reader. ?The reader then pulls over and applies the changes, in
>> essence, to see the latest version of the object. ?However this works
>> really only with simple objects, though I suspect it could work with
>> more canonical object model designs. ?The other issue is it only
>> supports one writer and one reader, or multiple readers (reader is a
>> thread) but requires tricky synchronization.
>>
>> Interested to know more about your approach, if you would be kind
>> enough to explain in more detail?
>>
>> On Thursday, March 24, 2011, Carfield Yim <carfield at carfield.com.hk> wrote:
>>> There will be no clone of the original data, the list contain original
>>> data, then with series of delta object.
>>>
>>> When system read the attributes, it do something like
>>>
>>> String getName() {
>>> ?? for ( int i = list.size() - ?1 ; i >= 0 ; i-- ) {
>>> ?? ? ?if(data[i].name().exist) return data[i].name().get();
>>> ?? }
>>> }
>>> On Fri, Mar 25, 2011 at 3:49 AM, Insane Membrane <mettafort at gmail.com> wrote:
>>>> How do you avoid impacting the GC? Arent you still creating that delta
>>>> object, a clone of the original order? And then what happens if more changes
>>>> come through? Do you simply apply each individual change event to a new
>>>> clone of the previous? This would result in many objects again wouldnt it?
>>>> Or maybe I am misunderstanding something.
>>>> And then the question is, what if Order has off of it a non-trivial object
>>>> model? Say a list of objects, each in turn containing other lists for other
>>>> data. ?When the object model from a root is complex, is the whole model
>>>> copied?
>>>> Thanks for your response.
>>>> On Thu, Mar 24, 2011 at 11:31 AM, Carfield Yim <carfield at carfield.com.hk>
>>>> wrote:
>>>>>
>>>>> I think update immutability have to clone the order. You may consider
>>>>> model the amendment as change set ( for me I usually name it as delta
>>>>> object ) and maintain in a collection, maybe a list.
>>>>>
>>>>> Although just that list maybe a mutable list, or maybe
>>>>> copyonwritelist, but it likely save you from a lot of GC and enjoy
>>>>> most benefit of immutable object
>>>>>
>>>>> On Thu, Mar 24, 2011 at 8:50 PM, Insane Membrane <mettafort at gmail.com>
>>>>> wrote:
>>>>> > Hi,
>>>>> > I am writing with the hope of finding some helpful advice on where to
>>>>> > look
>>>>> > deeper into the theory and practice of building concurrent, object
>>>>> > models
>>>>> > that involve large numbers of updates. ?In front office finance, there
>>>>> > is
>>>>> > the need to handle multiple data feeds, and then apply them to some
>>>>> > object
>>>>> > model which is to be ultimately rendered in a trading UI.
>>>>> > A common approach is to use immutability, but that often involves
>>>>> > copying
>>>>> > objects and then applying an update, or some other mechanism....I am
>>>>> > curious
>>>>> > to find out other approaches. ?Copying at the data feed/communications
>>>>> > level
>>>>> > can result in large numbers of objects being created, causing a GC
>>>>> > issue. ?I
>>>>> > have built designs that allow object state to be stored in arrays, which
>>>>> > allows the copying to avoid creating new objects, but these designs can
>>>>> > be
>>>>> > complicated, possibly prematurely.
>>>>> > Could anyone offer advice/papers/concepts on this topic? I have
>>>>> > Concurrency
>>>>> > in Practice and Concurrent Programming in Java on my desk now, but
>>>>> > looking
>>>>> > for other sources if there are any.
>>>>> > Many thanks,
>>>>> > MF.
>>>>> > _______________________________________________
>>>>> > Concurrency-interest mailing list
>>>>> > Concurrency-interest at cs.oswego.edu
>>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> >
>>>>> >
>>>>
>>>>
>>>
>>
>


From lukas at krecan.net  Mon Mar 28 06:04:22 2011
From: lukas at krecan.net (lukas at krecan.net)
Date: Mon, 28 Mar 2011 12:04:22 +0200
Subject: [concurrency-interest] Fork/Join visualization
Message-ID: <b07e39ee52b3069757671a08bc497455@krecan.net>



Hi, 

 on GPars Hackathon we have created a Fork/Join visualization.
It thought you might find it interesting
http://blog.krecan.net/2011/03/27/visualizing-forkjoin/ 

 Cheers 


Lukas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110328/08282abf/attachment.html>

From rmcilroy at microsoft.com  Wed Mar 30 07:59:31 2011
From: rmcilroy at microsoft.com (Ross McIlroy)
Date: Wed, 30 Mar 2011 11:59:31 +0000
Subject: [concurrency-interest] [SFMA'11] Workshop on Systems for Future
	Multi-Core Architectures
Message-ID: <5109AB538DE99B41A5C31A5882E1EE962832DC29@TK5EX14MBXC111.redmond.corp.microsoft.com>

[Apologies if you receive multiple copies of this call for participation]

*****************************************************************
We're pleased to announce the full programme for the SFMA multi-core workshop, and keynote
talk from Andrew Baumann on his experience building software for non-cache-coherent systems.
Full details below.
*****************************************************************

Please consider attending the following workshop we are holding at EuroSys
2011 in Salzburg.

=================================================================
                     Call for Participation

Workshop on Systems for Future Multi-Core Architectures (SFMA'11)

               April 10th 2011,  Salzburg, Austria
                   Co-located with EuroSys 2011<http://eurosys2011.cs.uni-salzburg.at/welcome.php>

          http://research.microsoft.com/~rmcilroy/SFMA/index.htm
=================================================================

The current trend towards multi-core computing is of significant
importance to practitioners of systems-level software, such as
operating systems, language runtimes and virtual machines. As the
layer between application software and the underlying hardware,
systems-level software must directly tackle the challenges of
multi-core hardware (e.g., scalability, concurrency control and
data-sharing costs), while providing appropriate abstractions to
higher-level software. Future hardware is likely to increase the
challenges encountered by systems software due to increasing
system diversity, core heterogeneity, complex memory hierarchies,
dynamic core failure, and non-cache coherent shared memory.
However, the abundance of parallelism and potential for core
specialization and inter-core message passing hardware also
provide a number of new opportunities for system software. The
current shift in hardware design provides an exciting opportunity
to radically rethink the design and implementation of systems-
level software.

The workshop on Systems for Future Multi-Core Architectures
(SFMA'11) brings together researchers in the operating systems,
language runtime and virtual machine communities to exchange
ideas and experiences on the challenges and opportunities
presented by future multi-core hardware.

This is a full day workshop, consisting of a keynote speech by
Andrew Baumann on "Non-cache-coherent systems: The Barrelfish
experience", two technical sessions, and a panel discussion
will with leading experts in the field on the challenges faced
by systems software due to future multi-core architectures.

The full program is available at:
http://research.microsoft.com/~rmcilroy/SFMA/index.htm

Program committee:
------------------
Ali-Reza Adl-Tabatabai (Intel)
Mats Brorsson (KTH Royal Institute of Technology, Sweden)
Juan Colmenares (University of California, Berkeley)
Stephan Diestelhorst (AMD)
Tim Harris (Microsoft Research Cambridge)
Hermann H?rtig (TU Dresden)
Ross McIlroy (Microsoft Research Cambridge)
Derek Murray (University of Cambridge)
Timothy Roscoe (ETH Z?rich)
Michael Scott (University of Rochester)
Jeremy Singer (University of Glasgow)
Joe Sventek (University of Glasgow)
Michael Swift (University of Wisconsin)
Ian Watson (University of Manchester)
Nickolai Zeldovich (MIT)




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110330/dd66202f/attachment.html>

