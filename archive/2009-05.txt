From csaah at cox.net  Sat May  2 12:21:43 2009
From: csaah at cox.net (Carol Saah)
Date: Sat, 02 May 2009 12:21:43 -0400
Subject: [concurrency-interest] Reordering clarification in DCL example
Message-ID: <C621EB57.1C4%csaah@cox.net>

Hello,

I understand from Brian Goetz that we should NOT use Double-Checked-Locking.

However, I am trying to understand exactly what can happen in the
incorrectly synchronized code that appears in the article, "Double-checked
locking: Clever, but broken."  I realize that
http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very old
and that some comments may be old; however, the reordering issue is not old.

The code in question is:

class SomeClass {
  private Resource resource = null;
  
  public Resource getResource() {
    if (resource == null) {
      synchronized (this) {
        if (resource == null)
          resource = new Resource();
      }
    }
    return resource;
  }
}

On page 4 of the article, "So what's broken about DCL?", I would like to get
a more precise description of the 1st scenario discussed in that section.
This is my understanding with questions.

The reordering discussed in the scenario happens in dynamic compilation and
would seem to happen once and apply to all threads for a class, correct?

And, does dynamic compilation happen just before JIT?

In the example, the compiler chooses to reorder the instructions of
getResource() to: allocate memory, assign reference to resource, call
constructor.  

But, in a thread, the as-if-sequential semantics hold, so do those
instructions happen AFTER the thread has done all of its work in the "local
memory."  

And, do the instructions apply to main memory only?

Here is the scenario in the example.

Thread A has invoked getResource() on a someClass object.

Thread A finds that resource is null and so Thread A invokes the constructor
for Resource and sets the resource.

Thread A still has not exited the synchronized block so no writes have been
made to main memory.

Thread A's state is in its "local memory" on the processor or cache in which
it is running.

But, before Thread B invokes, getResource(), these "reordered instructions"
happen for Thread A:  allocate memory, assign reference to resource

Now, Thread B invokes getResource().

Thread B finds the reference to resource that the "reordered" instructions
from Thread A put into main memory.

But, in the meantime, the processor executing the instructions for Thread A
still has not caused the constructor to execute in main memory so main
memory has a reference but nothing in it.

So, is this why Thread B is partially constructed?

Thanks.  

Carol Saah



From tim at peierls.net  Sat May  2 12:38:25 2009
From: tim at peierls.net (Tim Peierls)
Date: Sat, 2 May 2009 12:38:25 -0400
Subject: [concurrency-interest] Reordering clarification in DCL example
In-Reply-To: <C621EB57.1C4%csaah@cox.net>
References: <C621EB57.1C4%csaah@cox.net>
Message-ID: <63b4e4050905020938u5c4ba444vec35d2ef003a5d11@mail.gmail.com>

See more recent discussions in Java Concurrency in Practice, Section 16.2.4
and Effective Java, 2nd edition, Item 71.
The fixed version of the code is:

class SomeClass {
  private volatile Resource resource;

  public Resource getResource() {
    Resource tmp = resource;
    if (tmp == null) {
      synchronized (this) {
        tmp = resource;
        if (tmp == null)
          resource = tmp = new Resource();
      }
    }
    return tmp;
  }
}

--tim

On Sat, May 2, 2009 at 12:21 PM, Carol Saah <csaah at cox.net> wrote:

> Hello,
>
> I understand from Brian Goetz that we should NOT use
> Double-Checked-Locking.
>
> However, I am trying to understand exactly what can happen in the
> incorrectly synchronized code that appears in the article, "Double-checked
> locking: Clever, but broken."  I realize that
> http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very old
> and that some comments may be old; however, the reordering issue is not
> old.
>
> The code in question is:
>
> class SomeClass {
>  private Resource resource = null;
>
>  public Resource getResource() {
>    if (resource == null) {
>      synchronized (this) {
>        if (resource == null)
>          resource = new Resource();
>      }
>    }
>    return resource;
>  }
> }
>
> On page 4 of the article, "So what's broken about DCL?", I would like to
> get
> a more precise description of the 1st scenario discussed in that section.
> This is my understanding with questions.
>
> The reordering discussed in the scenario happens in dynamic compilation and
> would seem to happen once and apply to all threads for a class, correct?
>
> And, does dynamic compilation happen just before JIT?
>
> In the example, the compiler chooses to reorder the instructions of
> getResource() to: allocate memory, assign reference to resource, call
> constructor.
>
> But, in a thread, the as-if-sequential semantics hold, so do those
> instructions happen AFTER the thread has done all of its work in the "local
> memory."
>
> And, do the instructions apply to main memory only?
>
> Here is the scenario in the example.
>
> Thread A has invoked getResource() on a someClass object.
>
> Thread A finds that resource is null and so Thread A invokes the
> constructor
> for Resource and sets the resource.
>
> Thread A still has not exited the synchronized block so no writes have been
> made to main memory.
>
> Thread A's state is in its "local memory" on the processor or cache in
> which
> it is running.
>
> But, before Thread B invokes, getResource(), these "reordered instructions"
> happen for Thread A:  allocate memory, assign reference to resource
>
> Now, Thread B invokes getResource().
>
> Thread B finds the reference to resource that the "reordered" instructions
> from Thread A put into main memory.
>
> But, in the meantime, the processor executing the instructions for Thread A
> still has not caused the constructor to execute in main memory so main
> memory has a reference but nothing in it.
>
> So, is this why Thread B is partially constructed?
>
> Thanks.
>
> Carol Saah
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/8937e0cd/attachment.html>

From csaah at cox.net  Sat May  2 13:10:58 2009
From: csaah at cox.net (Carol Saah)
Date: Sat, 02 May 2009 13:10:58 -0400
Subject: [concurrency-interest] Reordering clarification in DCL example
In-Reply-To: <63b4e4050905020938u5c4ba444vec35d2ef003a5d11@mail.gmail.com>
Message-ID: <C621F6E2.1CA%csaah@cox.net>

Tim,

Thanks.  I?m not trying to find a way to fix DCL.  I am familiar with Item
71 in Effective Java.  And, section 16.2.4 of the Java Concurrency in
Practice does not answer my questions.

I?m trying to understand exactly what goes on under-the-covers when the
compiler reorders code in incorrectly synchronized code.

The example given in the old article does touch on what I am interesting in
understanding.

Carol

On 5/2/09 12:38 PM, "Tim Peierls" <tim at peierls.net> wrote:

> See more recent discussions in Java Concurrency in Practice, Section 16.2.4
> and Effective Java, 2nd edition, Item 71.
> 
> The fixed version of the code is:
> 
> class SomeClass {
> ??private volatile Resource resource;
> 
> ??public Resource getResource() {
> ?? ?Resource tmp = resource;
> ?? ?if (tmp == null) {
> ?? ? ?synchronized (this) {
> ?? ? ? ?tmp = resource;
> ?? ? ? ?if (tmp == null)
> ?? ? ? ? ?resource = tmp = new Resource();
> ?? ? ?}
> ?? ?}
> ?? ?return tmp;
> ??}
> }
> 
> --tim
> 
> On Sat, May 2, 2009 at 12:21 PM, Carol Saah <csaah at cox.net> wrote:
>> Hello,
>> 
>> I understand from Brian Goetz that we should NOT use Double-Checked-Locking.
>> 
>> However, I am trying to understand exactly what can happen in the
>> incorrectly synchronized code that appears in the article, "Double-checked
>> locking: Clever, but broken." ?I realize that
>> http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very old
>> and that some comments may be old; however, the reordering issue is not old.
>> 
>> The code in question is:
>> 
>> class SomeClass {
>>  ?private Resource resource = null;
>> 
>>  ?public Resource getResource() {
>>  ? ?if (resource == null) {
>>  ? ? ?synchronized (this) {
>>  ? ? ? ?if (resource == null)
>>  ? ? ? ? ?resource = new Resource();
>>  ? ? ?}
>>  ? ?}
>>  ? ?return resource;
>>  ?}
>> }
>> 
>> On page 4 of the article, "So what's broken about DCL?", I would like to get
>> a more precise description of the 1st scenario discussed in that section.
>> This is my understanding with questions.
>> 
>> The reordering discussed in the scenario happens in dynamic compilation and
>> would seem to happen once and apply to all threads for a class, correct?
>> 
>> And, does dynamic compilation happen just before JIT?
>> 
>> In the example, the compiler chooses to reorder the instructions of
>> getResource() to: allocate memory, assign reference to resource, call
>> constructor.
>> 
>> But, in a thread, the as-if-sequential semantics hold, so do those
>> instructions happen AFTER the thread has done all of its work in the "local
>> memory."
>> 
>> And, do the instructions apply to main memory only?
>> 
>> Here is the scenario in the example.
>> 
>> Thread A has invoked getResource() on a someClass object.
>> 
>> Thread A finds that resource is null and so Thread A invokes the constructor
>> for Resource and sets the resource.
>> 
>> Thread A still has not exited the synchronized block so no writes have been
>> made to main memory.
>> 
>> Thread A's state is in its "local memory" on the processor or cache in which
>> it is running.
>> 
>> But, before Thread B invokes, getResource(), these "reordered instructions"
>> happen for Thread A: ?allocate memory, assign reference to resource
>> 
>> Now, Thread B invokes getResource().
>> 
>> Thread B finds the reference to resource that the "reordered" instructions
>> from Thread A put into main memory.
>> 
>> But, in the meantime, the processor executing the instructions for Thread A
>> still has not caused the constructor to execute in main memory so main
>> memory has a reference but nothing in it.
>> 
>> So, is this why Thread B is partially constructed?
>> 
>> Thanks.
>> 
>> Carol Saah
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/2e031a2b/attachment.html>

From joe.bowbeer at gmail.com  Sat May  2 13:12:32 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 2 May 2009 10:12:32 -0700
Subject: [concurrency-interest] Reordering clarification in DCL example
In-Reply-To: <63b4e4050905020938u5c4ba444vec35d2ef003a5d11@mail.gmail.com>
References: <C621EB57.1C4%csaah@cox.net>
	<63b4e4050905020938u5c4ba444vec35d2ef003a5d11@mail.gmail.com>
Message-ID: <31f2a7bd0905021012w36e944beg3aaf2cf771482354@mail.gmail.com>

On Sat, May 2, 2009 at 9:38 AM, Tim Peierls wrote:

> See more recent discussions in Java Concurrency in Practice, Section 16.2.4
> and Effective Java, 2nd edition, Item 71.
> The fixed version of the code is:
>
> class SomeClass {
>   private volatile Resource resource;
>
>   public Resource getResource() {
>     Resource tmp = resource;
>     if (tmp == null) {
>       synchronized (this) {
>         tmp = resource;
>         if (tmp == null)
>           resource = tmp = new Resource();
>       }
>     }
>     return tmp;
>   }
> }
>
> --tim
>

A clarification of "fixed" that I think Tim would agree with:

"Fixed" is used in the sense that if you have to have double-check locking,
presumably for performance reasons, then this is one of the very few ways to
code it correctly.

On the other hand, this "fix" adds even more surface area to the code, and
more modifiers for maintainers to contemplate ("volatile"), so as with all
optimizations, it is best avoid going this route unless you really need it.

Compiler: In JMM and DCL discussions, the "compiler" refers to javac plus
JIT plus anything else that reorders or optimizes at the instruction level.
In addition to the compiler(s), the processors and memory system (and
anything that might be rigged to run Java apps), can reorder operations --
but only to the extent allowed by the JMM.

As you surmise, the danger here is that thread B will see a non-null
reference to an incompletely constructed resource.

Joe

PS - The JMM FAQ could use another update on this topic:

http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#dcl


On Sat, May 2, 2009 at 12:21 PM, Carol Saah wrote:
>
>> Hello,
>>
>> I understand from Brian Goetz that we should NOT use
>> Double-Checked-Locking.
>>
>> However, I am trying to understand exactly what can happen in the
>> incorrectly synchronized code that appears in the article, "Double-checked
>> locking: Clever, but broken."  I realize that
>> http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very
>> old
>> and that some comments may be old; however, the reordering issue is not
>> old.
>>
>> The code in question is:
>>
>> class SomeClass {
>>  private Resource resource = null;
>>
>>  public Resource getResource() {
>>    if (resource == null) {
>>      synchronized (this) {
>>        if (resource == null)
>>          resource = new Resource();
>>      }
>>    }
>>    return resource;
>>  }
>> }
>>
>> On page 4 of the article, "So what's broken about DCL?", I would like to
>> get
>> a more precise description of the 1st scenario discussed in that section.
>> This is my understanding with questions.
>>
>> The reordering discussed in the scenario happens in dynamic compilation
>> and
>> would seem to happen once and apply to all threads for a class, correct?
>>
>> And, does dynamic compilation happen just before JIT?
>>
>> In the example, the compiler chooses to reorder the instructions of
>> getResource() to: allocate memory, assign reference to resource, call
>> constructor.
>>
>> But, in a thread, the as-if-sequential semantics hold, so do those
>> instructions happen AFTER the thread has done all of its work in the
>> "local
>> memory."
>>
>> And, do the instructions apply to main memory only?
>>
>> Here is the scenario in the example.
>>
>> Thread A has invoked getResource() on a someClass object.
>>
>> Thread A finds that resource is null and so Thread A invokes the
>> constructor
>> for Resource and sets the resource.
>>
>> Thread A still has not exited the synchronized block so no writes have
>> been
>> made to main memory.
>>
>> Thread A's state is in its "local memory" on the processor or cache in
>> which
>> it is running.
>>
>> But, before Thread B invokes, getResource(), these "reordered
>> instructions"
>> happen for Thread A:  allocate memory, assign reference to resource
>>
>> Now, Thread B invokes getResource().
>>
>> Thread B finds the reference to resource that the "reordered" instructions
>> from Thread A put into main memory.
>>
>> But, in the meantime, the processor executing the instructions for Thread
>> A
>> still has not caused the constructor to execute in main memory so main
>> memory has a reference but nothing in it.
>>
>> So, is this why Thread B is partially constructed?
>>
>> Thanks.
>>
>> Carol Saah
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/6695c706/attachment-0001.html>

From joe.bowbeer at gmail.com  Sat May  2 14:56:01 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 2 May 2009 11:56:01 -0700
Subject: [concurrency-interest] Reordering clarification in DCL example
In-Reply-To: <C621F6E2.1CA%csaah@cox.net>
References: <63b4e4050905020938u5c4ba444vec35d2ef003a5d11@mail.gmail.com>
	<C621F6E2.1CA%csaah@cox.net>
Message-ID: <31f2a7bd0905021156k74e944b2n459fadc8aeddab5d@mail.gmail.com>

On Sat, May 2, 2009 at 10:10 AM, Carol Saah wrote:

>  Tim,
>
> Thanks.  I?m not trying to find a way to fix DCL.  I am familiar with Item
> 71 in Effective Java.  And, section 16.2.4 of the Java Concurrency in
> Practice does not answer my questions.
>
> I?m trying to understand exactly what goes on under-the-covers when the
> compiler reorders code in incorrectly synchronized code.
>
> The example given in the old article does touch on what I am interesting in
> understanding.
>
> Carol
>

If you haven't already, check out Doug Lea's cookbook for compiler writers:

http://gee.cs.oswego.edu/dl/jmm/cookbook.html

and imagine that the loads and stores are "normal".

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/cd8c42cb/attachment.html>

From reachbach at gmail.com  Sun May  3 06:12:33 2009
From: reachbach at gmail.com (Bharath Ravi Kumar)
Date: Sun, 3 May 2009 15:42:33 +0530
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 1
In-Reply-To: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
Message-ID: <76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>

Hi Tim,

How does the local variable tmp help? Why not carry out a null checks on
(and subsequent assignment to) resource directly? Does the usage of tmp
reduce contention in any manner? Could you please explain?

Thanks,
Bharath



> From: Tim Peierls <tim at peierls.net>
> Subject: Re: [concurrency-interest] Reordering clarification in DCL
>        example
> To: Carol Saah <csaah at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Message-ID:
>        <63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> See more recent discussions in Java Concurrency in Practice, Section 16.2.4
> and Effective Java, 2nd edition, Item 71.
> The fixed version of the code is:
>
> class SomeClass {
>  private volatile Resource resource;
>
>  public Resource getResource() {
>    Resource tmp = resource;
>    if (tmp == null) {
>      synchronized (this) {
>        tmp = resource;
>        if (tmp == null)
>          resource = tmp = new Resource();
>      }
>    }
>    return tmp;
>  }
> }
>
> --tim
>
> On Sat, May 2, 2009 at 12:21 PM, Carol Saah <csaah at cox.net> wrote:
>
> > Hello,
> >
> > I understand from Brian Goetz that we should NOT use
> > Double-Checked-Locking.
> >
> > However, I am trying to understand exactly what can happen in the
> > incorrectly synchronized code that appears in the article,
> "Double-checked
> > locking: Clever, but broken."  I realize that
> > http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very
> old
> > and that some comments may be old; however, the reordering issue is not
> > old.
> >
> > The code in question is:
> >
> > class SomeClass {
> >  private Resource resource = null;
> >
> >  public Resource getResource() {
> >    if (resource == null) {
> >      synchronized (this) {
> >        if (resource == null)
> >          resource = new Resource();
> >      }
> >    }
> >    return resource;
> >  }
> > }
> >
> > On page 4 of the article, "So what's broken about DCL?", I would like to
> > get
> > a more precise description of the 1st scenario discussed in that section.
> > This is my understanding with questions.
> >
> > The reordering discussed in the scenario happens in dynamic compilation
> and
> > would seem to happen once and apply to all threads for a class, correct?
> >
> > And, does dynamic compilation happen just before JIT?
> >
> > In the example, the compiler chooses to reorder the instructions of
> > getResource() to: allocate memory, assign reference to resource, call
> > constructor.
> >
> > But, in a thread, the as-if-sequential semantics hold, so do those
> > instructions happen AFTER the thread has done all of its work in the
> "local
> > memory."
> >
> > And, do the instructions apply to main memory only?
> >
> > Here is the scenario in the example.
> >
> > Thread A has invoked getResource() on a someClass object.
> >
> > Thread A finds that resource is null and so Thread A invokes the
> > constructor
> > for Resource and sets the resource.
> >
> > Thread A still has not exited the synchronized block so no writes have
> been
> > made to main memory.
> >
> > Thread A's state is in its "local memory" on the processor or cache in
> > which
> > it is running.
> >
> > But, before Thread B invokes, getResource(), these "reordered
> instructions"
> > happen for Thread A:  allocate memory, assign reference to resource
> >
> > Now, Thread B invokes getResource().
> >
> > Thread B finds the reference to resource that the "reordered"
> instructions
> > from Thread A put into main memory.
> >
> > But, in the meantime, the processor executing the instructions for Thread
> A
> > still has not caused the constructor to execute in main memory so main
> > memory has a reference but nothing in it.
> >
> > So, is this why Thread B is partially constructed?
> >
> > Thanks.
> >
> > Carol Saah
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/8937e0cd/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Sat, 02 May 2009 13:10:58 -0400
> From: Carol Saah <csaah at cox.net>
> Subject: Re: [concurrency-interest] Reordering clarification in DCL
>        example
> To: Tim Peierls <tim at peierls.net>
> Cc: concurrency-interest at cs.oswego.edu
> Message-ID: <C621F6E2.1CA%csaah at cox.net <C621F6E2.1CA%25csaah at cox.net>>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Tim,
>
> Thanks.  I?m not trying to find a way to fix DCL.  I am familiar with Item
> 71 in Effective Java.  And, section 16.2.4 of the Java Concurrency in
> Practice does not answer my questions.
>
> I?m trying to understand exactly what goes on under-the-covers when the
> compiler reorders code in incorrectly synchronized code.
>
> The example given in the old article does touch on what I am interesting in
> understanding.
>
> Carol
>
> On 5/2/09 12:38 PM, "Tim Peierls" <tim at peierls.net> wrote:
>
> > See more recent discussions in Java Concurrency in Practice, Section
> 16.2.4
> > and Effective Java, 2nd edition, Item 71.
> >
> > The fixed version of the code is:
> >
> > class SomeClass {
> > ??private volatile Resource resource;
> >
> > ??public Resource getResource() {
> > ?? ?Resource tmp = resource;
> > ?? ?if (tmp == null) {
> > ?? ? ?synchronized (this) {
> > ?? ? ? ?tmp = resource;
> > ?? ? ? ?if (tmp == null)
> > ?? ? ? ? ?resource = tmp = new Resource();
> > ?? ? ?}
> > ?? ?}
> > ?? ?return tmp;
> > ??}
> > }
> >
> > --tim
> >
> > On Sat, May 2, 2009 at 12:21 PM, Carol Saah <csaah at cox.net> wrote:
> >> Hello,
> >>
> >> I understand from Brian Goetz that we should NOT use
> Double-Checked-Locking.
> >>
> >> However, I am trying to understand exactly what can happen in the
> >> incorrectly synchronized code that appears in the article,
> "Double-checked
> >> locking: Clever, but broken." ?I realize that
> >> http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very
> old
> >> and that some comments may be old; however, the reordering issue is not
> old.
> >>
> >> The code in question is:
> >>
> >> class SomeClass {
> >>  ?private Resource resource = null;
> >>
> >>  ?public Resource getResource() {
> >>  ? ?if (resource == null) {
> >>  ? ? ?synchronized (this) {
> >>  ? ? ? ?if (resource == null)
> >>  ? ? ? ? ?resource = new Resource();
> >>  ? ? ?}
> >>  ? ?}
> >>  ? ?return resource;
> >>  ?}
> >> }
> >>
> >> On page 4 of the article, "So what's broken about DCL?", I would like to
> get
> >> a more precise description of the 1st scenario discussed in that
> section.
> >> This is my understanding with questions.
> >>
> >> The reordering discussed in the scenario happens in dynamic compilation
> and
> >> would seem to happen once and apply to all threads for a class, correct?
> >>
> >> And, does dynamic compilation happen just before JIT?
> >>
> >> In the example, the compiler chooses to reorder the instructions of
> >> getResource() to: allocate memory, assign reference to resource, call
> >> constructor.
> >>
> >> But, in a thread, the as-if-sequential semantics hold, so do those
> >> instructions happen AFTER the thread has done all of its work in the
> "local
> >> memory."
> >>
> >> And, do the instructions apply to main memory only?
> >>
> >> Here is the scenario in the example.
> >>
> >> Thread A has invoked getResource() on a someClass object.
> >>
> >> Thread A finds that resource is null and so Thread A invokes the
> constructor
> >> for Resource and sets the resource.
> >>
> >> Thread A still has not exited the synchronized block so no writes have
> been
> >> made to main memory.
> >>
> >> Thread A's state is in its "local memory" on the processor or cache in
> which
> >> it is running.
> >>
> >> But, before Thread B invokes, getResource(), these "reordered
> instructions"
> >> happen for Thread A: ?allocate memory, assign reference to resource
> >>
> >> Now, Thread B invokes getResource().
> >>
> >> Thread B finds the reference to resource that the "reordered"
> instructions
> >> from Thread A put into main memory.
> >>
> >> But, in the meantime, the processor executing the instructions for
> Thread A
> >> still has not caused the constructor to execute in main memory so main
> >> memory has a reference but nothing in it.
> >>
> >> So, is this why Thread B is partially constructed?
> >>
> >> Thanks.
> >>
> >> Carol Saah
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/2e031a2b/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 4
> Date: Sat, 2 May 2009 10:12:32 -0700
> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> Subject: Re: [concurrency-interest] Reordering clarification in DCL
>        example
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Message-ID:
>        <31f2a7bd0905021012w36e944beg3aaf2cf771482354 at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> On Sat, May 2, 2009 at 9:38 AM, Tim Peierls wrote:
>
> > See more recent discussions in Java Concurrency in Practice, Section
> 16.2.4
> > and Effective Java, 2nd edition, Item 71.
> > The fixed version of the code is:
> >
> > class SomeClass {
> >   private volatile Resource resource;
> >
> >   public Resource getResource() {
> >     Resource tmp = resource;
> >     if (tmp == null) {
> >       synchronized (this) {
> >         tmp = resource;
> >         if (tmp == null)
> >           resource = tmp = new Resource();
> >       }
> >     }
> >     return tmp;
> >   }
> > }
> >
> > --tim
> >
>
> A clarification of "fixed" that I think Tim would agree with:
>
> "Fixed" is used in the sense that if you have to have double-check locking,
> presumably for performance reasons, then this is one of the very few ways
> to
> code it correctly.
>
> On the other hand, this "fix" adds even more surface area to the code, and
> more modifiers for maintainers to contemplate ("volatile"), so as with all
> optimizations, it is best avoid going this route unless you really need it.
>
> Compiler: In JMM and DCL discussions, the "compiler" refers to javac plus
> JIT plus anything else that reorders or optimizes at the instruction level.
> In addition to the compiler(s), the processors and memory system (and
> anything that might be rigged to run Java apps), can reorder operations --
> but only to the extent allowed by the JMM.
>
> As you surmise, the danger here is that thread B will see a non-null
> reference to an incompletely constructed resource.
>
> Joe
>
> PS - The JMM FAQ could use another update on this topic:
>
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#dcl<http://www.cs.umd.edu/%7Epugh/java/memoryModel/jsr-133-faq.html#dcl>
>
>
> On Sat, May 2, 2009 at 12:21 PM, Carol Saah wrote:
> >
> >> Hello,
> >>
> >> I understand from Brian Goetz that we should NOT use
> >> Double-Checked-Locking.
> >>
> >> However, I am trying to understand exactly what can happen in the
> >> incorrectly synchronized code that appears in the article,
> "Double-checked
> >> locking: Clever, but broken."  I realize that
> >> http://www.javaworld.com/jw-02-2001/jw-0209-double.html?page=4 is very
> >> old
> >> and that some comments may be old; however, the reordering issue is not
> >> old.
> >>
> >> The code in question is:
> >>
> >> class SomeClass {
> >>  private Resource resource = null;
> >>
> >>  public Resource getResource() {
> >>    if (resource == null) {
> >>      synchronized (this) {
> >>        if (resource == null)
> >>          resource = new Resource();
> >>      }
> >>    }
> >>    return resource;
> >>  }
> >> }
> >>
> >> On page 4 of the article, "So what's broken about DCL?", I would like to
> >> get
> >> a more precise description of the 1st scenario discussed in that
> section.
> >> This is my understanding with questions.
> >>
> >> The reordering discussed in the scenario happens in dynamic compilation
> >> and
> >> would seem to happen once and apply to all threads for a class, correct?
> >>
> >> And, does dynamic compilation happen just before JIT?
> >>
> >> In the example, the compiler chooses to reorder the instructions of
> >> getResource() to: allocate memory, assign reference to resource, call
> >> constructor.
> >>
> >> But, in a thread, the as-if-sequential semantics hold, so do those
> >> instructions happen AFTER the thread has done all of its work in the
> >> "local
> >> memory."
> >>
> >> And, do the instructions apply to main memory only?
> >>
> >> Here is the scenario in the example.
> >>
> >> Thread A has invoked getResource() on a someClass object.
> >>
> >> Thread A finds that resource is null and so Thread A invokes the
> >> constructor
> >> for Resource and sets the resource.
> >>
> >> Thread A still has not exited the synchronized block so no writes have
> >> been
> >> made to main memory.
> >>
> >> Thread A's state is in its "local memory" on the processor or cache in
> >> which
> >> it is running.
> >>
> >> But, before Thread B invokes, getResource(), these "reordered
> >> instructions"
> >> happen for Thread A:  allocate memory, assign reference to resource
> >>
> >> Now, Thread B invokes getResource().
> >>
> >> Thread B finds the reference to resource that the "reordered"
> instructions
> >> from Thread A put into main memory.
> >>
> >> But, in the meantime, the processor executing the instructions for
> Thread
> >> A
> >> still has not caused the constructor to execute in main memory so main
> >> memory has a reference but nothing in it.
> >>
> >> So, is this why Thread B is partially constructed?
> >>
> >> Thanks.
> >>
> >> Carol Saah
> >>
> >>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090502/6695c706/attachment.html
> >
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 52, Issue 1
> ***************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090503/7cf088e7/attachment-0001.html>

From tim at peierls.net  Sun May  3 08:24:48 2009
From: tim at peierls.net (Tim Peierls)
Date: Sun, 3 May 2009 08:24:48 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	1
In-Reply-To: <76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
	<76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>
Message-ID: <63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>

Reads and writes of volatiles have memory effects under the JMM. The idiom
described in EJ2e, Item 71 minimizes the number of volatile reads and writes
through the use of a temporary variable. Josh Bloch says (in Item 71) that
on his machine this code was 25 percent faster than the obvious version with
no temporary variable. Item 71 also has good advice about when the use of
this idiom is appropriate.

--tim

On Sun, May 3, 2009 at 6:12 AM, Bharath Ravi Kumar <reachbach at gmail.com>wrote:

> Hi Tim,
>
> How does the local variable tmp help? Why not carry out a null checks on
> (and subsequent assignment to) resource directly? Does the usage of tmp
> reduce contention in any manner? Could you please explain?
>
> Thanks,
> Bharath
>
>
>
>> From: Tim Peierls <tim at peierls.net>
>> Subject: Re: [concurrency-interest] Reordering clarification in DCL
>>        example
>> To: Carol Saah <csaah at cox.net>
>> Cc: concurrency-interest at cs.oswego.edu
>> Message-ID:
>>        <63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
>> Content-Type: text/plain; charset="iso-8859-1"
>>
>> See more recent discussions in Java Concurrency in Practice, Section
>> 16.2.4
>> and Effective Java, 2nd edition, Item 71.
>> The fixed version of the code is:
>>
>> class SomeClass {
>>  private volatile Resource resource;
>>
>>  public Resource getResource() {
>>    Resource tmp = resource;
>>    if (tmp == null) {
>>      synchronized (this) {
>>        tmp = resource;
>>        if (tmp == null)
>>          resource = tmp = new Resource();
>>      }
>>    }
>>    return tmp;
>>  }
>> }
>>
>> --tim
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090503/a49cf734/attachment.html>

From alarmnummer at gmail.com  Tue May  5 07:02:45 2009
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 5 May 2009 13:02:45 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
Message-ID: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>

I have a question about the JIT and optimisations.

Is the JIT able to remove conditions based on volatile fields?

e.g.

class FooStatistics{.....}

class Foo{

   private final FooStatistics statisics;

   public Foo(FooStatistics statistics){
       this.statistics = statistics
   }

   void foo(){
      ... some logic

      if(statistics!=null)
           statistics.incSomeCasCounter();
   }

}

If I create a Foo with a null as statistics, the statistics part is
completely removed and my foo method is transformed to:

void foo(){
    ..... some logic
}

The problem is that you can't activate/deactivate statistics on an
existing object. So you could create something like this:

class Foo{

   private volatile FooStatistics statisics;

   public Foo(FooStatistics statistics){
       this.statistics = statistics
   }

   public void setStatistics(FooStatistics statistics){
      this.statistics = statistics;
   }

   void foo(){
      ... some logic

      if(statistics!=null)
           statistics.incSomeCasCounter();
   }

}

In this case I presume the JIT will have a very hard time removing the
statistics logic and as an additional cost, the system needs to access
an 'expensive' volatile variable.

I'm working on an STM implementation, and every read/write to
CAS/volatile variables counts..

So.. can anyone tell me if the final approach is the only way to go?
Or is the JIT able to do some magic like speculative removal of code..
and re-adding it if it is needed?

Peter

From karmazilla at gmail.com  Tue May  5 09:17:21 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Tue, 5 May 2009 15:17:21 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
Message-ID: <90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>

On Tue, May 5, 2009 at 1:02 PM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I have a question about the JIT and optimisations.
>
> Is the JIT able to remove conditions based on volatile fields?
>
> e.g.
>
> class FooStatistics{.....}
>
> class Foo{
>
> ? private final FooStatistics statisics;
>
> ? public Foo(FooStatistics statistics){
> ? ? ? this.statistics = statistics
> ? }
>
> ? void foo(){
> ? ? ?... some logic
>
> ? ? ?if(statistics!=null)
> ? ? ? ? ? statistics.incSomeCasCounter();
> ? }
>
> }
>
> If I create a Foo with a null as statistics, the statistics part is
> completely removed and my foo method is transformed to:
>
> void foo(){
> ? ?..... some logic
> }

Different instances of this class may have different values assigned
to the statistics field, but the code itself is static and, I would
assume, optimized as such. Therefor, a JIT that eliminates that
condition would still have to be mindful of any future instances where
such an optimization is invalid.

>
> The problem is that you can't activate/deactivate statistics on an
> existing object. So you could create something like this:
>
> class Foo{
>
> ? private volatile FooStatistics statisics;
>
> ? public Foo(FooStatistics statistics){
> ? ? ? this.statistics = statistics
> ? }
>
> ? public void setStatistics(FooStatistics statistics){
> ? ? ?this.statistics = statistics;
> ? }
>
> ? void foo(){
> ? ? ?... some logic
>
> ? ? ?if(statistics!=null)
> ? ? ? ? ? statistics.incSomeCasCounter();
> ? }
>
> }
>
> In this case I presume the JIT will have a very hard time removing the
> statistics logic and as an additional cost, the system needs to access
> an 'expensive' volatile variable.
>
> I'm working on an STM implementation, and every read/write to
> CAS/volatile variables counts..
>
> So.. can anyone tell me if the final approach is the only way to go?
> Or is the JIT able to do some magic like speculative removal of code..
> and re-adding it if it is needed?

I believe that hotspot is able to do dynamic deoptimization of code
that is conditional on normal variables, but the visibility semantics
of a volatile read might prevent this from happening. That said,
volatile reads are "pretty cheap" and the CPU might be able to predict
the branch, though pretty much all I know about branch prediction is
the name.

>
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From cleber at nightcoders.com.br  Wed May  6 09:40:54 2009
From: cleber at nightcoders.com.br (Cleber Muramoto)
Date: Wed, 6 May 2009 10:40:54 -0300
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 3
In-Reply-To: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
Message-ID: <668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>

I'd be interested in knowing how such "cheap" operations are profiled. Does
he mention it on the book?

There's also one interesting detail on the Exchanger implementation where
a method local variable is created outside the synch block to be (possibly)
further assigned
to the instance variable.

private void createSlot(int index) {
        // Create slot outside of lock to narrow sync region
        Slot newSlot = new Slot();
        Slot[] a = arena;
        synchronized (a) {
            if (a[index] == null)
                a[index] = newSlot;
        }
}

How can one estimate the trade-offs of running into possibly unnecessary
instantiations in favor of narrower synch blocks?




> Message: 1
> Date: Sun, 3 May 2009 08:24:48 -0400
> From: Tim Peierls <tim at peierls.net>
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>        52,     Issue   1
> To: Bharath Ravi Kumar <reachbach at gmail.com>
> Cc: concurrency-interest at cs.oswego.edu
> Message-ID:
>        <63b4e4050905030524y31ee259fn1b7fcd78718bd4 at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Reads and writes of volatiles have memory effects under the JMM. The idiom
> described in EJ2e, Item 71 minimizes the number of volatile reads and
> writes
> through the use of a temporary variable. Josh Bloch says (in Item 71) that
> on his machine this code was 25 percent faster than the obvious version
> with
> no temporary variable. Item 71 also has good advice about when the use of
> this idiom is appropriate.
>
> --tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090506/f5018169/attachment.html>

From dl at cs.oswego.edu  Wed May  6 09:58:26 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 06 May 2009 09:58:26 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
 Issue 3
In-Reply-To: <668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
References: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
	<668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
Message-ID: <4A019782.7040800@cs.oswego.edu>

Cleber Muramoto wrote:
> How can one estimate the trade-offs of running into possibly unnecessary
> instantiations in favor of narrower synch blocks?

If you don't know, measure. And when you measure, do so
across as many kinds of workloads as you believe will arise.
We measure a lot, which in turn causes us to usually choose
a good approach before measuring. But even still, "usually"
is far from "always".

The JCiP (http://jcip.net/) testing chapter(12) contains some
introductory discussions on performance measurement.

-Doug



From tim at peierls.net  Wed May  6 12:02:26 2009
From: tim at peierls.net (Tim Peierls)
Date: Wed, 6 May 2009 12:02:26 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	3
In-Reply-To: <4A019782.7040800@cs.oswego.edu>
References: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
	<668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
	<4A019782.7040800@cs.oswego.edu>
Message-ID: <63b4e4050905060902g6b653689ge1d017bad7161434@mail.gmail.com>

On Wed, May 6, 2009 at 9:58 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> Cleber Muramoto wrote:
>
>> How can one estimate the trade-offs of running into possibly
>> unnecessary instantiations in favor of narrower synch blocks?
>>
>
> If you don't know, measure. And when you measure, do so across as many
> kinds of workloads as you believe will arise. We measure a lot, which in
> turn causes us to usually choose a good approach before measuring. But even
> still, "usually" is far from "always".
>
> The JCiP (http://jcip.net/) testing chapter(12) contains some introductory
> discussions on performance measurement.
>

And see http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/ for
many examples of measurement code. The techniques used are sketchily
documented at best, but in conjunction with the JCiP advice they might give
you a sense of what kinds of things you need to worry about.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090506/1146bbb1/attachment.html>

From tim at peierls.net  Wed May  6 12:03:08 2009
From: tim at peierls.net (Tim Peierls)
Date: Wed, 6 May 2009 12:03:08 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	3
In-Reply-To: <4A019782.7040800@cs.oswego.edu>
References: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
	<668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
	<4A019782.7040800@cs.oswego.edu>
Message-ID: <63b4e4050905060903x98ade63v86cfe3b5b786ce17@mail.gmail.com>

On Wed, May 6, 2009 at 9:58 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> Cleber Muramoto wrote:
>
>> How can one estimate the trade-offs of running into possibly
>> unnecessary instantiations in favor of narrower synch blocks?
>>
>
> If you don't know, measure. And when you measure, do so across as many
> kinds of workloads as you believe will arise. We measure a lot, which in
> turn causes us to usually choose a good approach before measuring. But even
> still, "usually" is far from "always".
>
> The JCiP (http://jcip.net/) testing chapter(12) contains some introductory
> discussions on performance measurement.
>

And see http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/ for
many examples of measurement code. The techniques used are sketchily
documented at best, but in conjunction with the JCiP advice they might give
you a sense of what kinds of things you need to worry about.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090506/c2534fb4/attachment.html>

From brian at briangoetz.com  Wed May  6 22:37:43 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 06 May 2009 22:37:43 -0400
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
	<90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
Message-ID: <4A024977.7010009@briangoetz.com>

> Different instances of this class may have different values assigned
> to the statistics field, but the code itself is static and, I would
> assume, optimized as such. 

This is not true.  There's nothing that says the JIT need only compile one 
version of a given method.  HotSpot will readily clone code where it can spot 
a slow/fast path, even based on deep inlining.  For example:

foo(Moo x) {
   goo(x);
}

goo(Moo x) {
   if (x != null) { something expensive }
}

Here, calls to foo() may will be inlined (or inlined-cached), and in the 
branch where it is inlined, it can pull the null check all the way out to the 
caller, and have two completely different paths based on a condition that is 
way far down the tree.


From brian at briangoetz.com  Wed May  6 22:53:09 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 06 May 2009 22:53:09 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
 Issue 3
In-Reply-To: <668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
References: <mailman.1.1241366401.20845.concurrency-interest@cs.oswego.edu>
	<668a9da40905060640t1e0ef35r3ca6d5c79b5634fb@mail.gmail.com>
Message-ID: <4A024D15.405@briangoetz.com>

You're comparing two different measures here.  Allocating less is a 
performance boost -- you are doing less work in the straight-line case.  Short 
sync blocks, on the other hand, are a scalability boost -- they don't entail 
less work (for the most part), but you're enabling greater concurrency by 
holding the lock for less time, which could under the right conditions turn 
into higher throughput (google for "Little's Law") given enough contention 
that it is an impediment.



Cleber Muramoto wrote:
> I'd be interested in knowing how such "cheap" operations are profiled. 
> Does he mention it on the book?
> 
> There's also one interesting detail on the Exchanger implementation where
> a method local variable is created outside the synch block to be 
> (possibly) further assigned
> to the instance variable.
> 
> private void createSlot(int index) {
>         // Create slot outside of lock to narrow sync region
>         Slot newSlot = new Slot();
>         Slot[] a = arena;
>         synchronized (a) {
>             if (a[index] == null)
>                 a[index] = newSlot;
>         }
> }
> 
> How can one estimate the trade-offs of running into possibly unnecessary
> instantiations in favor of narrower synch blocks?
> 
> 
> 
> 
>     Message: 1
>     Date: Sun, 3 May 2009 08:24:48 -0400
>     From: Tim Peierls <tim at peierls.net <mailto:tim at peierls.net>>
>     Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>            52,     Issue   1
>     To: Bharath Ravi Kumar <reachbach at gmail.com
>     <mailto:reachbach at gmail.com>>
>     Cc: concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>
>     Message-ID:
>            <63b4e4050905030524y31ee259fn1b7fcd78718bd4 at mail.gmail.com
>     <mailto:63b4e4050905030524y31ee259fn1b7fcd78718bd4 at mail.gmail.com>>
>     Content-Type: text/plain; charset="iso-8859-1"
> 
>     Reads and writes of volatiles have memory effects under the JMM. The
>     idiom
>     described in EJ2e, Item 71 minimizes the number of volatile reads
>     and writes
>     through the use of a temporary variable. Josh Bloch says (in Item
>     71) that
>     on his machine this code was 25 percent faster than the obvious
>     version with
>     no temporary variable. Item 71 also has good advice about when the
>     use of
>     this idiom is appropriate.
> 
>     --tim
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From karmazilla at gmail.com  Thu May  7 03:01:11 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Thu, 7 May 2009 09:01:11 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <4A024977.7010009@briangoetz.com>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
	<90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
	<4A024977.7010009@briangoetz.com>
Message-ID: <90622e530905070001q6c26e027mc6da71a74f1b2c40@mail.gmail.com>

Awesome. I stand corrected.

On Thu, May 7, 2009 at 4:37 AM, Brian Goetz <brian at briangoetz.com> wrote:
>> Different instances of this class may have different values assigned
>> to the statistics field, but the code itself is static and, I would
>> assume, optimized as such.
>
> This is not true. ?There's nothing that says the JIT need only compile one
> version of a given method. ?HotSpot will readily clone code where it can
> spot a slow/fast path, even based on deep inlining. ?For example:
>
> foo(Moo x) {
> ?goo(x);
> }
>
> goo(Moo x) {
> ?if (x != null) { something expensive }
> }
>
> Here, calls to foo() may will be inlined (or inlined-cached), and in the
> branch where it is inlined, it can pull the null check all the way out to
> the caller, and have two completely different paths based on a condition
> that is way far down the tree.
>
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From pierre.foures at gmail.com  Thu May  7 10:05:34 2009
From: pierre.foures at gmail.com (=?ISO-8859-1?Q?Pierre_Four=E8s?=)
Date: Thu, 7 May 2009 16:05:34 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
Message-ID: <4eeb497a0905070705q2503eb18rd5e2164f80caf08c@mail.gmail.com>

Hello Peter,

An other approach, not based on the ability of the JIT optimizing your code,
could merge benefits of the two approach : keeping the ability to update the
statistics variable while not adding overhead of a volatile variable, at the
expense of increased code complexity.


The way I understand your problem make me believe the foo() method
invocation ratio should be quite high over the amount of updates between
having or not statistics for your foo object. I also presume the clients of
your foo objects accomplish some kind of large amount of treatments
split over some kind of "work units".


If this is the case, you may want to encapsulate your foo instance behind
some kind of monitor. Access to the foo instance would go through the
monitor and a synchronized bloc would ensure the memory visibility over the
"statistics" field. The same technique would be used to set or unset the
FooStatistics object on the foo instance.


You could use something like this :


FooMonitor {

    private final Foo foo;


    FooMonitor(Foo foo) {

        this.foo = foo;

    }


    public synchronized Foo getFoo() { return foo;}


    public synchronized void setStatistics(FooStatistics stats} {

        foo.setStatistics(stats);

    }

}


While this approach is more slow than accessing a single volatile field this
code is invoked much more rarely and could/should outperform the use of the
volatile field. However, note that you can't predict if the current working
unit may eventually see or not the update in the middle of its execution.


For statistics purposes this might sound ok but depending on your needs and
requirements you could use a mutual exclusion policy over the foo instance
in order to grant when the statistic object would be visible by the clients
of the foo instance. This last point make the solution even more complex and
requires strong coding discipline and extensive testing.


The (probably wrong) sketch currently in my mind would look like this (with
no fairness consideration in mind) :


FooMonitor {

    private final Foo foo;


    private boolean accessible = true;

    private Object lock = new Object();


    FooMonitor(Foo foo) {

        this.foo = foo;

    }


    public Foo getFoo() {

        synchronized(lock) {

            while(!accessible) wait();

            accessible = false;

            return foo;

        }

    }


    public void releaseFoo() {

        synchronized(lock) {

            accessible = true;

        }

    }


    public void setStatistics(FooStatistics stats} {

        synchronized(lock) {

            while(!accessible) wait();

            foo.setStatistics(stats);

        }

    }

}


Of course both solutions should only be considered under extreme seek for
performance (but if for you every read/write to CAS/volatile variables
counts, this seems to be the case.) and I even wouldn't consider using the
last solution with only the argument of performance in mind (I would rather
upgrade the server or add a node to the cluster).


Regards,

Pierre.

2009/5/5 Peter Veentjer <alarmnummer at gmail.com>

> I have a question about the JIT and optimisations.
>
> Is the JIT able to remove conditions based on volatile fields?
>
> e.g.
>
> class FooStatistics{.....}
>
> class Foo{
>
>   private final FooStatistics statisics;
>
>   public Foo(FooStatistics statistics){
>       this.statistics = statistics
>   }
>
>   void foo(){
>      ... some logic
>
>      if(statistics!=null)
>           statistics.incSomeCasCounter();
>   }
>
> }
>
> If I create a Foo with a null as statistics, the statistics part is
> completely removed and my foo method is transformed to:
>
> void foo(){
>    ..... some logic
> }
>
> The problem is that you can't activate/deactivate statistics on an
> existing object. So you could create something like this:
>
> class Foo{
>
>   private volatile FooStatistics statisics;
>
>   public Foo(FooStatistics statistics){
>       this.statistics = statistics
>   }
>
>   public void setStatistics(FooStatistics statistics){
>      this.statistics = statistics;
>   }
>
>   void foo(){
>      ... some logic
>
>      if(statistics!=null)
>           statistics.incSomeCasCounter();
>   }
>
> }
>
> In this case I presume the JIT will have a very hard time removing the
> statistics logic and as an additional cost, the system needs to access
> an 'expensive' volatile variable.
>
> I'm working on an STM implementation, and every read/write to
> CAS/volatile variables counts..
>
> So.. can anyone tell me if the final approach is the only way to go?
> Or is the JIT able to do some magic like speculative removal of code..
> and re-adding it if it is needed?
>
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090507/96dcda6f/attachment-0001.html>

From pierre.foures at gmail.com  Thu May  7 11:59:53 2009
From: pierre.foures at gmail.com (=?ISO-8859-1?Q?Pierre_Four=E8s?=)
Date: Thu, 7 May 2009 17:59:53 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <4eeb497a0905070705q2503eb18rd5e2164f80caf08c@mail.gmail.com>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
	<4eeb497a0905070705q2503eb18rd5e2164f80caf08c@mail.gmail.com>
Message-ID: <4eeb497a0905070859na503067kaf227bd10a89cb54@mail.gmail.com>

Just to note that the second example was indeed flawed.

The wait() method have to be invoked on the lock object for methods getFoo()
and method setStatistics(). I also omitted in method releaseFoo() to call
lock.notify() just after setting accessible to true in order to wake up an
eventually "setStatistics" pending threads.


Regards,

Pierre.

Le 7 mai 2009 16:05, Pierre Four?s <pierre.foures at gmail.com> a ?crit :

> Hello Peter,
>
> An other approach, not based on the ability of the JIT optimizing your
> code, could merge benefits of the two approach : keeping the ability to
> update the statistics variable while not adding overhead of a volatile
> variable, at the expense of increased code complexity.
>
>
> The way I understand your problem make me believe the foo() method
> invocation ratio should be quite high over the amount of updates between
> having or not statistics for your foo object. I also presume the clients of
> your foo objects accomplish some kind of large amount of treatments
> split over some kind of "work units".
>
>
> If this is the case, you may want to encapsulate your foo instance behind
> some kind of monitor. Access to the foo instance would go through the
> monitor and a synchronized bloc would ensure the memory visibility over the
> "statistics" field. The same technique would be used to set or unset the
> FooStatistics object on the foo instance.
>
>
> You could use something like this :
>
>
> FooMonitor {
>
>     private final Foo foo;
>
>
>     FooMonitor(Foo foo) {
>
>         this.foo = foo;
>
>     }
>
>
>     public synchronized Foo getFoo() { return foo;}
>
>
>     public synchronized void setStatistics(FooStatistics stats} {
>
>         foo.setStatistics(stats);
>
>     }
>
> }
>
>
> While this approach is more slow than accessing a single volatile field
> this code is invoked much more rarely and could/should outperform the use of
> the volatile field. However, note that you can't predict if the current
> working unit may eventually see or not the update in the middle of its
> execution.
>
>
> For statistics purposes this might sound ok but depending on your needs and
> requirements you could use a mutual exclusion policy over the foo instance
> in order to grant when the statistic object would be visible by the clients
> of the foo instance. This last point make the solution even more complex and
> requires strong coding discipline and extensive testing.
>
>
> The (probably wrong) sketch currently in my mind would look like this (with
> no fairness consideration in mind) :
>
>
> FooMonitor {
>
>     private final Foo foo;
>
>
>     private boolean accessible = true;
>
>     private Object lock = new Object();
>
>
>     FooMonitor(Foo foo) {
>
>         this.foo = foo;
>
>     }
>
>
>     public Foo getFoo() {
>
>         synchronized(lock) {
>
>             while(!accessible) wait();
>
>             accessible = false;
>
>             return foo;
>
>         }
>
>     }
>
>
>     public void releaseFoo() {
>
>         synchronized(lock) {
>
>             accessible = true;
>
>         }
>
>     }
>
>
>     public void setStatistics(FooStatistics stats} {
>
>         synchronized(lock) {
>
>             while(!accessible) wait();
>
>             foo.setStatistics(stats);
>
>         }
>
>     }
>
> }
>
>
> Of course both solutions should only be considered under extreme seek for
> performance (but if for you every read/write to CAS/volatile variables
> counts, this seems to be the case.) and I even wouldn't consider using the
> last solution with only the argument of performance in mind (I would rather
> upgrade the server or add a node to the cluster).
>
>
> Regards,
>
> Pierre.
>
> 2009/5/5 Peter Veentjer <alarmnummer at gmail.com>
>
> I have a question about the JIT and optimisations.
>>
>> Is the JIT able to remove conditions based on volatile fields?
>>
>> e.g.
>>
>> class FooStatistics{.....}
>>
>> class Foo{
>>
>>   private final FooStatistics statisics;
>>
>>   public Foo(FooStatistics statistics){
>>       this.statistics = statistics
>>   }
>>
>>   void foo(){
>>      ... some logic
>>
>>      if(statistics!=null)
>>           statistics.incSomeCasCounter();
>>   }
>>
>> }
>>
>> If I create a Foo with a null as statistics, the statistics part is
>> completely removed and my foo method is transformed to:
>>
>> void foo(){
>>    ..... some logic
>> }
>>
>> The problem is that you can't activate/deactivate statistics on an
>> existing object. So you could create something like this:
>>
>> class Foo{
>>
>>   private volatile FooStatistics statisics;
>>
>>   public Foo(FooStatistics statistics){
>>       this.statistics = statistics
>>   }
>>
>>   public void setStatistics(FooStatistics statistics){
>>      this.statistics = statistics;
>>   }
>>
>>   void foo(){
>>      ... some logic
>>
>>      if(statistics!=null)
>>           statistics.incSomeCasCounter();
>>   }
>>
>> }
>>
>> In this case I presume the JIT will have a very hard time removing the
>> statistics logic and as an additional cost, the system needs to access
>> an 'expensive' volatile variable.
>>
>> I'm working on an STM implementation, and every read/write to
>> CAS/volatile variables counts..
>>
>> So.. can anyone tell me if the final approach is the only way to go?
>> Or is the JIT able to do some magic like speculative removal of code..
>> and re-adding it if it is needed?
>>
>> Peter
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090507/6f6544bc/attachment.html>

From mthornton at optrak.co.uk  Thu May  7 17:29:28 2009
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Thu, 07 May 2009 22:29:28 +0100
Subject: [concurrency-interest] Atomic assignment
Message-ID: <4A0352B8.5010403@optrak.co.uk>

I was sure that the problem of (non)atomic assignment to volatile longs 
and doubles had been fixed, but this bug report suggests otherwise:

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233

Anyone know for sure?

Mark Thornton


From davidcholmes at aapt.net.au  Thu May  7 18:40:07 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 8 May 2009 08:40:07 +1000
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A0352B8.5010403@optrak.co.uk>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>

Hi Mark,

That bug is (or became) a RFE for the spec to make all accesses to
double/long atomic and that is not going to happen hence the "will not fix".
There are a number of other bugs that pertain to atomic access to volatile
long/double eg: 4247780 which was fixed back in 1.2.2

Thanks
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mark
> Thornton
> Sent: Friday, 8 May 2009 7:29 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Atomic assignment
>
>
>
> I was sure that the problem of (non)atomic assignment to volatile longs
> and doubles had been fixed, but this bug report suggests otherwise:
>
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
>
> Anyone know for sure?
>
> Mark Thornton
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From mthornton at optrak.co.uk  Fri May  8 03:51:23 2009
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Fri, 08 May 2009 08:51:23 +0100
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
Message-ID: <4A03E47B.3020800@optrak.co.uk>

David Holmes wrote:
> Hi Mark,
>
> That bug is (or became) a RFE for the spec to make all accesses to
> double/long atomic and that is not going to happen hence the "will not fix".
> There are a number of other bugs that pertain to atomic access to volatile
> long/double eg: 4247780 which was fixed back in 1.2.2
>
> Thanks
> David Holmes
>   
By all accesses I presume you mean including things like ++, which is 
reasonably well documented as not atomic.

4247780 doesn't appear to exist in the bug database.

Regards,
Mark Thornton


From joe.bowbeer at gmail.com  Fri May  8 04:32:07 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 8 May 2009 01:32:07 -0700
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A03E47B.3020800@optrak.co.uk>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
	<4A03E47B.3020800@optrak.co.uk>
Message-ID: <31f2a7bd0905080132h3041e2aax408874558f723b7a@mail.gmail.com>

On Fri, May 8, 2009 at 12:51 AM, Mark Thornton wrote:

> By all accesses I presume you mean including things like ++, which is
> reasonably well documented as not atomic.
>
> 4247780 doesn't appear to exist in the bug database.
>
>
Bug report 4526490 and related indicate that volatile long and double were
fixed in 2001.

  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4526490

++ is two long accesses.  Atomic applies to each long access separately, not
to the pair of accesses.

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090508/5c102038/attachment.html>

From gregg at cytetech.com  Fri May  8 10:27:24 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 08 May 2009 09:27:24 -0500
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
Message-ID: <4A04414C.7060303@cytetech.com>

So will the compiler be changed to not allow volatile to exist on double/long 
declarations since it does not work?  Or might there be a warning that volatile 
does not produce atomic results on all statements that assign to or reference a 
double/long value?

Sigh...

Gregg Wonderly

David Holmes wrote:
> Hi Mark,
> 
> That bug is (or became) a RFE for the spec to make all accesses to
> double/long atomic and that is not going to happen hence the "will not fix".
> There are a number of other bugs that pertain to atomic access to volatile
> long/double eg: 4247780 which was fixed back in 1.2.2
> 
> Thanks
> David Holmes
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mark
>> Thornton
>> Sent: Friday, 8 May 2009 7:29 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Atomic assignment
>>
>>
>>
>> I was sure that the problem of (non)atomic assignment to volatile longs
>> and doubles had been fixed, but this bug report suggests otherwise:
>>
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
>>
>> Anyone know for sure?
>>
>> Mark Thornton
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From sberlin at gmail.com  Fri May  8 10:50:24 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Fri, 8 May 2009 10:50:24 -0400
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A04414C.7060303@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
	<4A04414C.7060303@cytetech.com>
Message-ID: <19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>

My interpretation is that the scenario today is that a volatile long/double
is atomic.  A non-volatile long/double is not atomic.  And the RFE that was
closed as will-not-fix was asking for all access to long/double (even
non-volatile) to be atomic.

Sam

On Fri, May 8, 2009 at 10:27 AM, Gregg Wonderly <gregg at cytetech.com> wrote:

> So will the compiler be changed to not allow volatile to exist on
> double/long declarations since it does not work?  Or might there be a
> warning that volatile does not produce atomic results on all statements that
> assign to or reference a double/long value?
>
> Sigh...
>
> Gregg Wonderly
>
>
> David Holmes wrote:
>
>> Hi Mark,
>>
>> That bug is (or became) a RFE for the spec to make all accesses to
>> double/long atomic and that is not going to happen hence the "will not
>> fix".
>> There are a number of other bugs that pertain to atomic access to volatile
>> long/double eg: 4247780 which was fixed back in 1.2.2
>>
>> Thanks
>> David Holmes
>>
>>  -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mark
>>> Thornton
>>> Sent: Friday, 8 May 2009 7:29 AM
>>> To: concurrency-interest at cs.oswego.edu
>>> Subject: [concurrency-interest] Atomic assignment
>>>
>>>
>>>
>>> I was sure that the problem of (non)atomic assignment to volatile longs
>>> and doubles had been fixed, but this bug report suggests otherwise:
>>>
>>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
>>>
>>> Anyone know for sure?
>>>
>>> Mark Thornton
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090508/7ca5fae0/attachment.html>

From gregg at cytetech.com  Fri May  8 11:02:48 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 08 May 2009 10:02:48 -0500
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
	<4A04414C.7060303@cytetech.com>
	<19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>
Message-ID: <4A044998.4020801@cytetech.com>

Okay, it we actually just have the ++/-- not atomic issue, than I am okay with 
that because I don't suddenly have new, broken code.  I looked around some more 
and ran the test code on that old issue on my dual core laptop and saw no 
failures.  But, I'm not sure after looking at that code whether it actually 
makes modifications in a way that would show a problem.

Gregg Wonderly

Sam Berlin wrote:
> My interpretation is that the scenario today is that a volatile 
> long/double is atomic.  A non-volatile long/double is not atomic.  And 
> the RFE that was closed as will-not-fix was asking for all access to 
> long/double (even non-volatile) to be atomic.
> 
> Sam
> 
> On Fri, May 8, 2009 at 10:27 AM, Gregg Wonderly <gregg at cytetech.com 
> <mailto:gregg at cytetech.com>> wrote:
> 
>     So will the compiler be changed to not allow volatile to exist on
>     double/long declarations since it does not work?  Or might there be
>     a warning that volatile does not produce atomic results on all
>     statements that assign to or reference a double/long value?
> 
>     Sigh...
> 
>     Gregg Wonderly
> 
> 
>     David Holmes wrote:
> 
>         Hi Mark,
> 
>         That bug is (or became) a RFE for the spec to make all accesses to
>         double/long atomic and that is not going to happen hence the
>         "will not fix".
>         There are a number of other bugs that pertain to atomic access
>         to volatile
>         long/double eg: 4247780 which was fixed back in 1.2.2
> 
>         Thanks
>         David Holmes
> 
>             -----Original Message-----
>             From: concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>
>             [mailto:concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>]On
>             Behalf Of Mark
>             Thornton
>             Sent: Friday, 8 May 2009 7:29 AM
>             To: concurrency-interest at cs.oswego.edu
>             <mailto:concurrency-interest at cs.oswego.edu>
>             Subject: [concurrency-interest] Atomic assignment
> 
> 
> 
>             I was sure that the problem of (non)atomic assignment to
>             volatile longs
>             and doubles had been fixed, but this bug report suggests
>             otherwise:
> 
>             http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
> 
>             Anyone know for sure?
> 
>             Mark Thornton
> 
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From markus.jevring at petercam.be  Fri May  8 11:10:01 2009
From: markus.jevring at petercam.be (Markus Jevring)
Date: Fri, 8 May 2009 17:10:01 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com><90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
	<4A024977.7010009@briangoetz.com>
Message-ID: <FC86FB1EEFB8994382A386021951CA900CD6C6DF@mail.petercam.corp>

In your last case, where you say that HotSpot will have multiple
potential paths to take (one of which examines the nullity higher up in
the tree), would the method(s) actually get invoked in this path, or
would it just be pruned based on this condition?
For example, if we have an aspect oriented solution that relies on
methods getting invoked, and these methods get pruned, then these
methods would never get called, and the aspect that is triggered only
when the methods get called would never run.
Or could it be that, since some languages (aspectj, for instance)
"weaves" code in at compile time, there *is* actually a reason to take
an execution path that would have otherwised gotten pruned?

The reason I ask is: if we have some generic tracing aspect that logs
method invocations, would the very presence of this aspect, whether it
does anything or not (logging enabled/disabled) prevent HotSpot from
pruning the path in question, leading to potentially lower performance
by forcing HotSpot's hand by disallowing certain optimizations that
might otherwise be done?

Granted, I don't see aspect oriented programming as something
necessarily performant, but I thought I'd ask.

Markus Jevring

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Brian
Goetz
Sent: jeudi 7 mai 2009 4:38
To: Christian Vest Hansen
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Volatile/final and the JIT

> Different instances of this class may have different values assigned
> to the statistics field, but the code itself is static and, I would
> assume, optimized as such. 

This is not true.  There's nothing that says the JIT need only compile
one 
version of a given method.  HotSpot will readily clone code where it can
spot 
a slow/fast path, even based on deep inlining.  For example:

foo(Moo x) {
   goo(x);
}

goo(Moo x) {
   if (x != null) { something expensive }
}

Here, calls to foo() may will be inlined (or inlined-cached), and in the

branch where it is inlined, it can pull the null check all the way out
to the 
caller, and have two completely different paths based on a condition
that is 
way far down the tree.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From karmazilla at gmail.com  Fri May  8 14:37:36 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Fri, 8 May 2009 20:37:36 +0200
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A044998.4020801@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
	<4A04414C.7060303@cytetech.com>
	<19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>
	<4A044998.4020801@cytetech.com>
Message-ID: <90622e530905081137p715f405es8dfe3a884a54a6ec@mail.gmail.com>

On Fri, May 8, 2009 at 5:02 PM, Gregg Wonderly <gregg at cytetech.com> wrote:
> Okay, it we actually just have the ++/-- not atomic issue, than I am okay
> with that because I don't suddenly have new, broken code. ?I looked around
> some more and ran the test code on that old issue on my dual core laptop and
> saw no failures. ?But, I'm not sure after looking at that code whether it
> actually makes modifications in a way that would show a problem.

Were you running it on 64 bit hardware? It may be possible that while
the JLS and the JVM makes no effort to keep the long & double
operations atomic, your hardware may be providing its own guarantees
that are stricter.

Everything I have read on volatile under the Java 5 JMM until I read
that bug report, have either stated clearly or assumed that there was
no atomicity guarantees in volatile - only visibility guarantees.
Volatile is atomic for references and primitives of 32 or fewer bits
only by virtue of these values having a general atomicity guarantee,
that has nothing to do with volatile itself.

I believe that JCiP and numerous articles on the internet, are pretty
clear on this. (Not having read the JLS or JMM myself.)

>
> Gregg Wonderly
>
> Sam Berlin wrote:
>>
>> My interpretation is that the scenario today is that a volatile
>> long/double is atomic. ?A non-volatile long/double is not atomic. ?And the
>> RFE that was closed as will-not-fix was asking for all access to long/double
>> (even non-volatile) to be atomic.
>>
>> Sam
>>
>> On Fri, May 8, 2009 at 10:27 AM, Gregg Wonderly <gregg at cytetech.com
>> <mailto:gregg at cytetech.com>> wrote:
>>
>> ? ?So will the compiler be changed to not allow volatile to exist on
>> ? ?double/long declarations since it does not work? ?Or might there be
>> ? ?a warning that volatile does not produce atomic results on all
>> ? ?statements that assign to or reference a double/long value?
>>
>> ? ?Sigh...
>>
>> ? ?Gregg Wonderly
>>
>>
>> ? ?David Holmes wrote:
>>
>> ? ? ? ?Hi Mark,
>>
>> ? ? ? ?That bug is (or became) a RFE for the spec to make all accesses to
>> ? ? ? ?double/long atomic and that is not going to happen hence the
>> ? ? ? ?"will not fix".
>> ? ? ? ?There are a number of other bugs that pertain to atomic access
>> ? ? ? ?to volatile
>> ? ? ? ?long/double eg: 4247780 which was fixed back in 1.2.2
>>
>> ? ? ? ?Thanks
>> ? ? ? ?David Holmes
>>
>> ? ? ? ? ? ?-----Original Message-----
>> ? ? ? ? ? ?From: concurrency-interest-bounces at cs.oswego.edu
>> ? ? ? ? ? ?<mailto:concurrency-interest-bounces at cs.oswego.edu>
>> ? ? ? ? ? ?[mailto:concurrency-interest-bounces at cs.oswego.edu
>> ? ? ? ? ? ?<mailto:concurrency-interest-bounces at cs.oswego.edu>]On
>> ? ? ? ? ? ?Behalf Of Mark
>> ? ? ? ? ? ?Thornton
>> ? ? ? ? ? ?Sent: Friday, 8 May 2009 7:29 AM
>> ? ? ? ? ? ?To: concurrency-interest at cs.oswego.edu
>> ? ? ? ? ? ?<mailto:concurrency-interest at cs.oswego.edu>
>> ? ? ? ? ? ?Subject: [concurrency-interest] Atomic assignment
>>
>>
>>
>> ? ? ? ? ? ?I was sure that the problem of (non)atomic assignment to
>> ? ? ? ? ? ?volatile longs
>> ? ? ? ? ? ?and doubles had been fixed, but this bug report suggests
>> ? ? ? ? ? ?otherwise:
>>
>> ? ? ? ? ? ?http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
>>
>> ? ? ? ? ? ?Anyone know for sure?
>>
>> ? ? ? ? ? ?Mark Thornton
>>
>> ? ? ? ? ? ?_______________________________________________
>> ? ? ? ? ? ?Concurrency-interest mailing list
>> ? ? ? ? ? ?Concurrency-interest at cs.oswego.edu
>> ? ? ? ? ? ?<mailto:Concurrency-interest at cs.oswego.edu>
>> ? ? ? ? ? ?http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> ? ? ? ?_______________________________________________
>> ? ? ? ?Concurrency-interest mailing list
>> ? ? ? ?Concurrency-interest at cs.oswego.edu
>> ? ? ? ?<mailto:Concurrency-interest at cs.oswego.edu>
>> ? ? ? ?http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> ? ?_______________________________________________
>> ? ?Concurrency-interest mailing list
>> ? ?Concurrency-interest at cs.oswego.edu
>> ? ?<mailto:Concurrency-interest at cs.oswego.edu>
>> ? ?http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From joe.bowbeer at gmail.com  Fri May  8 15:31:20 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 8 May 2009 12:31:20 -0700
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <90622e530905081137p715f405es8dfe3a884a54a6ec@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>
	<4A04414C.7060303@cytetech.com>
	<19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>
	<4A044998.4020801@cytetech.com>
	<90622e530905081137p715f405es8dfe3a884a54a6ec@mail.gmail.com>
Message-ID: <31f2a7bd0905081231g6b006f59y6fc868e3ba25f971@mail.gmail.com>

JLS 17.7 states that writes and reads of volatile long and double values are
always atomic (see below), and the JVMs have been in compliance for at least
5 years.

There's source to test corruption in the bug report:

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4526490

( from http://java.sun.com/docs/books/jls/third_edition/html/memory.html )

17.7 Non-atomic Treatment of double and long
>
> Some implementations may find it convenient to divide a single write action
> on a 64-bit long or double value into two write actions on adjacent 32 bit
> values. For efficiency's sake, this behavior is implementation specific;
> Java virtual machines are free to perform writes to long and double values
> atomically or in two parts.
>
> For the purposes of the Java programming language memory model, a single
> write to a non-volatile long or double value is treated as two separate
> writes: one to each 32-bit half. This can result in a situation where a
> thread sees the first 32 bits of a 64 bit value from one write, and the
> second 32 bits from another write. Writes and reads of volatile long and
> double values are always atomic. Writes to and reads of references are
> always atomic, regardless of whether they are implemented as 32 or 64 bit
> values.
>
> VM implementors are encouraged to avoid splitting their 64-bit values where
> possible. Programmers are encouraged to declare shared 64-bit values as
> volatile or synchronize their programs correctly to avoid possible
> complications.
>

Joe

On Fri, May 8, 2009 at 11:37 AM, Christian Vest Hansen wrote:

> Everything I have read on volatile under the Java 5 JMM until I read
> that bug report, have either stated clearly or assumed that there was
> no atomicity guarantees in volatile - only visibility guarantees.
>
> I believe that JCiP and numerous articles on the internet, are pretty
> clear on this. (Not having read the JLS or JMM myself.)
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090508/90d7a7d0/attachment.html>

From davidcholmes at aapt.net.au  Fri May  8 22:26:17 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 9 May 2009 12:26:17 +1000
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A03E47B.3020800@optrak.co.uk>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEGNIBAA.davidcholmes@aapt.net.au>

Mark Thornton writes:
> David Holmes wrote:
> > That bug is (or became) a RFE for the spec to make all accesses to
> > double/long atomic and that is not going to happen hence the
> > "will not fix". There are a number of other bugs that pertain
> > to atomic access to volatile long/double eg: 4247780 which was
> > fixed back in 1.2.2
> >
> >
> By all accesses I presume you mean including things like ++, which is
> reasonably well documented as not atomic.

No - "access" is a simple load or store. ++ is an assignment operator that
involves multiple operations: load, increment, store.

Not sure why this has suddenly stirred up confusion. As Sam Berlin
re-stated, all accesses to 32-bit types are atomic. The atomicity guarantee
extends to double and long if they are declared 'volatile'. No assignment
operators are defined to be atomic.

> 4247780 doesn't appear to exist in the bug database.

Sorry about that - that was actually a bug in the ExactVM which was the
production VM on Solaris. Hotspot has its own bugs with regard to volatile,
but as has been said the main bug was fixed years ago now.

Cheers,
David Holmes



From pierre.foures at gmail.com  Sat May  9 04:57:27 2009
From: pierre.foures at gmail.com (=?ISO-8859-1?Q?Pierre_Four=E8s?=)
Date: Sat, 9 May 2009 10:57:27 +0200
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <FC86FB1EEFB8994382A386021951CA900CD6C6DF@mail.petercam.corp>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com>
	<90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
	<4A024977.7010009@briangoetz.com>
	<FC86FB1EEFB8994382A386021951CA900CD6C6DF@mail.petercam.corp>
Message-ID: <4eeb497a0905090157p69d4f3b0wd93e564a0f7f1f17@mail.gmail.com>

While AOP just offers to programmers an other dimension to modularize and
compose their softwares but produces at last regular java (byte)code the JIT
should see it as an other regular sequence of bytecode to optimize.

Having the JIT to respect program semantics I believe it can't prune the
calls to advices. However it could inline them higher up with still passing
the right context to them (which is not magic, just some basic references to
java objects). Thus the JIT would be able to prune away some unnecessary
paths.

For the case of code doing nothing and with the facts the JIT being able to
figurate this out for regular java programs and that aspectized programs
becomes regulars java program when weaved, I believe the presence of aspects
would not disallow certain optimizations (like to prune away empty aspect
with the rest of the path). As more code is involved I guess it just make it
harder for the JIT to figure out what it could do or not.

All this makes me wonder about the JIT abilities : In the case of creating
and populating some new instance but not using it later on (some flag went
of) could the JIT optimize away all the path to creating this instance ? Up
to check that no side effects are encountered while invoking foo during its
population ?

class Process {
    void process() {
        Foo foo = new Foo();
        foo.setX(); foo.getX().add();
        /* do other stuff not concerning foo */
        fooHandler.record(foo);
    }
}

class FooHandler {
    void record() {
        if (flagOn)
            doSomething(foo);
    }
}

Quite curious about the subject, do you recommend some pointers about the
JIT capabilities and optimizations techniques ?

Regards,
Pierre.

2009/5/8 Markus Jevring <markus.jevring at petercam.be>

> In your last case, where you say that HotSpot will have multiple
> potential paths to take (one of which examines the nullity higher up in
> the tree), would the method(s) actually get invoked in this path, or
> would it just be pruned based on this condition?
> For example, if we have an aspect oriented solution that relies on
> methods getting invoked, and these methods get pruned, then these
> methods would never get called, and the aspect that is triggered only
> when the methods get called would never run.
> Or could it be that, since some languages (aspectj, for instance)
> "weaves" code in at compile time, there *is* actually a reason to take
> an execution path that would have otherwised gotten pruned?
>
> The reason I ask is: if we have some generic tracing aspect that logs
> method invocations, would the very presence of this aspect, whether it
> does anything or not (logging enabled/disabled) prevent HotSpot from
> pruning the path in question, leading to potentially lower performance
> by forcing HotSpot's hand by disallowing certain optimizations that
> might otherwise be done?
>
> Granted, I don't see aspect oriented programming as something
> necessarily performant, but I thought I'd ask.
>
> Markus Jevring
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Brian
> Goetz
> Sent: jeudi 7 mai 2009 4:38
> To: Christian Vest Hansen
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile/final and the JIT
>
> > Different instances of this class may have different values assigned
> > to the statistics field, but the code itself is static and, I would
> > assume, optimized as such.
>
> This is not true.  There's nothing that says the JIT need only compile
> one
> version of a given method.  HotSpot will readily clone code where it can
> spot
> a slow/fast path, even based on deep inlining.  For example:
>
> foo(Moo x) {
>   goo(x);
> }
>
> goo(Moo x) {
>   if (x != null) { something expensive }
> }
>
> Here, calls to foo() may will be inlined (or inlined-cached), and in the
>
> branch where it is inlined, it can pull the null check all the way out
> to the
> caller, and have two completely different paths based on a condition
> that is
> way far down the tree.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090509/ba899806/attachment-0001.html>

From peter.kitt.reilly at gmail.com  Sat May  9 08:45:03 2009
From: peter.kitt.reilly at gmail.com (Peter Reilly)
Date: Sat, 9 May 2009 13:45:03 +0100
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEGNIBAA.davidcholmes@aapt.net.au>
References: <4A03E47B.3020800@optrak.co.uk>
	<NFBBKALFDCPFIDBNKAPCGEGNIBAA.davidcholmes@aapt.net.au>
Message-ID: <dffc72020905090545n646ba4aek5044b81cb59735d0@mail.gmail.com>

On Sat, May 9, 2009 at 3:26 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Mark Thornton writes:
>> David Holmes wrote:
>> > That bug is (or became) a RFE for the spec to make all accesses to
>> > double/long atomic and that is not going to happen hence the
>> > "will not fix". There are a number of other bugs that pertain
>> > to atomic access to volatile long/double eg: 4247780 which was
>> > fixed back in 1.2.2
>> >
>> >
>> By all accesses I presume you mean including things like ++, which is
>> reasonably well documented as not atomic.
>
> No - "access" is a simple load or store. ++ is an assignment operator that
> involves multiple operations: load, increment, store.
>
> Not sure why this has suddenly stirred up confusion. As Sam Berlin
> re-stated, all accesses to 32-bit types are atomic. The atomicity guarantee
> extends to double and long if they are declared 'volatile'. No assignment
> operators are defined to be atomic.
>
Just to be clear, writing to volatile longs/doubles/references are also atomic

from .above - 17.7 Non-atomic Treatment of double and long
"""
Writes and reads of volatile long and double values are always atomic.
Writes to and reads of references are always atomic, regardless of
whether they are implemented as 32 or 64 bit values.
"""
Peter

>> 4247780 doesn't appear to exist in the bug database.
>
> Sorry about that - that was actually a bug in the ExactVM which was the
> production VM on Solaris. Hotspot has its own bugs with regard to volatile,
> but as has been said the main bug was fixed years ago now.
>
> Cheers,
> David Holmes
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From davidcholmes at aapt.net.au  Sat May  9 20:00:49 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 10 May 2009 10:00:49 +1000
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <dffc72020905090545n646ba4aek5044b81cb59735d0@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHGIBAA.davidcholmes@aapt.net.au>

Peter Reilly writes:
> Just to be clear, writing to volatile longs/doubles/references 
> are also atomic

Accesses to all data types, other than double and long, are always atomic.

Accesses to a double or long are only atomic if the variable is volatile.

David



From brian at briangoetz.com  Sun May 10 17:34:11 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Sun, 10 May 2009 17:34:11 -0400
Subject: [concurrency-interest] Atomic assignment
In-Reply-To: <4A044998.4020801@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEEGGIBAA.davidcholmes@aapt.net.au>	<4A04414C.7060303@cytetech.com>	<19196d860905080750i52d9bc28q9f2b793bb8b51a40@mail.gmail.com>
	<4A044998.4020801@cytetech.com>
Message-ID: <4A074853.3040101@briangoetz.com>

In general, the issue over word tearing for nonvolatile longs/doubles is kind 
of a red herring; word tearing would occur when the variable is shared between 
threads, in which case, you should be synchronizing anyway (otherwise you risk 
stale values.)  So there's very little "special" about thread-safety for 
longs/doubles -- unless you are deliberately writing code with data races, in 
which case there is just one more failure mode.

Gregg Wonderly wrote:
> Okay, it we actually just have the ++/-- not atomic issue, than I am 
> okay with that because I don't suddenly have new, broken code.  I looked 
> around some more and ran the test code on that old issue on my dual core 
> laptop and saw no failures.  But, I'm not sure after looking at that 
> code whether it actually makes modifications in a way that would show a 
> problem.
> 
> Gregg Wonderly
> 
> Sam Berlin wrote:
>> My interpretation is that the scenario today is that a volatile 
>> long/double is atomic.  A non-volatile long/double is not atomic.  And 
>> the RFE that was closed as will-not-fix was asking for all access to 
>> long/double (even non-volatile) to be atomic.
>>
>> Sam
>>
>> On Fri, May 8, 2009 at 10:27 AM, Gregg Wonderly <gregg at cytetech.com 
>> <mailto:gregg at cytetech.com>> wrote:
>>
>>     So will the compiler be changed to not allow volatile to exist on
>>     double/long declarations since it does not work?  Or might there be
>>     a warning that volatile does not produce atomic results on all
>>     statements that assign to or reference a double/long value?
>>
>>     Sigh...
>>
>>     Gregg Wonderly
>>
>>
>>     David Holmes wrote:
>>
>>         Hi Mark,
>>
>>         That bug is (or became) a RFE for the spec to make all 
>> accesses to
>>         double/long atomic and that is not going to happen hence the
>>         "will not fix".
>>         There are a number of other bugs that pertain to atomic access
>>         to volatile
>>         long/double eg: 4247780 which was fixed back in 1.2.2
>>
>>         Thanks
>>         David Holmes
>>
>>             -----Original Message-----
>>             From: concurrency-interest-bounces at cs.oswego.edu
>>             <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>             [mailto:concurrency-interest-bounces at cs.oswego.edu
>>             <mailto:concurrency-interest-bounces at cs.oswego.edu>]On
>>             Behalf Of Mark
>>             Thornton
>>             Sent: Friday, 8 May 2009 7:29 AM
>>             To: concurrency-interest at cs.oswego.edu
>>             <mailto:concurrency-interest at cs.oswego.edu>
>>             Subject: [concurrency-interest] Atomic assignment
>>
>>
>>
>>             I was sure that the problem of (non)atomic assignment to
>>             volatile longs
>>             and doubles had been fixed, but this bug report suggests
>>             otherwise:
>>
>>             http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4023233
>>
>>             Anyone know for sure?
>>
>>             Mark Thornton
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at briangoetz.com  Sun May 10 17:36:38 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Sun, 10 May 2009 17:36:38 -0400
Subject: [concurrency-interest] Volatile/final and the JIT
In-Reply-To: <FC86FB1EEFB8994382A386021951CA900CD6C6DF@mail.petercam.corp>
References: <1466c1d60905050402p2364a3dao4440ff96f909bebe@mail.gmail.com><90622e530905050617n1ee53486pb72ae5922bf21bd7@mail.gmail.com>
	<4A024977.7010009@briangoetz.com>
	<FC86FB1EEFB8994382A386021951CA900CD6C6DF@mail.petercam.corp>
Message-ID: <4A0748E6.4020507@briangoetz.com>

AOP works at the bytecode level, so if you were weaving in aspects, the code 
that HotSpot would see would be the post-woven code.

Adding generic tracing aspects has all the same distortive effects as 
injecting the tracing into the source code; adding more code into the source 
can definitely distort how HotSpot will JIT the code, so the AOP version could 
do the same.

Markus Jevring wrote:
> In your last case, where you say that HotSpot will have multiple
> potential paths to take (one of which examines the nullity higher up in
> the tree), would the method(s) actually get invoked in this path, or
> would it just be pruned based on this condition?
> For example, if we have an aspect oriented solution that relies on
> methods getting invoked, and these methods get pruned, then these
> methods would never get called, and the aspect that is triggered only
> when the methods get called would never run.
> Or could it be that, since some languages (aspectj, for instance)
> "weaves" code in at compile time, there *is* actually a reason to take
> an execution path that would have otherwised gotten pruned?
> 
> The reason I ask is: if we have some generic tracing aspect that logs
> method invocations, would the very presence of this aspect, whether it
> does anything or not (logging enabled/disabled) prevent HotSpot from
> pruning the path in question, leading to potentially lower performance
> by forcing HotSpot's hand by disallowing certain optimizations that
> might otherwise be done?
> 
> Granted, I don't see aspect oriented programming as something
> necessarily performant, but I thought I'd ask.
> 
> Markus Jevring
> 
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Brian
> Goetz
> Sent: jeudi 7 mai 2009 4:38
> To: Christian Vest Hansen
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile/final and the JIT
> 
>> Different instances of this class may have different values assigned
>> to the statistics field, but the code itself is static and, I would
>> assume, optimized as such. 
> 
> This is not true.  There's nothing that says the JIT need only compile
> one 
> version of a given method.  HotSpot will readily clone code where it can
> spot 
> a slow/fast path, even based on deep inlining.  For example:
> 
> foo(Moo x) {
>    goo(x);
> }
> 
> goo(Moo x) {
>    if (x != null) { something expensive }
> }
> 
> Here, calls to foo() may will be inlined (or inlined-cached), and in the
> 
> branch where it is inlined, it can pull the null check all the way out
> to the 
> caller, and have two completely different paths based on a condition
> that is 
> way far down the tree.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From alexdmiller at yahoo.com  Mon May 11 01:40:22 2009
From: alexdmiller at yahoo.com (Alex Miller)
Date: Sun, 10 May 2009 22:40:22 -0700 (PDT)
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
Message-ID: <225670.73847.qm@web32205.mail.mud.yahoo.com>


This is maybe a better question for the jmm list, but I'll start here and you can tell me to stuff it if so.

If you have 2 unbounded LBQs:

  LinkedBlockingQueue queueA = new LinkedBlockingQueue();
  LinkedBlockingQueue queueB = new LinkedBlockingQueue();

and 1 producer thread puts objects on queue A then queue B (no other synchronization):

  queueA.put(obj1);
  queueB.put(obj2);

and you have a consumer that reads those queues (no other synchronization):
  
  Object seen1 = queueA.take();
  Object seen2 = queueB.remove();

Is
it possible to see NoSuchElementException from the final remove()?  I
think this boils down to a JMM question of whether the puts
(which are changing the queues on separate internal locks but are
sequentially ordered in the producer thread) *guarantee* an ordering in the consumer thread such that obj1 must show up on queueA before obj2 on
queueB.  

My intuitive answer is yes but the actual answer might be that this is merely likely, not guaranteed.  

Thanks...
Alex

From davidcholmes at aapt.net.au  Mon May 11 01:58:03 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 11 May 2009 15:58:03 +1000
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <225670.73847.qm@web32205.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHLIBAA.davidcholmes@aapt.net.au>

Hi Alex,

JMM visibility and ordering issues aside, you can easily get
NoSuchElementException if the interleaving is as follows:

       Thread 1                 Thread 2
   queueA.put(obj1);
                      Object seen1 = queueA.take();
                      Object seen2 = queueB.remove();
   queueB.put(obj2);


David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
> Miller
> Sent: Monday, 11 May 2009 3:40 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Ordered puts and takes
>
>
>
>
> This is maybe a better question for the jmm list, but I'll start
> here and you can tell me to stuff it if so.
>
> If you have 2 unbounded LBQs:
>
>   LinkedBlockingQueue queueA = new LinkedBlockingQueue();
>   LinkedBlockingQueue queueB = new LinkedBlockingQueue();
>
> and 1 producer thread puts objects on queue A then queue B (no
> other synchronization):
>
>   queueA.put(obj1);
>   queueB.put(obj2);
>
> and you have a consumer that reads those queues (no other
> synchronization):
>
>   Object seen1 = queueA.take();
>   Object seen2 = queueB.remove();
>
> Is
> it possible to see NoSuchElementException from the final remove()?  I
> think this boils down to a JMM question of whether the puts
> (which are changing the queues on separate internal locks but are
> sequentially ordered in the producer thread) *guarantee* an
> ordering in the consumer thread such that obj1 must show up on
> queueA before obj2 on
> queueB.
>
> My intuitive answer is yes but the actual answer might be that
> this is merely likely, not guaranteed.
>
> Thanks...
> Alex
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From joe.bowbeer at gmail.com  Mon May 11 02:54:39 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 10 May 2009 23:54:39 -0700
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <225670.73847.qm@web32205.mail.mud.yahoo.com>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
	<225670.73847.qm@web32205.mail.mud.yahoo.com>
Message-ID: <31f2a7bd0905102354h6dfbbc93ve7fd9eba7661d382@mail.gmail.com>

Perhaps you meant:

Thread 1:

/* a */  queueA.put(obj1);
/* b */  queueB.put(obj2);

Thread 2:

/* c */  queueB.take();
/* d */  queueA.remove();

The BlockingQueue javadoc defines its memory consistency effects, making
this fair game for the c-i list:

"Actions in a thread prior to placing an object into a BlockingQueue
happen-before actions subsequent to the access or removal of that element
from the BlockingQueue in another thread."

Connecting the dots:

a happens before b (Thread 1 program order)
b happens before c (BlockingQueue spec)
c happens before d (Thread 2 program order)

Therefore a happens before d, and remove will not throw
NoSuchElementException.

Joe

On Sun, May 10, 2009 at 10:40 PM, Alex Miller wrote:

>
> This is maybe a better question for the jmm list, but I'll start here and
> you can tell me to stuff it if so.
>
> If you have 2 unbounded LBQs:
>
>  LinkedBlockingQueue queueA = new LinkedBlockingQueue();
>  LinkedBlockingQueue queueB = new LinkedBlockingQueue();
>
> and 1 producer thread puts objects on queue A then queue B (no other
> synchronization):
>
>  queueA.put(obj1);
>  queueB.put(obj2);
>
> and you have a consumer that reads those queues (no other synchronization):
>
>  Object seen1 = queueA.take();
>  Object seen2 = queueB.remove();
>
> Is
> it possible to see NoSuchElementException from the final remove()?  I
> think this boils down to a JMM question of whether the puts
> (which are changing the queues on separate internal locks but are
> sequentially ordered in the producer thread) *guarantee* an ordering in the
> consumer thread such that obj1 must show up on queueA before obj2 on
> queueB.
>
> My intuitive answer is yes but the actual answer might be that this is
> merely likely, not guaranteed.
>
> Thanks...
> Alex
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090510/961ac74d/attachment.html>

From stinkyminky at gmail.com  Mon May 11 08:23:52 2009
From: stinkyminky at gmail.com (Michael Lee)
Date: Mon, 11 May 2009 08:23:52 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	1
In-Reply-To: <63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
	<76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>
	<63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>
Message-ID: <77ce5b7d0905110523g4b7d5c7dk6b862af132402a11@mail.gmail.com>

Can I assume that read/write volatile variable is expensive?

I was told that it depends on the hardware platform but I am getting
conflicting information.  In the old version JSR133 FAQ, it said that
volatile is expensive.  But now it has been corrected to say it is not
expensive.

What kind of overhead do I expect to see with using volatile?  I know
that we should not assume until we measure but it will nice to know
the trade-off using volatile / synchronization when creating a new
class...

Thanks

- Michael

On Sun, May 3, 2009 at 8:24 AM, Tim Peierls <tim at peierls.net> wrote:
> Reads and writes of volatiles have memory effects under the JMM. The idiom
> described in EJ2e, Item 71 minimizes the number of volatile reads and writes
> through the use of a temporary variable.?Josh Bloch says (in Item 71) that
> on his machine this code was 25 percent faster than the obvious version with
> no temporary variable. Item 71 also has good advice about when the use of
> this idiom is appropriate.
> --tim
>
> On Sun, May 3, 2009 at 6:12 AM, Bharath Ravi Kumar <reachbach at gmail.com>
> wrote:
>>
>> Hi Tim,
>>
>> How does the local variable tmp help? Why not carry out a null checks on
>> (and subsequent assignment to) resource directly? Does the usage of tmp
>> reduce contention in any manner? Could you please explain?
>>
>> Thanks,
>> Bharath
>>
>>
>>>
>>> From: Tim Peierls <tim at peierls.net>
>>> Subject: Re: [concurrency-interest] Reordering clarification in DCL
>>> ? ? ? ?example
>>> To: Carol Saah <csaah at cox.net>
>>> Cc: concurrency-interest at cs.oswego.edu
>>> Message-ID:
>>> ? ? ? ?<63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
>>> Content-Type: text/plain; charset="iso-8859-1"
>>>
>>> See more recent discussions in Java Concurrency in Practice, Section
>>> 16.2.4
>>> and Effective Java, 2nd edition, Item 71.
>>> The fixed version of the code is:
>>>
>>> class SomeClass {
>>> ?private volatile Resource resource;
>>>
>>> ?public Resource getResource() {
>>> ? ?Resource tmp = resource;
>>> ? ?if (tmp == null) {
>>> ? ? ?synchronized (this) {
>>> ? ? ? ?tmp = resource;
>>> ? ? ? ?if (tmp == null)
>>> ? ? ? ? ?resource = tmp = new Resource();
>>> ? ? ?}
>>> ? ?}
>>> ? ?return tmp;
>>> ?}
>>> }
>>>
>>> --tim
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From gregg at cytetech.com  Mon May 11 11:44:30 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 11 May 2009 10:44:30 -0500
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEHLIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEHLIBAA.davidcholmes@aapt.net.au>
Message-ID: <4A0847DE.1060307@cytetech.com>

and the easy fix for this is

qA.put();
qB.put();

... in the other thread ...

qB.take();
qA.take();

so that you always block, waiting for the last event that you need to have 
happen of multiple and then you get the implied ordering of operations needed.

The queue operations take care of visibility themselves.

Gregg Wonderly

David Holmes wrote:
> Hi Alex,
> 
> JMM visibility and ordering issues aside, you can easily get
> NoSuchElementException if the interleaving is as follows:
> 
>        Thread 1                 Thread 2
>    queueA.put(obj1);
>                       Object seen1 = queueA.take();
>                       Object seen2 = queueB.remove();
>    queueB.put(obj2);
> 
> 
> David Holmes
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
>> Miller
>> Sent: Monday, 11 May 2009 3:40 PM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Ordered puts and takes
>>
>>
>>
>>
>> This is maybe a better question for the jmm list, but I'll start
>> here and you can tell me to stuff it if so.
>>
>> If you have 2 unbounded LBQs:
>>
>>   LinkedBlockingQueue queueA = new LinkedBlockingQueue();
>>   LinkedBlockingQueue queueB = new LinkedBlockingQueue();
>>
>> and 1 producer thread puts objects on queue A then queue B (no
>> other synchronization):
>>
>>   queueA.put(obj1);
>>   queueB.put(obj2);
>>
>> and you have a consumer that reads those queues (no other
>> synchronization):
>>
>>   Object seen1 = queueA.take();
>>   Object seen2 = queueB.remove();
>>
>> Is
>> it possible to see NoSuchElementException from the final remove()?  I
>> think this boils down to a JMM question of whether the puts
>> (which are changing the queues on separate internal locks but are
>> sequentially ordered in the producer thread) *guarantee* an
>> ordering in the consumer thread such that obj1 must show up on
>> queueA before obj2 on
>> queueB.
>>
>> My intuitive answer is yes but the actual answer might be that
>> this is merely likely, not guaranteed.
>>
>> Thanks...
>> Alex
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From alexdmiller at yahoo.com  Mon May 11 12:01:11 2009
From: alexdmiller at yahoo.com (Alex Miller)
Date: Mon, 11 May 2009 09:01:11 -0700 (PDT)
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <4A0847DE.1060307@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCMEHLIBAA.davidcholmes@aapt.net.au>
	<4A0847DE.1060307@cytetech.com>
Message-ID: <316655.32515.qm@web32203.mail.mud.yahoo.com>


Sorry, as Joe surmised, that is actually what I meant.  Note to self: don't email concurrency list at 1 am.... ;)



----- Original Message ----
> From: Gregg Wonderly <gregg at cytetech.com>
> To: dholmes at ieee.org
> Cc: Alex Miller <alexdmiller at yahoo.com>; concurrency-interest at cs.oswego.edu
> Sent: Monday, May 11, 2009 10:44:30 AM
> Subject: Re: [concurrency-interest] Ordered puts and takes
> 
> and the easy fix for this is
> 
> qA.put();
> qB.put();
> 
> ... in the other thread ...
> 
> qB.take();
> qA.take();
> 
> so that you always block, waiting for the last event that you need to have 
> happen of multiple and then you get the implied ordering of operations needed.
> 
> The queue operations take care of visibility themselves.
> 
> Gregg Wonderly
> 
> David Holmes wrote:
> > Hi Alex,
> > 
> > JMM visibility and ordering issues aside, you can easily get
> > NoSuchElementException if the interleaving is as follows:
> > 
> >        Thread 1                 Thread 2
> >    queueA.put(obj1);
> >                       Object seen1 = queueA.take();
> >                       Object seen2 = queueB.remove();
> >    queueB.put(obj2);
> > 
> > 
> > David Holmes
> > 
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
> >> Miller
> >> Sent: Monday, 11 May 2009 3:40 PM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] Ordered puts and takes
> >>
> >>
> >>
> >>
> >> This is maybe a better question for the jmm list, but I'll start
> >> here and you can tell me to stuff it if so.
> >>
> >> If you have 2 unbounded LBQs:
> >>
> >>   LinkedBlockingQueue queueA = new LinkedBlockingQueue();
> >>   LinkedBlockingQueue queueB = new LinkedBlockingQueue();
> >>
> >> and 1 producer thread puts objects on queue A then queue B (no
> >> other synchronization):
> >>
> >>   queueA.put(obj1);
> >>   queueB.put(obj2);
> >>
> >> and you have a consumer that reads those queues (no other
> >> synchronization):
> >>
> >>   Object seen1 = queueA.take();
> >>   Object seen2 = queueB.remove();
> >>
> >> Is
> >> it possible to see NoSuchElementException from the final remove()?  I
> >> think this boils down to a JMM question of whether the puts
> >> (which are changing the queues on separate internal locks but are
> >> sequentially ordered in the producer thread) *guarantee* an
> >> ordering in the consumer thread such that obj1 must show up on
> >> queueA before obj2 on
> >> queueB.
> >>
> >> My intuitive answer is yes but the actual answer might be that
> >> this is merely likely, not guaranteed.
> >>
> >> Thanks...
> >> Alex
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > 
> > 
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > 
> > 


From jim.andreou at gmail.com  Mon May 11 12:25:23 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Mon, 11 May 2009 19:25:23 +0300
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <225670.73847.qm@web32205.mail.mud.yahoo.com>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
	<225670.73847.qm@web32205.mail.mud.yahoo.com>
Message-ID: <7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>

2009/5/11 Alex Miller <alexdmiller at yahoo.com>:
>
> This is maybe a better question for the jmm list, but I'll start here and you can tell me to stuff it if so.
>
> If you have 2 unbounded LBQs:
>
> ?LinkedBlockingQueue queueA = new LinkedBlockingQueue();
> ?LinkedBlockingQueue queueB = new LinkedBlockingQueue();
>
> and 1 producer thread puts objects on queue A then queue B (no other synchronization):
>
> ?queueA.put(obj1);
> ?queueB.put(obj2);
>
> and you have a consumer that reads those queues (no other synchronization):
>
> ?Object seen1 = queueA.take();
> ?Object seen2 = queueB.remove();
>
> Is
> it possible to see NoSuchElementException from the final remove()? ?I
> think this boils down to a JMM question of whether the puts
> (which are changing the queues on separate internal locks but are
> sequentially ordered in the producer thread) *guarantee* an ordering in the consumer thread such that obj1 must show up on queueA before obj2 on
> queueB.

I don't see a guarantee that the consumer will observe the program
order of the producer. The two puts can be reordered if this
reordering can't be observed in the producer thread, so the consumer
may see the "wrong" order.

Also, think that the producer could be preempted before putting to the
second queue.

I think that if you reordered the consumer like this:

Object seen2 = queueB.take();
Object seen1 = queueA.remove();

Then you would have the intented ordering. If you see the second
object, then the put on the queueB happened-before, and the put on
queueA happened-before that because it was earlier in program order.

>
> My intuitive answer is yes but the actual answer might be that this is merely likely, not guaranteed.
>
> Thanks...
> Alex
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jim.andreou at gmail.com  Mon May 11 12:26:46 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Mon, 11 May 2009 19:26:46 +0300
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
	<225670.73847.qm@web32205.mail.mud.yahoo.com>
	<7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>
Message-ID: <7d7138c10905110926k26b1de61w733cb33c31dfc55f@mail.gmail.com>

Wow, sorry about that. I somehow didn't see the earlier replies.

2009/5/11 Jim Andreou <jim.andreou at gmail.com>:
> 2009/5/11 Alex Miller <alexdmiller at yahoo.com>:
>>
>> This is maybe a better question for the jmm list, but I'll start here and you can tell me to stuff it if so.
>>
>> If you have 2 unbounded LBQs:
>>
>> ?LinkedBlockingQueue queueA = new LinkedBlockingQueue();
>> ?LinkedBlockingQueue queueB = new LinkedBlockingQueue();
>>
>> and 1 producer thread puts objects on queue A then queue B (no other synchronization):
>>
>> ?queueA.put(obj1);
>> ?queueB.put(obj2);
>>
>> and you have a consumer that reads those queues (no other synchronization):
>>
>> ?Object seen1 = queueA.take();
>> ?Object seen2 = queueB.remove();
>>
>> Is
>> it possible to see NoSuchElementException from the final remove()? ?I
>> think this boils down to a JMM question of whether the puts
>> (which are changing the queues on separate internal locks but are
>> sequentially ordered in the producer thread) *guarantee* an ordering in the consumer thread such that obj1 must show up on queueA before obj2 on
>> queueB.
>
> I don't see a guarantee that the consumer will observe the program
> order of the producer. The two puts can be reordered if this
> reordering can't be observed in the producer thread, so the consumer
> may see the "wrong" order.
>
> Also, think that the producer could be preempted before putting to the
> second queue.
>
> I think that if you reordered the consumer like this:
>
> Object seen2 = queueB.take();
> Object seen1 = queueA.remove();
>
> Then you would have the intented ordering. If you see the second
> object, then the put on the queueB happened-before, and the put on
> queueA happened-before that because it was earlier in program order.
>
>>
>> My intuitive answer is yes but the actual answer might be that this is merely likely, not guaranteed.
>>
>> Thanks...
>> Alex
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From hans.boehm at hp.com  Mon May 11 15:42:55 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 11 May 2009 19:42:55 +0000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	1
In-Reply-To: <77ce5b7d0905110523g4b7d5c7dk6b862af132402a11@mail.gmail.com>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
	<76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>
	<63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>
	<77ce5b7d0905110523g4b7d5c7dk6b862af132402a11@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042476F907FA5@GVW0436EXB.americas.hpqcorp.net>

This is highly hardware-specific.  In most cases, volatile accesses should be appreciably cheaper than acquiring and releasing a lock.  (Since they can by implemented by acquiring and releasing a lock, they certainly shouldn't be appreciably worse.)

On X86, a volatile load should cost roughly the same as an ordinary load, but a volatile store is appreciably more expensive, and will probably cost close to the time it takes to acquire a lock.  But even there, cycle counts for volatile stores will differ by up to a decimal order of magnitude (roughly 10 to 100 without cache misses) depending on the specific processor microarchitecture.  And there may be hope that this overhead might decrease in the future, since most current hardware ISAs don't appear to be a great match for recently redesigned programming language memory models.  (Disclaimer: I'm not a hardware architect, and don't really know what they're up to.)

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Michael Lee
> Sent: Monday, May 11, 2009 5:24 AM
> To: Tim Peierls
> Cc: Bharath Ravi Kumar; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Concurrency-interest 
> Digest, Vol 52, Issue 1
> 
> Can I assume that read/write volatile variable is expensive?
> 
> I was told that it depends on the hardware platform but I am 
> getting conflicting information.  In the old version JSR133 
> FAQ, it said that volatile is expensive.  But now it has been 
> corrected to say it is not expensive.
> 
> What kind of overhead do I expect to see with using volatile? 
>  I know that we should not assume until we measure but it 
> will nice to know the trade-off using volatile / 
> synchronization when creating a new class...
> 
> Thanks
> 
> - Michael
> 
> On Sun, May 3, 2009 at 8:24 AM, Tim Peierls <tim at peierls.net> wrote:
> > Reads and writes of volatiles have memory effects under the 
> JMM. The 
> > idiom described in EJ2e, Item 71 minimizes the number of volatile 
> > reads and writes through the use of a temporary variable.?
> Josh Bloch 
> > says (in Item 71) that on his machine this code was 25 
> percent faster 
> > than the obvious version with no temporary variable. Item 
> 71 also has 
> > good advice about when the use of this idiom is appropriate.
> > --tim
> >
> > On Sun, May 3, 2009 at 6:12 AM, Bharath Ravi Kumar 
> > <reachbach at gmail.com>
> > wrote:
> >>
> >> Hi Tim,
> >>
> >> How does the local variable tmp help? Why not carry out a 
> null checks 
> >> on (and subsequent assignment to) resource directly? Does 
> the usage 
> >> of tmp reduce contention in any manner? Could you please explain?
> >>
> >> Thanks,
> >> Bharath
> >>
> >>
> >>>
> >>> From: Tim Peierls <tim at peierls.net>
> >>> Subject: Re: [concurrency-interest] Reordering 
> clarification in DCL
> >>> ? ? ? ?example
> >>> To: Carol Saah <csaah at cox.net>
> >>> Cc: concurrency-interest at cs.oswego.edu
> >>> Message-ID:
> >>> ? ? ? ?
> <63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
> >>> Content-Type: text/plain; charset="iso-8859-1"
> >>>
> >>> See more recent discussions in Java Concurrency in 
> Practice, Section
> >>> 16.2.4
> >>> and Effective Java, 2nd edition, Item 71.
> >>> The fixed version of the code is:
> >>>
> >>> class SomeClass {
> >>> ?private volatile Resource resource;
> >>>
> >>> ?public Resource getResource() {
> >>> ? ?Resource tmp = resource;
> >>> ? ?if (tmp == null) {
> >>> ? ? ?synchronized (this) {
> >>> ? ? ? ?tmp = resource;
> >>> ? ? ? ?if (tmp == null)
> >>> ? ? ? ? ?resource = tmp = new Resource();
> >>> ? ? ?}
> >>> ? ?}
> >>> ? ?return tmp;
> >>> ?}
> >>> }
> >>>
> >>> --tim
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From ashwin.jayaprakash at gmail.com  Mon May 11 22:08:54 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 11 May 2009 19:08:54 -0700
Subject: [concurrency-interest] Potential recursive failure of
	ThreadPoolExecutor.DiscardOldestPolicy?
Message-ID: <837c23d40905111908h343c141eyf7e4942df950ae6e@mail.gmail.com>

I was looking at the source of ThreadPoolExecutor.DiscardOldestPolicy which
seems to have a rather naive way of running the rejected tasks - at least it
seems to me like that.

If the incoming jobs arrive at a faster rate than the rate at which the job
queue gets cleared, won't the rejected task policy potentially cause a
recursive stack overflow? If so, shouldn't the documentation warn us about
such things?

DiscardOldestPolicy code snippet:
    public static class DiscardOldestPolicy implements
RejectedExecutionHandler {
     .. .. .. ..
        public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
            if (!e.isShutdown()) {
                e.getQueue().poll();
                e.execute(r);
            }
        }
    }


ThreadPoolExecutor code snippet:
    public void execute(Runnable command) {
        .. .. ....
        for (;;) {
            if (runState != RUNNING) {
                reject(command);
                return;
            }
            if (poolSize < corePoolSize && addIfUnderCorePoolSize(command))
                return;
            if (workQueue.offer(command))
                return;
            Runnable r = addIfUnderMaximumPoolSize(command);
            if (r == command)
                return;
            if (r == null) {
                reject(command);
                return;
            }
            // else retry
        }
    }

Ashwin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090511/cea54174/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon May 11 23:17:18 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 May 2009 13:17:18 +1000
Subject: [concurrency-interest] Potential recursive failure
	ofThreadPoolExecutor.DiscardOldestPolicy?
In-Reply-To: <837c23d40905111908h343c141eyf7e4942df950ae6e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEICIBAA.davidcholmes@aapt.net.au>

Ashwin,

The DiscardOldest policy doesn't try to run rejected tasks, it tries to make
room and re-submit them. But yes, this is done in a recursive manner (there
is no other way to do it directly) and so it is possible that the space made
is consumed by another submitter, and so you could get a StackOverflowError.

I agree the recursive aspect of this could be documented more clearly.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Tuesday, 12 May 2009 12:09 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Potential recursive failure
ofThreadPoolExecutor.DiscardOldestPolicy?


  I was looking at the source of ThreadPoolExecutor.DiscardOldestPolicy
which seems to have a rather naive way of running the rejected tasks - at
least it seems to me like that.

  If the incoming jobs arrive at a faster rate than the rate at which the
job queue gets cleared, won't the rejected task policy potentially cause a
recursive stack overflow? If so, shouldn't the documentation warn us about
such things?

  DiscardOldestPolicy code snippet:
      public static class DiscardOldestPolicy implements
RejectedExecutionHandler {
       .. .. .. ..
          public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
              if (!e.isShutdown()) {
                  e.getQueue().poll();
                  e.execute(r);
              }
          }
      }


  ThreadPoolExecutor code snippet:
      public void execute(Runnable command) {
          .. .. ....
          for (;;) {
              if (runState != RUNNING) {
                  reject(command);
                  return;
              }
              if (poolSize < corePoolSize &&
addIfUnderCorePoolSize(command))
                  return;
              if (workQueue.offer(command))
                  return;
              Runnable r = addIfUnderMaximumPoolSize(command);
              if (r == command)
                  return;
              if (r == null) {
                  reject(command);
                  return;
              }
              // else retry
          }
      }

  Ashwin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090512/c50c1202/attachment.html>

From brian at briangoetz.com  Tue May 12 03:13:09 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Tue, 12 May 2009 03:13:09 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
 Issue 	1
In-Reply-To: <238A96A773B3934685A7269CC8A8D042476F907FA5@GVW0436EXB.americas.hpqcorp.net>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>	<76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>	<63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>	<77ce5b7d0905110523g4b7d5c7dk6b862af132402a11@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042476F907FA5@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <4A092185.7020906@briangoetz.com>

Or, to simplify further: volatile reads cheap, volatile writes expensive.

Boehm, Hans wrote:
> This is highly hardware-specific.  In most cases, volatile accesses should be appreciably cheaper than acquiring and releasing a lock.  (Since they can by implemented by acquiring and releasing a lock, they certainly shouldn't be appreciably worse.)
> 
> On X86, a volatile load should cost roughly the same as an ordinary load, but a volatile store is appreciably more expensive, and will probably cost close to the time it takes to acquire a lock.  But even there, cycle counts for volatile stores will differ by up to a decimal order of magnitude (roughly 10 to 100 without cache misses) depending on the specific processor microarchitecture.  And there may be hope that this overhead might decrease in the future, since most current hardware ISAs don't appear to be a great match for recently redesigned programming language memory models.  (Disclaimer: I'm not a hardware architect, and don't really know what they're up to.)
> 
> Hans
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu 
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
>> Of Michael Lee
>> Sent: Monday, May 11, 2009 5:24 AM
>> To: Tim Peierls
>> Cc: Bharath Ravi Kumar; concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Concurrency-interest 
>> Digest, Vol 52, Issue 1
>>
>> Can I assume that read/write volatile variable is expensive?
>>
>> I was told that it depends on the hardware platform but I am 
>> getting conflicting information.  In the old version JSR133 
>> FAQ, it said that volatile is expensive.  But now it has been 
>> corrected to say it is not expensive.
>>
>> What kind of overhead do I expect to see with using volatile? 
>>  I know that we should not assume until we measure but it 
>> will nice to know the trade-off using volatile / 
>> synchronization when creating a new class...
>>
>> Thanks
>>
>> - Michael
>>
>> On Sun, May 3, 2009 at 8:24 AM, Tim Peierls <tim at peierls.net> wrote:
>>> Reads and writes of volatiles have memory effects under the 
>> JMM. The 
>>> idiom described in EJ2e, Item 71 minimizes the number of volatile 
>>> reads and writes through the use of a temporary variable. 
>> Josh Bloch 
>>> says (in Item 71) that on his machine this code was 25 
>> percent faster 
>>> than the obvious version with no temporary variable. Item 
>> 71 also has 
>>> good advice about when the use of this idiom is appropriate.
>>> --tim
>>>
>>> On Sun, May 3, 2009 at 6:12 AM, Bharath Ravi Kumar 
>>> <reachbach at gmail.com>
>>> wrote:
>>>> Hi Tim,
>>>>
>>>> How does the local variable tmp help? Why not carry out a 
>> null checks 
>>>> on (and subsequent assignment to) resource directly? Does 
>> the usage 
>>>> of tmp reduce contention in any manner? Could you please explain?
>>>>
>>>> Thanks,
>>>> Bharath
>>>>
>>>>
>>>>> From: Tim Peierls <tim at peierls.net>
>>>>> Subject: Re: [concurrency-interest] Reordering 
>> clarification in DCL
>>>>>        example
>>>>> To: Carol Saah <csaah at cox.net>
>>>>> Cc: concurrency-interest at cs.oswego.edu
>>>>> Message-ID:
>>>>>        
>> <63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
>>>>> Content-Type: text/plain; charset="iso-8859-1"
>>>>>
>>>>> See more recent discussions in Java Concurrency in 
>> Practice, Section
>>>>> 16.2.4
>>>>> and Effective Java, 2nd edition, Item 71.
>>>>> The fixed version of the code is:
>>>>>
>>>>> class SomeClass {
>>>>>  private volatile Resource resource;
>>>>>
>>>>>  public Resource getResource() {
>>>>>    Resource tmp = resource;
>>>>>    if (tmp == null) {
>>>>>      synchronized (this) {
>>>>>        tmp = resource;
>>>>>        if (tmp == null)
>>>>>          resource = tmp = new Resource();
>>>>>      }
>>>>>    }
>>>>>    return tmp;
>>>>>  }
>>>>> }
>>>>>
>>>>> --tim
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From davidcholmes at aapt.net.au  Tue May 12 03:22:14 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 May 2009 17:22:14 +1000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
	Issue 	1
In-Reply-To: <4A092185.7020906@briangoetz.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIDIBAA.davidcholmes@aapt.net.au>

But in the context of choosing volatile over synchronized, volatile can be
cheaper because synchronized always has the same memory synchronization
affects as a volatile-read _and_ a volatile-write.

However, for cases where volatile is an alternative to synchronized, it is
contention costs that would be the dominant consideration.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Brian
> Goetz
> Sent: Tuesday, 12 May 2009 5:13 PM
> To: Boehm, Hans
> Cc: Bharath Ravi Kumar; concurrency-interest at cs.oswego.edu; Tim Peierls
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol 52,
> Issue 1
>
>
>
> Or, to simplify further: volatile reads cheap, volatile writes expensive.
>
> Boehm, Hans wrote:
> > This is highly hardware-specific.  In most cases, volatile
> accesses should be appreciably cheaper than acquiring and
> releasing a lock.  (Since they can by implemented by acquiring
> and releasing a lock, they certainly shouldn't be appreciably worse.)
> >
> > On X86, a volatile load should cost roughly the same as an
> ordinary load, but a volatile store is appreciably more
> expensive, and will probably cost close to the time it takes to
> acquire a lock.  But even there, cycle counts for volatile stores
> will differ by up to a decimal order of magnitude (roughly 10 to
> 100 without cache misses) depending on the specific processor
> microarchitecture.  And there may be hope that this overhead
> might decrease in the future, since most current hardware ISAs
> don't appear to be a great match for recently redesigned
> programming language memory models.  (Disclaimer: I'm not a
> hardware architect, and don't really know what they're up to.)
> >
> > Hans
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> >> Of Michael Lee
> >> Sent: Monday, May 11, 2009 5:24 AM
> >> To: Tim Peierls
> >> Cc: Bharath Ravi Kumar; concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] Concurrency-interest
> >> Digest, Vol 52, Issue 1
> >>
> >> Can I assume that read/write volatile variable is expensive?
> >>
> >> I was told that it depends on the hardware platform but I am
> >> getting conflicting information.  In the old version JSR133
> >> FAQ, it said that volatile is expensive.  But now it has been
> >> corrected to say it is not expensive.
> >>
> >> What kind of overhead do I expect to see with using volatile?
> >>  I know that we should not assume until we measure but it
> >> will nice to know the trade-off using volatile /
> >> synchronization when creating a new class...
> >>
> >> Thanks
> >>
> >> - Michael
> >>
> >> On Sun, May 3, 2009 at 8:24 AM, Tim Peierls <tim at peierls.net> wrote:
> >>> Reads and writes of volatiles have memory effects under the
> >> JMM. The
> >>> idiom described in EJ2e, Item 71 minimizes the number of volatile
> >>> reads and writes through the use of a temporary variable.
> >> Josh Bloch
> >>> says (in Item 71) that on his machine this code was 25
> >> percent faster
> >>> than the obvious version with no temporary variable. Item
> >> 71 also has
> >>> good advice about when the use of this idiom is appropriate.
> >>> --tim
> >>>
> >>> On Sun, May 3, 2009 at 6:12 AM, Bharath Ravi Kumar
> >>> <reachbach at gmail.com>
> >>> wrote:
> >>>> Hi Tim,
> >>>>
> >>>> How does the local variable tmp help? Why not carry out a
> >> null checks
> >>>> on (and subsequent assignment to) resource directly? Does
> >> the usage
> >>>> of tmp reduce contention in any manner? Could you please explain?
> >>>>
> >>>> Thanks,
> >>>> Bharath
> >>>>
> >>>>
> >>>>> From: Tim Peierls <tim at peierls.net>
> >>>>> Subject: Re: [concurrency-interest] Reordering
> >> clarification in DCL
> >>>>>        example
> >>>>> To: Carol Saah <csaah at cox.net>
> >>>>> Cc: concurrency-interest at cs.oswego.edu
> >>>>> Message-ID:
> >>>>>
> >> <63b4e4050905020938u5c4ba444vec35d2ef003a5d11 at mail.gmail.com>
> >>>>> Content-Type: text/plain; charset="iso-8859-1"
> >>>>>
> >>>>> See more recent discussions in Java Concurrency in
> >> Practice, Section
> >>>>> 16.2.4
> >>>>> and Effective Java, 2nd edition, Item 71.
> >>>>> The fixed version of the code is:
> >>>>>
> >>>>> class SomeClass {
> >>>>>  private volatile Resource resource;
> >>>>>
> >>>>>  public Resource getResource() {
> >>>>>    Resource tmp = resource;
> >>>>>    if (tmp == null) {
> >>>>>      synchronized (this) {
> >>>>>        tmp = resource;
> >>>>>        if (tmp == null)
> >>>>>          resource = tmp = new Resource();
> >>>>>      }
> >>>>>    }
> >>>>>    return tmp;
> >>>>>  }
> >>>>> }
> >>>>>
> >>>>> --tim
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From karmazilla at gmail.com  Tue May 12 03:50:58 2009
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Tue, 12 May 2009 09:50:58 +0200
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
	<225670.73847.qm@web32205.mail.mud.yahoo.com>
	<7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>
Message-ID: <90622e530905120050v44d41967pa41a8e4363a61b39@mail.gmail.com>

On Mon, May 11, 2009 at 6:25 PM, Jim Andreou <jim.andreou at gmail.com> wrote:
> I don't see a guarantee that the consumer will observe the program
> order of the producer. The two puts can be reordered if this
> reordering can't be observed in the producer thread, so the consumer
> may see the "wrong" order.

Correct me if I'm wrong, but...

LBQ.put has visibility semantics and reordering is therefor not
allowed, no? Just like volatile and synchronized cannot be reordered.

>
> Also, think that the producer could be preempted before putting to the
> second queue.
>
> I think that if you reordered the consumer like this:
>
> Object seen2 = queueB.take();
> Object seen1 = queueA.remove();
>
> Then you would have the intented ordering. If you see the second
> object, then the put on the queueB happened-before, and the put on
> queueA happened-before that because it was earlier in program order.
>
>>
>> My intuitive answer is yes but the actual answer might be that this is merely likely, not guaranteed.
>>
>> Thanks...
>> Alex
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

From jim.andreou at gmail.com  Tue May 12 04:14:03 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Tue, 12 May 2009 11:14:03 +0300
Subject: [concurrency-interest] Ordered puts and takes
In-Reply-To: <90622e530905120050v44d41967pa41a8e4363a61b39@mail.gmail.com>
References: <mailman.3.1241971201.26414.concurrency-interest@cs.oswego.edu>
	<225670.73847.qm@web32205.mail.mud.yahoo.com>
	<7d7138c10905110925h21c27a8fm4fbaf65cbb47a02@mail.gmail.com>
	<90622e530905120050v44d41967pa41a8e4363a61b39@mail.gmail.com>
Message-ID: <7d7138c10905120114p77c848f2u66ca7f4477ad06f7@mail.gmail.com>

The part where I imply that the consumer could somehow see the puts in
different order is certainly wrong (I corrected/contradicted myself
latter in my message - where I implied that seeing qB.put guarantees
seeing qA.put). Thanks for pointing out!

2009/5/12 Christian Vest Hansen <karmazilla at gmail.com>:
> On Mon, May 11, 2009 at 6:25 PM, Jim Andreou <jim.andreou at gmail.com> wrote:
>> I don't see a guarantee that the consumer will observe the program
>> order of the producer. The two puts can be reordered if this
>> reordering can't be observed in the producer thread, so the consumer
>> may see the "wrong" order.
>
> Correct me if I'm wrong, but...
>
> LBQ.put has visibility semantics and reordering is therefor not
> allowed, no? Just like volatile and synchronized cannot be reordered.
>
>>
>> Also, think that the producer could be preempted before putting to the
>> second queue.
>>
>> I think that if you reordered the consumer like this:
>>
>> Object seen2 = queueB.take();
>> Object seen1 = queueA.remove();
>>
>> Then you would have the intented ordering. If you see the second
>> object, then the put on the queueB happened-before, and the put on
>> queueA happened-before that because it was earlier in program order.
>>
>>>
>>> My intuitive answer is yes but the actual answer might be that this is merely likely, not guaranteed.
>>>
>>> Thanks...
>>> Alex
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Venlig hilsen / Kind regards,
> Christian Vest Hansen.
>

From Hans.Boehm at hp.com  Tue May 12 11:38:48 2009
From: Hans.Boehm at hp.com (Hans Boehm)
Date: Tue, 12 May 2009 08:38:48 -0700 (PDT)
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 52,
 Issue 	1
In-Reply-To: <4A092185.7020906@briangoetz.com>
References: <mailman.1.1241284358.2796.concurrency-interest@cs.oswego.edu>
	<76b5ba080905030312r6ad87311sf2c6ccaa27c3b9a0@mail.gmail.com>
	<63b4e4050905030524y31ee259fn1b7fcd78718bd4@mail.gmail.com>
	<77ce5b7d0905110523g4b7d5c7dk6b862af132402a11@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042476F907FA5@GVW0436EXB.americas.hpqcorp.net>
	<4A092185.7020906@briangoetz.com>
Message-ID: <alpine.LNX.1.10.0905120834090.25866@192.168.2.2>



On Tue, 12 May 2009, Brian Goetz wrote:

> Or, to simplify further: volatile reads cheap, volatile writes expensive.
>

Yes, provided you remember that that's X86-specific.  At least for C++
atomics, I believe IBM finally decided on an implementation strategy
for PowerPC that makes writes cheaper.  Java volatiles are equivalent
in this respect, so I would expect the same.  For ARM, they both require
the same, relatively cheap, I'm told, fence.  Itanium is similar to X86
in this respect.

Hans


From jeroen.de.maeijer at capgemini.com  Tue May 12 16:55:00 2009
From: jeroen.de.maeijer at capgemini.com (Maeijer, Jeroen de)
Date: Tue, 12 May 2009 22:55:00 +0200
Subject: [concurrency-interest] Problem with TreeMaps in
	backport-util-concurrent.jar version 3.0
Message-ID: <53A8F7D0D75FBF498B570DB64C6523D8EED1CF@excbebr208.europe.unity>

Hi,

 

We have been using backport-util-concurrent.jar version 3.0 in a
production situation until our customer started to have some serious
problems with it.

After long examination we tracked it back to a TreeMap which was
unsorted, while it should have been sorted. Under normal circumstances
our application behaved well, but after stressing it then it seemed like
the TreeMap choked on this.

This strange behavior was not going back to normal until starting the
application again. Reason is still unknown till this moment.

 

Our examination of the problem gave the following results:

1.       Upgrading to backport-util-concurrent.jar 3.1 eliminated the
problem.

2.       Implementing a unique hashCode() function into the key objects
that were put into the TreeMap with backport-util-concurrent.jar 3.0
also eliminated the problem. (TreeMaps should not depend on this
hashCode, but only on the compareTo function, so it's a bit strange)

 

Our question is now that is this a known bug in version 3.0 that the
TreeMap sometimes chokes on non-unique hashcodes under stressed
situations?

Where can I find a list of bug fixes, because the list at the webpage
only shows 3 bug fixes?

 

Version 3.1 (Jul 5, 2006)
SVN logs: [Java 1.4
<http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/dist/back
port-util-concurrent-3.1/backport-util-concurrent-3.1-changelog.html> ,
Java 1.2 - 1.3
<http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/dist/back
port-util-concurrent-3.1/backport-util-concurrent-Java12-3.1-changelog.h
tml> , Java 5.0
<http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/dist/back
port-util-concurrent-3.1/backport-util-concurrent-Java50-3.1-changelog.h
tml> , Java 6.0
<http://www.mathcs.emory.edu/dcl/util/backport-util-concurrent/dist/back
port-util-concurrent-3.1/backport-util-concurrent-Java60-3.1-changelog.h
tml> ] 

*	New features 

	*	Version for Java 6.0 available! The 5.0 and 6.0 version
matches the performance of java.util.concurrent. 
	*	Javadoc clarifications and small improvements, following
JSR 166. 

*	Bug fixes 

	*	6523756: ThreadPoolExecutor shutdownNow vs execute race.

	*	6464365: FutureTask.{set,setException} not called by
run(). 
	*	6529795: (coll)Iterator.remove() fails if next() threw
NoSuchElementException. 

Who can give us some more information that indeed this bug we describe
has been fixed in version 3.1.

Regards,

 

 

_______________________________________________________________________

Jeroen De Maeijer / Capgemini BAS B.V. / Nieuwegein Bakenmonde
Application Designer II Qualified / Retail

jeroen.de.maeijer at capgemini.com <mailto:jeroen.de.maeijer at capgemini.com>

Tel.: +31 30 212 5077 / Fax: +31 30 212 5032 / www.nl.capgemini.com
<http://www.nl.capgemini.com/> 
Mob. +31 6 54768310
Together. Free your energies
_______________________________________________________________________

Capgemini is a trading name used by the Capgemini Group of companies
which 
includes Capgemini BAS B.V., a company registered in the Netherlands
(KvK 
27322974) whose registered office is at Papendorpseweg 100, Utrecht.

Please consider the environment and only print this email if absolutely 
necessary. Capgemini encourages environmental awareness.

 

</PRE><p style="font-family:arial;color:grey" style="font-size:13px">This message contains information that may be privileged or confidential and is the property of the Capgemini Group. It is intended only for the person to whom it is addressed. If you are not the intended recipient, you are not authorized to read, print, retain, copy, disseminate, distribute, or use this message or any part thereof. If you receive this message in error, please notify the sender immediately and delete all copies of this message.</p><PRE>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090512/1fab1d73/attachment.html>

From holger at wizards.de  Tue May 12 18:08:59 2009
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Wed, 13 May 2009 00:08:59 +0200
Subject: [concurrency-interest] Problem with TreeMaps
 in	backport-util-concurrent.jar version 3.0
In-Reply-To: <53A8F7D0D75FBF498B570DB64C6523D8EED1CF@excbebr208.europe.unity>
References: <53A8F7D0D75FBF498B570DB64C6523D8EED1CF@excbebr208.europe.unity>
Message-ID: <4A09F37B.9040906@wizards.de>

Maeijer, Jeroen de wrote:
> We have been using backport-util-concurrent.jar version 3.0 in a
> production situation until our customer started to have some serious
> problems with it.
> 
> After long examination we tracked it back to a TreeMap which was
> unsorted, while it should have been sorted. Under normal circumstances
> our application behaved well, but after stressing it then it seemed like
> the TreeMap choked on this.

Based on the description of your problem I'd say you have a concurrency
problem somewhere in your own code. The sources for the TreeMap hierarchy
are completely identical between 3.0 and 3.1.

Btw TreeMap is not a concurrent collection, despite being part of the
backport library. Maybe someone assumed this was the case.

-h


From concurrency-interest at cs.oswego.edu  Thu May 14 06:21:33 2009
From: concurrency-interest at cs.oswego.edu (Brend VIAGRA )
Date: Thu, 14 May 2009 06:21:33 -0400 (EDT)
Subject: [concurrency-interest] Message #36428.MJW. SALE 57%
Message-ID: <200905141021.n4EALXYt022142@cs.oswego.edu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/1929430d/attachment.html>

From yechielf at gigaspaces.com  Thu May 14 07:16:16 2009
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Thu, 14 May 2009 14:16:16 +0300
Subject: [concurrency-interest] a question regarding volatile & cache
	flushing/invalidating
Message-ID: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com>

When a thread writes (assigns)  values to some non-volatile variables
and then to a volatile variable, in order to make the non-volatile
values visible to any other thread which reads the volatile variable,
the writing thread has to flush all its dirty cache to main memory.

Is it a  correct assumption? 

For the reading thread, when reading from the volatile variable it has
to invalidate its (non-dirty) cache in order to retrieve the values of
the non-volatile vars from main memory- in order to get the correct
values of the non-volatiles when accessed after reading the volatile
variable

Is it a correct assumption?

 

If so- doesn't it mean that

1.	touching (writing to or reading from) volatile must be heavier
when large caches are used or caches are fuller?
2.	most multi threading programs are using ,explicitly or
implicitly (via concurrent data structures) volatiles or locks. Does it
mean that caches are becoming quite useless or even a burden in such
systems?

 

Regards,

Yechiel Fefer

 

 

  

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/58805c4d/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu May 14 07:27:27 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 14 May 2009 21:27:27 +1000
Subject: [concurrency-interest] a question regarding volatile &
	cacheflushing/invalidating
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEJIIBAA.davidcholmes@aapt.net.au>

The cache-flushing model as used by the old Java Memory Model was a simple
conceptual model to explain the basic concepts, but was inadequate. The new
Java Memory Model does not use that model. In practice there are few
remaining architectures where actual cache flushing is needed. On main
platforms write-through caches are often used.

This is a subject that has been discussed many, many times and the best
resource is to check the mailing list archives of the Java Memory Model
mailing list:

https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
  Sent: Thursday, 14 May 2009 9:16 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] a question regarding volatile &
cacheflushing/invalidating


  When a thread writes (assigns)  values to some non-volatile variables and
then to a volatile variable, in order to make the non-volatile values
visible to any other thread which reads the volatile variable, the writing
thread has to flush all its dirty cache to main memory.

  Is it a  correct assumption?

  For the reading thread, when reading from the volatile variable it has to
invalidate its (non-dirty) cache in order to retrieve the values of the
non-volatile vars from main memory- in order to get the correct values of
the non-volatiles when accessed after reading the volatile variable

  Is it a correct assumption?



  If so- doesn't it mean that

    1.. touching (writing to or reading from) volatile must be heavier when
large caches are used or caches are fuller?
    2.. most multi threading programs are using ,explicitly or implicitly
(via concurrent data structures) volatiles or locks. Does it mean that
caches are becoming quite useless or even a burden in such systems?


  Regards,

  Yechiel Fefer






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/4cff2f80/attachment.html>

From hans.boehm at hp.com  Thu May 14 13:55:19 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 14 May 2009 17:55:19 +0000
Subject: [concurrency-interest] a question regarding volatile &
	cache	flushing/invalidating
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com>
References: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com>
Message-ID: <238A96A773B3934685A7269CC8A8D04247704DA654@GVW0436EXB.americas.hpqcorp.net>

This isn't really the way most modern hardware works.  The hardware already keeps the contents of caches basically consistent.  The problem is that memory operations may not become visible to other processors in the order in which they were issues, due to write buffers and other optimizations.  Volatile variables have to constrain this kind of reordering, but I don't think the cost has much to do with cache size.

This does mean that memory accesses will usually miss the cache (at least the local one) on the first access after the corresponding cache line was last updated by another processor.  In that weak sense caches are arguably less effective for multithreaded applications.

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Yechiel Feffer
Sent: Thursday, May 14, 2009 4:16 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] a question regarding volatile & cache flushing/invalidating

When a thread writes (assigns)  values to some non-volatile variables and then to a volatile variable, in order to make the non-volatile values visible to any other thread which reads the volatile variable, the writing thread has to flush all its dirty cache to main memory.
Is it a  correct assumption?
For the reading thread, when reading from the volatile variable it has to invalidate its (non-dirty) cache in order to retrieve the values of the non-volatile vars from main memory- in order to get the correct values of the non-volatiles when accessed after reading the volatile variable
Is it a correct assumption?

If so- doesn't it mean that

 1.  touching (writing to or reading from) volatile must be heavier when large caches are used or caches are fuller?
 2.  most multi threading programs are using ,explicitly or implicitly (via concurrent data structures) volatiles or locks. Does it mean that caches are becoming quite useless or even a burden in such systems?

Regards,
Yechiel Fefer



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/e3468c85/attachment.html>

From i30817 at gmail.com  Thu May 14 15:51:12 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 14 May 2009 20:51:12 +0100
Subject: [concurrency-interest] a question regarding volatile & cache
	flushing/invalidating
In-Reply-To: <238A96A773B3934685A7269CC8A8D04247704DA654@GVW0436EXB.americas.hpqcorp.net>
References: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com> 
	<238A96A773B3934685A7269CC8A8D04247704DA654@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <212322090905141251m5ae92f90w180a6563c66baecd@mail.gmail.com>

I have a small question regarding static values. Do they have to be marked
volatile in order that the new value is visible, or does the new value
eventually get visible. Does volatile work the same for static values
regarding multi-threaded visibility?

The operation that I'm trying to short circuit is a simple boolean value
test to shorten useless (but inoffensive) additional work, as in setting the
(shared) value to false on a exception and then forgetting it. Instances
will be used by many threads off course.

I'm just going to mark the static value as volatile, but i'm curious.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/210c84a0/attachment.html>

From joe.bowbeer at gmail.com  Thu May 14 16:29:06 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 14 May 2009 13:29:06 -0700
Subject: [concurrency-interest] a question regarding volatile & cache
	flushing/invalidating
In-Reply-To: <212322090905141251m5ae92f90w180a6563c66baecd@mail.gmail.com>
References: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com>
	<238A96A773B3934685A7269CC8A8D04247704DA654@GVW0436EXB.americas.hpqcorp.net>
	<212322090905141251m5ae92f90w180a6563c66baecd@mail.gmail.com>
Message-ID: <31f2a7bd0905141329t1ec820eer1178b5a76c76af66@mail.gmail.com>

I assume you a mean static member variable rather than a static value.  (If
its value truly were static, you should declare the field as "final".)

In the worst case, without volatile, the compiler might reorder the read
and/or write accesses to the static field, rendering it useless as a
decision variable.

In practice, the compiler probably isn't that smart, and there's probably
too much other synchronization in the way, limiting the extent to which the
compiler can relocate these accesses.

I would declare it volatile and document why.

Joe

On Thu, May 14, 2009 at 12:51 PM, Paulo Levi wrote:

> I have a small question regarding static values. Do they have to be marked
> volatile in order that the new value is visible, or does the new value
> eventually get visible. Does volatile work the same for static values
> regarding multi-threaded visibility?
>
> The operation that I'm trying to short circuit is a simple boolean value
> test to shorten useless (but inoffensive) additional work, as in setting the
> (shared) value to false on a exception and then forgetting it. Instances
> will be used by many threads off course.
>
> I'm just going to mark the static value as volatile, but i'm curious.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/523478f9/attachment.html>

From i30817 at gmail.com  Thu May 14 17:03:09 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Thu, 14 May 2009 22:03:09 +0100
Subject: [concurrency-interest] a question regarding volatile & cache
	flushing/invalidating
In-Reply-To: <31f2a7bd0905141329t1ec820eer1178b5a76c76af66@mail.gmail.com>
References: <3623E06481E65B45866CB3AF32C4FA870218259A@hercules.gspaces.com> 
	<238A96A773B3934685A7269CC8A8D04247704DA654@GVW0436EXB.americas.hpqcorp.net>
	<212322090905141251m5ae92f90w180a6563c66baecd@mail.gmail.com> 
	<31f2a7bd0905141329t1ec820eer1178b5a76c76af66@mail.gmail.com>
Message-ID: <212322090905141403x60d05dd8g687514b411dad49f@mail.gmail.com>

static volatile it is. Thanks.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/714d8368/attachment.html>

From binhle2 at illinois.edu  Thu May 14 19:40:59 2009
From: binhle2 at illinois.edu (Binh Le)
Date: Thu, 14 May 2009 18:40:59 -0500
Subject: [concurrency-interest] Running ForkJoin framework on J9 virtual
	machine
Message-ID: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>

When I run the Fibonacci program using the FJ framework with J9 on a Linux
box, I got the following exception (presumably related to JNI). I just
wonder if someone also run into the same problem. It appears that IBM's
javac can compile FJ programs but J9 can not run FJ bytecode. When I browse
the FJ framework implementation, it uses a class named "Unsafe". Probably
this class is the cause for the exception.

I would appreciate pointers to resolve the problems of running FJ programs
with J9. Thanks.

Exception in thread "main" java.lang.UnsatisfiedLinkError:
sun/misc/Unsafe.putOrderedObject(Ljava/lang/Object;JLjava/lang/Object;)V
        at
jsr166y.ForkJoinWorkerThread.setSlot(ForkJoinWorkerThread.java:395)
        at
jsr166y.ForkJoinWorkerThread.pushTask(ForkJoinWorkerThread.java:424)
        at jsr166y.ForkJoinTask.fork(ForkJoinTask.java:481)
        at jsr166y.ForkJoinTask.invokeAll(ForkJoinTask.java:550)
        at LBJ2.nlp.coref.CorefRootAction.compute(CorefRootAction.java:22)
        at jsr166y.RecursiveAction.exec(RecursiveAction.java:147)
        at jsr166y.ForkJoinTask.quietlyExec(ForkJoinTask.java:422)
        at
jsr166y.ForkJoinWorkerThread.mainLoop(ForkJoinWorkerThread.java:327)
        at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:312)

-- 
Binh Le
http://www.cs.uiuc.edu/homes/binhle2/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090514/58a0bcab/attachment-0001.html>

From forax at univ-mlv.fr  Thu May 14 20:32:54 2009
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 15 May 2009 02:32:54 +0200
Subject: [concurrency-interest] Running ForkJoin framework on J9 virtual
 machine
In-Reply-To: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>
References: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>
Message-ID: <4A0CB836.5090806@univ-mlv.fr>

Binh Le a ?crit :
> When I run the Fibonacci program using the FJ framework with J9 on a 
> Linux box, I got the following exception (presumably related to JNI). 
> I just wonder if someone also run into the same problem. It appears 
> that IBM's javac can compile FJ programs but J9 can not run FJ 
> bytecode. When I browse the FJ framework implementation, it uses a 
> class named "Unsafe". Probably this class is the cause for the exception.
sun.misc.Unsafe is a class containing lot of magic traps to the hotspot 
VM (technically it's doesn't use JNI).
You can find it in all java environment that use the OpenJDK implementation
for their VM (Sun JDK, JRockit, AppleJDK) but not IBM one.

>
> I would appreciate pointers to resolve the problems of running FJ 
> programs with J9. Thanks.
>
> Exception in thread "main" java.lang.UnsatisfiedLinkError: 
> sun/misc/Unsafe.putOrderedObject(Ljava/lang/Object;JLjava/lang/Object;)V
>         at 
> jsr166y.ForkJoinWorkerThread.setSlot(ForkJoinWorkerThread.java:395)
>         at 
> jsr166y.ForkJoinWorkerThread.pushTask(ForkJoinWorkerThread.java:424)
>         at jsr166y.ForkJoinTask.fork(ForkJoinTask.java:481)
>         at jsr166y.ForkJoinTask.invokeAll(ForkJoinTask.java:550)
>         at LBJ2.nlp.coref.CorefRootAction.compute(CorefRootAction.java:22)
>         at jsr166y.RecursiveAction.exec(RecursiveAction.java:147)
>         at jsr166y.ForkJoinTask.quietlyExec(ForkJoinTask.java:422)
>         at 
> jsr166y.ForkJoinWorkerThread.mainLoop(ForkJoinWorkerThread.java:327)
>         at jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:312)
>
> -- 
> Binh Le
> http://www.cs.uiuc.edu/homes/binhle2/
R?mi

From dl at cs.oswego.edu  Fri May 15 06:52:20 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 15 May 2009 06:52:20 -0400
Subject: [concurrency-interest] Running ForkJoin framework on J9 virtual
 machine
In-Reply-To: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>
References: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>
Message-ID: <4A0D4964.5030500@cs.oswego.edu>

Binh Le wrote:
> When I run the Fibonacci program using the FJ framework with J9 on a 
> Linux box, I got the following exception (presumably related to JNI). I 
> just wonder if someone also run into the same problem. It appears that 
> IBM's javac can compile FJ programs but J9 can not run FJ bytecode. When 
> I browse the FJ framework implementation, it uses a class named 
> "Unsafe". Probably this class is the cause for the exception.

This is due to an odd state of affairs that I ought to try to
help resolve. Here's the backstory:

Java6 introduced AtomicX.lazySet methods. The openjdk/hotspot RI
implementation required new Unsafe methods to implement them.
"Unsafe" is basically a backdoor for introducing new low-level
capabilities that would otherwise require new bytecodes. It
truly is unsafe though because its methods (intrinsically)
rely on offsets within objects to indicate the element to
operate on. Even though not officially spec'ed anywhere,
these Unsafe methods are for the most part supported in
the same way across Sun, IBM, etc JVMs, because in most
cases there aren't any other good choices for how to do it.
However, for some versions of IBM J9, they apparently did find
some other way to support the lazySet methods (I believe this
on x86 only, PowerPC J9 seems to have them). This makes it
hard for us to put out preliminary (jsr166y) versions
of upcoming classes that rely on and extend the underlying JVM
support that cannot be guaranteed to work until (here, Java7) platform
release.

There are internal workarounds for this that I could use
in ForkJoin classes that would cost a little more. But
without any kind of "#ifdef J9" construction
available, I don't know how to release these in jsr166y
form. In the mean time, one workaround that is correct but
needlessly heavy is to replace setOrderedX methods with
setXVolatile in sources and rebuild.

-Doug


From aph at redhat.com  Fri May 15 07:39:11 2009
From: aph at redhat.com (Andrew Haley)
Date: Fri, 15 May 2009 12:39:11 +0100
Subject: [concurrency-interest] Running ForkJoin framework on J9 virtual
 machine
In-Reply-To: <4A0D4964.5030500@cs.oswego.edu>
References: <19ff42870905141640p38d8f2d5le9441ab5f6e8e801@mail.gmail.com>
	<4A0D4964.5030500@cs.oswego.edu>
Message-ID: <4A0D545F.10607@redhat.com>

Doug Lea wrote:
> This makes it
> hard for us to put out preliminary (jsr166y) versions
> of upcoming classes that rely on and extend the underlying JVM
> support that cannot be guaranteed to work until (here, Java7) platform
> release.
> 
> There are internal workarounds for this that I could use
> in ForkJoin classes that would cost a little more. But
> without any kind of "#ifdef J9" construction
> available, I don't know how to release these in jsr166y
> form. In the mean time, one workaround that is correct but
> needlessly heavy is to replace setOrderedX methods with
> setXVolatile in sources and rebuild.

For evaluation, surely it's easiest just to produce a native version of
the unsafe methods.  Any VM that has the intrinsics will use them,
and will fall back to the native versions only if they're missing from
the VM.

OK, that's not much use for benchmarking, but it'd work.

Andrew.

From dl at cs.oswego.edu  Sat May 16 09:06:38 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 16 May 2009 09:06:38 -0400
Subject: [concurrency-interest] Outages
Message-ID: <4A0EBA5E.4080906@cs.oswego.edu>


Tomorrow (Sunday May 17) we will be doing various reconfigurations
and upgrades here. Most likely, jsr166 CVS, list archives,
and web access will be unavailable for at least a few hours, and
other things may be transiently broken for up to a day or two.

-Doug



From manik at jboss.org  Mon May 18 07:14:11 2009
From: manik at jboss.org (Manik Surtani)
Date: Mon, 18 May 2009 12:14:11 +0100
Subject: [concurrency-interest] A concurrent, linked HashMap impl
Message-ID: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>

I've implemented a concurrent, linked map-like structure [1] [2] based  
on CHM-like array of segments for hash lookups and  Sundell and  
Tsigas' "Lock-free deques and doubly linked lists" [3] for maintaining  
the linked list.  The impl offers constant-time operation for put(),  
get(), remove() and iteration.

I haven't implemented ConcurrentMap as my needs haven't dictated so,  
but I can't see why this should not be possible.

Is there interest in a ConcurrentMap implementation based on what I  
have done for JSR-166?

Cheers
Manik

[1] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236
[2] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219
[3] http://www.md.chalmers.se/~tsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf










From manik at jboss.org  Mon May 18 08:38:07 2009
From: manik at jboss.org (Manik Surtani)
Date: Mon, 18 May 2009 13:38:07 +0100
Subject: [concurrency-interest] Receiving notifications from Futures
Message-ID: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>

Hi

I'm creating an API that returns Futures.  One of the things that may  
be useful to callers is to be notified when the Future completes, so  
rather than doing a future.get(), the thread can register a callback  
on the future and proceed with other stuff instead, being notified  
when the future has completed and can then call future.get() to  
retrieve any results.

I have looked at the CompletionService API, and while this does help  
to some degree, it is still not something I can expose to any calling  
code, mainly because calling code does not register Callables on my  
interface.  Instead it calls methods that return Futures.

I'm guessing I cannot be the first person with the need for a  
notifying future - how has this been achieved before?

I was thinking of something like:

public interface  NotifyingFuture<T> extends Future<T> {
    NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
  }

public interface NotifyingListener<T> {
   void futureCompleted(NotifyingFuture<T> f);
}

so that calling code could do:

Future f =  
myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);

Cheers
Manik


From mthornton at optrak.co.uk  Mon May 18 08:59:19 2009
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Mon, 18 May 2009 13:59:19 +0100
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
Message-ID: <4A115BA7.3010705@optrak.co.uk>

Manik Surtani wrote:
> Hi
>
> I'm creating an API that returns Futures.  One of the things that may 
> be useful to callers is to be notified when the Future completes, so 
> rather than doing a future.get(), the thread can register a callback 
> on the future and proceed with other stuff instead, being notified 
> when the future has completed and can then call future.get() to 
> retrieve any results.
When I thought I wanted notification I found it easier to change the API 
so that it adds completed results to a Collection (normally a threadsafe 
queue). Then if I still want notification I can provide a Collection 
implementation which notifies interested parties when it becomes non empty.

Mark Thornton


From tim at peierls.net  Mon May 18 09:38:57 2009
From: tim at peierls.net (Tim Peierls)
Date: Mon, 18 May 2009 09:38:57 -0400
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
Message-ID: <63b4e4050905180638g33db01bcjddaa2a21ccd3c790@mail.gmail.com>

The need for completion callbacks has come up before on this list, each time
with slightly different requirements. This is the first that I recall that
didn't involve Callable/Runnable as well.

One problem with adding methods to Future to configure a completion callback
is that (in general) the Future might complete before the configuration
method is called. This is not an insurmountable problem -- the configuration
method could, upon noticing that the Future has already completed its work,
simply call the callback directly -- but it seems a little like closing the
barn door after the horse is gone.
How about designing your API so that methods that return Futures optionally
take a callback object?

Incidentally, if the implementation of the Future returned by your methods
is a subtype of FutureTask, then you might be able to implement your
callbacks through the FutureTask.done() method.

--tim

On Mon, May 18, 2009 at 8:38 AM, Manik Surtani <manik at jboss.org> wrote:

> Hi
>
> I'm creating an API that returns Futures.  One of the things that may be
> useful to callers is to be notified when the Future completes, so rather
> than doing a future.get(), the thread can register a callback on the future
> and proceed with other stuff instead, being notified when the future has
> completed and can then call future.get() to retrieve any results.
>
> I have looked at the CompletionService API, and while this does help to
> some degree, it is still not something I can expose to any calling code,
> mainly because calling code does not register Callables on my interface.
>  Instead it calls methods that return Futures.
>
> I'm guessing I cannot be the first person with the need for a notifying
> future - how has this been achieved before?
>
> I was thinking of something like:
>
> public interface  NotifyingFuture<T> extends Future<T> {
>   NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
>  }
>
> public interface NotifyingListener<T> {
>  void futureCompleted(NotifyingFuture<T> f);
> }
>
> so that calling code could do:
>
> Future f =
> myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);
>
> Cheers
> Manik
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090518/28ffa1dc/attachment.html>

From sberlin at gmail.com  Mon May 18 10:40:47 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Mon, 18 May 2009 10:40:47 -0400
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
Message-ID: <19196d860905180740o5e49b8d4g8a624b5c59660e9a@mail.gmail.com>

We needed this also, so came up with a "ListeningFuture" and surrounding API
(that also utilizes a separate listening package we've come up with, but
it's easy to divorce it from the listening package if necessary).

See:
https://www.limewire.org/fisheye/browse/limecvs/components/common/src/main/java/org/limewire/concurrent

& the classes:
 ListeningFuture [the interface that adds listening]
 ListeningFutureExecutorService [extension to ExecutorService that returns
ListeningFutures instead of Futures]
 ListeningFutureTask [an implementation of FutureTask that sends events]
 RunnableListeningFuture [an extension to RunnableFuture that adds
listening]
 RunnableScheduledListeningFuture [an extension to RunnableScheduledFuture
that adds listening]
 ScheduledListeningFuture [an extension to ScheduledFuture]
 ScheduledListeningExecutorService [an extension to ScheduledExecutorService
that returns ScheduledListeningFutures]
 ThreadPoolListeningExecutor [an extension to ThreadPoolExecutor]
 AbstractListeningExecutorService [an extension to AbstractExecutorService]
 FutureEvent [the event a listening future sends]

Sam

On Mon, May 18, 2009 at 8:38 AM, Manik Surtani <manik at jboss.org> wrote:

> Hi
>
> I'm creating an API that returns Futures.  One of the things that may be
> useful to callers is to be notified when the Future completes, so rather
> than doing a future.get(), the thread can register a callback on the future
> and proceed with other stuff instead, being notified when the future has
> completed and can then call future.get() to retrieve any results.
>
> I have looked at the CompletionService API, and while this does help to
> some degree, it is still not something I can expose to any calling code,
> mainly because calling code does not register Callables on my interface.
>  Instead it calls methods that return Futures.
>
> I'm guessing I cannot be the first person with the need for a notifying
> future - how has this been achieved before?
>
> I was thinking of something like:
>
> public interface  NotifyingFuture<T> extends Future<T> {
>   NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
>  }
>
> public interface NotifyingListener<T> {
>  void futureCompleted(NotifyingFuture<T> f);
> }
>
> so that calling code could do:
>
> Future f =
> myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);
>
> Cheers
> Manik
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090518/40c32aa4/attachment.html>

From peter.royal at pobox.com  Mon May 18 10:47:32 2009
From: peter.royal at pobox.com (peter royal)
Date: Mon, 18 May 2009 10:47:32 -0400
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
Message-ID: <CA04F391-F5ED-4932-9EFA-8490E50FCBE4@pobox.com>

On May 18, 2009, at 8:38 AM, Manik Surtani wrote:
> I'm guessing I cannot be the first person with the need for a  
> notifying future - how has this been achieved before?

Sam Berlin put forth an implementation last November on the list and  
there was some discussion around it then:

http://cs.oswego.edu/pipermail/concurrency-interest/2008-November/005588.html

-pete

-- 
(peter.royal|osi)@pobox.com - http://fotap.org/~osi

-------------- next part --------------
A non-text attachment was scrubbed...
Name: smime.p7s
Type: application/pkcs7-signature
Size: 2454 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090518/7b796a17/attachment-0001.bin>

From bronee at gmail.com  Mon May 18 11:23:13 2009
From: bronee at gmail.com (Brian S O'Neill)
Date: Mon, 18 May 2009 08:23:13 -0700
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <CA04F391-F5ED-4932-9EFA-8490E50FCBE4@pobox.com>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
	<CA04F391-F5ED-4932-9EFA-8490E50FCBE4@pobox.com>
Message-ID: <4A117D61.2000302@gmail.com>

Some of these solutions require the addition of many more classes and 
interfaces, which I think greatly complicates things. I agree with 
Mark's suggestion: pass a collection/queue to the API that created the 
Future. Manik suggestion can also be simplified by accepting an ordinary 
queue as input instead of a special listener. This is the approach I 
took, and it keeps things simple.

https://dirmi.dev.java.net/nonav/javadoc/org/cojen/dirmi/Completion.html

peter royal wrote:
> On May 18, 2009, at 8:38 AM, Manik Surtani wrote:
>> I'm guessing I cannot be the first person with the need for a 
>> notifying future - how has this been achieved before?
>
> Sam Berlin put forth an implementation last November on the list and 
> there was some discussion around it then:
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2008-November/005588.html 
>
>
> -pete
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

From sberlin at gmail.com  Mon May 18 11:49:44 2009
From: sberlin at gmail.com (Sam Berlin)
Date: Mon, 18 May 2009 11:49:44 -0400
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <4A117D61.2000302@gmail.com>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
	<CA04F391-F5ED-4932-9EFA-8490E50FCBE4@pobox.com>
	<4A117D61.2000302@gmail.com>
Message-ID: <19196d860905180849h19243d8wee975eb49b08a7a3@mail.gmail.com>

I agree, the proliferation of interfaces & classes is unwieldy.
Fortunately, it's a one-time cost, the extensions straightforward, and the
changes are limited to just supporting the new 'ListeningFuture' method as
return values.  Folding asynchronous notification of a Future into the JDK's
API would be the ideal solution.

Sam

On Mon, May 18, 2009 at 11:23 AM, Brian S O'Neill <bronee at gmail.com> wrote:

> Some of these solutions require the addition of many more classes and
> interfaces, which I think greatly complicates things. I agree with Mark's
> suggestion: pass a collection/queue to the API that created the Future.
> Manik suggestion can also be simplified by accepting an ordinary queue as
> input instead of a special listener. This is the approach I took, and it
> keeps things simple.
>
> https://dirmi.dev.java.net/nonav/javadoc/org/cojen/dirmi/Completion.html
>
> peter royal wrote:
>
>> On May 18, 2009, at 8:38 AM, Manik Surtani wrote:
>>
>>> I'm guessing I cannot be the first person with the need for a notifying
>>> future - how has this been achieved before?
>>>
>>
>> Sam Berlin put forth an implementation last November on the list and there
>> was some discussion around it then:
>>
>>
>> http://cs.oswego.edu/pipermail/concurrency-interest/2008-November/005588.html
>>
>> -pete
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090518/edc6d271/attachment.html>

From joe.bowbeer at gmail.com  Mon May 18 13:02:17 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 18 May 2009 10:02:17 -0700
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <63b4e4050905180638g33db01bcjddaa2a21ccd3c790@mail.gmail.com>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
	<63b4e4050905180638g33db01bcjddaa2a21ccd3c790@mail.gmail.com>
Message-ID: <31f2a7bd0905181002y3f90f224m602e439cbd8c2fa0@mail.gmail.com>

On Mon, May 18, 2009 at 6:38 AM, Tim Peierls wrote:

> The need for completion callbacks has come up before on this list, each
> time with slightly different requirements. This is the first that I recall
> that didn't involve Callable/Runnable as well.
>
> One problem with adding methods to Future to configure a completion
> callback is that (in general) the Future might complete before the
> configuration method is called. This is not an insurmountable problem -- the
> configuration method could, upon noticing that the Future
> has already completed its work, simply call the callback directly -- but it
> seems a little like closing the barn door after the horse is gone.
> How about designing your API so that methods that return Futures optionally
> take a callback object?
>
> Incidentally, if the implementation of the Future returned by your methods
> is a subtype of FutureTask, then you might be able to implement your
> callbacks through the FutureTask.done() method.
>
> --tim
>
>
> On Mon, May 18, 2009 at 8:38 AM, Manik Surtani wrote:
>
>> Hi
>>
>> I'm creating an API that returns Futures.  One of the things that may be
>> useful to callers is to be notified when the Future completes, so rather
>> than doing a future.get(), the thread can register a callback on the future
>> and proceed with other stuff instead, being notified when the future has
>> completed and can then call future.get() to retrieve any results.
>>
>> I have looked at the CompletionService API, and while this does help to
>> some degree, it is still not something I can expose to any calling code,
>> mainly because calling code does not register Callables on my interface.
>>  Instead it calls methods that return Futures.
>>
>> I'm guessing I cannot be the first person with the need for a notifying
>> future - how has this been achieved before?
>>
>> I was thinking of something like:
>>
>> public interface  NotifyingFuture<T> extends Future<T> {
>>   NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
>>  }
>>
>> public interface NotifyingListener<T> {
>>  void futureCompleted(NotifyingFuture<T> f);
>> }
>>
>> so that calling code could do:
>>
>> Future f =
>> myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);
>>
>> Cheers
>> Manik
>>
>>

Also search for "custom completion task" in the April 2009 archives.  And
"ListenableFuture" a couple years ago.

I tend to approach this using custom tasks that are constructed with a
reference to their listener (or completion queue) -- as Tim describes.

As Tim notes, one of the problems with registering a listener after the
future is dispatched is that the future may complete before the listener is
registered.

Rather than pass the listener into the method that returns the future (task
handle), I tend to create my custom "completion" task first, before
executing it.  This way, when the listener notifies me, I'm certain to have
already seen the task instance to which it refers.  Otherwise, if the future
may complete before I've obtained a reference to it, trying to match-up the
completion event with its cause can lead to some tricky bookkeeping .

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090518/601cd499/attachment.html>

From manik at jboss.org  Mon May 18 19:27:48 2009
From: manik at jboss.org (Manik Surtani)
Date: Tue, 19 May 2009 00:27:48 +0100
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <31f2a7bd0905181002y3f90f224m602e439cbd8c2fa0@mail.gmail.com>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>
	<63b4e4050905180638g33db01bcjddaa2a21ccd3c790@mail.gmail.com>
	<31f2a7bd0905181002y3f90f224m602e439cbd8c2fa0@mail.gmail.com>
Message-ID: <B3325A70-3E2C-49BD-B454-96E74AAD9678@jboss.org>

All makes a lot of sense.  Is there any effort for a future JDK  
solution?  Seems to be a feature that is often used.

On 18 May 2009, at 18:02, Joe Bowbeer wrote:

> On Mon, May 18, 2009 at 6:38 AM, Tim Peierls wrote:
> The need for completion callbacks has come up before on this list,  
> each time with slightly different requirements. This is the first  
> that I recall that didn't involve Callable/Runnable as well.
>
> One problem with adding methods to Future to configure a completion  
> callback is that (in general) the Future might complete before the  
> configuration method is called. This is not an insurmountable  
> problem -- the configuration method could, upon noticing that the  
> Future has already completed its work, simply call the callback  
> directly -- but it seems a little like closing the barn door after  
> the horse is gone.
>
> How about designing your API so that methods that return Futures  
> optionally take a callback object?
>
> Incidentally, if the implementation of the Future returned by your  
> methods is a subtype of FutureTask, then you might be able to  
> implement your callbacks through the FutureTask.done() method.
>
> --tim
>
>
> On Mon, May 18, 2009 at 8:38 AM, Manik Surtani wrote:
> Hi
>
> I'm creating an API that returns Futures.  One of the things that  
> may be useful to callers is to be notified when the Future  
> completes, so rather than doing a future.get(), the thread can  
> register a callback on the future and proceed with other stuff  
> instead, being notified when the future has completed and can then  
> call future.get() to retrieve any results.
>
> I have looked at the CompletionService API, and while this does help  
> to some degree, it is still not something I can expose to any  
> calling code, mainly because calling code does not register  
> Callables on my interface.  Instead it calls methods that return  
> Futures.
>
> I'm guessing I cannot be the first person with the need for a  
> notifying future - how has this been achieved before?
>
> I was thinking of something like:
>
> public interface  NotifyingFuture<T> extends Future<T> {
>   NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
>  }
>
> public interface NotifyingListener<T> {
>  void futureCompleted(NotifyingFuture<T> f);
> }
>
> so that calling code could do:
>
> Future f =  
> myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);
>
> Cheers
> Manik
>
>
>
> Also search for "custom completion task" in the April 2009  
> archives.  And "ListenableFuture" a couple years ago.
>
> I tend to approach this using custom tasks that are constructed with  
> a reference to their listener (or completion queue) -- as Tim  
> describes.
>
> As Tim notes, one of the problems with registering a listener after  
> the future is dispatched is that the future may complete before the  
> listener is registered.
>
> Rather than pass the listener into the method that returns the  
> future (task handle), I tend to create my custom "completion" task  
> first, before executing it.  This way, when the listener notifies  
> me, I'm certain to have already seen the task instance to which it  
> refers.  Otherwise, if the future may complete before I've obtained  
> a reference to it, trying to match-up the completion event with its  
> cause can lead to some tricky bookkeeping .
>
> Joe
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--
Manik Surtani
manik at jboss.org
Lead, Infinispan
Lead, JBoss Cache
http://www.infinispan.org
http://www.jbosscache.org




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090519/db2c78f0/attachment.html>

From brian at briangoetz.com  Mon May 18 21:17:57 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Mon, 18 May 2009 21:17:57 -0400
Subject: [concurrency-interest] Receiving notifications from Futures
In-Reply-To: <B3325A70-3E2C-49BD-B454-96E74AAD9678@jboss.org>
References: <9A453523-C677-4C0B-97DD-8F34D01F37F2@jboss.org>	<63b4e4050905180638g33db01bcjddaa2a21ccd3c790@mail.gmail.com>	<31f2a7bd0905181002y3f90f224m602e439cbd8c2fa0@mail.gmail.com>
	<B3325A70-3E2C-49BD-B454-96E74AAD9678@jboss.org>
Message-ID: <4A1208C5.2020204@briangoetz.com>

Its pretty easy to roll your own by overriding done() to do whatever you want 
-- signal on a Condition, put an element on a queue, counting down on a latch, 
calling a chain of listeners, etc.  While you're right that this is a feature 
that is often used, we have found that everyone has their own version of how 
this feature should work -- so the low-level solution of providing a done() 
hook enables all of them (of course with additional work.)

Manik Surtani wrote:
> All makes a lot of sense.  Is there any effort for a future JDK 
> solution?  Seems to be a feature that is often used.
> 
> On 18 May 2009, at 18:02, Joe Bowbeer wrote:
> 
>> On Mon, May 18, 2009 at 6:38 AM, Tim Peierls wrote:
>>
>>     The need for completion callbacks has come up before on this list,
>>     each time with slightly different requirements. This is the first
>>     that I recall that didn't involve Callable/Runnable as well.
>>
>>     One problem with adding methods to Future to configure a
>>     completion callback is that (in general) the Future might complete
>>     before the configuration method is called. This is not an
>>     insurmountable problem -- the configuration method could, upon
>>     noticing that the Future has already completed its work, simply
>>     call the callback directly -- but it seems a little like closing
>>     the barn door after the horse is gone.
>>
>>     How about designing your API so that methods that return Futures
>>     optionally take a callback object?
>>
>>     Incidentally, if the implementation of the Future returned by your
>>     methods is a subtype of FutureTask, then you might be able to
>>     implement your callbacks through the FutureTask.done() method.
>>
>>     --tim
>>
>>
>>     On Mon, May 18, 2009 at 8:38 AM, Manik Surtani wrote:
>>
>>         Hi
>>
>>         I'm creating an API that returns Futures.  One of the things
>>         that may be useful to callers is to be notified when the
>>         Future completes, so rather than doing a future.get(), the
>>         thread can register a callback on the future and proceed with
>>         other stuff instead, being notified when the future has
>>         completed and can then call future.get() to retrieve any results.
>>
>>         I have looked at the CompletionService API, and while this
>>         does help to some degree, it is still not something I can
>>         expose to any calling code, mainly because calling code does
>>         not register Callables on my interface.  Instead it calls
>>         methods that return Futures.
>>
>>         I'm guessing I cannot be the first person with the need for a
>>         notifying future - how has this been achieved before?
>>
>>         I was thinking of something like:
>>
>>         public interface  NotifyingFuture<T> extends Future<T> {
>>           NotifyingFuture<T> notifyOnCompletion(NotifyingListener<T> l);
>>          }
>>
>>         public interface NotifyingListener<T> {
>>          void futureCompleted(NotifyingFuture<T> f);
>>         }
>>
>>         so that calling code could do:
>>
>>         Future f =
>>         myAPI.masyncMethodThatReturnsFuture().notifyOnCompletion(listener);
>>
>>         Cheers
>>         Manik
>>
>>
>>
>> Also search for "custom completion task" in the April 2009 archives.  
>> And "ListenableFuture" a couple years ago.
>>
>> I tend to approach this using custom tasks that are constructed with a 
>> reference to their listener (or completion queue) -- as Tim describes.
>>
>> As Tim notes, one of the problems with registering a listener after 
>> the future is dispatched is that the future may complete before the 
>> listener is registered.
>>
>> Rather than pass the listener into the method that returns the future 
>> (task handle), I tend to create my custom "completion" task first, 
>> before executing it.  This way, when the listener notifies me, I'm 
>> certain to have already seen the task instance to which it refers.  
>> Otherwise, if the future may complete before I've obtained a reference 
>> to it, trying to match-up the completion event with its cause can lead 
>> to some tricky bookkeeping .
>>
>> Joe
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> --
> Manik Surtani
> manik at jboss.org <mailto:manik at jboss.org>
> Lead, Infinispan
> Lead, JBoss Cache
> http://www.infinispan.org
> http://www.jbosscache.org
> 
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From jean.morissette at gmail.com  Tue May 19 09:33:51 2009
From: jean.morissette at gmail.com (Jean Morissette)
Date: Tue, 19 May 2009 09:33:51 -0400
Subject: [concurrency-interest] Real-time task scheduling
Message-ID: <97ad10900905190633p5820791au31acdf0dc7fabac3@mail.gmail.com>

Hi,
Giving a task with an execution cost of sub-microsecond and a period
in the order of 10 microseconds, I would like to schedule periodically
this task with hard real-time behaviour on the Java SE platform.

I take as granted the following assumptions:
(1) All objects in the program are allocated at the start and reused
such that the GC does not pause the program.
(2) All classes are loaded and compiled at the start (by example
running in interpreted mode using -Xint JVM option or using AOT
compiler like Excelsior JET) avoiding pause from JIT compiler.
(3) No other user-program is running on the OS.
(4) The number of threads created by the program is lower than the
number of CPU, avoiding context-switching.
(5) Concurrency is performed using lock-free data structures.
(6) OS independance is not a requirement.

Considering those assumptions are valid, one issue remaining is the
unpredictability of the OS, which could use the CPU for its own needs
at any moment, stoping the Java worker thread long enough to make the
task miss its deadline. Is-it possible to resolve this issue?

By example, assuming that the program run on a Real-Time OS, do the OS
threads associated with the Java threads will have the same priority
than the Java program? Otherwise, is-it possible to create a JNI call
to change the OS thread priority associated with the current Java
thread such that it cant be preempted? Or is-it possible to configure
the OS to set an upper bound on thread pause time?

Thanks for your help,
Jean Morissette

From hans.boehm at hp.com  Tue May 19 12:37:38 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 19 May 2009 16:37:38 +0000
Subject: [concurrency-interest] Real-time task scheduling
In-Reply-To: <97ad10900905190633p5820791au31acdf0dc7fabac3@mail.gmail.com>
References: <97ad10900905190633p5820791au31acdf0dc7fabac3@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0424DF80AC5DB@GVW0436EXB.americas.hpqcorp.net>

You're presumably really asking about a particular implementation?

The JLS really says very little about progress guarantees, much less real-time guarantees.  It doesn't guarantee that garbage collection is only triggered on allocation.  I don't think it guarantees sufficient timer accuracy, etc.

Even then, I suspect there are other issues, e.g. various routines in the runtime and library that ship with the JVM are probably not lock-free.

If the goal is to make this work nearly all of the time on a particular implementation, maybe ...

Hans 

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Jean Morissette
> Sent: Tuesday, May 19, 2009 6:34 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Real-time task scheduling
> 
> Hi,
> Giving a task with an execution cost of sub-microsecond and a 
> period in the order of 10 microseconds, I would like to 
> schedule periodically this task with hard real-time behaviour 
> on the Java SE platform.
> 
> I take as granted the following assumptions:
> (1) All objects in the program are allocated at the start and 
> reused such that the GC does not pause the program.
> (2) All classes are loaded and compiled at the start (by 
> example running in interpreted mode using -Xint JVM option or 
> using AOT compiler like Excelsior JET) avoiding pause from 
> JIT compiler.
> (3) No other user-program is running on the OS.
> (4) The number of threads created by the program is lower 
> than the number of CPU, avoiding context-switching.
> (5) Concurrency is performed using lock-free data structures.
> (6) OS independance is not a requirement.
> 
> Considering those assumptions are valid, one issue remaining 
> is the unpredictability of the OS, which could use the CPU 
> for its own needs at any moment, stoping the Java worker 
> thread long enough to make the task miss its deadline. Is-it 
> possible to resolve this issue?
> 
> By example, assuming that the program run on a Real-Time OS, 
> do the OS threads associated with the Java threads will have 
> the same priority than the Java program? Otherwise, is-it 
> possible to create a JNI call to change the OS thread 
> priority associated with the current Java thread such that it 
> cant be preempted? Or is-it possible to configure the OS to 
> set an upper bound on thread pause time?
> 
> Thanks for your help,
> Jean Morissette
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From davidcholmes at aapt.net.au  Tue May 19 17:49:42 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 20 May 2009 07:49:42 +1000
Subject: [concurrency-interest] Real-time task scheduling
In-Reply-To: <97ad10900905190633p5820791au31acdf0dc7fabac3@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOELMIBAA.davidcholmes@aapt.net.au>

Hi Jean,

So what you are really asking is: can I approximate the behaviour of an
implementation of the Real-time Specification for Java (JSR-1, JSR-282), by
constraining the application behaviour in a similar manner to
Safety-critical Java (JSR-302), but do all this on a stock-standard JVM ?

How close you can get depends on the VM you are using. If you remove all GC
activity (not always possible even if you don't allocate) and all
compilation activity (again not necessarily easily accomplished) then there
can still be other activities in the VM that interfere with your application
threads - for example if considering Hotspot, non-GC related safepoints,
periodic WatcherThread, and possibly library level threads like the
ReferenceHandler and Finalizer threads (which in theory can be idle, but in
practice might wakeup occasionally). So getting a VM that will support what
you are trying to do is not easy.

Then there is the OS. Again it depends on which OS but if you are thinking
Solaris or a real-time Linux, or anything that presents a POSIX-like
execution environment then you need to be using the realtime scheduling
classes  of that OS. You need a VM which maps Java threads directly to
native threads. You could use a JNI call to set the priority to the desired
real-time level. But there are a couple of things you need to further
consider:

a) you need to prioritize your threads at the correct level relative to
system services, such that you don't execute in preference to OS activities
that you will be relying upon

On Solaris the RT priorities lie between the IRQ priorities (highest) and
the system activities. On RT-linux you can change the priority of anything -
but you need to know what you are doing.

b) Having RT priority threads in your application might break the JVM
because RT priority threads enjoy a run-till-block (or preempted, or
voluntarily relinquish the CPU) scheduling policy, and that can interference
with VM coordination protocols which don't expect this, and which don't
expect strict priority based scheduling.

Your use of lock-free might also be problematic - first because with
priority-based scheduling you can more easily get starvation; but also
because you can't (easily) control all the locks used by Java code so some
regular monitors might be involved as well. Then again if it is a thread per
CPU you could use simply spin-locks because you won't care about the wasted
cycles (but you'll need all threads to have the same priority for it to work
effectively).

One final issue is that you are unlikely to find a mechanism in a standard
VM that will allow you to schedule a periodic task with microsecond
resolution: Thread.sleep and Object.wait are unlikely to do it. But it
depends on the actual VM implementation.

HTH

David Holmes
(VM Engineer on Sun's Java Real-Time System)

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jean
> Morissette
> Sent: Tuesday, 19 May 2009 11:34 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Real-time task scheduling
>
>
>
> Hi,
> Giving a task with an execution cost of sub-microsecond and a period
> in the order of 10 microseconds, I would like to schedule periodically
> this task with hard real-time behaviour on the Java SE platform.
>
> I take as granted the following assumptions:
> (1) All objects in the program are allocated at the start and reused
> such that the GC does not pause the program.
> (2) All classes are loaded and compiled at the start (by example
> running in interpreted mode using -Xint JVM option or using AOT
> compiler like Excelsior JET) avoiding pause from JIT compiler.
> (3) No other user-program is running on the OS.
> (4) The number of threads created by the program is lower than the
> number of CPU, avoiding context-switching.
> (5) Concurrency is performed using lock-free data structures.
> (6) OS independance is not a requirement.
>
> Considering those assumptions are valid, one issue remaining is the
> unpredictability of the OS, which could use the CPU for its own needs
> at any moment, stoping the Java worker thread long enough to make the
> task miss its deadline. Is-it possible to resolve this issue?
>
> By example, assuming that the program run on a Real-Time OS, do the OS
> threads associated with the Java threads will have the same priority
> than the Java program? Otherwise, is-it possible to create a JNI call
> to change the OS thread priority associated with the current Java
> thread such that it cant be preempted? Or is-it possible to configure
> the OS to set an upper bound on thread pause time?
>
> Thanks for your help,
> Jean Morissette
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From peter.kovacs.1.0rc at gmail.com  Wed May 20 09:18:00 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 20 May 2009 15:18:00 +0200
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
Message-ID: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>

Hi,

Is it possible to get java.lang.InterruptedException, even if none of
the method along the call stack declares this exception?

We are experiencing such a case. Class.forName(String) appears to
throw the exception.

Is it not a breach of one of the fundamental contracts of the Java
language: "checked" exception are not allowed to be thrown in a method
without this method being declared to (potentially) throw it? (We
catch this exception as a Throwable in a catch(Throwable) clause.)

Thanks
Peter

From jim.andreou at gmail.com  Wed May 20 09:42:38 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Wed, 20 May 2009 16:42:38 +0300
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
Message-ID: <7d7138c10905200642l72454dd5h6b33192f75378a98@mail.gmail.com>

There are quite a few ways someone could throw a checked exception
without declaring it at compile time, actually.
Thread.stop(Throwable), Class.newInstance(), and there must be another
one too.

2009/5/20 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> Hi,
>
> Is it possible to get java.lang.InterruptedException, even if none of
> the method along the call stack declares this exception?
>
> We are experiencing such a case. Class.forName(String) appears to
> throw the exception.
>
> Is it not a breach of one of the fundamental contracts of the Java
> language: "checked" exception are not allowed to be thrown in a method
> without this method being declared to (potentially) throw it? (We
> catch this exception as a Throwable in a catch(Throwable) clause.)
>
> Thanks
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From mthornton at optrak.co.uk  Wed May 20 09:47:39 2009
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Wed, 20 May 2009 14:47:39 +0100
Subject: [concurrency-interest] Thread interruption
 protocol:	InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
Message-ID: <4A1409FB.3090502@optrak.co.uk>

P?ter Kov?cs wrote:
> Hi,
>
> Is it possible to get java.lang.InterruptedException, even if none of
> the method along the call stack declares this exception?
>
> We are experiencing such a case. Class.forName(String) appears to
> throw the exception.
>
> Is it not a breach of one of the fundamental contracts of the Java
> language: "checked" exception are not allowed to be thrown in a method
> without this method being declared to (potentially) throw it? (We
> catch this exception as a Throwable in a catch(Throwable) clause.)
>
>   
The restriction is not enforced by the JVM --- it is a compile time 
rule. So native code or byte code generated by a compiler other than 
javac can throw any exception regardless of whether it has been declared.

Mark Thornton


From peter.kovacs.1.0rc at gmail.com  Wed May 20 10:04:02 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 20 May 2009 16:04:02 +0200
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <7d7138c10905200642l72454dd5h6b33192f75378a98@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<7d7138c10905200642l72454dd5h6b33192f75378a98@mail.gmail.com>
Message-ID: <fdeb32eb0905200704v298db431rcd61f11913455830@mail.gmail.com>

Thanks, Jim,

I see: there are ways to break the contract. If Class.forName(String)
does this, is it a bug in the JVM?

Thanks
Peter

2009/5/20 Jim Andreou <jim.andreou at gmail.com>:
> There are quite a few ways someone could throw a checked exception
> without declaring it at compile time, actually.
> Thread.stop(Throwable), Class.newInstance(), and there must be another
> one too.
>
> 2009/5/20 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
>> Hi,
>>
>> Is it possible to get java.lang.InterruptedException, even if none of
>> the method along the call stack declares this exception?
>>
>> We are experiencing such a case. Class.forName(String) appears to
>> throw the exception.
>>
>> Is it not a breach of one of the fundamental contracts of the Java
>> language: "checked" exception are not allowed to be thrown in a method
>> without this method being declared to (potentially) throw it? (We
>> catch this exception as a Throwable in a catch(Throwable) clause.)
>>
>> Thanks
>> Peter
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From Thomas.Hawtin at Sun.COM  Wed May 20 10:11:19 2009
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Wed, 20 May 2009 15:11:19 +0100
Subject: [concurrency-interest] Thread interruption
	protocol:	InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
Message-ID: <4A140F87.6030403@sun.com>

P?ter Kov?cs wrote:
> Is it possible to get java.lang.InterruptedException, even if none of
> the method along the call stack declares this exception?
> 
> We are experiencing such a case. Class.forName(String) appears to
> throw the exception.

Class.newInstance is more likely to be the culprit:

"Note that this method propagates any exception thrown by the nullary 
constructor, including a checked exception. Use of this method 
effectively bypasses the compile-time exception checking that would 
otherwise be performed by the compiler. The Constructor.newInstance 
method avoids this problem by wrapping any exception thrown by the 
constructor in a (checked) InvocationTargetException."

http://java.sun.com/javase/6/docs/api/java/lang/Class.html#newInstance()

The same thing can be done with Thread.stop(Throwable), unchecked casts 
and playing silly buggers with bytecode

> Is it not a breach of one of the fundamental contracts of the Java
> language: "checked" exception are not allowed to be thrown in a method
> without this method being declared to (potentially) throw it? (We
> catch this exception as a Throwable in a catch(Throwable) clause.)

Of the language yes (with exception for unsafe casts); of the VM no.

Tom Hawtin

From peter.kovacs.1.0rc at gmail.com  Wed May 20 11:15:22 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 20 May 2009 17:15:22 +0200
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A140F87.6030403@sun.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
Message-ID: <fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>

Tom,

I highly appreciate your input.

The doc you quoted says that the original exception is wrapped in an
InvocationTargetException instance. Similarly, I'd expect that if an
exception occurs during Class loading, it is wrapped in an
ExceptionInInitializerError, correct?

What we're seeing in our case is a plain InterruptedException:

...
Caused by: java.lang.InterruptedException
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:164)
...

Thanks
Peter

2009/5/20 Tom Hawtin <Thomas.Hawtin at sun.com>:
> P?ter Kov?cs wrote:
>>
>> Is it possible to get java.lang.InterruptedException, even if none of
>> the method along the call stack declares this exception?
>>
>> We are experiencing such a case. Class.forName(String) appears to
>> throw the exception.
>
> Class.newInstance is more likely to be the culprit:
>
> "Note that this method propagates any exception thrown by the nullary
> constructor, including a checked exception. Use of this method effectively
> bypasses the compile-time exception checking that would otherwise be
> performed by the compiler. The Constructor.newInstance method avoids this
> problem by wrapping any exception thrown by the constructor in a (checked)
> InvocationTargetException."
>
> http://java.sun.com/javase/6/docs/api/java/lang/Class.html#newInstance()
>
> The same thing can be done with Thread.stop(Throwable), unchecked casts and
> playing silly buggers with bytecode
>
>> Is it not a breach of one of the fundamental contracts of the Java
>> language: "checked" exception are not allowed to be thrown in a method
>> without this method being declared to (potentially) throw it? (We
>> catch this exception as a Throwable in a catch(Throwable) clause.)
>
> Of the language yes (with exception for unsafe casts); of the VM no.
>
> Tom Hawtin
>


From i30817 at gmail.com  Wed May 20 11:32:09 2009
From: i30817 at gmail.com (Paulo Levi)
Date: Wed, 20 May 2009 16:32:09 +0100
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com> 
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
Message-ID: <212322090905200832j619dcccbx6c624a09d87177d@mail.gmail.com>

You're not reading the javadoc correctly:
*newInstance()*
"Note that this method propagates any exception thrown by the nullary
constructor, *including a checked exception*. Use of this method effectively
bypasses the compile-time exception checking that would otherwise be
performed by the compiler. The *Constructor.newInstance* method avoids this
problem by wrapping any exception thrown by the constructor in a (checked)
InvocationTargetException."
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090520/468ca8b2/attachment.html>

From peter.kovacs.1.0rc at gmail.com  Wed May 20 12:05:42 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 20 May 2009 18:05:42 +0200
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <212322090905200832j619dcccbx6c624a09d87177d@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<212322090905200832j619dcccbx6c624a09d87177d@mail.gmail.com>
Message-ID: <fdeb32eb0905200905v64bb98d1n9453f169b016140@mail.gmail.com>

Oh, I see Constructor.newInstance() as opposed to Class.newInstance().
But this probably irrelevant here as the stack trace fragment I
produced in my previous mail appears to show that the exception is
from Class.forName(String). Also, we call Constructor.newInstance()
instead of Class.newInstance():

	    Class<?> cl = Class.forName(name);
	    Constructor constr = cl.getConstructor(argcl);
	    return constr.newInstance(args);

I was just trying to apply Tom's input about passing exception
checking to the case we appear to have.

Thanks for making me aware of the difference anyway.

Peter


2009/5/20 Paulo Levi <i30817 at gmail.com>:
> You're not reading the javadoc correctly:
> newInstance()
> "Note that this method propagates any exception thrown by the nullary
> constructor, including a checked exception. Use of this method effectively
> bypasses the compile-time exception checking that would otherwise be
> performed by the compiler. The Constructor.newInstance method avoids this
> problem by wrapping any exception thrown by the constructor in a (checked)
> InvocationTargetException."
>

From takeshi10 at gmail.com  Wed May 20 13:23:22 2009
From: takeshi10 at gmail.com (Marcelo Fukushima)
Date: Wed, 20 May 2009 14:23:22 -0300
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
Message-ID: <7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>

in your code, the exception seems to come from native code, which also
doesnt have the checked exception restrictions that javac enforces (i
think)

2009/5/20 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> Tom,
>
> I highly appreciate your input.
>
> The doc you quoted says that the original exception is wrapped in an
> InvocationTargetException instance. Similarly, I'd expect that if an
> exception occurs during Class loading, it is wrapped in an
> ExceptionInInitializerError, correct?
>
> What we're seeing in our case is a plain InterruptedException:
>
> ...
> Caused by: java.lang.InterruptedException
> at java.lang.Class.forName0(Native Method)
> at java.lang.Class.forName(Class.java:164)
> ...
>
> Thanks
> Peter
>
> 2009/5/20 Tom Hawtin <Thomas.Hawtin at sun.com>:
>> P?ter Kov?cs wrote:
>>>
>>> Is it possible to get java.lang.InterruptedException, even if none of
>>> the method along the call stack declares this exception?
>>>
>>> We are experiencing such a case. Class.forName(String) appears to
>>> throw the exception.
>>
>> Class.newInstance is more likely to be the culprit:
>>
>> "Note that this method propagates any exception thrown by the nullary
>> constructor, including a checked exception. Use of this method effectively
>> bypasses the compile-time exception checking that would otherwise be
>> performed by the compiler. The Constructor.newInstance method avoids this
>> problem by wrapping any exception thrown by the constructor in a (checked)
>> InvocationTargetException."
>>
>> http://java.sun.com/javase/6/docs/api/java/lang/Class.html#newInstance()
>>
>> The same thing can be done with Thread.stop(Throwable), unchecked casts and
>> playing silly buggers with bytecode
>>
>>> Is it not a breach of one of the fundamental contracts of the Java
>>> language: "checked" exception are not allowed to be thrown in a method
>>> without this method being declared to (potentially) throw it? (We
>>> catch this exception as a Throwable in a catch(Throwable) clause.)
>>
>> Of the language yes (with exception for unsafe casts); of the VM no.
>>
>> Tom Hawtin
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
[]'s
Marcelo Takeshi Fukushima


From peter.kovacs.1.0rc at gmail.com  Wed May 20 13:56:03 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Wed, 20 May 2009 19:56:03 +0200
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
Message-ID: <fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>

Thank you, Marcelo!

May this be, then, a bug in the JVM?

Thanks
Peter

2009/5/20 Marcelo Fukushima <takeshi10 at gmail.com>:
> in your code, the exception seems to come from native code, which also
> doesnt have the checked exception restrictions that javac enforces (i
> think)
>
> 2009/5/20 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
>> Tom,
>>
>> I highly appreciate your input.
>>
>> The doc you quoted says that the original exception is wrapped in an
>> InvocationTargetException instance. Similarly, I'd expect that if an
>> exception occurs during Class loading, it is wrapped in an
>> ExceptionInInitializerError, correct?
>>
>> What we're seeing in our case is a plain InterruptedException:
>>
>> ...
>> Caused by: java.lang.InterruptedException
>> at java.lang.Class.forName0(Native Method)
>> at java.lang.Class.forName(Class.java:164)
>> ...
>>
>> Thanks
>> Peter
>>
>> 2009/5/20 Tom Hawtin <Thomas.Hawtin at sun.com>:
>>> P?ter Kov?cs wrote:
>>>>
>>>> Is it possible to get java.lang.InterruptedException, even if none of
>>>> the method along the call stack declares this exception?
>>>>
>>>> We are experiencing such a case. Class.forName(String) appears to
>>>> throw the exception.
>>>
>>> Class.newInstance is more likely to be the culprit:
>>>
>>> "Note that this method propagates any exception thrown by the nullary
>>> constructor, including a checked exception. Use of this method effectively
>>> bypasses the compile-time exception checking that would otherwise be
>>> performed by the compiler. The Constructor.newInstance method avoids this
>>> problem by wrapping any exception thrown by the constructor in a (checked)
>>> InvocationTargetException."
>>>
>>> http://java.sun.com/javase/6/docs/api/java/lang/Class.html#newInstance()
>>>
>>> The same thing can be done with Thread.stop(Throwable), unchecked casts and
>>> playing silly buggers with bytecode
>>>
>>>> Is it not a breach of one of the fundamental contracts of the Java
>>>> language: "checked" exception are not allowed to be thrown in a method
>>>> without this method being declared to (potentially) throw it? (We
>>>> catch this exception as a Throwable in a catch(Throwable) clause.)
>>>
>>> Of the language yes (with exception for unsafe casts); of the VM no.
>>>
>>> Tom Hawtin
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> []'s
> Marcelo Takeshi Fukushima
>


From gregg at cytetech.com  Wed May 20 14:54:25 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 20 May 2009 13:54:25 -0500
Subject: [concurrency-interest] Thread interruption protocol:
 InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
Message-ID: <4A1451E1.5080907@cytetech.com>

Any thread may be interrupted at any point in its execution path.  The fact that 
InterruptedException is a checked exception is the predominant issue from my 
perspective.  It can be thrown from code which does not declare it to be thrown 
as a checked exception.  InterruptedException should be a RuntimeException 
because of this...  But, this will likely never be something that can be changed 
for backward compatibility.

One "solution" could be to create a new RuntimeException, perhaps 
ThreadInterruptedException and the documentation changed to deprecate 
InterruptedException in favor of such a RuntimException.  But changing what is 
thrown would be a compatibility issue in and of itself.  Thus, a new 
Class.forName() derivative would have to be created as well which would throw 
this new exception.

Thus, I'm not convinced there is anything you can do besides what you are doing, 
and catch "Exception" and do "instanceof" checks to see if it is something that 
you know how to handle specifically.

Gregg Wonderly

P?ter Kov?cs wrote:
> Thank you, Marcelo!
> 
> May this be, then, a bug in the JVM?
> 
> Thanks
> Peter
> 
> 2009/5/20 Marcelo Fukushima <takeshi10 at gmail.com>:
>> in your code, the exception seems to come from native code, which also
>> doesnt have the checked exception restrictions that javac enforces (i
>> think)
>>
>> 2009/5/20 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
>>> Tom,
>>>
>>> I highly appreciate your input.
>>>
>>> The doc you quoted says that the original exception is wrapped in an
>>> InvocationTargetException instance. Similarly, I'd expect that if an
>>> exception occurs during Class loading, it is wrapped in an
>>> ExceptionInInitializerError, correct?
>>>
>>> What we're seeing in our case is a plain InterruptedException:
>>>
>>> ...
>>> Caused by: java.lang.InterruptedException
>>> at java.lang.Class.forName0(Native Method)
>>> at java.lang.Class.forName(Class.java:164)
>>> ...
>>>
>>> Thanks
>>> Peter
>>>
>>> 2009/5/20 Tom Hawtin <Thomas.Hawtin at sun.com>:
>>>> P?ter Kov?cs wrote:
>>>>> Is it possible to get java.lang.InterruptedException, even if none of
>>>>> the method along the call stack declares this exception?
>>>>>
>>>>> We are experiencing such a case. Class.forName(String) appears to
>>>>> throw the exception.
>>>> Class.newInstance is more likely to be the culprit:
>>>>
>>>> "Note that this method propagates any exception thrown by the nullary
>>>> constructor, including a checked exception. Use of this method effectively
>>>> bypasses the compile-time exception checking that would otherwise be
>>>> performed by the compiler. The Constructor.newInstance method avoids this
>>>> problem by wrapping any exception thrown by the constructor in a (checked)
>>>> InvocationTargetException."
>>>>
>>>> http://java.sun.com/javase/6/docs/api/java/lang/Class.html#newInstance()
>>>>
>>>> The same thing can be done with Thread.stop(Throwable), unchecked casts and
>>>> playing silly buggers with bytecode
>>>>
>>>>> Is it not a breach of one of the fundamental contracts of the Java
>>>>> language: "checked" exception are not allowed to be thrown in a method
>>>>> without this method being declared to (potentially) throw it? (We
>>>>> catch this exception as a Throwable in a catch(Throwable) clause.)
>>>> Of the language yes (with exception for unsafe casts); of the VM no.
>>>>
>>>> Tom Hawtin
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> --
>> []'s
>> Marcelo Takeshi Fukushima
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Wed May 20 15:34:12 2009
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 20 May 2009 14:34:12 -0500
Subject: [concurrency-interest] Thread interruption protocol:
 InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A1451E1.5080907@cytetech.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>	<4A140F87.6030403@sun.com>	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
	<4A1451E1.5080907@cytetech.com>
Message-ID: <4A145B34.6010102@redhat.com>

On 05/20/2009 01:54 PM, Gregg Wonderly wrote:
> Any thread may be interrupted at any point in its execution path.  The 
> fact that InterruptedException is a checked exception is the predominant 
> issue from my perspective.  It can be thrown from code which does not 
> declare it to be thrown as a checked exception.

Um, no, at least no more than any other checked exception.

All interruption does is set a flag.  If the interrupted thread is blocked 
in an interruptible operation, then the thread is unblocked and it will 
generally either throw InterruptedException or re-set the interrupted flag 
and do Something Else (which might include throwing some other exception).

The point of this issue is that Class.newInstance() can throw undeclared 
checked exceptions, so you shouldn't use it, period.  Use 
Class.getConstructor().newInstance() instead.  Other than that, you 
generally won't get InterruptedException unless you're calling a method 
that declares it.

- DML


From davidcholmes at aapt.net.au  Wed May 20 15:57:16 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 21 May 2009 05:57:16 +1000
Subject: [concurrency-interest] Thread interruption
	protocol:InterruptedException is a "checked" exception, correct?
In-Reply-To: <fdeb32eb0905200905v64bb98d1n9453f169b016140@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEMBIBAA.davidcholmes@aapt.net.au>

Right - Class.newInstance() was recognized as being broken but deemed
unfixable. Constructor.newInstance() does the right thing.

In general - absent of Class.newInstance() (I vaguely recall there being
another case too ...) or Thread.stop(t) - you should not get an unexpected
checked exception.

Back to the case at hand - which version of the VM are you using? There is a
bug in older Hotspot versions were an interrupt while blocked waiting on the
class initialization monitor, causes InterruptedException to be propagated -
which causes it to be thrown from very unexpected places. In fact I think
this affects all JDK 5 versions of Hotspot.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Thursday, 21 May 2009 2:06 AM
> To: Paulo Levi
> Cc: concurrency-interest at cs.oswego.edu; Tom Hawtin
> Subject: Re: [concurrency-interest] Thread interruption
> protocol:InterruptedException is a "checked" exception, correct?
>
>
>
> Oh, I see Constructor.newInstance() as opposed to Class.newInstance().
> But this probably irrelevant here as the stack trace fragment I
> produced in my previous mail appears to show that the exception is
> from Class.forName(String). Also, we call Constructor.newInstance()
> instead of Class.newInstance():
>
> 	    Class<?> cl = Class.forName(name);
> 	    Constructor constr = cl.getConstructor(argcl);
> 	    return constr.newInstance(args);
>
> I was just trying to apply Tom's input about passing exception
> checking to the case we appear to have.
>
> Thanks for making me aware of the difference anyway.
>
> Peter
>
>
> 2009/5/20 Paulo Levi <i30817 at gmail.com>:
> > You're not reading the javadoc correctly:
> > newInstance()
> > "Note that this method propagates any exception thrown by the nullary
> > constructor, including a checked exception. Use of this method
> effectively
> > bypasses the compile-time exception checking that would otherwise be
> > performed by the compiler. The Constructor.newInstance method
> avoids this
> > problem by wrapping any exception thrown by the constructor in
> a (checked)
> > InvocationTargetException."
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From davidcholmes at aapt.net.au  Wed May 20 16:55:16 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 21 May 2009 06:55:16 +1000
Subject: [concurrency-interest] Thread
	interruptionprotocol:InterruptedException is a "checked"
	exception, correct?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEMBIBAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEMCIBAA.davidcholmes@aapt.net.au>

I wrote: 
> In fact I think this affects all JDK 5 versions of Hotspot.

It's just been fixed in 5u18

David

> 
> David Holmes
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> > Kovacs
> > Sent: Thursday, 21 May 2009 2:06 AM
> > To: Paulo Levi
> > Cc: concurrency-interest at cs.oswego.edu; Tom Hawtin
> > Subject: Re: [concurrency-interest] Thread interruption
> > protocol:InterruptedException is a "checked" exception, correct?
> >
> >
> >
> > Oh, I see Constructor.newInstance() as opposed to Class.newInstance().
> > But this probably irrelevant here as the stack trace fragment I
> > produced in my previous mail appears to show that the exception is
> > from Class.forName(String). Also, we call Constructor.newInstance()
> > instead of Class.newInstance():
> >
> > 	    Class<?> cl = Class.forName(name);
> > 	    Constructor constr = cl.getConstructor(argcl);
> > 	    return constr.newInstance(args);
> >
> > I was just trying to apply Tom's input about passing exception
> > checking to the case we appear to have.
> >
> > Thanks for making me aware of the difference anyway.
> >
> > Peter
> >
> >
> > 2009/5/20 Paulo Levi <i30817 at gmail.com>:
> > > You're not reading the javadoc correctly:
> > > newInstance()
> > > "Note that this method propagates any exception thrown by the nullary
> > > constructor, including a checked exception. Use of this method
> > effectively
> > > bypasses the compile-time exception checking that would otherwise be
> > > performed by the compiler. The Constructor.newInstance method
> > avoids this
> > > problem by wrapping any exception thrown by the constructor in
> > a (checked)
> > > InvocationTargetException."
> > >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.kovacs.1.0rc at gmail.com  Thu May 21 08:29:47 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Thu, 21 May 2009 14:29:47 +0200
Subject: [concurrency-interest] Thread
	interruptionprotocol:InterruptedException is a "checked"
	exception, correct?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEMCIBAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEMBIBAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCCEMCIBAA.davidcholmes@aapt.net.au>
Message-ID: <fdeb32eb0905210529y78343960tb619f25f63b6ef11@mail.gmail.com>

Bingo! We use 1.5.0_15. Thank you, David, for this info! It helps a lot.

May I push on with this issue?

Can you, please, tell me how Class.forName is supposed to handle
thread interruptions? Will/should it simply ignore them? (What else
could it do about interruption.)

Also, I am a bit uncertain about the notion of thread interruption in
this context. Does it mean a purely Java mechanism? Inside the JVM
only? Or does it translate to some operating system mechanism -- on
some platforms, perhaps (on Solaris e.g.)? Are there ways to interrupt
a Java thread apart from the Thread.interrupt() API call (through some
operating system facilities e.g.)?

Thanks,
Peter

On Wed, May 20, 2009 at 10:55 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> I wrote:
>> In fact I think this affects all JDK 5 versions of Hotspot.
>
> It's just been fixed in 5u18
>
> David
>
>>
>> David Holmes
>>
>> > -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
>> > Kovacs
>> > Sent: Thursday, 21 May 2009 2:06 AM
>> > To: Paulo Levi
>> > Cc: concurrency-interest at cs.oswego.edu; Tom Hawtin
>> > Subject: Re: [concurrency-interest] Thread interruption
>> > protocol:InterruptedException is a "checked" exception, correct?
>> >
>> >
>> >
>> > Oh, I see Constructor.newInstance() as opposed to Class.newInstance().
>> > But this probably irrelevant here as the stack trace fragment I
>> > produced in my previous mail appears to show that the exception is
>> > from Class.forName(String). Also, we call Constructor.newInstance()
>> > instead of Class.newInstance():
>> >
>> > ? ? ? ? Class<?> cl = Class.forName(name);
>> > ? ? ? ? Constructor constr = cl.getConstructor(argcl);
>> > ? ? ? ? return constr.newInstance(args);
>> >
>> > I was just trying to apply Tom's input about passing exception
>> > checking to the case we appear to have.
>> >
>> > Thanks for making me aware of the difference anyway.
>> >
>> > Peter
>> >
>> >
>> > 2009/5/20 Paulo Levi <i30817 at gmail.com>:
>> > > You're not reading the javadoc correctly:
>> > > newInstance()
>> > > "Note that this method propagates any exception thrown by the nullary
>> > > constructor, including a checked exception. Use of this method
>> > effectively
>> > > bypasses the compile-time exception checking that would otherwise be
>> > > performed by the compiler. The Constructor.newInstance method
>> > avoids this
>> > > problem by wrapping any exception thrown by the constructor in
>> > a (checked)
>> > > InvocationTargetException."
>> > >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From Thomas.Hawtin at Sun.COM  Thu May 21 08:43:02 2009
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Thu, 21 May 2009 13:43:02 +0100
Subject: [concurrency-interest] Thread interruption protocol:
 InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A1451E1.5080907@cytetech.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
	<4A1451E1.5080907@cytetech.com>
Message-ID: <4A154C56.3010002@sun.com>

Gregg Wonderly wrote:
> Any thread may be interrupted at any point in its execution path.  The 
> fact that InterruptedException is a checked exception is the predominant 
> issue from my perspective.  It can be thrown from code which does not 
> declare it to be thrown as a checked exception.  InterruptedException 
> should be a RuntimeException because of this...  But, this will likely 
> never be something that can be changed for backward compatibility.

Inserting a superclass like that should not usually be a significant 
campatibility problem. In the specific case of Exception -> 
RuntimeException there are a couple problems that spring to mind.

  * Where methods (such as Atomic*FieldUpdate.newInstance) are specified 
to throw a RuntimeException in some cases, which may result in a catch 
RuntimeException without a catching all checked Exceptions.
  * AccessController.doPrivileged has freaky a specification and, in 
Sun's JRE, implementations - four overloads, but only two 
implementations. The PrivilegedExceptionAction versions let unchecked 
exceptions through (to be similar to the PrivilegedAction versions) but 
wrap checked exceptions.

OTOH, interruprs are a PITA. Making them disappear is probably not going 
to encourage people to make an effort to handle them sensibly.

Tom Hawtin

From jason_mehrens at hotmail.com  Thu May 21 11:29:08 2009
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Thu, 21 May 2009 10:29:08 -0500
Subject: [concurrency-interest] Thread interruption protocol:
 InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A145B34.6010102@redhat.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
	<4A1451E1.5080907@cytetech.com>  <4A145B34.6010102@redhat.com>
Message-ID: <BLU134-W321EBDA4420E774C2A4A683590@phx.gbl>



> The point of this issue is that Class.newInstance() can throw undeclared 
> checked exceptions, so you shouldn't use it, period. Use 
> Class.getConstructor().newInstance() instead. Other than that, you 
> generally won't get InterruptedException unless you're calling a method 
> that declares it.


On a related note, I've filed an RFE with findbugs so that a future version may actually flag calls to Class.newInstance.  I have yet to write a detector for it.

 

Jason Mehrens

_________________________________________________________________
Hotmail? has ever-growing storage! Don?t worry about storage limits.
http://windowslive.com/Tutorial/Hotmail/Storage?ocid=TXT_TAGLM_WL_HM_Tutorial_Storage1_052009
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090521/7ad0a233/attachment.html>

From gregg at cytetech.com  Thu May 21 17:46:24 2009
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 21 May 2009 16:46:24 -0500
Subject: [concurrency-interest] Thread interruption protocol:
 InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A145B34.6010102@redhat.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
	<4A1451E1.5080907@cytetech.com> <4A145B34.6010102@redhat.com>
Message-ID: <4A15CBB0.6030209@cytetech.com>

David M. Lloyd wrote:
> On 05/20/2009 01:54 PM, Gregg Wonderly wrote:
>> Any thread may be interrupted at any point in its execution path.  The 
>> fact that InterruptedException is a checked exception is the 
>> predominant issue from my perspective.  It can be thrown from code 
>> which does not declare it to be thrown as a checked exception.
> 
> Um, no, at least no more than any other checked exception.

Class.forName() is the issue in this case.  This specific instance seems to be 
caused by a JVM bug in Sun's jdk1.5.

I've tried to stay away from InterruptedException and Thread.interrupt().  I 
guess I am under the incorrect assumption that a call to Thread.interrupt() can 
cause an InterruptedException to be raised in a thread executing inside of a 
method without a "throws InterruptedException" clause?

Gregg Wonderly

From davidcholmes at aapt.net.au  Thu May 21 17:57:28 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 22 May 2009 07:57:28 +1000
Subject: [concurrency-interest] Thread
	interruptionprotocol:InterruptedException is a "checked"
	exception, correct?
In-Reply-To: <fdeb32eb0905210529y78343960tb619f25f63b6ef11@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEMIIBAA.davidcholmes@aapt.net.au>

Hi Peter,

Thread.interrupt() is a purely Java-level functionality - it exists only in the VM. It is not a "signalling" mechanism (by which I mean POSIX signals), and has nothing to do with "interrupts" (normal hardware and software notion). It is simply a mechanism to request a thread to cancel what it is doing via a simple flag, coupled with a handful of specific blocking methods that say "if a thread is interrupted on entry to this method, or is interrupted while blocked in this method, then it will return immediately and throw InterruptedException." In this way Thread.interrupt can be used to implement a cancellation mechanism, and any method that wants to make itself a cancellation point can declare that it will check the interrupt state and respond accordingly. (But all this is covered in detail in "Concurrent Programming in Java" and "Java Concurrency in Practice" ;-) ).

Class.forName - like most methods - doesn't say anything about Thread interruption, which means (in my view) that it should do nothing about it ie. it should ignore the fact that the thread might be interrupted, and if by some chance it encountered InterruptedException directly (which it can't rethrow) then it should simply re-assert the interrupt state of the thread (which is the standard advice for handling interruption).

One other area where it gets a little murky is "interruptible I/O" which was only ever enabled on Solaris - it can have unintended side-effects too, hence it's now disabled by default in the latest Java 6 and 7 VMs. Interruptible I/O is generally unworkable and has been replaced with more well-defined interruption semantics in the NIO package (streams/channels are closed upon interrupt). Again this is discussed in the books as well as, I expect, some onloine articles.

HTH

David

> -----Original Message-----
> From: peter.dunay.kovacs at gmail.com
> [mailto:peter.dunay.kovacs at gmail.com]On Behalf Of P?ter Kov?cs
> Sent: Thursday, 21 May 2009 10:30 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Thread
> interruptionprotocol:InterruptedException is a "checked" exception,
> correct?
> 
> 
> 
> Bingo! We use 1.5.0_15. Thank you, David, for this info! It helps a lot.
> 
> May I push on with this issue?
> 
> Can you, please, tell me how Class.forName is supposed to handle
> thread interruptions? Will/should it simply ignore them? (What else
> could it do about interruption.)
> 
> Also, I am a bit uncertain about the notion of thread interruption in
> this context. Does it mean a purely Java mechanism? Inside the JVM
> only? Or does it translate to some operating system mechanism -- on
> some platforms, perhaps (on Solaris e.g.)? Are there ways to interrupt
> a Java thread apart from the Thread.interrupt() API call (through some
> operating system facilities e.g.)?
> 
> Thanks,
> Peter
> 
> On Wed, May 20, 2009 at 10:55 PM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> > I wrote:
> >> In fact I think this affects all JDK 5 versions of Hotspot.
> >
> > It's just been fixed in 5u18
> >
> > David
> >
> >>
> >> David Holmes
> >>
> >> > -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> >> > Kovacs
> >> > Sent: Thursday, 21 May 2009 2:06 AM
> >> > To: Paulo Levi
> >> > Cc: concurrency-interest at cs.oswego.edu; Tom Hawtin
> >> > Subject: Re: [concurrency-interest] Thread interruption
> >> > protocol:InterruptedException is a "checked" exception, correct?
> >> >
> >> >
> >> >
> >> > Oh, I see Constructor.newInstance() as opposed to 
> Class.newInstance().
> >> > But this probably irrelevant here as the stack trace fragment I
> >> > produced in my previous mail appears to show that the exception is
> >> > from Class.forName(String). Also, we call Constructor.newInstance()
> >> > instead of Class.newInstance():
> >> >
> >> >         Class<?> cl = Class.forName(name);
> >> >         Constructor constr = cl.getConstructor(argcl);
> >> >         return constr.newInstance(args);
> >> >
> >> > I was just trying to apply Tom's input about passing exception
> >> > checking to the case we appear to have.
> >> >
> >> > Thanks for making me aware of the difference anyway.
> >> >
> >> > Peter
> >> >
> >> >
> >> > 2009/5/20 Paulo Levi <i30817 at gmail.com>:
> >> > > You're not reading the javadoc correctly:
> >> > > newInstance()
> >> > > "Note that this method propagates any exception thrown by 
> the nullary
> >> > > constructor, including a checked exception. Use of this method
> >> > effectively
> >> > > bypasses the compile-time exception checking that would 
> otherwise be
> >> > > performed by the compiler. The Constructor.newInstance method
> >> > avoids this
> >> > > problem by wrapping any exception thrown by the constructor in
> >> > a (checked)
> >> > > InvocationTargetException."
> >> > >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 




From davidcholmes at aapt.net.au  Thu May 21 18:08:36 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 22 May 2009 08:08:36 +1000
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A15CBB0.6030209@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMIIBAA.davidcholmes@aapt.net.au>

Gregg Wonderly writes:
> I've tried to stay away from InterruptedException and
> Thread.interrupt().  I guess I am under the incorrect assumption that a
call to
> Thread.interrupt() can cause an InterruptedException to be raised in a
thread executing
> inside of a method without a "throws InterruptedException" clause?

Thread.interrupt does not trigger an asynchronous exception the way
Thread.stop does. Specific blocking methods in the core APIs are specified
to raise/throw InterruptedException if they detect thread interruption and
as it is a checked exception you are generally aware (modulo VM bugs and
Class.newInstance quirkiness) where this can arise. Other methods that
utilize blocking are then encouraged to throw InterruptedException if they
wish to be "cancellation points". The I/O API's take a different approach to
responding to interrupts, but are still basically cancellation points -
where cancelling implies "close this channel".

David Holmes



From takeshi10 at gmail.com  Thu May 21 18:08:58 2009
From: takeshi10 at gmail.com (Marcelo Fukushima)
Date: Thu, 21 May 2009 19:08:58 -0300
Subject: [concurrency-interest] Thread interruption protocol:
	InterruptedException is a "checked" exception, correct?
In-Reply-To: <4A15CBB0.6030209@cytetech.com>
References: <fdeb32eb0905200618m5390afb1r1cc635f1accb57cd@mail.gmail.com>
	<4A140F87.6030403@sun.com>
	<fdeb32eb0905200815kccad6fagfc4f2ef3fa2b272e@mail.gmail.com>
	<7288749d0905201023k4f61bc55l3839fbf5160d1d14@mail.gmail.com>
	<fdeb32eb0905201056k229d796bk8dacba32d2824067@mail.gmail.com>
	<4A1451E1.5080907@cytetech.com> <4A145B34.6010102@redhat.com>
	<4A15CBB0.6030209@cytetech.com>
Message-ID: <7288749d0905211508of490998m372d05d1d68bf909@mail.gmail.com>

just to make it clear:

-Native code and generated bytecode (compiled by compilers other than
javac) can throw any checked exception without the throws clause given
that at the bytecode level, theres no such thing as checked or
unchecked exception
-Class.newInstance is a degenerated method that also breaks the
checked exception ruling.
-The InterruptedException from the OP was a result of a bug in the jvm



On Thu, May 21, 2009 at 6:46 PM, Gregg Wonderly <gregg at cytetech.com> wrote:
> David M. Lloyd wrote:
>>
>> On 05/20/2009 01:54 PM, Gregg Wonderly wrote:
>>>
>>> Any thread may be interrupted at any point in its execution path. ?The
>>> fact that InterruptedException is a checked exception is the predominant
>>> issue from my perspective. ?It can be thrown from code which does not
>>> declare it to be thrown as a checked exception.
>>
>> Um, no, at least no more than any other checked exception.
>
> Class.forName() is the issue in this case. ?This specific instance seems to
> be caused by a JVM bug in Sun's jdk1.5.
>
> I've tried to stay away from InterruptedException and Thread.interrupt(). ?I
> guess I am under the incorrect assumption that a call to Thread.interrupt()
> can cause an InterruptedException to be raised in a thread executing inside
> of a method without a "throws InterruptedException" clause?
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
[]'s
Marcelo Takeshi Fukushima


From davidcholmes at aapt.net.au  Thu May 21 18:25:56 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 22 May 2009 08:25:56 +1000
Subject: [concurrency-interest] Thread interruption
	protocol:InterruptedException is a "checked" exception, correct?
In-Reply-To: <7288749d0905211508of490998m372d05d1d68bf909@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMJIBAA.davidcholmes@aapt.net.au>

Marcelo Fukushima writes:
> just to make it clear:
>
> -Native code and generated bytecode (compiled by compilers other than
> javac) can throw any checked exception without the throws clause given
> that at the bytecode level, theres no such thing as checked or
> unchecked exception
> -Class.newInstance is a degenerated method that also breaks the
> checked exception ruling.
> -The InterruptedException from the OP was a result of a bug in the jvm

All correct. "checked exceptions" are only a compile-time concept in the
Java language.

David Holmes



From brian at briangoetz.com  Mon May 25 02:56:56 2009
From: brian at briangoetz.com (Brian Goetz)
Date: Mon, 25 May 2009 02:56:56 -0400
Subject: [concurrency-interest] Interesting story from Dave Dice
Message-ID: <4A1A4138.9010405@briangoetz.com>

http://blogs.sun.com/dave/entry/memcpy_concurrency_curiosities


From peter.kovacs.1.0rc at gmail.com  Mon May 25 05:43:21 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Mon, 25 May 2009 11:43:21 +0200
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <4A1A4138.9010405@briangoetz.com>
References: <4A1A4138.9010405@briangoetz.com>
Message-ID: <fdeb32eb0905250243l4b65c648ka773aa53de89cecb@mail.gmail.com>

1. If an operation/class/method/whatever is not declared to be
thread-safe, you can assume it is not. Correct? (If it is declared to
be so, you can, perhaps, be optimistic and assume it is.)

2. If a variable (an array in this case) is written to, the variable
is not "effectively immutable". (Probably useless of nitpicking on the
meaning of "effectively".)

Is this really novel?

Peter

On Mon, May 25, 2009 at 8:56 AM, Brian Goetz <brian at briangoetz.com> wrote:
> http://blogs.sun.com/dave/entry/memcpy_concurrency_curiosities
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From davidcholmes at aapt.net.au  Mon May 25 06:46:22 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 25 May 2009 20:46:22 +1000
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <fdeb32eb0905250243l4b65c648ka773aa53de89cecb@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCENGIBAA.davidcholmes@aapt.net.au>

Peter Kovacs writes:
> 1. If an operation/class/method/whatever is not declared to be
> thread-safe, you can assume it is not. Correct? (If it is declared to
> be so, you can, perhaps, be optimistic and assume it is.)
>
> 2. If a variable (an array in this case) is written to, the variable
> is not "effectively immutable". (Probably useless of nitpicking on the
> meaning of "effectively".)
>
> Is this really novel?
>
I think this certainly violates the principle of least surprise. If you just
want to _read_ an array from native code, then there are no specific methods
for that, you have to use these JNI methods that assume you want to write to
the array as well. Even realizing that these are potentially mutating
methods, I still find it surprising that writing back the same values that
you read is not guaranteed to be seen correctly. Looking at what actually
happens I can go "Oh I see, yes that makes sense" - but there's no way I
would have thought of that happening before-hand.

David Holmes



From peter.kovacs.1.0rc at gmail.com  Mon May 25 08:13:02 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Mon, 25 May 2009 14:13:02 +0200
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCENGIBAA.davidcholmes@aapt.net.au>
References: <fdeb32eb0905250243l4b65c648ka773aa53de89cecb@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENGIBAA.davidcholmes@aapt.net.au>
Message-ID: <fdeb32eb0905250513s66c9b987v7dd92695cde2d85d@mail.gmail.com>

It is certainly rather subjective who finds what surprising. :-) To
me, the surprising part is that you cannot read without subsequently
writing back.

Thanks
Peter

On Mon, May 25, 2009 at 12:46 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Peter Kovacs writes:
>> 1. If an operation/class/method/whatever is not declared to be
>> thread-safe, you can assume it is not. Correct? (If it is declared to
>> be so, you can, perhaps, be optimistic and assume it is.)
>>
>> 2. If a variable (an array in this case) is written to, the variable
>> is not "effectively immutable". (Probably useless of nitpicking on the
>> meaning of "effectively".)
>>
>> Is this really novel?
>>
> I think this certainly violates the principle of least surprise. If you just
> want to _read_ an array from native code, then there are no specific methods
> for that, you have to use these JNI methods that assume you want to write to
> the array as well. Even realizing that these are potentially mutating
> methods, I still find it surprising that writing back the same values that
> you read is not guaranteed to be seen correctly. Looking at what actually
> happens I can go "Oh I see, yes that makes sense" - but there's no way I
> would have thought of that happening before-hand.
>
> David Holmes
>
>
>

From peter.kovacs.1.0rc at gmail.com  Mon May 25 11:27:45 2009
From: peter.kovacs.1.0rc at gmail.com (=?UTF-8?B?UMOpdGVyIEtvdsOhY3M=?=)
Date: Mon, 25 May 2009 17:27:45 +0200
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <fdeb32eb0905250513s66c9b987v7dd92695cde2d85d@mail.gmail.com>
References: <fdeb32eb0905250243l4b65c648ka773aa53de89cecb@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENGIBAA.davidcholmes@aapt.net.au>
	<fdeb32eb0905250513s66c9b987v7dd92695cde2d85d@mail.gmail.com>
Message-ID: <fdeb32eb0905250827v7df2298blb9945e686a064c84@mail.gmail.com>

I have to retract my previous mail. I see that the write-back feature
is even mentioned in the Tech Tips article. Apart from the awkward
name for this JNI constant, it appears to be a pure operator error not
to use JNI_ABORT for an immutable array.

Peter

2009/5/25 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> It is certainly rather subjective who finds what surprising. :-) To
> me, the surprising part is that you cannot read without subsequently
> writing back.
>
> Thanks
> Peter
>
> On Mon, May 25, 2009 at 12:46 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
>> Peter Kovacs writes:
>>> 1. If an operation/class/method/whatever is not declared to be
>>> thread-safe, you can assume it is not. Correct? (If it is declared to
>>> be so, you can, perhaps, be optimistic and assume it is.)
>>>
>>> 2. If a variable (an array in this case) is written to, the variable
>>> is not "effectively immutable". (Probably useless of nitpicking on the
>>> meaning of "effectively".)
>>>
>>> Is this really novel?
>>>
>> I think this certainly violates the principle of least surprise. If you just
>> want to _read_ an array from native code, then there are no specific methods
>> for that, you have to use these JNI methods that assume you want to write to
>> the array as well. Even realizing that these are potentially mutating
>> methods, I still find it surprising that writing back the same values that
>> you read is not guaranteed to be seen correctly. Looking at what actually
>> happens I can go "Oh I see, yes that makes sense" - but there's no way I
>> would have thought of that happening before-hand.
>>
>> David Holmes
>>
>>
>>
>


From gauravty at in.ibm.com  Mon May 25 12:56:53 2009
From: gauravty at in.ibm.com (Gaurav Kant Tyagi)
Date: Mon, 25 May 2009 22:26:53 +0530
Subject: [concurrency-interest] Gaurav Kant Tyagi is out of the office.
Message-ID: <OFBBF425B2.D710993B-ON652575C1.005D193D-652575C1.005D193D@in.ibm.com>

I will be out of the office starting  05/25/2009 and will not return until
06/01/2009.

I will respond to your message when I return.


From concurrency-interest at cs.oswego.edu  Tue May 26 06:43:02 2009
From: concurrency-interest at cs.oswego.edu (VIAGRA Inc.)
Date: Tue, 26 May 2009 12:43:02 +0200
Subject: [concurrency-interest] Pharmacy Online Sale 81% OFF!
Message-ID: <20090526144302.2585.qmail@LB-1205>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090526/aa77480f/attachment.html>
-------------- next part --------------


New from WebMD: Dear concurrency-interest at cs.oswego.edu!. Sign-up today! 



You are subscribed as concurrency-interest at cs.oswego.edu.
View and manage your WebMD newsletter preferences.
Subscribe to more newsletters. Change/update  your email address. 

WebMD Privacy Policy 
WebMD Office of Privacy
1175 Peachtree Street, Suite 2400, Atlanta, GA 30361
? 2009 WebMD, LLC. All rights reserved.

From karlthepagan at gmail.com  Mon May 25 20:08:37 2009
From: karlthepagan at gmail.com (karlthepagan)
Date: Mon, 25 May 2009 17:08:37 -0700
Subject: [concurrency-interest] Offering data to Futures
Message-ID: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>

As a followup to last week's great discussion of FutureTask.done() callbacks
I'd like to ask if anyone else has rolled their own "remote Future"
implementation or something which interfaces with a messaging library? I
found one attribute of FutureTask.Sync is that it protects innerSet and
innerSetException from being set outside of innerRun so as far as I imagined
it cannot be extend to provide a receiver for messages.

I find the Future API so comfortable that I wanted to apply it to other
IO-bound and remote tasks rather than simply CPU-bound tasks, but consuming
a thread per request isn't usually desirable. For example the Jersey HTTP
client API makes use of futures to solve an IO-bound task:
http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html


With that in mind here is an API inspired by a few iterations of this
problem:

public interface MessageFuture<V> extends Future<V> {
    boolean offer(V data);
    boolean offerException(Throwable t);
}


I have a possibly naive implementation up on GoogleCode, but feedback is
welcome:
https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/

After providing something that could meet the needs of Jersey and/or Grizzly
http-client I would like to move this in the direction of simplifying the
interface to message-based systems without creating additional message
handles when they are not solicited (i.e. submit() would create a Future
task but execute() would not).


-karl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090525/31beba50/attachment.html>

From hans.boehm at hp.com  Mon May 25 20:32:49 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 26 May 2009 00:32:49 +0000
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <fdeb32eb0905250827v7df2298blb9945e686a064c84@mail.gmail.com>
References: <fdeb32eb0905250243l4b65c648ka773aa53de89cecb@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENGIBAA.davidcholmes@aapt.net.au>
	<fdeb32eb0905250513s66c9b987v7dd92695cde2d85d@mail.gmail.com>
	<fdeb32eb0905250827v7df2298blb9945e686a064c84@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0424DF8D67497@GVW0436EXB.americas.hpqcorp.net>

> From: P?ter Kov?cs
> 
> ... Apart 
> from the awkward name for this JNI constant, it appears to be 
> a pure operator error not to use JNI_ABORT for an immutable array.
> 
Agreed, at least if there may be concurrent readers.  Anything else results in a data race, which is by definition a bug on the C/C++ side, and bad practice on the Java side.

If the array were updated, I don't see how this could be correct in the presence of data races, since the update may have happened in place, and the C code makes no guarantees in the presence of data races.

Hans

From joe.bowbeer at gmail.com  Mon May 25 21:14:35 2009
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 25 May 2009 18:14:35 -0700
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
Message-ID: <31f2a7bd0905251814j125e8355oedd90c44d2654f97@mail.gmail.com>

I don't understand the comment about FutureTask.Sync protecting innerSet and
innerSetException, because FutureTask provides public "set" and
"setException" methods for this purpose.

I'm not authorized to access MessageFuture at the svn link below, but I can
access it at:

http://code.google.com/p/karlthepagan/source/browse/

Joe

On Mon, May 25, 2009 at 5:08 PM, karlthepagan wrote:

> As a followup to last week's great discussion of FutureTask.done()
> callbacks I'd like to ask if anyone else has rolled their own "remote
> Future" implementation or something which interfaces with a messaging
> library? I found one attribute of FutureTask.Sync is that it protects
> innerSet and innerSetException from being set outside of innerRun so as far
> as I imagined it cannot be extend to provide a receiver for messages.
>
> I find the Future API so comfortable that I wanted to apply it to other
> IO-bound and remote tasks rather than simply CPU-bound tasks, but consuming
> a thread per request isn't usually desirable. For example the Jersey HTTP
> client API makes use of futures to solve an IO-bound task:
>
> http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html
>
>
> With that in mind here is an API inspired by a few iterations of this
> problem:
>
> public interface MessageFuture<V> extends Future<V> {
>     boolean offer(V data);
>     boolean offerException(Throwable t);
> }
>
>
> I have a possibly naive implementation up on GoogleCode, but feedback is
> welcome:
> https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/
>
> After providing something that could meet the needs of Jersey and/or
> Grizzly http-client I would like to move this in the direction of
> simplifying the interface to message-based systems without creating
> additional message handles when they are not solicited (i.e. submit() would
> create a Future task but execute() would not).
>
>
> -karl
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090525/13f15425/attachment.html>

From karlthepagan at gmail.com  Mon May 25 21:33:32 2009
From: karlthepagan at gmail.com (karlthepagan)
Date: Mon, 25 May 2009 18:33:32 -0700
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <31f2a7bd0905251814j125e8355oedd90c44d2654f97@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
	<31f2a7bd0905251814j125e8355oedd90c44d2654f97@mail.gmail.com>
Message-ID: <39008ef30905251833g69da977dtaa8f5ed52617700d@mail.gmail.com>

Thanks Joe, seems my vision was a little myopic. I'll switch over to
FutureTask and continue the completion queue stuff with that. ;)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090525/02086165/attachment.html>

From davidcholmes at aapt.net.au  Mon May 25 22:09:01 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 26 May 2009 12:09:01 +1000
Subject: [concurrency-interest] Interesting story from Dave Dice
In-Reply-To: <fdeb32eb0905250827v7df2298blb9945e686a064c84@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMENKIBAA.davidcholmes@aapt.net.au>

Totally agree! I wasn't familiar with the commit/abort side of this. Even if you assumed the write-back was harmless you shouldn't be doing the write-back at all.

David

> -----Original Message-----
> From: peter.dunay.kovacs at gmail.com
> [mailto:peter.dunay.kovacs at gmail.com]On Behalf Of P?ter Kov?cs
> Sent: Tuesday, 26 May 2009 1:28 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Interesting story from Dave Dice
> 
> 
> 
> I have to retract my previous mail. I see that the write-back feature
> is even mentioned in the Tech Tips article. Apart from the awkward
> name for this JNI constant, it appears to be a pure operator error not
> to use JNI_ABORT for an immutable array.
> 
> Peter
> 
> 2009/5/25 P?ter Kov?cs <peter.kovacs.1.0rc at gmail.com>:
> > It is certainly rather subjective who finds what surprising. :-) To
> > me, the surprising part is that you cannot read without subsequently
> > writing back.
> >
> > Thanks
> > Peter
> >
> > On Mon, May 25, 2009 at 12:46 PM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> >> Peter Kovacs writes:
> >>> 1. If an operation/class/method/whatever is not declared to be
> >>> thread-safe, you can assume it is not. Correct? (If it is declared to
> >>> be so, you can, perhaps, be optimistic and assume it is.)
> >>>
> >>> 2. If a variable (an array in this case) is written to, the variable
> >>> is not "effectively immutable". (Probably useless of nitpicking on the
> >>> meaning of "effectively".)
> >>>
> >>> Is this really novel?
> >>>
> >> I think this certainly violates the principle of least 
> surprise. If you just
> >> want to _read_ an array from native code, then there are no 
> specific methods
> >> for that, you have to use these JNI methods that assume you 
> want to write to
> >> the array as well. Even realizing that these are potentially mutating
> >> methods, I still find it surprising that writing back the same 
> values that
> >> you read is not guaranteed to be seen correctly. Looking at 
> what actually
> >> happens I can go "Oh I see, yes that makes sense" - but 
> there's no way I
> >> would have thought of that happening before-hand.
> >>
> >> David Holmes
> >>
> >>
> >>
> >
> 




From manik at jboss.org  Tue May 26 06:47:27 2009
From: manik at jboss.org (Manik Surtani)
Date: Tue, 26 May 2009 11:47:27 +0100
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
Message-ID: <4E5E8050-7956-4EBF-BBD2-C845D73D9F5C@jboss.org>

Bump.  Any thoughts/comments?  Especially around whether there is  
interest around a ConcurrentMap implementation similar to this?

Cheers
Manik

On 18 May 2009, at 12:14, Manik Surtani wrote:

> I've implemented a concurrent, linked map-like structure [1] [2]  
> based on CHM-like array of segments for hash lookups and  Sundell  
> and Tsigas' "Lock-free deques and doubly linked lists" [3] for  
> maintaining the linked list.  The impl offers constant-time  
> operation for put(), get(), remove() and iteration.
>
> I haven't implemented ConcurrentMap as my needs haven't dictated so,  
> but I can't see why this should not be possible.
>
> Is there interest in a ConcurrentMap implementation based on what I  
> have done for JSR-166?
>
> Cheers
> Manik
>
> [1] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236
> [2] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219
> [3] http://www.md.chalmers.se/~tsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf
>
>
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest






From david.lloyd at redhat.com  Tue May 26 11:36:32 2009
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 26 May 2009 10:36:32 -0500
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
Message-ID: <4A1C0C80.70509@redhat.com>

On 05/25/2009 07:08 PM, karlthepagan wrote:
> As a followup to last week's great discussion of FutureTask.done() 
> callbacks I'd like to ask if anyone else has rolled their own "remote 
> Future" implementation or something which interfaces with a messaging 
> library?

While I really like the Future API as well, I've found it to be inadequate 
for I/O purposes and have "rolled my own" for I/O use.

http://docs.jboss.org/xnio/latest/api/index.html?org/jboss/xnio/IoFuture.html

The main distinctions are that cancel() cannot specify interruption, the 
operation may result in an IOException, there is much more flexibility in 
terms of how you wait for the result, and you may register one or more 
asynchronous notifiers.

The abstract implementation provides some useful methods:

http://docs.jboss.org/xnio/latest/api/index.html?org/jboss/xnio/AbstractIoFuture.html

There are three protected methods which may be used to either acknowledge 
completion, failure, or cancellation asynchronously; in addition, they 
return a boolean which can be used to indicate whether that method 
invocation was the first result (subsequent invocations are ignored and 
return false), which in many cases can eliminate extra checks in the user 
implementation code.

Hope this is helpful.

- DML


From karlthepagan at gmail.com  Tue May 26 14:34:27 2009
From: karlthepagan at gmail.com (karlthepagan)
Date: Tue, 26 May 2009 11:34:27 -0700
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <4A1C0C80.70509@redhat.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
	<4A1C0C80.70509@redhat.com>
Message-ID: <39008ef30905261134j3385ff00h73d2229ed1e18764@mail.gmail.com>

On Tue, May 26, 2009 at 8:36 AM, David M. Lloyd <david.lloyd at redhat.com>wrote:

> The main distinctions are that cancel() cannot specify interruption, the
> operation may result in an IOException, there is much more flexibility in
> terms of how you wait for the result, and you may register one or more
> asynchronous notifiers.
>

Thanks David. I'll add your links to the list of alternatives I'm building
and see what else I've left out. I ended up doing almost exactly the same
thing with protected completion methods.

The converting future is also something I sketched out but haven't
reimplemented here. Cpu-tasks can simply nest callables in order to convert
results, but io-tasks have to filter after the results are available.

I agree with your conclusions on cancellation. I would like to allow IO
cancelation to keep the library generalized, but I'm allowing implementers
to override the default behavior and disallow interrupting with a "boolean
mayCancel(boolean)" method.

One thing I haven't decided yet is how to propagate InterruptedIOException
in the API. In the case that IO is canceled the number of bytes written
before interruption may be significant. That byte count isn't communicated
by cancel/isCanceled so I've considered either subclassing
CancellationException to hold this info or wrap it using ExecutionException.

-karl
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090526/4cb22fac/attachment.html>

From david.lloyd at redhat.com  Tue May 26 15:25:56 2009
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 26 May 2009 14:25:56 -0500
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <39008ef30905261134j3385ff00h73d2229ed1e18764@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>	<4A1C0C80.70509@redhat.com>
	<39008ef30905261134j3385ff00h73d2229ed1e18764@mail.gmail.com>
Message-ID: <4A1C4244.6080806@redhat.com>

On 05/26/2009 01:34 PM, karlthepagan wrote:
> One thing I haven't decided yet is how to propagate 
> InterruptedIOException in the API. In the case that IO is canceled the 
> number of bytes written before interruption may be significant. That 
> byte count isn't communicated by cancel/isCanceled so I've considered 
> either subclassing CancellationException to hold this info or wrap it 
> using ExecutionException.

I've found it to not be an issue so far.  With low-level I/O, NIO-class 
operations are generally atomic and non-blocking.  So in the case of 
writes, either data was successfully written, in which case the result is a 
byte count, or the thread was interrupted before the write took place so 
nothing is written at all.  Likewise for reads, connects, accepts, etc.  If 
the writer thread is interrupted after or even during the write, the caller 
would never need to know about it anyway.

With high-level I/O (like remote invocations for example), interruption 
doesn't enter into it at all.  If the local (waiting) thread is 
interrupted, it is your option whether to fire off a cancellation on the 
IoFuture or just let it finish.  If the remote invocation is interrupted, 
the framework would relay the result as a general cancellation (i.e. 
operation failed, request was received but operation wasn't yet started, 
request was not fully received, etc).  And cancellation may itself 
interrupt the remote invocation (why not?) as the mechanism by which 
cancellation is effected.  In any case, the mechanics of the cancellation 
is an implementation detail and it won't make a difference to the caller 
whether there was an actual interrupt or not.  The only possible surprise 
is that a cancellation might happen without the initial caller actually 
calling "cancel()" - for example if the remote side is being shut down when 
an invocation is made.

I believe there is some history with respect to InterruptedIOException and 
the design of interruption with blocking I/O - I'm sure I remember Alan 
Bateman discussing it on the NIO.2 list not long ago.  In any case I guess 
we're rapidly diverging from the parameters of concurrency-interest. :-)

- DML

From dl at cs.oswego.edu  Tue May 26 19:45:35 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 26 May 2009 19:45:35 -0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
Message-ID: <4A1C7F1F.6090402@cs.oswego.edu>

Manik Surtani wrote:
> I've implemented a concurrent, linked map-like structure [1] [2] 
>

Sorry for the delay! (I've mostly been playing volunteer sysadmin
for a bunch of reconfigurations here.)

We've been considering things like this for the several-times-delayed
but still upcoming CustomConcurrentHashMap (CCHM) allowing weak/soft
entries, identity comparisons and (too :-) much more.

I gather that the main motivation for your approach is to support
LRU replacement. Considering that many concurrent maps are used
as caches, this seems to be a common desire. But we've been resisting
supporting pure LRU because list-based approaches
force a sequential bottlenecks at the ends of the lists. So even
a lock-free approach to maintaining an access-ordered list may
encounter enough contention to negate the advantages of cheap
get()'s and usually-uncontended modifications. Additionally,
if you trigger LRU-based removal when hitting a size threshold,
then you end up calling size() a lot, which is also not cheap.
All in all, it is not clear when or by how much a concurrent hash
map approach will have better performance for strict LRU caching
over a simple synchronized LinkedHashMap.

There are a few ways to get LRU-like replacement that
don't impose bottlenecks. Hardware L1 etc caches, facing
the same issues, tend to use various "pseudo-LRU" (google it)
techniques that don't have very good software analogs.
Among the leading candidates is a random sampling approach
(similar to one in the thesis by Konstantinos Psounisy
http://www-rcf.usc.edu/~kpsounis/thesis.html) triggered
by local per-segment threshold overflows. Hopefully
I'll get something using it ready for preliminary release soon.

While I'm at it... There are several alternative concurrent
hash maps in various stages of development, including...

* Moran Tzafrir's Hopscotch hashing:
http://groups.google.com/group/hopscotch-hashing

* Cliff Click's state-based table:
http://sourceforge.net/projects/high-scale-lib/

* Amino project cbbs dictionary based on Shalev & Shavit
http://amino-cbbs.wiki.sourceforge.net/

The diversity reflects the fact that there are a lot of
tradeoffs in hash table design. Some of these are probably
better than j.u.c.ConcurrentHashMap in some contexts, but
it is still not clear if any of them are or can become
better in enough contexts to replace the current algorithms.

-Doug

> 
> [1] 
> http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236 
> 
> [2] 
> http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219 
> 
> [3] 
> http://www.md.chalmers.se/~tsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf 

From ben_manes at yahoo.com  Tue May 26 20:59:05 2009
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 26 May 2009 17:59:05 -0700 (PDT)
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <4A1C7F1F.6090402@cs.oswego.edu>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
Message-ID: <308293.10251.qm@web38804.mail.mud.yahoo.com>

Similarly, I have a CLHM implementation.  However I didn't base it off of an existing algorithm as this would not be very educational and my goal was purely for fun and to learn something.  Instead I've slowly been maturing it as I get a better grasp of how to write complex, concurrent algorithms.  I expect to make it lock-free in the next iteration where I'll be trying to understand how to get concurrent operations to work together to an end state rather than relying on mutual exclusion to block operations that could corrupt the shared state.  So my approach is much more oriented towards personal growth than pragmatically implementing an algorithm I don't quite grok.

> pure LRU because list-based approaches force a sequential bottlenecks at the ends of the lists.

For this exact reason I've adopted is the Second Chance algorithm.  The efficiency is similar, but worse, than a pure LRU yet provides the concurrency characteristics of a FIFO.  This allows reads to be as cheap as a CHM, which is the 80% usage of a cache.

> Additionally, if you trigger LRU-based removal when hitting a size threshold, then you end up calling size() a lot, which is also not cheap.

This is why I maintain a the length of the list as a separate variable (AtomicInteger).  The clients are made aware that the size of the cache may temporarily fluctuate above the "maximum capacity" and the size is slightly more variable.  Due to this approach the length variable could, of course, temporarily be negative.  In that case I simply return zero.  Its an acceptable trade-off for the use-case of a cache.

> The diversity reflects the fact that there are a lot of tradeoffs in hash table design.

Whether to use a hash table is, by itself, a trade-off in usage.  For example, on the Ehcache forum a user reported a performance problem with a cache of 60M entries.  With a 32-bit hashcode, there is a 99% probability of a collision at 200k entries.  With CHM's maximum of 2^16 segments, the average length of the link chain is 915 entries.  The performance problem was most likely due to traversing this list on every operation.  While I would argue that 60M entries is an abusive use of the data structure, it shows that CHM is not always the appropriate data store.  If instead the data was stored in a ConcurrentSkipListMap then the height (# of comparisons) is a mere 26.  There are other ways to solve it, but it shows nicely how you must chose your data structures wisely.

On a side-note, I would argue two points in design.  The first is that for a JDK version this should be based off of Doug's ConcurrentLinkedDeque.  The second is that from an application's architecture the need for a CLHM indicates a fundamental flaw in its design.  A JVM-level cache under such heavy load to require such a map is a serious flaw in the usage pattern.  Instead the topology of caches and execution flows should be redone.  For instance, thread-local caching should be leveraged across an execution flow when applicable.  The cost of a local miss should be fairly cheap.  The requirement that the CLHM provides a noticeable performance gain should be treated as giving a buffer to solve the underlying architectural problems.  While it is always nice to get a performance gain, the change should be more theoretical than practical.  Hence, my major motivation for implementing a CLHM was for fun as from a real-world purpose its need should be quite
 rare.

Cheers,
Ben





________________________________
From: Doug Lea <dl at cs.oswego.edu>
To: Manik Surtani <manik at jboss.org>
Cc: concurrency-interest at cs.oswego.edu
Sent: Tuesday, May 26, 2009 4:45:35 PM
Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

Manik Surtani wrote:
> I've implemented a concurrent, linked map-like structure [1] [2] 

Sorry for the delay! (I've mostly been playing volunteer sysadmin
for a bunch of reconfigurations here.)

We've been considering things like this for the several-times-delayed
but still upcoming CustomConcurrentHashMap (CCHM) allowing weak/soft
entries, identity comparisons and (too :-) much more.

I gather that the main motivation for your approach is to support
LRU replacement. Considering that many concurrent maps are used
as caches, this seems to be a common desire. But we've been resisting
supporting pure LRU because list-based approaches
force a sequential bottlenecks at the ends of the lists. So even
a lock-free approach to maintaining an access-ordered list may
encounter enough contention to negate the advantages of cheap
get()'s and usually-uncontended modifications. Additionally,
if you trigger LRU-based removal when hitting a size threshold,
then you end up calling size() a lot, which is also not cheap.
All in all, it is not clear when or by how much a concurrent hash
map approach will have better performance for strict LRU caching
over a simple synchronized LinkedHashMap.

There are a few ways to get LRU-like replacement that
don't impose bottlenecks. Hardware L1 etc caches, facing
the same issues, tend to use various "pseudo-LRU" (google it)
techniques that don't have very good software analogs.
Among the leading candidates is a random sampling approach
(similar to one in the thesis by Konstantinos Psounisy
http://www-rcf.usc.edu/~kpsounis/thesis.html) triggered
by local per-segment threshold overflows. Hopefully
I'll get something using it ready for preliminary release soon.

While I'm at it... There are several alternative concurrent
hash maps in various stages of development, including...

* Moran Tzafrir's Hopscotch hashing:
http://groups.google.com/group/hopscotch-hashing

* Cliff Click's state-based table:
http://sourceforge.net/projects/high-scale-lib/

* Amino project cbbs dictionary based on Shalev & Shavit
http://amino-cbbs.wiki.sourceforge.net/

The diversity reflects the fact that there are a lot of
tradeoffs in hash table design. Some of these are probably
better than j.u.c.ConcurrentHashMap in some contexts, but
it is still not clear if any of them are or can become
better in enough contexts to replace the current algorithms.

-Doug

> 
> [1] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236 
> [2] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219 
> [3] http://www.md.chalmers.se/~tsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090526/8a802c5b/attachment-0001.html>

From elizarov at devexperts.com  Wed May 27 01:59:50 2009
From: elizarov at devexperts.com (Roman Elizarov)
Date: Wed, 27 May 2009 09:59:50 +0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <4A1C7F1F.6090402@cs.oswego.edu>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
Message-ID: <1094521558.20090527095950@devexperts.com>

Hello Doug!

DL> There are a few ways to get LRU-like replacement that
DL> don't impose bottlenecks. Hardware L1 etc caches, facing
DL> the same issues, tend to use various "pseudo-LRU" (google it)
DL> techniques that don't have very good software analogs.
DL> Among the leading candidates is a random sampling approach
DL> (similar to one in the thesis by Konstantinos Psounisy
DL> http://www-rcf.usc.edu/~kpsounis/thesis.html) triggered
DL> by local per-segment threshold overflows. Hopefully
DL> I'll get something using it ready for preliminary release soon.

We usually employ a very simple scheme for heavily read caches. We put
"last access timestamps" in each cached object. It does not have to
call currentTimeMillis to update this timestamp, it can take current
timestamp from a variable that is periodically updated by a different
thread. When the size of the cache grows beyond a certain threshold,
we scan our cache and remove half of its elements that were "least
recently accessed". This operation combines nicely with rehash. Our
caches are typically mostly read, so we employ ConcurrentHashMap-like
data structures (albeit without any lock striping due to low update
frequency) that are wait-free for read and require synchronization for
write.

Sincerely,
Roman Elizarov


From manik at jboss.org  Wed May 27 06:46:46 2009
From: manik at jboss.org (Manik Surtani)
Date: Wed, 27 May 2009 11:46:46 +0100
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <4A1C7F1F.6090402@cs.oswego.edu>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
Message-ID: <44423AED-BC5D-4066-A0E3-2B25ABFCC5FC@jboss.org>

Hi Doug

On 27 May 2009, at 00:45, Doug Lea wrote:

> I gather that the main motivation for your approach is to support
> LRU replacement. Considering that many concurrent maps are used
> as caches, this seems to be a common desire. But we've been resisting
> supporting pure LRU because list-based approaches
> force a sequential bottlenecks at the ends of the lists. So even
> a lock-free approach to maintaining an access-ordered list may
> encounter enough contention to negate the advantages of cheap
> get()'s and usually-uncontended modifications. Additionally,
> if you trigger LRU-based removal when hitting a size threshold,
> then you end up calling size() a lot, which is also not cheap.
> All in all, it is not clear when or by how much a concurrent hash
> map approach will have better performance for strict LRU caching
> over a simple synchronized LinkedHashMap.

The main reason is to support eviction based on either FIFO or LRU.

I also weaken the guarantees of size() such that an approximate sizing  
is considered "good enough" (add up segment counts without locking  
segments), but I understand how this may not fit a more general  
purpose use of such a Map.

> There are a few ways to get LRU-like replacement that
> don't impose bottlenecks. Hardware L1 etc caches, facing
> the same issues, tend to use various "pseudo-LRU" (google it)
> techniques that don't have very good software analogs.
> Among the leading candidates is a random sampling approach
> (similar to one in the thesis by Konstantinos Psounisy
> http://www-rcf.usc.edu/~kpsounis/thesis.html) triggered
> by local per-segment threshold overflows. Hopefully
> I'll get something using it ready for preliminary release soon.

Random sampling sounds interesting - something I will look into.   
Efficiency is more important than precision in this case.

Cheers
Manik





From manik at jboss.org  Wed May 27 06:59:23 2009
From: manik at jboss.org (Manik Surtani)
Date: Wed, 27 May 2009 11:59:23 +0100
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <308293.10251.qm@web38804.mail.mud.yahoo.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
	<308293.10251.qm@web38804.mail.mud.yahoo.com>
Message-ID: <E75C277E-D6B0-46F1-A065-A7D6EFD9C6E2@jboss.org>


On 27 May 2009, at 01:59, Ben Manes wrote:

> Similarly, I have a CLHM implementation.  However I didn't base it  
> off of an existing algorithm as this would not be very educational  
> and my goal was purely for fun and to learn something.  Instead I've  
> slowly been maturing it as I get a better grasp of how to write  
> complex, concurrent algorithms.  I expect to make it lock-free in  
> the next iteration where I'll be trying to understand how to get  
> concurrent operations to work together to an end state rather than  
> relying on mutual exclusion to block operations that could corrupt  
> the shared state.  So my approach is much more oriented towards  
> personal growth than pragmatically implementing an algorithm I don't  
> quite grok.
>
> > pure LRU because list-based approaches force a sequential  
> bottlenecks at the ends of the lists.
>
> For this exact reason I've adopted is the Second Chance algorithm.   
> The efficiency is similar, but worse, than a pure LRU yet provides  
> the concurrency characteristics of a FIFO.  This allows reads to be  
> as cheap as a CHM, which is the 80% usage of a cache.
>
> > Additionally, if you trigger LRU-based removal when hitting a size  
> threshold, then you end up calling size() a lot, which is also not  
> cheap.
>
> This is why I maintain a the length of the list as a separate  
> variable (AtomicInteger).  The clients are made aware that the size  
> of the cache may temporarily fluctuate above the "maximum capacity"  
> and the size is slightly more variable.  Due to this approach the  
> length variable could, of course, temporarily be negative.  In that  
> case I simply return zero.  Its an acceptable trade-off for the use- 
> case of a cache.

This is useful if there isn't a stringent constraint on the accuracy  
of size().  Something to consider, good idea.

> > The diversity reflects the fact that there are a lot of tradeoffs  
> in hash table design.
>
> Whether to use a hash table is, by itself, a trade-off in usage.   
> For example, on the Ehcache forum a user reported a performance  
> problem with a cache of 60M entries.  With a 32-bit hashcode, there  
> is a 99% probability of a collision at 200k entries.  With CHM's  
> maximum of 2^16 segments, the average length of the link chain is  
> 915 entries.  The performance problem was most likely due to  
> traversing this list on every operation.  While I would argue that  
> 60M entries is an abusive use of the data structure, it shows that  
> CHM is not always the appropriate data store.  If instead the data  
> was stored in a ConcurrentSkipListMap then the height (# of  
> comparisons) is a mere 26.  There are other ways to solve it, but it  
> shows nicely how you must chose your data structures wisely.
>
> On a side-note, I would argue two points in design.  The first is  
> that for a JDK version this should be based off of Doug's  
> ConcurrentLinkedDeque.

I need to support efficient remove() as well.  And traversing the  
linked entry list to locate the node to be removed is O(n).  In my  
impl I locate the entry to be removed using the hash table first, and  
then unlink.

>   The second is that from an application's architecture the need for  
> a CLHM indicates a fundamental flaw in its design.  A JVM-level  
> cache under such heavy load to require such a map is a serious flaw  
> in the usage pattern.

Why?  Maintaining a bounded map is not uncommon, and deciding which  
entries to evict from such a bounded map usually requires some sort of  
order being maintained.

> Instead the topology of caches and execution flows should be  
> redone.  For instance, thread-local caching should be leveraged  
> across an execution flow when applicable.  The cost of a local miss  
> should be fairly cheap.

That makes many application level assumptions.  Thread-local caching  
may not be possible or useful - threads may only need a small subset  
of data, but several threads may access this same data and fetching/ 
generating repeatedly may be unnecessary/wasteful.  And a local miss  
is not necessarily cheap if it involves an expensive remote lookup or  
database hit.

> The requirement that the CLHM provides a noticeable performance gain  
> should be treated as giving a buffer to solve the underlying  
> architectural problems.  While it is always nice to get a  
> performance gain, the change should be more theoretical than  
> practical.  Hence, my major motivation for implementing a CLHM was  
> for fun as from a real-world purpose its need should be quite rare.

Essentially, anywhere a cache is needed, some sort or bounded map is  
useful.  (Perhaps you are arguing that caches aren't useful?  :)  And  
for a bounded map to make sense, some form of eviction ordering or  
preference of entries should be maintained.  As I mentioned in my  
response to Doug, this order doesn't need to be precise, but at least  
some form of favour of removing older entries over newer ones should  
be there.

>
>
> Cheers,
> Ben
>
>
> From: Doug Lea <dl at cs.oswego.edu>
> To: Manik Surtani <manik at jboss.org>
> Cc: concurrency-interest at cs.oswego.edu
> Sent: Tuesday, May 26, 2009 4:45:35 PM
> Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl
>
> Manik Surtani wrote:
> > I've implemented a concurrent, linked map-like structure [1] [2]
>
> Sorry for the delay! (I've mostly been playing volunteer sysadmin
> for a bunch of reconfigurations here.)
>
> We've been considering things like this for the several-times-delayed
> but still upcoming CustomConcurrentHashMap (CCHM) allowing weak/soft
> entries, identity comparisons and (too :-) much more.
>
> I gather that the main motivation for your approach is to support
> LRU replacement. Considering that many concurrent maps are used
> as caches, this seems to be a common desire. But we've been resisting
> supporting pure LRU because list-based approaches
> force a sequential bottlenecks at the ends of the lists. So even
> a lock-free approach to maintaining an access-ordered list may
> encounter enough contention to negate the advantages of cheap
> get()'s and usually-uncontended modifications. Additionally,
> if you trigger LRU-based removal when hitting a size threshold,
> then you end up calling size() a lot, which is also not cheap.
> All in all, it is not clear when or by how much a concurrent hash
> map approach will have better performance for strict LRU caching
> over a simple synchronized LinkedHashMap.
>
> There are a few ways to get LRU-like replacement that
> don't impose bottlenecks. Hardware L1 etc caches, facing
> the same issues, tend to use various "pseudo-LRU" (google it)
> techniques that don't have very good software analogs.
> Among the leading candidates is a random sampling approach
> (similar to one in the thesis by Konstantinos Psounisy
> http://www-rcf.usc.edu/~kpsounis/thesis.html) triggered
> by local per-segment threshold overflows. Hopefully
> I'll get something using it ready for preliminary release soon.
>
> While I'm at it... There are several alternative concurrent
> hash maps in various stages of development, including...
>
> * Moran Tzafrir's Hopscotch hashing:
> http://groups.google.com/group/hopscotch-hashing
>
> * Cliff Click's state-based table:
> http://sourceforge.net/projects/high-scale-lib/
>
> * Amino project cbbs dictionary based on Shalev & Shavit
> http://amino-cbbs.wiki.sourceforge.net/
>
> The diversity reflects the fact that there are a lot of
> tradeoffs in hash table design. Some of these are probably
> better than j.u.c.ConcurrentHashMap in some contexts, but
> it is still not clear if any of them are or can become
> better in enough contexts to replace the current algorithms.
>
> -Doug
>
> >
> > [1]http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236
> > [2]http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219
> > [3] http://www.md.chalmers.se/~tsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

--
Manik Surtani
manik at jboss.org
Lead, Infinispan
Lead, JBoss Cache
http://www.infinispan.org
http://www.jbosscache.org




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090527/0662e4db/attachment-0001.html>

From ben_manes at yahoo.com  Wed May 27 12:15:18 2009
From: ben_manes at yahoo.com (Ben Manes)
Date: Wed, 27 May 2009 09:15:18 -0700 (PDT)
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <E75C277E-D6B0-46F1-A065-A7D6EFD9C6E2@jboss.org>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
	<308293.10251.qm@web38804.mail.mud.yahoo.com>
	<E75C277E-D6B0-46F1-A065-A7D6EFD9C6E2@jboss.org>
Message-ID: <119495.83194.qm@web38806.mail.mud.yahoo.com>

> I need to support efficient remove() as well.  And
traversing the linked entry list to locate the node to be removed is
O(n).  In my impl I locate the entry to be removed using the hash table
first, and then unlink.

Since you have been aware of my CLHM for at least 3 months given your previous comments, I'd expect that.  Given that its a LHM, I'd expect that as well (that's kind of the point).  My point is that Doug has an excellent lock-free deque implementation that has undergone performance testing and was considered for JDK-6.  If the JDK was to have a CLHM, it only seems natural that it would be based off of his CHM and CLD.

> Why?  Maintaining a bounded map is not uncommon, and
deciding which entries to evict from such a bounded map usually
requires some sort of order being maintained

Yes, but an application having a critical need for a 2% in VM performance gain is uncommon.



> That makes many application level assumptions.
 Thread-local caching may not be possible or useful - threads may only
need a small subset of data, but several threads may access this same
data and fetching/generating
> repeatedly may be unnecessary/wasteful.
 And a local miss is not necessarily cheap if it involves an expensive
remote lookup or database hit.

The point is that you should use the right tool for the right job.  The wrong thing is to assume that all data should be shared and a faster algorithm solves fundamental design flaws.  If a local cache miss is not cheap then having a faster map doesn't make it much better.  Using a memoizer, ala Ehcache's SelfPopulatingCache (lock striping) or JCiP future-based approach, provides a per-entry lock and allows concurrent cache operations.  In real-world usage there is very little value squeezing the last 2% of in VM performance when the bottleneck is making a remote call (100x+ slower).  This is why a synchronized LHM is good enough in most applications as the miss penalty far outweighs the speedup of faster access on a cache hit.  The few cases where I've seen this not been the case has tended to be due to obvious design flaws and fairly easy to fix.

> Essentially, anywhere a cache is needed, some sort
or bounded map is useful.  (Perhaps you are arguing that caches aren't
useful?  :)  And for a bounded map to make sense, some form of eviction
ordering or preference of entries 
> should be maintained.  As I mentioned
in my response to Doug, this order doesn't need to be precise, but at
least some form of favour of removing older entries over newer ones
should be there.

Your comments give a probably wrong impression that you are starting to believe that a LHM-based cache is original, not already norm, that others haven't solved this problem prior to your attempt, and that a faster version solves a major bottleneck.  The argument has been that when you begin to see that a CLHM is required to solve a noticable performance problem then one needs to take a step back and understand that 99% of the time your doing something wrong.



________________________________
From: Manik Surtani <manik at jboss.org>
To: Ben Manes <ben_manes at yahoo.com>
Cc: Doug Lea <dl at cs.oswego.edu>; concurrency-interest at cs.oswego.edu
Sent: Wednesday, May 27, 2009 3:59:23 AM
Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl



On 27 May 2009, at 01:59, Ben Manes wrote:

Similarly, I have a CLHM implementation.  However I didn't base it off of an existing algorithm as this would not be very educational and my goal was purely for fun and to learn something.  Instead I've slowly been maturing it as I get a better grasp of how to write complex, concurrent algorithms.  I expect to make it lock-free in the next iteration where I'll be trying to understand how to get concurrent operations to work together to an end state rather than relying on mutual exclusion to block operations that could corrupt the shared state.  So my approach is much more oriented towards personal growth than pragmatically implementing an algorithm I don't quite grok.

> pure LRU because list-based approaches force a sequential bottlenecks at the ends of the lists.

For this exact reason I've adopted is the Second Chance algorithm.  The efficiency is similar, but worse, than a pure LRU yet provides the concurrency characteristics of a FIFO.  This allows reads to be as cheap as a CHM, which is the 80% usage of a cache.

> Additionally, if you trigger LRU-based removal when hitting a size threshold, then you end up calling size() a lot, which is also not cheap.

This is why I maintain a the length of the list as a separate variable (AtomicInteger).  The clients are made aware that the size of the cache may temporarily fluctuate above the "maximum capacity" and the size is slightly more variable.  Due to this approach the length variable could, of course, temporarily be negative.  In that case I simply return zero.  Its an acceptable trade-off for the use-case of a cache.

This is useful if there isn't a stringent constraint on the accuracy of size().  Something to consider, good idea.

> The diversity reflects the fact that there are a lot of tradeoffs in hash table design.

Whether to use a hash table is, by itself, a trade-off in usage.  For example, on the Ehcache forum a user reported a performance problem with a cache of 60M entries.  With a 32-bit hashcode, there is a 99% probability of a collision at 200k entries.  With CHM's maximum of 2^16 segments, the average length of the link chain is 915 entries.  The performance problem was most likely due to traversing this list on every operation.  While I would argue that 60M entries is an abusive use of the data structure, it shows that CHM is not always the appropriate data store.  If instead the data was stored in a ConcurrentSkipListMap then the height (# of comparisons) is a mere 26.  There are other ways to solve it, but it shows nicely how you must chose your data structures wisely.

On a side-note, I would argue two points in design.  The first is that for a JDK version this should be based off of Doug's ConcurrentLinkedDeque.

I need to support efficient remove() as well.  And traversing the linked entry list to locate the node to be removed is O(n).  In my impl I locate the entry to be removed using the hash table first, and then unlink.

  The second is that from an application's architecture the need for a CLHM indicates a fundamental flaw in its design.  A JVM-level cache under such heavy load to require such a map is a serious flaw in the usage pattern.
Why?  Maintaining a bounded map is not uncommon, and deciding which entries to evict from such a bounded map usually requires some sort of order being maintained. 


Instead the topology of caches and execution flows should be redone.  For instance, thread-local caching should be leveraged across an execution flow when applicable.  The cost of a local miss should be fairly cheap.

That makes many application level assumptions.  Thread-local caching may not be possible or useful - threads may only need a small subset of data, but several threads may access this same data and fetching/generating repeatedly may be unnecessary/wasteful.  And a local miss is not necessarily cheap if it involves an expensive remote lookup or database hit.

The requirement that the CLHM provides a noticeable performance gain should be treated as giving a buffer to solve the underlying architectural problems.  While it is always nice to get a performance gain, the change should be more theoretical than practical.  Hence, my major motivation for implementing a CLHM was for fun as from a real-world purpose its need should be quite rare.

Essentially, anywhere a cache is needed, some sort or bounded map is useful.  (Perhaps you are arguing that caches aren't useful?  :)  And for a bounded map to make sense, some form of eviction ordering or preference of entries should be maintained.  As I mentioned in my response to Doug, this order doesn't need to be precise, but at least some form of favour of removing older entries over newer ones should be there.



Cheers,
Ben





________________________________
From: Doug Lea <dl at cs.oswego.edu>
To: Manik Surtani <manik at jboss.org>
Cc: concurrency-interest at cs.oswego.edu
Sent: Tuesday, May 26, 2009 4:45:35 PM
Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

Manik Surtani wrote:
> I've implemented a concurrent, linked map-like structure [1] [2] 

Sorry for the delay! (I've mostly been playing volunteer sysadmin
for a bunch of reconfigurations here.)

We've been considering things like this for the several-times-delayed
but still upcoming CustomConcurrentHashMap (CCHM) allowing weak/soft
entries, identity comparisons and (too :-) much more.

I gather that the main motivation for your approach is to support
LRU replacement. Considering that many concurrent maps are used
as caches, this seems to be a common desire. But we've been resisting
supporting pure LRU because list-based approaches
force a sequential bottlenecks at the ends of the lists. So even
a lock-free approach to maintaining an access-ordered list may
encounter enough contention to negate the advantages of cheap
get()'s and usually-uncontended modifications. Additionally,
if you trigger LRU-based removal when hitting a size threshold,
then you end up calling size() a lot, which is also not cheap.
All in all, it is not clear when or by how much a concurrent hash
map approach will have better performance for strict LRU caching
over a simple synchronized LinkedHashMap.

There are a few ways to get LRU-like replacement that
don't impose bottlenecks. Hardware L1 etc caches, facing
the same issues, tend to use various "pseudo-LRU" (google it)
techniques that don't have very good software analogs.
Among the leading candidates is a random sampling approach
(similar to one in the thesis by Konstantinos Psounisy
http://www-rcf.usc.edu/%7Ekpsounis/thesis.html) triggered
by local per-segment threshold overflows. Hopefully
I'll get something using it ready for preliminary release soon.

While I'm at it... There are several alternative concurrent
hash maps in various stages of development, including...

* Moran Tzafrir's Hopscotch hashing:
http://groups.google.com/group/hopscotch-hashing

* Cliff Click's state-based table:
http://sourceforge.net/projects/high-scale-lib/

* Amino project cbbs dictionary based on Shalev & Shavit
http://amino-cbbs.wiki.sourceforge.net/

The diversity reflects the fact that there are a lot of
tradeoffs in hash table design. Some of these are probably
better than j.u.c.ConcurrentHashMap in some contexts, but
it is still not clear if any of them are or can become
better in enough contexts to replace the current algorithms.

-Doug

> 
> [1]http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/FIFODataContainer.java?r=236 
> [2]http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/container/LRUDataContainer.java?r=219 
> [3] http://www.md.chalmers.se/%7Etsigas/papers/Lock-Free-Deques-Doubly-Lists-JPDC.pdf 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




--
Manik Surtani
manik at jboss.org
Lead, Infinispan
Lead, JBoss Cache
http://www.infinispan.org
http://www.jbosscache.org


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090527/ccf459d7/attachment-0001.html>

From jed at atlassian.com  Wed May 27 21:24:50 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Thu, 28 May 2009 11:24:50 +1000
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
Message-ID: <7564C44B-3D4E-4694-886B-4F954344F569@atlassian.com>

I have a similar class that implements Future but does not take a  
Runnable/Callable like FutureTask. I realised it didn't support  
exceptions (I didn't need it at the time) so I added that this morning.

It is considerably simpler in design to your version. It simply uses  
an AtomicReference and a CountDownLatch to co-ordinate updates and  
waiting.

To do the onDone/onFail/onCancel stuff I'd probably use another class  
that decorates this one and pass handlers rather than using sub- 
classing.

http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2413

cheers,
jed.

On 26/05/2009, at 10:08 AM, karlthepagan wrote:

> As a followup to last week's great discussion of FutureTask.done()  
> callbacks I'd like to ask if anyone else has rolled their own  
> "remote Future" implementation or something which interfaces with a  
> messaging library? I found one attribute of FutureTask.Sync is that  
> it protects innerSet and innerSetException from being set outside of  
> innerRun so as far as I imagined it cannot be extend to provide a  
> receiver for messages.
>
> I find the Future API so comfortable that I wanted to apply it to  
> other IO-bound and remote tasks rather than simply CPU-bound tasks,  
> but consuming a thread per request isn't usually desirable. For  
> example the Jersey HTTP client API makes use of futures to solve an  
> IO-bound task:
> http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html
>
>
> With that in mind here is an API inspired by a few iterations of  
> this problem:
>
> public interface MessageFuture<V> extends Future<V> {
>     boolean offer(V data);
>     boolean offerException(Throwable t);
> }
>
>
> I have a possibly naive implementation up on GoogleCode, but  
> feedback is welcome:
> https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/
>
> After providing something that could meet the needs of Jersey and/or  
> Grizzly http-client I would like to move this in the direction of  
> simplifying the interface to message-based systems without creating  
> additional message handles when they are not solicited (i.e.  
> submit() would create a Future task but execute() would not).
>
>
> -karl
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jed at atlassian.com  Wed May 27 22:30:59 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Thu, 28 May 2009 12:30:59 +1000
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <7564C44B-3D4E-4694-886B-4F954344F569@atlassian.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
	<7564C44B-3D4E-4694-886B-4F954344F569@atlassian.com>
Message-ID: <663FF5B5-A758-4E50-B494-7CB4EA960601@atlassian.com>

I realised if I am offering up that class up for public consumption I  
should probably support cancellation. I also managed to shave a  
reference off for the byte conscious.

http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2414

cheers,
jed.

On 28/05/2009, at 11:24 AM, Jed Wesley-Smith wrote:

> I have a similar class that implements Future but does not take a  
> Runnable/Callable like FutureTask. I realised it didn't support  
> exceptions (I didn't need it at the time) so I added that this  
> morning.
>
> It is considerably simpler in design to your version. It simply uses  
> an AtomicReference and a CountDownLatch to co-ordinate updates and  
> waiting.
>
> To do the onDone/onFail/onCancel stuff I'd probably use another  
> class that decorates this one and pass handlers rather than using  
> sub-classing.
>
> http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2413
>
> cheers,
> jed.
>
> On 26/05/2009, at 10:08 AM, karlthepagan wrote:
>
>> As a followup to last week's great discussion of FutureTask.done()  
>> callbacks I'd like to ask if anyone else has rolled their own  
>> "remote Future" implementation or something which interfaces with a  
>> messaging library? I found one attribute of FutureTask.Sync is that  
>> it protects innerSet and innerSetException from being set outside  
>> of innerRun so as far as I imagined it cannot be extend to provide  
>> a receiver for messages.
>>
>> I find the Future API so comfortable that I wanted to apply it to  
>> other IO-bound and remote tasks rather than simply CPU-bound tasks,  
>> but consuming a thread per request isn't usually desirable. For  
>> example the Jersey HTTP client API makes use of futures to solve an  
>> IO-bound task:
>> http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html
>>
>>
>> With that in mind here is an API inspired by a few iterations of  
>> this problem:
>>
>> public interface MessageFuture<V> extends Future<V> {
>>    boolean offer(V data);
>>    boolean offerException(Throwable t);
>> }
>>
>>
>> I have a possibly naive implementation up on GoogleCode, but  
>> feedback is welcome:
>> https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/
>>
>> After providing something that could meet the needs of Jersey and/ 
>> or Grizzly http-client I would like to move this in the direction  
>> of simplifying the interface to message-based systems without  
>> creating additional message handles when they are not solicited  
>> (i.e. submit() would create a Future task but execute() would not).
>>
>>
>> -karl
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu May 28 09:03:30 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 28 May 2009 09:03:30 -0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <3FA4C6109D066541B52E1AF02D28A91406E1F2FA@sgtulmsp04.Global.ad.sabre.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org><4A1C7F1F.6090402@cs.oswego.edu>
	<1094521558.20090527095950@devexperts.com>
	<3FA4C6109D066541B52E1AF02D28A91406E1F2FA@sgtulmsp04.Global.ad.sabre.com>
Message-ID: <4A1E8BA2.4050508@cs.oswego.edu>

Bhasin, Vishal wrote:
> There is a high need for LRU in CHM - folks end up either creating their
> own or using a caching product (like JCS or EHcache) for this
> functionality.

First, we cannot hope to supply something that automates all
possible caching needs, especially those with persistent
(disk-based) components or integrated with J2EE services.

But we can supply some eviction policies in custom maps.
Basically, a cache is a map with an eviction policy.
An eviction policy in turn has two main parts:
   * (approximately) when to evict
   * (approximately) what to evict

For concurrent maps we only want to consider policies
compatible with our concurrency properties.
So strict LIFO-based LRU is out. However, this is not
a big loss. People usually pick LRU because it is
cheap (which it is in non-concurrent maps) and usually
performs OK. However, there are many other related
policies that usually perform at least as well, especially
for those that are ultimately tied to web-based requests
For a slightly dated survey see
http://portal.acm.org/citation.cfm?id=954341
(you might need ACM membership to access this)
A survey of Web cache replacement strategies
Stefan Podlipnig, Laszlo B?sz?rmenyi, ACM Computing Surveys 2003.
Most of these policies choose eviction victims based on
some combination of recency, frequency, and size.

We can provide generalized support for a number of
these by allowing users to tell us, for each
cached value:
   * The item's "utility" as an eviction preference score.
     for example for LRU-ish behavior, the score is access timestamp
   * The item's "weight" as its contribution to the decision of
     whether any item should be evicted. For example, for
     a bounded cache of equal-sized items, each weight is just 1.

Given these (definable in subclassable ValueRecords), we plan to
support methods that will evict low-utility items when a given
total-weight threshold is exceeded. The method can have only
probabilistic specs, both in terms when eviction triggers and
which items are removed.

Aside: It is really too bad that there is no way to
automate size calculations. Just bounding by total number
of items can lead to unbounded footprint growth. The only
recourse is to also declare the values as "Soft" references
to allow the GC to delete, which we will also support, even though
this can also seriously slow down GC.

-Doug





From Darron_Shaffer at stercomm.com  Thu May 28 10:43:31 2009
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Thu, 28 May 2009 10:43:31 -0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <4A1E8BA2.4050508@cs.oswego.edu>
Message-ID: <FC30D8A2D3DEE64D93E8DA54A1DB349A07355387@IWDUBCORMSG007.sci.local>

One eviction strategy that mirrors hardware caches is to use the concept of a "cache line".  Make each hash bucket a LRU list and evict if the bucket size grows over a threshold.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
Sent: Thursday, May 28, 2009 8:04 AM
To: Bhasin, Vishal
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

Bhasin, Vishal wrote:
> There is a high need for LRU in CHM - folks end up either creating their
> own or using a caching product (like JCS or EHcache) for this
> functionality.

First, we cannot hope to supply something that automates all
possible caching needs, especially those with persistent
(disk-based) components or integrated with J2EE services.

But we can supply some eviction policies in custom maps.
Basically, a cache is a map with an eviction policy.
An eviction policy in turn has two main parts:
   * (approximately) when to evict
   * (approximately) what to evict

For concurrent maps we only want to consider policies
compatible with our concurrency properties.
So strict LIFO-based LRU is out. However, this is not
a big loss. People usually pick LRU because it is
cheap (which it is in non-concurrent maps) and usually
performs OK. However, there are many other related
policies that usually perform at least as well, especially
for those that are ultimately tied to web-based requests
For a slightly dated survey see
http://portal.acm.org/citation.cfm?id=954341
(you might need ACM membership to access this)
A survey of Web cache replacement strategies
Stefan Podlipnig, Laszlo B?sz?rmenyi, ACM Computing Surveys 2003.
Most of these policies choose eviction victims based on
some combination of recency, frequency, and size.

We can provide generalized support for a number of
these by allowing users to tell us, for each
cached value:
   * The item's "utility" as an eviction preference score.
     for example for LRU-ish behavior, the score is access timestamp
   * The item's "weight" as its contribution to the decision of
     whether any item should be evicted. For example, for
     a bounded cache of equal-sized items, each weight is just 1.

Given these (definable in subclassable ValueRecords), we plan to
support methods that will evict low-utility items when a given
total-weight threshold is exceeded. The method can have only
probabilistic specs, both in terms when eviction triggers and
which items are removed.

Aside: It is really too bad that there is no way to
automate size calculations. Just bounding by total number
of items can lead to unbounded footprint growth. The only
recourse is to also declare the values as "Soft" references
to allow the GC to delete, which we will also support, even though
this can also seriously slow down GC.

-Doug




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From elizarov at devexperts.com  Thu May 28 11:42:57 2009
From: elizarov at devexperts.com (Roman Elizarov)
Date: Thu, 28 May 2009 19:42:57 +0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <FC30D8A2D3DEE64D93E8DA54A1DB349A07355387@IWDUBCORMSG007.sci.local>
References: <4A1E8BA2.4050508@cs.oswego.edu>
	<FC30D8A2D3DEE64D93E8DA54A1DB349A07355387@IWDUBCORMSG007.sci.local>
Message-ID: <1497161057.20090528194257@devexperts.com>

Hello Darron!

That means you have a somehow filled hash bucket in the first place,
which makes your hash slow on get operations. We try to keep our
hashes at a very low fill factor (average 50%), so that our gets
operations stumble on the right element at the first attempt most of
the time.

Sincerely,
Roman Elizarov

On Thursday, May 28, 2009 6:43:31 PM you wrote:

SD> One eviction strategy that mirrors hardware caches is to use the
SD> concept of a "cache line".  Make each hash bucket a LRU list and evict if the bucket
SD> size grows over a threshold.

SD> -----Original Message-----
SD> From: concurrency-interest-bounces at cs.oswego.edu
SD> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
SD> Sent: Thursday, May 28, 2009 8:04 AM
SD> To: Bhasin, Vishal
SD> Cc: concurrency-interest at cs.oswego.edu
SD> Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

SD> Bhasin, Vishal wrote:
>> There is a high need for LRU in CHM - folks end up either creating their
>> own or using a caching product (like JCS or EHcache) for this
>> functionality.

SD> First, we cannot hope to supply something that automates all
SD> possible caching needs, especially those with persistent
SD> (disk-based) components or integrated with J2EE services.

SD> But we can supply some eviction policies in custom maps.
SD> Basically, a cache is a map with an eviction policy.
SD> An eviction policy in turn has two main parts:
SD>    * (approximately) when to evict
SD>    * (approximately) what to evict

SD> For concurrent maps we only want to consider policies
SD> compatible with our concurrency properties.
SD> So strict LIFO-based LRU is out. However, this is not
SD> a big loss. People usually pick LRU because it is
SD> cheap (which it is in non-concurrent maps) and usually
SD> performs OK. However, there are many other related
SD> policies that usually perform at least as well, especially
SD> for those that are ultimately tied to web-based requests
SD> For a slightly dated survey see
SD> http://portal.acm.org/citation.cfm?id=954341
SD> (you might need ACM membership to access this)
SD> A survey of Web cache replacement strategies
SD> Stefan Podlipnig, Laszlo B?sz?rmenyi, ACM Computing Surveys 2003.
SD> Most of these policies choose eviction victims based on
SD> some combination of recency, frequency, and size.

SD> We can provide generalized support for a number of
SD> these by allowing users to tell us, for each
SD> cached value:
SD>    * The item's "utility" as an eviction preference score.
SD>      for example for LRU-ish behavior, the score is access timestamp
SD>    * The item's "weight" as its contribution to the decision of
SD>      whether any item should be evicted. For example, for
SD>      a bounded cache of equal-sized items, each weight is just 1.

SD> Given these (definable in subclassable ValueRecords), we plan to
SD> support methods that will evict low-utility items when a given
SD> total-weight threshold is exceeded. The method can have only
SD> probabilistic specs, both in terms when eviction triggers and
SD> which items are removed.

SD> Aside: It is really too bad that there is no way to
SD> automate size calculations. Just bounding by total number
SD> of items can lead to unbounded footprint growth. The only
SD> recourse is to also declare the values as "Soft" references
SD> to allow the GC to delete, which we will also support, even though
SD> this can also seriously slow down GC.

SD> -Doug




SD> _______________________________________________
SD> Concurrency-interest mailing list
SD> Concurrency-interest at cs.oswego.edu
SD> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

SD> _______________________________________________
SD> Concurrency-interest mailing list
SD> Concurrency-interest at cs.oswego.edu
SD> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From karlthepagan at gmail.com  Thu May 28 12:21:19 2009
From: karlthepagan at gmail.com (karlthepagan)
Date: Thu, 28 May 2009 09:21:19 -0700
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <663FF5B5-A758-4E50-B494-7CB4EA960601@atlassian.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>
	<7564C44B-3D4E-4694-886B-4F954344F569@atlassian.com>
	<663FF5B5-A758-4E50-B494-7CB4EA960601@atlassian.com>
Message-ID: <39008ef30905280921w1c53f981v7c9b0e20b392cd60@mail.gmail.com>

Thanks Jed, that is a much cleaner technique.
I've started looking at the memory consumption (empirically, 3M iterations
per) for the states initial / set / setException:FutureTask(Callable): 64 /
64 / 64
SettableFuture: 72 / 88 / 88
LockMessageFuture: 88 / 88 / 104
in a 64-bit vm:
FutureTask(Callable): 112 / 112 / 112
SettableFuture: 128 / 152 / 152
LockMessageFuture: 152 / 152 / 176


I think that neither SettableFuture's polymorphic Value holder nor
MessageFuture's marker objects would be as memory efficient as adding a
discrete synchronizer state for setting exceptions using the implementation
pattern of FutureTask. That could produce a 48 (88) byte object.

Going lower is possible, by either directly extending
AbstractQueuedSynchronizer (not recommended) or perhaps using intrinsic
locks.

-karl

On Wed, May 27, 2009 at 7:30 PM, Jed Wesley-Smith <jed at atlassian.com> wrote:

> I realised if I am offering up that class up for public consumption I
> should probably support cancellation. I also managed to shave a reference
> off for the byte conscious.
>
>
> http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2414
>
> cheers,
> jed.
>
>
> On 28/05/2009, at 11:24 AM, Jed Wesley-Smith wrote:
>
>  I have a similar class that implements Future but does not take a
>> Runnable/Callable like FutureTask. I realised it didn't support exceptions
>> (I didn't need it at the time) so I added that this morning.
>>
>> It is considerably simpler in design to your version. It simply uses an
>> AtomicReference and a CountDownLatch to co-ordinate updates and waiting.
>>
>> To do the onDone/onFail/onCancel stuff I'd probably use another class that
>> decorates this one and pass handlers rather than using sub-classing.
>>
>>
>> http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2413
>>
>> cheers,
>> jed.
>>
>> On 26/05/2009, at 10:08 AM, karlthepagan wrote:
>>
>>  As a followup to last week's great discussion of FutureTask.done()
>>> callbacks I'd like to ask if anyone else has rolled their own "remote
>>> Future" implementation or something which interfaces with a messaging
>>> library? I found one attribute of FutureTask.Sync is that it protects
>>> innerSet and innerSetException from being set outside of innerRun so as far
>>> as I imagined it cannot be extend to provide a receiver for messages.
>>>
>>> I find the Future API so comfortable that I wanted to apply it to other
>>> IO-bound and remote tasks rather than simply CPU-bound tasks, but consuming
>>> a thread per request isn't usually desirable. For example the Jersey HTTP
>>> client API makes use of futures to solve an IO-bound task:
>>>
>>> http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html
>>>
>>>
>>> With that in mind here is an API inspired by a few iterations of this
>>> problem:
>>>
>>> public interface MessageFuture<V> extends Future<V> {
>>>   boolean offer(V data);
>>>   boolean offerException(Throwable t);
>>> }
>>>
>>>
>>> I have a possibly naive implementation up on GoogleCode, but feedback is
>>> welcome:
>>> https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/
>>>
>>> After providing something that could meet the needs of Jersey and/or
>>> Grizzly http-client I would like to move this in the direction of
>>> simplifying the interface to message-based systems without creating
>>> additional message handles when they are not solicited (i.e. submit() would
>>> create a Future task but execute() would not).
>>>
>>>
>>> -karl
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090528/e7c86a4a/attachment.html>

From jim.andreou at gmail.com  Thu May 28 19:40:46 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Fri, 29 May 2009 02:40:46 +0300
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <308293.10251.qm@web38804.mail.mud.yahoo.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
	<308293.10251.qm@web38804.mail.mud.yahoo.com>
Message-ID: <7d7138c10905281640t1fa65efg936a8119b4826146@mail.gmail.com>

I'm not sure I'm reading this correctly, but...

2009/5/27 Ben Manes <ben_manes at yahoo.com>:
>
> Whether to use a hash table is, by itself, a trade-off in usage.? For
> example, on the Ehcache forum a user reported a performance problem with a
> cache of 60M entries.? With a 32-bit hashcode, there is a 99% probability of
> a collision at 200k entries.? With CHM's maximum of 2^16 segments, the
> average length of the link chain is 915 entries.

If you have 2^16 segments, then indeed each segment averages to 915
entries. But a segment itself is a hash table, not a single link
chain. (Unless all 915 entries would conflict! Kind of unlikely...).
In any case, I fail to see how the number of segments affect the
number of collisions.

Do I miss something here?

> The performance problem
> was most likely due to traversing this list on every operation.? While I
> would argue that 60M entries is an abusive use of the data structure, it
> shows that CHM is not always the appropriate data store.? If instead the
> data was stored in a ConcurrentSkipListMap then the height (# of
> comparisons) is a mere 26.? There are other ways to solve it, but it shows
> nicely how you must chose your data structures wisely.

Given the above, I think that a CSLM would actually be slower than
CHM, not faster.

Dimitris


From jed at atlassian.com  Thu May 28 20:16:57 2009
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Fri, 29 May 2009 10:16:57 +1000
Subject: [concurrency-interest] Offering data to Futures
In-Reply-To: <39008ef30905280921w1c53f981v7c9b0e20b392cd60@mail.gmail.com>
References: <39008ef30905251708u6feb16f1s6fa0b87b828ab56d@mail.gmail.com>	
	<7564C44B-3D4E-4694-886B-4F954344F569@atlassian.com>	
	<663FF5B5-A758-4E50-B494-7CB4EA960601@atlassian.com>
	<39008ef30905280921w1c53f981v7c9b0e20b392cd60@mail.gmail.com>
Message-ID: <4A1F2979.7050504@atlassian.com>

Thanks Karl.

Is the memory consumption so important in this case? I would have 
thought that generally these objects have a medium sized life and you 
would generally discard them once get() returns.

If it is, there would definitely be a couple of ways to put them on a 
diet at the expense of implementation complexity. Additionally, the 
producer in both the FutureTask and SettableFuture is lock-free, 
maintaining that would be difficult.

There are a number of techniques I can think of to shave off some bytes 
but they would all negatively affect readability and maintainability. 
And Doug has conniptions if you use Unsafe directly :-)

cheers,
jed.

karlthepagan wrote:
> Thanks Jed, that is a much cleaner technique.
>
> I've started looking at the memory consumption (empirically, 3M 
> iterations per) for the states initial / set / setException:
> FutureTask(Callable): 64 / 64 / 64
> SettableFuture: 72 / 88 / 88
> LockMessageFuture: 88 / 88 / 104
> in a 64-bit vm:
> FutureTask(Callable): 112 / 112 / 112
> SettableFuture: 128 / 152 / 152
> LockMessageFuture: 152 / 152 / 176
>
>
> I think that neither SettableFuture's polymorphic Value holder nor 
> MessageFuture's marker objects would be as memory efficient as adding 
> a discrete synchronizer state for setting exceptions using the 
> implementation pattern of FutureTask. That could produce a 48 (88) 
> byte object.
>
> Going lower is possible, by either directly extending 
> AbstractQueuedSynchronizer (not recommended) or perhaps using 
> intrinsic locks.
>
> -karl
>
> On Wed, May 27, 2009 at 7:30 PM, Jed Wesley-Smith <jed at atlassian.com 
> <mailto:jed at atlassian.com>> wrote:
>
>     I realised if I am offering up that class up for public
>     consumption I should probably support cancellation. I also managed
>     to shave a reference off for the byte conscious.
>
>     http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2414
>
>     cheers,
>     jed.
>
>
>     On 28/05/2009, at 11:24 AM, Jed Wesley-Smith wrote:
>
>         I have a similar class that implements Future but does not
>         take a Runnable/Callable like FutureTask. I realised it didn't
>         support exceptions (I didn't need it at the time) so I added
>         that this morning.
>
>         It is considerably simpler in design to your version. It
>         simply uses an AtomicReference and a CountDownLatch to
>         co-ordinate updates and waiting.
>
>         To do the onDone/onFail/onCancel stuff I'd probably use
>         another class that decorates this one and pass handlers rather
>         than using sub-classing.
>
>         http://labs.atlassian.com/source/browse/CONCURRENT/trunk/src/main/java/com/atlassian/util/concurrent/SettableFuture.java?r=2413
>
>         cheers,
>         jed.
>
>         On 26/05/2009, at 10:08 AM, karlthepagan wrote:
>
>             As a followup to last week's great discussion of
>             FutureTask.done() callbacks I'd like to ask if anyone else
>             has rolled their own "remote Future" implementation or
>             something which interfaces with a messaging library? I
>             found one attribute of FutureTask.Sync is that it protects
>             innerSet and innerSetException from being set outside of
>             innerRun so as far as I imagined it cannot be extend to
>             provide a receiver for messages.
>
>             I find the Future API so comfortable that I wanted to
>             apply it to other IO-bound and remote tasks rather than
>             simply CPU-bound tasks, but consuming a thread per request
>             isn't usually desirable. For example the Jersey HTTP
>             client API makes use of futures to solve an IO-bound task:
>             http://www.nabble.com/HTTP-Client-API--was%3A-Jersey-client-side-API--td22289451.html
>
>
>             With that in mind here is an API inspired by a few
>             iterations of this problem:
>
>             public interface MessageFuture<V> extends Future<V> {
>               boolean offer(V data);
>               boolean offerException(Throwable t);
>             }
>
>
>             I have a possibly naive implementation up on GoogleCode,
>             but feedback is welcome:
>             https://karlthepagan.googlecode.com/svn/trunk/MessageFuture/
>
>             After providing something that could meet the needs of
>             Jersey and/or Grizzly http-client I would like to move
>             this in the direction of simplifying the interface to
>             message-based systems without creating additional message
>             handles when they are not solicited (i.e. submit() would
>             create a Future task but execute() would not).
>
>
>             -karl
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>


From ben_manes at yahoo.com  Thu May 28 21:14:28 2009
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 28 May 2009 18:14:28 -0700 (PDT)
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <7d7138c10905281640t1fa65efg936a8119b4826146@mail.gmail.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
	<308293.10251.qm@web38804.mail.mud.yahoo.com>
	<7d7138c10905281640t1fa65efg936a8119b4826146@mail.gmail.com>
Message-ID: <351246.49590.qm@web38807.mail.mud.yahoo.com>

I may be wrong in my reading of how CHM works, so it could be a misunderstanding on my part.  If you look at the implementation it does walk a link chain.

static final class Segment<K,V> extends ReentrantLock implements Serializable {
    // ...
    V get(Object key, int hash) {
        if (count != 0) { // read-volatile
            HashEntry<K,V> e = getFirst(hash);
            while (e != null) {
                if (e.hash == hash && key.equals(e.key)) {
                    V v = e.value;
                    if (v != null)
                        return v;
                    return readValueUnderLock(e); // recheck
                }
                e = e.next;
            }
        }
        return null;
    }

In regards to collisions and the number of segments, a segment is chosen by:

    final Segment<K,V> segmentFor(int hash) {
        return (Segment<K,V>) segments[(hash >>> segmentShift) & segmentMask];
    }

The number of segments to chose from, the likelihood of a hash collision, and the number of elements to store should all effect the number of entries in a segment.  In the 60M entries example this could result is ~900 comparisons.

Of course, the experts are right here and can correct me if I'm wrong. :-)

Cheers!
Ben





________________________________
From: Jim Andreou <jim.andreou at gmail.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: Doug Lea <dl at cs.oswego.edu>; Manik Surtani <manik at jboss.org>; concurrency-interest at cs.oswego.edu
Sent: Thursday, May 28, 2009 4:40:46 PM
Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

I'm not sure I'm reading this correctly, but...

2009/5/27 Ben Manes <ben_manes at yahoo.com>:
>
> Whether to use a hash table is, by itself, a trade-off in usage.  For
> example, on the Ehcache forum a user reported a performance problem with a
> cache of 60M entries.  With a 32-bit hashcode, there is a 99% probability of
> a collision at 200k entries.  With CHM's maximum of 2^16 segments, the
> average length of the link chain is 915 entries.

If you have 2^16 segments, then indeed each segment averages to 915
entries. But a segment itself is a hash table, not a single link
chain. (Unless all 915 entries would conflict! Kind of unlikely...).
In any case, I fail to see how the number of segments affect the
number of collisions.

Do I miss something here?

> The performance problem
> was most likely due to traversing this list on every operation.  While I
> would argue that 60M entries is an abusive use of the data structure, it
> shows that CHM is not always the appropriate data store.  If instead the
> data was stored in a ConcurrentSkipListMap then the height (# of
> comparisons) is a mere 26.  There are other ways to solve it, but it shows
> nicely how you must chose your data structures wisely.

Given the above, I think that a CSLM would actually be slower than
CHM, not faster.

Dimitris



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090528/88e09cc5/attachment-0001.html>

From jim.andreou at gmail.com  Fri May 29 05:19:16 2009
From: jim.andreou at gmail.com (Jim Andreou)
Date: Fri, 29 May 2009 12:19:16 +0300
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <351246.49590.qm@web38807.mail.mud.yahoo.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>
	<4A1C7F1F.6090402@cs.oswego.edu>
	<308293.10251.qm@web38804.mail.mud.yahoo.com>
	<7d7138c10905281640t1fa65efg936a8119b4826146@mail.gmail.com>
	<351246.49590.qm@web38807.mail.mud.yahoo.com>
Message-ID: <7d7138c10905290219q2a53e308maf9f3db67fc10902@mail.gmail.com>

Yes, it's a link chain of the elements hashed _to the same bucket_.

The default load factor is 0.75. That's the average number of elements
that each bucket contains.

It would take a really, really bad hashCode() impl to get 915 elements
in a single bucket chain.

Btw, I see this comment in the implementation:
>(The average length [of the bucket chain] is less than two for the default load factor threshold.)

I'm not sure why it says "less than two" and not "less than one".
While the former is certainly true, it isn't the strictest possible,
unless what I say is inaccurate (load factor == avg. bucket chain
length) at least for this hash table implementation.

Dimitris

2009/5/29 Ben Manes <ben_manes at yahoo.com>:
> I may be wrong in my reading of how CHM works, so it could be a
> misunderstanding on my part.? If you look at the implementation it does walk
> a link chain.
>
> static final class Segment<K,V> extends ReentrantLock implements
> Serializable {
> ??? // ...
> ??? V get(Object key, int hash) {
> ??????? if (count != 0) { // read-volatile
> ??????????? HashEntry<K,V> e = getFirst(hash);
> ??????????? while (e != null) {
> ??????????????? if (e.hash == hash && key.equals(e.key)) {
> ??????????????????? V v = e.value;
> ??????????????????? if (v != null)
> ??????????????????????? return v;
> ??????????????????? return readValueUnderLock(e); // recheck
> ??????????????? }
> ??????????????? e = e.next;
> ??????????? }
> ??????? }
> ??????? return null;
> ??? }
>
> In regards to collisions and the number of segments, a segment is chosen by:
>
> ??? final Segment<K,V> segmentFor(int hash) {
> ??????? return (Segment<K,V>) segments[(hash >>> segmentShift) &
> segmentMask];
> ??? }
>
> The number of segments to chose from, the likelihood of a hash collision,
> and the number of elements to store should all effect the number of entries
> in a segment.? In the 60M entries example this could result is ~900
> comparisons.
>
> Of course, the experts are right here and can correct me if I'm wrong. :-)
>
> Cheers!
> Ben
>
>
> ________________________________
> From: Jim Andreou <jim.andreou at gmail.com>
> To: Ben Manes <ben_manes at yahoo.com>
> Cc: Doug Lea <dl at cs.oswego.edu>; Manik Surtani <manik at jboss.org>;
> concurrency-interest at cs.oswego.edu
> Sent: Thursday, May 28, 2009 4:40:46 PM
> Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl
>
> I'm not sure I'm reading this correctly, but...
>
> 2009/5/27 Ben Manes <ben_manes at yahoo.com>:
>>
>> Whether to use a hash table is, by itself, a trade-off in usage.? For
>> example, on the Ehcache forum a user reported a performance problem with a
>> cache of 60M entries.? With a 32-bit hashcode, there is a 99% probability
>> of
>> a collision at 200k entries.? With CHM's maximum of 2^16 segments, the
>> average length of the link chain is 915 entries.
>
> If you have 2^16 segments, then indeed each segment averages to 915
> entries. But a segment itself is a hash table, not a single link
> chain. (Unless all 915 entries would conflict! Kind of unlikely...).
> In any case, I fail to see how the number of segments affect the
> number of collisions.
>
> Do I miss something here?
>
>> The performance problem
>> was most likely due to traversing this list on every operation.? While I
>> would argue that 60M entries is an abusive use of the data structure, it
>> shows that CHM is not always the appropriate data store.? If instead the
>> data was stored in a ConcurrentSkipListMap then the height (# of
>> comparisons) is a mere 26.? There are other ways to solve it, but it shows
>> nicely how you must chose your data structures wisely.
>
> Given the above, I think that a CSLM would actually be slower than
> CHM, not faster.
>
> Dimitris
>
>


From dl at cs.oswego.edu  Fri May 29 07:08:15 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 29 May 2009 07:08:15 -0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <7d7138c10905290219q2a53e308maf9f3db67fc10902@mail.gmail.com>
References: <70AFED0B-1CCD-437E-9C75-33FD59C0C8D5@jboss.org>	
	<4A1C7F1F.6090402@cs.oswego.edu>	
	<308293.10251.qm@web38804.mail.mud.yahoo.com>	
	<7d7138c10905281640t1fa65efg936a8119b4826146@mail.gmail.com>	
	<351246.49590.qm@web38807.mail.mud.yahoo.com>
	<7d7138c10905290219q2a53e308maf9f3db67fc10902@mail.gmail.com>
Message-ID: <4A1FC21F.6000701@cs.oswego.edu>

Jim Andreou wrote:
> Yes, it's a link chain of the elements hashed _to the same bucket_.
> 
> The default load factor is 0.75. That's the average number of elements
> that each bucket contains.
> 
> It would take a really, really bad hashCode() impl to get 915 elements
> in a single bucket chain.

Right; and the worst case is even worse --
if all hashCodes are  the same, then all elements go into
the same bucket of the same segment.

Aside: It might be handy to have special versions
of JDK hash tables that support a "development mode" where
the table aborts if you have too many elements with same
hashCode. Many people seem to define hashCode() producing
too many duplicates by mistake and never find out.
(We internally apply bit-spreading functions that protect
against nonuniform bit distributions of hashCodes, but cannot
do anything about multiple identical ones.)
Also, when using identity hash codes (including use
of keys that do not override Object.hashCode), the
number of bits of hashCode can be less than you think
(22 in hotspot 32bit mode, even fewer in some other JVMs).
So you may start getting a lot of collisions in large
identity-based tables (for example more than a million).

In general though, segmenting does not impact entry distribution
across buckets. A segmented hash table has the same statistical
properties as an unsegmented one. About which see for example
   http://en.wikipedia.org/wiki/Hash_table
   http://en.wikipedia.org/wiki/Hash_(computer_science)

Large segmented resizable tables (as in CHM) do have a space
advantage over unsegmented ones in that the total
space will be closer to N/loadFactor buckets because
segments independently double in size without doubling whole
table. On the other hand smaller segmented tables occupy
more space than unsegmented because of initial fixed
segment-object overhead.
A paper coming up in ECOOP09 ("Making Sense of Large Heaps"
by Nick Mitchell, Edith Schonberg, and Gary Sevitsky) found
a case where an application was making many thousands of
nearly empty CHMs, which of course leads to massive bloat.
(Don't do that!)
It is possible to reduce this fixed initial overhead
by postponing some initialization. This is being done for
CustomConcurrentHashMap, and should be applied to plain
CHM as well. However in both cases, it requires the upcoming
Fences API to work correctly under the memory model, or
Unsafe cheats until then.


> 
> Btw, I see this comment in the implementation:
>> (The average length [of the bucket chain] is less than two for the default load factor threshold.)
> 
> I'm not sure why it says "less than two" and not "less than one".

Thanks for pointing out that this ought to be clarified.
The context for the comment is for
moves, where you know you have at least one node. The
conditional probability of having another is nearly the
same as plain probability of having one. So total less than
two.

-Doug



From dl at cs.oswego.edu  Fri May 29 09:14:06 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 29 May 2009 09:14:06 -0400
Subject: [concurrency-interest] Amino cbbs (Concurrent Building Blocks)
Message-ID: <4A1FDF9E.8000509@cs.oswego.edu>

To expand on a fleeting reference I made in another post,
people on this list might be interested in trying out
components from the Amino Concurrent Building Blocks
project:
   http://amino-cbbs.wiki.sourceforge.net/
This is an open-source effort sponsored by IBM to create
concurrent data structures and algorithms in C++ and Java.
I'm sure they welcome new contributions as well.

It includes some useful components that for various reasons
are probably not immediate candidates for JDK inclusion --
for example emulation of multi-way CAS, graph algorithms
operating on particular graph representations, and various
lists and trees. One difference from jsr166 efforts is that most
classes are not constrained to adhere to java.util.* interfaces
or conventions. This enables inclusion of for example
class LockFreeVector that doesn't conform to the List API
(and would be difficult at best to make conform) but still
may be useful when you don't require Lists.
Similarly, java.util.*, like the standardized/core libraries
of most languages, doesn't include graph algorithms, since most
rely on particular representations, and there are just
too many commonly used representations -- neighbor
lists, collections of edge-objects,  adjacency matrices,
weighted versions, etc. However, if you can use the
representations offered in Amino cbbs, you can use those
algorithms.

-Doug

From Darron_Shaffer at stercomm.com  Fri May 29 12:17:14 2009
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Fri, 29 May 2009 12:17:14 -0400
Subject: [concurrency-interest] A concurrent, linked HashMap impl
In-Reply-To: <1497161057.20090528194257@devexperts.com>
Message-ID: <FC30D8A2D3DEE64D93E8DA54A1DB349A0742323E@IWDUBCORMSG007.sci.local>

That's just a cache size vs. lookup tradeoff.  If you want to match normal hardware caches, you just make the rule that each bucket may only contain one item.

-- Darron

-----Original Message-----
From: Roman Elizarov [mailto:elizarov at devexperts.com] 
Sent: Thursday, May 28, 2009 10:43 AM
To: Shaffer, Darron
Cc: Doug Lea; Bhasin, Vishal; concurrency-interest at cs.oswego.edu
Subject: Re[2]: [concurrency-interest] A concurrent, linked HashMap impl

Hello Darron!

That means you have a somehow filled hash bucket in the first place,
which makes your hash slow on get operations. We try to keep our
hashes at a very low fill factor (average 50%), so that our gets
operations stumble on the right element at the first attempt most of
the time.

Sincerely,
Roman Elizarov

On Thursday, May 28, 2009 6:43:31 PM you wrote:

SD> One eviction strategy that mirrors hardware caches is to use the
SD> concept of a "cache line".  Make each hash bucket a LRU list and evict if the bucket
SD> size grows over a threshold.

SD> -----Original Message-----
SD> From: concurrency-interest-bounces at cs.oswego.edu
SD> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
SD> Sent: Thursday, May 28, 2009 8:04 AM
SD> To: Bhasin, Vishal
SD> Cc: concurrency-interest at cs.oswego.edu
SD> Subject: Re: [concurrency-interest] A concurrent, linked HashMap impl

SD> Bhasin, Vishal wrote:
>> There is a high need for LRU in CHM - folks end up either creating their
>> own or using a caching product (like JCS or EHcache) for this
>> functionality.

SD> First, we cannot hope to supply something that automates all
SD> possible caching needs, especially those with persistent
SD> (disk-based) components or integrated with J2EE services.

SD> But we can supply some eviction policies in custom maps.
SD> Basically, a cache is a map with an eviction policy.
SD> An eviction policy in turn has two main parts:
SD>    * (approximately) when to evict
SD>    * (approximately) what to evict

SD> For concurrent maps we only want to consider policies
SD> compatible with our concurrency properties.
SD> So strict LIFO-based LRU is out. However, this is not
SD> a big loss. People usually pick LRU because it is
SD> cheap (which it is in non-concurrent maps) and usually
SD> performs OK. However, there are many other related
SD> policies that usually perform at least as well, especially
SD> for those that are ultimately tied to web-based requests
SD> For a slightly dated survey see
SD> http://portal.acm.org/citation.cfm?id=954341
SD> (you might need ACM membership to access this)
SD> A survey of Web cache replacement strategies
SD> Stefan Podlipnig, Laszlo B?sz?rmenyi, ACM Computing Surveys 2003.
SD> Most of these policies choose eviction victims based on
SD> some combination of recency, frequency, and size.

SD> We can provide generalized support for a number of
SD> these by allowing users to tell us, for each
SD> cached value:
SD>    * The item's "utility" as an eviction preference score.
SD>      for example for LRU-ish behavior, the score is access timestamp
SD>    * The item's "weight" as its contribution to the decision of
SD>      whether any item should be evicted. For example, for
SD>      a bounded cache of equal-sized items, each weight is just 1.

SD> Given these (definable in subclassable ValueRecords), we plan to
SD> support methods that will evict low-utility items when a given
SD> total-weight threshold is exceeded. The method can have only
SD> probabilistic specs, both in terms when eviction triggers and
SD> which items are removed.

SD> Aside: It is really too bad that there is no way to
SD> automate size calculations. Just bounding by total number
SD> of items can lead to unbounded footprint growth. The only
SD> recourse is to also declare the values as "Soft" references
SD> to allow the GC to delete, which we will also support, even though
SD> this can also seriously slow down GC.

SD> -Doug




SD> _______________________________________________
SD> Concurrency-interest mailing list
SD> Concurrency-interest at cs.oswego.edu
SD> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

SD> _______________________________________________
SD> Concurrency-interest mailing list
SD> Concurrency-interest at cs.oswego.edu
SD> http://cs.oswego.edu/mailman/listinfo/concurrency-interest





