From radhakrishnan.mohan at gmail.com  Tue Jan  8 03:11:40 2019
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue, 8 Jan 2019 13:41:40 +0530
Subject: [concurrency-interest] j.u.c. Flow usecase
Message-ID: <CAOoXFP_Zc+cQARPVoPTb3QzAooO9ZBGXxa7TTbCBMv1iEJMtcw@mail.gmail.com>

Hi,

This is the explanation from Doug Lea I am trying to understand. Push-style
and pull-style API's are both mentioned here. Which one is Flow adding
support for ?
NIO, AIO, reactivity and back pressure etc. are somewhat confounding. I
think.

Thanks,
Mohan
--------------------

Over the past year, the "reactive-streams"
(http://www.reactive-streams.org/) effort has been defining a minimal
set of interfaces expressing commonalities and allowing
interoperablility across frameworks (including Rx and Akka Play), that
is nearing release.

"These interfaces include provisions for a simple
form of async flow control allowing developers to address resource
control issues that can otherwise cause problems in push-based
systems. Supporting this mini-framework helps avoid unpleasant
surprises possible when trying to use pull-style APIs for "hot"
reactive sources (but conversely is not as good a choice as
java.util.Stream for "cold" sources like collections)."

--------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190108/fb3a8d64/attachment.html>

From viktor.klang at gmail.com  Tue Jan  8 03:57:54 2019
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 8 Jan 2019 09:57:54 +0100
Subject: [concurrency-interest] j.u.c. Flow usecase
In-Reply-To: <CAOoXFP_Zc+cQARPVoPTb3QzAooO9ZBGXxa7TTbCBMv1iEJMtcw@mail.gmail.com>
References: <CAOoXFP_Zc+cQARPVoPTb3QzAooO9ZBGXxa7TTbCBMv1iEJMtcw@mail.gmail.com>
Message-ID: <CANPzfU8_gdWF83Dw5960i58rVeogEv1ROyWRZM=fdS9FeyeyWQ@mail.gmail.com>

Hi Mohan,

Reactive Streams (Flow) can be thought of as ”dynamic push-pull”—its
behavior will oscillate at runtime between push (when consumer is faster)
and pull (when producer is faster) without intervention. The technical
reason for this is that it uses a decoupled amortized pull strategy.
Decoupled in the sense that the consumer is free to request more data when
it needs it and does not need to wait for data to be received first,
amortized in the sense that it can request multiple elements with a single
pull, and pull because it is the consumer who is requesting more data.

Cheers,
V

On Tue, 8 Jan 2019 at 09:15, Mohan Radhakrishnan via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Hi,
>
> This is the explanation from Doug Lea I am trying to understand.
> Push-style and pull-style API's are both mentioned here. Which one is Flow
> adding support for ?
> NIO, AIO, reactivity and back pressure etc. are somewhat confounding. I
> think.
>
> Thanks,
> Mohan
> --------------------
>
> Over the past year, the "reactive-streams"
> (http://www.reactive-streams.org/) effort has been defining a minimal
> set of interfaces expressing commonalities and allowing
> interoperablility across frameworks (including Rx and Akka Play), that
> is nearing release.
>
> "These interfaces include provisions for a simple
> form of async flow control allowing developers to address resource
> control issues that can otherwise cause problems in push-based
> systems. Supporting this mini-framework helps avoid unpleasant
> surprises possible when trying to use pull-style APIs for "hot"
> reactive sources (but conversely is not as good a choice as
> java.util.Stream for "cold" sources like collections)."
>
> --------------------
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190108/238d1df0/attachment.html>

From nathanila at gmail.com  Tue Jan  8 15:42:56 2019
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 8 Jan 2019 13:42:56 -0700
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent Stack
Message-ID: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>

One can use ConcurentLinkedDeque as a stack by using addFirst() and 
pollFirst().  However, each addFirst() call creates a 
ConcurrentLinkedDeque.Node object.

I tried embedding the Node into the data stored in the stack. However, I 
run into the ABA problem where one thread holds a reference to the top's 
next while other threads pop the top, change the stack and push the 
top.  The next reference is now invalid.  I solved this using an 
AtomicStampedReference; however, this means addFirst() allocates with 
every call.

Where can I find a non-blocking allocation-free concurrent stack 
implementation?  Where can I find "documentation" on how to build such a 
stack?

-- 
-Nathan


From david.lloyd at redhat.com  Tue Jan  8 16:32:56 2019
From: david.lloyd at redhat.com (David Lloyd)
Date: Tue, 8 Jan 2019 15:32:56 -0600
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
	Stack
In-Reply-To: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
Message-ID: <CANghgrQARd4LMkyagsZyO5PtfT1pTx45otyjsPrhpSEPNACyag@mail.gmail.com>

You don't need any complex data structure for a concurrent stack,
particularly if you've embedded the "next" pointer in the data.  You
can do it simply in any class like this pseudocode:

AtomicReference<Node> top;

// push an item on the stack

oldTop = top.get();
node.next = oldTop;
while (! top.compareAndSet(oldTop, node)) {
    onSpinWait(); // optional
    oldTop = top.get();
    node.next = oldTop;
}

// pop an item off the stack

node = top.get();
newTop = node.next;
while (! top.compareAndSet(node, newTop)) {
    onSpinWait(); // optional
    node = top.get();
    newTop = node.next;
}

The 'top' field can be a volatile with Atomic*Updater or VarHandle, or
even Unsafe if you wanted to, but the algorithm is basically the same.
The key point is to freeze "node.next" when the value is successfully
put on the stack.

On Tue, Jan 8, 2019 at 2:45 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>
> One can use ConcurentLinkedDeque as a stack by using addFirst() and
> pollFirst().  However, each addFirst() call creates a
> ConcurrentLinkedDeque.Node object.
>
> I tried embedding the Node into the data stored in the stack. However, I
> run into the ABA problem where one thread holds a reference to the top's
> next while other threads pop the top, change the stack and push the
> top.  The next reference is now invalid.  I solved this using an
> AtomicStampedReference; however, this means addFirst() allocates with
> every call.
>
> Where can I find a non-blocking allocation-free concurrent stack
> implementation?  Where can I find "documentation" on how to build such a
> stack?
>
> --
> -Nathan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-- 
- DML

From oleksandr.otenko at gmail.com  Tue Jan  8 16:49:25 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 8 Jan 2019 21:49:25 +0000
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
 Stack
In-Reply-To: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
Message-ID: <4671FC24-1383-4545-ACCB-152CBFE283DD@gmail.com>

You have some unusual use-case in mind.

Alex

> On 8 Jan 2019, at 20:42, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> One can use ConcurentLinkedDeque as a stack by using addFirst() and pollFirst().  However, each addFirst() call creates a ConcurrentLinkedDeque.Node object.
> 
> I tried embedding the Node into the data stored in the stack. However, I run into the ABA problem where one thread holds a reference to the top's next while other threads pop the top, change the stack and push the top.  The next reference is now invalid.  I solved this using an AtomicStampedReference; however, this means addFirst() allocates with every call.
> 
> Where can I find a non-blocking allocation-free concurrent stack implementation?  Where can I find "documentation" on how to build such a stack?
> 
> -- 
> -Nathan
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathanila at gmail.com  Tue Jan  8 16:51:05 2019
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 8 Jan 2019 14:51:05 -0700
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
 Stack
In-Reply-To: <CANghgrQARd4LMkyagsZyO5PtfT1pTx45otyjsPrhpSEPNACyag@mail.gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
 <CANghgrQARd4LMkyagsZyO5PtfT1pTx45otyjsPrhpSEPNACyag@mail.gmail.com>
Message-ID: <4efe614b-7aca-9337-bf75-f867afc27c5d@gmail.com>

That code suffers from the ABA problem.  Here is how that could happen.

1) Thread 1 executes node = top.get();
2) Thread 1 executes newTop = node.next;
3) Thread 2 executes the entire pop()
4) Thread 2 executes push() for some new value
5) Thread 2 executes push() with the value from step #3
6) Thread 1 executes top.compareAndSet(node, newTop).  This loses the 
value from step #4.

Do you have any ideas on how to fix this without incurring an allocation?

I thought about using AtomicStampedReference for top but compareAndSet() 
allocates an object.

-Nathan

On 1/8/2019 2:32 PM, David Lloyd wrote:
> You don't need any complex data structure for a concurrent stack,
> particularly if you've embedded the "next" pointer in the data.  You
> can do it simply in any class like this pseudocode:
>
> AtomicReference<Node> top;
>
> // push an item on the stack
>
> oldTop = top.get();
> node.next = oldTop;
> while (! top.compareAndSet(oldTop, node)) {
>      onSpinWait(); // optional
>      oldTop = top.get();
>      node.next = oldTop;
> }
>
> // pop an item off the stack
>
> node = top.get();
> newTop = node.next;
> while (! top.compareAndSet(node, newTop)) {
>      onSpinWait(); // optional
>      node = top.get();
>      newTop = node.next;
> }
>
> The 'top' field can be a volatile with Atomic*Updater or VarHandle, or
> even Unsafe if you wanted to, but the algorithm is basically the same.
> The key point is to freeze "node.next" when the value is successfully
> put on the stack.
>
> On Tue, Jan 8, 2019 at 2:45 PM Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>> One can use ConcurentLinkedDeque as a stack by using addFirst() and
>> pollFirst().  However, each addFirst() call creates a
>> ConcurrentLinkedDeque.Node object.
>>
>> I tried embedding the Node into the data stored in the stack. However, I
>> run into the ABA problem where one thread holds a reference to the top's
>> next while other threads pop the top, change the stack and push the
>> top.  The next reference is now invalid.  I solved this using an
>> AtomicStampedReference; however, this means addFirst() allocates with
>> every call.
>>
>> Where can I find a non-blocking allocation-free concurrent stack
>> implementation?  Where can I find "documentation" on how to build such a
>> stack?
>>
>> --
>> -Nathan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From martinrb at google.com  Tue Jan  8 16:51:04 2019
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 8 Jan 2019 13:51:04 -0800
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
	Stack
In-Reply-To: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
Message-ID: <CA+kOe08KgvJuFqk0DdQozg0n1MaXdJbEX-A-Ky08S-HfVOzJAg@mail.gmail.com>

David is referring to
https://en.wikipedia.org/wiki/Treiber_stack

It would occasionally be useful to offer versions of Node-based data
structures that exposed and returned Nodes and allowed users to create
Nodes containing user data.

ConcurentLinkedDeque is designed so that previous and next pointers can
always be followed, even in the presence of concurrent modification, since
that is needed for iterators, but trickery is required.  Nodes can never be
reused - some other thread may still be in the middle of a traversal even
after the Node is removed.


On Tue, Jan 8, 2019 at 12:45 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

> One can use ConcurentLinkedDeque as a stack by using addFirst() and
> pollFirst().  However, each addFirst() call creates a
> ConcurrentLinkedDeque.Node object.
>
> I tried embedding the Node into the data stored in the stack. However, I
> run into the ABA problem where one thread holds a reference to the top's
> next while other threads pop the top, change the stack and push the
> top.  The next reference is now invalid.  I solved this using an
> AtomicStampedReference; however, this means addFirst() allocates with
> every call.
>
> Where can I find a non-blocking allocation-free concurrent stack
> implementation?  Where can I find "documentation" on how to build such a
> stack?
>
> --
> -Nathan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190108/80bb01f3/attachment.html>

From nathanila at gmail.com  Tue Jan  8 16:53:46 2019
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 8 Jan 2019 14:53:46 -0700
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
 Stack
In-Reply-To: <4671FC24-1383-4545-ACCB-152CBFE283DD@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
 <4671FC24-1383-4545-ACCB-152CBFE283DD@gmail.com>
Message-ID: <33cc9f06-1108-b3b5-d782-21cd2d15bb35@gmail.com>

Currently, I am using ConcurrentLinkedDeque as a stack.  The heavy use 
creates a lot of allocations which in turns triggers a lot of GCs and 
hence consumes a lot of CPU time.  So, I am looking for a stack that 
does not allocate on the common path.

-Nathan

On 1/8/2019 2:49 PM, Alex Otenko wrote:
> You have some unusual use-case in mind.
>
> Alex
>
>> On 8 Jan 2019, at 20:42, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>>
>> One can use ConcurentLinkedDeque as a stack by using addFirst() and pollFirst().  However, each addFirst() call creates a ConcurrentLinkedDeque.Node object.
>>
>> I tried embedding the Node into the data stored in the stack. However, I run into the ABA problem where one thread holds a reference to the top's next while other threads pop the top, change the stack and push the top.  The next reference is now invalid.  I solved this using an AtomicStampedReference; however, this means addFirst() allocates with every call.
>>
>> Where can I find a non-blocking allocation-free concurrent stack implementation?  Where can I find "documentation" on how to build such a stack?
>>
>> -- 
>> -Nathan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From nathanila at gmail.com  Tue Jan  8 16:55:43 2019
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Tue, 8 Jan 2019 14:55:43 -0700
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
 Stack
In-Reply-To: <CA+kOe08KgvJuFqk0DdQozg0n1MaXdJbEX-A-Ky08S-HfVOzJAg@mail.gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
 <CA+kOe08KgvJuFqk0DdQozg0n1MaXdJbEX-A-Ky08S-HfVOzJAg@mail.gmail.com>
Message-ID: <15853deb-99cc-7c34-b2b9-8b6ffb31ef5a@gmail.com>

Thanks for the link.  I am wondering if a Treiber stack can be modified 
so that nothing is allocated in push() or pop().  My first attempt ran 
into the ABA problem.

-Nathan

On 1/8/2019 2:51 PM, Martin Buchholz wrote:
> David is referring to
> https://en.wikipedia.org/wiki/Treiber_stack
>
> It would occasionally be useful to offer versions of Node-based data 
> structures that exposed and returned Nodes and allowed users to create 
> Nodes containing user data.
>
> ConcurentLinkedDeque is designed so that previous and next pointers 
> can always be followed, even in the presence of concurrent 
> modification, since that is needed for iterators, but trickery is 
> required.  Nodes can never be reused - some other thread may still be 
> in the middle of a traversal even after the Node is removed.
>
>
> On Tue, Jan 8, 2019 at 12:45 PM Nathan and Ila Reynolds via 
> Concurrency-interest <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>
>     One can use ConcurentLinkedDeque as a stack by using addFirst() and
>     pollFirst().  However, each addFirst() call creates a
>     ConcurrentLinkedDeque.Node object.
>
>     I tried embedding the Node into the data stored in the stack.
>     However, I
>     run into the ABA problem where one thread holds a reference to the
>     top's
>     next while other threads pop the top, change the stack and push the
>     top.  The next reference is now invalid.  I solved this using an
>     AtomicStampedReference; however, this means addFirst() allocates with
>     every call.
>
>     Where can I find a non-blocking allocation-free concurrent stack
>     implementation?  Where can I find "documentation" on how to build
>     such a
>     stack?
>
>     -- 
>     -Nathan
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190108/29b8e1bd/attachment.html>

From ben.manes at gmail.com  Tue Jan  8 17:16:23 2019
From: ben.manes at gmail.com (Benjamin Manes)
Date: Tue, 8 Jan 2019 14:16:23 -0800
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
	Stack
In-Reply-To: <15853deb-99cc-7c34-b2b9-8b6ffb31ef5a@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
 <CA+kOe08KgvJuFqk0DdQozg0n1MaXdJbEX-A-Ky08S-HfVOzJAg@mail.gmail.com>
 <15853deb-99cc-7c34-b2b9-8b6ffb31ef5a@gmail.com>
Message-ID: <CAGu0=MNs1H23qjzjPK1RReNtFcrAkuu_mQ0fxwGXY1Us5w4Drg@mail.gmail.com>

You can reduce allocations by using elimination arenas to attempt an
exchange, which is helpful to avoid contention on the top of the stack by
canceling operations. Most often contention is a bigger concern than the
allocation rate of a concurrent stack.

I have some old code that did this, but dropped it because I found a race
in the combining logic (where producers combined such that only one pushed
multiple nodes in a single shot) and I no longer had a use-case to be worth
trying to fix it.
https://github.com/ben-manes/caffeine/blob/v2.0.3/caffeine/src/main/java/com/github/benmanes/caffeine/ConcurrentLinkedStack.java

On Tue, Jan 8, 2019 at 1:56 PM Nathan and Ila Reynolds via
Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:

> Thanks for the link.  I am wondering if a Treiber stack can be modified so
> that nothing is allocated in push() or pop().  My first attempt ran into
> the ABA problem.
>
> -Nathan
>
> On 1/8/2019 2:51 PM, Martin Buchholz wrote:
>
> David is referring to
> https://en.wikipedia.org/wiki/Treiber_stack
>
> It would occasionally be useful to offer versions of Node-based data
> structures that exposed and returned Nodes and allowed users to create
> Nodes containing user data.
>
> ConcurentLinkedDeque is designed so that previous and next pointers can
> always be followed, even in the presence of concurrent modification, since
> that is needed for iterators, but trickery is required.  Nodes can never be
> reused - some other thread may still be in the middle of a traversal even
> after the Node is removed.
>
>
> On Tue, Jan 8, 2019 at 12:45 PM Nathan and Ila Reynolds via
> Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
>
>> One can use ConcurentLinkedDeque as a stack by using addFirst() and
>> pollFirst().  However, each addFirst() call creates a
>> ConcurrentLinkedDeque.Node object.
>>
>> I tried embedding the Node into the data stored in the stack. However, I
>> run into the ABA problem where one thread holds a reference to the top's
>> next while other threads pop the top, change the stack and push the
>> top.  The next reference is now invalid.  I solved this using an
>> AtomicStampedReference; however, this means addFirst() allocates with
>> every call.
>>
>> Where can I find a non-blocking allocation-free concurrent stack
>> implementation?  Where can I find "documentation" on how to build such a
>> stack?
>>
>> --
>> -Nathan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190108/a50fa535/attachment.html>

From david.lloyd at redhat.com  Tue Jan  8 17:45:02 2019
From: david.lloyd at redhat.com (David Lloyd)
Date: Tue, 8 Jan 2019 16:45:02 -0600
Subject: [concurrency-interest] Non-blocking Allocation-free Concurrent
	Stack
In-Reply-To: <4efe614b-7aca-9337-bf75-f867afc27c5d@gmail.com>
References: <564165c4-01c3-8840-6872-acea846202a9@gmail.com>
 <CANghgrQARd4LMkyagsZyO5PtfT1pTx45otyjsPrhpSEPNACyag@mail.gmail.com>
 <4efe614b-7aca-9337-bf75-f867afc27c5d@gmail.com>
Message-ID: <CANghgrRC5NUF7OTMVNmMujZKi+EE9P+1vFFo4Qn8gnM_H1HrZg@mail.gmail.com>

You could maybe use a one-bit spin lock for mutual exclusion instead
of CAS, which would guarantee a clean handoff of ownership between a
pushing and popping thread.  If you're not doing allocation, then the
critical section should only be a couple of field accesses.

On Tue, Jan 8, 2019 at 3:51 PM Nathan and Ila Reynolds
<nathanila at gmail.com> wrote:
>
> That code suffers from the ABA problem.  Here is how that could happen.
>
> 1) Thread 1 executes node = top.get();
> 2) Thread 1 executes newTop = node.next;
> 3) Thread 2 executes the entire pop()
> 4) Thread 2 executes push() for some new value
> 5) Thread 2 executes push() with the value from step #3
> 6) Thread 1 executes top.compareAndSet(node, newTop).  This loses the
> value from step #4.
>
> Do you have any ideas on how to fix this without incurring an allocation?
>
> I thought about using AtomicStampedReference for top but compareAndSet()
> allocates an object.
>
> -Nathan
>
> On 1/8/2019 2:32 PM, David Lloyd wrote:
> > You don't need any complex data structure for a concurrent stack,
> > particularly if you've embedded the "next" pointer in the data.  You
> > can do it simply in any class like this pseudocode:
> >
> > AtomicReference<Node> top;
> >
> > // push an item on the stack
> >
> > oldTop = top.get();
> > node.next = oldTop;
> > while (! top.compareAndSet(oldTop, node)) {
> >      onSpinWait(); // optional
> >      oldTop = top.get();
> >      node.next = oldTop;
> > }
> >
> > // pop an item off the stack
> >
> > node = top.get();
> > newTop = node.next;
> > while (! top.compareAndSet(node, newTop)) {
> >      onSpinWait(); // optional
> >      node = top.get();
> >      newTop = node.next;
> > }
> >
> > The 'top' field can be a volatile with Atomic*Updater or VarHandle, or
> > even Unsafe if you wanted to, but the algorithm is basically the same.
> > The key point is to freeze "node.next" when the value is successfully
> > put on the stack.
> >
> > On Tue, Jan 8, 2019 at 2:45 PM Nathan and Ila Reynolds via
> > Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> >> One can use ConcurentLinkedDeque as a stack by using addFirst() and
> >> pollFirst().  However, each addFirst() call creates a
> >> ConcurrentLinkedDeque.Node object.
> >>
> >> I tried embedding the Node into the data stored in the stack. However, I
> >> run into the ABA problem where one thread holds a reference to the top's
> >> next while other threads pop the top, change the stack and push the
> >> top.  The next reference is now invalid.  I solved this using an
> >> AtomicStampedReference; however, this means addFirst() allocates with
> >> every call.
> >>
> >> Where can I find a non-blocking allocation-free concurrent stack
> >> implementation?  Where can I find "documentation" on how to build such a
> >> stack?
> >>
> >> --
> >> -Nathan
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >



-- 
- DML

From radhakrishnan.mohan at gmail.com  Wed Jan  9 02:25:13 2019
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Wed, 9 Jan 2019 12:55:13 +0530
Subject: [concurrency-interest] j.u.c. Flow usecase
In-Reply-To: <CANPzfU8_gdWF83Dw5960i58rVeogEv1ROyWRZM=fdS9FeyeyWQ@mail.gmail.com>
References: <CAOoXFP_Zc+cQARPVoPTb3QzAooO9ZBGXxa7TTbCBMv1iEJMtcw@mail.gmail.com>
 <CANPzfU8_gdWF83Dw5960i58rVeogEv1ROyWRZM=fdS9FeyeyWQ@mail.gmail.com>
Message-ID: <CAOoXFP8udpcz=u62uYUGaugJO0Cbq4HJgVt=WpWGzfKwLmJ_Aw@mail.gmail.com>

Thanks Victor.

I think these interfaces are provided based on the spec. for library
developers to implement ?

Different libraries could implement "dynamic push-pull" in different ways ?

Mohan
On Tuesday, January 8, 2019, Viktor Klang <viktor.klang at gmail.com> wrote:

> Hi Mohan,
>
> Reactive Streams (Flow) can be thought of as ”dynamic push-pull”—its
> behavior will oscillate at runtime between push (when consumer is faster)
> and pull (when producer is faster) without intervention. The technical
> reason for this is that it uses a decoupled amortized pull strategy.
> Decoupled in the sense that the consumer is free to request more data when
> it needs it and does not need to wait for data to be received first,
> amortized in the sense that it can request multiple elements with a single
> pull, and pull because it is the consumer who is requesting more data.
>
> Cheers,
> V
>
> On Tue, 8 Jan 2019 at 09:15, Mohan Radhakrishnan via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> Hi,
>>
>> This is the explanation from Doug Lea I am trying to understand.
>> Push-style and pull-style API's are both mentioned here. Which one is Flow
>> adding support for ?
>> NIO, AIO, reactivity and back pressure etc. are somewhat confounding. I
>> think.
>>
>> Thanks,
>> Mohan
>> --------------------
>>
>> Over the past year, the "reactive-streams"
>> (http://www.reactive-streams.org/) effort has been defining a minimal
>> set of interfaces expressing commonalities and allowing
>> interoperablility across frameworks (including Rx and Akka Play), that
>> is nearing release.
>>
>> "These interfaces include provisions for a simple
>> form of async flow control allowing developers to address resource
>> control issues that can otherwise cause problems in push-based
>> systems. Supporting this mini-framework helps avoid unpleasant
>> surprises possible when trying to use pull-style APIs for "hot"
>> reactive sources (but conversely is not as good a choice as
>> java.util.Stream for "cold" sources like collections)."
>>
>> --------------------
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> --
> Cheers,
> √
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190109/fb0168bf/attachment-0001.html>

From viktor.klang at gmail.com  Wed Jan  9 04:19:49 2019
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 9 Jan 2019 10:19:49 +0100
Subject: [concurrency-interest] j.u.c. Flow usecase
In-Reply-To: <CAOoXFP8udpcz=u62uYUGaugJO0Cbq4HJgVt=WpWGzfKwLmJ_Aw@mail.gmail.com>
References: <CAOoXFP_Zc+cQARPVoPTb3QzAooO9ZBGXxa7TTbCBMv1iEJMtcw@mail.gmail.com>
 <CANPzfU8_gdWF83Dw5960i58rVeogEv1ROyWRZM=fdS9FeyeyWQ@mail.gmail.com>
 <CAOoXFP8udpcz=u62uYUGaugJO0Cbq4HJgVt=WpWGzfKwLmJ_Aw@mail.gmail.com>
Message-ID: <CANPzfU9NKNS7Q45y_VUX3Fotg+Ms9CSvUNBJEfEeuKUcAca0Gg@mail.gmail.com>

Hi Mohan,

you can find all related information here: http://www.reactive-streams.org/

In short, there's a spec, jars, a TCK, and converters to/from
org.reactive-streams and java.util.concurrent.Flow

On Wed, Jan 9, 2019 at 8:26 AM Mohan Radhakrishnan via Concurrency-interest
<concurrency-interest at cs.oswego.edu> wrote:

>
> Thanks Victor.
>
> I think these interfaces are provided based on the spec. for library
> developers to implement ?
>
> Different libraries could implement "dynamic push-pull" in different ways ?
>
> Mohan
> On Tuesday, January 8, 2019, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>> Hi Mohan,
>>
>> Reactive Streams (Flow) can be thought of as ”dynamic push-pull”—its
>> behavior will oscillate at runtime between push (when consumer is faster)
>> and pull (when producer is faster) without intervention. The technical
>> reason for this is that it uses a decoupled amortized pull strategy.
>> Decoupled in the sense that the consumer is free to request more data when
>> it needs it and does not need to wait for data to be received first,
>> amortized in the sense that it can request multiple elements with a single
>> pull, and pull because it is the consumer who is requesting more data.
>>
>> Cheers,
>> V
>>
>> On Tue, 8 Jan 2019 at 09:15, Mohan Radhakrishnan via Concurrency-interest
>> <concurrency-interest at cs.oswego.edu> wrote:
>>
>>> Hi,
>>>
>>> This is the explanation from Doug Lea I am trying to understand.
>>> Push-style and pull-style API's are both mentioned here. Which one is Flow
>>> adding support for ?
>>> NIO, AIO, reactivity and back pressure etc. are somewhat confounding. I
>>> think.
>>>
>>> Thanks,
>>> Mohan
>>> --------------------
>>>
>>> Over the past year, the "reactive-streams"
>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>> set of interfaces expressing commonalities and allowing
>>> interoperablility across frameworks (including Rx and Akka Play), that
>>> is nearing release.
>>>
>>> "These interfaces include provisions for a simple
>>> form of async flow control allowing developers to address resource
>>> control issues that can otherwise cause problems in push-based
>>> systems. Supporting this mini-framework helps avoid unpleasant
>>> surprises possible when trying to use pull-style APIs for "hot"
>>> reactive sources (but conversely is not as good a choice as
>>> java.util.Stream for "cold" sources like collections)."
>>>
>>> --------------------
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>> --
>> Cheers,
>> √
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190109/20b6860d/attachment.html>

From felix.riemann at andrena.de  Tue Jan 15 10:10:08 2019
From: felix.riemann at andrena.de (Felix Riemann)
Date: Tue, 15 Jan 2019 16:10:08 +0100
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
Message-ID: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>

Hi,

I was asking myself whether the monitor lock/unlock semantics are 
present somewhere in the JMM.

What I mean is the behavior that when one thread is holding a lock, 
another cannot acquire it. I can find it in Section 17.1, but I would 
expect there to be an explicit requirement for the synchronization 
order, but it's not there.

Basic example:

final Object lock = new Object();

T1:
synchronized (lock) {
   //some non-synchronization actions
}

T2:
synchronized (lock) {
   //some non-synchronization actions
}

What prevents the synchronization order from being e.g.
1: acquire lock (by T1)
2: acquire lock (by T2)
3: release lock (by T2)
4: release lock (by T1)


Greets,
Felix



-- 
andrena objects ag
Albert-Nestler-Str. 9
76131 Karlsruhe

t: +49 (0) 721 6105 122
f: +49 (0) 721 6105 140

http://www.andrena.de

Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Schürle
Aufsichtsratsvorsitzender: Rolf Hetzelberger

Sitz der Gesellschaft: Karlsruhe
Amtsgericht Mannheim, HRB 109694
USt-IdNr. DE174314824

From shade at redhat.com  Tue Jan 15 10:24:40 2019
From: shade at redhat.com (Aleksey Shipilev)
Date: Tue, 15 Jan 2019 16:24:40 +0100
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
Message-ID: <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>

On 1/15/19 4:10 PM, Felix Riemann via Concurrency-interest wrote:
> Hi,
> 
> I was asking myself whether the monitor lock/unlock semantics are present somewhere in the JMM.
> 
> What I mean is the behavior that when one thread is holding a lock, another cannot acquire it. I can
> find it in Section 17.1, but I would expect there to be an explicit requirement for the
> synchronization order, but it's not there.

17.1 should be enough, no?

> Basic example:
> 
> final Object lock = new Object();
> 
> T1:
> synchronized (lock) {
>   //some non-synchronization actions
> }
> 
> T2:
> synchronized (lock) {
>   //some non-synchronization actions
> }
> 
> What prevents the synchronization order from being e.g.
> 1: acquire lock (by T1)
> 2: acquire lock (by T2)
> 3: release lock (by T2)
> 4: release lock (by T1)

I think the confusion here is the confusion between "statement" and "action". In JMM sense, (2)
means "T2 had acquired the lock". Note, it is not "T2 had entered synchronized block", not "T2 had
attempted to acquire the lock", it is about the actual acquisition happened.

In that interpretation, this execution is impossible, because JLS 17.1 requires T1 to relinquish the
lock before T2 can acquire it. JMM alone does not allow/forbid everything that happens in the
program, the rest of the specification should also hold. JMM connects with that by requiring that
relatable execution should have the actions that each thread does according to intra-thread
semantics (JLS 17.4.3).

-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190115/6172c551/attachment.sig>

From felix.riemann at andrena.de  Tue Jan 15 12:21:37 2019
From: felix.riemann at andrena.de (Felix Riemann)
Date: Tue, 15 Jan 2019 18:21:37 +0100
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
 <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
Message-ID: <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>

 > JMM connects with that by requiring that
 > relatable execution should have the actions that each thread does
 > according to intra-thread semantics (JLS 17.4.3).

True, but the locking/unlocking actions are inter-thread I believe.


 > In that interpretation, this execution is impossible, because JLS 17.1
 > requires T1 to relinquish the lock before T2 can acquire it.

Yes, but what "before" actually means only becomes clear in 17.4. I
can't think of any reasonable interpretation of "before" aside from
being derived from SO.



Maybe I should interpret it in this way so that 17.1 has an influence in
the set of allowed SO?



[sorry, didn't replay to the list the first time]


-- 
andrena objects ag
Albert-Nestler-Str. 9
76131 Karlsruhe

t: +49 (0) 721 6105 122
f: +49 (0) 721 6105 140

http://www.andrena.de

Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Schürle
Aufsichtsratsvorsitzender: Rolf Hetzelberger

Sitz der Gesellschaft: Karlsruhe
Amtsgericht Mannheim, HRB 109694
USt-IdNr. DE174314824

From davidcholmes at aapt.net.au  Tue Jan 15 15:37:52 2019
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 16 Jan 2019 06:37:52 +1000
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
 <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
 <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>
Message-ID: <00a801d4ad12$30692240$913b66c0$@aapt.net.au>

Felix Riemann writes:
> 
>  > JMM connects with that by requiring that  > relatable execution should have the actions that each thread does  > according to intra-
> thread semantics (JLS 17.4.3).
> 
> True, but the locking/unlocking actions are inter-thread I believe.
> 
>  > In that interpretation, this execution is impossible, because JLS 17.1  > requires T1 to relinquish the lock before T2 can acquire it.
> 
> Yes, but what "before" actually means only becomes clear in 17.4. I can't think of any reasonable interpretation of "before" aside from
> being derived from SO.

"before" is the simple/obvious temporal ordering. The semantics of monitors means that only one thread can hold a given monitor at a time, so an acquire-the-lock in T2 can only happen temporally after the release-the-lock in T1. There's nothing subtle here.

David
-------
 
> 
> 
> Maybe I should interpret it in this way so that 17.1 has an influence in the set of allowed SO?
> 
> 
> 
> [sorry, didn't replay to the list the first time]
> 
> 
> --
> andrena objects ag
> Albert-Nestler-Str. 9
> 76131 Karlsruhe
> 
> t: +49 (0) 721 6105 122
> f: +49 (0) 721 6105 140
> 
> http://www.andrena.de
> 
> Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Sch rle
> Aufsichtsratsvorsitzender: Rolf Hetzelberger
> 
> Sitz der Gesellschaft: Karlsruhe
> Amtsgericht Mannheim, HRB 109694
> USt-IdNr. DE174314824
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From shade at redhat.com  Tue Jan 15 16:03:21 2019
From: shade at redhat.com (Aleksey Shipilev)
Date: Tue, 15 Jan 2019 22:03:21 +0100
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <00a801d4ad12$30692240$913b66c0$@aapt.net.au>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
 <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
 <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>
 <00a801d4ad12$30692240$913b66c0$@aapt.net.au>
Message-ID: <8738c708-b3ac-78d2-c747-7859bd6f98bd@redhat.com>

On 1/15/19 9:37 PM, David Holmes via Concurrency-interest wrote:
>>> JMM connects with that by requiring that  > relatable execution should have the actions that
>>> each thread does  > according to intra-
>> thread semantics (JLS 17.4.3).
>> 
>> True, but the locking/unlocking actions are inter-thread I believe.
>> 
>>> In that interpretation, this execution is impossible, because JLS 17.1  > requires T1 to
>>> relinquish the lock before T2 can acquire it.
>> 
>> Yes, but what "before" actually means only becomes clear in 17.4. I can't think of any
>> reasonable interpretation of "before" aside from being derived from SO.
> 
> "before" is the simple/obvious temporal ordering. The semantics of monitors means that only one
> thread can hold a given monitor at a time, so an acquire-the-lock in T2 can only happen
> temporally after the release-the-lock in T1. There's nothing subtle here.
This.

But in case you want a constructionist explanation, that gives some JMM-style intuition about
"before", consider this. "Acquire lock" has to observe the lock state "unlocked", and it changes it
to "locked", atomically. "Release lock" changes the lock state to "unlocked". This is intuitive from
17.1 about locking semantics.

So, your execution here, assuming that these actions are in SO:

1: acquire lock (by T1)
2: acquire lock (by T2)
3: release lock (by T2)
4: release lock (by T1)

...says that (2) had observed the lock state "unlocked". Which violates SO consistency, because all
the actions that change the lock state to "unlocked" are either after (2) in SO, like (3) or (4), or
before (2) in SO, but protected by (1) that definitely changed the value to "locked".

-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190115/60455919/attachment.sig>

From felix.riemann at andrena.de  Tue Jan 15 16:58:10 2019
From: felix.riemann at andrena.de (Felix Riemann)
Date: Tue, 15 Jan 2019 22:58:10 +0100
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <8738c708-b3ac-78d2-c747-7859bd6f98bd@redhat.com>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
 <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
 <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>
 <00a801d4ad12$30692240$913b66c0$@aapt.net.au>
 <8738c708-b3ac-78d2-c747-7859bd6f98bd@redhat.com>
Message-ID: <baa3dfc2-6e30-31ff-0ab6-fb1839be312c@andrena.de>

 >> "before" is the simple/obvious temporal ordering.

After all I've read about JMM, to me the implications that time has on 
the possible executions of a program seem far from obvious.

Aleksey, you once wrote yourself in a footnote of one of your blogposts 
once how the "intuitive notion of simultaneity and global time" gets 
deconstructed.

 > But in case you want a constructionist explanation

Yes, that's exactly what I was aiming for: I believe that 17.1 can (and 
actually should) be translated purely into SO rules, and your 
explanation can serve as a basis for that (one would still need to 
include that a lock can be acquired multiple times).

Thanks.

Am 15.01.2019 um 22:03 schrieb Aleksey Shipilev:
> On 1/15/19 9:37 PM, David Holmes via Concurrency-interest wrote:
>>>> JMM connects with that by requiring that  > relatable execution should have the actions that
>>>> each thread does  > according to intra-
>>> thread semantics (JLS 17.4.3).
>>>
>>> True, but the locking/unlocking actions are inter-thread I believe.
>>>
>>>> In that interpretation, this execution is impossible, because JLS 17.1  > requires T1 to
>>>> relinquish the lock before T2 can acquire it.
>>>
>>> Yes, but what "before" actually means only becomes clear in 17.4. I can't think of any
>>> reasonable interpretation of "before" aside from being derived from SO.
>>
>> "before" is the simple/obvious temporal ordering. The semantics of monitors means that only one
>> thread can hold a given monitor at a time, so an acquire-the-lock in T2 can only happen
>> temporally after the release-the-lock in T1. There's nothing subtle here.
> This.
> 
> But in case you want a constructionist explanation, that gives some JMM-style intuition about
> "before", consider this. "Acquire lock" has to observe the lock state "unlocked", and it changes it
> to "locked", atomically. "Release lock" changes the lock state to "unlocked". This is intuitive from
> 17.1 about locking semantics.
> 
> So, your execution here, assuming that these actions are in SO:
> 
> 1: acquire lock (by T1)
> 2: acquire lock (by T2)
> 3: release lock (by T2)
> 4: release lock (by T1)
> 
> ...says that (2) had observed the lock state "unlocked". Which violates SO consistency, because all
> the actions that change the lock state to "unlocked" are either after (2) in SO, like (3) or (4), or
> before (2) in SO, but protected by (1) that definitely changed the value to "locked".
> 
> -Aleksey
> 

-- 
andrena objects ag
Albert-Nestler-Str. 9
76131 Karlsruhe

t: +49 (0) 721 6105 122
f: +49 (0) 721 6105 140

http://www.andrena.de

Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Schürle
Aufsichtsratsvorsitzender: Rolf Hetzelberger

Sitz der Gesellschaft: Karlsruhe
Amtsgericht Mannheim, HRB 109694
USt-IdNr. DE174314824

From oleksandr.otenko at gmail.com  Tue Jan 15 17:22:27 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 15 Jan 2019 22:22:27 +0000
Subject: [concurrency-interest] Monitor lock/unlock semantics in the JMM
In-Reply-To: <baa3dfc2-6e30-31ff-0ab6-fb1839be312c@andrena.de>
References: <064f58b54e53f019da39408f884dd5b3@mail.andrena.de>
 <f5858958-139f-f698-daec-27bd182fb6da@redhat.com>
 <af3b9353-ac5f-81d8-4706-fcd20109bab3@andrena.de>
 <00a801d4ad12$30692240$913b66c0$@aapt.net.au>
 <8738c708-b3ac-78d2-c747-7859bd6f98bd@redhat.com>
 <baa3dfc2-6e30-31ff-0ab6-fb1839be312c@andrena.de>
Message-ID: <5B495A59-BAC7-4B59-9FE9-4F2F03DDBC7A@gmail.com>

“Global time” means total order. Not all things have total order, but synchronization actions are in a total order (so they do have a notion of “global time”)

Alex

> On 15 Jan 2019, at 21:58, Felix Riemann via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> >> "before" is the simple/obvious temporal ordering.
> 
> After all I've read about JMM, to me the implications that time has on the possible executions of a program seem far from obvious.
> 
> Aleksey, you once wrote yourself in a footnote of one of your blogposts once how the "intuitive notion of simultaneity and global time" gets deconstructed.
> 
> > But in case you want a constructionist explanation
> 
> Yes, that's exactly what I was aiming for: I believe that 17.1 can (and actually should) be translated purely into SO rules, and your explanation can serve as a basis for that (one would still need to include that a lock can be acquired multiple times).
> 
> Thanks.
> 
> Am 15.01.2019 um 22:03 schrieb Aleksey Shipilev:
>> On 1/15/19 9:37 PM, David Holmes via Concurrency-interest wrote:
>>>>> JMM connects with that by requiring that  > relatable execution should have the actions that
>>>>> each thread does  > according to intra-
>>>> thread semantics (JLS 17.4.3).
>>>> 
>>>> True, but the locking/unlocking actions are inter-thread I believe.
>>>> 
>>>>> In that interpretation, this execution is impossible, because JLS 17.1  > requires T1 to
>>>>> relinquish the lock before T2 can acquire it.
>>>> 
>>>> Yes, but what "before" actually means only becomes clear in 17.4. I can't think of any
>>>> reasonable interpretation of "before" aside from being derived from SO.
>>> 
>>> "before" is the simple/obvious temporal ordering. The semantics of monitors means that only one
>>> thread can hold a given monitor at a time, so an acquire-the-lock in T2 can only happen
>>> temporally after the release-the-lock in T1. There's nothing subtle here.
>> This.
>> But in case you want a constructionist explanation, that gives some JMM-style intuition about
>> "before", consider this. "Acquire lock" has to observe the lock state "unlocked", and it changes it
>> to "locked", atomically. "Release lock" changes the lock state to "unlocked". This is intuitive from
>> 17.1 about locking semantics.
>> So, your execution here, assuming that these actions are in SO:
>> 1: acquire lock (by T1)
>> 2: acquire lock (by T2)
>> 3: release lock (by T2)
>> 4: release lock (by T1)
>> ...says that (2) had observed the lock state "unlocked". Which violates SO consistency, because all
>> the actions that change the lock state to "unlocked" are either after (2) in SO, like (3) or (4), or
>> before (2) in SO, but protected by (1) that definitely changed the value to "locked".
>> -Aleksey
> 
> -- 
> andrena objects ag
> Albert-Nestler-Str. 9
> 76131 Karlsruhe
> 
> t: +49 (0) 721 6105 122
> f: +49 (0) 721 6105 140
> 
> http://www.andrena.de <http://www.andrena.de/>
> 
> Vorstand: Hagen Buchwald, Dr. Dieter Kuhn, Stefan Schürle
> Aufsichtsratsvorsitzender: Rolf Hetzelberger
> 
> Sitz der Gesellschaft: Karlsruhe
> Amtsgericht Mannheim, HRB 109694
> USt-IdNr. DE174314824
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190115/2d20c51a/attachment-0001.html>

From rba at activeviam.com  Fri Jan 25 10:40:15 2019
From: rba at activeviam.com (=?UTF-8?Q?R=C3=A9mi_Barat?=)
Date: Fri, 25 Jan 2019 16:40:15 +0100
Subject: [concurrency-interest] livelocks in jdk11 not occurring in jdk1.8
 due to work stealing policy change
Message-ID: <CAKsH56WYCB1D4c9kAvPpk3T7v+j3Xyb3LHPcy7MyAf1hHYEd8A@mail.gmail.com>

Hi,

When upgrading our application from jdk1.8 to jdk11, we experienced many
(dead|live)locks. This seems to be due to a change in the way work stealing
is performed.

I created two examples to reproduce such situations. Both show that work
stealing behaves differently between jdk1.8 and jdk11. Both work fine when
compiled and executed with jdk1.8 but do not finish when using jdk11. The
second example illustrates how our code gets into this kind of livelocks.
The source code of the examples is available here:
https://gist.github.com/remibarat/889293cad02dc45a13d98aa4dbfe716e

=== First example

This example uses a Parent task (e.g. mapping in a custom Map when the Map
is full) that will launch a Child task (e.g. to resize the Map, we need to
rehash it). Any new Parent task needs to wait for the Child task to finish.
The Child task will also launch Child tasks.

The idea is that, a thread B in a child task C0 `join`s a child task C2 and
manages to steal a parent task P1 from thread A. However, P1 `join`s C0, so
even though thread A finishes C2, B is blocked by itself.

The following explains how this example behaves, on the left when executed
in jdk11, and on the right when executed with jdk1.8. The code uses a
`ForkJoinPool` with 2 threads, and `CountDownLatch`es to control work
stealing. Note that, although we use a `ForkJoinPool` with 2 threads, with
jdk1.8 we see a third thread coming to help when both thread A and thread B
are blocked.

                ,-------,                         ||
,--------,
                | jdk11 |                         ||                |
jdk1.8 |
                '-------'                         ||
'--------'
                                                  ||
      thread A                      thread B      ||     thread A
            thread B             thread C
time  --------                      --------      ||     --------
            --------             --------
|     starts P0                                   ||     starts P0
|     creates C0                                  ||     creates C0
|         +-- forks C0 <-----  steals --+         ||         +-- forks C0
<-----  steals --+
|         |                         starts C0     ||         |
           starts C0
|         |                         creates C2    ||         |
           creates C2
|         |              ,-> forks C2 --+         ||         |
    forks C2 --+
|         |              |          invokes C1    ||         |
     ^     invokes C1
|     creates P1         |              |         ||     creates P1
      |         |
|         +-- forks P1 <-|-,            |         ||         +-- forks P1
<----------------|--,
|     joins C0           | |            |         ||     joins C0
      |         |  |
|         +-- steals ----' |            |         ||         |
     |         |  '-- compensates --+
|     starts C2            |            |         ||         |
     |         |                starts P1
|         |                |        finishes C1   ||         |
     |         |                joins C0
|         |                |            |         ||         |
     '---------|---------- steals --+
|         |                |        joins C2      ||         |
               |                starts C2
|         |                '-- steals --+         ||         |
           finishes C1              |
|         |                         starts P1     ||         |
           joins C2 -- nothing      |
|         |                         joins C0      ||         |
               |       to steal     |
|     finishes C2                       |         ||         |
               |       from C   finishes C2
|         x                             x         ||         |
           finishes C0              |
|     cannot proceed               cannot proceed ||         |
                                finishes C1
|     (join C0)                    even though    ||     finishes P0
|                                  C2 finished    ||
v                                                 ||

                                               Outputs

=== Thread 13 is ForkJoinPool-1-worker-3          || === Thread 11 is
ForkJoinPool-1-worker-1
=== Thread 14 is ForkJoinPool-1-worker-1          || === Thread 12 is
ForkJoinPool-1-worker-0
                                                  ||
 thread 13 | thread 14                            ||  thread 11 | thread 12
-----------|------------                          ||
-----------|------------
 P0 starts |                                      ||  P0 starts |
           | C0 starts                            ||            | C0 starts
           | C1 starts                            ||            | C1 starts
 C2 starts |                                      ||            |
   | P1 starts (new Thread: 13)
           | C1 ends                              ||            |
   | C2 starts (new Thread: 13)
           | P1 starts                            ||            |
   | C2 ends   (new Thread: 13)
 C2 ends   |                                      ||            | C1 ends
                                                  ||            | C0 ends
[Program does not finish]                         ||            |
   | P1 ends   (new Thread: 13)
                                                  ||  P0 ends   |


The main difference between jdk1.8 and jdk11 seems to be that, when thread
A `join`s thread B:
* In jdk1.8, if A still has a task (e.g. P1) in its work queue, a new
thread (e.g. C) pops up and begins to perform A's task (C is "compensating"
while A is blocked). It is only when the joining thread (e.g. C) has an
empty work queue that it will steal from the thread it joins (e.g. B).
* In jdk11, even though A still has a task in its work queue, A itself will
steal from B. This is documented in `ForkJoinPool.awaitJoin(...)`: "First
tries to locally helping, then scans other queues for a task produced by
one of w's stealers; compensating and blocking if none are found."

As one may wonder why thread A joins C0 after forking P1, I have added
another example, which illustrates how our code encounters a livelock with
jdk11.

=== Second example

This example is based on how our code works. A `CopyAction` will create
`WriteAction`s (e.g. W0, W1, W2) that concurrently write in a custom
dictionary. When the dictionary is full, it must be resized and rehashed,
which
is performed concurrently by a `RehashAction` (R0 created by W0). The
following `WriteAction` (e.g. W1, W2) that will try to write in the
dictionary will need to wait for the `RehashAction` to finish, so they
`join` it. This is where work stealing will occur.

                ,-------,                         ||
,--------,
                | jdk11 |                         ||                |
jdk1.8 |
                '-------'                         ||
'--------'
                                                  ||
      thread A                      thread B      ||     thread A
            thread B             thread C
time  --------                      --------      ||     --------
            --------             --------
|     creates W0                        |         ||     creates W0
                |
|         +-- forks W0 <------ steals --+         ||         +-- forks W0
<------ steals --+
|         |                         starts W0     ||         |
           starts W0
|         |                         starts R0     ||         |
           starts R0
|         |                         creates R1    ||         |
           creates R1
|         |              ,-> forks R1 --+         ||         |
    forks R1 --+
|         |              |          invokes R2    ||         |
           invokes R2
|     creates W1         |              |         ||     creates W1
                |
|         +-- forks W1 <-|-,            |         ||         +-- forks W1
<----------------|--,
|     invokes W2         | |            |         ||     invokes W2
                |  |
|     joins R0           | |            |         ||     joins R0
                |  |
|         +-- steals ----' |            |         ||         |
               |  '--- compensates --+
|     starts R1            |            |         ||         |
               |                 starts W1
|         |                |        finishes R2   ||         |
               |                 joins R0 -- does not
|         |                '-- steals --+         ||         |
           finishes R2               |       steal
|         |                         starts W1     ||         |
           joins R1 --- nothing      |       from B (?)
|         |                         joins R0      ||         |
           starts R1    to steal     |
|     finishes R1                       |         ||         |
           finishes R1  from C       |
|         x                             x         ||         |
           finishes R0               |
|            both tasks cannot proceed            ||         |
           finishes W0           finishes W1
|     (join C0)                    even though    ||     finishes W2
|                                  C2 finished    ||
v                                                 ||

                                               Outputs

=== Thread 13 is ForkJoinPool-1-worker-3          ||   === Thread 11 is
ForkJoinPool-1-worker-1
=== Thread 14 is ForkJoinPool-1-worker-1          ||   === Thread 12 is
ForkJoinPool-1-worker-0
                                                  ||
 thread 13 | thread 14                            ||    thread 11 | thread
12
-----------|------------                          ||
 -----------|------------
 W2 starts |                                      ||    W2 starts |
           | R0 starts                            ||              | R0
starts
           | R2 starts                            ||              | R2
starts
 R1 starts |                                      ||              |
     | W1 starts (new Thread: 13)
           | R2 ends                              ||              | R2 ends
 R1 ends   |                                      ||              | R1
starts
           | W1 starts                            ||              | R1 ends
                                                  ||              | R0 ends
                                                  ||              | W0 ends
                                                  ||              |
     | W1 ends   (new Thread: 13)
                                                  ||    W2 ends   |

Here, with jdk1.8, surprisingly, thread C does not steal from B when it
could, although it stole C2 in the first example.

=======

Is this the expected behavior? Should we re-write our code without using
`joins` (for example, using CountedCompleters), or re-design the way we use
ForkJoinPools?

Thank you for your time,
Rémi Barat
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190125/39bd92ab/attachment-0001.html>

From valentin.male.kovalenko at gmail.com  Mon Jan 28 02:29:48 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 00:29:48 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
Message-ID: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>

Hi everyone,

It seems to be common knowledge (see "Lightweight Contention Management for
E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf)
that simple exponential backoff drastically increases the throughput of
CAS-based operations (e.g. getAndIncrement). I checked this on my own, and
yes, this is still true for OpenJDK 11:
(the test and full measurements is available here
https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
)
[single CPU] 3.4 GHz Intel Core i5 (4 cores)
[OS] macOS 10.13.6 (17G4015)
[JDK] OpenJDK 11.0.1+13

1 thread                                Mode  Cnt    Score   Error  Units
atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
4 threads
atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
32 threads
atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us

The fact that atomicLongGetAndIncrement is faster
than atomicLongGetAndIncrementManual for 1 thread tells me that as a result
of @HotSpotIntrinsicCandidate this method at runtime uses an implementation
different from what could have been compiled from Java sources, and this
different implementation has nothing to do with backoff because with 1
thread there are no CAS failures.

What are the reasons behind the choice to not use backoff in OpenJDK's
AtomicLong.getAndIncrement?

Regards,
Valentin
[image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image: GitHub]
<https://github.com/stIncMale>   [image: YouTube]
<https://www.youtube.com/user/stIncMale>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/4f74182c/attachment.html>

From oleksandr.otenko at gmail.com  Mon Jan 28 03:15:08 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 28 Jan 2019 08:15:08 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
Message-ID: <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>

The intrinsic probably compiles to lock:xadd on x86, which has the getAndAdd semantics.

The reason for not using it for backoff is because there is no backoff in the intrinsic. The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.


Alex

> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> Hi everyone,
> 
> It seems to be common knowledge (see "Lightweight Contention Management for E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf <https://arxiv.org/pdf/1305.5800.pdf>) that simple exponential backoff drastically increases the throughput of CAS-based operations (e.g. getAndIncrement). I checked this on my own, and yes, this is still true for OpenJDK 11:
> (the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java <https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java>)
> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
> [OS] macOS 10.13.6 (17G4015)
> [JDK] OpenJDK 11.0.1+13
> 
> 1 thread                                Mode  Cnt    Score   Error  Units
> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
> 4 threads
> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
> 32 threads
> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
> 
> The fact that atomicLongGetAndIncrement is faster than atomicLongGetAndIncrementManual for 1 thread tells me that as a result of @HotSpotIntrinsicCandidate this method at runtime uses an implementation different from what could have been compiled from Java sources, and this different implementation has nothing to do with backoff because with 1 thread there are no CAS failures.
> 
> What are the reasons behind the choice to not use backoff in OpenJDK's AtomicLong.getAndIncrement?
> 
> Regards,
> Valentin
>  <https://www.linkedin.com/in/stIncMale>    <https://github.com/stIncMale>    <https://www.youtube.com/user/stIncMale>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/048f7773/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon Jan 28 03:49:30 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 01:49:30 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
Message-ID: <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>

> The reason for not using it for backoff is because there is no backoff in
the intrinsic
So lock:xadd does not have backoff logic, may it be then a good idea to not
use this intrinsic and use "manual" getAndIncrement implementation with
backoff in JDK since it results in a significantly higher throughput? In
the mentioned article authors tested not only multiple architectures with
many CPU threads (not 4 like I do), but also CAS-based data structures, and
they achieved throughput improvements with backoff in all tested cases.

> The reason for getting different results is possibly sharing the cache:
backoff allows one CPU to usurp the cache line and make progress; without
the backoff the line is always shared between all the CPUs.
Sure, since threads retry CAS less often once encounter failures, this
effectively reduces contention.

Valentin

On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> The intrinsic probably compiles to lock:xadd on x86, which has the
> getAndAdd semantics.
>
> The reason for not using it for backoff is because there is no backoff in
> the intrinsic. The reason for getting different results is possibly sharing
> the cache: backoff allows one CPU to usurp the cache line and make
> progress; without the backoff the line is always shared between all the
> CPUs.
>
>
> Alex
>
> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
> Hi everyone,
>
> It seems to be common knowledge (see "Lightweight Contention Management
> for E cient Compare-and-Swap Operations",
> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> drastically increases the throughput of CAS-based operations (e.g.
> getAndIncrement). I checked this on my own, and yes, this is still true for
> OpenJDK 11:
> (the test and full measurements is available here
> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
> )
> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
> [OS] macOS 10.13.6 (17G4015)
> [JDK] OpenJDK 11.0.1+13
>
> 1 thread                                Mode  Cnt    Score   Error  Units
> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
> 4 threads
> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
> 32 threads
> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>
> The fact that atomicLongGetAndIncrement is faster
> than atomicLongGetAndIncrementManual for 1 thread tells me that as a result
> of @HotSpotIntrinsicCandidate this method at runtime uses an implementation
> different from what could have been compiled from Java sources, and this
> different implementation has nothing to do with backoff because with 1
> thread there are no CAS failures.
>
> What are the reasons behind the choice to not use backoff in OpenJDK's
> AtomicLong.getAndIncrement?
>
> Regards,
> Valentin
> [image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image:
> GitHub] <https://github.com/stIncMale>   [image: YouTube]
> <https://www.youtube.com/user/stIncMale>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/a30dface/attachment-0001.html>

From oleksandr.otenko at gmail.com  Mon Jan 28 04:24:18 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 28 Jan 2019 09:24:18 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
Message-ID: <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>

Last time I heard this discussed the conclusion was that there isn’t a universal backoff strategy that suits all platforms and cases.

For example, the logic of backoff used in this test case is good for continuous atomic incrementing. Will the CPUs really be spinning in a trivial increment loop? More likely you’ll slam the hot spot with multiple CPUs at once, then the threads go away to do things, then come back. Backing off the way it’s done in the test is not necessarily going to help - the win comes from one thread being able to do several increments while the others do not contend to do the same; but if the threads are not attempting to do several increments, backoff is going to work like the usual CAS loop.

lock:xadd defers this to the hardware. The instruction has no “redo” - but of course internally at hardware level it probably is still a CAS-like loop with a retry - and a backoff (I think all consensus protocols have corner cases that can only be resolved through timing a race).

Alex

> On 28 Jan 2019, at 08:49, Valentin Kovalenko <valentin.male.kovalenko at gmail.com> wrote:
> 
> > The reason for not using it for backoff is because there is no backoff in the intrinsic
> So lock:xadd does not have backoff logic, may it be then a good idea to not use this intrinsic and use "manual" getAndIncrement implementation with backoff in JDK since it results in a significantly higher throughput? In the mentioned article authors tested not only multiple architectures with many CPU threads (not 4 like I do), but also CAS-based data structures, and they achieved throughput improvements with backoff in all tested cases.
> 
> > The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
> Sure, since threads retry CAS less often once encounter failures, this effectively reduces contention.
> 
> Valentin
> 
> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> The intrinsic probably compiles to lock:xadd on x86, which has the getAndAdd semantics.
> 
> The reason for not using it for backoff is because there is no backoff in the intrinsic. The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
> 
> 
> Alex
> 
>> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>> 
>> Hi everyone,
>> 
>> It seems to be common knowledge (see "Lightweight Contention Management for E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf <https://arxiv.org/pdf/1305.5800.pdf>) that simple exponential backoff drastically increases the throughput of CAS-based operations (e.g. getAndIncrement). I checked this on my own, and yes, this is still true for OpenJDK 11:
>> (the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java <https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java>)
>> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
>> [OS] macOS 10.13.6 (17G4015)
>> [JDK] OpenJDK 11.0.1+13
>> 
>> 1 thread                                Mode  Cnt    Score   Error  Units
>> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
>> 4 threads
>> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
>> 32 threads
>> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>> 
>> The fact that atomicLongGetAndIncrement is faster than atomicLongGetAndIncrementManual for 1 thread tells me that as a result of @HotSpotIntrinsicCandidate this method at runtime uses an implementation different from what could have been compiled from Java sources, and this different implementation has nothing to do with backoff because with 1 thread there are no CAS failures.
>> 
>> What are the reasons behind the choice to not use backoff in OpenJDK's AtomicLong.getAndIncrement?
>> 
>> Regards,
>> Valentin
>>  <https://www.linkedin.com/in/stIncMale>    <https://github.com/stIncMale>    <https://www.youtube.com/user/stIncMale>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/ec550d91/attachment.html>

From nigro.fra at gmail.com  Mon Jan 28 04:24:24 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 28 Jan 2019 10:24:24 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
Message-ID: <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>

@alex
> may it be then a good idea to not use this intrinsic and use "manual"
getAndIncrement implementation with backoff in JDK since it results in a
significantly higher throughput?
Depends if you're seeking max throughput or lower (global) latencies

>  In the mentioned article authors tested not only multiple architectures
with many CPU threads (not 4 like I do), but also CAS-based data
structures, and they achieved throughput improvements with backoff in all
tested cases.

It depends how the data-structures are designed: if taking advantage of
higher throughput or lower latencies: I've recently created this XADD-based
q on https://github.com/JCTools/JCTools/pull/230 and with a number of
producers <= number of cores (pinned) I'm not getting any improvement (if
not worst latencies) if I manual backoff instead of using XADD.

Thanks for sharing the article anyway, is very interesting :)



Il giorno lun 28 gen 2019 alle ore 09:51 Valentin Kovalenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> > The reason for not using it for backoff is because there is no backoff
> in the intrinsic
> So lock:xadd does not have backoff logic, may it be then a good idea to
> not use this intrinsic and use "manual" getAndIncrement implementation with
> backoff in JDK since it results in a significantly higher throughput? In
> the mentioned article authors tested not only multiple architectures with
> many CPU threads (not 4 like I do), but also CAS-based data structures, and
> they achieved throughput improvements with backoff in all tested cases.
>
> > The reason for getting different results is possibly sharing the cache:
> backoff allows one CPU to usurp the cache line and make progress; without
> the backoff the line is always shared between all the CPUs.
> Sure, since threads retry CAS less often once encounter failures, this
> effectively reduces contention.
>
> Valentin
>
> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> The intrinsic probably compiles to lock:xadd on x86, which has the
>> getAndAdd semantics.
>>
>> The reason for not using it for backoff is because there is no backoff in
>> the intrinsic. The reason for getting different results is possibly sharing
>> the cache: backoff allows one CPU to usurp the cache line and make
>> progress; without the backoff the line is always shared between all the
>> CPUs.
>>
>>
>> Alex
>>
>> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu> wrote:
>>
>> Hi everyone,
>>
>> It seems to be common knowledge (see "Lightweight Contention Management
>> for E cient Compare-and-Swap Operations",
>> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
>> drastically increases the throughput of CAS-based operations (e.g.
>> getAndIncrement). I checked this on my own, and yes, this is still true for
>> OpenJDK 11:
>> (the test and full measurements is available here
>> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
>> )
>> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
>> [OS] macOS 10.13.6 (17G4015)
>> [JDK] OpenJDK 11.0.1+13
>>
>> 1 thread                                Mode  Cnt    Score   Error  Units
>> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
>> 4 threads
>> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
>> 32 threads
>> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>>
>> The fact that atomicLongGetAndIncrement is faster
>> than atomicLongGetAndIncrementManual for 1 thread tells me that as a result
>> of @HotSpotIntrinsicCandidate this method at runtime uses an implementation
>> different from what could have been compiled from Java sources, and this
>> different implementation has nothing to do with backoff because with 1
>> thread there are no CAS failures.
>>
>> What are the reasons behind the choice to not use backoff in OpenJDK's
>> AtomicLong.getAndIncrement?
>>
>> Regards,
>> Valentin
>> [image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image:
>> GitHub] <https://github.com/stIncMale>   [image: YouTube]
>> <https://www.youtube.com/user/stIncMale>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/325db494/attachment-0001.html>

From nigro.fra at gmail.com  Mon Jan 28 04:28:10 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 28 Jan 2019 10:28:10 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
Message-ID: <CAKxGtTWr1gYYq2C7Kfy5Z4seZ4KyOHWpKQj0U2XNdv+Dj0OcPg@mail.gmail.com>

@alex
Sorry, the previous answer was for @valentin ! :P

Il giorno lun 28 gen 2019 alle ore 10:25 Alex Otenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> Last time I heard this discussed the conclusion was that there isn’t a
> universal backoff strategy that suits all platforms and cases.
>
> For example, the logic of backoff used in this test case is good for
> continuous atomic incrementing. Will the CPUs really be spinning in a
> trivial increment loop? More likely you’ll slam the hot spot with multiple
> CPUs at once, then the threads go away to do things, then come back.
> Backing off the way it’s done in the test is not necessarily going to help
> - the win comes from one thread being able to do several increments while
> the others do not contend to do the same; but if the threads are not
> attempting to do several increments, backoff is going to work like the
> usual CAS loop.
>
> lock:xadd defers this to the hardware. The instruction has no “redo” - but
> of course internally at hardware level it probably is still a CAS-like loop
> with a retry - and a backoff (I think all consensus protocols have corner
> cases that can only be resolved through timing a race).
>
> Alex
>
>
> On 28 Jan 2019, at 08:49, Valentin Kovalenko <
> valentin.male.kovalenko at gmail.com> wrote:
>
> > The reason for not using it for backoff is because there is no backoff
> in the intrinsic
> So lock:xadd does not have backoff logic, may it be then a good idea to
> not use this intrinsic and use "manual" getAndIncrement implementation with
> backoff in JDK since it results in a significantly higher throughput? In
> the mentioned article authors tested not only multiple architectures with
> many CPU threads (not 4 like I do), but also CAS-based data structures, and
> they achieved throughput improvements with backoff in all tested cases.
>
> > The reason for getting different results is possibly sharing the cache:
> backoff allows one CPU to usurp the cache line and make progress; without
> the backoff the line is always shared between all the CPUs.
> Sure, since threads retry CAS less often once encounter failures, this
> effectively reduces contention.
>
> Valentin
>
> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> The intrinsic probably compiles to lock:xadd on x86, which has the
>> getAndAdd semantics.
>>
>> The reason for not using it for backoff is because there is no backoff in
>> the intrinsic. The reason for getting different results is possibly sharing
>> the cache: backoff allows one CPU to usurp the cache line and make
>> progress; without the backoff the line is always shared between all the
>> CPUs.
>>
>>
>> Alex
>>
>> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu> wrote:
>>
>> Hi everyone,
>>
>> It seems to be common knowledge (see "Lightweight Contention Management
>> for E cient Compare-and-Swap Operations",
>> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
>> drastically increases the throughput of CAS-based operations (e.g.
>> getAndIncrement). I checked this on my own, and yes, this is still true for
>> OpenJDK 11:
>> (the test and full measurements is available here
>> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
>> )
>> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
>> [OS] macOS 10.13.6 (17G4015)
>> [JDK] OpenJDK 11.0.1+13
>>
>> 1 thread                                Mode  Cnt    Score   Error  Units
>> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
>> 4 threads
>> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
>> 32 threads
>> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>>
>> The fact that atomicLongGetAndIncrement is faster
>> than atomicLongGetAndIncrementManual for 1 thread tells me that as a result
>> of @HotSpotIntrinsicCandidate this method at runtime uses an implementation
>> different from what could have been compiled from Java sources, and this
>> different implementation has nothing to do with backoff because with 1
>> thread there are no CAS failures.
>>
>> What are the reasons behind the choice to not use backoff in OpenJDK's
>> AtomicLong.getAndIncrement?
>>
>> Regards,
>> Valentin
>> [image: LinkedIn] <https://www.linkedin.com/in/stIncMale>   [image:
>> GitHub] <https://github.com/stIncMale>   [image: YouTube]
>> <https://www.youtube.com/user/stIncMale>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/2adcbb6a/attachment.html>

From aph at redhat.com  Mon Jan 28 06:07:25 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 11:07:25 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>
Message-ID: <28e1e6dd-3027-7539-2805-b56f6369009a@redhat.com>

On 1/28/19 9:24 AM, Francesco Nigro via Concurrency-interest wrote:
> @alex

>> may it be then a good idea to not use this intrinsic and use
>> "manual" getAndIncrement implementation with backoff in JDK since
>> it results in a significantly higher throughput?

> Depends if you're seeking max throughput or lower (global) latencies

I wonder. If your getAndIncrement() is implemented with a LL/SC loop,
the backoff path is only executed when an SC fails, so unless there
really is significant contention there won't be any increase of
latency.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From nigro.fra at gmail.com  Mon Jan 28 06:20:33 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 28 Jan 2019 12:20:33 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <28e1e6dd-3027-7539-2805-b56f6369009a@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>
 <28e1e6dd-3027-7539-2805-b56f6369009a@redhat.com>
Message-ID: <CAKxGtTVmu4nsSKin6-UyAaw6-ifxYM0xCxXWQ-HSjWbz6fG0Vg@mail.gmail.com>

@andrew
correct, but my concern about latencies is actually the backoff strategy
itself: if not chosen correctly would introduce safepoint polls that
"could" lead to unpredictable slowdown on specific cases.
Same should be said if using Thread::onSpinWait that could be backed by a
pause instruction and recently on the center of a discussion about its
effectiveness (bad implemented AFAIK).

As @alex has said I'm totally of the idea that a real data structure will
provide something interesting to be done when a backoff should occur, but
if it's not the case the risk is just to spend time tuning htimer to avoid
park strategies to work as expected...


Il giorno lun 28 gen 2019, 12:07 Andrew Haley <aph at redhat.com> ha scritto:

> On 1/28/19 9:24 AM, Francesco Nigro via Concurrency-interest wrote:
> > @alex
>
> >> may it be then a good idea to not use this intrinsic and use
> >> "manual" getAndIncrement implementation with backoff in JDK since
> >> it results in a significantly higher throughput?
>
> > Depends if you're seeking max throughput or lower (global) latencies
>
> I wonder. If your getAndIncrement() is implemented with a LL/SC loop,
> the backoff path is only executed when an SC fails, so unless there
> really is significant contention there won't be any increase of
> latency.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/5becb0f9/attachment-0001.html>

From aph at redhat.com  Mon Jan 28 06:33:34 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 11:33:34 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAKxGtTVmu4nsSKin6-UyAaw6-ifxYM0xCxXWQ-HSjWbz6fG0Vg@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>
 <28e1e6dd-3027-7539-2805-b56f6369009a@redhat.com>
 <CAKxGtTVmu4nsSKin6-UyAaw6-ifxYM0xCxXWQ-HSjWbz6fG0Vg@mail.gmail.com>
Message-ID: <2e36bcdd-6ff0-e762-5f7f-d11c0dd88620@redhat.com>

On 1/28/19 11:20 AM, Francesco Nigro wrote:

> correct, but my concern about latencies is actually the backoff
> strategy itself: if not chosen correctly would introduce safepoint
> polls that "could" lead to unpredictable slowdown on specific cases.

True, but that would be a really extreme case.

> Same should be said if using Thread::onSpinWait that could be backed
> by a pause instruction and recently on the center of a discussion
> about its effectiveness (bad implemented AFAIK).

I think so too. I'm waiting to see some proper working measurements on
AArch64 to convince me that it's useful.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Mon Jan 28 06:37:52 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 11:37:52 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <2e36bcdd-6ff0-e762-5f7f-d11c0dd88620@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <CAKxGtTVVJwwcw9ABdGf6SVOtn4P+ir5kgf7KJ-0E43efZ4OW4g@mail.gmail.com>
 <28e1e6dd-3027-7539-2805-b56f6369009a@redhat.com>
 <CAKxGtTVmu4nsSKin6-UyAaw6-ifxYM0xCxXWQ-HSjWbz6fG0Vg@mail.gmail.com>
 <2e36bcdd-6ff0-e762-5f7f-d11c0dd88620@redhat.com>
Message-ID: <b0ab1c37-1182-00e3-b3a7-c43dee9ba698@redhat.com>

On 1/28/19 11:33 AM, Andrew Haley via Concurrency-interest wrote:
> On 1/28/19 11:20 AM, Francesco Nigro wrote:
> 
>> correct, but my concern about latencies is actually the backoff
>> strategy itself: if not chosen correctly would introduce safepoint
>> polls that "could" lead to unpredictable slowdown on specific cases.
> 
> True, but that would be a really extreme case.
> 
>> Same should be said if using Thread::onSpinWait that could be backed
>> by a pause instruction and recently on the center of a discussion
>> about its effectiveness (bad implemented AFAIK).
> 
> I think so too. I'm waiting to see some proper working measurements on
> AArch64 to convince me that it's useful.

e.g. http://mail.openjdk.java.net/pipermail/aarch64-port-dev/2017-August/004870.html

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From valentin.male.kovalenko at gmail.com  Mon Jan 28 06:43:42 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 04:43:42 -0700
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 167,
	Issue 10
In-Reply-To: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
Message-ID: <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>

> there isn’t a universal backoff strategy that suits all platforms and
cases
It does not seem a problem to have different intrinsic backoff strategies
for different platforms, but this, of course, can't be done for different
cases. However, since backoff only comes to play in case of a CAS failure
(or even multiple failures), it should neither affect throughput nor
latency for a low contention scenario. But Francesco provided an
explanation of how this still can negatively affect the performance
(though, I, unfortunately, didn't understand it). Francesco, may I ask you
to explain the same thing for a less educated audience?:)

> lock:xadd defers this to the hardware. The instruction has no “redo” -
but of course internally at hardware level it probably is still a CAS-like
loop with a retry - and a backoff (I think all consensus protocols have
corner cases that can only be resolved through timing a race).
So basically this is the same idea we usually use in programming: rely on
the underlying layer's implementation if it is provided. And the underlying
layer is hardware in this case.

Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/f89d35ed/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon Jan 28 06:46:03 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 04:46:03 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
Message-ID: <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>

> there isn’t a universal backoff strategy that suits all platforms and
cases

It does not seem a problem to have different intrinsic backoff strategies
for different platforms, but this, of course, can't be done for different
cases. However, since backoff only comes to play in case of a CAS failure
(or even multiple failures), it should neither affect throughput nor
latency for a low contention scenario. But Francesco provided an
explanation of how this still can negatively affect the performance
(though, I, unfortunately, didn't understand it). Francesco, may I ask you
to explain the same thing for a less educated audience?:)

> lock:xadd defers this to the hardware. The instruction has no “redo” -
but of course internally at hardware level it probably is still a CAS-like
loop with a retry - and a backoff (I think all consensus protocols have
corner cases that can only be resolved through timing a race).

So basically this is the same idea we usually use in programming: rely on
the underlying layer's implementation if it is provided. And the underlying
layer is hardware in this case.

Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/25ced3a7/attachment.html>

From dl at cs.oswego.edu  Mon Jan 28 06:52:41 2019
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 28 Jan 2019 06:52:41 -0500
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
Message-ID: <8b609916-ae32-8073-d1f2-06d2ac4a4a52@cs.oswego.edu>

On 1/28/19 2:29 AM, Valentin Kovalenko via Concurrency-interest wrote:

> It seems to be common knowledge (see "Lightweight Contention Management
> for E cient Compare-and-Swap Operations",
> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> drastically increases the throughput of CAS-based operations (e.g.
> getAndIncrement). I checked this on my own, and yes, this is still true
> for OpenJDK 11:

I am surprised that you did not compare LongAdder, that uses CAS
failures to guide contention-spreading that is usually much more
effective than backoff. On the other hand it cannot be used when you
need the atomic value of the "get".

-Doug


From nigro.fra at gmail.com  Mon Jan 28 07:33:53 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 28 Jan 2019 13:33:53 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <8b609916-ae32-8073-d1f2-06d2ac4a4a52@cs.oswego.edu>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <8b609916-ae32-8073-d1f2-06d2ac4a4a52@cs.oswego.edu>
Message-ID: <CAKxGtTXVU8egC16xD7qy7CD+RUUq8uoRWbEdeSndfE5Zok0bdg@mail.gmail.com>

*@velentin*

> But Francesco provided an
explanation of how this still can negatively affect the performance
(though, I, unfortunately, didn't understand it). Francesco, may I ask you
to explain the same thing for a less educated audience?:)

Sorry: i've spoken without context :P
The long story short is that there is an implementation detail (but
sort-of-defined in the OpenJDK glossary) of the JVM called "safepoint":
these are safe states
in which the JVM can perform specific operations/optimizations relying on
the fact that during these intervals the mutator threads ie Java threads
cannot break
specific invariants that allows to perform such operations.
The mechanism that allow to reach a safepoint (global or
individual/per-Java-Thread, given that recent changes on JDK 10 has
introduced the notion of thread-.ocal handshake on
https://bugs.openjdk.java.net/browse/JDK-8185640), is by adding safepoint
polls (using the LinuxPefAsmProfiler of JMH show them as {poll} or
{poll_return}
in the annotated ASM) among the compiled code instructions (the bytecode
too, not just ASM).
Such polls, when reached AND a local/global safepoint is needed, will lead
to issue a SEGV (aka segmentation fault) that, when handled, allows the JVM
to start the local/global safepoint.

If a backoff strategy contains a safepoint poll the risk lie when a global
safepoint is being requested; it will wait untill all the mutator threads
will reach it AND the safepoint operation(s) will finish,
leading to latencies outliers (or maybe just spikes, given that you could
choose  -XX:GuaranteedSafepointInterval too).

Doing the same thing in C should be equivalent, considering the most
concurrent primitives are intrinsics (and won't contains such polls AFAIK)
and the resulting ASM should be the same:
the reality is that if you write your own code you can't be sure that polls
are not added, unless you enjoy reading both the JVM source code and ASM
from JMH :P)

AS *@andrew* as already written (and he for sure he can correct any
imprecision about safepoint/safepoint polls I've written) is not something
that a user should care about, but if you're seeking
the best sustainable throughput (or just predictable latencies) is
something that IMHO you should consider at least: then you can ignore it
right after :P

*@doug*
LongAdder rocks +100

Cheers,
Franz (Francesco is longer and most people prefer call me with this :))

Il giorno lun 28 gen 2019 alle ore 12:53 Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> ha scritto:

> On 1/28/19 2:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
>
> > It seems to be common knowledge (see "Lightweight Contention Management
> > for E cient Compare-and-Swap Operations",
> > https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> > drastically increases the throughput of CAS-based operations (e.g.
> > getAndIncrement). I checked this on my own, and yes, this is still true
> > for OpenJDK 11:
>
> I am surprised that you did not compare LongAdder, that uses CAS
> failures to guide contention-spreading that is usually much more
> effective than backoff. On the other hand it cannot be used when you
> need the atomic value of the "get".
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/7767cc68/attachment-0001.html>

From oleksandr.otenko at gmail.com  Mon Jan 28 07:52:54 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 28 Jan 2019 12:52:54 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
 <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
Message-ID: <D8C08F26-BA53-4F98-9CF6-954C47A42378@gmail.com>

Well, you can take a look at the locks used for synchronized. The logic there is aiming at precisely the same cases: should it bias to one thread, should it be a spin-lock, should it inflate to a "fat lock" (OS-provided primitive). But each lock behaves differently, so the JVM makes different decisions for each individual lock.

The atomic increment intrinsics will suffer from the same - each hot spot will require different treatment (but then you also have to come up with, and justify the cost of, added tracking for contention of individual atomic longs).

Alex

> On 28 Jan 2019, at 11:46, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> > there isn’t a universal backoff strategy that suits all platforms and cases
> 
> It does not seem a problem to have different intrinsic backoff strategies
> for different platforms, but this, of course, can't be done for different
> cases. However, since backoff only comes to play in case of a CAS failure
> (or even multiple failures), it should neither affect throughput nor
> latency for a low contention scenario. But Francesco provided an
> explanation of how this still can negatively affect the performance
> (though, I, unfortunately, didn't understand it). Francesco, may I ask you
> to explain the same thing for a less educated audience?:)
> 
> > lock:xadd defers this to the hardware. The instruction has no “redo” - but of course internally at hardware level it probably is still a CAS-like loop with a retry - and a backoff (I think all consensus protocols have corner cases that can only be resolved through timing a race).
> 
> So basically this is the same idea we usually use in programming: rely on
> the underlying layer's implementation if it is provided. And the underlying
> layer is hardware in this case.
> 
> Valentin
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Mon Jan 28 09:32:55 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 14:32:55 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
Message-ID: <6661dc1c-38cf-c2d7-8750-89d5d0519a11@redhat.com>

On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
> (the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java)

I can't build this stuff.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Mon Jan 28 09:50:03 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 14:50:03 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <6661dc1c-38cf-c2d7-8750-89d5d0519a11@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <6661dc1c-38cf-c2d7-8750-89d5d0519a11@redhat.com>
Message-ID: <7c91db56-7177-7a1e-79af-f769927576f6@redhat.com>

On 1/28/19 2:32 PM, Andrew Haley via Concurrency-interest wrote:
> On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
>> (the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java)
> 
> I can't build this stuff.

Actually, I got it done, as long as I use exactly jdk11. No idea how to
run the test though. A simple JMH test would have helped a lot.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From nathanila at gmail.com  Mon Jan 28 10:48:28 2019
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Mon, 28 Jan 2019 08:48:28 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
 <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
Message-ID: <273c4f2a-0dd3-d63a-783e-8b1d361ab96b@gmail.com>

Here is a clarification about lock:xadd on Intel x86 platforms. The 
hardware does **not** execute the operation in a CAS-like loop with a 
retry.  The hardware thread retrieves the cache line in the exclusive 
state, loads the value, does the add and stores the value in the cache 
line.  While the operation is progressing, the core will not allow the 
cache line to be modified by another hardware thread.  Hence, there is 
no chance for failure or a retry.

Here is a typical CAS loop I write in software...

do
{
    expect = m_value.get();
    update = compute(expect);
}
while (!m_value.compareAndSet(expect, update));

The hardware thread will retrieve the cache line in the shared state 
during m_value.get().  The cache line is the vulnerable to being 
modified by another hardware thread during compute(expect). The hardware 
thread then executes compareAndSet().  It changes the cache line to the 
exclusive state, loads the value, compares the value to expect and if 
equal, stores update.  While the CAS instruction is executing, no other 
hardware thread can modify the cache line just like lock:xadd.  A 
backoff supposedly can help with reducing the number of times 
compute(expect) is executed.

There is another concern.  m_value.get() retrieves the cache line in the 
shared state which costs a bus operation and then compareAndSet() has to 
change the cache line to the exclusive state which costs another more 
expensive bus operation.  Think about this code.

do
{
    m_nearby = 0;
    expect   = m_value.get();
    update   = compute(expect);
}
while (!m_value.compareAndSet(expect, update));

The idea is to retrieve the cache line in the exclusive state to begin 
with by writing to the cache line at the beginning of the loop (i.e. 
m_nearby = 0).  This will reduce the number of bus operations.  I have 
never tried this but it might perform better.

If compute() is a long operation, an idea is to use transactional 
memory.  If there is a conflict, then the transaction aborts immediately 
and the hardware thread starts over immediately.  This way the hardware 
thread does not waste time finishing the execution of compute() when the 
store to m_value will definitely fail.

beginTransaction();

m_value= compute(m_value);

commitTransaction();

-Nathan

On 1/28/2019 4:46 AM, Valentin Kovalenko via Concurrency-interest wrote:
> > there isn’t a universal backoff strategy that suits all platforms 
> and cases
>
> It does not seem a problem to have different intrinsic backoff strategies
> for different platforms, but this, of course, can't be done for different
> cases. However, since backoff only comes to play in case of a CAS failure
> (or even multiple failures), it should neither affect throughput nor
> latency for a low contention scenario. But Francesco provided an
> explanation of how this still can negatively affect the performance
> (though, I, unfortunately, didn't understand it). Francesco, may I ask you
> to explain the same thing for a less educated audience?:)
>
> > lock:xadd defers this to the hardware. The instruction has no “redo” 
> - but of course internally at hardware level it probably is still a 
> CAS-like loop with a retry - and a backoff (I think all consensus 
> protocols have corner cases that can only be resolved through timing a 
> race).
>
> So basically this is the same idea we usually use in programming: rely on
> the underlying layer's implementation if it is provided. And the 
> underlying
> layer is hardware in this case.
>
> Valentin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/ac3b5bdc/attachment.html>

From gil at azul.com  Mon Jan 28 11:48:22 2019
From: gil at azul.com (Gil Tene)
Date: Mon, 28 Jan 2019 16:48:22 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>,
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
Message-ID: <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>

The key difference is that, on architectures that support atomic increment or atomic add instructions, getAndIncrement() can (and should) be implemented as a wait-free construct. For various concurrent mechanisms that would use this construct, this makes the difference between being wait free and lock free. And that difference is generally much more important and valuable than the (potentially non-forward-progress-guaranteeing-for-each-thread) throughout benefit of the primitive itself under highly contended situations.

Note that e.g. x86’s lock xadd is wait free in implementation. But so is the LOCK CAS instruction, and so are LL and SC instructions on any processor that support them. AFAIK all user mode instructions are wait free, by definition, and must complete in a small and reasonable bound as a fundamental requirement of proper hardware design (this quality is easy to deduce from basic expectations we have of all hardware with user mode instructions: e.g. just think of the obvious system-hanging user-mode DoS attack if any non-wait-free user mode instructions actually existed).

Any looping or redo logic is always a pure software implementation detail. Whether or not a given construct can be implemented to be wait free depends on the wait-free primitives it has at its disposal (which in turn depend on the capabilities of hardware instructions available).

For the basic atomic constructs (like the various atomic operations offered by the JDK for e.g. AtomicLong) the choice should always be to implement the construct in a wait-free manner where possible. Wherever we don’t, we hobble the ability of concurrent mechanisms written using those constructs to be wait-free.

Sent from my iPad

On Jan 28, 2019, at 1:25 AM, Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>> wrote:

Last time I heard this discussed the conclusion was that there isn’t a universal backoff strategy that suits all platforms and cases.

For example, the logic of backoff used in this test case is good for continuous atomic incrementing. Will the CPUs really be spinning in a trivial increment loop? More likely you’ll slam the hot spot with multiple CPUs at once, then the threads go away to do things, then come back. Backing off the way it’s done in the test is not necessarily going to help - the win comes from one thread being able to do several increments while the others do not contend to do the same; but if the threads are not attempting to do several increments, backoff is going to work like the usual CAS loop.

lock:xadd defers this to the hardware. The instruction has no “redo” - but of course internally at hardware level it probably is still a CAS-like loop with a retry - and a backoff (I think all consensus protocols have corner cases that can only be resolved through timing a race).

Alex

On 28 Jan 2019, at 08:49, Valentin Kovalenko <valentin.male.kovalenko at gmail.com<mailto:valentin.male.kovalenko at gmail.com>> wrote:

> The reason for not using it for backoff is because there is no backoff in the intrinsic
So lock:xadd does not have backoff logic, may it be then a good idea to not use this intrinsic and use "manual" getAndIncrement implementation with backoff in JDK since it results in a significantly higher throughput? In the mentioned article authors tested not only multiple architectures with many CPU threads (not 4 like I do), but also CAS-based data structures, and they achieved throughput improvements with backoff in all tested cases.

> The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
Sure, since threads retry CAS less often once encounter failures, this effectively reduces contention.

Valentin

On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com<mailto:oleksandr.otenko at gmail.com>> wrote:
The intrinsic probably compiles to lock:xadd on x86, which has the getAndAdd semantics.

The reason for not using it for backoff is because there is no backoff in the intrinsic. The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.


Alex

On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>> wrote:

Hi everyone,

It seems to be common knowledge (see "Lightweight Contention Management for E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff drastically increases the throughput of CAS-based operations (e.g. getAndIncrement). I checked this on my own, and yes, this is still true for OpenJDK 11:
(the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java)
[single CPU] 3.4 GHz Intel Core i5 (4 cores)
[OS] macOS 10.13.6 (17G4015)
[JDK] OpenJDK 11.0.1+13

1 thread                                Mode  Cnt    Score   Error  Units
atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
4 threads
atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
32 threads
atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us

The fact that atomicLongGetAndIncrement is faster than atomicLongGetAndIncrementManual for 1 thread tells me that as a result of @HotSpotIntrinsicCandidate this method at runtime uses an implementation different from what could have been compiled from Java sources, and this different implementation has nothing to do with backoff because with 1 thread there are no CAS failures.

What are the reasons behind the choice to not use backoff in OpenJDK's AtomicLong.getAndIncrement?

Regards,
Valentin
[LinkedIn]<https://www.linkedin.com/in/stIncMale>   [GitHub] <https://github.com/stIncMale>    [YouTube] <https://www.youtube.com/user/stIncMale>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/f692926e/attachment-0001.html>

From oleksandr.otenko at gmail.com  Mon Jan 28 11:52:34 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 28 Jan 2019 16:52:34 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
Message-ID: <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>

I think you are right, as long as we stay out of the rabbit hole.

I don’t know how hardware works, but at the finest level two cores attempting to lock the bus, or cache line, or whatever the mechanism to implement mutual exclusion, at that level one of the cores has to wait. So all we do is delegate the wait to that hardware level wait. (But that’s if we do go down the rabbit hole)

Alex

> On 28 Jan 2019, at 16:48, Gil Tene <gil at azul.com> wrote:
> 
> The key difference is that, on architectures that support atomic increment or atomic add instructions, getAndIncrement() can (and should) be implemented as a wait-free construct. For various concurrent mechanisms that would use this construct, this makes the difference between being wait free and lock free. And that difference is generally much more important and valuable than the (potentially non-forward-progress-guaranteeing-for-each-thread) throughout benefit of the primitive itself under highly contended situations.
> 
> Note that e.g. x86’s lock xadd is wait free in implementation. But so is the LOCK CAS instruction, and so are LL and SC instructions on any processor that support them. AFAIK all user mode instructions are wait free, by definition, and must complete in a small and reasonable bound as a fundamental requirement of proper hardware design (this quality is easy to deduce from basic expectations we have of all hardware with user mode instructions: e.g. just think of the obvious system-hanging user-mode DoS attack if any non-wait-free user mode instructions actually existed).
> 
> Any looping or redo logic is always a pure software implementation detail. Whether or not a given construct can be implemented to be wait free depends on the wait-free primitives it has at its disposal (which in turn depend on the capabilities of hardware instructions available).
> 
> For the basic atomic constructs (like the various atomic operations offered by the JDK for e.g. AtomicLong) the choice should always be to implement the construct in a wait-free manner where possible. Wherever we don’t, we hobble the ability of concurrent mechanisms written using those constructs to be wait-free.
> 
> Sent from my iPad
> 
> On Jan 28, 2019, at 1:25 AM, Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>> wrote:
> 
>> Last time I heard this discussed the conclusion was that there isn’t a universal backoff strategy that suits all platforms and cases.
>> 
>> For example, the logic of backoff used in this test case is good for continuous atomic incrementing. Will the CPUs really be spinning in a trivial increment loop? More likely you’ll slam the hot spot with multiple CPUs at once, then the threads go away to do things, then come back. Backing off the way it’s done in the test is not necessarily going to help - the win comes from one thread being able to do several increments while the others do not contend to do the same; but if the threads are not attempting to do several increments, backoff is going to work like the usual CAS loop.
>> 
>> lock:xadd defers this to the hardware. The instruction has no “redo” - but of course internally at hardware level it probably is still a CAS-like loop with a retry - and a backoff (I think all consensus protocols have corner cases that can only be resolved through timing a race).
>> 
>> Alex
>> 
>>> On 28 Jan 2019, at 08:49, Valentin Kovalenko <valentin.male.kovalenko at gmail.com <mailto:valentin.male.kovalenko at gmail.com>> wrote:
>>> 
>>> > The reason for not using it for backoff is because there is no backoff in the intrinsic
>>> So lock:xadd does not have backoff logic, may it be then a good idea to not use this intrinsic and use "manual" getAndIncrement implementation with backoff in JDK since it results in a significantly higher throughput? In the mentioned article authors tested not only multiple architectures with many CPU threads (not 4 like I do), but also CAS-based data structures, and they achieved throughput improvements with backoff in all tested cases.
>>> 
>>> > The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
>>> Sure, since threads retry CAS less often once encounter failures, this effectively reduces contention.
>>> 
>>> Valentin
>>> 
>>> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>>> The intrinsic probably compiles to lock:xadd on x86, which has the getAndAdd semantics.
>>> 
>>> The reason for not using it for backoff is because there is no backoff in the intrinsic. The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
>>> 
>>> 
>>> Alex
>>> 
>>>> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>>>> 
>>>> Hi everyone,
>>>> 
>>>> It seems to be common knowledge (see "Lightweight Contention Management for E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf <https://arxiv.org/pdf/1305.5800.pdf>) that simple exponential backoff drastically increases the throughput of CAS-based operations (e.g. getAndIncrement). I checked this on my own, and yes, this is still true for OpenJDK 11:
>>>> (the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java <https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java>)
>>>> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
>>>> [OS] macOS 10.13.6 (17G4015)
>>>> [JDK] OpenJDK 11.0.1+13
>>>> 
>>>> 1 thread                                Mode  Cnt    Score   Error  Units
>>>> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
>>>> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
>>>> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
>>>> 4 threads
>>>> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
>>>> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
>>>> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
>>>> 32 threads
>>>> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
>>>> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
>>>> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>>>> 
>>>> The fact that atomicLongGetAndIncrement is faster than atomicLongGetAndIncrementManual for 1 thread tells me that as a result of @HotSpotIntrinsicCandidate this method at runtime uses an implementation different from what could have been compiled from Java sources, and this different implementation has nothing to do with backoff because with 1 thread there are no CAS failures.
>>>> 
>>>> What are the reasons behind the choice to not use backoff in OpenJDK's AtomicLong.getAndIncrement?
>>>> 
>>>> Regards,
>>>> Valentin
>>>>  <https://www.linkedin.com/in/stIncMale>    <https://github.com/stIncMale>    <https://www.youtube.com/user/stIncMale>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/11b2fcd3/attachment-0001.html>

From gil at azul.com  Mon Jan 28 12:11:25 2019
From: gil at azul.com (Gil Tene)
Date: Mon, 28 Jan 2019 17:11:25 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>,
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
Message-ID: <551A3C2C-52CE-46B3-901B-7C99883DBECA@azul.com>



Sent from my iPad

On Jan 28, 2019, at 8:54 AM, Alex Otenko <oleksandr.otenko at gmail.com<mailto:oleksandr.otenko at gmail.com>> wrote:

I think you are right, as long as we stay out of the rabbit hole.

I don’t know how hardware works, but at the finest level two cores attempting to lock the bus, or cache line, or whatever the mechanism to implement mutual exclusion, at that level one of the cores has to wait. So all we do is delegate the wait to that hardware level wait. (But that’s if we do go down the rabbit hole)

That rabbit hole takes you to a world where any waits are completely bounded (and where the bounds are very small and reasonable). It is a wait-free rabbit hole.


Alex

On 28 Jan 2019, at 16:48, Gil Tene <gil at azul.com<mailto:gil at azul.com>> wrote:

The key difference is that, on architectures that support atomic increment or atomic add instructions, getAndIncrement() can (and should) be implemented as a wait-free construct. For various concurrent mechanisms that would use this construct, this makes the difference between being wait free and lock free. And that difference is generally much more important and valuable than the (potentially non-forward-progress-guaranteeing-for-each-thread) throughout benefit of the primitive itself under highly contended situations.

Note that e.g. x86’s lock xadd is wait free in implementation. But so is the LOCK CAS instruction, and so are LL and SC instructions on any processor that support them. AFAIK all user mode instructions are wait free, by definition, and must complete in a small and reasonable bound as a fundamental requirement of proper hardware design (this quality is easy to deduce from basic expectations we have of all hardware with user mode instructions: e.g. just think of the obvious system-hanging user-mode DoS attack if any non-wait-free user mode instructions actually existed).

Any looping or redo logic is always a pure software implementation detail. Whether or not a given construct can be implemented to be wait free depends on the wait-free primitives it has at its disposal (which in turn depend on the capabilities of hardware instructions available).

For the basic atomic constructs (like the various atomic operations offered by the JDK for e.g. AtomicLong) the choice should always be to implement the construct in a wait-free manner where possible. Wherever we don’t, we hobble the ability of concurrent mechanisms written using those constructs to be wait-free.

Sent from my iPad

On Jan 28, 2019, at 1:25 AM, Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>> wrote:

Last time I heard this discussed the conclusion was that there isn’t a universal backoff strategy that suits all platforms and cases.

For example, the logic of backoff used in this test case is good for continuous atomic incrementing. Will the CPUs really be spinning in a trivial increment loop? More likely you’ll slam the hot spot with multiple CPUs at once, then the threads go away to do things, then come back. Backing off the way it’s done in the test is not necessarily going to help - the win comes from one thread being able to do several increments while the others do not contend to do the same; but if the threads are not attempting to do several increments, backoff is going to work like the usual CAS loop.

lock:xadd defers this to the hardware. The instruction has no “redo” - but of course internally at hardware level it probably is still a CAS-like loop with a retry - and a backoff (I think all consensus protocols have corner cases that can only be resolved through timing a race).

Alex

On 28 Jan 2019, at 08:49, Valentin Kovalenko <valentin.male.kovalenko at gmail.com<mailto:valentin.male.kovalenko at gmail.com>> wrote:

> The reason for not using it for backoff is because there is no backoff in the intrinsic
So lock:xadd does not have backoff logic, may it be then a good idea to not use this intrinsic and use "manual" getAndIncrement implementation with backoff in JDK since it results in a significantly higher throughput? In the mentioned article authors tested not only multiple architectures with many CPU threads (not 4 like I do), but also CAS-based data structures, and they achieved throughput improvements with backoff in all tested cases.

> The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.
Sure, since threads retry CAS less often once encounter failures, this effectively reduces contention.

Valentin

On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com<mailto:oleksandr.otenko at gmail.com>> wrote:
The intrinsic probably compiles to lock:xadd on x86, which has the getAndAdd semantics.

The reason for not using it for backoff is because there is no backoff in the intrinsic. The reason for getting different results is possibly sharing the cache: backoff allows one CPU to usurp the cache line and make progress; without the backoff the line is always shared between all the CPUs.


Alex

On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>> wrote:

Hi everyone,

It seems to be common knowledge (see "Lightweight Contention Management for E cient Compare-and-Swap Operations", https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff drastically increases the throughput of CAS-based operations (e.g. getAndIncrement). I checked this on my own, and yes, this is still true for OpenJDK 11:
(the test and full measurements is available here https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java)
[single CPU] 3.4 GHz Intel Core i5 (4 cores)
[OS] macOS 10.13.6 (17G4015)
[JDK] OpenJDK 11.0.1+13

1 thread                                Mode  Cnt    Score   Error  Units
atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
4 threads
atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
32 threads
atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us

The fact that atomicLongGetAndIncrement is faster than atomicLongGetAndIncrementManual for 1 thread tells me that as a result of @HotSpotIntrinsicCandidate this method at runtime uses an implementation different from what could have been compiled from Java sources, and this different implementation has nothing to do with backoff because with 1 thread there are no CAS failures.

What are the reasons behind the choice to not use backoff in OpenJDK's AtomicLong.getAndIncrement?

Regards,
Valentin
[LinkedIn]<https://www.linkedin.com/in/stIncMale>   [GitHub] <https://github.com/stIncMale>    [YouTube] <https://www.youtube.com/user/stIncMale>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/3a142548/attachment-0001.html>

From valentin.male.kovalenko at gmail.com  Mon Jan 28 12:18:40 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 10:18:40 -0700
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 167,
	Issue 11
In-Reply-To: <mailman.23.1548678848.1936.concurrency-interest@cs.oswego.edu>
References: <mailman.23.1548678848.1936.concurrency-interest@cs.oswego.edu>
Message-ID: <CAO-wXwKXoAnQMyqkP_5XOKuzFO4FBJ-eRLdAD+hWhTOdfKHuxA@mail.gmail.com>

[Doug] > I am surprised that you did not compare LongAdder, that uses CAS
failures to guide contention-spreading that is usually much more effective
than backoff. On the other hand it cannot be used when you need the atomic
value of the "get".

Exactly, I was only comparing "atomic" API, and LongAdder while being fast
can only be used for counting and not for concurrency control.

[Franz],
Thank you for the more detailed explanation!

Valentin

On Mon, 28 Jan 2019 at 05:39, <concurrency-interest-request at cs.oswego.edu>
wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Re: getAndIncrement with backoff (Andrew Haley)
>    2. Re: getAndIncrement with backoff (Andrew Haley)
>    3. Re: Concurrency-interest Digest, Vol 167, Issue 10
>       (Valentin Kovalenko)
>    4. getAndIncrement with backoff (Valentin Kovalenko)
>    5. Re: getAndIncrement with backoff (Doug Lea)
>    6. Re: getAndIncrement with backoff (Francesco Nigro)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 28 Jan 2019 11:33:34 +0000
> From: Andrew Haley <aph at redhat.com>
> To: Francesco Nigro <nigro.fra at gmail.com>
> Cc: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>         concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <2e36bcdd-6ff0-e762-5f7f-d11c0dd88620 at redhat.com>
> Content-Type: text/plain; charset=utf-8
>
> On 1/28/19 11:20 AM, Francesco Nigro wrote:
>
> > correct, but my concern about latencies is actually the backoff
> > strategy itself: if not chosen correctly would introduce safepoint
> > polls that "could" lead to unpredictable slowdown on specific cases.
>
> True, but that would be a really extreme case.
>
> > Same should be said if using Thread::onSpinWait that could be backed
> > by a pause instruction and recently on the center of a discussion
> > about its effectiveness (bad implemented AFAIK).
>
> I think so too. I'm waiting to see some proper working measurements on
> AArch64 to convince me that it's useful.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 28 Jan 2019 11:37:52 +0000
> From: Andrew Haley <aph at redhat.com>
> To: Francesco Nigro <nigro.fra at gmail.com>
> Cc: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>         concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <b0ab1c37-1182-00e3-b3a7-c43dee9ba698 at redhat.com>
> Content-Type: text/plain; charset=utf-8
>
> On 1/28/19 11:33 AM, Andrew Haley via Concurrency-interest wrote:
> > On 1/28/19 11:20 AM, Francesco Nigro wrote:
> >
> >> correct, but my concern about latencies is actually the backoff
> >> strategy itself: if not chosen correctly would introduce safepoint
> >> polls that "could" lead to unpredictable slowdown on specific cases.
> >
> > True, but that would be a really extreme case.
> >
> >> Same should be said if using Thread::onSpinWait that could be backed
> >> by a pause instruction and recently on the center of a discussion
> >> about its effectiveness (bad implemented AFAIK).
> >
> > I think so too. I'm waiting to see some proper working measurements on
> > AArch64 to convince me that it's useful.
>
> e.g.
> http://mail.openjdk.java.net/pipermail/aarch64-port-dev/2017-August/004870.html
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 28 Jan 2019 04:43:42 -0700
> From: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>         167,    Issue 10
> Message-ID:
>         <
> CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> > there isn’t a universal backoff strategy that suits all platforms and
> cases
> It does not seem a problem to have different intrinsic backoff strategies
> for different platforms, but this, of course, can't be done for different
> cases. However, since backoff only comes to play in case of a CAS failure
> (or even multiple failures), it should neither affect throughput nor
> latency for a low contention scenario. But Francesco provided an
> explanation of how this still can negatively affect the performance
> (though, I, unfortunately, didn't understand it). Francesco, may I ask you
> to explain the same thing for a less educated audience?:)
>
> > lock:xadd defers this to the hardware. The instruction has no “redo” -
> but of course internally at hardware level it probably is still a CAS-like
> loop with a retry - and a backoff (I think all consensus protocols have
> corner cases that can only be resolved through timing a race).
> So basically this is the same idea we usually use in programming: rely on
> the underlying layer's implementation if it is provided. And the underlying
> layer is hardware in this case.
>
> Valentin
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/f89d35ed/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 4
> Date: Mon, 28 Jan 2019 04:46:03 -0700
> From: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: [concurrency-interest] getAndIncrement with backoff
> Message-ID:
>         <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=
> hCniEJaZtB2A at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> > there isn’t a universal backoff strategy that suits all platforms and
> cases
>
> It does not seem a problem to have different intrinsic backoff strategies
> for different platforms, but this, of course, can't be done for different
> cases. However, since backoff only comes to play in case of a CAS failure
> (or even multiple failures), it should neither affect throughput nor
> latency for a low contention scenario. But Francesco provided an
> explanation of how this still can negatively affect the performance
> (though, I, unfortunately, didn't understand it). Francesco, may I ask you
> to explain the same thing for a less educated audience?:)
>
> > lock:xadd defers this to the hardware. The instruction has no “redo” -
> but of course internally at hardware level it probably is still a CAS-like
> loop with a retry - and a backoff (I think all consensus protocols have
> corner cases that can only be resolved through timing a race).
>
> So basically this is the same idea we usually use in programming: rely on
> the underlying layer's implementation if it is provided. And the underlying
> layer is hardware in this case.
>
> Valentin
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/25ced3a7/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 5
> Date: Mon, 28 Jan 2019 06:52:41 -0500
> From: Doug Lea <dl at cs.oswego.edu>
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <8b609916-ae32-8073-d1f2-06d2ac4a4a52 at cs.oswego.edu>
> Content-Type: text/plain; charset=utf-8
>
> On 1/28/19 2:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
>
> > It seems to be common knowledge (see "Lightweight Contention Management
> > for E cient Compare-and-Swap Operations",
> > https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> > drastically increases the throughput of CAS-based operations (e.g.
> > getAndIncrement). I checked this on my own, and yes, this is still true
> > for OpenJDK 11:
>
> I am surprised that you did not compare LongAdder, that uses CAS
> failures to guide contention-spreading that is usually much more
> effective than backoff. On the other hand it cannot be used when you
> need the atomic value of the "get".
>
> -Doug
>
>
>
> ------------------------------
>
> Message: 6
> Date: Mon, 28 Jan 2019 13:33:53 +0100
> From: Francesco Nigro <nigro.fra at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID:
>         <
> CAKxGtTXVU8egC16xD7qy7CD+RUUq8uoRWbEdeSndfE5Zok0bdg at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> *@velentin*
>
> > But Francesco provided an
> explanation of how this still can negatively affect the performance
> (though, I, unfortunately, didn't understand it). Francesco, may I ask you
> to explain the same thing for a less educated audience?:)
>
> Sorry: i've spoken without context :P
> The long story short is that there is an implementation detail (but
> sort-of-defined in the OpenJDK glossary) of the JVM called "safepoint":
> these are safe states
> in which the JVM can perform specific operations/optimizations relying on
> the fact that during these intervals the mutator threads ie Java threads
> cannot break
> specific invariants that allows to perform such operations.
> The mechanism that allow to reach a safepoint (global or
> individual/per-Java-Thread, given that recent changes on JDK 10 has
> introduced the notion of thread-.ocal handshake on
> https://bugs.openjdk.java.net/browse/JDK-8185640), is by adding safepoint
> polls (using the LinuxPefAsmProfiler of JMH show them as {poll} or
> {poll_return}
> in the annotated ASM) among the compiled code instructions (the bytecode
> too, not just ASM).
> Such polls, when reached AND a local/global safepoint is needed, will lead
> to issue a SEGV (aka segmentation fault) that, when handled, allows the JVM
> to start the local/global safepoint.
>
> If a backoff strategy contains a safepoint poll the risk lie when a global
> safepoint is being requested; it will wait untill all the mutator threads
> will reach it AND the safepoint operation(s) will finish,
> leading to latencies outliers (or maybe just spikes, given that you could
> choose  -XX:GuaranteedSafepointInterval too).
>
> Doing the same thing in C should be equivalent, considering the most
> concurrent primitives are intrinsics (and won't contains such polls AFAIK)
> and the resulting ASM should be the same:
> the reality is that if you write your own code you can't be sure that polls
> are not added, unless you enjoy reading both the JVM source code and ASM
> from JMH :P)
>
> AS *@andrew* as already written (and he for sure he can correct any
> imprecision about safepoint/safepoint polls I've written) is not something
> that a user should care about, but if you're seeking
> the best sustainable throughput (or just predictable latencies) is
> something that IMHO you should consider at least: then you can ignore it
> right after :P
>
> *@doug*
> LongAdder rocks +100
>
> Cheers,
> Franz (Francesco is longer and most people prefer call me with this :))
>
> Il giorno lun 28 gen 2019 alle ore 12:53 Doug Lea via Concurrency-interest
> <
> concurrency-interest at cs.oswego.edu> ha scritto:
>
> > On 1/28/19 2:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
> >
> > > It seems to be common knowledge (see "Lightweight Contention Management
> > > for E cient Compare-and-Swap Operations",
> > > https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> > > drastically increases the throughput of CAS-based operations (e.g.
> > > getAndIncrement). I checked this on my own, and yes, this is still true
> > > for OpenJDK 11:
> >
> > I am surprised that you did not compare LongAdder, that uses CAS
> > failures to guide contention-spreading that is usually much more
> > effective than backoff. On the other hand it cannot be used when you
> > need the atomic value of the "get".
> >
> > -Doug
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/7767cc68/attachment.html
> >
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------
>
> End of Concurrency-interest Digest, Vol 167, Issue 11
> *****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/da9d8436/attachment-0001.html>

From aph at redhat.com  Mon Jan 28 12:24:43 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 17:24:43 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
Message-ID: <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>

On 1/28/19 4:52 PM, Alex Otenko via Concurrency-interest wrote:

> I don’t know how hardware works, but at the finest level two cores
> attempting to lock the bus, or cache line, or whatever the mechanism
> to implement mutual exclusion, at that level one of the cores has to
> wait.

Yes, I agree, but that's true whenever you write to memory. When memory
is updated, only one core has the cache line in exclusive state: the
others have read-only local copies. There's nothing special happening
with something like a CAS in that regard. The cache coherency protocol
handles it in the same way as usual.

However, there is an exception to this. With Arm's "far atomic"
instructions, the intention is not to move cache lines around at
all. AIUI, whichever core already a the cache line in exclusive
state keeps it locally. The core issuing the far atomic op sends a
message to whichever core owns the cache line asking for the op to be
done remotely. The acknowledgment message contains the result. This
avoids cache-line ping-ponging altogether, at least in theory. There
will be some delay at times of high contention because processors will
be waiting for replies, of course.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From valentin.male.kovalenko at gmail.com  Mon Jan 28 12:40:45 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 10:40:45 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <mailman.25.1548694105.1936.concurrency-interest@cs.oswego.edu>
References: <mailman.25.1548694105.1936.concurrency-interest@cs.oswego.edu>
Message-ID: <CAO-wXwKMncPZAzGoj3RuPY5pFxp2t5QGfnWH6rgfT2GHc6nmrw@mail.gmail.com>

[Andrew Haley] > I can't build this stuff.
> Actually, I got it done, as long as I use exactly jdk11. No idea how to
run the test though. A simple JMH test would have helped a lot.

This is, in fact, a plain JMH test :) It can be run by executing
mvn clean test -f benchmarks/pom.xml
-Dstincmale.sandbox.benchmarks.dryRun=false -Dtest=AtomicApiComparisonTest

The only non-JMH part is that I start it via Junit5 runner, but one may
simply add a main method which would
call throughputThreads4/throughputThreads32 methods directly.

[Gil Tene] > x86’s lock xadd is wait free in implementation
Oh, I didn't know this. Yes, the manual retry cycle with CAS is only
lock-free. Interestingly, atomics documentation (
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/atomic/package-summary.html)
only guarantees lock freedom, and does not even mention wait freedom. So we
can't exactly rely on it anyway.

Valentin

On Mon, 28 Jan 2019 at 09:53, <concurrency-interest-request at cs.oswego.edu>
wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Re: getAndIncrement with backoff (Alex Otenko)
>    2. Re: getAndIncrement with backoff (Andrew Haley)
>    3. Re: getAndIncrement with backoff (Andrew Haley)
>    4. Re: getAndIncrement with backoff (Nathan and Ila Reynolds)
>    5. Re: getAndIncrement with backoff (Gil Tene)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 28 Jan 2019 12:52:54 +0000
> From: Alex Otenko <oleksandr.otenko at gmail.com>
> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <D8C08F26-BA53-4F98-9CF6-954C47A42378 at gmail.com>
> Content-Type: text/plain;       charset=utf-8
>
> Well, you can take a look at the locks used for synchronized. The logic
> there is aiming at precisely the same cases: should it bias to one thread,
> should it be a spin-lock, should it inflate to a "fat lock" (OS-provided
> primitive). But each lock behaves differently, so the JVM makes different
> decisions for each individual lock.
>
> The atomic increment intrinsics will suffer from the same - each hot spot
> will require different treatment (but then you also have to come up with,
> and justify the cost of, added tracking for contention of individual atomic
> longs).
>
> Alex
>
> > On 28 Jan 2019, at 11:46, Valentin Kovalenko via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
> >
> > > there isn’t a universal backoff strategy that suits all platforms and
> cases
> >
> > It does not seem a problem to have different intrinsic backoff strategies
> > for different platforms, but this, of course, can't be done for different
> > cases. However, since backoff only comes to play in case of a CAS failure
> > (or even multiple failures), it should neither affect throughput nor
> > latency for a low contention scenario. But Francesco provided an
> > explanation of how this still can negatively affect the performance
> > (though, I, unfortunately, didn't understand it). Francesco, may I ask
> you
> > to explain the same thing for a less educated audience?:)
> >
> > > lock:xadd defers this to the hardware. The instruction has no “redo” -
> but of course internally at hardware level it probably is still a CAS-like
> loop with a retry - and a backoff (I think all consensus protocols have
> corner cases that can only be resolved through timing a race).
> >
> > So basically this is the same idea we usually use in programming: rely on
> > the underlying layer's implementation if it is provided. And the
> underlying
> > layer is hardware in this case.
> >
> > Valentin
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> ------------------------------
>
> Message: 2
> Date: Mon, 28 Jan 2019 14:32:55 +0000
> From: Andrew Haley <aph at redhat.com>
> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>         concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <6661dc1c-38cf-c2d7-8750-89d5d0519a11 at redhat.com>
> Content-Type: text/plain; charset=utf-8
>
> On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
> > (the test and full measurements is available here
> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
> )
>
> I can't build this stuff.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
>
> ------------------------------
>
> Message: 3
> Date: Mon, 28 Jan 2019 14:50:03 +0000
> From: Andrew Haley <aph at redhat.com>
> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>         concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <7c91db56-7177-7a1e-79af-f769927576f6 at redhat.com>
> Content-Type: text/plain; charset=utf-8
>
> On 1/28/19 2:32 PM, Andrew Haley via Concurrency-interest wrote:
> > On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
> >> (the test and full measurements is available here
> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
> )
> >
> > I can't build this stuff.
>
> Actually, I got it done, as long as I use exactly jdk11. No idea how to
> run the test though. A simple JMH test would have helped a lot.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
>
> ------------------------------
>
> Message: 4
> Date: Mon, 28 Jan 2019 08:48:28 -0700
> From: Nathan and Ila Reynolds <nathanila at gmail.com>
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <273c4f2a-0dd3-d63a-783e-8b1d361ab96b at gmail.com>
> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>
> Here is a clarification about lock:xadd on Intel x86 platforms. The
> hardware does **not** execute the operation in a CAS-like loop with a
> retry.  The hardware thread retrieves the cache line in the exclusive
> state, loads the value, does the add and stores the value in the cache
> line.  While the operation is progressing, the core will not allow the
> cache line to be modified by another hardware thread.  Hence, there is
> no chance for failure or a retry.
>
> Here is a typical CAS loop I write in software...
>
> do
> {
>     expect = m_value.get();
>     update = compute(expect);
> }
> while (!m_value.compareAndSet(expect, update));
>
> The hardware thread will retrieve the cache line in the shared state
> during m_value.get().  The cache line is the vulnerable to being
> modified by another hardware thread during compute(expect). The hardware
> thread then executes compareAndSet().  It changes the cache line to the
> exclusive state, loads the value, compares the value to expect and if
> equal, stores update.  While the CAS instruction is executing, no other
> hardware thread can modify the cache line just like lock:xadd.  A
> backoff supposedly can help with reducing the number of times
> compute(expect) is executed.
>
> There is another concern.  m_value.get() retrieves the cache line in the
> shared state which costs a bus operation and then compareAndSet() has to
> change the cache line to the exclusive state which costs another more
> expensive bus operation.  Think about this code.
>
> do
> {
>     m_nearby = 0;
>     expect   = m_value.get();
>     update   = compute(expect);
> }
> while (!m_value.compareAndSet(expect, update));
>
> The idea is to retrieve the cache line in the exclusive state to begin
> with by writing to the cache line at the beginning of the loop (i.e.
> m_nearby = 0).  This will reduce the number of bus operations.  I have
> never tried this but it might perform better.
>
> If compute() is a long operation, an idea is to use transactional
> memory.  If there is a conflict, then the transaction aborts immediately
> and the hardware thread starts over immediately.  This way the hardware
> thread does not waste time finishing the execution of compute() when the
> store to m_value will definitely fail.
>
> beginTransaction();
>
> m_value= compute(m_value);
>
> commitTransaction();
>
> -Nathan
>
> On 1/28/2019 4:46 AM, Valentin Kovalenko via Concurrency-interest wrote:
> > > there isn’t a universal backoff strategy that suits all platforms
> > and cases
> >
> > It does not seem a problem to have different intrinsic backoff strategies
> > for different platforms, but this, of course, can't be done for different
> > cases. However, since backoff only comes to play in case of a CAS failure
> > (or even multiple failures), it should neither affect throughput nor
> > latency for a low contention scenario. But Francesco provided an
> > explanation of how this still can negatively affect the performance
> > (though, I, unfortunately, didn't understand it). Francesco, may I ask
> you
> > to explain the same thing for a less educated audience?:)
> >
> > > lock:xadd defers this to the hardware. The instruction has no “redo”
> > - but of course internally at hardware level it probably is still a
> > CAS-like loop with a retry - and a backoff (I think all consensus
> > protocols have corner cases that can only be resolved through timing a
> > race).
> >
> > So basically this is the same idea we usually use in programming: rely on
> > the underlying layer's implementation if it is provided. And the
> > underlying
> > layer is hardware in this case.
> >
> > Valentin
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/ac3b5bdc/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 5
> Date: Mon, 28 Jan 2019 16:48:22 +0000
> From: Gil Tene <gil at azul.com>
> To: Alex Otenko <oleksandr.otenko at gmail.com>
> Cc: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>         concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] getAndIncrement with backoff
> Message-ID: <9E185769-58E8-4D85-A49F-93720048F5ED at azul.com>
> Content-Type: text/plain; charset="utf-8"
>
> The key difference is that, on architectures that support atomic increment
> or atomic add instructions, getAndIncrement() can (and should) be
> implemented as a wait-free construct. For various concurrent mechanisms
> that would use this construct, this makes the difference between being wait
> free and lock free. And that difference is generally much more important
> and valuable than the (potentially
> non-forward-progress-guaranteeing-for-each-thread) throughout benefit of
> the primitive itself under highly contended situations.
>
> Note that e.g. x86’s lock xadd is wait free in implementation. But so is
> the LOCK CAS instruction, and so are LL and SC instructions on any
> processor that support them. AFAIK all user mode instructions are wait
> free, by definition, and must complete in a small and reasonable bound as a
> fundamental requirement of proper hardware design (this quality is easy to
> deduce from basic expectations we have of all hardware with user mode
> instructions: e.g. just think of the obvious system-hanging user-mode DoS
> attack if any non-wait-free user mode instructions actually existed).
>
> Any looping or redo logic is always a pure software implementation detail.
> Whether or not a given construct can be implemented to be wait free depends
> on the wait-free primitives it has at its disposal (which in turn depend on
> the capabilities of hardware instructions available).
>
> For the basic atomic constructs (like the various atomic operations
> offered by the JDK for e.g. AtomicLong) the choice should always be to
> implement the construct in a wait-free manner where possible. Wherever we
> don’t, we hobble the ability of concurrent mechanisms written using those
> constructs to be wait-free.
>
> Sent from my iPad
>
> On Jan 28, 2019, at 1:25 AM, Alex Otenko via Concurrency-interest <
> concurrency-interest at cs.oswego.edu<mailto:
> concurrency-interest at cs.oswego.edu>> wrote:
>
> Last time I heard this discussed the conclusion was that there isn’t a
> universal backoff strategy that suits all platforms and cases.
>
> For example, the logic of backoff used in this test case is good for
> continuous atomic incrementing. Will the CPUs really be spinning in a
> trivial increment loop? More likely you’ll slam the hot spot with multiple
> CPUs at once, then the threads go away to do things, then come back.
> Backing off the way it’s done in the test is not necessarily going to help
> - the win comes from one thread being able to do several increments while
> the others do not contend to do the same; but if the threads are not
> attempting to do several increments, backoff is going to work like the
> usual CAS loop.
>
> lock:xadd defers this to the hardware. The instruction has no “redo” - but
> of course internally at hardware level it probably is still a CAS-like loop
> with a retry - and a backoff (I think all consensus protocols have corner
> cases that can only be resolved through timing a race).
>
> Alex
>
> On 28 Jan 2019, at 08:49, Valentin Kovalenko <
> valentin.male.kovalenko at gmail.com<mailto:valentin.male.kovalenko at gmail.com>>
> wrote:
>
> > The reason for not using it for backoff is because there is no backoff
> in the intrinsic
> So lock:xadd does not have backoff logic, may it be then a good idea to
> not use this intrinsic and use "manual" getAndIncrement implementation with
> backoff in JDK since it results in a significantly higher throughput? In
> the mentioned article authors tested not only multiple architectures with
> many CPU threads (not 4 like I do), but also CAS-based data structures, and
> they achieved throughput improvements with backoff in all tested cases.
>
> > The reason for getting different results is possibly sharing the cache:
> backoff allows one CPU to usurp the cache line and make progress; without
> the backoff the line is always shared between all the CPUs.
> Sure, since threads retry CAS less often once encounter failures, this
> effectively reduces contention.
>
> Valentin
>
> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com
> <mailto:oleksandr.otenko at gmail.com>> wrote:
> The intrinsic probably compiles to lock:xadd on x86, which has the
> getAndAdd semantics.
>
> The reason for not using it for backoff is because there is no backoff in
> the intrinsic. The reason for getting different results is possibly sharing
> the cache: backoff allows one CPU to usurp the cache line and make
> progress; without the backoff the line is always shared between all the
> CPUs.
>
>
> Alex
>
> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <
> concurrency-interest at cs.oswego.edu<mailto:
> concurrency-interest at cs.oswego.edu>> wrote:
>
> Hi everyone,
>
> It seems to be common knowledge (see "Lightweight Contention Management
> for E cient Compare-and-Swap Operations",
> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
> drastically increases the throughput of CAS-based operations (e.g.
> getAndIncrement). I checked this on my own, and yes, this is still true for
> OpenJDK 11:
> (the test and full measurements is available here
> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
> )
> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
> [OS] macOS 10.13.6 (17G4015)
> [JDK] OpenJDK 11.0.1+13
>
> 1 thread                                Mode  Cnt    Score   Error  Units
> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
> 4 threads
> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
> 32 threads
> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>
> The fact that atomicLongGetAndIncrement is faster than
> atomicLongGetAndIncrementManual for 1 thread tells me that as a result of
> @HotSpotIntrinsicCandidate this method at runtime uses an implementation
> different from what could have been compiled from Java sources, and this
> different implementation has nothing to do with backoff because with 1
> thread there are no CAS failures.
>
> What are the reasons behind the choice to not use backoff in OpenJDK's
> AtomicLong.getAndIncrement?
>
> Regards,
> Valentin
> [LinkedIn]<https://www.linkedin.com/in/stIncMale>   [GitHub] <
> https://github.com/stIncMale>    [YouTube] <
> https://www.youtube.com/user/stIncMale>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:
> Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:
> Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/f692926e/attachment.html
> >
>
> ------------------------------
>
> Subject: Digest Footer
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------
>
> End of Concurrency-interest Digest, Vol 167, Issue 12
> *****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/91804344/attachment-0001.html>

From nigro.fra at gmail.com  Mon Jan 28 12:44:46 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Mon, 28 Jan 2019 18:44:46 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwKMncPZAzGoj3RuPY5pFxp2t5QGfnWH6rgfT2GHc6nmrw@mail.gmail.com>
References: <mailman.25.1548694105.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwKMncPZAzGoj3RuPY5pFxp2t5QGfnWH6rgfT2GHc6nmrw@mail.gmail.com>
Message-ID: <CAKxGtTXtNnDZs791xU5iz-Zv5_1_CT32J52VJiHS8zJFQy3abg@mail.gmail.com>

@valentin
> Interestingly, atomics documentation (
https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/atomic/package-summary.html)
only guarantees lock freedom, and does not even mention wait freedom

It makes sense, considering that not all the architectures provide a
wait-free instr for it


Il giorno lun 28 gen 2019 alle ore 18:42 Valentin Kovalenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> [Andrew Haley] > I can't build this stuff.
> > Actually, I got it done, as long as I use exactly jdk11. No idea how to
> run the test though. A simple JMH test would have helped a lot.
>
> This is, in fact, a plain JMH test :) It can be run by executing
> mvn clean test -f benchmarks/pom.xml
> -Dstincmale.sandbox.benchmarks.dryRun=false -Dtest=AtomicApiComparisonTest
>
> The only non-JMH part is that I start it via Junit5 runner, but one may
> simply add a main method which would
> call throughputThreads4/throughputThreads32 methods directly.
>
> [Gil Tene] > x86’s lock xadd is wait free in implementation
> Oh, I didn't know this. Yes, the manual retry cycle with CAS is only
> lock-free. Interestingly, atomics documentation (
> https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/atomic/package-summary.html)
> only guarantees lock freedom, and does not even mention wait freedom. So we
> can't exactly rely on it anyway.
>
> Valentin
>
> On Mon, 28 Jan 2019 at 09:53, <concurrency-interest-request at cs.oswego.edu>
> wrote:
>
>> Send Concurrency-interest mailing list submissions to
>>         concurrency-interest at cs.oswego.edu
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> or, via email, send a message with subject or body 'help' to
>>         concurrency-interest-request at cs.oswego.edu
>>
>> You can reach the person managing the list at
>>         concurrency-interest-owner at cs.oswego.edu
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of Concurrency-interest digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Re: getAndIncrement with backoff (Alex Otenko)
>>    2. Re: getAndIncrement with backoff (Andrew Haley)
>>    3. Re: getAndIncrement with backoff (Andrew Haley)
>>    4. Re: getAndIncrement with backoff (Nathan and Ila Reynolds)
>>    5. Re: getAndIncrement with backoff (Gil Tene)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 28 Jan 2019 12:52:54 +0000
>> From: Alex Otenko <oleksandr.otenko at gmail.com>
>> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
>> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] getAndIncrement with backoff
>> Message-ID: <D8C08F26-BA53-4F98-9CF6-954C47A42378 at gmail.com>
>> Content-Type: text/plain;       charset=utf-8
>>
>> Well, you can take a look at the locks used for synchronized. The logic
>> there is aiming at precisely the same cases: should it bias to one thread,
>> should it be a spin-lock, should it inflate to a "fat lock" (OS-provided
>> primitive). But each lock behaves differently, so the JVM makes different
>> decisions for each individual lock.
>>
>> The atomic increment intrinsics will suffer from the same - each hot spot
>> will require different treatment (but then you also have to come up with,
>> and justify the cost of, added tracking for contention of individual atomic
>> longs).
>>
>> Alex
>
>
>>
>> > On 28 Jan 2019, at 11:46, Valentin Kovalenko via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu> wrote:
>> >
>> > > there isn’t a universal backoff strategy that suits all platforms and
>> cases
>> >
>>
> > It does not seem a problem to have different intrinsic backoff strategies
>> > for different platforms, but this, of course, can't be done for
>> different
>> > cases. However, since backoff only comes to play in case of a CAS
>> failure
>> > (or even multiple failures), it should neither affect throughput nor
>> > latency for a low contention scenario. But Francesco provided an
>
>
>> > explanation of how this still can negatively affect the performance
>> > (though, I, unfortunately, didn't understand it). Francesco, may I ask
>> you
>> > to explain the same thing for a less educated audience?:)
>> >
>> > > lock:xadd defers this to the hardware. The instruction has no “redo”
>> - but of course internally at hardware level it probably is still a
>> CAS-like loop with a retry - and a backoff (I think all consensus protocols
>> have corner cases that can only be resolved through timing a race).
>> >
>>
> > So basically this is the same idea we usually use in programming: rely on
>> > the underlying layer's implementation if it is provided. And the
>> underlying
>> > layer is hardware in this case.
>> >
>> > Valentin
>
>
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Mon, 28 Jan 2019 14:32:55 +0000
>> From: Andrew Haley <aph at redhat.com>
>> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>>         concurrency-interest <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] getAndIncrement with backoff
>> Message-ID: <6661dc1c-38cf-c2d7-8750-89d5d0519a11 at redhat.com>
>> Content-Type: text/plain; charset=utf-8
>
>
>>
>> On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
>> > (the test and full measurements is available here
>> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
>> )
>>
>> I can't build this stuff.
>>
>> --
>> Andrew Haley
>> Java Platform Lead Engineer
>> Red Hat UK Ltd. <https://www.redhat.com>
>> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>>
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Mon, 28 Jan 2019 14:50:03 +0000
>> From: Andrew Haley <aph at redhat.com>
>> To: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>>         concurrency-interest <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] getAndIncrement with backoff
>> Message-ID: <7c91db56-7177-7a1e-79af-f769927576f6 at redhat.com>
>> Content-Type: text/plain; charset=utf-8
>
>
>>
>> On 1/28/19 2:32 PM, Andrew Haley via Concurrency-interest wrote:
>> > On 1/28/19 7:29 AM, Valentin Kovalenko via Concurrency-interest wrote:
>> >> (the test and full measurements is available here
>> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
>> )
>> >
>> > I can't build this stuff.
>>
>> Actually, I got it done, as long as I use exactly jdk11. No idea how to
>> run the test though. A simple JMH test would have helped a lot.
>>
>> --
>> Andrew Haley
>> Java Platform Lead Engineer
>> Red Hat UK Ltd. <https://www.redhat.com>
>> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>>
>>
>> ------------------------------
>>
>> Message: 4
>> Date: Mon, 28 Jan 2019 08:48:28 -0700
>> From: Nathan and Ila Reynolds <nathanila at gmail.com>
>> To: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] getAndIncrement with backoff
>> Message-ID: <273c4f2a-0dd3-d63a-783e-8b1d361ab96b at gmail.com>
>> Content-Type: text/plain; charset="utf-8"; Format="flowed"
>>
>> Here is a clarification about lock:xadd on Intel x86 platforms. The
>> hardware does **not** execute the operation in a CAS-like loop with a
>> retry.  The hardware thread retrieves the cache line in the exclusive
>> state, loads the value, does the add and stores the value in the cache
>> line.  While the operation is progressing, the core will not allow the
>> cache line to be modified by another hardware thread.  Hence, there is
>> no chance for failure or a retry.
>>
>> Here is a typical CAS loop I write in software...
>>
>> do
>> {
>>     expect = m_value.get();
>>     update = compute(expect);
>> }
>> while (!m_value.compareAndSet(expect, update));
>>
>> The hardware thread will retrieve the cache line in the shared state
>> during m_value.get().  The cache line is the vulnerable to being
>> modified by another hardware thread during compute(expect). The hardware
>> thread then executes compareAndSet().  It changes the cache line to the
>> exclusive state, loads the value, compares the value to expect and if
>> equal, stores update.  While the CAS instruction is executing, no other
>> hardware thread can modify the cache line just like lock:xadd.  A
>> backoff supposedly can help with reducing the number of times
>> compute(expect) is executed.
>>
>> There is another concern.  m_value.get() retrieves the cache line in the
>> shared state which costs a bus operation and then compareAndSet() has to
>> change the cache line to the exclusive state which costs another more
>> expensive bus operation.  Think about this code.
>>
>> do
>> {
>>     m_nearby = 0;
>>     expect   = m_value.get();
>>     update   = compute(expect);
>> }
>> while (!m_value.compareAndSet(expect, update));
>>
>> The idea is to retrieve the cache line in the exclusive state to begin
>> with by writing to the cache line at the beginning of the loop (i.e.
>> m_nearby = 0).  This will reduce the number of bus operations.  I have
>> never tried this but it might perform better.
>>
>> If compute() is a long operation, an idea is to use transactional
>> memory.  If there is a conflict, then the transaction aborts immediately
>> and the hardware thread starts over immediately.  This way the hardware
>> thread does not waste time finishing the execution of compute() when the
>> store to m_value will definitely fail.
>>
>> beginTransaction();
>>
>> m_value= compute(m_value);
>>
>> commitTransaction();
>>
>> -Nathan
>
>
>>
>> On 1/28/2019 4:46 AM, Valentin Kovalenko via Concurrency-interest wrote:
>> > > there isn’t a universal backoff strategy that suits all platforms
>> > and cases
>> >
>>
> > It does not seem a problem to have different intrinsic backoff strategies
>> > for different platforms, but this, of course, can't be done for
>> different
>> > cases. However, since backoff only comes to play in case of a CAS
>> failure
>> > (or even multiple failures), it should neither affect throughput nor
>> > latency for a low contention scenario. But Francesco provided an
>
>
>> > explanation of how this still can negatively affect the performance
>> > (though, I, unfortunately, didn't understand it). Francesco, may I ask
>> you
>> > to explain the same thing for a less educated audience?:)
>> >
>> > > lock:xadd defers this to the hardware. The instruction has no “redo”
>> > - but of course internally at hardware level it probably is still a
>> > CAS-like loop with a retry - and a backoff (I think all consensus
>> > protocols have corner cases that can only be resolved through timing a
>> > race).
>> >
>>
> > So basically this is the same idea we usually use in programming: rely on
>> > the underlying layer's implementation if it is provided. And the
>> > underlying
>> > layer is hardware in this case.
>> >
>> > Valentin
>
>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/ac3b5bdc/attachment-0001.html
>> >
>>
>> ------------------------------
>>
>> Message: 5
>> Date: Mon, 28 Jan 2019 16:48:22 +0000
>> From: Gil Tene <gil at azul.com>
>> To: Alex Otenko <oleksandr.otenko at gmail.com>
>> Cc: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>,
>>         concurrency-interest <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] getAndIncrement with backoff
>> Message-ID: <9E185769-58E8-4D85-A49F-93720048F5ED at azul.com>
>> Content-Type: text/plain; charset="utf-8"
>
>
>>
>> The key difference is that, on architectures that support atomic
>> increment or atomic add instructions, getAndIncrement() can (and should) be
>> implemented as a wait-free construct. For various concurrent mechanisms
>> that would use this construct, this makes the difference between being wait
>> free and lock free. And that difference is generally much more important
>> and valuable than the (potentially
>> non-forward-progress-guaranteeing-for-each-thread) throughout benefit of
>> the primitive itself under highly contended situations.
>>
>> Note that e.g. x86’s lock xadd is wait free in implementation. But so is
>> the LOCK CAS instruction, and so are LL and SC instructions on any
>> processor that support them. AFAIK all user mode instructions are wait
>> free, by definition, and must complete in a small and reasonable bound as a
>> fundamental requirement of proper hardware design (this quality is easy to
>> deduce from basic expectations we have of all hardware with user mode
>> instructions: e.g. just think of the obvious system-hanging user-mode DoS
>> attack if any non-wait-free user mode instructions actually existed).
>>
>> Any looping or redo logic is always a pure software implementation
>> detail. Whether or not a given construct can be implemented to be wait free
>> depends on the wait-free primitives it has at its disposal (which in turn
>> depend on the capabilities of hardware instructions available).
>>
>> For the basic atomic constructs (like the various atomic operations
>> offered by the JDK for e.g. AtomicLong) the choice should always be to
>> implement the construct in a wait-free manner where possible. Wherever we
>> don’t, we hobble the ability of concurrent mechanisms written using those
>> constructs to be wait-free.
>>
>> Sent from my iPad
>>
>> On Jan 28, 2019, at 1:25 AM, Alex Otenko via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu<mailto:
>> concurrency-interest at cs.oswego.edu>> wrote:
>>
>> Last time I heard this discussed the conclusion was that there isn’t a
>> universal backoff strategy that suits all platforms and cases.
>>
>> For example, the logic of backoff used in this test case is good for
>> continuous atomic incrementing. Will the CPUs really be spinning in a
>> trivial increment loop? More likely you’ll slam the hot spot with multiple
>> CPUs at once, then the threads go away to do things, then come back.
>> Backing off the way it’s done in the test is not necessarily going to help
>> - the win comes from one thread being able to do several increments while
>> the others do not contend to do the same; but if the threads are not
>> attempting to do several increments, backoff is going to work like the
>> usual CAS loop.
>>
>> lock:xadd defers this to the hardware. The instruction has no “redo” -
>> but of course internally at hardware level it probably is still a CAS-like
>> loop with a retry - and a backoff (I think all consensus protocols have
>> corner cases that can only be resolved through timing a race).
>>
>> Alex
>>
>> On 28 Jan 2019, at 08:49, Valentin Kovalenko <
>> valentin.male.kovalenko at gmail.com<mailto:
>> valentin.male.kovalenko at gmail.com>> wrote:
>>
>> > The reason for not using it for backoff is because there is no backoff
>> in the intrinsic
>> So lock:xadd does not have backoff logic, may it be then a good idea to
>> not use this intrinsic and use "manual" getAndIncrement implementation with
>> backoff in JDK since it results in a significantly higher throughput? In
>> the mentioned article authors tested not only multiple architectures with
>> many CPU threads (not 4 like I do), but also CAS-based data structures, and
>> they achieved throughput improvements with backoff in all tested cases.
>>
>> > The reason for getting different results is possibly sharing the cache:
>> backoff allows one CPU to usurp the cache line and make progress; without
>> the backoff the line is always shared between all the CPUs.
>> Sure, since threads retry CAS less often once encounter failures, this
>> effectively reduces contention.
>>
>> Valentin
>>
>> On Mon, 28 Jan 2019 at 01:17, Alex Otenko <oleksandr.otenko at gmail.com
>> <mailto:oleksandr.otenko at gmail.com>> wrote:
>> The intrinsic probably compiles to lock:xadd on x86, which has the
>> getAndAdd semantics.
>>
>> The reason for not using it for backoff is because there is no backoff in
>> the intrinsic. The reason for getting different results is possibly sharing
>> the cache: backoff allows one CPU to usurp the cache line and make
>> progress; without the backoff the line is always shared between all the
>> CPUs.
>>
>>
>> Alex
>>
>> On 28 Jan 2019, at 07:29, Valentin Kovalenko via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu<mailto:
>> concurrency-interest at cs.oswego.edu>> wrote:
>>
>> Hi everyone,
>>
>> It seems to be common knowledge (see "Lightweight Contention Management
>> for E cient Compare-and-Swap Operations",
>> https://arxiv.org/pdf/1305.5800.pdf) that simple exponential backoff
>> drastically increases the throughput of CAS-based operations (e.g.
>> getAndIncrement). I checked this on my own, and yes, this is still true for
>> OpenJDK 11:
>> (the test and full measurements is available here
>> https://github.com/stIncMale/sandbox/blob/master/benchmarks/src/test/java/stincmale/sandbox/benchmarks/AtomicApiComparisonTest.java
>> )
>> [single CPU] 3.4 GHz Intel Core i5 (4 cores)
>> [OS] macOS 10.13.6 (17G4015)
>> [JDK] OpenJDK 11.0.1+13
>>
>> 1 thread                                Mode  Cnt    Score   Error  Units
>> atomicLongGetAndIncrement              thrpt   45  139.645 ± 1.383  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45  111.469 ± 0.665  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  111.545 ± 0.663  ops/us
>> 4 threads
>> atomicLongGetAndIncrement              thrpt   45   50.892 ± 0.131  ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.435 ± 0.162  ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45   92.659 ± 1.459  ops/us
>> 32 threads
>> atomicLongGetAndIncrement              thrpt   45   51.844 ±  0.803 ops/us
>> atomicLongGetAndIncrementManual        thrpt   45   12.495 ±  0.331 ops/us
>> atomicLongGetAndIncrementManualBackoff thrpt   45  157.771 ± 19.528 ops/us
>>
>> The fact that atomicLongGetAndIncrement is faster than
>> atomicLongGetAndIncrementManual for 1 thread tells me that as a result of
>> @HotSpotIntrinsicCandidate this method at runtime uses an implementation
>> different from what could have been compiled from Java sources, and this
>> different implementation has nothing to do with backoff because with 1
>> thread there are no CAS failures.
>>
>> What are the reasons behind the choice to not use backoff in OpenJDK's
>> AtomicLong.getAndIncrement?
>>
>> Regards,
>> Valentin
>>
> [LinkedIn]<https://www.linkedin.com/in/stIncMale>   [GitHub] <
>> https://github.com/stIncMale>    [YouTube] <
>> https://www.youtube.com/user/stIncMale>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu<mailto:
>> Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu<mailto:
>> Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/f692926e/attachment.html
>> >
>>
>> ------------------------------
>>
>> Subject: Digest Footer
>
>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> ------------------------------
>>
>> End of Concurrency-interest Digest, Vol 167, Issue 12
>> *****************************************************
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/bb6215f3/attachment-0001.html>

From aph at redhat.com  Mon Jan 28 12:50:24 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 17:50:24 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwKMncPZAzGoj3RuPY5pFxp2t5QGfnWH6rgfT2GHc6nmrw@mail.gmail.com>
References: <mailman.25.1548694105.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwKMncPZAzGoj3RuPY5pFxp2t5QGfnWH6rgfT2GHc6nmrw@mail.gmail.com>
Message-ID: <876da4c7-95c6-3702-3470-01ddee4c9e99@redhat.com>

On 1/28/19 5:40 PM, Valentin Kovalenko via Concurrency-interest wrote:

> This is, in fact, a plain JMH test :)

It's not! A *plain* JMH test is in a .java file and does not depend on a
bunch of libraries. So there!

Anyway, I got it working in the end. I had some problems with stuff
like javax.annotation nt being available, but I just commented that
out.

> It can be run by executing
> mvn clean test -f benchmarks/pom.xml -Dstincmale.sandbox.benchmarks.dryRun=false -Dtest=AtomicApiComparisonTest

I had no idea that maven could be used to run JMH tests. But that'd be
no use to me anyway, because I'm usually running them in a debugger. I
just use java -jar target/benchmarks.jar.

Did I ever tell you how much I hate Maven? Don't get me
started. Sheesh. ;-)

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From valentin.male.kovalenko at gmail.com  Mon Jan 28 13:13:05 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 11:13:05 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <D8C08F26-BA53-4F98-9CF6-954C47A42378@gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
 <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
 <D8C08F26-BA53-4F98-9CF6-954C47A42378@gmail.com>
Message-ID: <CAO-wXwK0ChmFhP8iFZAX=vnMhXzsLfvqq=4Vyf=o5WycJbXUGg@mail.gmail.com>

So, I would like to sum up all the ideas. Please, correct/complete me if I
missed something from the discussion.

JDK does not use backoff or any other logic which may sometimes increase
the throughput of CAS-based actions (e.g. getAndIncrement) because:

   1. Some hardware provides wait-free implementations, which means that
   once we saturate all CPU threads, the throughput of CAS will not degrade
   with the growth of software threads (Gil,
   http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016784.html;
   Nathan and Ila,
   http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016783.html
   )
   2. In many real cases, backoff or other logic reducing CAS contention
   will likely have no positive effect unless threads do almost nothing else
   but CAS-ing. And the overhead of trying to pick the best strategy for each
   situation is non-justifiable (Alex,
   http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016769.html
   ,
   http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016780.html
   )
   3. backoff logic introduces more possibilities for a thread to wait for
   safepoint poll thus potentially drastically increasing max latencies
   (Franc,
   http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016779.html
   )


Valentin

On Mon, 28 Jan 2019 at 05:52, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Well, you can take a look at the locks used for synchronized. The logic
> there is aiming at precisely the same cases: should it bias to one thread,
> should it be a spin-lock, should it inflate to a "fat lock" (OS-provided
> primitive). But each lock behaves differently, so the JVM makes different
> decisions for each individual lock.
>
> The atomic increment intrinsics will suffer from the same - each hot spot
> will require different treatment (but then you also have to come up with,
> and justify the cost of, added tracking for contention of individual atomic
> longs).
>
> Alex
>
> > On 28 Jan 2019, at 11:46, Valentin Kovalenko via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
> >
> > > there isn’t a universal backoff strategy that suits all platforms and
> cases
> >
> > It does not seem a problem to have different intrinsic backoff strategies
> > for different platforms, but this, of course, can't be done for different
> > cases. However, since backoff only comes to play in case of a CAS failure
> > (or even multiple failures), it should neither affect throughput nor
> > latency for a low contention scenario. But Francesco provided an
> > explanation of how this still can negatively affect the performance
> > (though, I, unfortunately, didn't understand it). Francesco, may I ask
> you
> > to explain the same thing for a less educated audience?:)
> >
> > > lock:xadd defers this to the hardware. The instruction has no “redo” -
> but of course internally at hardware level it probably is still a CAS-like
> loop with a retry - and a backoff (I think all consensus protocols have
> corner cases that can only be resolved through timing a race).
> >
> > So basically this is the same idea we usually use in programming: rely on
> > the underlying layer's implementation if it is provided. And the
> underlying
> > layer is hardware in this case.
> >
> > Valentin
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/756cae23/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon Jan 28 14:20:58 2019
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 28 Jan 2019 12:20:58 -0700
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <CAO-wXwK0ChmFhP8iFZAX=vnMhXzsLfvqq=4Vyf=o5WycJbXUGg@mail.gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
 <CAO-wXwKO=T8fL5BTtGw8=Ad_SqdzE31ydpaSY=hCniEJaZtB2A@mail.gmail.com>
 <D8C08F26-BA53-4F98-9CF6-954C47A42378@gmail.com>
 <CAO-wXwK0ChmFhP8iFZAX=vnMhXzsLfvqq=4Vyf=o5WycJbXUGg@mail.gmail.com>
Message-ID: <CAO-wXw+ERj-=0NAqDTxghZv+NPfTBqwR40dayfFn=GSO6BjOMQ@mail.gmail.com>

Sorry, the thing in the item 1 about throughput is true for both lock-free
and wait- free, the difference is in the max latencies: it is bound for a
wait-fee implementation.

On Mon, Jan 28, 2019, 11:13 Valentin Kovalenko <
valentin.male.kovalenko at gmail.com wrote:

> So, I would like to sum up all the ideas. Please, correct/complete me if I
> missed something from the discussion.
>
> JDK does not use backoff or any other logic which may sometimes increase
> the throughput of CAS-based actions (e.g. getAndIncrement) because:
>
>    1. Some hardware provides wait-free implementations, which means that
>    once we saturate all CPU threads, the throughput of CAS will not degrade
>    with the growth of software threads (Gil,
>    http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016784.html;
>    Nathan and Ila,
>    http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016783.html
>    )
>    2. In many real cases, backoff or other logic reducing CAS contention
>    will likely have no positive effect unless threads do almost nothing else
>    but CAS-ing. And the overhead of trying to pick the best strategy for each
>    situation is non-justifiable (Alex,
>    http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016769.html
>    ,
>    http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016780.html
>    )
>    3. backoff logic introduces more possibilities for a thread to wait
>    for safepoint poll thus potentially drastically increasing max latencies
>    (Franc,
>    http://cs.oswego.edu/pipermail/concurrency-interest/2019-January/016779.html
>    )
>
>
> Valentin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/6f2d2cdc/attachment.html>

From aph at redhat.com  Mon Jan 28 15:43:03 2019
From: aph at redhat.com (Andrew Haley)
Date: Mon, 28 Jan 2019 20:43:03 +0000
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 167,
 Issue 10
In-Reply-To: <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
References: <mailman.21.1548674452.1936.concurrency-interest@cs.oswego.edu>
 <CAO-wXwJx7vkaiZqdxB5e8e79691iPuVRMQxi89fphMqgs8Wi3Q@mail.gmail.com>
Message-ID: <1a6c4a5f-15f7-3720-184f-128eea5e13ff@redhat.com>

On 1/28/19 11:43 AM, Valentin Kovalenko via Concurrency-interest wrote:
>> there isn’t a universal backoff strategy that suits all platforms and cases

Here's one result:

Benchmark                                                       Mode  Cnt         Score          Error  Units
AtomicApiComparisonTest.atomicLongGetAndIncrement               avgt    5      1123.350 ±     1121.332  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManual         avgt    5       461.725 ±      370.705  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManualBackoff  avgt    5  27382411.415 ± 96039209.596  ns/op

If I had to guess, it'd be that the threads are backing off from each other in perfect
synchronization, and the CAS never completes.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From gil at azul.com  Mon Jan 28 17:44:44 2019
From: gil at azul.com (Gil Tene)
Date: Mon, 28 Jan 2019 22:44:44 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
Message-ID: <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>

Andrew, on an ARMv8.1A or later, will getAndIncrement() end up using LDADD variant (probably LDADDAL for the full required ordering semantics) instead of a CAS loop? If so, do the numbers for such cores look very different from those for a CAS-loop implementation?

— Gil.

> On Jan 28, 2019, at 9:24 AM, Andrew Haley <aph at redhat.com> wrote:
> 
> On 1/28/19 4:52 PM, Alex Otenko via Concurrency-interest wrote:
> 
>> I don’t know how hardware works, but at the finest level two cores
>> attempting to lock the bus, or cache line, or whatever the mechanism
>> to implement mutual exclusion, at that level one of the cores has to
>> wait.
> 
> Yes, I agree, but that's true whenever you write to memory. When memory
> is updated, only one core has the cache line in exclusive state: the
> others have read-only local copies. There's nothing special happening
> with something like a CAS in that regard. The cache coherency protocol
> handles it in the same way as usual.
> 
> However, there is an exception to this. With Arm's "far atomic"
> instructions, the intention is not to move cache lines around at
> all. AIUI, whichever core already a the cache line in exclusive
> state keeps it locally. The core issuing the far atomic op sends a
> message to whichever core owns the cache line asking for the op to be
> done remotely. The acknowledgment message contains the result. This
> avoids cache-line ping-ponging altogether, at least in theory. There
> will be some delay at times of high contention because processors will
> be waiting for replies, of course.
> 
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190128/adb0c4cc/attachment.sig>

From adinn at redhat.com  Tue Jan 29 04:24:39 2019
From: adinn at redhat.com (Andrew Dinn)
Date: Tue, 29 Jan 2019 09:24:39 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
Message-ID: <36f67d7c-051d-7396-4633-be3bf0841dfc@redhat.com>

Hi Gil,

On 28/01/2019 22:44, Gil Tene via Concurrency-interest wrote:
> Andrew, on an ARMv8.1A or later, will getAndIncrement() end up using
> LDADD variant (probably LDADDAL for the full required ordering
> semantics) instead of a CAS loop? If so, do the numbers for such
> cores look very different from those for a CAS-loop implementation?

Different Andrew here but yes it uses those instructions by default when
available.

If you want to see the details look for macro ATOMIC_OP defined in

  src/hotspot/cpu/aarch64/macroAdssembler.cpp

It tests flag UseLSE and selects either an atomic add or a CAS loop
accordingly.

And, of course, UseLSE is auto-configured in file

  src/hotspot/cpu/aarch64/vm_version_aarch64.cpp

according to the architecture (or command line override).

I don't have numbers to quantify any improvement but I recall Andrew
saying that the add instructions are a win.

regards,


Andrew Dinn
-----------
Senior Principal Software Engineer
Red Hat UK Ltd
Registered in England and Wales under Company Registration No. 03798903
Directors: Michael Cunningham, Michael ("Mike") O'Neill, Eric Shander

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190129/97f24a30/attachment.sig>

From aph at redhat.com  Tue Jan 29 04:33:04 2019
From: aph at redhat.com (Andrew Haley)
Date: Tue, 29 Jan 2019 09:33:04 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <36f67d7c-051d-7396-4633-be3bf0841dfc@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
 <36f67d7c-051d-7396-4633-be3bf0841dfc@redhat.com>
Message-ID: <e9d8eb52-06f8-032b-df61-1a099bf79819@redhat.com>

On 1/29/19 9:24 AM, Andrew Dinn wrote:
> I don't have numbers to quantify any improvement but I recall Andrew
> saying that the add instructions are a win.

They *can* be. But what can also happen is that there is additional
latency, precisely because after a CAS completes you don't have the
memory in your local cache. This is the property that far atomics were
designed for, but it's not always what you want. The most profound
advantages are those you'd see on a large network of cores such as the
Post-K supercomputer. It's these huge systems for which far atomics
were designed. Tragically, I don't have one. :-)

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From oleksandr.otenko at gmail.com  Tue Jan 29 04:37:46 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 29 Jan 2019 09:37:46 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
Message-ID: <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>

Thanks.

I was actually after a bolder statement.

Gil is saying CAS can be wait-free. But isn’t this statement made under assumption that all “threads” are making progress? That is, neither CPU can “hog” the ownership of a bus, or the state of a cache line, or whatever decides what the others are allowed to see. But is it fundamentally even lock-free? I don’t see how this is even possible, so I am posing these questions to understand what assumptions are made, and see if there is a lesson to learn on concurrent designs.

Like, if some core were to “hang” - suppose, hardware failure of some sort, or design flaw that makes it get stuck for many cycles - will the other cores be able to make progress and modify the location that the “hanging” core claimed to own and is failing to “release”.

It seems to me that it is possible to make progress only if we stop guaranteeing system-wide coherence - that is, we don’t really have even a system that guarantees CAS behaviour at *all* times - it does guarantee it at the times when the “threads” are “not hanging”. But that’s hardly a definition of lock-free (let alone wait-free), and there is nothing to learn from this system.

Alex

> On 28 Jan 2019, at 17:24, Andrew Haley <aph at redhat.com> wrote:
> 
> On 1/28/19 4:52 PM, Alex Otenko via Concurrency-interest wrote:
> 
>> I don’t know how hardware works, but at the finest level two cores
>> attempting to lock the bus, or cache line, or whatever the mechanism
>> to implement mutual exclusion, at that level one of the cores has to
>> wait.
> 
> Yes, I agree, but that's true whenever you write to memory. When memory
> is updated, only one core has the cache line in exclusive state: the
> others have read-only local copies. There's nothing special happening
> with something like a CAS in that regard. The cache coherency protocol
> handles it in the same way as usual.
> 
> However, there is an exception to this. With Arm's "far atomic"
> instructions, the intention is not to move cache lines around at
> all. AIUI, whichever core already a the cache line in exclusive
> state keeps it locally. The core issuing the far atomic op sends a
> message to whichever core owns the cache line asking for the op to be
> done remotely. The acknowledgment message contains the result. This
> avoids cache-line ping-ponging altogether, at least in theory. There
> will be some delay at times of high contention because processors will
> be waiting for replies, of course.
> 
> -- 
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671


From adinn at redhat.com  Tue Jan 29 04:47:44 2019
From: adinn at redhat.com (Andrew Dinn)
Date: Tue, 29 Jan 2019 09:47:44 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <e9d8eb52-06f8-032b-df61-1a099bf79819@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
 <36f67d7c-051d-7396-4633-be3bf0841dfc@redhat.com>
 <e9d8eb52-06f8-032b-df61-1a099bf79819@redhat.com>
Message-ID: <06bf74e9-bb80-294f-3cc7-26bd8eaadf44@redhat.com>

On 29/01/2019 09:33, Andrew Haley wrote:
> On 1/29/19 9:24 AM, Andrew Dinn wrote:
>> I don't have numbers to quantify any improvement but I recall Andrew
>> saying that the add instructions are a win.
> 
> They *can* be. But what can also happen is that there is additional
> latency, precisely because after a CAS completes you don't have the
> memory in your local cache. This is the property that far atomics were
> designed for, but it's not always what you want. The most profound
> advantages are those you'd see on a large network of cores such as the
> Post-K supercomputer. It's these huge systems for which far atomics
> were designed. Tragically, I don't have one. :-)
Oh, yes, the tragedy (of someone who has access to a wider range  of
AArch64 kit than almost anyone else on the planet ... ;-).

Anyway, I guess I should have known there was a reason for providing
that command line override for UseLSE.

regards,


Andrew Dinn
-----------
Senior Principal Software Engineer
Red Hat UK Ltd
Registered in England and Wales under Company Registration No. 03798903
Directors: Michael Cunningham, Michael ("Mike") O'Neill, Eric Shander

From aph at redhat.com  Tue Jan 29 05:01:07 2019
From: aph at redhat.com (Andrew Haley)
Date: Tue, 29 Jan 2019 10:01:07 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
Message-ID: <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>

On 1/29/19 9:37 AM, Alex Otenko wrote:

> Gil is saying CAS can be wait-free. But isn’t this statement made
> under assumption that all “threads” are making progress? That is,
> neither CPU can “hog” the ownership of a bus, or the state of a
> cache line, or whatever decides what the others are allowed to
> see.

Yes, exactly. That's how the system is designed to work. The duration in
which a core has exclusive access to a cache line is bounded, and this
is enforced by the hardware.

> But is it fundamentally even lock-free? I don’t see how this is even
> possible, so I am posing these questions to understand what
> assumptions are made, and see if there is a lesson to learn on
> concurrent designs.

Cores are designed to relinquish exclusive ownership after a
time. When another core requests exclusive ownership its request goes
into a queue, and it has to wait for an acknowledgment. Exactly how
the network handles this is secret sauce for the CPU manufacturers, of
course, but if we assume that requests are handled in a
first-come-first-served manner then the system is wait free.

> Like, if some core were to “hang” - suppose, hardware failure of
> some sort, or design flaw that makes it get stuck for many cycles -
> will the other cores be able to make progress and modify the
> location that the “hanging” core claimed to own and is failing to
> “release”.

Then the hardware is broken. You can't guarantee any of its
properties.

> It seems to me that it is possible to make progress only if we stop
> guaranteeing system-wide coherence - that is, we don’t really have
> even a system that guarantees CAS behaviour at *all* times - it does
> guarantee it at the times when the “threads” are “not hanging”.

It depends on what you man by "make progress": cores are waiting all
the time, for other cores or for main memory. This can't be avoided in
a distributed memory system, which all of today's large computers are.

> But that’s hardly a definition of lock-free (let alone wait-free),
> and there is nothing to learn from this system.

Umm, what? Why not?

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From oleksandr.otenko at gmail.com  Tue Jan 29 05:19:47 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 29 Jan 2019 10:19:47 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
 <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>
Message-ID: <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>

Well, I find it is self-confirming: “you are not allowed to suspend any threads; other than that it is wait-free”


Alex

> On 29 Jan 2019, at 10:01, Andrew Haley <aph at redhat.com> wrote:
> 
> On 1/29/19 9:37 AM, Alex Otenko wrote:
> 
>> Gil is saying CAS can be wait-free. But isn’t this statement made
>> under assumption that all “threads” are making progress? That is,
>> neither CPU can “hog” the ownership of a bus, or the state of a
>> cache line, or whatever decides what the others are allowed to
>> see.
> 
> Yes, exactly. That's how the system is designed to work. The duration in
> which a core has exclusive access to a cache line is bounded, and this
> is enforced by the hardware.
> 
>> But is it fundamentally even lock-free? I don’t see how this is even
>> possible, so I am posing these questions to understand what
>> assumptions are made, and see if there is a lesson to learn on
>> concurrent designs.
> 
> Cores are designed to relinquish exclusive ownership after a
> time. When another core requests exclusive ownership its request goes
> into a queue, and it has to wait for an acknowledgment. Exactly how
> the network handles this is secret sauce for the CPU manufacturers, of
> course, but if we assume that requests are handled in a
> first-come-first-served manner then the system is wait free.
> 
>> Like, if some core were to “hang” - suppose, hardware failure of
>> some sort, or design flaw that makes it get stuck for many cycles -
>> will the other cores be able to make progress and modify the
>> location that the “hanging” core claimed to own and is failing to
>> “release”.
> 
> Then the hardware is broken. You can't guarantee any of its
> properties.
> 
>> It seems to me that it is possible to make progress only if we stop
>> guaranteeing system-wide coherence - that is, we don’t really have
>> even a system that guarantees CAS behaviour at *all* times - it does
>> guarantee it at the times when the “threads” are “not hanging”.
> 
> It depends on what you man by "make progress": cores are waiting all
> the time, for other cores or for main memory. This can't be avoided in
> a distributed memory system, which all of today's large computers are.
> 
>> But that’s hardly a definition of lock-free (let alone wait-free),
>> and there is nothing to learn from this system.
> 
> Umm, what? Why not?
> 
> -- 
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671


From nigro.fra at gmail.com  Tue Jan 29 05:22:35 2019
From: nigro.fra at gmail.com (Francesco Nigro)
Date: Tue, 29 Jan 2019 11:22:35 +0100
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
 <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>
 <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>
Message-ID: <CAKxGtTXdz9fpM5R9_Ox_JaXnJffZBt53FpvVnbNr=4jDFNwuRg@mail.gmail.com>

I often use these definitions:
http://concurrencyfreaks.blogspot.com/2013/05/lock-free-and-wait-free-definition-and.html
Consider that are less pragmatic and abstract away how the HW should/would
ensure them, but I found them useful...

Il giorno mar 29 gen 2019 alle ore 11:20 Alex Otenko via
Concurrency-interest <concurrency-interest at cs.oswego.edu> ha scritto:

> Well, I find it is self-confirming: “you are not allowed to suspend any
> threads; other than that it is wait-free”
>
>
> Alex
>
> > On 29 Jan 2019, at 10:01, Andrew Haley <aph at redhat.com> wrote:
> >
> > On 1/29/19 9:37 AM, Alex Otenko wrote:
> >
> >> Gil is saying CAS can be wait-free. But isn’t this statement made
> >> under assumption that all “threads” are making progress? That is,
> >> neither CPU can “hog” the ownership of a bus, or the state of a
> >> cache line, or whatever decides what the others are allowed to
> >> see.
> >
> > Yes, exactly. That's how the system is designed to work. The duration in
> > which a core has exclusive access to a cache line is bounded, and this
> > is enforced by the hardware.
> >
> >> But is it fundamentally even lock-free? I don’t see how this is even
> >> possible, so I am posing these questions to understand what
> >> assumptions are made, and see if there is a lesson to learn on
> >> concurrent designs.
> >
> > Cores are designed to relinquish exclusive ownership after a
> > time. When another core requests exclusive ownership its request goes
> > into a queue, and it has to wait for an acknowledgment. Exactly how
> > the network handles this is secret sauce for the CPU manufacturers, of
> > course, but if we assume that requests are handled in a
> > first-come-first-served manner then the system is wait free.
> >
> >> Like, if some core were to “hang” - suppose, hardware failure of
> >> some sort, or design flaw that makes it get stuck for many cycles -
> >> will the other cores be able to make progress and modify the
> >> location that the “hanging” core claimed to own and is failing to
> >> “release”.
> >
> > Then the hardware is broken. You can't guarantee any of its
> > properties.
> >
> >> It seems to me that it is possible to make progress only if we stop
> >> guaranteeing system-wide coherence - that is, we don’t really have
> >> even a system that guarantees CAS behaviour at *all* times - it does
> >> guarantee it at the times when the “threads” are “not hanging”.
> >
> > It depends on what you man by "make progress": cores are waiting all
> > the time, for other cores or for main memory. This can't be avoided in
> > a distributed memory system, which all of today's large computers are.
> >
> >> But that’s hardly a definition of lock-free (let alone wait-free),
> >> and there is nothing to learn from this system.
> >
> > Umm, what? Why not?
> >
> > --
> > Andrew Haley
> > Java Platform Lead Engineer
> > Red Hat UK Ltd. <https://www.redhat.com>
> > EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190129/c9f1d22c/attachment.html>

From aph at redhat.com  Tue Jan 29 05:26:16 2019
From: aph at redhat.com (Andrew Haley)
Date: Tue, 29 Jan 2019 10:26:16 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
 <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>
 <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>
Message-ID: <71cfa502-fce0-52fe-073e-313e3a655eb1@redhat.com>

On 1/29/19 10:19 AM, Alex Otenko wrote:

> Well, I find it is self-confirming: “you are not allowed to suspend
> any threads; other than that it is wait-free”

I'm sorry, this statement is beyond my understanding. What are you
trying to say?

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From oleksandr.otenko at gmail.com  Tue Jan 29 05:35:22 2019
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 29 Jan 2019 10:35:22 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <71cfa502-fce0-52fe-073e-313e3a655eb1@redhat.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
 <56fe0145-0faf-3713-ef71-ffbfb4d18a8f@redhat.com>
 <93F0C147-2C9A-4E12-916F-503DFF693304@gmail.com>
 <71cfa502-fce0-52fe-073e-313e3a655eb1@redhat.com>
Message-ID: <E5B966D3-2335-41F0-B7FB-E0FB43B97C71@gmail.com>

Lock freedom: suspending any thread does not preclude progress.

With hardware we first say that we are not allowed to suspend threads (CPU stands for threads; here we say that a system with suspended threads is a broken system), then we say that it’s lock-free (or even stronger - wait-free).


Alex

> On 29 Jan 2019, at 10:26, Andrew Haley <aph at redhat.com> wrote:
> 
> On 1/29/19 10:19 AM, Alex Otenko wrote:
> 
>> Well, I find it is self-confirming: “you are not allowed to suspend
>> any threads; other than that it is wait-free”
> 
> I'm sorry, this statement is beyond my understanding. What are you
> trying to say?
> 
> -- 
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671


From gil at azul.com  Tue Jan 29 06:12:35 2019
From: gil at azul.com (Gil Tene)
Date: Tue, 29 Jan 2019 11:12:35 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <FAFC89EA-797C-4C9F-86E6-800C03F92359@gmail.com>
Message-ID: <4A241C11-F07A-4017-B6AB-DDD0FF7E1D43@azul.com>



> On Jan 29, 2019, at 1:37 AM, Alex Otenko <oleksandr.otenko at gmail.com> wrote:
> 
> Thanks.
> 
> I was actually after a bolder statement.
> 
> Gil is saying CAS can be wait-free. But isn’t this statement made under assumption that all “threads” are making progress? That is, neither CPU can “hog” the ownership of a bus, or the state of a cache line, or whatever decides what the others are allowed to see. But is it fundamentally even lock-free? I don’t see how this is even possible, so I am posing these questions to understand what assumptions are made, and see if there is a lesson to learn on concurrent designs.

CAS is always wait-free, or it is a hardware design bug. Hardware designs have the benefit of having known absolute bounds to timing (of e.g. bus cycles, or message latencies, etc.) and number of elements (e.g. cores, caches, coherency coordination points, etc.), and will generally use various techniques to guarantee forward progress of instruction execution to all cores in a system, and an absolute (and usually very reasonable) bound on the time it can take to execute any one instruction (including e.g. a cache missing, cross-NUMA-node memory-accessing instruction that happens to address a cache line currently owned by some other arbitrary core's cache, or accesses a memory location that, upon reading, finds that it needs to perform an ECC correction). Having actually tackled the question of whether or not CAS instructions must be guaranteed forward progress (for the CAS, not for the loops that try to make them succeed) on all cores of a real-world 864-way SMP design, and considered the various terrible implications of succumbing to the temptation of not doing so, I can venture to guess that no hardware team designing multi-processors will be allowed to skimp on that guarantee. Hardware designers tend to do quite a bit to both prove and verify such guarantees.

Don't confuse the notion of a core temporarily "locking" a bus or temporarily preventing a coherency protocol from making forward progress with the "lock" referred to in "lock-free" lingo. While those individual underlying things preventing others from using the bus or the coherency protocol from making forward process are sometimes casually referred to as "locks" in hardware-lingo, they are "locks" whose acquisition time by any individual element is completely bound by the system hardware design, This makes them wait-free elements in software-lingo terms.

> 
> Like, if some core were to “hang” - suppose, hardware failure of some sort, or design flaw that makes it get stuck for many cycles - will the other cores be able to make progress and modify the location that the “hanging” core claimed to own and is failing to “release”.

The hanging case due to hardware failure is "uninteresting" and out of scope. I can also hang things by pulling out the power cords. How would you protect against that?

A design flaw is just that - a design flaw. Any non-wait-free user mode instruction implementation would represent a fundamental hardware design flaw. And dealing with hardware design flaws is also out of scope. If you don't plan on dealing with hardware that occasionally computes 1+1=3, you also don't need to deal with hardware that executes any user mode instructions that are not wait-free.

> It seems to me that it is possible to make progress only if we stop guaranteeing system-wide coherence - that is, we don’t really have even a system that guarantees CAS behaviour at *all* times - it does guarantee it at the times when the “threads” are “not hanging”. But that’s hardly a definition of lock-free (let alone wait-free), and there is nothing to learn from this system.

Coherent systems fail together, have known bounds on the number of things (cores, caches, links, etc.). In the small, they get to use physical things like synchronized clocks and PLLs, and in the large they have the ability to send signals across links in a way that can be reliably parsed on the other side without fear of loss, corruption, out-of-order-delivery, or unbounded delay (when those qualities or clocking or reliable links goes away, the system simply fails). I think that this is where your thinking diverges. Consensus-based approaches usually rely on redundant state, are often meant to deal with (and work in the presence) of partial failure, and usually cannot rely on synchronized clock edges or reliable links. This usually means that they avoid the simple bounding benefits that coherent system designs can usually rely on.

Generally speaking, any coherent memory system is also a fail-together system. If any coherent part of the system fails completely, the system as a whole will likely fail as well, and attempts to make it survive with unbounded workarounds are "uninteresting". E.g. in most coherent multiprocessor systems where cache-to-cache transfers are supported (without the transferred contents being written to memory), one L1 cache can fail while holding the only valid copy of a memory location that was previously computed and modified by several other processors. There is simply no coming back from such a failure. At least not for anything needing the state that was lost.

Once you buy into the "if one thing completely fails, everything fails" notion that pretty much all coherent memory systems buy into, you can now take advantage of design-enabling shortcuts, including common synchronized clocking, hardware latches, fail-together coherence protocols, and being able to completely bound the execution time of any instruction in the system.

> 
> Alex
> 
>> On 28 Jan 2019, at 17:24, Andrew Haley <aph at redhat.com> wrote:
>> 
>> On 1/28/19 4:52 PM, Alex Otenko via Concurrency-interest wrote:
>> 
>>> I don’t know how hardware works, but at the finest level two cores
>>> attempting to lock the bus, or cache line, or whatever the mechanism
>>> to implement mutual exclusion, at that level one of the cores has to
>>> wait.
>> 
>> Yes, I agree, but that's true whenever you write to memory. When memory
>> is updated, only one core has the cache line in exclusive state: the
>> others have read-only local copies. There's nothing special happening
>> with something like a CAS in that regard. The cache coherency protocol
>> handles it in the same way as usual.
>> 
>> However, there is an exception to this. With Arm's "far atomic"
>> instructions, the intention is not to move cache lines around at
>> all. AIUI, whichever core already a the cache line in exclusive
>> state keeps it locally. The core issuing the far atomic op sends a
>> message to whichever core owns the cache line asking for the op to be
>> done remotely. The acknowledgment message contains the result. This
>> avoids cache-line ping-ponging altogether, at least in theory. There
>> will be some delay at times of high contention because processors will
>> be waiting for replies, of course.
>> 
>> --
>> Andrew Haley
>> Java Platform Lead Engineer
>> Red Hat UK Ltd. <https://www.redhat.com>
>> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: Message signed with OpenPGP
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20190129/48ac1738/attachment.sig>

From aph at redhat.com  Tue Jan 29 09:31:05 2019
From: aph at redhat.com (Andrew Haley)
Date: Tue, 29 Jan 2019 14:31:05 +0000
Subject: [concurrency-interest] getAndIncrement with backoff
In-Reply-To: <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
References: <CAO-wXwK1+9f3+VWAU2dqtT+yZLEVtrTSqaE-dHhMi=0YiTokEw@mail.gmail.com>
 <42A5124C-8041-4F0E-8FC3-7C509C7A7153@gmail.com>
 <CAO-wXw+rpxkDNXc2jtzEJa6XkwAXOJPDiooJOSzt5T4ehTafCg@mail.gmail.com>
 <835EF32F-AE4D-47F0-9CA8-7DB4F0C6C47C@gmail.com>
 <9E185769-58E8-4D85-A49F-93720048F5ED@azul.com>
 <DABCD4DA-7E3F-44C4-B18B-5F330AD1D85B@gmail.com>
 <eafc5251-617d-4274-2e0a-7bc5e7103f26@redhat.com>
 <22AA4BBA-B227-4754-ABB4-37A8F0034CF0@azul.com>
Message-ID: <b87f6bbb-578e-11e5-a27c-ff9b9f8c9499@redhat.com>

On 1/28/19 10:44 PM, Gil Tene wrote:

> Andrew, on an ARMv8.1A or later, will getAndIncrement() end up using
> LDADD variant (probably LDADDAL for the full required ordering
> semantics) instead of a CAS loop? If so, do the numbers for such
> cores look very different from those for a CAS-loop implementation?

Yes, spectacularly so.

With LSE instructions:

Benchmark                                                       Mode  Cnt      Score      Error  Units
AtomicApiComparisonTest.atomicLongGetAndIncrement               avgt    5   2433.577 ±  146.355  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManual         avgt    5  89115.687 ± 5460.361  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManualBackoff  avgt    5   3607.760 ±  453.339  ns/op

With CAS:

Benchmark                                                       Mode  Cnt      Score      Error  Units
AtomicApiComparisonTest.atomicLongGetAndIncrement               avgt    5  39522.028 ±  342.643  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManual         avgt    5  65589.301 ± 2958.604  ns/op
AtomicApiComparisonTest.atomicLongGetAndIncrementManualBackoff  avgt    5   2270.836 ±  546.710  ns/op

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

