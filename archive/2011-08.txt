From mohanr at fss.co.in  Mon Aug  1 01:24:53 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Mon, 1 Aug 2011 10:54:53 +0530
Subject: [concurrency-interest] LongAdder (was: StripedAdder)
	andLongAdderTable
In-Reply-To: <CACyP5Pdc-5b32bfpCBjCRC=oF=-4AXo6j9mpmL4xZrVpO+yGkg@mail.gmail.com>
Message-ID: <025153D79DA9884ABDF85638747FD49712E829@fssbemail.fss.india>

Hi,

 

Hope this question doesn't divert the thread because it is too
simplistic. In this reply the reference to cores is only to highlight
the concurrency contention effect and how it is addressed. Right ? 

What is the difference in effect here between multiple threads in the
same CPU and multiple threads in different cores ? Is this one of the
key things I need to understand to start using the JSE 7 concurrency
utilities and FJ properly ?

 

>LongAdder vastly outperforms
>AtomicLong (sometimes by more than a factor of 100)
>when all cores are trying to update.  (Please let us know if
>you see different or surprising results.)



 

 

Thanks,

Mohan

 

________________________________

From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of
Christian Vest Hansen
Sent: Sunday, July 31, 2011 10:09 PM
To: Doug Lea
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LongAdder (was: StripedAdder)
andLongAdderTable

 

Some interfaces for these things might be a good idea, as I can imagine
data grid libraries might want to provide distributed implementations.
Counter and CounterTable comes to mind as possible names.

On Sun, Jul 31, 2011 at 16:55, Doug Lea <dl at cs.oswego.edu> wrote:

It is much nicer not to have to decide ahead of time that
a program will have enough threads updating a variable
to need a class that can adaptively spread out contention.
So a reworked version of StripedAdder is now known as "LongAdder".
It is a variant of AtomicLong with a stripped-down API (suitable
only for adding and counting), that expands to use striping
when needed while avoiding noticeable overhead when not needed.

As the cost of contended atomic operations increases
(even as the cost of uncontended ones decreases on
some platforms), this will probably become a more commonly
useful class.

If you are curious, try out LongAdderDemo in the jsr166e tests.
On machines with more than a few cores LongAdder vastly outperforms
AtomicLong (sometimes by more than a factor of 100)
when all cores are trying to update.  (Please let us know if
you see different or surprising results.)

This class is currently in jsr166e but is targeted to
go into java.util.concurrent.atomic. APIs and implementations
are still subject to change.

Also, because it is likely to be among the more
common uses of LongAdders, we created AtomicLongTable,
that maintains a hash map of adders with arbitrary keys,
supporting usages including table.increment(key)

Links:
*  API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar
(compiled using Java7 javac).
* Browsable CVS sources:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/
* Browsable CVS test file sources:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/jsr166e/
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/1319398c/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon Aug  1 03:31:56 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 1 Aug 2011 17:31:56 +1000
Subject: [concurrency-interest] LongAdder (was:
	StripedAdder)andLongAdderTable
In-Reply-To: <025153D79DA9884ABDF85638747FD49712E829@fssbemail.fss.india>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEAFIPAA.davidcholmes@aapt.net.au>

Hi Mohan,

Yes this is only to highlight the contention effects that concurrent updates
on multiple cores can produce.

Multiple threads on the same CPU/core can never contend with each other at
the level of the atomic instruction. A thread can get a failed CAS of
course, because it was context switched out and another thread updated the
value, but that is "contention" at the logical level, not at the hardware
level.

Performance under contention is a key consideration in concurrent programs,
but the goal should always be to remove the contention, not just to find
mechanisms that perform better under contention.

David Holmes

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohan
Radhakrishnan
Sent: Monday, 1 August 2011 3:25 PM
To: concurrency-interest
Subject: Re: [concurrency-interest] LongAdder (was:
StripedAdder)andLongAdderTable


  Hi,



  Hope this question doesn't divert the thread because it is too simplistic.
In this reply the reference to cores is only to highlight the concurrency
contention effect and how it is addressed. Right ?

  What is the difference in effect here between multiple threads in the same
CPU and multiple threads in different cores ? Is this one of the key things
I need to understand to start using the JSE 7 concurrency utilities and FJ
properly ?



  >LongAdder vastly outperforms
  >AtomicLong (sometimes by more than a factor of 100)
  >when all cores are trying to update.  (Please let us know if
  >you see different or surprising results.)







  Thanks,

  Mohan




----------------------------------------------------------------------------
--

  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Christian
Vest Hansen
  Sent: Sunday, July 31, 2011 10:09 PM
  To: Doug Lea
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] LongAdder (was: StripedAdder)
andLongAdderTable



  Some interfaces for these things might be a good idea, as I can imagine
data grid libraries might want to provide distributed implementations.
Counter and CounterTable comes to mind as possible names.

  On Sun, Jul 31, 2011 at 16:55, Doug Lea <dl at cs.oswego.edu> wrote:

  It is much nicer not to have to decide ahead of time that
  a program will have enough threads updating a variable
  to need a class that can adaptively spread out contention.
  So a reworked version of StripedAdder is now known as "LongAdder".
  It is a variant of AtomicLong with a stripped-down API (suitable
  only for adding and counting), that expands to use striping
  when needed while avoiding noticeable overhead when not needed.

  As the cost of contended atomic operations increases
  (even as the cost of uncontended ones decreases on
  some platforms), this will probably become a more commonly
  useful class.

  If you are curious, try out LongAdderDemo in the jsr166e tests.
  On machines with more than a few cores LongAdder vastly outperforms
  AtomicLong (sometimes by more than a factor of 100)
  when all cores are trying to update.  (Please let us know if
  you see different or surprising results.)

  This class is currently in jsr166e but is targeted to
  go into java.util.concurrent.atomic. APIs and implementations
  are still subject to change.

  Also, because it is likely to be among the more
  common uses of LongAdders, we created AtomicLongTable,
  that maintains a hash map of adders with arbitrary keys,
  supporting usages including table.increment(key)

  Links:
  *  API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
  * jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled
using Java7 javac).
  * Browsable CVS sources:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/
  * Browsable CVS test file sources:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/jsr166e/
  _______________________________________________
  Concurrency-interest mailing list
  Concurrency-interest at cs.oswego.edu
  http://cs.oswego.edu/mailman/listinfo/concurrency-interest




  --
  Venlig hilsen / Kind regards,
  Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/c2dc4829/attachment.html>

From yechielf at gigaspaces.com  Mon Aug  1 05:00:51 2011
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Mon, 1 Aug 2011 02:00:51 -0700
Subject: [concurrency-interest] a question regarding SeqeuenceLock
Message-ID: <F5A3854EDBB4D440A098DEF1EFFC922B08403356@IE2RD2XVS661.red002.local>

In the api doc it is mentioned that awaitAvailability api should be used (as demonstrated in the Point class) in conjunction with reading volatile variables (in the Point class it is the x and y vars)
Why the need for volatile ? the sequence # is a volatile (barrier) tested by awaitAvailability and updated by unlock() , so x and y will be visible without being volatile

Regrds,
Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/785bbdcb/attachment.html>

From dl at cs.oswego.edu  Mon Aug  1 06:42:13 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 01 Aug 2011 06:42:13 -0400
Subject: [concurrency-interest] a question regarding SeqeuenceLock
In-Reply-To: <F5A3854EDBB4D440A098DEF1EFFC922B08403356@IE2RD2XVS661.red002.local>
References: <F5A3854EDBB4D440A098DEF1EFFC922B08403356@IE2RD2XVS661.red002.local>
Message-ID: <4E368305.4050905@cs.oswego.edu>

On 08/01/11 05:00, Yechiel Feffer wrote:
> In the api doc it is mentioned that awaitAvailability api should be used (as
> demonstrated in the Point class) in conjunction with reading volatile variables
> (in the Point class it is the x and y vars)
>
> Why the need for volatile ? the sequence # is a volatile (barrier) tested by
> awaitAvailability and updated by unlock() , so x and y will be visible without
> being volatile

There are a few reasons for placing this advice in the javadocs.
One is that if the fields are long or double, they should be
volatile to ensure atomicity. Also, the "roach motel"
properties of the JMM/JLS specs allow non-volatile reads
to be reordered below volatile reads in some cases. If you
are an expert in the subtleties of the JMM, you may be able
to work around these issues without always only reading
volatiles or values dependent on those reads. But given that
SequenceLocks are mainly applicable when reads overwhelm writes,
and that volatile reads are normally much cheaper than volatile
writes, there's rarely any reason not to follow this advice.
For example, some tests of the sample jsr166e.extra ReadMostlyVector
class showed no consistent performance differences using volatile
vs non-volatile internal fields across Intel, AMD, and
Sparc multiprocessor test machines.

-Doug

From hans.boehm at hp.com  Mon Aug  1 12:30:21 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 1 Aug 2011 17:30:21 +0100
Subject: [concurrency-interest] a question regarding SeqeuenceLock
In-Reply-To: <4E368305.4050905@cs.oswego.edu>
References: <F5A3854EDBB4D440A098DEF1EFFC922B08403356@IE2RD2XVS661.red002.local>
	<4E368305.4050905@cs.oswego.edu>
Message-ID: <238A96A773B3934685A7269CC8A8D04273CBF597BC@GVW0436EXB.americas.hpqcorp.net>

The use of volatile should also remind you that these are potentially racing reads, and you get very few guarantees about the consistency of the values you see.  You cannot call an arbitrary read-only method inside such a read-only critical, since it can see inconsistent state.  You will eventually discover that there was a race, and you need to retry.  But there are likely to be subtleties involved in guaranteeing that nothing crashes in the meantime.  SequenceLocks are really very strange beasts.  I view them as very much an experts-only facility, though potentially a useful one.

I also think that Doug is a bit optimistic about working around the volatile restriction in a way that is not dependent on the JVM implementation.  We had a prior discussion of this, and it does appear to be mostly safe given current standard JVM implementations, but that's largely accidental.  Fundamentally, the roach motel guarantees do not require that non-volatile reads in a read-only critical section become visible before the end of the critical section.  Using non-volatile reads here means that your code has data races, implying that you need to reason about the full Java memory model (which we don't actually know how to do), and you are leaving the sequentially consistent subset.  It also means that running a data race detector on your code will become more painful.

Hans


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
> Sent: Monday, August 01, 2011 3:42 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] a question regarding SeqeuenceLock
> 
> On 08/01/11 05:00, Yechiel Feffer wrote:
> > In the api doc it is mentioned that awaitAvailability api should be
> used (as
> > demonstrated in the Point class) in conjunction with reading volatile
> variables
> > (in the Point class it is the x and y vars)
> >
> > Why the need for volatile ? the sequence # is a volatile (barrier)
> tested by
> > awaitAvailability and updated by unlock() , so x and y will be
> visible without
> > being volatile
> 
> There are a few reasons for placing this advice in the javadocs.
> One is that if the fields are long or double, they should be
> volatile to ensure atomicity. Also, the "roach motel"
> properties of the JMM/JLS specs allow non-volatile reads
> to be reordered below volatile reads in some cases. If you
> are an expert in the subtleties of the JMM, you may be able
> to work around these issues without always only reading
> volatiles or values dependent on those reads. But given that
> SequenceLocks are mainly applicable when reads overwhelm writes,
> and that volatile reads are normally much cheaper than volatile
> writes, there's rarely any reason not to follow this advice.
> For example, some tests of the sample jsr166e.extra ReadMostlyVector
> class showed no consistent performance differences using volatile
> vs non-volatile internal fields across Intel, AMD, and
> Sparc multiprocessor test machines.
> 
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From kimchy at gmail.com  Mon Aug  1 14:54:36 2011
From: kimchy at gmail.com (Shay Banon)
Date: Mon, 1 Aug 2011 21:54:36 +0300
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com> <4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
Message-ID: <CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>

Hi,

  I find the solution to change to volatile fields problematic. In my case,
I use the mutable version of the DateTime to have UTC based timestamps
converted automatically to relevant timezone and rounding, over a very large
collection. Having volatile field means heavy writing and then reading of
those values, which will really hurt perf. Here is an example of what I do:

long[] timestamps = ... //very large dataset of timestamps
MutableDateTime time = ..
for (int i = 0; i < timestamps.length; i++) {
     time.setMillis(timestamp[i]);
     ....
     time.getMillis();
}

I think changing to volatile will hurt many more programs out there than one
might think...

I would even say that the immutable values will almost always have to go
through a memory barrier of some sort when "transfered" to another thread,
so in practice the volatile solution will break more code out there (by it
performing badly). I think if this can't be solved in the proper way (finals
on immutable, non volatile on mutable, and hack serialization), then my vote
is to break serialization, and if not, then do nothing and make a note in
the docs regarding the immutable values.

-shay.banon

p.s. Sorry if this discussion has diverged to pure Joda, will be happy to
continue it on the Joda mailing list if it makes sense

On Tue, Jun 21, 2011 at 2:49 PM, Stephen Colebourne <scolebourne at joda.org>wrote:

> On 21 June 2011 09:11, Mark Thornton <mthornton at optrak.com> wrote:
> > This should be backward compatible with existing serialized instances
> while
> > allowing the fields in BaseDateTime to be final. It comes at the expense
> of
> > a slightly larger MutableDateTime object, and slightly slower
> > deserialization.
>
> In the end, I think Doug's advice is probably right. Just use
> volatile. I don't think its ideal, but it "Just Works", which is a
> good thing in this codebase.
>
> Mark, I think your change would work (and I like the neat set before
> defaultReadFields). However it would have required any mutable
> application subclass to be changed, which I was hoping to avoid.
> Compared to the volatile approach I decided simple is best.
>
>
> https://github.com/JodaOrg/joda-time/commit/67f1a30fc0fceda751b6347b2b16a5081878ac1e
>
> Another variation which would also appear to work would be to change
> the data storage in BaseDateTime to a transient final two element
> array and use ObjectStreamField to read and write the existing
> serialized format. (The final array would presumably keep immutability
> happy, while the array settability even when final would keep the
> mutable classes happy). But the complexity was considerably greater,
> so volatile won (unless someone screams that it kills their
> performance).
>
> Thanks everyone for your thoughts.
>
> Stephen
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/277b7bf8/attachment.html>

From ashwin.jayaprakash at gmail.com  Mon Aug  1 16:06:29 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 1 Aug 2011 13:06:29 -0700
Subject: [concurrency-interest] j.u.c.Phaser tutorial?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEABIPAA.davidcholmes@aapt.net.au>
References: <CAF9YjSDjbg9=0L1kUUQGf2uhxkMa6NVrKasBSo9zLrUE5=waXg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEABIPAA.davidcholmes@aapt.net.au>
Message-ID: <CAF9YjSDPP2a9b3tt7rfy1EenjNGugwjxoceuNN37Rd=kn9VNpQ@mail.gmail.com>

Why doesn't the main thread just call arrive() at the end?  I guess only for
clarity - to show that only x worker threads (minus 1 starter thread) are
required for the processing.


On Sun, Jul 31, 2011 at 3:47 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Ashwin,
>
> > I learned later that the arriveAndDeregister() method makes sense only
> when you create a hierarchy of phasers
>
> Not so. arriveAndDeregister is typically used by the main thread when the
> phaser acts as both a cyclic barrier and a starting latch. You create the
> phaser for N+1 threads and have all workers wait on the phaser in a loop.
> The main thread is needed to release the phaser the first time so that all
> workers have been created but thereafter is not needed so it can arrive and
> deregister.
>
> David Holmes
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/346214bf/attachment.html>

From davidcholmes at aapt.net.au  Mon Aug  1 16:27:57 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 2 Aug 2011 06:27:57 +1000
Subject: [concurrency-interest] j.u.c.Phaser tutorial?
In-Reply-To: <CAF9YjSDPP2a9b3tt7rfy1EenjNGugwjxoceuNN37Rd=kn9VNpQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEAIIPAA.davidcholmes@aapt.net.au>

Not sure what you mean by "the end". The main thread calls
arriveAndDeregister to allow the first release of the barrier to occur while
at the same time removing itself from future cycles of the barrier (so the
main processing loops only require the N worker threads to meet at the
barrier). If the main thread didn't deregister then the N worker threads
would block at the barrier waiting for a non-existent N+1 thread to turn up.

David
  -----Original Message-----
  From: Ashwin Jayaprakash [mailto:ashwin.jayaprakash at gmail.com]
  Sent: Tuesday, 2 August 2011 6:06 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] j.u.c.Phaser tutorial?


  Why doesn't the main thread just call arrive() at the end?  I guess only
for clarity - to show that only x worker threads (minus 1 starter thread)
are required for the processing.



  On Sun, Jul 31, 2011 at 3:47 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Ashwin,

    > I learned later that the arriveAndDeregister() method makes sense only
when you create a hierarchy of phasers

    Not so. arriveAndDeregister is typically used by the main thread when
the phaser acts as both a cyclic barrier and a starting latch. You create
the phaser for N+1 threads and have all workers wait on the phaser in a
loop. The main thread is needed to release the phaser the first time so that
all workers have been created but thereafter is not needed so it can arrive
and deregister.

    David Holmes



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110802/59aa4f19/attachment.html>

From scolebourne at joda.org  Mon Aug  1 16:29:04 2011
From: scolebourne at joda.org (Stephen Colebourne)
Date: Mon, 1 Aug 2011 21:29:04 +0100
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com> <4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
Message-ID: <CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>

I'd be interested in any real world benchmark that indicates a
significant slow down at the application level (not a micro
benchmark).

However, this thread didn't exactly give me any better options in line
with compatibility (esp. now 2.0 is released)

Stephen


On 1 August 2011 19:54, Shay Banon <kimchy at gmail.com> wrote:
> Hi,
> ? I find the solution to change to volatile fields problematic. In my case,
> I use the mutable version of the DateTime to have UTC based timestamps
> converted automatically to relevant timezone and rounding, over a very large
> collection. Having volatile field means heavy writing and then reading of
> those values, which will really hurt perf. Here is an example of what I do:
> long[] timestamps = ... //very large dataset of timestamps
> MutableDateTime time = ..
> for (int i = 0; i < timestamps.length; i++) {
> ? ? ?time.setMillis(timestamp[i]);
> ? ? ?....
> ? ? ?time.getMillis();
> }
> I think changing to volatile will hurt many more programs out there than one
> might think...
> I would even say that the immutable values will almost always have to go
> through a memory barrier of some sort when "transfered" to another thread,
> so in practice the volatile solution will break more code out there (by it
> performing badly). I think if this can't be solved in the proper way (finals
> on immutable, non volatile on mutable, and hack serialization), then my vote
> is to break serialization, and if not, then do nothing and make a note in
> the docs regarding the immutable values.
> -shay.banon
> p.s. Sorry if this discussion has?diverged?to pure Joda, will be happy to
> continue it on the Joda mailing list if it makes sense
> On Tue, Jun 21, 2011 at 2:49 PM, Stephen Colebourne <scolebourne at joda.org>
> wrote:
>>
>> On 21 June 2011 09:11, Mark Thornton <mthornton at optrak.com> wrote:
>> > This should be backward compatible with existing serialized instances
>> > while
>> > allowing the fields in BaseDateTime to be final. It comes at the expense
>> > of
>> > a slightly larger MutableDateTime object, and slightly slower
>> > deserialization.
>>
>> In the end, I think Doug's advice is probably right. Just use
>> volatile. I don't think its ideal, but it "Just Works", which is a
>> good thing in this codebase.
>>
>> Mark, I think your change would work (and I like the neat set before
>> defaultReadFields). However it would have required any mutable
>> application subclass to be changed, which I was hoping to avoid.
>> Compared to the volatile approach I decided simple is best.
>>
>>
>> https://github.com/JodaOrg/joda-time/commit/67f1a30fc0fceda751b6347b2b16a5081878ac1e
>>
>> Another variation which would also appear to work would be to change
>> the data storage in BaseDateTime to a transient final two element
>> array and use ObjectStreamField to read and write the existing
>> serialized format. (The final array would presumably keep immutability
>> happy, while the array settability even when final would keep the
>> mutable classes happy). But the complexity was considerably greater,
>> so volatile won (unless someone screams that it kills their
>> performance).
>>
>> Thanks everyone for your thoughts.
>>
>> Stephen
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From kimchy at gmail.com  Mon Aug  1 17:15:45 2011
From: kimchy at gmail.com (Shay Banon)
Date: Tue, 2 Aug 2011 00:15:45 +0300
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com> <4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
Message-ID: <CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>

I just did a quick test by changing to volatile field the 1.6 release, and
on my machine I see 6x worse perf on 10 million data points (on a "real
world" use case in my system, end to end). Naturally, this will vary
depending on many aspects (number of cores, os) in terms of how volatile
write and read are handled. I really think that this is the wrong solution
to the "problem", but up to you...

On Mon, Aug 1, 2011 at 11:29 PM, Stephen Colebourne <scolebourne at joda.org>wrote:

> I'd be interested in any real world benchmark that indicates a
> significant slow down at the application level (not a micro
> benchmark).
>
> However, this thread didn't exactly give me any better options in line
> with compatibility (esp. now 2.0 is released)
>
> Stephen
>
>
> On 1 August 2011 19:54, Shay Banon <kimchy at gmail.com> wrote:
> > Hi,
> >   I find the solution to change to volatile fields problematic. In my
> case,
> > I use the mutable version of the DateTime to have UTC based timestamps
> > converted automatically to relevant timezone and rounding, over a very
> large
> > collection. Having volatile field means heavy writing and then reading of
> > those values, which will really hurt perf. Here is an example of what I
> do:
> > long[] timestamps = ... //very large dataset of timestamps
> > MutableDateTime time = ..
> > for (int i = 0; i < timestamps.length; i++) {
> >      time.setMillis(timestamp[i]);
> >      ....
> >      time.getMillis();
> > }
> > I think changing to volatile will hurt many more programs out there than
> one
> > might think...
> > I would even say that the immutable values will almost always have to go
> > through a memory barrier of some sort when "transfered" to another
> thread,
> > so in practice the volatile solution will break more code out there (by
> it
> > performing badly). I think if this can't be solved in the proper way
> (finals
> > on immutable, non volatile on mutable, and hack serialization), then my
> vote
> > is to break serialization, and if not, then do nothing and make a note in
> > the docs regarding the immutable values.
> > -shay.banon
> > p.s. Sorry if this discussion has diverged to pure Joda, will be happy to
> > continue it on the Joda mailing list if it makes sense
> > On Tue, Jun 21, 2011 at 2:49 PM, Stephen Colebourne <
> scolebourne at joda.org>
> > wrote:
> >>
> >> On 21 June 2011 09:11, Mark Thornton <mthornton at optrak.com> wrote:
> >> > This should be backward compatible with existing serialized instances
> >> > while
> >> > allowing the fields in BaseDateTime to be final. It comes at the
> expense
> >> > of
> >> > a slightly larger MutableDateTime object, and slightly slower
> >> > deserialization.
> >>
> >> In the end, I think Doug's advice is probably right. Just use
> >> volatile. I don't think its ideal, but it "Just Works", which is a
> >> good thing in this codebase.
> >>
> >> Mark, I think your change would work (and I like the neat set before
> >> defaultReadFields). However it would have required any mutable
> >> application subclass to be changed, which I was hoping to avoid.
> >> Compared to the volatile approach I decided simple is best.
> >>
> >>
> >>
> https://github.com/JodaOrg/joda-time/commit/67f1a30fc0fceda751b6347b2b16a5081878ac1e
> >>
> >> Another variation which would also appear to work would be to change
> >> the data storage in BaseDateTime to a transient final two element
> >> array and use ObjectStreamField to read and write the existing
> >> serialized format. (The final array would presumably keep immutability
> >> happy, while the array settability even when final would keep the
> >> mutable classes happy). But the complexity was considerably greater,
> >> so volatile won (unless someone screams that it kills their
> >> performance).
> >>
> >> Thanks everyone for your thoughts.
> >>
> >> Stephen
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110802/cd4498f9/attachment.html>

From heinz at javaspecialists.eu  Mon Aug  1 17:25:53 2011
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 02 Aug 2011 00:25:53 +0300
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>	<4E0039E2.2090102@optrak.com>
	<4E00524C.8000202@optrak.com>	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
	<CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
Message-ID: <4E3719E1.4030508@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110802/401c18c3/attachment.html>

From scolebourne at joda.org  Mon Aug  1 17:35:00 2011
From: scolebourne at joda.org (Stephen Colebourne)
Date: Mon, 1 Aug 2011 22:35:00 +0100
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com> <4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
	<CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
Message-ID: <CACzrW9DB0ybJaJdFZVKYSCVMRqbj=Yu=Fhj7sCfPDvmH5ODK9A@mail.gmail.com>

If the choice is Correct and slower or Risky and faster, I choose correct.

Stephen


On 1 August 2011 22:15, Shay Banon <kimchy at gmail.com> wrote:
> I just did a quick test by changing to volatile field the 1.6 release, and
> on my machine I see 6x worse perf on 10 million data points (on a "real
> world" use case in my system, end to end). Naturally, this will vary
> depending on many aspects (number of cores, os) in terms of how volatile
> write and read are handled. I really think that this is the wrong solution
> to the "problem", but up to you...
>
> On Mon, Aug 1, 2011 at 11:29 PM, Stephen Colebourne <scolebourne at joda.org>
> wrote:
>>
>> I'd be interested in any real world benchmark that indicates a
>> significant slow down at the application level (not a micro
>> benchmark).
>>
>> However, this thread didn't exactly give me any better options in line
>> with compatibility (esp. now 2.0 is released)
>>
>> Stephen
>>
>>
>> On 1 August 2011 19:54, Shay Banon <kimchy at gmail.com> wrote:
>> > Hi,
>> > ? I find the solution to change to volatile fields problematic. In my
>> > case,
>> > I use the mutable version of the DateTime to have UTC based timestamps
>> > converted automatically to relevant timezone and rounding, over a very
>> > large
>> > collection. Having volatile field means heavy writing and then reading
>> > of
>> > those values, which will really hurt perf. Here is an example of what I
>> > do:
>> > long[] timestamps = ... //very large dataset of timestamps
>> > MutableDateTime time = ..
>> > for (int i = 0; i < timestamps.length; i++) {
>> > ? ? ?time.setMillis(timestamp[i]);
>> > ? ? ?....
>> > ? ? ?time.getMillis();
>> > }
>> > I think changing to volatile will hurt many more programs out there than
>> > one
>> > might think...
>> > I would even say that the immutable values will almost always have to go
>> > through a memory barrier of some sort when "transfered" to another
>> > thread,
>> > so in practice the volatile solution will break more code out there (by
>> > it
>> > performing badly). I think if this can't be solved in the proper way
>> > (finals
>> > on immutable, non volatile on mutable, and hack serialization), then my
>> > vote
>> > is to break serialization, and if not, then do nothing and make a note
>> > in
>> > the docs regarding the immutable values.
>> > -shay.banon
>> > p.s. Sorry if this discussion has?diverged?to pure Joda, will be happy
>> > to
>> > continue it on the Joda mailing list if it makes sense
>> > On Tue, Jun 21, 2011 at 2:49 PM, Stephen Colebourne
>> > <scolebourne at joda.org>
>> > wrote:
>> >>
>> >> On 21 June 2011 09:11, Mark Thornton <mthornton at optrak.com> wrote:
>> >> > This should be backward compatible with existing serialized instances
>> >> > while
>> >> > allowing the fields in BaseDateTime to be final. It comes at the
>> >> > expense
>> >> > of
>> >> > a slightly larger MutableDateTime object, and slightly slower
>> >> > deserialization.
>> >>
>> >> In the end, I think Doug's advice is probably right. Just use
>> >> volatile. I don't think its ideal, but it "Just Works", which is a
>> >> good thing in this codebase.
>> >>
>> >> Mark, I think your change would work (and I like the neat set before
>> >> defaultReadFields). However it would have required any mutable
>> >> application subclass to be changed, which I was hoping to avoid.
>> >> Compared to the volatile approach I decided simple is best.
>> >>
>> >>
>> >>
>> >> https://github.com/JodaOrg/joda-time/commit/67f1a30fc0fceda751b6347b2b16a5081878ac1e
>> >>
>> >> Another variation which would also appear to work would be to change
>> >> the data storage in BaseDateTime to a transient final two element
>> >> array and use ObjectStreamField to read and write the existing
>> >> serialized format. (The final array would presumably keep immutability
>> >> happy, while the array settability even when final would keep the
>> >> mutable classes happy). But the complexity was considerably greater,
>> >> so volatile won (unless someone screams that it kills their
>> >> performance).
>> >>
>> >> Thanks everyone for your thoughts.
>> >>
>> >> Stephen
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From mthornton at optrak.com  Mon Aug  1 17:46:33 2011
From: mthornton at optrak.com (Mark Thornton)
Date: Mon, 01 Aug 2011 22:46:33 +0100
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>	<4E0039E2.2090102@optrak.com>
	<4E00524C.8000202@optrak.com>	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
	<CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
Message-ID: <4E371EB9.2010203@optrak.com>

On 01/08/11 22:15, Shay Banon wrote:
> I just did a quick test by changing to volatile field the 1.6 release, 
> and on my machine I see 6x worse perf on 10 million data points (on a 
> "real world" use case in my system, end to end). Naturally, this will 
> vary depending on many aspects (number of cores, os) in terms of how 
> volatile write and read are handled. I really think that this is the 
> wrong solution to the "problem", but up to you...
What sort of machine are you using? My impression was that volatile is 
quite cheap on x86.

Mark Thornton


From kimchy at gmail.com  Mon Aug  1 18:31:04 2011
From: kimchy at gmail.com (Shay Banon)
Date: Tue, 2 Aug 2011 01:31:04 +0300
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CACzrW9DB0ybJaJdFZVKYSCVMRqbj=Yu=Fhj7sCfPDvmH5ODK9A@mail.gmail.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com> <4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
	<CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
	<CACzrW9DB0ybJaJdFZVKYSCVMRqbj=Yu=Fhj7sCfPDvmH5ODK9A@mail.gmail.com>
Message-ID: <CALzs+uyPihfevw_8eth8kcviYq=4DmyR=vpK8_gKJF3tkmpCGA@mail.gmail.com>

Risky and Correct are on two different spectrums, so thats just feels
like propaganda. I'll give my line of reasoning another shot, but as I said
before, its really up to you, I am a happy user of Joda time and thank you
for the effort of building it (and from 2.0 onwards, will have my custom
build without the volatile because I both understand the implications of it
not being volatile/final, and have problems with the performance that comes
with it).

In practice, when constructing an object on one thread and passing it to
another thread, you already go through a memory barrier (for example, some
sort of a concurrent queue), so, I think the visibility concerns, compared
to the implications of changing the variables to volatile are misplaced.
Especially since this can potentially be solved in different manners. Heck,
even introducing a new ReallyHonestToGodImmutableDateTime class :)

On Tue, Aug 2, 2011 at 12:35 AM, Stephen Colebourne <scolebourne at joda.org>wrote:

> If the choice is Correct and slower or Risky and faster, I choose correct.
>
> Stephen
>
>
> On 1 August 2011 22:15, Shay Banon <kimchy at gmail.com> wrote:
> > I just did a quick test by changing to volatile field the 1.6 release,
> and
> > on my machine I see 6x worse perf on 10 million data points (on a "real
> > world" use case in my system, end to end). Naturally, this will vary
> > depending on many aspects (number of cores, os) in terms of how volatile
> > write and read are handled. I really think that this is the wrong
> solution
> > to the "problem", but up to you...
> >
> > On Mon, Aug 1, 2011 at 11:29 PM, Stephen Colebourne <
> scolebourne at joda.org>
> > wrote:
> >>
> >> I'd be interested in any real world benchmark that indicates a
> >> significant slow down at the application level (not a micro
> >> benchmark).
> >>
> >> However, this thread didn't exactly give me any better options in line
> >> with compatibility (esp. now 2.0 is released)
> >>
> >> Stephen
> >>
> >>
> >> On 1 August 2011 19:54, Shay Banon <kimchy at gmail.com> wrote:
> >> > Hi,
> >> >   I find the solution to change to volatile fields problematic. In my
> >> > case,
> >> > I use the mutable version of the DateTime to have UTC based timestamps
> >> > converted automatically to relevant timezone and rounding, over a very
> >> > large
> >> > collection. Having volatile field means heavy writing and then reading
> >> > of
> >> > those values, which will really hurt perf. Here is an example of what
> I
> >> > do:
> >> > long[] timestamps = ... //very large dataset of timestamps
> >> > MutableDateTime time = ..
> >> > for (int i = 0; i < timestamps.length; i++) {
> >> >      time.setMillis(timestamp[i]);
> >> >      ....
> >> >      time.getMillis();
> >> > }
> >> > I think changing to volatile will hurt many more programs out there
> than
> >> > one
> >> > might think...
> >> > I would even say that the immutable values will almost always have to
> go
> >> > through a memory barrier of some sort when "transfered" to another
> >> > thread,
> >> > so in practice the volatile solution will break more code out there
> (by
> >> > it
> >> > performing badly). I think if this can't be solved in the proper way
> >> > (finals
> >> > on immutable, non volatile on mutable, and hack serialization), then
> my
> >> > vote
> >> > is to break serialization, and if not, then do nothing and make a note
> >> > in
> >> > the docs regarding the immutable values.
> >> > -shay.banon
> >> > p.s. Sorry if this discussion has diverged to pure Joda, will be happy
> >> > to
> >> > continue it on the Joda mailing list if it makes sense
> >> > On Tue, Jun 21, 2011 at 2:49 PM, Stephen Colebourne
> >> > <scolebourne at joda.org>
> >> > wrote:
> >> >>
> >> >> On 21 June 2011 09:11, Mark Thornton <mthornton at optrak.com> wrote:
> >> >> > This should be backward compatible with existing serialized
> instances
> >> >> > while
> >> >> > allowing the fields in BaseDateTime to be final. It comes at the
> >> >> > expense
> >> >> > of
> >> >> > a slightly larger MutableDateTime object, and slightly slower
> >> >> > deserialization.
> >> >>
> >> >> In the end, I think Doug's advice is probably right. Just use
> >> >> volatile. I don't think its ideal, but it "Just Works", which is a
> >> >> good thing in this codebase.
> >> >>
> >> >> Mark, I think your change would work (and I like the neat set before
> >> >> defaultReadFields). However it would have required any mutable
> >> >> application subclass to be changed, which I was hoping to avoid.
> >> >> Compared to the volatile approach I decided simple is best.
> >> >>
> >> >>
> >> >>
> >> >>
> https://github.com/JodaOrg/joda-time/commit/67f1a30fc0fceda751b6347b2b16a5081878ac1e
> >> >>
> >> >> Another variation which would also appear to work would be to change
> >> >> the data storage in BaseDateTime to a transient final two element
> >> >> array and use ObjectStreamField to read and write the existing
> >> >> serialized format. (The final array would presumably keep
> immutability
> >> >> happy, while the array settability even when final would keep the
> >> >> mutable classes happy). But the complexity was considerably greater,
> >> >> so volatile won (unless someone screams that it kills their
> >> >> performance).
> >> >>
> >> >> Thanks everyone for your thoughts.
> >> >>
> >> >> Stephen
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110802/f8aa90d3/attachment.html>

From ashwin.jayaprakash at gmail.com  Mon Aug  1 23:47:30 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 1 Aug 2011 20:47:30 -0700
Subject: [concurrency-interest] j.u.c.Phaser tutorial?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEAIIPAA.davidcholmes@aapt.net.au>
References: <CAF9YjSDPP2a9b3tt7rfy1EenjNGugwjxoceuNN37Rd=kn9VNpQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEAIIPAA.davidcholmes@aapt.net.au>
Message-ID: <CAF9YjSA8N2yOTDQuEmW_O9g4uKXVA6OeTN8_ydTtzLj1JMwwyA@mail.gmail.com>

Ok, I tried writing some sample code and I think I got the hang of it. I
wrote about it here -
http://javaforu.blogspot.com/2011/08/java-7s-jucphaser-short-tutorial.html.
Comments welcome.


Regards,
Ashwin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110801/0ac8025e/attachment.html>

From william.louth at jinspired.com  Tue Aug  2 06:59:17 2011
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Tue, 02 Aug 2011 12:59:17 +0200
Subject: [concurrency-interest] LongAdder (was: StripedAdder) and
	LongAdderTable
In-Reply-To: <4E35AB6B.3080108@jinspired.com>
References: <4E356CEF.6050003@cs.oswego.edu>
	<CACyP5Pdc-5b32bfpCBjCRC=oF=-4AXo6j9mpmL4xZrVpO+yGkg@mail.gmail.com>
	<4E35A54C.9020300@cs.oswego.edu> <4E35AB6B.3080108@jinspired.com>
Message-ID: <4E37D885.8000306@jinspired.com>

Wanted to add that I see named thread counters as being the only 
realistically option for greater instrumentation & observation in the 
concurrent package without having to expose more superfluous methods in 
the interfaces which anyway would not be consistent and constitute 
causality. In fact I think we need to see more of this in all libraries 
especially container like interfaces which offer dynamic capabilities. 
For example in interacting w/ a map it would be great to be able to 
expose resizing.

With savepoint & changeset support this has many cool possibilities.

http://opencore.jinspired.com/?page_id=3553#p23

http://williamlouth.wordpress.com/2010/01/05/user-level-metering-with-savepoints-changesets/ 


On 31/07/2011 21:22, William Louth (JINSPIRED.COM) wrote:
> I am currently writing up a proposal (will be posted on blog next week 
> hopefully) for having an intrinsic (long) counter data type in the 
> Java that would be thread local (though not access through this 
> interface) and optimized by the JVM whilst affording the ability to 
> introspect the current set of named counters in the JVM as well as 
> their instance (and value) on a per thread basis (preferably within 
> the thread itself) via an API. I think we should be encouraging 
> developers to move away from process level JMX like counters and 
> instead thread specific & event based (without state) which could be 
> in turn be accessed at a process level if need be but more so at the 
> thread level and from a caller/chain perspective forming a foundation 
> for the ultimate feedback loop between callers and callees.
>
> This a part of a much bigger proposal for software activity metering 
> to be a core aspect of the runtime, library and possibly language (via 
> event counters)
>
> OpenCore Metering Runtime Actors
> http://opencore.jinspired.com/?p=1888
>
> Activity Based Costing & Metering (ABC/M) ? The Ultimate Feedback Loop
> http://opencore.jinspired.com/?p=4052
>
> Automated Performance Management starts with Software?s Self Observation
> http://opencore.jinspired.com/?p=2709
>
> Metering (Probes) Open API
> http://opencore.jinspired.com/?page_id=715
>
> On 31/07/2011 20:56, Doug Lea wrote:
>> On 07/31/11 12:39, Christian Vest Hansen wrote:
>>> Some interfaces for these things might be a good idea, as I can 
>>> imagine data
>>> grid libraries might want to provide distributed implementations. 
>>> Counter and
>>> CounterTable comes to mind as possible names.
>>
>> I had proposed this, but talked myself out of it.
>> The APIs are tied to a particular scalar type (long),
>> and might grow to include others (in particular, a
>> DoubleAdder class). It may be that the only
>> commonality is that they extend java.lang.Number. Which
>> we declared for the AtomicX classes, but even that was not
>> obviously helpful.
>>
>>> On Sun, Jul 31, 2011 at 16:55, Doug Lea <dl at cs.oswego.edu
>>> <mailto:dl at cs.oswego.edu>> wrote:
>>
>>> Also, because it is likely to be among the more
>>> common uses of LongAdders, we created AtomicLongTable,
>>
>> Oops. I meant LongAdderTable. (The names changed several times
>> times before check-in.)
>>
>> -Doug
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From hans.boehm at hp.com  Tue Aug  2 14:51:23 2011
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 2 Aug 2011 18:51:23 +0000
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <4E371EB9.2010203@optrak.com>
References: <BANLkTim+z_8vSTwneeP4MTq03iLoA+fciw@mail.gmail.com>
	<4E0039E2.2090102@optrak.com>	<4E00524C.8000202@optrak.com>
	<BANLkTikw8h4kSm1cm2Mf7n54oGfWQ7EHOQ@mail.gmail.com>
	<CALzs+uyxRpvUv-Gg0fKgLePJ=t2kBP1o8cYO3VeYqnYJJm4U2w@mail.gmail.com>
	<CACzrW9DToU70Mwcx_WAGXby8oPj90LcY+XefgeO5cceq44RGkw@mail.gmail.com>
	<CALzs+uwezogODuGFP-jHPXEQP37p5+k9bZPNkjSAHiD6aOGG4A@mail.gmail.com>
	<4E371EB9.2010203@optrak.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD201DBD1@G4W3299.americas.hpqcorp.net>

Volatile loads should be cheap on x86.  Volatile stores involve an explicit fence or XCHG operation.  In my experience, this costs somewhere between a dozen and hundreds of cycles, depending on the particular processor, and context.  The (now essentially obsolete) Pentium 4 generation of Intel processors seemed to have the highest costs, often by a wide margin.

My intuition is that this usually doesn't matter too much, since:

1) Loads tend to be more frequent than stores, and
2) If stores are frequent and executed by multiple threads, you are likely to spend even more time dealing with coherence misses.

It may well be that neither observation applies here.

(There is also an argument that much of these costs should be avoidable by adding a couple of instructions that essentially perform volatile loads and stores.  The fence here is often substantial over-kill.  But it's the only hammer we currently have ...)

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of Mark Thornton
> Sent: Monday, August 01, 2011 2:47 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Joda-Time immutability
> 
> On 01/08/11 22:15, Shay Banon wrote:
> > I just did a quick test by changing to volatile field the 1.6
> release,
> > and on my machine I see 6x worse perf on 10 million data points (on a
> > "real world" use case in my system, end to end). Naturally, this will
> > vary depending on many aspects (number of cores, os) in terms of how
> > volatile write and read are handled. I really think that this is the
> > wrong solution to the "problem", but up to you...
> What sort of machine are you using? My impression was that volatile is
> quite cheap on x86.
> 
> Mark Thornton
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Tue Aug  2 14:54:36 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 02 Aug 2011 14:54:36 -0400
Subject: [concurrency-interest] {Long, Double}Adder, {Long, Double}MaxUpdater
Message-ID: <4E3847EC.2070103@cs.oswego.edu>


To help round out the set of tools needed for most
problems needing scalable scalar updates, jsr166e
now also contains a Double version of adder, as well as
Long and Double versions of updaters for maintaining
a running maximum. They all have similar APIs, that
now extend java.lang.Number.

The case for releasing versions for Doubles was
especially strong since the only supported alternative
was using locks. We do not support an AtomicDouble
class because the atomic notion of compareAndSet
(which relies on bitwise equality) clashes with
double-precision "==" equals (which does not strictly
rely on bitwise equality, for example for NaN and
+0.0 vs -0.0). However, we can and do still use
bitwise-equals-based CAS internally in DoubleAdder just to
manage contention, without any visible external impact.
The only byproduct is that interconversion overhead slows
down uncontended performance just barely enough to notice,
but performance under contention is vastly better
than alternatives.

You cannot mix sum and max computations in the same objects,
because these classes internally rely on a form of
reduction requiring associative operations with identity.
(If and when lambdas and scalar function types become
available, we might consider a more general form that
supports any monoid. But sum and max are by far the most
common forms so might as well in any case remain specially
coded. Note that you can also use MaxUpdaters for tracking a
minimum just by negating arguments, and can use Adders for
double products by taking logs.


Links:
     *  API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
     * jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled 
using Java7 javac).
     * Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/
     * Browsable CVS test file sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/jsr166e/

From zhong.j.yu at gmail.com  Wed Aug  3 04:42:53 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 3 Aug 2011 03:42:53 -0500
Subject: [concurrency-interest] Joda-Time immutability
Message-ID: <CACuKZqGOyUFMkX-zrUyFzk_UY41_N3nRLCepw-B4s1F9hVEW6Q@mail.gmail.com>

The fix with `volatile` is incorrect. Another thread may observe an
uninitialized field - writing the volatile field and publishing `this`
reference can be reordered. Only a `final` field can prevent that.

The original problem is not about making updates to the field visible
to other thread - MutableDateTime was never intended to be thread
safe.

My understanding is that it can be fixed like this:

    class Base // immutable

        final state;

        getState(){ return state; }

        method(){ calculate( getState() ); }

    class Derived extends Base // mutable

        my_state;

        setState( state ){ my_state=state; }

        @Override
        getState(){ return my_state? my_state : super.getState(); }



- Zhong Yu

From scolebourne at joda.org  Wed Aug  3 13:18:29 2011
From: scolebourne at joda.org (Stephen Colebourne)
Date: Wed, 3 Aug 2011 18:18:29 +0100
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CACuKZqGOyUFMkX-zrUyFzk_UY41_N3nRLCepw-B4s1F9hVEW6Q@mail.gmail.com>
References: <CACuKZqGOyUFMkX-zrUyFzk_UY41_N3nRLCepw-B4s1F9hVEW6Q@mail.gmail.com>
Message-ID: <CACzrW9CtLr30HqMO6LMCnhDLZvQpV_zi4N3o1Ot+=6nzVYSWXg@mail.gmail.com>

On 3 August 2011 09:42, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> The fix with `volatile` is incorrect. Another thread may observe an
> uninitialized field - writing the volatile field and publishing `this`
> reference can be reordered. Only a `final` field can prevent that.

Would that not require the constructor to expose the reference to 'this'?

In the DateTime class, the 'this' reference is not exposed as far as I can see:
https://github.com/JodaOrg/joda-time/blob/master/src/main/java/org/joda/time/DateTime.java
https://github.com/JodaOrg/joda-time/blob/master/src/main/java/org/joda/time/base/BaseDateTime.java
https://github.com/JodaOrg/joda-time/blob/master/src/main/java/org/joda/time/base/AbstractDateTime.java
https://github.com/JodaOrg/joda-time/blob/master/src/main/java/org/joda/time/base/AbstractInstant.java

> The original problem is not about making updates to the field visible
> to other thread - MutableDateTime was never intended to be thread
> safe.
Agreed. MutableDateTime does not need to be thread-safe. I only care
about the immutable classes being truly immutable.

> My understanding is that it can be fixed like this:
>
> ? ?class Base // immutable
>
> ? ? ? ?final state;
>
> ? ? ? ?getState(){ return state; }
>
> ? ? ? ?method(){ calculate( getState() ); }
>
> ? ?class Derived extends Base // mutable
>
> ? ? ? ?my_state;
>
> ? ? ? ?setState( state ){ my_state=state; }
>
> ? ? ? ?@Override
> ? ? ? ?getState(){ return my_state? my_state : super.getState(); }

Such a design might work, but would change the serialization of the
mutable class.

Stephen


From zhong.j.yu at gmail.com  Wed Aug  3 15:01:35 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 3 Aug 2011 14:01:35 -0500
Subject: [concurrency-interest] Joda-Time immutability
In-Reply-To: <CACzrW9CtLr30HqMO6LMCnhDLZvQpV_zi4N3o1Ot+=6nzVYSWXg@mail.gmail.com>
References: <CACuKZqGOyUFMkX-zrUyFzk_UY41_N3nRLCepw-B4s1F9hVEW6Q@mail.gmail.com>
	<CACzrW9CtLr30HqMO6LMCnhDLZvQpV_zi4N3o1Ot+=6nzVYSWXg@mail.gmail.com>
Message-ID: <CACuKZqETg6hvSEfVTuoZnwzofY8XJtEhFXGGinOoe4Nk1veucQ@mail.gmail.com>

On Wed, Aug 3, 2011 at 12:18 PM, Stephen Colebourne
<scolebourne at joda.org> wrote:
> On 3 August 2011 09:42, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> The fix with `volatile` is incorrect. Another thread may observe an
>> uninitialized field - writing the volatile field and publishing `this`
>> reference can be reordered. Only a `final` field can prevent that.
>
> Would that not require the constructor to expose the reference to 'this'?

static public DateTime globalVar;

// thread 1
globalVar = DateTime.now();

// thread 2
DateTime var = globalVar;
if(var!=null)
    var.getMillis(); // may return 0

Thread 2 may observe that globalVar is assigned before its iMillis
field is assigned. This will not happen if the field is final.

> Such a design might work, but would change the serialization of the
> mutable class.

I think it's backward compatible. It would be a problem if the
serializer uses the new class, and deserializer uses the old class. Is
that a use case you must code against?

- Zhong Yu

From ashwin.jayaprakash at gmail.com  Fri Aug  5 19:43:17 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Fri, 5 Aug 2011 16:43:17 -0700
Subject: [concurrency-interest] int[] in the ForkJoin example
Message-ID: <CAF9YjSC7hGdEK30fO2-Hq=tS2ZkP09_6-fT2tFH7GUau9mrt5A@mail.gmail.com>

The Fork-Join example/doc shows an int[] being written to by multiple
threads (
http://download.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html).
Each task writes to a separate location in the array, but how is it valid
without being marked as volatile or using AtomicIntegerArray?

public class ForkBlur extends RecursiveAction {
 ...
  private int[] mDestination;
  ...
 }

  protected void computeDirectly() {
    ...
    mDestination[index] = dpixel;
  }
}

What is the guarantee that at the end of ForkJoinPool.invoke(), the
waiting/submitting thread will be able to see all the changes made to the
array by the various FJ pool threads?

Am I wrong or am I wrong? :)

I had asked a similar question about just writing to a large byte[] but not
about reading it back .. word tearing and such -
http://markmail.org/message/bca7ywwtdjbqbnu3. But in this FJ demo, we are
reading the results at the end.

Thanks,
Ashwin.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110805/0bc98b8b/attachment.html>

From dl at cs.oswego.edu  Sat Aug  6 09:43:05 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 06 Aug 2011 09:43:05 -0400
Subject: [concurrency-interest] int[] in the ForkJoin example
In-Reply-To: <CAF9YjSC7hGdEK30fO2-Hq=tS2ZkP09_6-fT2tFH7GUau9mrt5A@mail.gmail.com>
References: <CAF9YjSC7hGdEK30fO2-Hq=tS2ZkP09_6-fT2tFH7GUau9mrt5A@mail.gmail.com>
Message-ID: <4E3D44E9.5020607@cs.oswego.edu>

On 08/05/11 19:43, Ashwin Jayaprakash wrote:
> The Fork-Join example/doc shows an int[] being written to by multiple threads
> (http://download.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html). Each
> task writes to a separate location in the array, but how is it valid without
> being marked as volatile or using AtomicIntegerArray?

As mentioned in the ForkJoinTask documentation,
(http://download.oracle.com/javase/7/docs/api/java/util/concurrent/ForkJoinTask.html)
it is only valid if the tasks each access distinct locations
before joining. See in particular the javadoc for "fork()", saying:

"Subsequent modifications to the state of this task or any data it operates on 
are not necessarily consistently observable by any thread other than the one 
executing it unless preceded by a call to join()  or related methods, or a call 
to isDone()  returning true. "

Internally, the guarantees about ordering and visibility upon
joins (as well as related methods like invoke) are arranged
via volatile/atomic operations that are similar to the ones
used for similar guranatees upon unlocking locks.

-Doug

From dmitry.miltsov at oracle.com  Sat Aug  6 12:03:37 2011
From: dmitry.miltsov at oracle.com (dmitry.miltsov at oracle.com)
Date: Sat, 6 Aug 2011 09:03:37 -0700 (PDT)
Subject: [concurrency-interest] Auto Reply: Concurrency-interest Digest,
	Vol 79, Issue 8
Message-ID: <5d08a017-3930-4d33-8830-7f4a3d27d07c@default>

This is an auto-replied message.
I'm on vacation from August 8, returning to the office on August 15.

My backup persons are:
java.lang, java.text, java.util - Yuri Gaevsky;
java.security, javax.security - Paul Rank.

Please contact my manager Pavel Klodin regarding other issues. 

Thanks,
Dmitry Miltsov


From ashwin.jayaprakash at gmail.com  Mon Aug  8 13:30:03 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Mon, 8 Aug 2011 10:30:03 -0700
Subject: [concurrency-interest] int[] in the ForkJoin example
Message-ID: <CAF9YjSB2X=MfjUn1yAYVRR=eP68j7jsaYvD7nVkAMFiHyfukAQ@mail.gmail.com>

Thanks.

I was trying to walk through the code to try and understand where that
barrier could be doing its thing. Obviously the FJ module is fairly complex,
but I'd appreciate it if someone could point to where/how it forces a
consistent read after all the recursive jobs are done.

Is it the synchronized(this) in FJTask.externalAwaitDone() or somewhere in
doJoin()?

Ashwin.



> ---------- Forwarded message ----------
> From: Doug Lea <dl at cs.oswego.edu>
> To: concurrency-interest at cs.oswego.edu
> Date: Sat, 06 Aug 2011 09:43:05 -0400
> Subject: Re: [concurrency-interest] int[] in the ForkJoin example
> On 08/05/11 19:43, Ashwin Jayaprakash wrote:
>
>> The Fork-Join example/doc shows an int[] being written to by multiple
>> threads
>> (http://download.oracle.com/**javase/tutorial/essential/**
>> concurrency/forkjoin.html<http://download.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html>).
>> Each
>> task writes to a separate location in the array, but how is it valid
>> without
>> being marked as volatile or using AtomicIntegerArray?
>>
>
> As mentioned in the ForkJoinTask documentation,
> (http://download.oracle.com/**javase/7/docs/api/java/util/**
> concurrent/ForkJoinTask.html<http://download.oracle.com/javase/7/docs/api/java/util/concurrent/ForkJoinTask.html>
> )
> it is only valid if the tasks each access distinct locations
> before joining. See in particular the javadoc for "fork()", saying:
>
> "Subsequent modifications to the state of this task or any data it operates
> on are not necessarily consistently observable by any thread other than the
> one executing it unless preceded by a call to join()  or related methods, or
> a call to isDone()  returning true. "
>
> Internally, the guarantees about ordering and visibility upon
> joins (as well as related methods like invoke) are arranged
> via volatile/atomic operations that are similar to the ones
> used for similar guranatees upon unlocking locks.
>
> -Doug
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110808/808e1b9f/attachment.html>

From headius at headius.com  Mon Aug  8 15:08:08 2011
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 8 Aug 2011 15:08:08 -0400
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
Message-ID: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>

Hello all!

I'm looking to improve the performance of JRuby's implementation of
Ruby's "Fiber". Fiber is intended as a lightweight thread or
coroutine. Fibers are bound to a parent thread, and only one Fiber or
the parent thread can be running at a given time. Control transfer --
in the logical sense -- pauses the caller and continues the callee
from where it left off. The callee can then "yield" control back to
the caller, pausing its execution.

Because Fibers retain execution state (call stack, etc), they are
implemented in JRuby using a native thread per Fiber. Control transfer
used to be done via SynchronousBlockingQueue objects, but I am making
changes to use LockSupport.park/unpark directly. LockSupport appears
to transfer control around 2x faster than using SBQ, but it's still
many times slower than C Ruby's mechanism of saving and restoring
native C stack frames within the same native thread.

So, I'm looking for suggestions on a better mechanism to explicitly
deschedule one thread and start another one, knowing that it's always
an explicit cooperative rescheduling.

Thoughts?

- Charlie

From ron.pressler at gmail.com  Mon Aug  8 15:30:26 2011
From: ron.pressler at gmail.com (Ron Pressler)
Date: Mon, 8 Aug 2011 22:30:26 +0300
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
Message-ID: <CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>

If each fiber is represented by a thread, then any transfer of control
between fibers would require an OS task switch, which is bound to be slower
than C Ruby's lightweight threads. However, the erjang
<https://github.com/trifork/erjang/wiki>project has implemented erlang on
top of the JVM, and erlang requires lots of "fiber" switches for its actors.
The way they did it is by using a modified version of Kilim
<http://www.malhar.net/sriram/kilim/>,
which uses bytecode instrumentation to implement continuations with
stack-capture - the same way C Ruby does it, I presume.
Scala uses java's Fork/Join tasks for its actor scheduling, and it sounds
like you might be able to use that too. You transfer control to another
fiber with fork, and block yourself with join.

But in any case, for best performance you will probably have to abandon the
one-thread-per-fiber model (which is also expensive on memory)

Ron

On Mon, Aug 8, 2011 at 10:08 PM, Charles Oliver Nutter
<headius at headius.com>wrote:

> Hello all!
>
> I'm looking to improve the performance of JRuby's implementation of
> Ruby's "Fiber". Fiber is intended as a lightweight thread or
> coroutine. Fibers are bound to a parent thread, and only one Fiber or
> the parent thread can be running at a given time. Control transfer --
> in the logical sense -- pauses the caller and continues the callee
> from where it left off. The callee can then "yield" control back to
> the caller, pausing its execution.
>
> Because Fibers retain execution state (call stack, etc), they are
> implemented in JRuby using a native thread per Fiber. Control transfer
> used to be done via SynchronousBlockingQueue objects, but I am making
> changes to use LockSupport.park/unpark directly. LockSupport appears
> to transfer control around 2x faster than using SBQ, but it's still
> many times slower than C Ruby's mechanism of saving and restoring
> native C stack frames within the same native thread.
>
> So, I'm looking for suggestions on a better mechanism to explicitly
> deschedule one thread and start another one, knowing that it's always
> an explicit cooperative rescheduling.
>
> Thoughts?
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110808/a44d3d0a/attachment.html>

From headius at headius.com  Mon Aug  8 16:24:06 2011
From: headius at headius.com (Charles Oliver Nutter)
Date: Mon, 8 Aug 2011 16:24:06 -0400
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
Message-ID: <CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>

On Mon, Aug 8, 2011 at 3:30 PM, Ron Pressler <ron.pressler at gmail.com> wrote:
> If each fiber is represented by a thread, then any transfer of control
> between fibers would require an OS task switch, which is bound to be slower
> than C Ruby's lightweight threads. However, the erjang project has
> implemented erlang on top of the JVM, and erlang requires lots of "fiber"
> switches for its actors. The way they did it is by using a modified version
> of Kilim?, which uses bytecode instrumentation to implement continuations
> with stack-capture - the same way C Ruby does it, I presume.

This would work, but the execution performance of bytecode
instrumentation would almost certainly be worse than what we have now.
Kilim-style bytecode manipulation also only works if you can
instrument all the code you're passing through (leaves excluded),
which would mean almost all of JRuby would need to be instrumented in
this way.

I would also wager there's no change of inlining code that's been
instrumented...or at least a large class of optimizations would not
work in the presence of stack trampolines.

> Scala uses java's Fork/Join tasks for its actor scheduling, and it sounds
> like you might be able to use that too. You transfer control to another
> fiber with fork, and block yourself with join.

I'm essentially doing this now with park/unpark, but I'll look into
whether Fork/Join makes it cleaner.

> But in any case, for best performance you will probably have to abandon the
> one-thread-per-fiber model (which is also expensive on memory)

Yes, I'd love to :) But unfortunately there's no way to do that on the
JVM right now.

Perhaps this is a good time to lobby for concurrency-interest's
support in Lukas Stadler's JVM coroutines. He implemented them for
OpenJDK as part of his PhD work, and they provide much faster Fiber
context-switching than even the C implementations of Ruby. I believe
he's looking to launch a JSR soon...and it seems like
pausable/resumable workers (in the form of coroutines) might be a
useful feature for concurrency (especially atop blocking library calls
that would steal a thread forever). Imagine having a worker pool that
only has N threads for M > N running worker coroutines, and uses
coroutine suspend/resume to choose between them as necessary.

- Charlie


From viktor.klang at gmail.com  Tue Aug  9 05:18:32 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 9 Aug 2011 11:18:32 +0200
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
Message-ID: <CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>

What about CPS transformation?

On Mon, Aug 8, 2011 at 10:24 PM, Charles Oliver Nutter
<headius at headius.com>wrote:

> On Mon, Aug 8, 2011 at 3:30 PM, Ron Pressler <ron.pressler at gmail.com>
> wrote:
> > If each fiber is represented by a thread, then any transfer of control
> > between fibers would require an OS task switch, which is bound to be
> slower
> > than C Ruby's lightweight threads. However, the erjang project has
> > implemented erlang on top of the JVM, and erlang requires lots of "fiber"
> > switches for its actors. The way they did it is by using a modified
> version
> > of Kilim , which uses bytecode instrumentation to implement continuations
> > with stack-capture - the same way C Ruby does it, I presume.
>
> This would work, but the execution performance of bytecode
> instrumentation would almost certainly be worse than what we have now.
> Kilim-style bytecode manipulation also only works if you can
> instrument all the code you're passing through (leaves excluded),
> which would mean almost all of JRuby would need to be instrumented in
> this way.
>
> I would also wager there's no change of inlining code that's been
> instrumented...or at least a large class of optimizations would not
> work in the presence of stack trampolines.
>
> > Scala uses java's Fork/Join tasks for its actor scheduling, and it sounds
> > like you might be able to use that too. You transfer control to another
> > fiber with fork, and block yourself with join.
>
> I'm essentially doing this now with park/unpark, but I'll look into
> whether Fork/Join makes it cleaner.
>
> > But in any case, for best performance you will probably have to abandon
> the
> > one-thread-per-fiber model (which is also expensive on memory)
>
> Yes, I'd love to :) But unfortunately there's no way to do that on the
> JVM right now.
>
> Perhaps this is a good time to lobby for concurrency-interest's
> support in Lukas Stadler's JVM coroutines. He implemented them for
> OpenJDK as part of his PhD work, and they provide much faster Fiber
> context-switching than even the C implementations of Ruby. I believe
> he's looking to launch a JSR soon...and it seems like
> pausable/resumable workers (in the form of coroutines) might be a
> useful feature for concurrency (especially atop blocking library calls
> that would steal a thread forever). Imagine having a worker pool that
> only has N threads for M > N running worker coroutines, and uses
> coroutine suspend/resume to choose between them as necessary.
>
> - Charlie
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110809/7a91719f/attachment-0001.html>

From joe.bowbeer at gmail.com  Tue Aug  9 11:09:23 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 9 Aug 2011 08:09:23 -0700
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
	<CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
Message-ID: <CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>

What? No chickens?

Do you mean CSP - communicating sequential processes?
On Aug 9, 2011 2:28 AM, "?iktor ?lang" <viktor.klang at gmail.com> wrote:
> What about CPS transformation?
>
> On Mon, Aug 8, 2011 at 10:24 PM, Charles Oliver Nutter
> <headius at headius.com>wrote:
>
>> On Mon, Aug 8, 2011 at 3:30 PM, Ron Pressler <ron.pressler at gmail.com>
>> wrote:
>> > If each fiber is represented by a thread, then any transfer of control
>> > between fibers would require an OS task switch, which is bound to be
>> slower
>> > than C Ruby's lightweight threads. However, the erjang project has
>> > implemented erlang on top of the JVM, and erlang requires lots of
"fiber"
>> > switches for its actors. The way they did it is by using a modified
>> version
>> > of Kilim , which uses bytecode instrumentation to implement
continuations
>> > with stack-capture - the same way C Ruby does it, I presume.
>>
>> This would work, but the execution performance of bytecode
>> instrumentation would almost certainly be worse than what we have now.
>> Kilim-style bytecode manipulation also only works if you can
>> instrument all the code you're passing through (leaves excluded),
>> which would mean almost all of JRuby would need to be instrumented in
>> this way.
>>
>> I would also wager there's no change of inlining code that's been
>> instrumented...or at least a large class of optimizations would not
>> work in the presence of stack trampolines.
>>
>> > Scala uses java's Fork/Join tasks for its actor scheduling, and it
sounds
>> > like you might be able to use that too. You transfer control to another
>> > fiber with fork, and block yourself with join.
>>
>> I'm essentially doing this now with park/unpark, but I'll look into
>> whether Fork/Join makes it cleaner.
>>
>> > But in any case, for best performance you will probably have to abandon
>> the
>> > one-thread-per-fiber model (which is also expensive on memory)
>>
>> Yes, I'd love to :) But unfortunately there's no way to do that on the
>> JVM right now.
>>
>> Perhaps this is a good time to lobby for concurrency-interest's
>> support in Lukas Stadler's JVM coroutines. He implemented them for
>> OpenJDK as part of his PhD work, and they provide much faster Fiber
>> context-switching than even the C implementations of Ruby. I believe
>> he's looking to launch a JSR soon...and it seems like
>> pausable/resumable workers (in the form of coroutines) might be a
>> useful feature for concurrency (especially atop blocking library calls
>> that would steal a thread forever). Imagine having a worker pool that
>> only has N threads for M > N running worker coroutines, and uses
>> coroutine suspend/resume to choose between them as necessary.
>>
>> - Charlie
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
> Experts
>
> Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110809/db18fea5/attachment.html>

From joe.bowbeer at gmail.com  Tue Aug  9 11:16:03 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 9 Aug 2011 08:16:03 -0700
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
	<CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
	<CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>
Message-ID: <CAHzJPEoq1xN0y3YRy_vN4WXXHKkG+DUdbpS6-Rnnswsc3z78fg@mail.gmail.com>

Ah, sorry, I was familiar with continuations, but not with CPS
transformation.

Is there a favorite reference you can recommend?
On Aug 9, 2011 8:09 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:
> What? No chickens?
>
> Do you mean CSP - communicating sequential processes?
> On Aug 9, 2011 2:28 AM, "?iktor ?lang" <viktor.klang at gmail.com> wrote:
>> What about CPS transformation?
>>
>> On Mon, Aug 8, 2011 at 10:24 PM, Charles Oliver Nutter
>> <headius at headius.com>wrote:
>>
>>> On Mon, Aug 8, 2011 at 3:30 PM, Ron Pressler <ron.pressler at gmail.com>
>>> wrote:
>>> > If each fiber is represented by a thread, then any transfer of control
>>> > between fibers would require an OS task switch, which is bound to be
>>> slower
>>> > than C Ruby's lightweight threads. However, the erjang project has
>>> > implemented erlang on top of the JVM, and erlang requires lots of
> "fiber"
>>> > switches for its actors. The way they did it is by using a modified
>>> version
>>> > of Kilim , which uses bytecode instrumentation to implement
> continuations
>>> > with stack-capture - the same way C Ruby does it, I presume.
>>>
>>> This would work, but the execution performance of bytecode
>>> instrumentation would almost certainly be worse than what we have now.
>>> Kilim-style bytecode manipulation also only works if you can
>>> instrument all the code you're passing through (leaves excluded),
>>> which would mean almost all of JRuby would need to be instrumented in
>>> this way.
>>>
>>> I would also wager there's no change of inlining code that's been
>>> instrumented...or at least a large class of optimizations would not
>>> work in the presence of stack trampolines.
>>>
>>> > Scala uses java's Fork/Join tasks for its actor scheduling, and it
> sounds
>>> > like you might be able to use that too. You transfer control to
another
>>> > fiber with fork, and block yourself with join.
>>>
>>> I'm essentially doing this now with park/unpark, but I'll look into
>>> whether Fork/Join makes it cleaner.
>>>
>>> > But in any case, for best performance you will probably have to
abandon
>>> the
>>> > one-thread-per-fiber model (which is also expensive on memory)
>>>
>>> Yes, I'd love to :) But unfortunately there's no way to do that on the
>>> JVM right now.
>>>
>>> Perhaps this is a good time to lobby for concurrency-interest's
>>> support in Lukas Stadler's JVM coroutines. He implemented them for
>>> OpenJDK as part of his PhD work, and they provide much faster Fiber
>>> context-switching than even the C implementations of Ruby. I believe
>>> he's looking to launch a JSR soon...and it seems like
>>> pausable/resumable workers (in the form of coroutines) might be a
>>> useful feature for concurrency (especially atop blocking library calls
>>> that would steal a thread forever). Imagine having a worker pool that
>>> only has N threads for M > N running worker coroutines, and uses
>>> coroutine suspend/resume to choose between them as necessary.
>>>
>>> - Charlie
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> --
>> Viktor Klang
>>
>> Akka Tech Lead
>> Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
>> Experts
>>
>> Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110809/e839c0e9/attachment.html>

From viktor.klang at gmail.com  Tue Aug  9 11:37:32 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 9 Aug 2011 17:37:32 +0200
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CAHzJPEoq1xN0y3YRy_vN4WXXHKkG+DUdbpS6-Rnnswsc3z78fg@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
	<CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
	<CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>
	<CAHzJPEoq1xN0y3YRy_vN4WXXHKkG+DUdbpS6-Rnnswsc3z78fg@mail.gmail.com>
Message-ID: <CANPzfU-i5qULK+00PWs8ktovpNQPGJEbATdFTebTAk-e61miGg@mail.gmail.com>

On Tue, Aug 9, 2011 at 5:16 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Ah, sorry, I was familiar with continuations, but not with CPS
> transformation.
>
> Is there a favorite reference you can recommend?
>
Scala has a CPS transformation compiler plugin. In Akka we've implemented
our Dataflow API using that:

Usage: http://akka.io/docs/akka/1.1.3/scala/dataflow.html#id3

the "flow" construct
https://github.com/jboner/akka/blob/v1.2-RC2/akka-actor/src/main/scala/akka/dispatch/Future.scala#L313

reading the value of a dataflow variable
https://github.com/jboner/akka/blob/v1.2-RC2/akka-actor/src/main/scala/akka/dispatch/Future.scala#L363

writing the value of a dataflow variable
https://github.com/jboner/akka/blob/v1.2-RC2/akka-actor/src/main/scala/akka/dispatch/Future.scala#L723

So calls to apply and << inside a "flow"-block will be CPS transformed,
which in this case gives you code that looks imperative, but is actually
asynchronous and non-blocking.

Cheers,
?



> On Aug 9, 2011 8:09 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:
> > What? No chickens?
> >
> > Do you mean CSP - communicating sequential processes?
> > On Aug 9, 2011 2:28 AM, "?iktor ?lang" <viktor.klang at gmail.com> wrote:
> >> What about CPS transformation?
> >>
> >> On Mon, Aug 8, 2011 at 10:24 PM, Charles Oliver Nutter
> >> <headius at headius.com>wrote:
> >>
> >>> On Mon, Aug 8, 2011 at 3:30 PM, Ron Pressler <ron.pressler at gmail.com>
> >>> wrote:
> >>> > If each fiber is represented by a thread, then any transfer of
> control
> >>> > between fibers would require an OS task switch, which is bound to be
> >>> slower
> >>> > than C Ruby's lightweight threads. However, the erjang project has
> >>> > implemented erlang on top of the JVM, and erlang requires lots of
> > "fiber"
> >>> > switches for its actors. The way they did it is by using a modified
> >>> version
> >>> > of Kilim , which uses bytecode instrumentation to implement
> > continuations
> >>> > with stack-capture - the same way C Ruby does it, I presume.
> >>>
> >>> This would work, but the execution performance of bytecode
> >>> instrumentation would almost certainly be worse than what we have now.
> >>> Kilim-style bytecode manipulation also only works if you can
> >>> instrument all the code you're passing through (leaves excluded),
> >>> which would mean almost all of JRuby would need to be instrumented in
> >>> this way.
> >>>
> >>> I would also wager there's no change of inlining code that's been
> >>> instrumented...or at least a large class of optimizations would not
> >>> work in the presence of stack trampolines.
> >>>
> >>> > Scala uses java's Fork/Join tasks for its actor scheduling, and it
> > sounds
> >>> > like you might be able to use that too. You transfer control to
> another
> >>> > fiber with fork, and block yourself with join.
> >>>
> >>> I'm essentially doing this now with park/unpark, but I'll look into
> >>> whether Fork/Join makes it cleaner.
> >>>
> >>> > But in any case, for best performance you will probably have to
> abandon
> >>> the
> >>> > one-thread-per-fiber model (which is also expensive on memory)
> >>>
> >>> Yes, I'd love to :) But unfortunately there's no way to do that on the
> >>> JVM right now.
> >>>
> >>> Perhaps this is a good time to lobby for concurrency-interest's
> >>> support in Lukas Stadler's JVM coroutines. He implemented them for
> >>> OpenJDK as part of his PhD work, and they provide much faster Fiber
> >>> context-switching than even the C implementations of Ruby. I believe
> >>> he's looking to launch a JSR soon...and it seems like
> >>> pausable/resumable workers (in the form of coroutines) might be a
> >>> useful feature for concurrency (especially atop blocking library calls
> >>> that would steal a thread forever). Imagine having a worker pool that
> >>> only has N threads for M > N running worker coroutines, and uses
> >>> coroutine suspend/resume to choose between them as necessary.
> >>>
> >>> - Charlie
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >>
> >>
> >> --
> >> Viktor Klang
> >>
> >> Akka Tech Lead
> >> Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
> >> Experts
> >>
> >> Twitter: @viktorklang
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110809/cdea45b5/attachment-0001.html>

From headius at headius.com  Tue Aug  9 14:22:48 2011
From: headius at headius.com (Charles Oliver Nutter)
Date: Tue, 9 Aug 2011 14:22:48 -0400
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CANPzfU-i5qULK+00PWs8ktovpNQPGJEbATdFTebTAk-e61miGg@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
	<CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
	<CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>
	<CAHzJPEoq1xN0y3YRy_vN4WXXHKkG+DUdbpS6-Rnnswsc3z78fg@mail.gmail.com>
	<CANPzfU-i5qULK+00PWs8ktovpNQPGJEbATdFTebTAk-e61miGg@mail.gmail.com>
Message-ID: <CAE-f1xTXDptoRHLpW1N90A6A6J0s0tvx=bYotp2SWyju8jYU0A@mail.gmail.com>

2011/8/9 ?iktor ?lang <viktor.klang at gmail.com>:
> Scala has a CPS transformation compiler plugin. In Akka we've implemented
> our Dataflow API using that:

Because Ruby is dynamically-dispatched, we can't see what methods
might be used downstream from a Fiber. We'd have to instrument or CPS
transform every called-through method in JRuby.

The Scala trick is cute, but only works where you can statically
determine downstream calls and transform them.

JVM really needs a general-purpose coroutine capability.

- Charlie


From viktor.klang at gmail.com  Tue Aug  9 14:53:47 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 9 Aug 2011 20:53:47 +0200
Subject: [concurrency-interest] Fast control transfer from one thread to
	another?
In-Reply-To: <CAE-f1xTXDptoRHLpW1N90A6A6J0s0tvx=bYotp2SWyju8jYU0A@mail.gmail.com>
References: <CAE-f1xTe8S0B0psYn4Oqit4Vt0NFmHtXoYty_0x9pTnLhuJnHg@mail.gmail.com>
	<CABg6-qgJDObrEWxy5Y4v-9hf6DqMDCG1UeZ+ZmZsaH_BffyZ6w@mail.gmail.com>
	<CAE-f1xT8q1gCZimQOXmXgK1=_PZq+gssd_93+_AkVnzFWHHvWQ@mail.gmail.com>
	<CANPzfU-RHjaAZ-JKdXZOJBsN6Zs5jTZNfvuLRNDC70DY2mzz3g@mail.gmail.com>
	<CAHzJPErouKy6CHk4qsT-OCDEaWBhx5ERwDYCuVekE6jhT+T2Lg@mail.gmail.com>
	<CAHzJPEoq1xN0y3YRy_vN4WXXHKkG+DUdbpS6-Rnnswsc3z78fg@mail.gmail.com>
	<CANPzfU-i5qULK+00PWs8ktovpNQPGJEbATdFTebTAk-e61miGg@mail.gmail.com>
	<CAE-f1xTXDptoRHLpW1N90A6A6J0s0tvx=bYotp2SWyju8jYU0A@mail.gmail.com>
Message-ID: <CANPzfU_VjkgPGyBFOAAkfoaGRXCdqO0Sdu2oW=T+o5TizasPug@mail.gmail.com>

2011/8/9 Charles Oliver Nutter <headius at headius.com>

> 2011/8/9 ?iktor ?lang <viktor.klang at gmail.com>:
> > Scala has a CPS transformation compiler plugin. In Akka we've implemented
> > our Dataflow API using that:
>
> Because Ruby is dynamically-dispatched, we can't see what methods
> might be used downstream from a Fiber. We'd have to instrument or CPS
> transform every called-through method in JRuby.
>
> The Scala trick is cute, but only works where you can statically
> determine downstream calls and transform them.
>
> JVM really needs a general-purpose coroutine capability.
>

I'd settle for JVM-level continuations, but my personal favorite would be
isolated threads (extra sugar on top for ability to control the scheduling)


>
> - Charlie
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110809/7e5ad08b/attachment.html>

From nathan.reynolds at oracle.com  Wed Aug 10 12:37:49 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Aug 2011 09:37:49 -0700
Subject: [concurrency-interest] Thoughts about LongAdder
Message-ID: <4E42B3DD.7050107@oracle.com>

By way of introduction, I work at Oracle in the PSR team (Performance, 
Scalability and Reliability).  This team is focuses on optimizing the 
applications and middleware.  My task is to optimize Weblogic server, 
the JVM, kernel and network for Oracle's Exalogic, a rack of Westmere-EP 
servers.  Most of my focus is on Weblogic server and some what on the 
other areas as needed.

Monday, I came up with a "new" data structure, ConcurrentCounter, to 
take advantage of a processor id API I proposed to the Oracle HotSpot 
team.  Alan Bateman sent me a link to an email in this list to 
LongAdder.  I reviewed the LongAdder implementation and learned a bit 
from it.  I enhanced ConcurrentCounter based on some of the ideas in 
LongAdder.  Here's a list of ideas of where LongAdder and 
ConcurrentCounter differ.  Hopefully, this list will help improve the 
final result .

 1. I would recommend striping the counter *not* by thread but by
    processor/core.  This will result in almost 0 contention and
    significantly reduce cache coherence traffic.  If striped by thread,
    then the counter will probably consume more memory, the thread to
    Cell mapping may not stabilize and when a thread migrates to another
    processor, the cache lines have to be transferred.  If striped by
    processor, then thread migration has no impact (unless it was in the
    middle of updating a Cell).  Because of this, striping by processor
    will outperform striping by thread.  (More on this in another
    email.)  Also, the number of Cells required will be fixed to the
    number of processors which is almost always fewer than the number of
    threads in server processes.  The attached ConcurrentCounter stripes
    by processor.
 2. LongAdder pads each Cell to avoid false sharing.  Good idea. 
    However, the amount of memory required can be quite costly in terms
    of capacity, wasting cache space and bandwidth.  ConcurrentCounter
    (attached) assumes the JVM allocator is NUMA aware.  Thus, each
    "Cell" will be allocated in memory local to the processor.  False
    sharing shouldn't be possible and if it happens then the contention
    will only happen among sibling cores.  If GC is NUMA aware, then
    each "Cell" will remain in local memory.  Again, false sharing isn't
    likely to be a problem.  Thus, padding is hopefully not needed.  I
    am interested in results showing padding vs no padding... assuming
    all of the JVM requirements are met.
 3. reset() should replace the array of Cell instead of setting the
    values to 0.  This will increase GC and allocator load but reset()
    can then be atomic.  ConcurrentCounter.clear() implements this logic
    quite simply.
 4. getAndReset() should also replace the array of Cell.  The get() can
    be computed on the old array of Cell and should be much more
    accurate since the concurrent updates should start using the new
    array right away.  The inaccuracies will come from threads that
    context switched off while in the middle of an update and then
    resume later using the old array.  ConcurrentCounter.getAndClear()
    does this.
 5. ConcurrentCounter supports a set() API.  It basically does the
    reset() trick I mentioned above but the new set of "Cells" is
    already assigned the given value.
 6. ConcurrentCounter executes only 1 atomic instruction per update (in
    the main path) and never uses non-blocking locks.
 7. retryUpdate() is extremely complicated code (due to thread striping
    and balancing).  ConcurrentCounter doesn't have such complexity.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/9c2b44f5/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ConcurrentCounter.java
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/9c2b44f5/attachment.ksh>

From nathan.reynolds at oracle.com  Wed Aug 10 12:38:03 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Aug 2011 09:38:03 -0700
Subject: [concurrency-interest] ThreadLocal vs ProcessorLocal
In-Reply-To: <4E41A40D.6080606@oracle.com>
References: <4DEE725C.9040503@oracle.com> <4DEE75C6.9070609@oracle.com>
	<4DEE8589.4060907@oracle.com> <4DEE9E09.4050201@oracle.com>
	<4DEEA6D8.7020706@oracle.com> <4DEEB0DD.4070803@oracle.com>
	<4DF0F786.8020309@oracle.com> <4DF0F9E6.1080403@oracle.com>
	<4DF114A8.2080209@oracle.com> <4DF1175E.5090606@oracle.com>
	<BF13EC2E-92E3-47A0-B00F-F4BF943E6F6B@oracle.com>
	<4DF23675.7060309@oracle.com> <4DF2A9A9.2050401@oracle.com>
	<4DF33074.5080403@oracle.com> <4DF69514.4020204@oracle.com>
	<4DF746D4.3080800@oracle.com> <4DF78AB9.9090700@oracle.com>
	<4E176BDC.5070202@oracle.com> <4E404239.5080509@oracle.com>
	<4E416651.6030909@oracle.com> <4E416FE1.2090101@oracle.com>
	<4E417440.6000704@oracle.com> <4E41A242.1060004@oracle.com>
	<4E41A40D.6080606@oracle.com>
Message-ID: <4E42B3EB.2060206@oracle.com>

I would like to recommend that we stripe data structures using a 
ProcessorLocal instead of ThreadLocal.  ProcessorLocal (attached) is 
exactly like ThreadLocal except the stored objects keyed off of the 
processor instead of the thread.  In order to implement ProcessorLocal, 
it needs an API that returns the current processor id that the thread is 
running on.  The HotSpot team has filed an RFE and are planning on 
providing such an API.  (Many of you are already aware of this.)

I would like to share a story and some results to further the discussion 
on processor striping (i.e. ProcessorLocal).

A long time ago, we found that an Oracle C++ program bottlenecked on a 
reader/writer lock.  Threads couldn't read-acquire the lock fast 
enough.  The problem was due to the contention on the cache line while 
executing the CAS instruction.  So, I striped the lock.  The code 
non-atomically incremented an int and masked it to select one of the 
reader/writer locks.  Multiple threads could end up selecting the same 
reader/writer lock because the int was incremented in an unprotected 
manner.  If multiple threads selected the same reader/writer lock, the 
lock would handle the concurrency and the only negative was lock 
performance.  This optimization worked great until Intel released 
Nehalem-EX.

A while ago, Intel found on Nehalem-EX that the same Oracle C++ program 
didn't scale to the 4?? Nehalem socket.  All of the processors/cores 
were 100% busy, but the throughput didn't improve by adding the 4?? 
Nehalem socket.  The problem was the cores were fighting to get the 
cache line holding the unprotected int!

I tried 4 approaches to select the reader/writer lock.

1) Processor id - This performed the best.  The cache lines holding the 
reader/writer locks are almost never invalidated due to another core 
accessing the reader/writer lock.  In other words, almost 0 CAS contention.
2) ThreadLocal - ThreadLocal had a 1:1 mapping of threads to locks.  It 
required too many locks and the locks had to migrate with the threads.
3) Hash the stack pointer - Hashing the stack pointer caused some 
collisions but essentially randomly selected locks and this hurt cache 
performance.
4) Shift and mask the cycle counter (i.e. rdtsc) - Contention was rare 
but again it randomly selected the locks.

Compared to non-atomically incrementing an int, processor id resulted in 
15% more throughput.  The other 3 only showed 5% more throughput.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/80cfbc4c/attachment-0001.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ProcessorLocal.java
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/80cfbc4c/attachment-0001.ksh>

From david.lloyd at redhat.com  Wed Aug 10 13:55:30 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 10 Aug 2011 12:55:30 -0500
Subject: [concurrency-interest] ThreadLocal vs ProcessorLocal
In-Reply-To: <4E42B3EB.2060206@oracle.com>
References: <4DEE725C.9040503@oracle.com>
	<4DEE75C6.9070609@oracle.com>	<4DEE8589.4060907@oracle.com>
	<4DEE9E09.4050201@oracle.com>	<4DEEA6D8.7020706@oracle.com>
	<4DEEB0DD.4070803@oracle.com>	<4DF0F786.8020309@oracle.com>
	<4DF0F9E6.1080403@oracle.com>	<4DF114A8.2080209@oracle.com>
	<4DF1175E.5090606@oracle.com>	<BF13EC2E-92E3-47A0-B00F-F4BF943E6F6B@oracle.com>	<4DF23675.7060309@oracle.com>
	<4DF2A9A9.2050401@oracle.com>	<4DF33074.5080403@oracle.com>
	<4DF69514.4020204@oracle.com>	<4DF746D4.3080800@oracle.com>
	<4DF78AB9.9090700@oracle.com>	<4E176BDC.5070202@oracle.com>
	<4E404239.5080509@oracle.com>	<4E416651.6030909@oracle.com>
	<4E416FE1.2090101@oracle.com>	<4E417440.6000704@oracle.com>
	<4E41A242.1060004@oracle.com>	<4E41A40D.6080606@oracle.com>
	<4E42B3EB.2060206@oracle.com>
Message-ID: <4E42C612.7000509@redhat.com>

On 08/10/2011 11:38 AM, Nathan Reynolds wrote:
> I would like to recommend that we stripe data structures using a
> ProcessorLocal instead of ThreadLocal.  ProcessorLocal (attached) is
> exactly like ThreadLocal except the stored objects keyed off of the
> processor instead of the thread.  In order to implement ProcessorLocal,
> it needs an API that returns the current processor id that the thread is
> running on.  The HotSpot team has filed an RFE and are planning on
> providing such an API.  (Many of you are already aware of this.)
[...]

A couple questions/comments - first, using CHM would seem to undermine a 
lot of the potential of ProcessorLocal due to its relatively heavy 
weight and extra locking.  You avoid arrays due to false sharing, yet it 
seems to me that the array could be padded out, or you could use an 
array of holder objects, etc. and save both memory and unnecessary 
processing in the CHM.

Second, is there any provision for the number of CPUs changing at runtime?

-- 
- DML

From nathan.reynolds at oracle.com  Wed Aug 10 18:50:51 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Aug 2011 15:50:51 -0700
Subject: [concurrency-interest] ThreadLocal vs ProcessorLocal
In-Reply-To: <4E42C612.7000509@redhat.com>
References: <4DEE725C.9040503@oracle.com>
	<4DEE75C6.9070609@oracle.com>	<4DEE8589.4060907@oracle.com>
	<4DEE9E09.4050201@oracle.com>	<4DEEA6D8.7020706@oracle.com>
	<4DEEB0DD.4070803@oracle.com>	<4DF0F786.8020309@oracle.com>
	<4DF0F9E6.1080403@oracle.com>	<4DF114A8.2080209@oracle.com>
	<4DF1175E.5090606@oracle.com>	<BF13EC2E-92E3-47A0-B00F-F4BF943E6F6B@oracle.com>	<4DF23675.7060309@oracle.com>
	<4DF2A9A9.2050401@oracle.com>	<4DF33074.5080403@oracle.com>
	<4DF69514.4020204@oracle.com>	<4DF746D4.3080800@oracle.com>
	<4DF78AB9.9090700@oracle.com>	<4E176BDC.5070202@oracle.com>
	<4E404239.5080509@oracle.com>	<4E416651.6030909@oracle.com>
	<4E416FE1.2090101@oracle.com>	<4E417440.6000704@oracle.com>
	<4E41A242.1060004@oracle.com>	<4E41A40D.6080606@oracle.com>
	<4E42B3EB.2060206@oracle.com> <4E42C612.7000509@redhat.com>
Message-ID: <4E430B4B.9060009@oracle.com>

Thank you for your questions David.  They got me thinking and it 
triggered another round of optimization.  I have attached a new version 
of ProcessorLocal which replaces ConcurrentHashMap with an array.  It 
uses ProcessorIndex (attached) to map processor ids to indexes.

I wrote ProcessorIndex a while ago but it used ConcurrentHashMap.  The 
original version of ProcessorLocal didn't use ProcessorIndex since it 
would require 2 ConcurrentHashMap.get() calls.  1 call to translate 
processor id into index inside ProcessorLocal.  1 call to map index into 
holder.  (I hadn't considered a copy-on-write array.)

Note: Not all (if any) hardware has an instruction or memory location 
which returns a processor index between 0 and N - 1 where N is the 
number of processors.  For example, x86's processor id is a 32-bit 
integer and the values can be spread across the entire 32-bit range.

I was able to come up with a better way to write ProcessorIndex.  The 
new version uses a HashMap (not ConcurrentHashMap) to translate 
processor ids to indexes.  If a processor id is not found in the 
HashMap, then it uses copy-on-write to replace the old HashMap with a 
new HashMap that has the discovered processor id.  The number of 
copy-on-writes is limited to the number of processors.  After warming 
up, we get HashMap read speed and continue supporting the ability to 
handle new processors.

If processors are added or removed from the system (e.g. hot swapping, 
processor affinity mask, virtual machine migration, virtual CPU 
reassignment), ProcessorIndex will simply add more entries to its 
internal HashMap.  The stale entries are kept.  Data structures that 
rely on the index for NUMA affinity will become less efficient if the 
indexes were rearranged.  Thus, data structures should call 
ProcessorIndex.reset() at ideal opportunities (e.g. when clear() is called).

Do the holders need padding?  I am not sure.  If the JVM's allocator and 
GC is NUMA aware, then false sharing should hopefully not happen since 
each element is allocated by the processor that will use it.  The 
allocator and GC will keep each element in memory local to the processor 
and not next to each other.  I suppose false sharing among sibling cores 
could happen but the impact is much less significant than false sharing 
among sockets.  If false sharing is a problem, then using padded holder 
objects will solve that problem.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/67d0a18a/attachment-0001.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ProcessorIndex.java
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/67d0a18a/attachment-0002.ksh>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ProcessorLocal.java
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110810/67d0a18a/attachment-0003.ksh>

From jeffhain at rocketmail.com  Sat Aug 13 18:57:45 2011
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sat, 13 Aug 2011 23:57:45 +0100 (BST)
Subject: [concurrency-interest]  concurrent counter : incrementAndGet
Message-ID: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>

Hello.

   To continue on the subject of concurrent counters, does anyone
know one that could be used instead of AtomicLong.incrementAndGet()
(possibly providing non-monotonic results, or monotonic but with gaps),
and would scale better?

   I tried to do something in that direction, but it's sometimes actually worse
(which still surprises me: I'm not totally aware of what I carefully made yet 
;).

Regards,

Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110813/8259cd60/attachment.html>

From aleksey.shipilev at gmail.com  Sat Aug 13 19:13:57 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Sun, 14 Aug 2011 03:13:57 +0400
Subject: [concurrency-interest] concurrent counter : incrementAndGet
In-Reply-To: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
Message-ID: <CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>

You might want to get the idea from JPA-s sequencers like this: get
each caller thread its own local range of numbers it can count, once
thread depletes the region, atomically allocate another region. You
can then greatly reduce contention by controlling region range. This
is still provide unique values in every thread, with some
non-monotonic perturbations.

-Aleksey.

On Sun, Aug 14, 2011 at 2:57 AM, Jeff Hain <jeffhain at rocketmail.com> wrote:
> Hello.
>
> ?? To continue on the subject of concurrent counters, does anyone
> know one that could be used instead of AtomicLong.incrementAndGet()
> (possibly providing non-monotonic results, or monotonic but with gaps),
> and would scale better?
>
> ?? I tried to do something in that direction, but it's sometimes actually
> worse
> (which still surprises me: I'm not totally aware of what I carefully made
> yet ;).
>
> Regards,
>
> Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From jeffhain at rocketmail.com  Sun Aug 14 07:03:06 2011
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sun, 14 Aug 2011 12:03:06 +0100 (BST)
Subject: [concurrency-interest] Re :  concurrent counter : incrementAndGet
In-Reply-To: <CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
Message-ID: <1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>


   That could work well for some cases indeed, but I'm working on a sort
of Disruptor, using the counter with modulo to pick up slots to write to in
a cyclic array, monotonically, or non-monotonically but for a short time
(for readers not to be blocked on a not-yet-written slot), and any writer
can stop to work anytime; in this case I don't see how that could apply
easily.

-Jeff




________________________________
De : Aleksey Shipilev <aleksey.shipilev at gmail.com>
? : Jeff Hain <jeffhain at rocketmail.com>
Cc : concurrency-interest at cs.oswego.edu
Envoy? le : Dim 14 ao?t 2011, 1h 13min 57s
Objet : Re: [concurrency-interest] concurrent counter : incrementAndGet

You might want to get the idea from JPA-s sequencers like this: get
each caller thread its own local range of numbers it can count, once
thread depletes the region, atomically allocate another region. You
can then greatly reduce contention by controlling region range. This
is still provide unique values in every thread, with some
non-monotonic perturbations.

-Aleksey.

On Sun, Aug 14, 2011 at 2:57 AM, Jeff Hain <jeffhain at rocketmail.com> wrote:
> Hello.
>
>    To continue on the subject of concurrent counters, does anyone
> know one that could be used instead of AtomicLong.incrementAndGet()
> (possibly providing non-monotonic results, or monotonic but with gaps),
> and would scale better?
>
>    I tried to do something in that direction, but it's sometimes actually
> worse
> (which still surprises me: I'm not totally aware of what I carefully made
> yet ;).
>
> Regards,
>
> Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110814/d71a00f6/attachment.html>

From jwesleysmith at atlassian.com  Mon Aug 15 23:04:37 2011
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Tue, 16 Aug 2011 13:04:37 +1000
Subject: [concurrency-interest] Re : concurrent counter : incrementAndGet
In-Reply-To: <1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
Message-ID: <CAKh+yi9BryJ6TmVKXpddPfOzOvwV3yhF+m0L7kQQFwVfY8gHoA@mail.gmail.com>

have a look at Cliff Click's ConcurrentAutoTable (github fork link for easy
viewing):

https://github.com/boundary/high-scale-lib/blob/master/src/main/java/org/cliffc/high_scale_lib/ConcurrentAutoTable.java

original source in sourceforge:

Highly Scalable Java | Download Highly Scalable Java software for
...<http://sourceforge.net/projects/high-scale-lib/>

On Sun, Aug 14, 2011 at 9:03 PM, Jeff Hain <jeffhain at rocketmail.com> wrote:

>
>    That could work well for some cases indeed, but I'm working on a sort
> of Disruptor, using the counter with modulo to pick up slots to write to in
> a cyclic array, monotonically, or non-monotonically but for a short time
> (for readers not to be blocked on a not-yet-written slot), and any writer
> can stop to work anytime; in this case I don't see how that could apply
> easily.
>
> -Jeff
>
> ------------------------------
> *De :* Aleksey Shipilev <aleksey.shipilev at gmail.com>
> *? :* Jeff Hain <jeffhain at rocketmail.com>
> *Cc :* concurrency-interest at cs.oswego.edu
> *Envoy? le :* Dim 14 ao?t 2011, 1h 13min 57s
> *Objet :* Re: [concurrency-interest] concurrent counter : incrementAndGet
>
> You might want to get the idea from JPA-s sequencers like this: get
> each caller thread its own local range of numbers it can count, once
> thread depletes the region, atomically allocate another region. You
> can then greatly reduce contention by controlling region range. This
> is still provide unique values in every thread, with some
> non-monotonic perturbations.
>
> -Aleksey.
>
> On Sun, Aug 14, 2011 at 2:57 AM, Jeff Hain <jeffhain at rocketmail.com>
> wrote:
> > Hello.
> >
> >    To continue on the subject of concurrent counters, does anyone
> > know one that could be used instead of AtomicLong.incrementAndGet()
> > (possibly providing non-monotonic results, or monotonic but with gaps),
> > and would scale better?
> >
> >    I tried to do something in that direction, but it's sometimes actually
> > worse
> > (which still surprises me: I'm not totally aware of what I carefully made
> > yet ;).
> >
> > Regards,
> >
> > Jeff
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110816/e58499eb/attachment.html>

From jeffhain at rocketmail.com  Tue Aug 16 17:24:20 2011
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Tue, 16 Aug 2011 22:24:20 +0100 (BST)
Subject: [concurrency-interest] Re : Re : concurrent counter :
	incrementAndGet
In-Reply-To: <CAKh+yi9BryJ6TmVKXpddPfOzOvwV3yhF+m0L7kQQFwVfY8gHoA@mail.gmail.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
	<CAKh+yi9BryJ6TmVKXpddPfOzOvwV3yhF+m0L7kQQFwVfY8gHoA@mail.gmail.com>
Message-ID: <1313529860.77124.YahooMailRC@web29203.mail.ird.yahoo.com>

   Unfortunately, CAT doesn't have an atomic incrementAndGet
operation. Just "increment" and then "get", which can return
multiple times a same result.

-Jeff



________________________________
De : Jed Wesley-Smith <jwesleysmith at atlassian.com>
? : Jeff Hain <jeffhain at rocketmail.com>
Cc : concurrency-interest at cs.oswego.edu
Envoy? le : Mar 16 ao?t 2011, 5h 04min 37s
Objet : Re: [concurrency-interest] Re : concurrent counter : incrementAndGet

have a look at Cliff Click's ConcurrentAutoTable (github fork link for easy 
viewing):

https://github.com/boundary/high-scale-lib/blob/master/src/main/java/org/cliffc/high_scale_lib/ConcurrentAutoTable.java



original source in sourceforge:

Highly Scalable Java | Download Highly Scalable Java software for ...


On Sun, Aug 14, 2011 at 9:03 PM, Jeff Hain <jeffhain at rocketmail.com> wrote:


>   That could work well for some cases indeed, but I'm working on a sort
>of Disruptor, using the counter with modulo to pick up slots to write to in
>a cyclic array, monotonically, or non-monotonically but for a short time
>(for readers not to be blocked on a not-yet-written slot), and any writer
>can stop to work anytime; in this case I don't see how that could apply
>easily.
>
>-Jeff
>
>
>
>
________________________________
De : Aleksey Shipilev <aleksey.shipilev at gmail.com>
>? : Jeff Hain  <jeffhain at rocketmail.com>
>Cc : concurrency-interest at cs.oswego.edu
>Envoy? le : Dim 14 ao?t 2011, 1h 13min 57s
>Objet : Re: [concurrency-interest] concurrent counter : incrementAndGet
>
>
>You might want to get the idea from JPA-s sequencers like this: get
>each caller thread its own local range of numbers it can count, once
>thread depletes the region, atomically allocate another region. You
>can then greatly reduce contention by controlling region range. This
>is still provide unique values in every thread, with some
>non-monotonic perturbations.
>
>-Aleksey.
>
>On Sun, Aug 14, 2011 at 2:57 AM, Jeff Hain <jeffhain at rocketmail.com> wrote:
>> Hello.
>>
>>     To continue on the subject of concurrent counters, does anyone
>> know one that could be used instead of AtomicLong.incrementAndGet()
>> (possibly providing non-monotonic results, or monotonic but with gaps),
>> and would scale better?
>>
>>    I tried to do something in that direction, but it's sometimes actually
>> worse
>> (which still surprises me: I'm not totally aware of what I carefully made
>> yet ;).
>>
>> Regards,
>>
>> Jeff
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110816/90a9668d/attachment.html>

From djg at cs.washington.edu  Tue Aug 16 18:00:31 2011
From: djg at cs.washington.edu (Dan Grossman)
Date: Tue, 16 Aug 2011 15:00:31 -0700
Subject: [concurrency-interest] java fork-join getting-started notes for
	beginners <-> Java 7
Message-ID: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>

Short version:

I'm looking for quick confirmation that using the fork-join framework
with the Java 7 JRE is just as easy as it seems and that I'm pointing
students to the right stable versions of things.

Long version:

Background:

As I've mentioned on this list a couple times, I've developed a
course-unit for second-year undergraduates that introduces parallelism
and concurrency using Java and the Fork-Join Framework (though it's
not really that Java-specific).  At Washington, we've used this unit
in our required data-structures course for 1.5 years now and it's been
picked up by 5 other schools so far.  In all, 10 instructors, most
non-experts in Java, parallelism, or both have used it and they all
claim success and, "I will do this again."  For more information,
http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/

One thing that has proven absolutely essential is step-by-step
instructions suitable for beginners, specialized to just what they
need: ForkJoinPool, RecursiveTask, RecursiveAction.  This was
particularly important for Java 1.6.  The url
http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
has these instructions and was last updated a few months ago.  For
those of you who have not taught undergraduates, let me assure you
that there are, nonetheless, a mind-boggling number of ways to enter
-Xbootclasspath/p:jsr166.jar incorrectly. :-)

So what now:

It seems time to update my step-by-step instructions to say:
  1. Please use Java 7 following steps a, b, c.
  2. If you really can't, then here are the more complicated steps for
using Java 6 following steps, d, e, f, g.

In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
machine that has never had Java on it, installed Eclipse IDE for Java
Developers, indigo release (my instructions prefer but don't mandate
eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
file.  It Just Worked.  This is So Wonderful and I send my heartfelt
appreciation to everyone on this list who helped make it happen.

Now my questions -- I think the answers are all 'yes' but this is the
place to confirm and I'm most concerned about (C):

A. Java 7: Is this the real deal -- the framework will use the
available processors and, after suitable VM warmup, be the parallel
execution engine we expect?

B. Installation: Will upgrading on machines that already have Java 6
be just as seamless?

C. Code: Is the attached file the way to show things to beginners?
(Note: My point is to show them the reduction explicitly rather than
using a library method.  This is for pedagogical purposes.  So no
complaining about that.)

D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
"The 1.7 compiler compliance level
   is not yet supported.  The new project will use a project specific
compiler compliance level of 1.6".  Am I correct that this can be
ignored since I'm not using any new /language/ features, just a new
/library/? (Note: My understanding is there are Eclipse versions
available with 1.7 compilers, but if we're okay with the most standard
most stable Eclipse release, this is extremely helpful.)

E: Anything else I can do to make this as bullet-proof for beginners
as possible?

Thanks!

--Dan
-------------- next part --------------
A non-text attachment was scrubbed...
Name: SumTest.java
Type: application/octet-stream
Size: 1349 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110816/3b1616bf/attachment.obj>

From viktor.klang at gmail.com  Wed Aug 17 05:12:20 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 17 Aug 2011 11:12:20 +0200
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
In-Reply-To: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
References: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
Message-ID: <CANPzfU9pQe10g7XeTBHdNKavJBhQmuhD_VB8eE1bVS9YZ2W+5w@mail.gmail.com>

Hi Dan,

have you've considered introducing them to a less explicit form of
concurrency like Actors, ActiveObjects or Dataflow?

Cheers,
?

On Wed, Aug 17, 2011 at 12:00 AM, Dan Grossman <djg at cs.washington.edu>wrote:

> Short version:
>
> I'm looking for quick confirmation that using the fork-join framework
> with the Java 7 JRE is just as easy as it seems and that I'm pointing
> students to the right stable versions of things.
>
> Long version:
>
> Background:
>
> As I've mentioned on this list a couple times, I've developed a
> course-unit for second-year undergraduates that introduces parallelism
> and concurrency using Java and the Fork-Join Framework (though it's
> not really that Java-specific).  At Washington, we've used this unit
> in our required data-structures course for 1.5 years now and it's been
> picked up by 5 other schools so far.  In all, 10 instructors, most
> non-experts in Java, parallelism, or both have used it and they all
> claim success and, "I will do this again."  For more information,
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
>
> One thing that has proven absolutely essential is step-by-step
> instructions suitable for beginners, specialized to just what they
> need: ForkJoinPool, RecursiveTask, RecursiveAction.  This was
> particularly important for Java 1.6.  The url
>
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
> has these instructions and was last updated a few months ago.  For
> those of you who have not taught undergraduates, let me assure you
> that there are, nonetheless, a mind-boggling number of ways to enter
> -Xbootclasspath/p:jsr166.jar incorrectly. :-)
>
> So what now:
>
> It seems time to update my step-by-step instructions to say:
>  1. Please use Java 7 following steps a, b, c.
>  2. If you really can't, then here are the more complicated steps for
> using Java 6 following steps, d, e, f, g.
>
> In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
> machine that has never had Java on it, installed Eclipse IDE for Java
> Developers, indigo release (my instructions prefer but don't mandate
> eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
> file.  It Just Worked.  This is So Wonderful and I send my heartfelt
> appreciation to everyone on this list who helped make it happen.
>
> Now my questions -- I think the answers are all 'yes' but this is the
> place to confirm and I'm most concerned about (C):
>
> A. Java 7: Is this the real deal -- the framework will use the
> available processors and, after suitable VM warmup, be the parallel
> execution engine we expect?
>
> B. Installation: Will upgrading on machines that already have Java 6
> be just as seamless?
>
> C. Code: Is the attached file the way to show things to beginners?
> (Note: My point is to show them the reduction explicitly rather than
> using a library method.  This is for pedagogical purposes.  So no
> complaining about that.)
>
> D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
> "The 1.7 compiler compliance level
>   is not yet supported.  The new project will use a project specific
> compiler compliance level of 1.6".  Am I correct that this can be
> ignored since I'm not using any new /language/ features, just a new
> /library/? (Note: My understanding is there are Eclipse versions
> available with 1.7 compilers, but if we're okay with the most standard
> most stable Eclipse release, this is extremely helpful.)
>
> E: Anything else I can do to make this as bullet-proof for beginners
> as possible?
>
> Thanks!
>
> --Dan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110817/f7213631/attachment-0001.html>

From 1001759L at student.gla.ac.uk  Wed Aug 17 11:38:25 2011
From: 1001759L at student.gla.ac.uk (WING HANG LI)
Date: Wed, 17 Aug 2011 16:38:25 +0100
Subject: [concurrency-interest] Fork Join Profiler for Masters Project
Message-ID: <7AB2A399CAFFA442A6262BF9939AC33B369B748331@CMS07.campus.gla.ac.uk>

Hello
My name is Wing Hang Li and I am a student studying an MSc in Information Technology at Glasgow University. For my Masters Project I have developed a Java Fork Join Profiler which aims to expose some of the behaviour of programs using the Fork Join library. It does this using an instrumented version of the jsr166y library and other tools like HProf. Currently it can show the number of tasks within the Fork Join pool, the mean and standard deviation of those tasks within the worker threads, the amount of garbage collection taking place and the amount of activity within each method. The profiler generates a series of text files which are used by a chart display program to show charts of the programs behaviour during execution.
I need some volunteers to help test and evaluate my profiler as part of my project. If you have time, please download the profiler and tell me what you think of it. The profiler comes in different versions for Java 6 and 7.
For Java 6:
http://dl.dropbox.com/u/18375884/Profiler%20Java%206.zip
For Java 7:
http://dl.dropbox.com/u/18375884/Profiler%20Java%207.zip
The zip contains the profiler files, a ReadMe and a MergeSort program which uses the Fork Join library. A tutorial is also included which provides step-by-step instruction to profile the sample program. If you have any comments or suggestions then please send them to me at:
1001759l at student.gla.ac.uk
or complete a short survey on the profiler at:
https://www.surveymonkey.com/s/ForkJoinProfilerSurvey
Thank you for any feedback you give!

Wing Hang Li
1001759L at student.gla.ac.uk
winghang.li at gmail.com

From dl at cs.oswego.edu  Thu Aug 18 15:56:38 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 18 Aug 2011 15:56:38 -0400
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
In-Reply-To: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
References: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
Message-ID: <4E4D6E76.3030800@cs.oswego.edu>

On 08/16/11 18:00, Dan Grossman wrote:
> Short version:
>
> I'm looking for quick confirmation that using the fork-join framework
> with the Java 7 JRE is just as easy as it seems and that I'm pointing
> students to the right stable versions of things.

I'm still hoping that someone else replies about most of this,
since I normally experimental run releases without IDE support.
But otherwise -- yes, things should Just Work.

-Doug

>
> Long version:
>
> Background:
>
> As I've mentioned on this list a couple times, I've developed a
> course-unit for second-year undergraduates that introduces parallelism
> and concurrency using Java and the Fork-Join Framework (though it's
> not really that Java-specific).  At Washington, we've used this unit
> in our required data-structures course for 1.5 years now and it's been
> picked up by 5 other schools so far.  In all, 10 instructors, most
> non-experts in Java, parallelism, or both have used it and they all
> claim success and, "I will do this again."  For more information,
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
>
> One thing that has proven absolutely essential is step-by-step
> instructions suitable for beginners, specialized to just what they
> need: ForkJoinPool, RecursiveTask, RecursiveAction.  This was
> particularly important for Java 1.6.  The url
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
> has these instructions and was last updated a few months ago.  For
> those of you who have not taught undergraduates, let me assure you
> that there are, nonetheless, a mind-boggling number of ways to enter
> -Xbootclasspath/p:jsr166.jar incorrectly. :-)
>
> So what now:
>
> It seems time to update my step-by-step instructions to say:
>    1. Please use Java 7 following steps a, b, c.
>    2. If you really can't, then here are the more complicated steps for
> using Java 6 following steps, d, e, f, g.
>
> In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
> machine that has never had Java on it, installed Eclipse IDE for Java
> Developers, indigo release (my instructions prefer but don't mandate
> eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
> file.  It Just Worked.  This is So Wonderful and I send my heartfelt
> appreciation to everyone on this list who helped make it happen.
>
> Now my questions -- I think the answers are all 'yes' but this is the
> place to confirm and I'm most concerned about (C):
>
> A. Java 7: Is this the real deal -- the framework will use the
> available processors and, after suitable VM warmup, be the parallel
> execution engine we expect?
>
> B. Installation: Will upgrading on machines that already have Java 6
> be just as seamless?
>
> C. Code: Is the attached file the way to show things to beginners?
> (Note: My point is to show them the reduction explicitly rather than
> using a library method.  This is for pedagogical purposes.  So no
> complaining about that.)
>
> D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
> "The 1.7 compiler compliance level
>     is not yet supported.  The new project will use a project specific
> compiler compliance level of 1.6".  Am I correct that this can be
> ignored since I'm not using any new /language/ features, just a new
> /library/? (Note: My understanding is there are Eclipse versions
> available with 1.7 compilers, but if we're okay with the most standard
> most stable Eclipse release, this is extremely helpful.)
>
> E: Anything else I can do to make this as bullet-proof for beginners
> as possible?
>
> Thanks!
>
> --Dan
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu Aug 18 16:05:59 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 18 Aug 2011 16:05:59 -0400
Subject: [concurrency-interest] Re : concurrent counter : incrementAndGet
In-Reply-To: <1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
Message-ID: <4E4D70A7.7060501@cs.oswego.edu>

On 08/14/11 07:03, Jeff Hain wrote:
>
> That could work well for some cases indeed, but I'm working on a sort
> of Disruptor, using the counter with modulo to pick up slots to write to in
> a cyclic array, monotonically, or non-monotonically but for a short time
> (for readers not to be blocked on a not-yet-written slot), and any writer
> can stop to work anytime; in this case I don't see how that could apply
> easily.

I don't think "non-monotonically but for a short time" makes
this easier, since any violations are likely to be unbounded.
You might be able to live with thread local random number generators,
so that in the long run on average you are balanced?
Otherwise, you have a version of the Counting problem described
in Herlihy & Shavit, for which all known scalable solutions
are expensive, complicated, and difficult to package as
j.u.c components. Although there are some special cases like
SNZI for detecting when counts hit the particular value of zero.

(This seems to be the main bottleneck in Disruptor-like designs.)

-Doug


From forax at univ-mlv.fr  Thu Aug 18 16:20:56 2011
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Thu, 18 Aug 2011 22:20:56 +0200
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
In-Reply-To: <4E4D6E76.3030800@cs.oswego.edu>
References: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
	<4E4D6E76.3030800@cs.oswego.edu>
Message-ID: <4E4D7428.3090602@univ-mlv.fr>

Eclipse Indigo's compiler is not updated for Java 7,
all language enhancements of Java 7 are not recognized,
that's why you have this warning, the release 3.7.1 (not yet release) 
should be Ok,
you can already test the compiler behaviour by installing the 3.8M1.

R?mi

On 08/18/2011 09:56 PM, Doug Lea wrote:
> On 08/16/11 18:00, Dan Grossman wrote:
>> Short version:
>>
>> I'm looking for quick confirmation that using the fork-join framework
>> with the Java 7 JRE is just as easy as it seems and that I'm pointing
>> students to the right stable versions of things.
>
> I'm still hoping that someone else replies about most of this,
> since I normally experimental run releases without IDE support.
> But otherwise -- yes, things should Just Work.
>
> -Doug
>
>>
>> Long version:
>>
>> Background:
>>
>> As I've mentioned on this list a couple times, I've developed a
>> course-unit for second-year undergraduates that introduces parallelism
>> and concurrency using Java and the Fork-Join Framework (though it's
>> not really that Java-specific).  At Washington, we've used this unit
>> in our required data-structures course for 1.5 years now and it's been
>> picked up by 5 other schools so far.  In all, 10 instructors, most
>> non-experts in Java, parallelism, or both have used it and they all
>> claim success and, "I will do this again."  For more information,
>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
>>
>> One thing that has proven absolutely essential is step-by-step
>> instructions suitable for beginners, specialized to just what they
>> need: ForkJoinPool, RecursiveTask, RecursiveAction.  This was
>> particularly important for Java 1.6.  The url
>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html 
>>
>> has these instructions and was last updated a few months ago.  For
>> those of you who have not taught undergraduates, let me assure you
>> that there are, nonetheless, a mind-boggling number of ways to enter
>> -Xbootclasspath/p:jsr166.jar incorrectly. :-)
>>
>> So what now:
>>
>> It seems time to update my step-by-step instructions to say:
>>    1. Please use Java 7 following steps a, b, c.
>>    2. If you really can't, then here are the more complicated steps for
>> using Java 6 following steps, d, e, f, g.
>>
>> In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
>> machine that has never had Java on it, installed Eclipse IDE for Java
>> Developers, indigo release (my instructions prefer but don't mandate
>> eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
>> file.  It Just Worked.  This is So Wonderful and I send my heartfelt
>> appreciation to everyone on this list who helped make it happen.
>>
>> Now my questions -- I think the answers are all 'yes' but this is the
>> place to confirm and I'm most concerned about (C):
>>
>> A. Java 7: Is this the real deal -- the framework will use the
>> available processors and, after suitable VM warmup, be the parallel
>> execution engine we expect?
>>
>> B. Installation: Will upgrading on machines that already have Java 6
>> be just as seamless?
>>
>> C. Code: Is the attached file the way to show things to beginners?
>> (Note: My point is to show them the reduction explicitly rather than
>> using a library method.  This is for pedagogical purposes.  So no
>> complaining about that.)
>>
>> D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
>> "The 1.7 compiler compliance level
>>     is not yet supported.  The new project will use a project specific
>> compiler compliance level of 1.6".  Am I correct that this can be
>> ignored since I'm not using any new /language/ features, just a new
>> /library/? (Note: My understanding is there are Eclipse versions
>> available with 1.7 compilers, but if we're okay with the most standard
>> most stable Eclipse release, this is extremely helpful.)
>>
>> E: Anything else I can do to make this as bullet-proof for beginners
>> as possible?
>>
>> Thanks!
>>
>> --Dan
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From djg at cs.washington.edu  Thu Aug 18 16:26:01 2011
From: djg at cs.washington.edu (Dan Grossman)
Date: Thu, 18 Aug 2011 13:26:01 -0700
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
In-Reply-To: <4E4D7428.3090602@univ-mlv.fr>
References: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
	<4E4D6E76.3030800@cs.oswego.edu> <4E4D7428.3090602@univ-mlv.fr>
Message-ID: <CADruQ+inQ0xdBWX_MiV8wEhyDCWyarwu3uqf7wUaZ1dTC8DwxA@mail.gmail.com>

Thanks, Remi.  My understanding is that it is fine to ignore this
warning since using the Java ForkJoin Framework does not require any
language enhancements.  Ignoring this warning is a small price to pay
for simple standard installation instructions, and, as you note, the
warning should go away with a near-term release of Eclipse.

--Dan

On Thu, Aug 18, 2011 at 1:20 PM, R?mi Forax <forax at univ-mlv.fr> wrote:
> Eclipse Indigo's compiler is not updated for Java 7,
> all language enhancements of Java 7 are not recognized,
> that's why you have this warning, the release 3.7.1 (not yet release) should
> be Ok,
> you can already test the compiler behaviour by installing the 3.8M1.
>
> R?mi
>
> On 08/18/2011 09:56 PM, Doug Lea wrote:
>>
>> On 08/16/11 18:00, Dan Grossman wrote:
>>>
>>> Short version:
>>>
>>> I'm looking for quick confirmation that using the fork-join framework
>>> with the Java 7 JRE is just as easy as it seems and that I'm pointing
>>> students to the right stable versions of things.
>>
>> I'm still hoping that someone else replies about most of this,
>> since I normally experimental run releases without IDE support.
>> But otherwise -- yes, things should Just Work.
>>
>> -Doug
>>
>>>
>>> Long version:
>>>
>>> Background:
>>>
>>> As I've mentioned on this list a couple times, I've developed a
>>> course-unit for second-year undergraduates that introduces parallelism
>>> and concurrency using Java and the Fork-Join Framework (though it's
>>> not really that Java-specific). ?At Washington, we've used this unit
>>> in our required data-structures course for 1.5 years now and it's been
>>> picked up by 5 other schools so far. ?In all, 10 instructors, most
>>> non-experts in Java, parallelism, or both have used it and they all
>>> claim success and, "I will do this again." ?For more information,
>>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
>>>
>>> One thing that has proven absolutely essential is step-by-step
>>> instructions suitable for beginners, specialized to just what they
>>> need: ForkJoinPool, RecursiveTask, RecursiveAction. ?This was
>>> particularly important for Java 1.6. ?The url
>>>
>>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
>>> has these instructions and was last updated a few months ago. ?For
>>> those of you who have not taught undergraduates, let me assure you
>>> that there are, nonetheless, a mind-boggling number of ways to enter
>>> -Xbootclasspath/p:jsr166.jar incorrectly. :-)
>>>
>>> So what now:
>>>
>>> It seems time to update my step-by-step instructions to say:
>>> ? 1. Please use Java 7 following steps a, b, c.
>>> ? 2. If you really can't, then here are the more complicated steps for
>>> using Java 6 following steps, d, e, f, g.
>>>
>>> In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
>>> machine that has never had Java on it, installed Eclipse IDE for Java
>>> Developers, indigo release (my instructions prefer but don't mandate
>>> eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
>>> file. ?It Just Worked. ?This is So Wonderful and I send my heartfelt
>>> appreciation to everyone on this list who helped make it happen.
>>>
>>> Now my questions -- I think the answers are all 'yes' but this is the
>>> place to confirm and I'm most concerned about (C):
>>>
>>> A. Java 7: Is this the real deal -- the framework will use the
>>> available processors and, after suitable VM warmup, be the parallel
>>> execution engine we expect?
>>>
>>> B. Installation: Will upgrading on machines that already have Java 6
>>> be just as seamless?
>>>
>>> C. Code: Is the attached file the way to show things to beginners?
>>> (Note: My point is to show them the reduction explicitly rather than
>>> using a library method. ?This is for pedagogical purposes. ?So no
>>> complaining about that.)
>>>
>>> D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
>>> "The 1.7 compiler compliance level
>>> ? ?is not yet supported. ?The new project will use a project specific
>>> compiler compliance level of 1.6". ?Am I correct that this can be
>>> ignored since I'm not using any new /language/ features, just a new
>>> /library/? (Note: My understanding is there are Eclipse versions
>>> available with 1.7 compilers, but if we're okay with the most standard
>>> most stable Eclipse release, this is extremely helpful.)
>>>
>>> E: Anything else I can do to make this as bullet-proof for beginners
>>> as possible?
>>>
>>> Thanks!
>>>
>>> --Dan
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From joe.bowbeer at gmail.com  Thu Aug 18 16:32:49 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 18 Aug 2011 13:32:49 -0700
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
Message-ID: <CAHzJPErnm3BBSA_M2A5LEJN8kPncQJxBK4bVw4cbhJV8jO-B2w@mail.gmail.com>

I'm trying hard to think of something to add.

Depending on how many java's they have installed, your students may need to
add a -vm in the eclipse command line for jdk7.

Btw, you might consider using maven to encapsulate more of the dependencies
(in a IDE agnostic way). But then your students using eclipse will need the
m2eclipse plugin for this, and then they may still need to supply a -vm
switch to get that working...
On Aug 18, 2011 1:04 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110818/6f575bef/attachment.html>

From dl at cs.oswego.edu  Thu Aug 18 16:41:17 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 18 Aug 2011 16:41:17 -0400
Subject: [concurrency-interest] Thoughts about LongAdder
In-Reply-To: <4E42B3DD.7050107@oracle.com>
References: <4E42B3DD.7050107@oracle.com>
Message-ID: <4E4D78ED.6010807@cs.oswego.edu>

Sorry for the delay on this!

On 08/10/11 12:37, Nathan Reynolds wrote:
> Here's a list of ideas of where
> LongAdder and ConcurrentCounter differ.
>
>    1. I would recommend striping the counter *not* by thread but by
>       processor/core. This will result in almost 0 contention and significantly
>       reduce cache coherence traffic.

This would be a good idea if we had a ProcessorLocal class.
Which is probably a good idea. Here's a minimal sketch
Add
   class java.lang.Processor {
      int getId();
      int getMaxId();
      static int proximity(Processor a, Processor b); // metrics TBD
      // non-public ProcessorLocal table
   }
Add to class java.lang.Thread:
       Processor getProcessor();
Add
    class java.lang.ProcessorLocal {
      // mostly like ThreadLocal
   }

However, in the mean time, the internal randomization done
inside LongAdder gives performance that seems to closely approximate
what you might get by doing this.


>    2. LongAdder pads each Cell to avoid false sharing. Good idea. However, the
>       amount of memory required can be quite costly in terms of capacity,
>       wasting cache space and bandwidth. ConcurrentCounter (attached) assumes
>       the JVM allocator is NUMA aware.

I agree that padding is a bit hacky, but I don't know how a JVM
can know to do this without either the assertion of or evidence
of a variable being contended. The best suggestion I've heard on
this is to add an annotation @Contended.


>       I am interested in results showing padding vs no padding...
>       assuming all of the JVM requirements are met.

I see more than a factor of 6 slowdown on Intel i7 and AMD 6172 test machines.

>    3. reset() should replace the array of Cell instead of setting the values to
>       0. This will increase GC and allocator load but reset() can then be
>       atomic.

Thanks for the suggestion, but I don't know of use cases for which
reset(), but not sum(), need to be atomic -- the only use cases I
know are to reset when it is otherwise known that threads are quiescent.

Doing this would also disable the nice property of current LongAdder
that it allocates no extra space until contended but instead uses
"base" field.

>    5. ConcurrentCounter supports a set() API. It basically does the reset()
>       trick I mentioned above but the new set of "Cells" is already assigned the
>       given value.

Ditto.


>    7. retryUpdate() is extremely complicated code (due to thread striping and
>       balancing).


We do the complicated stuff so you don't have to  :-)

-Doug

From nathan.reynolds at oracle.com  Thu Aug 18 19:05:21 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 18 Aug 2011 16:05:21 -0700
Subject: [concurrency-interest] Thoughts about LongAdder
In-Reply-To: <4E4D78ED.6010807@cs.oswego.edu>
References: <4E42B3DD.7050107@oracle.com> <4E4D78ED.6010807@cs.oswego.edu>
Message-ID: <4E4D9AB1.1040405@oracle.com>

On 8/18/2011 1:41 PM, Doug Lea wrote:
> Sorry for the delay on this!
>
> On 08/10/11 12:37, Nathan Reynolds wrote:
>> Here's a list of ideas of where
>> LongAdder and ConcurrentCounter differ.
>>
>>    1. I would recommend striping the counter *not* by thread but by
>>       processor/core. This will result in almost 0 contention and 
>> significantly
>>       reduce cache coherence traffic.
>
> This would be a good idea if we had a ProcessorLocal class.
> Which is probably a good idea. Here's a minimal sketch
> Add
>   class java.lang.Processor {
>      int getId();
>      int getMaxId();
>      static int proximity(Processor a, Processor b); // metrics TBD
>      // non-public ProcessorLocal table
>   }
> Add to class java.lang.Thread:
>       Processor getProcessor();
> Add
>    class java.lang.ProcessorLocal {
>      // mostly like ThreadLocal
>   }
In another email I sent on 8/10/2011 at 3:50 pm, it has my attempt at 
ProcessorLocal and ProcessorIndex.  The latter class maps raw CPU/OS 
processor ID into a 0 to N-1 index.  It doesn't provide a Processor 
class or a proximity() metric.
> However, in the mean time, the internal randomization done
> inside LongAdder gives performance that seems to closely approximate
> what you might get by doing this.
>
>
>>    2. LongAdder pads each Cell to avoid false sharing. Good idea. 
>> However, the
>>       amount of memory required can be quite costly in terms of 
>> capacity,
>>       wasting cache space and bandwidth. ConcurrentCounter (attached) 
>> assumes
>>       the JVM allocator is NUMA aware.
>
> I agree that padding is a bit hacky, but I don't know how a JVM
> can know to do this without either the assertion of or evidence
> of a variable being contended. The best suggestion I've heard on
> this is to add an annotation @Contended.
>
>
>>       I am interested in results showing padding vs no padding...
>>       assuming all of the JVM requirements are met.
>
> I see more than a factor of 6 slowdown on Intel i7 and AMD 6172 test 
> machines.
>
>>    3. reset() should replace the array of Cell instead of setting the 
>> values to
>>       0. This will increase GC and allocator load but reset() can 
>> then be
>>       atomic.
>
> Thanks for the suggestion, but I don't know of use cases for which
> reset(), but not sum(), need to be atomic -- the only use cases I
> know are to reset when it is otherwise known that threads are quiescent.

Thanks for pointing that out.  I can't think of a use case either.  My 
implementation provided a simple way to make them atomic and so I did.

> Doing this would also disable the nice property of current LongAdder
> that it allocates no extra space until contended but instead uses
> "base" field.
>
>>    5. ConcurrentCounter supports a set() API. It basically does the 
>> reset()
>>       trick I mentioned above but the new set of "Cells" is already 
>> assigned the
>>       given value.
>
> Ditto.
>
>
>>    7. retryUpdate() is extremely complicated code (due to thread 
>> striping and
>>       balancing).
>
>
> We do the complicated stuff so you don't have to  :-)
>

Oops.  I now realize what I said is a bit rude, but I didn't intend it 
as such.  Thank you for not taking offense.  What I mean to say is that 
ProcessorLocal will make LongAdder much simpler to implement.  But, you 
already understood that.

> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110818/a8e12c04/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Aug 18 19:22:06 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 19 Aug 2011 09:22:06 +1000
Subject: [concurrency-interest] ThreadLocal vs ProcessorLocal
In-Reply-To: <4E42B3EB.2060206@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEFMIPAA.davidcholmes@aapt.net.au>

Aside: Just for the public record. I think an API to support use of Processor sets would be a useful addition to the platform to allow better partitioning of CPU resources. The Real-time Specification for Java 1.1 under JSR-282 is looking at such an API primarily to work in with OS level processor sets. For our purposes, more locally within the JVM the processors made available to the JVM could be partitioned both internally (ie binding GC to a specific set of cores) and at the application/library level (such as allocating ForkJoinPools  disjoint sets of cores). The abiility to query the current processor ID is inherently needed and so I would make it part of the Processors class. This would provide the foundation API for ProcessortLocal.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nathan Reynolds
  Sent: Thursday, 11 August 2011 2:38 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ThreadLocal vs ProcessorLocal


  I would like to recommend that we stripe data structures using a ProcessorLocal instead of ThreadLocal.  ProcessorLocal (attached) is exactly like ThreadLocal except the stored objects keyed off of the processor instead of the thread.  In order to implement ProcessorLocal, it needs an API that returns the current processor id that the thread is running on.  The HotSpot team has filed an RFE and are planning on providing such an API.  (Many of you are already aware of this.)

  I would like to share a story and some results to further the discussion on processor striping (i.e. ProcessorLocal).

  A long time ago, we found that an Oracle C++ program bottlenecked on a reader/writer lock.  Threads couldn't read-acquire the lock fast enough.  The problem was due to the contention on the cache line while executing the CAS instruction.  So, I striped the lock.  The code non-atomically incremented an int and masked it to select one of the reader/writer locks.  Multiple threads could end up selecting the same reader/writer lock because the int was incremented in an unprotected manner.  If multiple threads selected the same reader/writer lock, the lock would handle the concurrency and the only negative was lock performance.  This optimization worked great until Intel released Nehalem-EX.

  A while ago, Intel found on Nehalem-EX that the same Oracle C++ program didn't scale to the 4?? Nehalem socket.  All of the processors/cores were 100% busy, but the throughput didn't improve by adding the 4?? Nehalem socket.  The problem was the cores were fighting to get the cache line holding the unprotected int!

  I tried 4 approaches to select the reader/writer lock.

  1) Processor id - This performed the best.  The cache lines holding the reader/writer locks are almost never invalidated due to another core accessing the reader/writer lock.  In other words, almost 0 CAS contention.
  2) ThreadLocal - ThreadLocal had a 1:1 mapping of threads to locks.  It required too many locks and the locks had to migrate with the threads.
  3) Hash the stack pointer - Hashing the stack pointer caused some collisions but essentially randomly selected locks and this hurt cache performance.
  4) Shift and mask the cycle counter (i.e. rdtsc) - Contention was rare but again it randomly selected the locks.

  Compared to non-atomically incrementing an int, processor id resulted in 15% more throughput.  The other 3 only showed 5% more throughput.


  Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
  Oracle PSR Engineering | Server Technology 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110819/43b29628/attachment.html>

From linzuxiong1988 at gmail.com  Sat Aug 20 03:35:05 2011
From: linzuxiong1988 at gmail.com (zuxiong lin)
Date: Sat, 20 Aug 2011 15:35:05 +0800
Subject: [concurrency-interest] java fork-join getting-started notes for
 beginners <-> Java 7
In-Reply-To: <CADruQ+inQ0xdBWX_MiV8wEhyDCWyarwu3uqf7wUaZ1dTC8DwxA@mail.gmail.com>
References: <CADruQ+i74Uxd_pWAQweQf36htJ7H+34=KQtxx7a1wh4GvSyHVg@mail.gmail.com>
	<4E4D6E76.3030800@cs.oswego.edu> <4E4D7428.3090602@univ-mlv.fr>
	<CADruQ+inQ0xdBWX_MiV8wEhyDCWyarwu3uqf7wUaZ1dTC8DwxA@mail.gmail.com>
Message-ID: <CACSA5wYdH8D+dm2BZoQ0aG7P==51goua7YP-v0r0m-NcJXVd2w@mail.gmail.com>

Or maybe use Eclipse Juno (4.2) M1.

The best paractice is like Doug .  Run releases without IDE support.



2011/8/19 Dan Grossman <djg at cs.washington.edu>

> Thanks, Remi.  My understanding is that it is fine to ignore this
> warning since using the Java ForkJoin Framework does not require any
> language enhancements.  Ignoring this warning is a small price to pay
> for simple standard installation instructions, and, as you note, the
> warning should go away with a near-term release of Eclipse.
>
> --Dan
>
> On Thu, Aug 18, 2011 at 1:20 PM, R?mi Forax <forax at univ-mlv.fr> wrote:
> > Eclipse Indigo's compiler is not updated for Java 7,
> > all language enhancements of Java 7 are not recognized,
> > that's why you have this warning, the release 3.7.1 (not yet release)
> should
> > be Ok,
> > you can already test the compiler behaviour by installing the 3.8M1.
> >
> > R?mi
> >
> > On 08/18/2011 09:56 PM, Doug Lea wrote:
> >>
> >> On 08/16/11 18:00, Dan Grossman wrote:
> >>>
> >>> Short version:
> >>>
> >>> I'm looking for quick confirmation that using the fork-join framework
> >>> with the Java 7 JRE is just as easy as it seems and that I'm pointing
> >>> students to the right stable versions of things.
> >>
> >> I'm still hoping that someone else replies about most of this,
> >> since I normally experimental run releases without IDE support.
> >> But otherwise -- yes, things should Just Work.
> >>
> >> -Doug
> >>
> >>>
> >>> Long version:
> >>>
> >>> Background:
> >>>
> >>> As I've mentioned on this list a couple times, I've developed a
> >>> course-unit for second-year undergraduates that introduces parallelism
> >>> and concurrency using Java and the Fork-Join Framework (though it's
> >>> not really that Java-specific).  At Washington, we've used this unit
> >>> in our required data-structures course for 1.5 years now and it's been
> >>> picked up by 5 other schools so far.  In all, 10 instructors, most
> >>> non-experts in Java, parallelism, or both have used it and they all
> >>> claim success and, "I will do this again."  For more information,
> >>> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/
> >>>
> >>> One thing that has proven absolutely essential is step-by-step
> >>> instructions suitable for beginners, specialized to just what they
> >>> need: ForkJoinPool, RecursiveTask, RecursiveAction.  This was
> >>> particularly important for Java 1.6.  The url
> >>>
> >>>
> http://www.cs.washington.edu/homes/djg/teachingMaterials/spac/grossmanSPAC_forkJoinFramework.html
> >>> has these instructions and was last updated a few months ago.  For
> >>> those of you who have not taught undergraduates, let me assure you
> >>> that there are, nonetheless, a mind-boggling number of ways to enter
> >>> -Xbootclasspath/p:jsr166.jar incorrectly. :-)
> >>>
> >>> So what now:
> >>>
> >>> It seems time to update my step-by-step instructions to say:
> >>>   1. Please use Java 7 following steps a, b, c.
> >>>   2. If you really can't, then here are the more complicated steps for
> >>> using Java 6 following steps, d, e, f, g.
> >>>
> >>> In preparation for this, I downloaded JDK 7 onto a [Windows 7, 64-bit]
> >>> machine that has never had Java on it, installed Eclipse IDE for Java
> >>> Developers, indigo release (my instructions prefer but don't mandate
> >>> eclipse), set the Java Project JRE to JavaSE-1.7, and ran the attached
> >>> file.  It Just Worked.  This is So Wonderful and I send my heartfelt
> >>> appreciation to everyone on this list who helped make it happen.
> >>>
> >>> Now my questions -- I think the answers are all 'yes' but this is the
> >>> place to confirm and I'm most concerned about (C):
> >>>
> >>> A. Java 7: Is this the real deal -- the framework will use the
> >>> available processors and, after suitable VM warmup, be the parallel
> >>> execution engine we expect?
> >>>
> >>> B. Installation: Will upgrading on machines that already have Java 6
> >>> be just as seamless?
> >>>
> >>> C. Code: Is the attached file the way to show things to beginners?
> >>> (Note: My point is to show them the reduction explicitly rather than
> >>> using a library method.  This is for pedagogical purposes.  So no
> >>> complaining about that.)
> >>>
> >>> D. Eclipse: When choosing JavaSE1-1.7, Eclipse Indigo release warns,
> >>> "The 1.7 compiler compliance level
> >>>    is not yet supported.  The new project will use a project specific
> >>> compiler compliance level of 1.6".  Am I correct that this can be
> >>> ignored since I'm not using any new /language/ features, just a new
> >>> /library/? (Note: My understanding is there are Eclipse versions
> >>> available with 1.7 compilers, but if we're okay with the most standard
> >>> most stable Eclipse release, this is extremely helpful.)
> >>>
> >>> E: Anything else I can do to make this as bullet-proof for beginners
> >>> as possible?
> >>>
> >>> Thanks!
> >>>
> >>> --Dan
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110820/d8c5cb1f/attachment.html>

From jeffhain at rocketmail.com  Sat Aug 20 06:17:54 2011
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sat, 20 Aug 2011 11:17:54 +0100 (BST)
Subject: [concurrency-interest] Re : Re : concurrent counter :
	incrementAndGet
In-Reply-To: <4E4D70A7.7060501@cs.oswego.edu>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
	<4E4D70A7.7060501@cs.oswego.edu>
Message-ID: <1313835474.94237.YahooMailRC@web29212.mail.ird.yahoo.com>

I hand-emulated ">" replies style, which is handier :)


________________________________
>De : Doug Lea <dl at cs.oswego.edu>
>? : concurrency-interest at cs.oswego.edu
>Envoy? le : Jeu 18 ao?t 2011, 22h 05min 59s
>Objet : Re: [concurrency-interest] Re : concurrent counter :  incrementAndGet
>
>On 08/14/11 07:03, Jeff Hain wrote:
>>
>> That could work well for some cases indeed, but I'm working on a sort
>> of Disruptor, using the counter with modulo to pick up slots to write to in
>> a cyclic array, monotonically, or non-monotonically but for a short time
>> (for readers not to be blocked on a not-yet-written slot), and any writer
>> can stop to work anytime; in this case I don't see how that could apply
>>  easily.
>
>I don't think "non-monotonically but for a short time" makes
>this easier, since any violations are likely to be unbounded.

   Violations are indeed unbounded, which could make for example all producers
claim wrapping sequences (on entries which could not be written for the time),
and let consumers stuck on not-yet-written entries (*).
   Though, these violations are dealt with, as follows: when a consumer 
encounters
a writable entry (for "current round"), and that the counter already provided a 
higher
sequence to a producer (hence there will be something to eat further), it tries 
to
CAS-it-up to being writable but for "next round" (i.e. sequence += buffer 
length),
so that consumers can pass on it and not be stuck (a producer can write it just 
after,
it doesn't hurt, consumers will still just pass on it). In the process, consumer 
also depletes
the counter of any lower  sequence, to lower violations, and so that no producer
subsequently tries to write on a CASed-up entry (if one does, i.e. entry doesn't
have the expected sequence, it just picks another sequence).
   At the time, on a core i7 X 980, when doing "no work" benches (i.e. consumers
do nothing), using a concurrent counter instead of AtomicLong is just a bit 
slower if
consumers are going faster than a few producers (due to the CASing-up going on I 
guess),
but if producers go faster, i.e. if consumers have something to eat most of the 
time,
I've seen up to 30 percent improvement with 8 producers and 1 consumer, and 
about
20 percent with anywhere between 32 and 128 producers.
   (My code is still a mess of comments and obsolete code, or dead debugging 
code,
so I can't attach it, but there's not much more in it than what I said.)

   (*) The "Disruptor" I use is "unicast", and producers and  consumers don't 
spy
on each other (which doesn't scale well), but only look at entries sequence and 
status
(writable,being written,readable,being read), both stored in a same long.

>You might be able to live with thread local random number generators,
>so that in the long run on average you are balanced?

   I tried to use random (Marsaglia XorShift) in the concurrent counter,
but it was actually slower than just doing "++index" for the next cell to
CAS on (it's an array of sub-counters, each incremented by the number
of sub-counters each time). If CAS fails, I try another cell. Also, I actually
don't do "++index", but "index = index + 1 + nbrOfFailedCAS" (with mask),
which seems to help to get away from contention.
   Also, as I said, balancing is dealt with, but the counter also has
a method that ensures monotonicity (for the thread-local, or
non-thread-safe-view-instance-local, data structure that holds previous
index and highest returned value). It is a bit slower, but it tends
to lower unbalancing, since when it gets a value lower than the
highest previously  returned one, it tries to CAS again on the same cell,
making it catch-up.

>Otherwise, you have a version of the Counting problem described
>in Herlihy & Shavit, for which all known scalable solutions
>are expensive, complicated, and difficult to package as
>j.u.c components. Although there are some special cases like
>SNZI for detecting when counts hit the particular value of zero.

   Thanks, I didn't know about these. I've tried Shavit & others's
flat-combining method though for the counter, but it was too slow.

>
>(This seems to be the main bottleneck in Disruptor-like designs.)

   Yes, that's why I started to focus on it. I think it's the only part of this 
disruptor
that is potentially proportionnal-or-worse to the number of threads (one could 
think
of ring buffer scanning by consumers too, but it should be possible to use
modulo so that each entry doesn't get  scanned by all consumers).

>
>-Doug

-Jeff

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110820/107bc2cd/attachment-0001.html>

From rreja2000 at yahoo.com  Wed Aug 24 10:58:27 2011
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Wed, 24 Aug 2011 07:58:27 -0700 (PDT)
Subject: [concurrency-interest] Concurrent indexed queue
Message-ID: <1314197907.56250.YahooMailMobile@web36207.mail.mud.yahoo.com>

 Hi,

I am working on a project where I need advise on designing following functionality.

Producer threads read quotes on a set of instruments (5000 instruments) from a message bus 
Which needs to be routed to a downstream system which can consume messages at a slower rate 
Than producers are producing messages. The catch is that only the latest quotes need to be published
To the downstream system. So we can miss out on stale quotes. So essentially we dont need to slow down the producers as the event queue is bounded by the number of instruments.

I want to keep the design flexible as to allow multiple producers and multiple consumers. 
So in short I need a concurrent indexed queue where i can update the entries in place. This is a latency sensitive application so we would want latency to be as low as possible. 

Please advise me for suitable data structures that can be of use.
 Regards,
Rohit 



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/a9dff8f3/attachment.html>

From rreja2000 at yahoo.com  Wed Aug 24 11:06:52 2011
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Wed, 24 Aug 2011 08:06:52 -0700 (PDT)
Subject: [concurrency-interest] Re : Re : concurrent counter :
	incrementAndGet
Message-ID: <1314198412.24315.YahooMailMobile@web36205.mail.mud.yahoo.com>

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/c0573f79/attachment.html>

From aleksey.shipilev at gmail.com  Wed Aug 24 11:28:23 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Wed, 24 Aug 2011 19:28:23 +0400
Subject: [concurrency-interest] Concurrent indexed queue
In-Reply-To: <1314197907.56250.YahooMailMobile@web36207.mail.mud.yahoo.com>
References: <1314197907.56250.YahooMailMobile@web36207.mail.mud.yahoo.com>
Message-ID: <CA+1LWGHD10dte6+FY=Y-MoEWEguTYiGKrHdiNGgw3oz-XuyE-A@mail.gmail.com>

Hi Rohit,

Are you looking for efficient CircularBuffer implementation then? Having one
of those, you can then have per-instrument CircularBuffer and poll each by
"key", which will provide essential indexing.

-Aleksey.

On Wed, Aug 24, 2011 at 6:58 PM, Rohit Reja <rreja2000 at yahoo.com> wrote:

> Hi,
>
> I am working on a project where I need advise on designing following
> functionality.
>
> Producer threads read quotes on a set of instruments (5000 instruments)
> from a message bus
> Which needs to be routed to a downstream system which can consume messages
> at a slower rate
> Than producers are producing messages. The catch is that only the latest
> quotes need to be published
> To the downstream system. So we can miss out on stale quotes. So
> essentially we dont need to slow down the producers as the event queue is
> bounded by the number of instruments.
>
> I want to keep the design flexible as to allow multiple producers and
> multiple consumers.
> So in short I need a concurrent indexed queue where i can update the
> entries in place. This is a latency sensitive application so we would want
> latency to be as low as possible.
>
> Please advise me for suitable data structures that can be of use.
> Regards,
> Rohit
>
>
>
>
>  ------------------------------
> * From: * Jeff Hain <jeffhain at rocketmail.com>;
> * To: * Doug Lea <dl at cs.oswego.edu>;
> * Cc: * <concurrency-interest at cs.oswego.edu>;
> * Subject: * [concurrency-interest] Re : Re : concurrent counter :
> incrementAndGet
> * Sent: * Sat, Aug 20, 2011 10:17:54 AM
>
>   I hand-emulated ">" replies style, which is handier :)
>
> ------------------------------
> *>De :* Doug Lea <dl at cs.oswego.edu>
> *>? :* concurrency-interest at cs.oswego.edu
> *>Envoy? le :* Jeu 18 ao?t 2011, 22h 05min 59s
> *>Objet :* Re: [concurrency-interest] Re : concurrent counter :
> incrementAndGet
> >
> >On 08/14/11 07:03, Jeff Hain wrote:
> >>
> >> That could work well for some cases indeed, but I'm working on a sort
> >> of Disruptor, using the counter with modulo to pick up slots to write to
> in
> >> a cyclic array, monotonically, or non-monotonically but for a short time
> >> (for readers not to be blocked on a not-yet-written slot), and any
> writer
> >> can stop to work anytime; in this case I don't see how that could apply
> >> easily.
> >
> >I don't think "non-monotonically but for a short time" makes
> >this easier, since any violations are likely to be unbounded.
>
>    Violations are indeed unbounded, which could make for example all
> producers
> claim wrapping sequences (on entries which could not be written for the
> time),
> and let consumers stuck on not-yet-written entries (*).
>    Though, these violations are dealt with, as follows: when a consumer
> encounters
> a writable entry (for "current round"), and that the counter already
> provided a higher
> sequence to a producer (hence there will be something to eat further), it
> tries to
> CAS-it-up to being writable but for "next round" (i.e. sequence += buffer
> length),
> so that consumers can pass on it and not be stuck (a producer can write it
> just after,
> it doesn't hurt, consumers will still just pass on it). In the process,
> consumer also depletes
> the counter of any lower sequence, to lower violations, and so that no
> producer
> subsequently tries to write on a CASed-up entry (if one does, i.e. entry
> doesn't
> have the expected sequence, it just picks another sequence).
>    At the time, on a core i7 X 980, when doing "no work" benches (i.e.
> consumers
> do nothing), using a concurrent counter instead of AtomicLong is just a bit
> slower if
> consumers are going faster than a few producers (due to the CASing-up going
> on I guess),
> but if producers go faster, i.e. if consumers have something to eat most of
> the time,
> I've seen up to 30 percent improvement with 8 producers and 1 consumer, and
> about
> 20 percent with anywhere between 32 and 128 producers.
>    (My code is still a mess of comments and obsolete code, or dead
> debugging code,
> so I can't attach it, but there's not much more in it than what I said.)
>
>    (*) The "Disruptor" I use is "unicast", and producers and consumers
> don't spy
> on each other (which doesn't scale well), but only look at entries sequence
> and status
> (writable,being written,readable,being read), both stored in a same long.
>
> >You might be able to live with thread local random number generators,
> >so that in the long run on average you are balanced?
>
>    I tried to use random (Marsaglia XorShift) in the concurrent counter,
> but it was actually slower than just doing "++index" for the next cell to
> CAS on (it's an array of sub-counters, each incremented by the number
> of sub-counters each time). If CAS fails, I try another cell. Also, I
> actually
> don't do "++index", but "index = index + 1 + nbrOfFailedCAS" (with mask),
> which seems to help to get away from contention.
>    Also, as I said, balancing is dealt with, but the counter also has
> a method that ensures monotonicity (for the thread-local, or
> non-thread-safe-view-instance-local, data structure that holds previous
> index and highest returned value). It is a bit slower, but it tends
> to lower unbalancing, since when it gets a value lower than the
> highest previously returned one, it tries to CAS again on the same cell,
> making it catch-up.
>
> >Otherwise, you have a version of the Counting problem described
> >in Herlihy & Shavit, for which all known scalable solutions
> >are expensive, complicated, and difficult to package as
> >j.u.c components. Although there are some special cases like
> >SNZI for detecting when counts hit the particular value of zero.
>
>    Thanks, I didn't know about these. I've tried Shavit & others's
> flat-combining method though for the counter, but it was too slow.
>
> >
> >(This seems to be the main bottleneck in Disruptor-like designs.)
>
>    Yes, that's why I started to focus on it. I think it's the only part of
> this disruptor
> that is potentially proportionnal-or-worse to the number of threads (one
> could think
> of ring buffer scanning by consumers too, but it should be possible to use
> modulo so that each entry doesn't get scanned by all consumers).
>
> >
> >-Doug
>
> -Jeff
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/b44958db/attachment-0001.html>

From rreja2000 at yahoo.com  Wed Aug 24 12:10:02 2011
From: rreja2000 at yahoo.com (Rohit Reja)
Date: Wed, 24 Aug 2011 09:10:02 -0700 (PDT)
Subject: [concurrency-interest] Concurrent indexed queue
Message-ID: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>

 Hi Aleksey, 

 To clarify a bit further, I dont need to keep any history of quotes, I just need to keep the latest quote for an instrument. 

So for e.g. One of the possible solution that i thought was to keep a CHM of instrumentid (key) and quote(value).
And an event queue of "instrumentids". so lets say events comes as 
E1( I1, Q1)
E2(I2, q1)
E3(I1, Q2)
 
Then CHM will have 2 entries. While the Queue will have 3 entries {I1, I2, I1}. 
Now when consumer consumes I1 it removes I1&#39;s entry from CHM, so that consumer can ignore the event 
I1 next time when it retrieves I1 from queue. 

This implementation would cause the event queue to grow indefinetely as the consumer is slower.
To avoid this , I can simply check if I1 is contained in the queue before putting in queue but that would make things definetely very slow.

I hope I have made things clearer now.

Thanks,
rohit
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/f4fcc18c/attachment.html>

From nathan.reynolds at oracle.com  Wed Aug 24 12:33:36 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 24 Aug 2011 09:33:36 -0700
Subject: [concurrency-interest] Concurrent indexed queue
In-Reply-To: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>
References: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>
Message-ID: <4E5527E0.9050302@oracle.com>

A CHM will work but there might be a lighter-weight approach.  But the 
lighter-weight approach requires that all of the instruments are known 
ahead of time and the producers simply put() the latest quote.

If these constraints are allowed, consider filling a HashMap with each 
instrument and a null value or a dummy quote.  As data new quotes 
arrive, simply put(I, Q).  Consumers will then simply get(I).

Outside the HashMap, all producers have to set a volatile variable (e.g. 
an int) to some constant (e.g. 0) after the put() call.  All consumers 
have to read the same volatile variable.  The volatile variable ensures 
that the JVM will insert any necessary memory barriers so that consumers 
will always see the latest quote.  If consumers are allowed to see 
slightly older quotes even though a newer one is in the HashMap, then 
the volatile variable is not necessary.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 8/24/2011 9:10 AM, Rohit Reja wrote:
> Hi Aleksey,
>
> To clarify a bit further, I dont need to keep any history of quotes, I 
> just need to keep the latest quote for an instrument.
>
> So for e.g. One of the possible solution that i thought was to keep a 
> CHM of instrumentid (key) and quote(value).
> And an event queue of "instrumentids". so lets say events comes as
> E1( I1, Q1)
> E2(I2, q1)
> E3(I1, Q2)
>
> Then CHM will have 2 entries. While the Queue will have 3 entries {I1, 
> I2, I1}.
> Now when consumer consumes I1 it removes I1's entry from CHM, so that 
> consumer can ignore the event
> I1 next time when it retrieves I1 from queue.
>
> This implementation would cause the event queue to grow indefinetely 
> as the consumer is slower.
> To avoid this , I can simply check if I1 is contained in the queue 
> before putting in queue but that would make things definetely very slow.
>
> I hope I have made things clearer now.
>
> Thanks,
> rohit
>
>
> ------------------------------------------------------------------------
> *From: * Aleksey Shipilev <aleksey.shipilev at gmail.com>;
> *To: * Rohit Reja <rreja2000 at yahoo.com>;
> *Cc: * <concurrency-interest at cs.oswego.edu>;
> *Subject: * Re: [concurrency-interest] Concurrent indexed queue
> *Sent: * Wed, Aug 24, 2011 3:28:23 PM
>
> Hi Rohit,
>
> Are you looking for efficient CircularBuffer implementation then? 
> Having one of those, you can then have per-instrument CircularBuffer 
> and poll each by "key", which will provide essential indexing.
>
> -Aleksey.
>
> On Wed, Aug 24, 2011 at 6:58 PM, Rohit Reja <rreja2000 at yahoo.com 
> <javascript:return>> wrote:
>
>     Hi,
>
>     I am working on a project where I need advise on designing
>     following functionality.
>
>     Producer threads read quotes on a set of instruments (5000
>     instruments) from a message bus
>     Which needs to be routed to a downstream system which can consume
>     messages at a slower rate
>     Than producers are producing messages. The catch is that only the
>     latest quotes need to be published
>     To the downstream system. So we can miss out on stale quotes. So
>     essentially we dont need to slow down the producers as the event
>     queue is bounded by the number of instruments.
>
>     I want to keep the design flexible as to allow multiple producers
>     and multiple consumers.
>     So in short I need a concurrent indexed queue where i can update
>     the entries in place. This is a latency sensitive application so
>     we would want latency to be as low as possible.
>
>     Please advise me for suitable data structures that can be of use.
>     Regards,
>     Rohit
>
>
>
>
>     ------------------------------------------------------------------------
>     *From: * Jeff Hain <jeffhain at rocketmail.com <javascript:return>>;
>     *To: * Doug Lea <dl at cs.oswego.edu <javascript:return>>;
>     *Cc: * <concurrency-interest at cs.oswego.edu <javascript:return>>;
>     *Subject: * [concurrency-interest] Re : Re : concurrent counter :
>     incrementAndGet
>     *Sent: * Sat, Aug 20, 2011 10:17:54 AM
>
>     I hand-emulated ">" replies style, which is handier :)
>
>     ------------------------------------------------------------------------
>     *>De :* Doug Lea <dl at cs.oswego.edu <javascript:return>>
>     *>? :* concurrency-interest at cs.oswego.edu <javascript:return>
>     *>Envoy? le :* Jeu 18 ao?t 2011, 22h 05min 59s
>     *>Objet :* Re: [concurrency-interest] Re : concurrent counter :
>     incrementAndGet
>     >
>     >On 08/14/11 07:03, Jeff Hain wrote:
>     >>
>     >> That could work well for some cases indeed, but I'm working on
>     a sort
>     >> of Disruptor, using the counter with modulo to pick up slots to
>     write to in
>     >> a cyclic array, monotonically, or non-monotonically but for a
>     short time
>     >> (for readers not to be blocked on a not-yet-written slot), and
>     any writer
>     >> can stop to work anytime; in this case I don't see how that
>     could apply
>     >> easily.
>     >
>     >I don't think "non-monotonically but for a short time" makes
>     >this easier, since any violations are likely to be unbounded.
>
>        Violations are indeed unbounded, which could make for example
>     all producers
>     claim wrapping sequences (on entries which could not be written
>     for the time),
>     and let consumers stuck on not-yet-written entries (*).
>        Though, these violations are dealt with, as follows: when a
>     consumer encounters
>     a writable entry (for "current round"), and that the counter
>     already provided a higher
>     sequence to a producer (hence there will be something to eat
>     further), it tries to
>     CAS-it-up to being writable but for "next round" (i.e. sequence +=
>     buffer length),
>     so that consumers can pass on it and not be stuck (a producer can
>     write it just after,
>     it doesn't hurt, consumers will still just pass on it). In the
>     process, consumer also depletes
>     the counter of any lower sequence, to lower violations, and so
>     that no producer
>     subsequently tries to write on a CASed-up entry (if one does, i.e.
>     entry doesn't
>     have the expected sequence, it just picks another sequence).
>        At the time, on a core i7 X 980, when doing "no work" benches
>     (i.e. consumers
>     do nothing), using a concurrent counter instead of AtomicLong is
>     just a bit slower if
>     consumers are going faster than a few producers (due to the
>     CASing-up going on I guess),
>     but if producers go faster, i.e. if consumers have something to
>     eat most of the time,
>     I've seen up to 30 percent improvement with 8 producers and 1
>     consumer, and about
>     20 percent with anywhere between 32 and 128 producers.
>        (My code is still a mess of comments and obsolete code, or dead
>     debugging code,
>     so I can't attach it, but there's not much more in it than what I
>     said.)
>
>        (*) The "Disruptor" I use is "unicast", and producers and
>     consumers don't spy
>     on each other (which doesn't scale well), but only look at entries
>     sequence and status
>     (writable,being written,readable,being read), both stored in a
>     same long.
>
>     >You might be able to live with thread local random number generators,
>     >so that in the long run on average you are balanced?
>
>        I tried to use random (Marsaglia XorShift) in the concurrent
>     counter,
>     but it was actually slower than just doing "++index" for the next
>     cell to
>     CAS on (it's an array of sub-counters, each incremented by the number
>     of sub-counters each time). If CAS fails, I try another cell.
>     Also, I actually
>     don't do "++index", but "index = index + 1 + nbrOfFailedCAS" (with
>     mask),
>     which seems to help to get away from contention.
>        Also, as I said, balancing is dealt with, but the counter also has
>     a method that ensures monotonicity (for the thread-local, or
>     non-thread-safe-view-instance-local, data structure that holds
>     previous
>     index and highest returned value). It is a bit slower, but it tends
>     to lower unbalancing, since when it gets a value lower than the
>     highest previously returned one, it tries to CAS again on the same
>     cell,
>     making it catch-up.
>
>     >Otherwise, you have a version of the Counting problem described
>     >in Herlihy & Shavit, for which all known scalable solutions
>     >are expensive, complicated, and difficult to package as
>     >j.u.c components. Although there are some special cases like
>     >SNZI for detecting when counts hit the particular value of zero.
>
>        Thanks, I didn't know about these. I've tried Shavit & others's
>     flat-combining method though for the counter, but it was too slow.
>
>     >
>     >(This seems to be the main bottleneck in Disruptor-like designs.)
>
>        Yes, that's why I started to focus on it. I think it's the only
>     part of this disruptor
>     that is potentially proportionnal-or-worse to the number of
>     threads (one could think
>     of ring buffer scanning by consumers too, but it should be
>     possible to use
>     modulo so that each entry doesn't get scanned by all consumers).
>
>     >
>     >-Doug
>
>     -Jeff
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu <javascript:return>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/18c83196/attachment-0001.html>

From ben_manes at yahoo.com  Wed Aug 24 14:59:36 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Wed, 24 Aug 2011 11:59:36 -0700 (PDT)
Subject: [concurrency-interest] Concurrent indexed queue
In-Reply-To: <4E5527E0.9050302@oracle.com>
References: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>
	<4E5527E0.9050302@oracle.com>
Message-ID: <1314212376.54154.YahooMailNeo@web38807.mail.mud.yahoo.com>

If you do not need to maintain write-order processing of the queue, then only the map is required. The value could maintain a flag so that when a consumer scans the map it can skip previously processed entries. At a size of 5,000 entries, the map is probably small enough to make this reasonably fast despite being brute force.


Another approach is to maintain a write-ordered queue that producers periodically prune. If queue exceeds a soft bounding then a producer can tryLock to exclusively prune, scan it to discard stale events, and perhaps bound the work to avoid an excessive penalty. This would effectively amortize the clean-up on the producer threads.

A more complicated variant would be to place the linkage pointers onto the values to allow O(1) removals or reorderings. The producers can add tasks to buffer(s) to allow the queue to be caught-up in a non-blocking fashion. When the tryLock is acquired by either a producer or consumer the buffers are drained to bring the queue up to date. The consumer would have the additional task of draining the queue when processing the entries. This would maintain write ordering, avoid unbounded growth, and maintain O(1) efficiency. However, its probably more complicated than needed in this situation as the design is styled based on a concurrent LinkedHashMap.



________________________________
From: Nathan Reynolds <nathan.reynolds at oracle.com>
To: Rohit Reja <rreja2000 at yahoo.com>
Cc: Concurrency <concurrency-interest at cs.oswego.edu>
Sent: Wednesday, August 24, 2011 9:33 AM
Subject: Re: [concurrency-interest] Concurrent indexed queue


 A CHM will work but there might be a lighter-weight approach.? But the lighter-weight approach requires that all of the instruments are known ahead of time and the producers simply put() the latest quote.

If these constraints are allowed, consider filling a HashMap with
    each instrument and a null value or a dummy quote.? As data new
    quotes arrive, simply put(I, Q).? Consumers will then simply get(I).

Outside the HashMap, all producers have to set a volatile variable
    (e.g. an int) to some constant (e.g. 0) after the put() call.? All
    consumers have to read the same volatile variable.? The volatile
    variable ensures that the JVM will insert any necessary memory
    barriers so that consumers will always see the latest quote.? If
    consumers are allowed to see slightly older quotes even though a
    newer one is in the HashMap, then the volatile variable is not
    necessary.


Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering | Server Technology 
On 8/24/2011 9:10 AM, Rohit Reja wrote: 
Hi Aleksey, 
>
>To clarify a bit further, I dont need to keep any
                history of quotes, I just need to keep the latest quote
                for an instrument. 
>
>So for e.g. One of the possible solution that i thought
                was to keep a CHM of instrumentid (key) and
                quote(value).
>And an event queue of "instrumentids". so lets say
                events comes as 
>E1( I1, Q1)
>E2(I2, q1)
>E3(I1, Q2)
>
>Then CHM will have 2 entries. While the Queue will have
                3 entries {I1, I2, I1}. 
>Now when consumer consumes I1 it removes I1's entry from
                CHM, so that consumer can ignore the event 
>I1 next time when it retrieves I1 from queue. 
>
>This implementation would cause the event queue to grow
                indefinetely as the consumer is slower.
>To avoid this , I can simply check if I1 is contained in
                the queue before putting in queue but that would make
                things definetely very slow.
>
>I hope I have made things clearer now.
>
>Thanks,
>rohit 
>
>
>
>________________________________
> From:  Aleksey Shipilev <aleksey.shipilev at gmail.com>; 
>To:  Rohit Reja <rreja2000 at yahoo.com>; 
>Cc:  <concurrency-interest at cs.oswego.edu>; 
>Subject:  Re: [concurrency-interest] Concurrent indexed queue 
>Sent:  Wed, Aug 24, 2011 3:28:23 PM 
> 
>
>Hi Rohit,? 
>
>
>Are you looking for efficient CircularBuffer implementation then? Having one of those, you can then have per-instrument CircularBuffer and poll each by "key", which will provide essential indexing.
>
>
>-Aleksey.
>
>
>On Wed, Aug 24, 2011 at 6:58 PM, Rohit Reja <rreja2000 at yahoo.com> wrote:
>
>Hi,
>>
>>I am working on a project where I
                                    need advise on designing following
                                    functionality.
>>
>>Producer threads read quotes on a
                                    set of instruments (5000
                                    instruments) from a message bus 
>>Which needs to be routed to a
                                    downstream system which can consume
                                    messages at a slower rate 
>>Than producers are producing
                                    messages. The catch is that only the
                                    latest quotes need to be published
>>To the downstream system. So we can
                                    miss out on stale quotes. So
                                    essentially we dont need to slow
                                    down the producers as the event
                                    queue is bounded by the number of
                                    instruments.
>>
>>I want to keep the design flexible
                                    as to allow multiple producers and
                                    multiple consumers. 
>>So in short I need a concurrent
                                    indexed queue where i can update the
                                    entries in place. This is a latency
                                    sensitive application so we would
                                    want latency to be as low as
                                    possible. 
>>
>>Please advise me for suitable data
                                    structures that can be of use.
>>Regards,
>>Rohit 
>>
>>
>>
>> 
>>
>>
>>
>>________________________________
>> From:  Jeff Hain <jeffhain at rocketmail.com>; 
>>To:  Doug Lea <dl at cs.oswego.edu>; 
>>Cc:  <concurrency-interest at cs.oswego.edu>; 
>>Subject:  [concurrency-interest] Re : Re : concurrent counter : incrementAndGet 
>>Sent:  Sat, Aug 20, 2011 10:17:54 AM 
>> 
>>
>>I hand-emulated ">" replies style, which is handier :)
>>
>>
>>________________________________
>>>De : Doug Lea <dl at cs.oswego.edu>
>>>? : concurrency-interest at cs.oswego.edu
>>>Envoy? le : Jeu 18 ao?t 2011, 22h 05min 59s
>>>Objet?: Re: [concurrency-interest] Re : concurrent counter : incrementAndGet
>>>
>>>On 08/14/11 07:03, Jeff
                                          Hain wrote:
>>>>
>>>> That could work well
                                          for some cases indeed, but I'm
                                          working on a sort
>>>> of Disruptor, using
                                          the counter with modulo to
                                          pick up slots to write to in
>>>> a cyclic array,
                                          monotonically, or
                                          non-monotonically but for a
                                          short time
>>>> (for readers not to
                                          be blocked on a
                                          not-yet-written slot), and any
                                          writer
>>>> can stop to work
                                          anytime; in this case I don't
                                          see how that could apply
>>>> easily.
>>>
>>>I don't think
                                          "non-monotonically but for a
                                          short time" makes
>>>this easier, since any
                                          violations are likely to be
                                          unbounded.
>>
>>?? Violations are indeed
                                          unbounded, which could make
                                          for example all producers
>>claim wrapping sequences (on
                                          entries which could not be
                                          written for the time),
>>and let consumers stuck on
                                          not-yet-written entries (*).
>>?? Though, these violations
                                          are dealt with, as follows:
                                          when a consumer encounters
>>a writable entry (for "current
                                          round"), and that the counter
                                          already provided a higher
>>sequence to a producer (hence
                                          there will be something to eat
                                          further), it tries to
>>CAS-it-up to being writable
                                          but for "next round" (i.e.
                                          sequence += buffer length),
>>so that consumers can pass on
                                          it and not be stuck (a
                                          producer can write it just
                                          after,
>>it doesn't hurt, consumers
                                          will still just pass on it).
                                          In the process, consumer also
                                          depletes
>>the counter of any lower
                                          sequence, to lower violations,
                                          and so that no producer
>>subsequently tries to write on
                                          a CASed-up entry (if one does,
                                          i.e. entry doesn't
>>have the expected sequence, it
                                          just picks another sequence).
>>?? At the time, on a core i7 X
                                          980, when doing "no work"
                                          benches (i.e. consumers
>>do nothing), using a
                                          concurrent counter instead of
                                          AtomicLong is just a bit
                                          slower if
>>consumers are going faster
                                          than a few producers (due to
                                          the CASing-up going on I
                                          guess),
>>but if producers go faster,
                                          i.e. if consumers have
                                          something to eat most of the
                                          time,
>>I've seen up to 30 percent
                                          improvement with 8 producers
                                          and 1 consumer, and about
>>20 percent with anywhere
                                          between 32 and 128 producers.
>>?? (My code is still a mess of
                                          comments and obsolete code, or
                                          dead debugging code,
>>so I can't attach it, but
                                          there's not much more in it
                                          than what I said.)
>>
>>?? (*) The "Disruptor" I use
                                          is "unicast", and producers
                                          and consumers don't spy
>>on each other (which doesn't
                                          scale well), but only look at
                                          entries sequence and status
>>(writable,being
                                          written,readable,being read),
                                          both stored in a same long.
>>
>>>You might be able to live
                                          with thread local random
                                          number generators,
>>>so that in the long run on
                                          average you are balanced?
>>
>>?? I tried to use random
                                          (Marsaglia XorShift) in the
                                          concurrent counter,
>>but it was actually slower
                                          than just doing "++index" for
                                          the next cell to
>>CAS on (it's an array of
                                          sub-counters, each incremented
                                          by the number
>>of sub-counters each time). If
                                          CAS fails, I try another cell.
                                          Also, I actually
>>don't do "++index", but "index
                                          = index + 1 + nbrOfFailedCAS"
                                          (with mask),
>>which seems to help to get
                                          away from contention.
>>?? Also, as I said, balancing
                                          is dealt with, but the counter
                                          also has
>>a method that ensures
                                          monotonicity (for the
                                          thread-local, or
>>non-thread-safe-view-instance-local,
                                          data structure that holds
                                          previous
>>index and highest returned
                                          value). It is a bit slower,
                                          but it tends
>>to lower unbalancing, since
                                          when it gets a value lower
                                          than the
>>highest previously returned
                                          one, it tries to CAS again on
                                          the same cell,
>>making it catch-up.
>>
>>>Otherwise, you have a
                                          version of the Counting
                                          problem described
>>>in Herlihy & Shavit,
                                          for which all known scalable
                                          solutions
>>>are expensive,
                                          complicated, and difficult to
                                          package as
>>>j.u.c components. Although
                                          there are some special cases
                                          like
>>>SNZI for detecting when
                                          counts hit the particular
                                          value of zero.
>>
>>?? Thanks, I didn't know about
                                          these. I've tried Shavit &
                                          others's
>>flat-combining method though
                                          for the counter, but it was
                                          too slow.
>>
>>>
>>>(This seems to be the main
                                          bottleneck in Disruptor-like
                                          designs.)
>>
>>?? Yes, that's why I started
                                          to focus on it. I think it's
                                          the only part of this
                                          disruptor
>>that is potentially
                                          proportionnal-or-worse to the
                                          number of threads (one could
                                          think
>>of ring buffer scanning by
                                          consumers too, but it should
                                          be possible to use
>>modulo so that each entry
                                          doesn't get scanned by all
                                          consumers).
>>
>>>
>>>-Doug 
>>
>>-Jeff
>>
>>_______________________________________________
>>Concurrency-interest
                                              mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>>_______________________________________________
>>Concurrency-interest mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> 
>
>
>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/c69215d3/attachment-0001.html>

From mikeb01 at gmail.com  Wed Aug 24 17:45:45 2011
From: mikeb01 at gmail.com (Michael Barker)
Date: Wed, 24 Aug 2011 22:45:45 +0100
Subject: [concurrency-interest] Re : Re : concurrent counter :
	incrementAndGet
In-Reply-To: <1313835474.94237.YahooMailRC@web29212.mail.ird.yahoo.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
	<4E4D70A7.7060501@cs.oswego.edu>
	<1313835474.94237.YahooMailRC@web29212.mail.ird.yahoo.com>
Message-ID: <CALwNKeSV5tgF9f8Hdz1VLgnUXQerxOAF-ZvrbX_4skKN=fy7Rg@mail.gmail.com>

>>(This seems to be the main bottleneck in Disruptor-like designs.)
>
> ?? Yes, that's why I started to focus on it. I think it's the only part of
> this disruptor
> that is potentially proportionnal-or-worse to the number of threads (one
> could think
> of ring buffer scanning by consumers too, but it should be possible to use
> modulo so that each entry doesn't get scanned by all consumers).

Yup, the counter is the significant point of contention in the
disruptor design.  However strict ordering was one of the key design
goals.  Annoyingly, atomic increment with Hotspot on Intel is
implemented LOCK CMPXCHG, we think a significant performance boost
would come from using a LOCK XADD instead.

Mike.


From ashwin.jayaprakash at gmail.com  Wed Aug 24 18:04:22 2011
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 24 Aug 2011 15:04:22 -0700
Subject: [concurrency-interest] Concurrent indexed queue
Message-ID: <CAF9YjSDAT3uFjhfUoQ=uVknHnVi-K0n29-ubKO_WZpfxWvuvfg@mail.gmail.com>

Wouldn't you need multiple datastructures to have indexed access to a queue?

Perhaps something like this?

   1. The simple queue on the left just ensures the insertion order
   2. But first you'd have to use a map where the values are a list of
   nodes. Each node in that list has a certain version of the data
   3. Add the node to that list (middle of figure) atomically, then enqueue
   a pointer to that node into the main queue
   4. The consumer will keep reading from the head of the queue, follow the
   pointer to the versioned node (the list in the middle)
      1. It will remove the first versioned node from the list in the middle
      2. If there are any other nodes, then it will keep going down that
      list and toggle those nodes are alreadySeen until it hits the end
   5. Obviously it will see this versioned chain again further down the
   queue. But it now knows that it has already seen it, so it will clear that
   versioned node
      1. It can follow that chain if there are any newer versions (Like Step
      4.2)



               ^                      +-----------+
               |                      |<head>     |<---+
               |                      +----^------+    |
(ConcurrentLinkedHashMap-LRU)
          +----+----+                      |           |
+------------+------------+
          |         |                 +----+------+    |   |
|key-ABC     |
      C   |         |+--------------->|version    |    |   |
|            |
      o   |         |                 |data       |    |
+------------|------------+
      n   |         |                 |alreadySeen|    |   |
|key-XYZ     |
      c   +----^----+                 +----^------+    +---+
|            |
      u        |                           |               |
|            |
      r        |                           |
+------------|------------+
      r   +----+----+                 +----+------+        |
|key-GHI     |
      e   |         |                 |version    |        |
|            |
      n   |         |                 |data       |        |
|            |
      t   |         |         +------>|alreadySeen|
+------------+------------+
      L   |         |         |       +-----------+
|                         |
      i   +----^----+         |                            |
.             |
      n        |              |      (Custom node list     |
.             |
      k        |              |       with atomic ops)     |
.             |
      e   +----+----+         |
|                         |
      d   |         |         |
|                         |
      Q   |         +---------+
|                         |
      u   |         |
|                         |
      e   |         |
|                         |
      u   +----^----+
+-------------------------+
      e        |
               |
          +---------+
          |         |



This would be a really useful datastructure to have as an open source
library (hint hint).

Regards,
Ashwin Jayaprakash.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/2102b640/attachment.html>

From nathan.reynolds at oracle.com  Wed Aug 24 18:34:32 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 24 Aug 2011 15:34:32 -0700
Subject: [concurrency-interest] Re : Re : concurrent counter :
	incrementAndGet
In-Reply-To: <CALwNKeSV5tgF9f8Hdz1VLgnUXQerxOAF-ZvrbX_4skKN=fy7Rg@mail.gmail.com>
References: <1313276265.57480.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CA+1LWGEVTsaNMry-UeqWOcyNcYL178u4nGbDRv-uOczFoULSmg@mail.gmail.com>
	<1313319786.16782.YahooMailRC@web29210.mail.ird.yahoo.com>
	<4E4D70A7.7060501@cs.oswego.edu>
	<1313835474.94237.YahooMailRC@web29212.mail.ird.yahoo.com>
	<CALwNKeSV5tgF9f8Hdz1VLgnUXQerxOAF-ZvrbX_4skKN=fy7Rg@mail.gmail.com>
Message-ID: <4E557C78.1040105@oracle.com>

David Dice's Weblog has some interesting information on this and 
confirms LOCK XADD will perform better.  
http://blogs.oracle.com/dave/entry/atomic_fetch_and_add_vs

There is a HotSpot bug for this as well.  
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7023898

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 8/24/2011 2:45 PM, Michael Barker wrote:
>>> (This seems to be the main bottleneck in Disruptor-like designs.)
>>     Yes, that's why I started to focus on it. I think it's the only part of
>> this disruptor
>> that is potentially proportionnal-or-worse to the number of threads (one
>> could think
>> of ring buffer scanning by consumers too, but it should be possible to use
>> modulo so that each entry doesn't get scanned by all consumers).
> Yup, the counter is the significant point of contention in the
> disruptor design.  However strict ordering was one of the key design
> goals.  Annoyingly, atomic increment with Hotspot on Intel is
> implemented LOCK CMPXCHG, we think a significant performance boost
> would come from using a LOCK XADD instead.
>
> Mike.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110824/840a9f19/attachment.html>

From jws at csse.unimelb.edu.au  Wed Aug 24 20:45:32 2011
From: jws at csse.unimelb.edu.au (Jeff Schultz)
Date: Thu, 25 Aug 2011 10:45:32 +1000
Subject: [concurrency-interest] Concurrent indexed queue
In-Reply-To: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>
References: <1314202202.87147.YahooMailMobile@web36201.mail.mud.yahoo.com>
Message-ID: <20110825004532.GA19219@mulga.csse.unimelb.edu.au>

On Wed, Aug 24, 2011 at 09:10:02AM -0700, Rohit Reja wrote:
>  To clarify a bit further, I dont need to keep any history of
> quotes, I just need to keep the latest quote for an instrument. 

> So for e.g. One of the possible solution that i thought was to keep
> a CHM of instrumentid (key) and quote(value).

> And an event queue of "instrumentids". so lets say events comes as 
> E1( I1, Q1)
> E2(I2, q1)
> E3(I1, Q2)

> Then CHM will have 2 entries. While the Queue will have 3 entries
> {I1, I2, I1}.  Now when consumer consumes I1 it removes I1&#39;s entry
> from CHM, so that consumer can ignore the event I1 next time when it
> retrieves I1 from queue. 

This does mean that more than one consumer can be processing quotes
for the same instrument.  Does that matter?


> This implementation would cause the event queue to grow indefinetely
> as the consumer is slower.

> To avoid this , I can simply check if I1 is contained in the queue
> before putting in queue but that would make things definetely very
> slow.

It looks like what you want is a collection of "slots", one for each
ID, which either contain an unprocessed quote or are empty, along with
a queue of IDs where, for each ID with a non-empty corresponding slot,
either the queue contains the ID or a consumer is "about to" process
the ID.

You don't need to inspect the queue to see if the new quote will be
processed by a previously queued entry, just check if the slot is
empty.

This can be done with a collection of AtomicReference.  Make some form
of mapping from ID to AtomicReference<Quote> and a BlockingQueue<ID>.

To publish a new (id, quote):

    look up the AtomicReference ref for id
    old = ref.getAndSet(quote)
    if(old == null)
        put id in the queue

To consume a quote:

    take an id from the queue
    look up the AtomicReference ref for id
    quote = ref.getAndSet(null)
    if(quote != null)
        process the quote

(As there is a one-to-one relationship between IDs and the
AtomicReferences, one could just queue the latter and save the
consumer the lookup.)

This has the same potential problem of multiple consumers processing
different quotes for the one instrument, of course.


    Jeff Schultz

From dig at illinois.edu  Wed Aug 24 23:07:37 2011
From: dig at illinois.edu (Danny Dig)
Date: Wed, 24 Aug 2011 22:07:37 -0500
Subject: [concurrency-interest] tools for parallel performance debugging
Message-ID: <CADiYQu5CECcZms+J66u_u6CcM_f55cAxQ-pxzV52hxSZyuyRPA@mail.gmail.com>

This community has been providing some great library features for
taming parallelism in Java.

When it comes to tools for parallel performance debugging, it looks to
me that Java is trailing behind C++/Intel's ParallelStudio, or
C#/Microsoft's Visual Studio. When working with Java programs, what
are the tools that you recommend for:
- identifying threading correctness issues (e.g., data-races, deadlocks),
- identifying performance issues (load imbalance, synchronization
contention, false sharing)

best,
Danny

-- 
Danny Dig
Visiting Research Assistant Professor at UIUC

http://netfiles.uiuc.edu/dig/www

Motto: "Success is not for the chosen few but for the few who choose"

From nathan.reynolds at oracle.com  Thu Aug 25 12:01:32 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 25 Aug 2011 09:01:32 -0700
Subject: [concurrency-interest] tools for parallel performance debugging
In-Reply-To: <CADiYQu5CECcZms+J66u_u6CcM_f55cAxQ-pxzV52hxSZyuyRPA@mail.gmail.com>
References: <CADiYQu5CECcZms+J66u_u6CcM_f55cAxQ-pxzV52hxSZyuyRPA@mail.gmail.com>
Message-ID: <4E5671DC.6090909@oracle.com>

For identifying threading correctness issues, I recommend using Java 
Path Finder (http://babelfish.arc.nasa.gov/trac/jpf).  It executes every 
possible thread scheduling combination.  It does so by trying one thread 
scheduling combination and then back tracking (i.e. revert the state of 
the program) to simulate other combinations.  If there is a data race or 
deadlock, it will find it and report it.  It doesn't just tell you that 
a data race or deadlock exists but the steps that got to that state.  
Because of the number of combinations, it is best to run it on a small 
piece of the overall program.

As for synchronization contention, there are several tools available.  
Looking through a dump of call stacks and seeing where the most threads 
are blocked works for cases where the lock is dropping the throughput by 
10% or more.  The call stacks are easy to obtain but can be time 
consuming to sift through.  JRockit's Mission Control can tell you 
exactly which locks are most contended and give you the call stacks of 
where that lock is used.  I am pretty sure most of the profilers can 
tell you which locks are most contended, but since JRockit's Mission 
Control is free for me I haven't looked at the profilers for a while.  I 
thought that JConsole or some tool in the JDK could tell you the most 
contended lock... but I am really not sure of that.

As for false sharing, this is something that only the processor can tell 
you.  For Intel, I *think* VTune can tell you about false sharing.  I 
know that VTune can deal with Java optimized code.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 8/24/2011 8:07 PM, Danny Dig wrote:
> This community has been providing some great library features for
> taming parallelism in Java.
>
> When it comes to tools for parallel performance debugging, it looks to
> me that Java is trailing behind C++/Intel's ParallelStudio, or
> C#/Microsoft's Visual Studio. When working with Java programs, what
> are the tools that you recommend for:
> - identifying threading correctness issues (e.g., data-races, deadlocks),
> - identifying performance issues (load imbalance, synchronization
> contention, false sharing)
>
> best,
> Danny
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110825/480bf264/attachment.html>

From mohanr at fss.co.in  Fri Aug 26 00:00:16 2011
From: mohanr at fss.co.in (Mohan Radhakrishnan)
Date: Fri, 26 Aug 2011 09:30:16 +0530
Subject: [concurrency-interest] tools for parallel performance debugging
In-Reply-To: <4E5671DC.6090909@oracle.com>
Message-ID: <65F4195D4E2C6042BA66D4114E5BB9DC0C5CC9@fssbemail.fss.india>

Hi,

 

       I came across http://hpctoolkit.org/ which seems to be for large
parallel systems also. 

Thanks,

Mohan

 

________________________________

From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan
Reynolds
Sent: Thursday, August 25, 2011 9:32 PM
To: Danny Dig
Cc: Concurrency
Subject: Re: [concurrency-interest] tools for parallel performance
debugging

 

For identifying threading correctness issues, I recommend using Java
Path Finder (http://babelfish.arc.nasa.gov/trac/jpf).  It executes every
possible thread scheduling combination.  It does so by trying one thread
scheduling combination and then back tracking (i.e. revert the state of
the program) to simulate other combinations.  If there is a data race or
deadlock, it will find it and report it.  It doesn't just tell you that
a data race or deadlock exists but the steps that got to that state.
Because of the number of combinations, it is best to run it on a small
piece of the overall program.

As for synchronization contention, there are several tools available.
Looking through a dump of call stacks and seeing where the most threads
are blocked works for cases where the lock is dropping the throughput by
10% or more.  The call stacks are easy to obtain but can be time
consuming to sift through.  JRockit's Mission Control can tell you
exactly which locks are most contended and give you the call stacks of
where that lock is used.  I am pretty sure most of the profilers can
tell you which locks are most contended, but since JRockit's Mission
Control is free for me I haven't looked at the profilers for a while.  I
thought that JConsole or some tool in the JDK could tell you the most
contended lock... but I am really not sure of that.

As for false sharing, this is something that only the processor can tell
you.  For Intel, I *think* VTune can tell you about false sharing.  I
know that VTune can deal with Java optimized code.

Nathan Reynolds
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>  |
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/>  | Server Technology 


On 8/24/2011 8:07 PM, Danny Dig wrote: 

This community has been providing some great library features for
taming parallelism in Java.
 
When it comes to tools for parallel performance debugging, it looks to
me that Java is trailing behind C++/Intel's ParallelStudio, or
C#/Microsoft's Visual Studio. When working with Java programs, what
are the tools that you recommend for:
- identifying threading correctness issues (e.g., data-races,
deadlocks),
- identifying performance issues (load imbalance, synchronization
contention, false sharing)
 
best,
Danny
 


DISCLAIMER:
==========================================================================================================================================================The information contained in this e-mail message may be privileged and/or confidential and protected from disclosure under applicable law. It is intended only for the individual to whom or entity to which it is addressed as shown at the beginning of the message. If the reader of this message is not the intended recipient, or if the employee or agent responsible for delivering the message is not an employee or agent of the intended recipient, you are hereby notified that any review, dissemination,distribution, use, or copying of this message is strictly prohibited. If you have received this message in error, please notify us immediately by return e-mail and permanently delete this message and your reply to the extent it includes this message. Any views or opinions presented in this message or attachments are those of the author and do not necessarily represent those of the Company. All e-mails and attachments sent and received are subject to monitoring, reading, and archival by the Company.==========================================================================================================================================================
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110826/5ec61483/attachment.html>

From dl at cs.oswego.edu  Sun Aug 28 15:24:16 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 28 Aug 2011 15:24:16 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
Message-ID: <4E5A95E0.1080309@cs.oswego.edu>


A candidate replacement for java.util.concurrent.ConcurrentHashMap
is now available as jsr166e.ConcurrentHashMapV8. This version
is much more amenable to upcoming support for aggregate
parallel operations (including, already, method "computeIfAbsent'
using a stand-in MappingFunction type).

The internal design is interestingly different.
Read the internal documentation for details.
(http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log)

Please give it a try, and let us know about experiences.

Links:
     *  API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
     * jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled 
using Java7 javac).
     * Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/



From viktor.klang at gmail.com  Sun Aug 28 15:35:55 2011
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 28 Aug 2011 21:35:55 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5A95E0.1080309@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
Message-ID: <CANPzfU8rwJ83=UhATVOmZ-pYthDnu7ZkBW4ROYD9F8q=ya+JcA@mail.gmail.com>

On Sun, Aug 28, 2011 at 9:24 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> A candidate replacement for java.util.concurrent.**ConcurrentHashMap
> is now available as jsr166e.ConcurrentHashMapV8. This version
> is much more amenable to upcoming support for aggregate
> parallel operations (including, already, method "computeIfAbsent'
>

This is awesome news, thanks Doug!


> using a stand-in MappingFunction type).
>
> The internal design is interestingly different.
> Read the internal documentation for details.
> (http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> jsr166e/ConcurrentHashMapV8.**java?view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log>
> )
>
> Please give it a try, and let us know about experiences.
>
> Links:
>    *  API specs:  http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166edocs/<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/>
>    * jar file: http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166e.jar<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar>(compiled using Java7 javac).
>    * Browsable CVS sources: http://gee.cs.oswego.edu/cgi-**
> bin/viewcvs.cgi/jsr166/src/**jsr166e/<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - Enterprise-Grade Scala from the
Experts

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110828/1f6f501e/attachment.html>

From ben_manes at yahoo.com  Sun Aug 28 17:05:39 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 28 Aug 2011 14:05:39 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <1314565247.43151.yint-ygo-j2me@web38805.mail.mud.yahoo.com>
References: <1314565247.43151.yint-ygo-j2me@web38805.mail.mud.yahoo.com>
Message-ID: <1314565539.35370.YahooMailNeo@web38804.mail.mud.yahoo.com>

reply-all:



________________________________
From: Ben Manes <ben_manes at yahoo.com>
To: dl at cs.oswego.edu
Sent: Sunday, August 28, 2011 2:00 PM
Subject: Re: [concurrency-interest] ConcurrentHashMapV8

If computation is being added, then bulk computation should be implemented as well. This is useful when a computation requires an I/O call, so a batch operation is an important optimization. The simplest approach is to insert placeholder nodes that are populated after the computation has completed. (e.g. see this old decorator that supplied it: http://code.google.com/p/concurrentlinkedhashmap/wiki/SelfPopulatingCache)

It wasn't obvious to me how recursive computations are handled for the same key and appears to livelock. This is an error condition that used to deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock() was true prior to waiting for the computation to finish.

The exceptional computation case may be worth documenting. CHM only fails the computing thread and allows the waiting threads to retry the computation. In other implementations the waiting threads rethrow the exception and don't recompute.

A recompute (a.k.a. refresh) method may also be useful, though can be adequately supplied by usages. This recomputes the value, but allows readers to continue to use the stale value until it completes. This can be useful for cache invalidation where it is preferred vs. exposing the latency to the user (if naively done by removing first).

-Ben

On Sun Aug 28th, 2011 12:24 PM PDT Doug Lea wrote:

>
>A candidate replacement for java.util.concurrent.ConcurrentHashMap
>is now available as jsr166e.ConcurrentHashMapV8. This version
>is much more amenable to upcoming support for aggregate
>parallel operations (including, already, method "computeIfAbsent'
>using a stand-in MappingFunction type).
>
>The internal design is interestingly different.
>Read the internal documentation for details.
>(http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log)
>
>Please give it a try, and let us know about experiences.
>
>Links:
>? ? *? API specs:? http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
>? ? * jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled using Java7 javac).
>? ? * Browsable CVS sources: http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/
>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110828/ca3309f3/attachment.html>

From joe.bowbeer at gmail.com  Sun Aug 28 17:42:29 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 28 Aug 2011 14:42:29 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5A95E0.1080309@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
Message-ID: <CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>

First impressions:

1. I like the MappingFunction name.  I hope the JDK8 name is as clear *and*
specialized.

2. I think the snippet in computeIfAbsent is wrong; should be something
like:

if (map.containsKey(key))
  return map.get(key);
value = mappingFunction.map(key);
if (value != null) {
  map.put(key, value);
}
return value;


Given the restrictions placed on computeIfAbsent's function (invoked
atomically, short & simple, must not attempt to update any other mappings),
I would like to see a more complete example.

3. The javadoc comment on computeVal reads "Implements computeIfAbsent"
which indicates to me that "computeIfAbsent" is an interface.  Even though
this is private-method doc, I suggest "Implementation of computeIfAbsent".
That would be less confusing.

Joe

On Sun, Aug 28, 2011 at 12:24 PM, Doug Lea wrote:

>
> A candidate replacement for java.util.concurrent.**ConcurrentHashMap
> is now available as jsr166e.ConcurrentHashMapV8. This version
> is much more amenable to upcoming support for aggregate
> parallel operations (including, already, method "computeIfAbsent'
> using a stand-in MappingFunction type).
>
> The internal design is interestingly different.
> Read the internal documentation for details.
> (http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> jsr166e/ConcurrentHashMapV8.**java?view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log>
> )
>
> Please give it a try, and let us know about experiences.
>
> Links:
>    *  API specs:  http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166edocs/<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/>
>    * jar file: http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166e.jar<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar>(compiled using Java7 javac).
>    * Browsable CVS sources: http://gee.cs.oswego.edu/cgi-**
> bin/viewcvs.cgi/jsr166/src/**jsr166e/<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110828/ce7e9e33/attachment.html>

From studdugie at gmail.com  Sun Aug 28 22:52:50 2011
From: studdugie at gmail.com (Dane Foster)
Date: Sun, 28 Aug 2011 22:52:50 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5A95E0.1080309@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
Message-ID: <CA+Wxin+imPpY55_SihRZZtem0MFUmpCAJHXJ8Js11qrmyHJ7kQ@mail.gmail.com>

Have you had an opportunity to benchmark this against Cliff Click's
concurrent hash map?

Dane


On Sun, Aug 28, 2011 at 3:24 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> A candidate replacement for java.util.concurrent.**ConcurrentHashMap
> is now available as jsr166e.ConcurrentHashMapV8. This version
> is much more amenable to upcoming support for aggregate
> parallel operations (including, already, method "computeIfAbsent'
> using a stand-in MappingFunction type).
>
> The internal design is interestingly different.
> Read the internal documentation for details.
> (http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> jsr166e/ConcurrentHashMapV8.**java?view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log>
> )
>
> Please give it a try, and let us know about experiences.
>
> Links:
>    *  API specs:  http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166edocs/<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/>
>    * jar file: http://gee.cs.oswego.edu/dl/**jsr166/dist/jsr166e.jar<http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar>(compiled using Java7 javac).
>    * Browsable CVS sources: http://gee.cs.oswego.edu/cgi-**
> bin/viewcvs.cgi/jsr166/src/**jsr166e/<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110828/8ec0fc77/attachment-0001.html>

From dl at cs.oswego.edu  Mon Aug 29 13:37:20 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 29 Aug 2011 13:37:20 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
Message-ID: <4E5BCE50.50205@cs.oswego.edu>

Thanks for all the on- and off- list comments and suggestions!
I committed an update with various improvements.
Some follow-ups:

On 08/28/11 17:42, Joe Bowbeer wrote:
> Given the restrictions placed on computeIfAbsent's function (invoked
> atomically, short & simple, must not attempt to update any other mappings), I
> would like to see a more complete example.

Yes, the javadoc could use an example. Suggestions for a
tiny but informative one would be welcome.

On 08/28/11 17:00, Ben Manes wrote:
> If computation is being added, then bulk computation should be implemented as
> well. This is useful when a computation requires an I/O call, so a batch
> operation is an important optimization. The simplest approach is to insert
> placeholder nodes that are populated after the computation has completed.

Thanks, but the plain version of CHM doesn't itself deal with Futures (aka
placeholders). However, a cache version might do so, possibly based on
a CHM<K, Future<V>>.

> It wasn't obvious to me how recursive computations are handled for the same
> key and appears to livelock. This is an error condition that used to
> deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock() was
> true prior to waiting for the computation to finish.

I'm not sure I follow. If the user function in turn calls computeIfAbsent
for the same key, then it seems irreparably broken in the same sense as,
for example. a user's equals() function that attempts to invoke map.put.

>
> The exceptional computation case may be worth documenting. CHM only fails

It is -- an exception leaves mapping untouched. The more arbitrary decision
is what to do if the computation returns null. ComputeIfAbsent could be defined
to throw NullPointerException, but this is a fairly heavy response to the
case where it turns out that the key has no mapping, since a null return
provides the same information.

> the computing thread and allows the waiting threads to retry the
> computation. In other implementations the waiting threads rethrow the
> exception and don't recompute.

Yes; I think this is the difference between a plain CHM and one that
uses Futures as values.

>
> A recompute (a.k.a. refresh) method may also be useful, though can be
> adequately supplied by usages. This recomputes the value, but allows readers
>  to continue to use the stale value until it completes. This can be useful
> for cache invalidation where it is preferred vs. exposing the latency to the
> user (if naively done by removing first).
>

Thanks! Yes, this (i.e. compute(k, f) vs computeIfAbsent(k, f))
is easy to add, and provides a symmetical API to the two forms
of put, and has some uses, so I added it.

On 08/28/11 22:52, Dane Foster wrote:
> Have you had an opportunity to benchmark this against Cliff Click's
> concurrent hash map?
>

Yes. Results vary across different kinds of tests, depending on operation
mixes, key types, temporal locality of lookups, etc, so I can't
give a better answer than: each is sometimes faster (and
sometimes by a lot) than the other. But the previous scalability
limits due to use of Segments is now gone, so the performance differences
now mostly reflect other design tradeoffs.

-Doug





From crazybob at crazybob.org  Mon Aug 29 13:52:33 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Mon, 29 Aug 2011 10:52:33 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5BCE50.50205@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
Message-ID: <CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>

On Mon, Aug 29, 2011 at 10:37 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> It wasn't obvious to me how recursive computations are handled for the same
>> key and appears to livelock. This is an error condition that used to
>> deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock()
>> was
>> true prior to waiting for the computation to finish.
>>
>
> I'm not sure I follow. If the user function in turn calls computeIfAbsent
> for the same key, then it seems irreparably broken in the same sense as,
> for example. a user's equals() function that attempts to invoke map.put.


It is irreparably broken, but it's an easy and common mistake to make. It's
difficult to identify the problem when the program just deadlocks vs.
throwing an explicit exception that identifies the at fault key.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110829/30d4020c/attachment.html>

From ben_manes at yahoo.com  Mon Aug 29 14:37:34 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Mon, 29 Aug 2011 11:37:34 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5BCE50.50205@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
Message-ID: <1314643054.98250.YahooMailNeo@web38803.mail.mud.yahoo.com>

> Thanks, but the plain version of CHM doesn't itself deal with Futures

MapMaker's entry objects are basically light-weight futures and we plan on adding bulk computation eventually. Since computation is done by holding the node's lock and inserting it prior to computation, I don't see why a bulk computation would be much different.

> If the user function in turn calls computeIfAbsent for the same key, then it seems irreparably broken in the same sense as, for example. a user's equals() function that attempts to invoke map.put.


Its broken, but failing fast helps identify the issue and is easier to recover from than livelocks or deadlocks are. Any computation function that updates the map itself is wrong, but will occur in the wild and surprise the developer when it seems to randomly (and silently) break.


________________________________
From: Doug Lea <dl at cs.oswego.edu>
To: concurrency-interest at cs.oswego.edu
Sent: Monday, August 29, 2011 10:37 AM
Subject: Re: [concurrency-interest] ConcurrentHashMapV8

Thanks for all the on- and off- list comments and suggestions!
I committed an update with various improvements.
Some follow-ups:

On 08/28/11 17:42, Joe Bowbeer wrote:
> Given the restrictions placed on computeIfAbsent's function (invoked
> atomically, short & simple, must not attempt to update any other mappings), I
> would like to see a more complete example.

Yes, the javadoc could use an example. Suggestions for a
tiny but informative one would be welcome.

On 08/28/11 17:00, Ben Manes wrote:
> If computation is being added, then bulk computation should be implemented as
> well. This is useful when a computation requires an I/O call, so a batch
> operation is an important optimization. The simplest approach is to insert
> placeholder nodes that are populated after the computation has completed.

Thanks, but the plain version of CHM doesn't itself deal with Futures (aka
placeholders). However, a cache version might do so, possibly based on
a CHM<K, Future<V>>.

> It wasn't obvious to me how recursive computations are handled for the same
> key and appears to livelock. This is an error condition that used to
> deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock() was
> true prior to waiting for the computation to finish.

I'm not sure I follow. If the user function in turn calls computeIfAbsent
for the same key, then it seems irreparably broken in the same sense as,
for example. a user's equals() function that attempts to invoke map.put.

>
> The exceptional computation case may be worth documenting. CHM only fails

It is -- an exception leaves mapping untouched. The more arbitrary decision
is what to do if the computation returns null. ComputeIfAbsent could be defined
to throw NullPointerException, but this is a fairly heavy response to the
case where it turns out that the key has no mapping, since a null return
provides the same information.

> the computing thread and allows the waiting threads to retry the
> computation. In other implementations the waiting threads rethrow the
> exception and don't recompute.

Yes; I think this is the difference between a plain CHM and one that
uses Futures as values.

>
> A recompute (a.k.a. refresh) method may also be useful, though can be
> adequately supplied by usages. This recomputes the value, but allows readers
>? to continue to use the stale value until it completes. This can be useful
> for cache invalidation where it is preferred vs. exposing the latency to the
> user (if naively done by removing first).
>

Thanks! Yes, this (i.e. compute(k, f) vs computeIfAbsent(k, f))
is easy to add, and provides a symmetical API to the two forms
of put, and has some uses, so I added it.

On 08/28/11 22:52, Dane Foster wrote:
> Have you had an opportunity to benchmark this against Cliff Click's
> concurrent hash map?
>

Yes. Results vary across different kinds of tests, depending on operation
mixes, key types, temporal locality of lookups, etc, so I can't
give a better answer than: each is sometimes faster (and
sometimes by a lot) than the other. But the previous scalability
limits due to use of Segments is now gone, so the performance differences
now mostly reflect other design tradeoffs.

-Doug




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110829/4fb9891f/attachment.html>

From dig at illinois.edu  Mon Aug 29 14:57:43 2011
From: dig at illinois.edu (Danny Dig)
Date: Mon, 29 Aug 2011 13:57:43 -0500
Subject: [concurrency-interest] tools for parallel performance debugging
In-Reply-To: <4E5671DC.6090909@oracle.com>
References: <CADiYQu5CECcZms+J66u_u6CcM_f55cAxQ-pxzV52hxSZyuyRPA@mail.gmail.com>
	<4E5671DC.6090909@oracle.com>
Message-ID: <CADiYQu6jx2j2KTZDwFEGzjw6H51PsKnmmXXKNfm4kB-dHHxAqg@mail.gmail.com>

Thanks Nathan. These tools are all good suggestions. I am curious what
other tools do people on this mailing list use for parallel
programming in Java.

best,
Danny


On Thu, Aug 25, 2011 at 11:01 AM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:
> For identifying threading correctness issues, I recommend using Java Path
> Finder (http://babelfish.arc.nasa.gov/trac/jpf).? It executes every possible
> thread scheduling combination.? It does so by trying one thread scheduling
> combination and then back tracking (i.e. revert the state of the program) to
> simulate other combinations.? If there is a data race or deadlock, it will
> find it and report it.? It doesn't just tell you that a data race or
> deadlock exists but the steps that got to that state.? Because of the number
> of combinations, it is best to run it on a small piece of the overall
> program.
>
> As for synchronization contention, there are several tools available.
> Looking through a dump of call stacks and seeing where the most threads are
> blocked works for cases where the lock is dropping the throughput by 10% or
> more.? The call stacks are easy to obtain but can be time consuming to sift
> through.? JRockit's Mission Control can tell you exactly which locks are
> most contended and give you the call stacks of where that lock is used.? I
> am pretty sure most of the profilers can tell you which locks are most
> contended, but since JRockit's Mission Control is free for me I haven't
> looked at the profilers for a while.? I thought that JConsole or some tool
> in the JDK could tell you the most contended lock... but I am really not
> sure of that.
>
> As for false sharing, this is something that only the processor can tell
> you.? For Intel, I *think* VTune can tell you about false sharing.? I know
> that VTune can deal with Java optimized code.
>
> Nathan Reynolds | Consulting Member of Technical Staff | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 8/24/2011 8:07 PM, Danny Dig wrote:
>
> This community has been providing some great library features for
> taming parallelism in Java.
>
> When it comes to tools for parallel performance debugging, it looks to
> me that Java is trailing behind C++/Intel's ParallelStudio, or
> C#/Microsoft's Visual Studio. When working with Java programs, what
> are the tools that you recommend for:
> - identifying threading correctness issues (e.g., data-races, deadlocks),
> - identifying performance issues (load imbalance, synchronization
> contention, false sharing)
>
> best,
> Danny
>
>



-- 
Danny Dig
Visiting Research Assistant Professor at UIUC

http://netfiles.uiuc.edu/dig/www

Motto: "Success is not for the chosen few but for the few who choose"


From nathan.reynolds at oracle.com  Mon Aug 29 15:04:31 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 29 Aug 2011 12:04:31 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <1314643054.98250.YahooMailNeo@web38803.mail.mud.yahoo.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<1314643054.98250.YahooMailNeo@web38803.mail.mud.yahoo.com>
Message-ID: <4E5BE2BF.2090904@oracle.com>

 >> If the user function in turn calls computeIfAbsent for the same key, 
then it seems irreparably broken in the same sense as, for example. a 
user's equals() function that attempts to invoke map.put.

 > Its broken, but failing fast helps identify the issue and is easier 
to recover from than livelocks or deadlocks are. Any computation 
function that updates the map itself is wrong, but will occur in the 
wild and surprise the developer when it seems to randomly (and silently) 
break.

When working on very large complex application servers, the complex 
interactions between objects might cause a computation function to 
inadvertently update the map.  This is due to multiple developers make 
changes to different pieces of code and creating the loop.  Throwing an 
exception will be invaluable since the call stack will show the 
developers how the loop was created and it will allow for the 
application server to have a chance at recovering.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 8/29/2011 11:37 AM, Ben Manes wrote:
> > Thanks, but the plain version of CHM doesn't itself deal with Futures
>
> MapMaker's entry objects are basically light-weight futures and we 
> plan on adding bulk computation eventually. Since computation is done 
> by holding the node's lock and inserting it prior to computation, I 
> don't see why a bulk computation would be much different.
>
> > If the user function in turn calls computeIfAbsent for the same key, 
> then it seems irreparably broken in the same sense as, for example. a 
> user's equals() function that attempts to invoke map.put.
>
> Its broken, but failing fast helps identify the issue and is easier to 
> recover from than livelocks or deadlocks are. Any computation function 
> that updates the map itself is wrong, but will occur in the wild and 
> surprise the developer when it seems to randomly (and silently) break.
>
> ------------------------------------------------------------------------
> *From:* Doug Lea <dl at cs.oswego.edu>
> *To:* concurrency-interest at cs.oswego.edu
> *Sent:* Monday, August 29, 2011 10:37 AM
> *Subject:* Re: [concurrency-interest] ConcurrentHashMapV8
>
> Thanks for all the on- and off- list comments and suggestions!
> I committed an update with various improvements.
> Some follow-ups:
>
> On 08/28/11 17:42, Joe Bowbeer wrote:
> > Given the restrictions placed on computeIfAbsent's function (invoked
> > atomically, short & simple, must not attempt to update any other 
> mappings), I
> > would like to see a more complete example.
>
> Yes, the javadoc could use an example. Suggestions for a
> tiny but informative one would be welcome.
>
> On 08/28/11 17:00, Ben Manes wrote:
> > If computation is being added, then bulk computation should be 
> implemented as
> > well. This is useful when a computation requires an I/O call, so a batch
> > operation is an important optimization. The simplest approach is to 
> insert
> > placeholder nodes that are populated after the computation has 
> completed.
>
> Thanks, but the plain version of CHM doesn't itself deal with Futures (aka
> placeholders). However, a cache version might do so, possibly based on
> a CHM<K, Future<V>>.
>
> > It wasn't obvious to me how recursive computations are handled for 
> the same
> > key and appears to livelock. This is an error condition that used to
> > deadlocked MapMaker. I fixed this by failing fast if 
> Thread.holdsLock() was
> > true prior to waiting for the computation to finish.
>
> I'm not sure I follow. If the user function in turn calls computeIfAbsent
> for the same key, then it seems irreparably broken in the same sense as,
> for example. a user's equals() function that attempts to invoke map.put.
>
> >
> > The exceptional computation case may be worth documenting. CHM only 
> fails
>
> It is -- an exception leaves mapping untouched. The more arbitrary 
> decision
> is what to do if the computation returns null. ComputeIfAbsent could 
> be defined
> to throw NullPointerException, but this is a fairly heavy response to the
> case where it turns out that the key has no mapping, since a null return
> provides the same information.
>
> > the computing thread and allows the waiting threads to retry the
> > computation. In other implementations the waiting threads rethrow the
> > exception and don't recompute.
>
> Yes; I think this is the difference between a plain CHM and one that
> uses Futures as values.
>
> >
> > A recompute (a.k.a. refresh) method may also be useful, though can be
> > adequately supplied by usages. This recomputes the value, but allows 
> readers
> >  to continue to use the stale value until it completes. This can be 
> useful
> > for cache invalidation where it is preferred vs. exposing the 
> latency to the
> > user (if naively done by removing first).
> >
>
> Thanks! Yes, this (i.e. compute(k, f) vs computeIfAbsent(k, f))
> is easy to add, and provides a symmetical API to the two forms
> of put, and has some uses, so I added it.
>
> On 08/28/11 22:52, Dane Foster wrote:
> > Have you had an opportunity to benchmark this against Cliff Click's
> > concurrent hash map?
> >
>
> Yes. Results vary across different kinds of tests, depending on operation
> mixes, key types, temporal locality of lookups, etc, so I can't
> give a better answer than: each is sometimes faster (and
> sometimes by a lot) than the other. But the previous scalability
> limits due to use of Segments is now gone, so the performance differences
> now mostly reflect other design tradeoffs.
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110829/06f7b8db/attachment-0001.html>

From fry at google.com  Mon Aug 29 18:02:06 2011
From: fry at google.com (Charles Fry)
Date: Mon, 29 Aug 2011 18:02:06 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5BCE50.50205@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
Message-ID: <CAOhq40p5VwW0nbdt0XFmi-2vVhkhVHDHXc+djjJ5WspjXpKp0g@mail.gmail.com>

Hi Doug,

This is an exciting development. MapMaker has definitely proved the value of
coupling maps with computation, and it's great to see low level support for
that.

There are a few typos in the new compute javadocs:

- "with he given key"
- the pseudocode doesn't return in the first if clause

Also, it would be nice to document what "operations on this map by other
threads may be blocked while computation is in progress." For example, I
assume that get will never block on computation, but that any writes will.

Charles

On Mon, Aug 29, 2011 at 13:37, Doug Lea <dl at cs.oswego.edu> wrote:

> Thanks for all the on- and off- list comments and suggestions!
> I committed an update with various improvements.
> Some follow-ups:
>
>
> On 08/28/11 17:42, Joe Bowbeer wrote:
>
>> Given the restrictions placed on computeIfAbsent's function (invoked
>> atomically, short & simple, must not attempt to update any other
>> mappings), I
>> would like to see a more complete example.
>>
>
> Yes, the javadoc could use an example. Suggestions for a
> tiny but informative one would be welcome.
>
>
> On 08/28/11 17:00, Ben Manes wrote:
>
>> If computation is being added, then bulk computation should be implemented
>> as
>> well. This is useful when a computation requires an I/O call, so a batch
>> operation is an important optimization. The simplest approach is to insert
>> placeholder nodes that are populated after the computation has completed.
>>
>
> Thanks, but the plain version of CHM doesn't itself deal with Futures (aka
> placeholders). However, a cache version might do so, possibly based on
> a CHM<K, Future<V>>.
>
>
>  It wasn't obvious to me how recursive computations are handled for the
>> same
>> key and appears to livelock. This is an error condition that used to
>> deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock()
>> was
>> true prior to waiting for the computation to finish.
>>
>
> I'm not sure I follow. If the user function in turn calls computeIfAbsent
> for the same key, then it seems irreparably broken in the same sense as,
> for example. a user's equals() function that attempts to invoke map.put.
>
>
>
>> The exceptional computation case may be worth documenting. CHM only fails
>>
>
> It is -- an exception leaves mapping untouched. The more arbitrary decision
> is what to do if the computation returns null. ComputeIfAbsent could be
> defined
> to throw NullPointerException, but this is a fairly heavy response to the
> case where it turns out that the key has no mapping, since a null return
> provides the same information.
>
>
>  the computing thread and allows the waiting threads to retry the
>> computation. In other implementations the waiting threads rethrow the
>> exception and don't recompute.
>>
>
> Yes; I think this is the difference between a plain CHM and one that
> uses Futures as values.
>
>
>
>> A recompute (a.k.a. refresh) method may also be useful, though can be
>> adequately supplied by usages. This recomputes the value, but allows
>> readers
>>  to continue to use the stale value until it completes. This can be useful
>> for cache invalidation where it is preferred vs. exposing the latency to
>> the
>> user (if naively done by removing first).
>>
>>
> Thanks! Yes, this (i.e. compute(k, f) vs computeIfAbsent(k, f))
> is easy to add, and provides a symmetical API to the two forms
> of put, and has some uses, so I added it.
>
>
> On 08/28/11 22:52, Dane Foster wrote:
>
>> Have you had an opportunity to benchmark this against Cliff Click's
>> concurrent hash map?
>>
>>
> Yes. Results vary across different kinds of tests, depending on operation
> mixes, key types, temporal locality of lookups, etc, so I can't
> give a better answer than: each is sometimes faster (and
> sometimes by a lot) than the other. But the previous scalability
> limits due to use of Segments is now gone, so the performance differences
> now mostly reflect other design tradeoffs.
>
> -Doug
>
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110829/864a5159/attachment.html>

From zhong.j.yu at gmail.com  Mon Aug 29 19:25:20 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 29 Aug 2011 18:25:20 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5A95E0.1080309@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
Message-ID: <CACuKZqENgZqnFKsU=UTgrDXFDACJZ5qB-Ka1Qgpt76isiH92iQ@mail.gmail.com>

This is great, a very much needed feature.

About recursive computeIfAbsent() calls (on the same key) - the
behavior can be modeled the same as nested transactions: the thread
that locks the key can update the entry, and a following read in the
same thread will see that value; however the value is only committed,
i.e. visible to other threads, upon the outermost unlock action. All
threads can read the latest committed value of an entry without
locking.

I have just implemented something similar, which I'll attach later.
The code is very straightforward, given the facility of
lock-update-unlock an entry. Actually, maybe ConcurrentHashMap should
expose this facility; value based locking ( lock(v)..unlock(v) ) has
been desired by some people, but there are no well known
implementations.

If ConcurrentHashMap exposes per-entry lock-update-unlock facility,
computeIfAbsent() impl would be so trivial, it doesn't need to be
provided by JDK, everybody can write it.


Zhong Yu

---------------------------------

// code below is released to public domain by the author

public interface Provider<T>
{
    T get();
}

public class ConcurrentScope extends AtomicHashMap<Object,Object>
{
    public <T> T computeIfAbsent(Object key, Provider<T> provider)
    {
        Object val = super.get(key);
        if(val==null)
        {
            if(provider==null)
                return null;

            try( Entry<Object> entry = super.lock(key) )
            {
                val = entry.getValue();
                if(val==null)
                {
                    val = provider.get();  // alien code. may block,
throw, recurse
                    entry.setValue(val);
                }
            } //auto unlock
        }
        return (T)val;
    }
}

// AtomicHashMap.java ---------------------------

/**
 * A concurrent hash map where an entry can be individually locked to
implement atomic operations.
 * Example usages:
 * <pre>
 * import ohm.infra.util.AtomicHashMap;
 * import ohm.infra.util.AtomicHashMap.Entry;
 *
 *     AtomicHashMap&lt;K,V> map = new AtomicHashMap&lt;>();
 *     Entry&lt;V> entry = map.lock(key)
 *     try
 *     {
 *         v1 = entry.getValue();
 *         ...
 *         entry.setValue(v2);
 *     }
 *     finally
 *     {   entry.unlock();    }
 * </pre>
 * <tt>AtomicHashMap.Entry</tt> is <tt>AutoCloseable</tt>, the
try-with-resource syntax would be simpler to use
 * <pre>
 *
 *     try( Entry&lt;V> entry = map.lock(key) ) // auto unlock
 *     {
 *        v1 = entry.getValue();
 *        ...
 *        entry.setValue(v2);
 *     }
 * </pre>
 * <p>
 *     The only way to get an entry for a <tt>key</tt> is through
{@link #lock lock(key)}.
 *     The entry must be used in the same thread that locked it; once
it's unlocked,
 *     the object representing the entry shall not be reused. To get
the entry again,
 *     {@link #lock lock(key)} must be invoked again. Nested
lock-unlock on the same key are allowed.
 *     Upon the outermost unlock action, the value set by the last
<tt>setValue()</tt> is "committed".
 * </p>
 * <p>
 *     {@link #get get(key) } reads the latest committed value without
locking. It's like
 *     a volatile read; its performance is on par with {@link
java.util.concurrent.ConcurrentHashMap#get}.
 * </p>
 * <p>
 *     Implementation note: an AtomicHashMap is internally backed up
by a {@link java.util.concurrent.ConcurrentHashMap}.
 *     Every entry is backed up by a {@link
java.util.concurrent.locks.ReentrantLock}.
 *     To save space, an entry is evicted from the internal map when
the entry is unlocked
 *     and its committed value is null. If the entry is needed again,
{@link #lock lock(key)} will create a new one.
 *     The "active entries" of the map, as referred to by {@link
#size()} and {@link #keySet()},
 *     are entries that are either being locked or having non-null
committed values.
 * </p>
 *
 * @param <K> type of keys
 * @param <V> type of values
 */
public class AtomicHashMap<K,V>
{
    /**
     * An entry in a AtomicHashMap.
     *
     * @param <V> type of value
     * @see ohm.infra.util.to_lea.AtomicHashMap
     */
    public interface Entry<V> extends AutoCloseable
    {
        V getValue();

        void setValue(V value);

        /**
         * @return number of times the current thread has locked the
entry (minus unlocks).
         */
        int getLockCount();

        /**
         * Unlock the entry; if lock count is 0, commit the value.
         */
        void unlock();

        /**
         * Same as {@link #unlock() }
         */
        void close();
    }

    static class EntryImpl<V> extends ReentrantLock implements Entry<V>
    {
        final Object key;
        final ConcurrentHashMap map;
        EntryImpl(boolean fair, Object key, ConcurrentHashMap map)
        {
            super(fair);
            this.key = key;
            this.map = map;
        }

        public int getLockCount(){ return super.getHoldCount(); }


        V workingValue;
        volatile V committedValue;
        boolean retired;

        // get/set must be called by the owner thread. we have given
plenty of warnings
        // so we skip the check of `super.isHeldByCurrentThread()` here.
        // if client violates that, `workingValue` is messed up; no
harm to our structure.

        public V getValue()
        {
            return workingValue;
        }
        public void setValue(V value)
        {
            workingValue = value;
        }

        // if false, entry is retired
        boolean acquire()
        {
            super.lock();
            if(!retired)
                return true;
            // retired
            super.unlock(); // another sucker may be blocked on acquire() me.
            return false;
        }

        public void unlock()
        {
            if(super.getHoldCount()==1) // outermost unlock
            {
                committedValue = workingValue;

                if(workingValue==null)
                {
                    retired=true;
                    map.remove(key);
                }
            }

            super.unlock();
        }

        public void close()
        {
            unlock();
        }
    }

    final ConcurrentHashMap<K,EntryImpl<V>> map;
    final boolean fair;

    /**
     * With default {@link java.util.concurrent.ConcurrentHashMap}
constructor parameters, and unfair locking.
     */
    public AtomicHashMap()
    {
        this.map = new ConcurrentHashMap<>();
        this.fair = false;
    }

    /**
     * See {@link
java.util.concurrent.ConcurrentHashMap#ConcurrentHashMap(int, float,
int)
     *            ConcurrentHashMap(initialCapacity, loadFactor,
concurrencyLevel)}
     * for meaning of first 3 parameters.
     *
     * @param fair whether locking should be fair
     */
    public AtomicHashMap(int initialCapacity, float loadFactor, int
concurrencyLevel, boolean fair)
    {
        this.map = new ConcurrentHashMap<>(initialCapacity,
loadFactor, concurrencyLevel);
        this.fair = fair;
    }

    /**
     * Exclusively lock the entry of the key.
     */
    public Entry<V> lock(K key)
    {
        while(true)
        {
            EntryImpl<V> entry = map.get(key); // in most cases entry
already exists.
            if(entry==null)
            {
                entry = new EntryImpl<>(fair, key, map);
                EntryImpl<V> prev = map.putIfAbsent(key, entry);
                if(prev!=null)
                    entry = prev;
            }

            if(entry.acquire())
                return entry;
            // else entry retired, re-try.
        }
    }

    /**
     * Get the latest committed value for the key. This call will not block.
     */
    public V get(Object key)
    {
        EntryImpl<V> entry = map.get(key);
        if(entry==null)
            return null;
        if(entry.isHeldByCurrentThread())
            return entry.workingValue;
        // entry could be retired, doesn't matter.
        return entry.committedValue;
    }

    /**
     * Atomically set the <tt>newValue</tt> to the entry of the key,
     * simply, lock the entry, set the value, unlock the entry.
     * If <tt>newValue</tt> is null, the entry may be evicted.
     * This call may block if the entry is being locked by other threads.
     *
     * @return the old value
     */
    public V set(K key, V newValue)
    {
        try( Entry<V> entry = lock(key) )
        {
            V oldValue = entry.getValue();
            entry.setValue(newValue);
            return oldValue;
        }
    }

    /**
     * @return the number of active entries; performs like {@link
java.util.concurrent.ConcurrentHashMap#size()}
     */
    public int size() { return map.size(); }

    /**
     * @return set of keys of active entries, read only.
     */
    public Set<K> keySet()
    {
        Set<K> keySet = map.keySet();
        // that set is mutable, and removing a key there will remove
an entry in map, bypassing our protocol.
        return Collections.unmodifiableSet(keySet); // we must forbid
that; read only to client.
    }

    /**
     * Set all entries' values to null. Every entry may be evicted.
     * This call may block if an entry is being locked by another thread.
     */
    public void clear()
    {
        // we can't simply call map.clear(). must lock entry before removal.
        for(Map.Entry<K,EntryImpl<V>> e : map.entrySet())
        {
            EntryImpl<V> entry = e.getValue(); // can be null due to
concurrent modification.
            if(entry!=null && entry.acquire())
            {
                entry.setValue(null);
                entry.unlock();
            }
        }
    }

}

From dl at cs.oswego.edu  Mon Aug 29 19:43:37 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 29 Aug 2011 19:43:37 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>
Message-ID: <4E5C2429.3080009@cs.oswego.edu>

On 08/29/11 13:52, Bob Lee wrote:
> On Mon, Aug 29, 2011 at 10:37 AM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>         It wasn't obvious to me how recursive computations are handled for the same
>         key and appears to livelock. This is an error condition that used to
>         deadlocked MapMaker. I fixed this by failing fast if Thread.holdsLock() was
>         true prior to waiting for the computation to finish.
>
>
>     I'm not sure I follow. If the user function in turn calls computeIfAbsent
>     for the same key, then it seems irreparably broken in the same sense as,
>     for example. a user's equals() function that attempts to invoke map.put.
>
>
> It is irreparably broken, but it's an easy and common mistake to make. It's
> difficult to identify the problem when the program just deadlocks vs. throwing
> an explicit exception that identifies the at fault key.
>

Well, in CHMV8. it won't deadlock, but instead gets into
an infinite loop (a form livelock), which makes it a more
marginal call whether to place a check that is not cheap
along a common path. But I'm tentatively planning to put it
in to next update anyway.

-Doug

From dl at cs.oswego.edu  Mon Aug 29 20:20:49 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 29 Aug 2011 20:20:49 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAOhq40p5VwW0nbdt0XFmi-2vVhkhVHDHXc+djjJ5WspjXpKp0g@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAOhq40p5VwW0nbdt0XFmi-2vVhkhVHDHXc+djjJ5WspjXpKp0g@mail.gmail.com>
Message-ID: <4E5C2CE1.8050402@cs.oswego.edu>

On 08/29/11 18:02, Charles Fry wrote:
> There are a few typos in the new compute javadocs:

Thanks! Fixed in next update.

> Also, it would be nice to document what "operations on this map by other threads
> may be blocked while computation is in progress." For example, I assume that get
> will never block on computation, but that any writes will.

Read-only operations (get, iterators, etc) never block at all.
Exactly which update operations block is a random phenomenon,
so left fuzzy. The number of entries covered by any given
node serving as a lock is, assuming lack of too many identical
hashCode values by different keys, approximately  Poisson
distributed with, on average at steady state, a parameter
of about 0.5 under 0.75 loadFactor. On average, about 75%
of the locks cover exactly one entry, the rest cover two
or more. And roughly, the probability that
holding a given lock will block an unrelated update
is around 1/8n (n the number of elements). On the other hand,
holding the lock will always prevent an ongoing resize operation
from completing, which may cause bins to overpopulate and
overall throughput to degrade. So people should avoid
long computations that are performed within these locks.

Further digressing: Despite disadvantages, using locks
for statistically very short lists turns out to be much more
stable and efficient than alternative non-blocking strategies
I tried that also meet the design goal of supporting efficient
partitioning and traversability, which is needed for upcoming
parallel operations on maps.

-Doug


From nathan.reynolds at oracle.com  Mon Aug 29 20:42:47 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 29 Aug 2011 17:42:47 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5C2CE1.8050402@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAOhq40p5VwW0nbdt0XFmi-2vVhkhVHDHXc+djjJ5WspjXpKp0g@mail.gmail.com>
	<4E5C2CE1.8050402@cs.oswego.edu>
Message-ID: <4E5C3207.6090909@oracle.com>

 > Further digressing: Despite disadvantages, using locks for 
statistically very short lists turns out to be much more stable and 
efficient than alternative non-blocking strategies I tried

I realize this is a digression, but it is interesting to me.  Are you 
saying that compareAndSet() took more time than locks?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

On 8/29/2011 5:20 PM, Doug Lea wrote:
> On 08/29/11 18:02, Charles Fry wrote:
>> There are a few typos in the new compute javadocs:
>
> Thanks! Fixed in next update.
>
>> Also, it would be nice to document what "operations on this map by 
>> other threads
>> may be blocked while computation is in progress." For example, I 
>> assume that get
>> will never block on computation, but that any writes will.
>
> Read-only operations (get, iterators, etc) never block at all.
> Exactly which update operations block is a random phenomenon,
> so left fuzzy. The number of entries covered by any given
> node serving as a lock is, assuming lack of too many identical
> hashCode values by different keys, approximately  Poisson
> distributed with, on average at steady state, a parameter
> of about 0.5 under 0.75 loadFactor. On average, about 75%
> of the locks cover exactly one entry, the rest cover two
> or more. And roughly, the probability that
> holding a given lock will block an unrelated update
> is around 1/8n (n the number of elements). On the other hand,
> holding the lock will always prevent an ongoing resize operation
> from completing, which may cause bins to overpopulate and
> overall throughput to degrade. So people should avoid
> long computations that are performed within these locks.
>
> Further digressing: Despite disadvantages, using locks
> for statistically very short lists turns out to be much more
> stable and efficient than alternative non-blocking strategies
> I tried that also meet the design goal of supporting efficient
> partitioning and traversability, which is needed for upcoming
> parallel operations on maps.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110829/8d069def/attachment.html>

From zhong.j.yu at gmail.com  Tue Aug 30 00:42:41 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 29 Aug 2011 23:42:41 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5C2429.3080009@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>
	<4E5C2429.3080009@cs.oswego.edu>
Message-ID: <CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>

On Mon, Aug 29, 2011 at 6:43 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Well, in CHMV8. it won't deadlock, but instead gets into
> an infinite loop (a form livelock), which makes it a more
> marginal call whether to place a check that is not cheap
> along a common path. But I'm tentatively planning to put it
> in to next update anyway.

If the caller has to worry about these things, it makes
computerIfAbsent(key, func) much less useful.

Usually this method is needed when the computation is expensive or has
side effects. (In many code structures the caller can't be sure about
the details of `func.map()`, therefore the worst is assumed.)

Requiring that the computation is transparent to the caller, must be
inexpensive and must not write to the map, will turn away majority of
potential users. This makes the method much less valuable than it
first appears.

In case the requirement can be met, `func.map()` can be cheaply and
safely invoked anyway, the saving by `computeIfAbsent()` is very
little, except in some extreme use cases (and these people can
probably afford their own customized implementations).

The requirement that the computation must not write to entries of
different keys seems especially hard to cope with. For example, if a
map is used as a cache; when computing value for `k1`, it might be
reasonable to disallow recursion on `k1`; however, it will be very odd
and inconvenient to disallow `cache.get( k2, #{ load(k2) } )` when
computing value for `k1`.

Zhong Yu

From dl at cs.oswego.edu  Tue Aug 30 06:33:41 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 30 Aug 2011 06:33:41 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5C3207.6090909@oracle.com>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAOhq40p5VwW0nbdt0XFmi-2vVhkhVHDHXc+djjJ5WspjXpKp0g@mail.gmail.com>
	<4E5C2CE1.8050402@cs.oswego.edu> <4E5C3207.6090909@oracle.com>
Message-ID: <4E5CBC85.80407@cs.oswego.edu>

On 08/29/11 20:42, Nathan Reynolds wrote:
>  > Further digressing: Despite disadvantages, using locks for statistically very
> short lists turns out to be much more stable and efficient than alternative
> non-blocking strategies I tried
>
> I realize this is a digression, but it is interesting to me. Are you saying that
> compareAndSet() took more time than locks?

No. The issue is that CHM needs to manage all five of:
insert/remove bin head, insert/remove internal node, and
split the list into upper and lower parts during
resize and replace with forwarding node; all the while maintaining
concurrent readability. There are non-blocking techniques for each
of these in isolation (mainly: deletion via dummy nodes that
I use in ConcurrentSkipLists, and bit-reversed node ordering
for splitting) but together, they introduce a lot of overhead
and contention storms. (For similar reasons, Herlihy & Shavit
recommend lazy-list-locking. which is an upside-down version
of the technique here.) for most list operations rather than
non-blocking techniques. Even though these require more atomic
operation overhead, in the vastly most common case they are
uncontended and thus cheap on recent processors, and more than make
up for the reduction  in other per-node bookkeeping that you'd
need to track dummy nodes etc. On contention, they block threads
out rather than allowing a lot of pointwise CAS misses and retries.

Plus, CHMV8 does still use a single CAS for the most common
update operation, adding a new node into an empty bin.

-Doug



From dl at cs.oswego.edu  Tue Aug 30 07:13:24 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 30 Aug 2011 07:13:24 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>	<4E5BCE50.50205@cs.oswego.edu>	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>	<4E5C2429.3080009@cs.oswego.edu>
	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>
Message-ID: <4E5CC5D4.4040104@cs.oswego.edu>

On 08/30/11 00:42, Zhong Yu wrote:
> The requirement that the computation must not write to entries of
> different keys seems especially hard to cope with. For example, if a
> map is used as a cache; when computing value for `k1`, it might be
> reasonable to disallow recursion on `k1`; however, it will be very odd
> and inconvenient to disallow `cache.get( k2, #{ load(k2) } )` when
> computing value for `k1`.

Yes. As I mentioned in other replies, a version suitable for
arbitrary caches should be based on an internal form of Futures
that can deal with reservations/place-holders. But this
(as well as accommodating weak references etc) would
add a lot of overhead and code bulk to operations for all
other CHM usages. So stay tuned for a version supporting such things.

> Requiring that the computation is transparent to the caller, must be
> inexpensive and must not write to the map, will turn away majority of
> potential users.

That's all for the best. The main problem computeIfAbsent
addresses is people often getting wrong the alternative
(and still usually best) idiom of:
if (map.get(k) == null)
    map.putIfAbsent(k, new V(k));
Even though it introduces potential contention (no free lunch),
with lambda syntax support, computeIfAbsent will be less
error-prone. As in, something like:
   map.computeIfAbsent(k, #{k -> new V(k)} };

In other words, the use of locks in CHMV8 is just an internal
implementation detail that may change at any time (although
as indicated in other postings, is not too likely to change).
Even without them, any support for computeIfAbsent introduces
contention etc, so users should keep the functions short.

-Doug


From jason.greene at redhat.com  Tue Aug 30 12:13:31 2011
From: jason.greene at redhat.com (Jason T. Greene)
Date: Tue, 30 Aug 2011 11:13:31 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5A95E0.1080309@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
Message-ID: <4E5D0C2B.9060502@redhat.com>

On 8/28/11 2:24 PM, Doug Lea wrote:
>
> A candidate replacement for java.util.concurrent.ConcurrentHashMap
> is now available as jsr166e.ConcurrentHashMapV8. This version
> is much more amenable to upcoming support for aggregate
> parallel operations (including, already, method "computeIfAbsent'
> using a stand-in MappingFunction type).
>
> The internal design is interestingly different.
> Read the internal documentation for details.
> (http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/ConcurrentHashMapV8.java?view=log)

Wow! This implementation has a very nice set of attributes, in 
particular the memory efficiency. I look forward to giving it a spin.

-- 
Jason T. Greene
JBoss, a division of Red Hat

From zhong.j.yu at gmail.com  Tue Aug 30 16:52:22 2011
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 30 Aug 2011 15:52:22 -0500
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5CC5D4.4040104@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>
	<4E5C2429.3080009@cs.oswego.edu>
	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>
	<4E5CC5D4.4040104@cs.oswego.edu>
Message-ID: <CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>

Thanks for clarifying the intended usage for me. Still, given the
heavy restriction on the function, I can't grasp the usefulness of the
method without a realistic use case.

In the following idiom

    if(map.get(k)==null)
        v  = calc(k);
        map.putIfAbsent( k, v );

the 1st line is an optimization, since both calc() and putIfAbsent()
are relatively more expensive than get(), if the key is likely
contained in the map, it's cheaper to try get() first.

The two concerns still exist for computeIfAbsent().

In the lambda expression example,

    computeIfAbsent( k, #{ k -> calc(k) } );

a new object (the function) is created every time, which is quite
expensive relative to get(). So it pays to

    if(map.get(k)==null)
        computeIfAbsent( k, #{ k -> calc(k) } );

If the function depends solely on `k`, we can eliminate the object creation by

    static final Function func = new ...;

    computeIfAbsent( k, func );

with loss of readability provided by lambda expression.

The 2nd concern is that computeIfAbsent() is more expensive* than
get(). So it pays to

    if(map.get(k)==null)
        computeIfAbsent( k, func );

(* based on the latest implementation of computeIfAbsent() - maybe it
can be improved so the idiom above becomes unnecessary )

Finally, comparing the performance of

    v = func.map(k);
    putIfAbsent( k, v );
vs
    computeIfAbsent( k, func );

The two versions perform almost the same, both in uncontended and
contended case, regardless of the complexity of func.map().

Zhong Yu

(I'm sorry if my comments are all negative in nature.)

On Tue, Aug 30, 2011 at 6:13 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 08/30/11 00:42, Zhong Yu wrote:
>>
>> The requirement that the computation must not write to entries of
>> different keys seems especially hard to cope with. For example, if a
>> map is used as a cache; when computing value for `k1`, it might be
>> reasonable to disallow recursion on `k1`; however, it will be very odd
>> and inconvenient to disallow `cache.get( k2, #{ load(k2) } )` when
>> computing value for `k1`.
>
> Yes. As I mentioned in other replies, a version suitable for
> arbitrary caches should be based on an internal form of Futures
> that can deal with reservations/place-holders. But this
> (as well as accommodating weak references etc) would
> add a lot of overhead and code bulk to operations for all
> other CHM usages. So stay tuned for a version supporting such things.
>
>> Requiring that the computation is transparent to the caller, must be
>> inexpensive and must not write to the map, will turn away majority of
>> potential users.
>
> That's all for the best. The main problem computeIfAbsent
> addresses is people often getting wrong the alternative
> (and still usually best) idiom of:
> if (map.get(k) == null)
> ? map.putIfAbsent(k, new V(k));
> Even though it introduces potential contention (no free lunch),
> with lambda syntax support, computeIfAbsent will be less
> error-prone. As in, something like:
> ?map.computeIfAbsent(k, #{k -> new V(k)} };
>
> In other words, the use of locks in CHMV8 is just an internal
> implementation detail that may change at any time (although
> as indicated in other postings, is not too likely to change).
> Even without them, any support for computeIfAbsent introduces
> contention etc, so users should keep the functions short.
>
> -Doug
>
>


From dl at cs.oswego.edu  Tue Aug 30 19:22:43 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 30 Aug 2011 19:22:43 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>	<4E5BCE50.50205@cs.oswego.edu>	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>	<4E5C2429.3080009@cs.oswego.edu>	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>	<4E5CC5D4.4040104@cs.oswego.edu>
	<CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>
Message-ID: <4E5D70C3.50901@cs.oswego.edu>

On 08/30/11 16:52, Zhong Yu wrote:
> Thanks for clarifying the intended usage for me. Still, given the
> heavy restriction on the function, I can't grasp the usefulness of the
> method without a realistic use case.

We didn't support computeIfAbsent before, for the
kinds of reasons you list. But the compelling reason
for including it anyway is error avoidance. An upcoming paper
in OOPSLA 2011 examined usages of ConcurrentHashMap in real-world
code, and found that by far, the most common error was
incorrect use of putIfAbsent in cases covered by computeIfAbsent.
Adding computeIfAbsent now may be too little, too late, but even this
is only made easily usable with lambda syntax. We do expect
to see some usages that don't follow advice and generate
occasional performance anomalies, which is a lot better than
people occasionally getting it completely wrong.

-Doug


From crazybob at crazybob.org  Tue Aug 30 19:36:59 2011
From: crazybob at crazybob.org (Bob Lee)
Date: Tue, 30 Aug 2011 16:36:59 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5D70C3.50901@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>
	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>
	<4E5BCE50.50205@cs.oswego.edu>
	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>
	<4E5C2429.3080009@cs.oswego.edu>
	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>
	<4E5CC5D4.4040104@cs.oswego.edu>
	<CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>
	<4E5D70C3.50901@cs.oswego.edu>
Message-ID: <CAGmsiP6XeTbzBietZ2iyxghtMD_K2bdHKMHBYXxpee_g1T47zQ@mail.gmail.com>

On Tue, Aug 30, 2011 at 4:22 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> An upcoming paper
> in OOPSLA 2011 examined usages of ConcurrentHashMap in real-world
> code, and found that by far, the most common error was
> incorrect use of putIfAbsent in cases covered by computeIfAbsent
>

Hey... remember when I did that for the JDK and Google's codebase back in
'09? :-)

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110830/0774d3d7/attachment.html>

From dl at cs.oswego.edu  Tue Aug 30 19:49:05 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 30 Aug 2011 19:49:05 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <CAGmsiP6XeTbzBietZ2iyxghtMD_K2bdHKMHBYXxpee_g1T47zQ@mail.gmail.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>	<4E5BCE50.50205@cs.oswego.edu>	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>	<4E5C2429.3080009@cs.oswego.edu>	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>	<4E5CC5D4.4040104@cs.oswego.edu>	<CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>	<4E5D70C3.50901@cs.oswego.edu>
	<CAGmsiP6XeTbzBietZ2iyxghtMD_K2bdHKMHBYXxpee_g1T47zQ@mail.gmail.com>
Message-ID: <4E5D76F1.3030806@cs.oswego.edu>

On 08/30/11 19:36, Bob Lee wrote:
> On Tue, Aug 30, 2011 at 4:22 PM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     An upcoming paper
>     in OOPSLA 2011 examined usages of ConcurrentHashMap in real-world
>     code, and found that by far, the most common error was
>     incorrect use of putIfAbsent in cases covered by computeIfAbsent
>
>
> Hey... remember when I did that for the JDK and Google's codebase back in '09? :-)

Sorry, I should have added that you and others have been asking
for this for years too!

-Doug

From nathan.reynolds at oracle.com  Tue Aug 30 19:45:32 2011
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Aug 2011 16:45:32 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5D70C3.50901@cs.oswego.edu>
References: <4E5A95E0.1080309@cs.oswego.edu>	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>	<4E5BCE50.50205@cs.oswego.edu>	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>	<4E5C2429.3080009@cs.oswego.edu>	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>	<4E5CC5D4.4040104@cs.oswego.edu>
	<CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>
	<4E5D70C3.50901@cs.oswego.edu>
Message-ID: <4E5D761C.3030408@oracle.com>

Doug,

I am wondering if you have data which shows how many cycles get(), 
put(), remove() and putIfAbsent() take on ConcurrentHashMapV8, 
ConcurrentHashMapV7, synchronized HashMap and unsynchronized HashMap?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110830/f9a2823c/attachment.html>

From dl at cs.oswego.edu  Tue Aug 30 20:28:55 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 30 Aug 2011 20:28:55 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8
In-Reply-To: <4E5D761C.3030408@oracle.com>
References: <4E5A95E0.1080309@cs.oswego.edu>	<CAHzJPEoXfdgW3xsVF-AL7995qDAc2G5W9Z_6oA3=eVZAEDEsiw@mail.gmail.com>	<4E5BCE50.50205@cs.oswego.edu>	<CAGmsiP6SsNufd=Zkc2J9_xZZvA30q9wZanbUZeFNDvujKuaBdg@mail.gmail.com>	<4E5C2429.3080009@cs.oswego.edu>	<CACuKZqGg++AiZ9s3Q6sozj+S=S2ZL1aM5FEX=TGKwuZ3TnsFAw@mail.gmail.com>	<4E5CC5D4.4040104@cs.oswego.edu>
	<CACuKZqGz=xU6R8Qyq8rCRwecSFnQ1jz90u0z4FJB1eVbPou-RQ@mail.gmail.com>
	<4E5D70C3.50901@cs.oswego.edu> <4E5D761C.3030408@oracle.com>
Message-ID: <4E5D8047.2010907@cs.oswego.edu>

On 08/30/11 19:45, Nathan Reynolds wrote:
> Doug,
>
> I am wondering if you have data which shows how many cycles get(), put(),
> remove() and putIfAbsent() take on ConcurrentHashMapV8, ConcurrentHashMapV7,
> synchronized HashMap and unsynchronized HashMap?
>

No cycle counts but...

There was a bunch of mail on measuring expected hash map performance
for typical loads on the core-libs openjdk list around July 2010(?).

For single-threaded performance, people seem to believe the results of
running our MapMicroBenchmark.
(See http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/)
For which on most machines, CHMV8 is about 3% faster than CHMV7,
and 10-25% slower (depending on #elements) than java.util.Hashmap.
I don't have results for synchronizedMap(HashMap) handy but
java.util.Hashtable (which is about the same) is generally
slower than either version of CHM, even with biased locking
turned on.

For multi-threaded tests, there's less consensus about the
best micro-benchmark. We have a few (including MapLoops),
and there are others out there -- including one by Cliff Click.
In general CHMV8 is the best of all the concurrent hash map
implementations I've tested on tests where there is some
temporal locality of access (which is typical) but open-address
versions such as Cliff Clicks's are faster for get/put/remove-only
tests with randomized key use, mainly because of fewer cache misses.

-Doug

