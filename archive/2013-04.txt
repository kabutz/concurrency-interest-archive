From jeffhain at rocketmail.com  Mon Apr  1 14:47:45 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Mon, 1 Apr 2013 19:47:45 +0100 (BST)
Subject: [concurrency-interest] concurrency and distribution for
	non-developers
Message-ID: <1364842065.24158.YahooMailNeo@web171705.mail.ir2.yahoo.com>

Hello.


? Could anyone point out educational content, for people
like managers, domain experts and "Powerpoint architects",
i.e. vulgarization material, about concurrency in general,
states in distributed systems, consistency/availability
trade-offs, and about rules for taking these into account
when designing systems, at least to avoid the most obvious
mistakes?
? (I found a lot of literature about most of it, but it was
all far too abstract, formal or technical for this audience:
they would never want to read it.)


-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130401/fba2b9dc/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 14:53:09 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Tue, 02 Apr 2013 22:53:09 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
Message-ID: <515B2915.60402@oracle.com>

Hello everybody!

Please review my proposal for the CopyOnWriteArrayList.addIfAbsent() method optimization.

http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html

Here is the original function body:
------------------------------------------------
     Object[] elements = getArray();
     int len = elements.length;
     Object[] newElements = new Object[len + 1]; <-- allocate new array in advance
     for (int i = 0; i < len; ++i) {
         if (eq(e, elements[i]))                 <-- check whether e is null on every iteration
             return false; // exit, throwing away copy
         else
             newElements[i] = elements[i];       <-- copy elements one by one
     }
     newElements[len] = e;
     setArray(newElements);
------------------------------------------------
The proposed change is to reuse CopyOnWriteArrayList.indexOf() function to check if e is already in the array.
If the check passed, new array is allocated withArrays.copyOf(). It uses native System.arraycopy(), which is probably faster than copying elements in the loop.

Sincerely yours,
Ivan


From martinrb at google.com  Tue Apr  2 15:05:32 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 2 Apr 2013 12:05:32 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B2915.60402@oracle.com>
References: <515B2915.60402@oracle.com>
Message-ID: <CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>

On Tue, Apr 2, 2013 at 11:53 AM, Ivan Gerasimov
<ivan.gerasimov at oracle.com>wrote:

> Hello everybody!
>
> Please review my proposal for the CopyOnWriteArrayList.**addIfAbsent()
> method optimization.
>
> http://washi.ru.oracle.com/~**igerasim/webrevs/8011215/**webrev/index.html<http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html>


This URL is not readable by external reviewers.

The "master" version of CopyOnWriteArrayList is here:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java?view=markup
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/dd1e8767/attachment.html>

From stanimir at riflexo.com  Tue Apr  2 15:25:25 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 2 Apr 2013 22:25:25 +0300
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B2915.60402@oracle.com>
References: <515B2915.60402@oracle.com>
Message-ID: <CAEJX8opu_oPU6KfnT34Zro9_Ar55vZT7P43eKVhrxPv0u6BgrA@mail.gmail.com>

The current version is cache oblivious. In any case for smaller arrays
(like COW) System.arrayCopy won't yield any noticeable difference.
Also, iirc System.arrayCopy places a memory barrier which in the COW case
is unneeded.

Stanimir


On Tue, Apr 2, 2013 at 9:53 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:

> Hello everybody!
>
> Please review my proposal for the CopyOnWriteArrayList.**addIfAbsent()
> method optimization.
>
> http://washi.ru.oracle.com/~**igerasim/webrevs/8011215/**webrev/index.html<http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>
> Here is the original function body:
> ------------------------------**------------------
>     Object[] elements = getArray();
>     int len = elements.length;
>     Object[] newElements = new Object[len + 1]; <-- allocate new array in
> advance
>     for (int i = 0; i < len; ++i) {
>         if (eq(e, elements[i]))                 <-- check whether e is
> null on every iteration
>             return false; // exit, throwing away copy
>         else
>             newElements[i] = elements[i];       <-- copy elements one by
> one
>     }
>     newElements[len] = e;
>     setArray(newElements);
> ------------------------------**------------------
> The proposed change is to reuse CopyOnWriteArrayList.indexOf() function to
> check if e is already in the array.
> If the check passed, new array is allocated withArrays.copyOf(). It uses
> native System.arraycopy(), which is probably faster than copying elements
> in the loop.
>
> Sincerely yours,
> Ivan
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/a79c0e12/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 16:38:02 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 00:38:02 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
Message-ID: <515B41AA.3040703@oracle.com>


>     Please review my proposal for the
>     CopyOnWriteArrayList.addIfAbsent() method optimization.
>
>     http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>     <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>
>
> This URL is not readable by external reviewers.

The webrev has been copied here:
http://cr.openjdk.java.net/~coffeys/webrev.8011215.ivan/

> The "master" version of CopyOnWriteArrayList is here:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java?view=markup
>
Thanks for the link!
I see that the code in the master version is identical to the one I've 
been working on.
So the optimization still could be applied.

Sincerely,
Ivan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/30db1ce1/attachment.html>

From martinrb at google.com  Tue Apr  2 16:55:21 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 2 Apr 2013 13:55:21 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B41AA.3040703@oracle.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
Message-ID: <CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>

Thanks for this change.  There is a tradeoff here.  If the element is never
present, then the older code might be a little faster, because we can avoid
re-traversing the array.  Otherwise, the new code is better.

I prefer it your way (I hate unneeded allocation), but the code was
intentionally written the other way.  Let's hear from Doug...

Martin


On Tue, Apr 2, 2013 at 1:38 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:

>
>   Please review my proposal for the CopyOnWriteArrayList.addIfAbsent()
>> method optimization.
>>
>> http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>
>
>  This URL is not readable by external reviewers.
>
>
> The webrev has been copied here:
> http://cr.openjdk.java.net/~coffeys/webrev.8011215.ivan/
>
>
>   The "master" version of CopyOnWriteArrayList is here:
>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java?view=markup
>
>    Thanks for the link!
> I see that the code in the master version is identical to the one I've
> been working on.
> So the optimization still could be applied.
>
> Sincerely,
> Ivan
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/3a847686/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 17:11:58 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 01:11:58 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
Message-ID: <515B499E.3090401@oracle.com>


> Thanks for this change.  There is a tradeoff here.  If the element is 
> never present, then the older code might be a little faster, because 
> we can avoid re-traversing the array.  Otherwise, the new code is better.

I've done a little testing on my side.
I used Integer as an underlying type and set length of the array to the 
values from 1 to 100.
My code shows a little performance gain - approximately 9%.
I understand it may not be there for all cases, but at least for some 
cases it is there.

> I prefer it your way (I hate unneeded allocation), but the code was 
> intentionally written the other way.  Let's hear from Doug...
>
> Martin
>
>
> On Tue, Apr 2, 2013 at 1:38 PM, Ivan Gerasimov 
> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>
>
>>         Please review my proposal for the
>>         CopyOnWriteArrayList.addIfAbsent() method optimization.
>>
>>         http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>>         <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>>
>>
>>     This URL is not readable by external reviewers.
>
>     The webrev has been copied here:
>     http://cr.openjdk.java.net/~coffeys/webrev.8011215.ivan/
>     <http://cr.openjdk.java.net/%7Ecoffeys/webrev.8011215.ivan/>
>
>
>>     The "master" version of CopyOnWriteArrayList is here:
>>     http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java?view=markup
>>
>     Thanks for the link!
>     I see that the code in the master version is identical to the one
>     I've been working on.
>     So the optimization still could be applied.
>
>     Sincerely,
>     Ivan
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/e145200f/attachment-0001.html>

From martinrb at google.com  Tue Apr  2 17:17:25 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 2 Apr 2013 14:17:25 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B499E.3090401@oracle.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com>
Message-ID: <CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>

Have you benchmarked the case where the element is never present?
(with the usual caveats about micro-benchmarking - perhaps use google
caliper?)


On Tue, Apr 2, 2013 at 2:11 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:

>
> I've done a little testing on my side.
> I used Integer as an underlying type and set length of the array to the
> values from 1 to 100.
> My code shows a little performance gain - approximately 9%.
> I understand it may not be there for all cases, but at least for some
> cases it is there.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/b172428a/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 17:34:50 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 01:34:50 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAEJX8opu_oPU6KfnT34Zro9_Ar55vZT7P43eKVhrxPv0u6BgrA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CAEJX8opu_oPU6KfnT34Zro9_Ar55vZT7P43eKVhrxPv0u6BgrA@mail.gmail.com>
Message-ID: <515B4EFA.9030207@oracle.com>

Thank you Stanimir!

My main goal was to get rid of early and possibly unneeded memory 
allocation.
I thought that System.arraycopy() would somehow compensate the need to 
traverse the array twice.
However testing shows that my code works a bit faster at least when 
dealing with Integer arrays of size from 1 to 100.

Sincerely,
Ivan

On 02.04.2013 23:25, Stanimir Simeonoff wrote:
> The current version is cache oblivious. In any case for smaller arrays 
> (like COW) System.arrayCopy won't yield any noticeable difference.
> Also, iirc System.arrayCopy places a memory barrier which in the COW 
> case is unneeded.
>
> Stanimir
>
>
> On Tue, Apr 2, 2013 at 9:53 PM, Ivan Gerasimov 
> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>
>     Hello everybody!
>
>     Please review my proposal for the
>     CopyOnWriteArrayList.addIfAbsent() method optimization.
>
>     http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>     <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>
>     Here is the original function body:
>     ------------------------------------------------
>         Object[] elements = getArray();
>         int len = elements.length;
>         Object[] newElements = new Object[len + 1]; <-- allocate new
>     array in advance
>         for (int i = 0; i < len; ++i) {
>             if (eq(e, elements[i]))                 <-- check whether
>     e is null on every iteration
>                 return false; // exit, throwing away copy
>             else
>                 newElements[i] = elements[i];       <-- copy elements
>     one by one
>         }
>         newElements[len] = e;
>         setArray(newElements);
>     ------------------------------------------------
>     The proposed change is to reuse CopyOnWriteArrayList.indexOf()
>     function to check if e is already in the array.
>     If the check passed, new array is allocated withArrays.copyOf().
>     It uses native System.arraycopy(), which is probably faster than
>     copying elements in the loop.
>
>     Sincerely yours,
>     Ivan
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/03c8d39f/attachment.html>

From stanimir at riflexo.com  Tue Apr  2 17:53:49 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 3 Apr 2013 00:53:49 +0300
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B4EFA.9030207@oracle.com>
References: <515B2915.60402@oracle.com>
	<CAEJX8opu_oPU6KfnT34Zro9_Ar55vZT7P43eKVhrxPv0u6BgrA@mail.gmail.com>
	<515B4EFA.9030207@oracle.com>
Message-ID: <CAEJX8oqAfiThjraDQnoq996gJP8prk4VLQ_uqKoEwAe2QmoVOw@mail.gmail.com>

My usual use of COWList (set) would be over 99%+ addIfAbsent to return
true.
The tricky part of the benchmark would be running w/ L1 cache full and
observing the effects of the double traverse to the rest of the code. The
effects would be more pronounced w/ shared L1 cache, i.e. hyperthreading
-as the 2nd traverse can trash the cache.

Stanimir


On Wed, Apr 3, 2013 at 12:34 AM, Ivan Gerasimov
<ivan.gerasimov at oracle.com>wrote:

>  Thank you Stanimir!
>
> My main goal was to get rid of early and possibly unneeded memory
> allocation.
> I thought that System.arraycopy() would somehow compensate the need to
> traverse the array twice.
> However testing shows that my code works a bit faster at least when
> dealing with Integer arrays of size from 1 to 100.
>
> Sincerely,
> Ivan
>
>
> On 02.04.2013 23:25, Stanimir Simeonoff wrote:
>
> The current version is cache oblivious. In any case for smaller arrays
> (like COW) System.arrayCopy won't yield any noticeable difference.
> Also, iirc System.arrayCopy places a memory barrier which in the COW case
> is unneeded.
>
> Stanimir
>
>
> On Tue, Apr 2, 2013 at 9:53 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:
>
>> Hello everybody!
>>
>> Please review my proposal for the CopyOnWriteArrayList.addIfAbsent()
>> method optimization.
>>
>> http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>>
>> Here is the original function body:
>> ------------------------------------------------
>>     Object[] elements = getArray();
>>     int len = elements.length;
>>     Object[] newElements = new Object[len + 1]; <-- allocate new array in
>> advance
>>     for (int i = 0; i < len; ++i) {
>>         if (eq(e, elements[i]))                 <-- check whether e is
>> null on every iteration
>>             return false; // exit, throwing away copy
>>         else
>>             newElements[i] = elements[i];       <-- copy elements one by
>> one
>>     }
>>     newElements[len] = e;
>>     setArray(newElements);
>> ------------------------------------------------
>> The proposed change is to reuse CopyOnWriteArrayList.indexOf() function
>> to check if e is already in the array.
>> If the check passed, new array is allocated withArrays.copyOf(). It uses
>> native System.arraycopy(), which is probably faster than copying elements
>> in the loop.
>>
>> Sincerely yours,
>> Ivan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/fc441357/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 18:25:31 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 02:25:31 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAGB9EW9GdQyv1h_3cDf=enps4yhCeP95RveXsVQD=bsY=JhEZg@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CAEJX8opu_oPU6KfnT34Zro9_Ar55vZT7P43eKVhrxPv0u6BgrA@mail.gmail.com>
	<515B4EFA.9030207@oracle.com>
	<CAGB9EW9GdQyv1h_3cDf=enps4yhCeP95RveXsVQD=bsY=JhEZg@mail.gmail.com>
Message-ID: <515B5ADB.7070607@oracle.com>

Sure!

Attached please find an archive with the tests.
Actually they are quite naive - they simply run the code snippet in loop 
for several hundreds times.

Sincerely,
Ivan

On 03.04.2013 1:49, Louis Wasserman wrote:
> Can we see the implementation of your benchmark?  Accurate 
> benchmarking is extremely nontrivial.
>
>
> On Tue, Apr 2, 2013 at 2:34 PM, Ivan Gerasimov 
> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>
>     Thank you Stanimir!
>
>     My main goal was to get rid of early and possibly unneeded memory
>     allocation.
>     I thought that System.arraycopy() would somehow compensate the
>     need to traverse the array twice.
>     However testing shows that my code works a bit faster at least
>     when dealing with Integer arrays of size from 1 to 100.
>
>     Sincerely,
>     Ivan
>
>     On 02.04.2013 23:25, Stanimir Simeonoff wrote:
>
>         The current version is cache oblivious. In any case for
>         smaller arrays (like COW) System.arrayCopy won't yield any
>         noticeable difference.
>         Also, iirc System.arrayCopy places a memory barrier which in
>         the COW case is unneeded.
>
>         Stanimir
>
>
>
>         On Tue, Apr 2, 2013 at 9:53 PM, Ivan Gerasimov
>         <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>
>         <mailto:ivan.gerasimov at oracle.com
>         <mailto:ivan.gerasimov at oracle.com>>> wrote:
>
>             Hello everybody!
>
>             Please review my proposal for the
>             CopyOnWriteArrayList.addIfAbsent() method optimization.
>
>         http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>         <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>            
>         <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html>
>
>
>
>             Here is the original function body:
>             ------------------------------------------------
>                 Object[] elements = getArray();
>                 int len = elements.length;
>                 Object[] newElements = new Object[len + 1]; <--
>         allocate new
>             array in advance
>                 for (int i = 0; i < len; ++i) {
>                     if (eq(e, elements[i])) <-- check whether
>             e is null on every iteration
>                         return false; // exit, throwing away copy
>                     else
>                         newElements[i] = elements[i]; <-- copy elements
>             one by one
>                 }
>                 newElements[len] = e;
>                 setArray(newElements);
>             ------------------------------------------------
>             The proposed change is to reuse CopyOnWriteArrayList.indexOf()
>             function to check if e is already in the array.
>             If the check passed, new array is allocated
>         withArrays.copyOf().
>             It uses native System.arraycopy(), which is probably
>         faster than
>             copying elements in the loop.
>
>             Sincerely yours,
>             Ivan
>
>             _______________________________________________
>             Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
> -- 
> Louis Wasserman

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/3983a7ba/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 8011215-benchmark.tar
Type: application/x-tar
Size: 10240 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/3983a7ba/attachment-0001.tar>

From ivan.gerasimov at oracle.com  Tue Apr  2 18:33:10 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 02:33:10 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com>
	<CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>
Message-ID: <515B5CA6.1040006@oracle.com>


On 03.04.2013 1:17, Martin Buchholz wrote:
> Have you benchmarked the case where the element is never present?
That's the only case I've tested.
If the element were in the array, my code would obviously win.

> (with the usual caveats about micro-benchmarking - perhaps use google 
> caliper?)
The tests I wrote are quite simple - they just run a code snippet for 
several hundreds of times.
I've just sent and archive with the tests in reply to the other message 
in the thread.

>
> On Tue, Apr 2, 2013 at 2:11 PM, Ivan Gerasimov 
> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>
>
>     I've done a little testing on my side.
>     I used Integer as an underlying type and set length of the array
>     to the values from 1 to 100.
>     My code shows a little performance gain - approximately 9%.
>     I understand it may not be there for all cases, but at least for
>     some cases it is there.
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/389d16a8/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 18:45:13 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 02:45:13 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B4AE5.4060104@CoSoCo.de>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
Message-ID: <515B5F79.9090603@oracle.com>

Thank you, Ulf!

> maybe the old code wins for looong arrays, so there could be a 
> threshold to decide between old and new code:

I've modified the benchmark code to test arrays with 90'000 to 100'000 
elements. (Previously was testing 1 to 100 elements.)
The performance gain turns out to be even more significant.
On my machine tests show that with that many elements the new code runs 
40% faster.

Honestly, I didn't expect that. I thought my code might be a bit slower 
and hoped that not much slower.

Sincerely,
Ivan

>
>
> Am 02.04.2013 23:11, schrieb Ivan Gerasimov:
>>
>>> Thanks for this change.  There is a tradeoff here.  If the element 
>>> is never present, then the older code might be a little faster, 
>>> because we can avoid re-traversing the array.  Otherwise, the new 
>>> code is better.
>>
>> I've done a little testing on my side.
>> I used Integer as an underlying type and set length of the array to 
>> the values from 1 to 100.
>> My code shows a little performance gain - approximately 9%.
>> I understand it may not be there for all cases, but at least for some 
>> cases it is there.
>>
>>> I prefer it your way (I hate unneeded allocation), but the code was 
>>> intentionally written the other way.  Let's hear from Doug...
>>>
>>> Martin
>>>
>>>
>>> On Tue, Apr 2, 2013 at 1:38 PM, Ivan Gerasimov 
>>> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>>>
>>>
>>>>         Please review my proposal for the
>>>>         CopyOnWriteArrayList.addIfAbsent() method optimization.
>>>>
>>>> http://washi.ru.oracle.com/~igerasim/webrevs/8011215/webrev/index.html
>>>> <http://washi.ru.oracle.com/%7Eigerasim/webrevs/8011215/webrev/index.html> 
>>>>
>>>>
>>>>
>>>>     This URL is not readable by external reviewers.
>>>
>>>     The webrev has been copied here:
>>>     http://cr.openjdk.java.net/~coffeys/webrev.8011215.ivan/
>>> <http://cr.openjdk.java.net/%7Ecoffeys/webrev.8011215.ivan/>
>>>
>>>
>>>>     The "master" version of CopyOnWriteArrayList is here:
>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CopyOnWriteArrayList.java?view=markup 
>>>>
>>>>
>>>     Thanks for the link!
>>>     I see that the code in the master version is identical to the one
>>>     I've been working on.
>>>     So the optimization still could be applied.
>>>
>>>     Sincerely,
>>>     Ivan
>>>
>>>
>>
>>
>
>
>


From martinrb at google.com  Tue Apr  2 18:45:26 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 2 Apr 2013 15:45:26 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B5CA6.1040006@oracle.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com>
	<CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>
	<515B5CA6.1040006@oracle.com>
Message-ID: <CA+kOe09E7X3GxepJLbAdD4mB=E16rPCemmwxD21xfpXWKc1qdQ@mail.gmail.com>

Ivan's code has my blessing without any more benchmarking efforts.
Let's still wait to hear from Doug.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/3bd01cbe/attachment.html>

From martinrb at google.com  Tue Apr  2 18:47:54 2013
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 2 Apr 2013 15:47:54 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B5F79.9090603@oracle.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
Message-ID: <CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>

On Tue, Apr 2, 2013 at 3:45 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:

> Thank you, Ulf!
>
>
>  maybe the old code wins for looong arrays, so there could be a threshold
>> to decide between old and new code:
>>
>
> I've modified the benchmark code to test arrays with 90'000 to 100'000
> elements. (Previously was testing 1 to 100 elements.)
> The performance gain turns out to be even more significant.
> On my machine tests show that with that many elements the new code runs
> 40% faster.
>
> Honestly, I didn't expect that. I thought my code might be a bit slower
> and hoped that not much slower.
>

Yeah, that's a bit surprising.  Perhaps because you're avoiding the branch
of testing object for null on each iteration?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/9c869df2/attachment.html>

From ivan.gerasimov at oracle.com  Tue Apr  2 19:28:29 2013
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Wed, 03 Apr 2013 03:28:29 +0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAGB9EW_kZMTNbMRn8cnCL8fAY3JQGiQAOiPi=kqcExGsP8YMoQ@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com>
	<CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>
	<515B5CA6.1040006@oracle.com>
	<CAGB9EW_kZMTNbMRn8cnCL8fAY3JQGiQAOiPi=kqcExGsP8YMoQ@mail.gmail.com>
Message-ID: <515B699D.6020008@oracle.com>

Thank you, Louis.

Yes, you're probably right.
I might want to get familiar with caliper mentioned earlier, or 
something else like that.
However, I'd like to note that in the tests exactly the same amount of 
allocations were made for both versions of code.
The difference in the code is around these allocations.

On 03.04.2013 2:36, Louis Wasserman wrote:
> I would be deeply suspicious of benchmarks that naive, especially for 
> benchmarks like this that involve lots of allocation -- you're most 
> likely benchmarking the GC, not the actual operation.
>
>
> On Tue, Apr 2, 2013 at 3:33 PM, Ivan Gerasimov 
> <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>> wrote:
>
>
>     On 03.04.2013 1:17, Martin Buchholz wrote:
>
>         Have you benchmarked the case where the element is never present?
>
>     That's the only case I've tested.
>     If the element were in the array, my code would obviously win.
>
>
>         (with the usual caveats about micro-benchmarking - perhaps use
>         google caliper?)
>
>     The tests I wrote are quite simple - they just run a code snippet
>     for several hundreds of times.
>     I've just sent and archive with the tests in reply to the other
>     message in the thread.
>
>
>
>         On Tue, Apr 2, 2013 at 2:11 PM, Ivan Gerasimov
>         <ivan.gerasimov at oracle.com <mailto:ivan.gerasimov at oracle.com>
>         <mailto:ivan.gerasimov at oracle.com
>         <mailto:ivan.gerasimov at oracle.com>>> wrote:
>
>
>             I've done a little testing on my side.
>             I used Integer as an underlying type and set length of the
>         array
>             to the values from 1 to 100.
>             My code shows a little performance gain - approximately 9%.
>             I understand it may not be there for all cases, but at
>         least for
>             some cases it is there.
>
>
>
>
>
> -- 
> Louis Wasserman

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/4637ea9e/attachment-0001.html>

From stanimir at riflexo.com  Tue Apr  2 19:37:40 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 3 Apr 2013 02:37:40 +0300
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
Message-ID: <CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>

On Wed, Apr 3, 2013 at 1:47 AM, Martin Buchholz <martinrb at google.com> wrote:

>
>
>
> On Tue, Apr 2, 2013 at 3:45 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com>wrote:
>
>> Thank you, Ulf!
>>
>>
>>  maybe the old code wins for looong arrays, so there could be a threshold
>>> to decide between old and new code:
>>>
>>
>> I've modified the benchmark code to test arrays with 90'000 to 100'000
>> elements. (Previously was testing 1 to 100 elements.)
>> The performance gain turns out to be even more significant.
>> On my machine tests show that with that many elements the new code runs
>> 40% faster.
>>
>> Honestly, I didn't expect that. I thought my code might be a bit slower
>> and hoped that not much slower.
>>
>
> Yeah, that's a bit surprising.  Perhaps because you're avoiding the branch
> of testing object for null on each iteration?
>
> The branch would be perfectly predicted by the CPU, so it cannot be that.
System.arraycopy would be much better for larger arrays as it uses SSE
extensions to copy but normally COW structures would not have so many
elements.
btw, the 2 pass version benefits of low memory footprint equals() (and all
in L2 cache). Basically if you don't get cache trashing Arrays.copy would
be the better one.

Stanimir


> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/059fa9e0/attachment.html>

From vitalyd at gmail.com  Tue Apr  2 19:43:15 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 2 Apr 2013 19:43:15 -0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
Message-ID: <CAHjP37ErPLM_YrqsJwyKf-N=LW9KZ9vic=SdO50vZD+TDiZZ9Q@mail.gmail.com>

I actually like Ivan's version as well.

The diff in perf may also be due to uneliminated range checks in current
code - loop bounds is stored in local but unclear whether jit can still
trace through and determine that it's always within bounds.  Also the store
into new array may get hit if, again, jit doesn't figure it out - would
have to look at disassembly. The Arrays.copyOf() intrinsic obviously will
avoid range checks.

Also unclear whether the branch in the loop body kills certain
optimizations, so Ivan's code, despite walking the array twice (possibly),
is easier to optimize aggressively to the point where it may become a
memory bandwidth limited operation (for very large sizes).

Sent from my phone
On Apr 2, 2013 6:48 PM, "Martin Buchholz" <martinrb at google.com> wrote:

> On Tue, Apr 2, 2013 at 3:45 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com
> >wrote:
>
> > Thank you, Ulf!
> >
> >
> >  maybe the old code wins for looong arrays, so there could be a threshold
> >> to decide between old and new code:
> >>
> >
> > I've modified the benchmark code to test arrays with 90'000 to 100'000
> > elements. (Previously was testing 1 to 100 elements.)
> > The performance gain turns out to be even more significant.
> > On my machine tests show that with that many elements the new code runs
> > 40% faster.
> >
> > Honestly, I didn't expect that. I thought my code might be a bit slower
> > and hoped that not much slower.
> >
>
> Yeah, that's a bit surprising.  Perhaps because you're avoiding the branch
> of testing object for null on each iteration?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/bfecda45/attachment.html>

From vitalyd at gmail.com  Tue Apr  2 19:55:02 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 2 Apr 2013 19:55:02 -0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
Message-ID: <CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>

CPU will predict but compiler may not optimize loop aggressively with the
branch there; e.g. if you unroll, you'll bloat code size by having a branch
on each element, and then you can hit icache misses.

Arrays.copyOf is intrinsic so should have perf similar to System.arrayCopy.

Sent from my phone
On Apr 2, 2013 7:51 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

>
> On Wed, Apr 3, 2013 at 1:47 AM, Martin Buchholz <martinrb at google.com>wrote:
>
>>
>>
>>
>> On Tue, Apr 2, 2013 at 3:45 PM, Ivan Gerasimov <ivan.gerasimov at oracle.com
>> > wrote:
>>
>>> Thank you, Ulf!
>>>
>>>
>>>  maybe the old code wins for looong arrays, so there could be a
>>>> threshold to decide between old and new code:
>>>>
>>>
>>> I've modified the benchmark code to test arrays with 90'000 to 100'000
>>> elements. (Previously was testing 1 to 100 elements.)
>>> The performance gain turns out to be even more significant.
>>> On my machine tests show that with that many elements the new code runs
>>> 40% faster.
>>>
>>> Honestly, I didn't expect that. I thought my code might be a bit slower
>>> and hoped that not much slower.
>>>
>>
>> Yeah, that's a bit surprising.  Perhaps because you're avoiding the
>> branch of testing object for null on each iteration?
>>
>> The branch would be perfectly predicted by the CPU, so it cannot be that.
> System.arraycopy would be much better for larger arrays as it uses SSE
> extensions to copy but normally COW structures would not have so many
> elements.
> btw, the 2 pass version benefits of low memory footprint equals() (and all
> in L2 cache). Basically if you don't get cache trashing Arrays.copy would
> be the better one.
>
> Stanimir
>
>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130402/dc259c94/attachment.html>

From Alan.Bateman at oracle.com  Wed Apr  3 04:03:49 2013
From: Alan.Bateman at oracle.com (Alan Bateman)
Date: Wed, 03 Apr 2013 09:03:49 +0100
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515B699D.6020008@oracle.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com>
	<CA+kOe080DO1Bssw5KvgUtDTrMhQ8wZMhr5gwxzn-CxhBBECTRA@mail.gmail.com>
	<515B5CA6.1040006@oracle.com>
	<CAGB9EW_kZMTNbMRn8cnCL8fAY3JQGiQAOiPi=kqcExGsP8YMoQ@mail.gmail.com>
	<515B699D.6020008@oracle.com>
Message-ID: <515BE265.4070400@oracle.com>

On 03/04/2013 00:28, Ivan Gerasimov wrote:
> Thank you, Louis.
>
> Yes, you're probably right.
> I might want to get familiar with caliper mentioned earlier, or 
> something else like that.
One other tool to also be aware of is the Java Microbenchmark Harness 
[1]. Aleksey Shipilev recently contributed this to the OpenJDK Code 
Tools Project.

-Alan

[1] http://openjdk.java.net/projects/code-tools/jmh/

From dl at cs.oswego.edu  Wed Apr  3 06:35:27 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 3 Apr 2013 06:35:27 -0400 (EDT)
Subject: [concurrency-interest] RFR [8011215] optimization of
 CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
Message-ID: <34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>


A few quick notes while travelling:

This was designed to perform best in the case of possibly contended
updates when the element is absent, by avoiding retraversal, and
thus minimizing lock hold times in the common case. (When not common,
it can be guarded by a contains check.) However even in this case,
it is possible that a retraversal via arraycopy could be faster
because it can use optimized cheaper writes (fewer card marks).
I'll recheck this on enough machines to make a better assessment
of impact withing a few days.

-Doug


> CPU will predict but compiler may not optimize loop aggressively with the
> branch there; e.g. if you unroll, you'll bloat code size by having a
> branch
> on each element, and then you can hit icache misses.
>
> Arrays.copyOf is intrinsic so should have perf similar to
> System.arrayCopy.
>
> Sent from my phone
> On Apr 2, 2013 7:51 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:
>
>>
>> On Wed, Apr 3, 2013 at 1:47 AM, Martin Buchholz
>> <martinrb at google.com>wrote:
>>
>>>
>>>
>>>
>>> On Tue, Apr 2, 2013 at 3:45 PM, Ivan Gerasimov
>>> <ivan.gerasimov at oracle.com
>>> > wrote:
>>>
>>>> Thank you, Ulf!
>>>>
>>>>
>>>>  maybe the old code wins for looong arrays, so there could be a
>>>>> threshold to decide between old and new code:
>>>>>
>>>>
>>>> I've modified the benchmark code to test arrays with 90'000 to 100'000
>>>> elements. (Previously was testing 1 to 100 elements.)
>>>> The performance gain turns out to be even more significant.
>>>> On my machine tests show that with that many elements the new code
>>>> runs
>>>> 40% faster.
>>>>
>>>> Honestly, I didn't expect that. I thought my code might be a bit
>>>> slower
>>>> and hoped that not much slower.
>>>>
>>>
>>> Yeah, that's a bit surprising.  Perhaps because you're avoiding the
>>> branch of testing object for null on each iteration?
>>>
>>> The branch would be perfectly predicted by the CPU, so it cannot be
>>> that.
>> System.arraycopy would be much better for larger arrays as it uses SSE
>> extensions to copy but normally COW structures would not have so many
>> elements.
>> btw, the 2 pass version benefits of low memory footprint equals() (and
>> all
>> in L2 cache). Basically if you don't get cache trashing Arrays.copy
>> would
>> be the better one.
>>
>> Stanimir
>>
>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From d.sannella at contemplateltd.com  Wed Apr  3 13:01:01 2013
From: d.sannella at contemplateltd.com (Don Sannella)
Date: Wed, 03 Apr 2013 18:01:01 +0100
Subject: [concurrency-interest] ThreadSafe static analysis tool for Java
	concurrency
Message-ID: <515C604D.6080203@contemplateltd.com>

Contemplate, an Edinburgh University spin-out company, has developed an 
advanced static analysis tool, ThreadSafe, that may be of interest to 
readers of concurrency-interest.  ThreadSafe specifically targets Java 
concurrency defects and includes some dedicated treatment for 
java.util.concurrent.  It handles enterprise-scale Java codebases and 
includes tight integration with Eclipse.

ThreadSafe is in an early stage - currently we are on version 1.1 - but 
it is already in use finding serious concurrency defects in projects at 
a couple of large investment banks.  You can get an impression of what 
it does from the very simple example in
http://contemplateltd.com/maintaining-safe-concurrent-code-with-threadsafe

We are looking for a limited number of participants for a trial, to give 
us feedback on what is in ThreadSafe now and on our plans for near-term 
new features.  Please get in touch with me directly if you are 
interested in taking part.

Of course, I am also very happy to demonstrate ThreadSafe to potential 
paying customers!  If you are in this category, please get in touch!

Regards,

Don Sannella



Here are answers to some obvious questions:

1. Can ThreadSafe find bugs in my tricky lock-free algorithm?

Probably not. We look for the sort of concurrency mistakes that 
application developers make in large code bases.

2. What kind of trial participants are you looking for?

We are mainly looking for two kinds of people:

a. Java concurrency gurus who might be interested in how ThreadSafe 
could be used by software developers that they teach/advise.

b. Application developers in industry who make substantial use of 
concurrency.

We need to be able to identify who you are and your affiliation.

3. Is this free software?

ThreadSafe is a commercial product and there is currently no free 
version.  At some point we may introduce a free or low-cost version. 
The trial is free, but we expect feedback.


----------------------------------------------------------------------
Prof. Donald Sannella, Laboratory for Foundations of Computer Science,
School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, UK
http://homepages.inf.ed.ac.uk/dts  dts at inf.ed.ac.uk  +44 131 650 5184

and

---------------------------------------------------------
Don Sannella                d.sannella at contemplateltd.com
Contemplate Ltd                    www.contemplateltd.com
tel +44 7939 132117  fax +44 131 6503474  skype dsannella

From viktor.klang at gmail.com  Wed Apr  3 13:21:08 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 3 Apr 2013 19:21:08 +0200
Subject: [concurrency-interest] ThreadSafe static analysis tool for Java
	concurrency
In-Reply-To: <515C604D.6080203@contemplateltd.com>
References: <515C604D.6080203@contemplateltd.com>
Message-ID: <CANPzfU82-Pbk_JAEk-2woebU0ew0ivdCVktkf4_RUYdLKn5JfA@mail.gmail.com>

Hi Don,

your link seems broken.

Cheers,
?


On Wed, Apr 3, 2013 at 7:01 PM, Don Sannella
<d.sannella at contemplateltd.com>wrote:

> Contemplate, an Edinburgh University spin-out company, has developed an
> advanced static analysis tool, ThreadSafe, that may be of interest to
> readers of concurrency-interest.  ThreadSafe specifically targets Java
> concurrency defects and includes some dedicated treatment for
> java.util.concurrent.  It handles enterprise-scale Java codebases and
> includes tight integration with Eclipse.
>
> ThreadSafe is in an early stage - currently we are on version 1.1 - but it
> is already in use finding serious concurrency defects in projects at a
> couple of large investment banks.  You can get an impression of what it
> does from the very simple example in
> http://contemplateltd.com/**maintaining-safe-concurrent-**
> code-with-threadsafe<http://contemplateltd.com/maintaining-safe-concurrent-code-with-threadsafe>
>
> We are looking for a limited number of participants for a trial, to give
> us feedback on what is in ThreadSafe now and on our plans for near-term new
> features.  Please get in touch with me directly if you are interested in
> taking part.
>
> Of course, I am also very happy to demonstrate ThreadSafe to potential
> paying customers!  If you are in this category, please get in touch!
>
> Regards,
>
> Don Sannella
>
>
>
> Here are answers to some obvious questions:
>
> 1. Can ThreadSafe find bugs in my tricky lock-free algorithm?
>
> Probably not. We look for the sort of concurrency mistakes that
> application developers make in large code bases.
>
> 2. What kind of trial participants are you looking for?
>
> We are mainly looking for two kinds of people:
>
> a. Java concurrency gurus who might be interested in how ThreadSafe could
> be used by software developers that they teach/advise.
>
> b. Application developers in industry who make substantial use of
> concurrency.
>
> We need to be able to identify who you are and your affiliation.
>
> 3. Is this free software?
>
> ThreadSafe is a commercial product and there is currently no free version.
>  At some point we may introduce a free or low-cost version. The trial is
> free, but we expect feedback.
>
>
> ------------------------------**------------------------------**----------
> Prof. Donald Sannella, Laboratory for Foundations of Computer Science,
> School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, UK
> http://homepages.inf.ed.ac.uk/**dts <http://homepages.inf.ed.ac.uk/dts>
> dts at inf.ed.ac.uk  +44 131 650 5184
>
> and
>
> ------------------------------**---------------------------
> Don Sannella                d.sannella at contemplateltd.com
> Contemplate Ltd                    www.contemplateltd.com
> tel +44 7939 132117  fax +44 131 6503474  skype dsannella
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130403/46476c9f/attachment.html>

From dl at cs.oswego.edu  Fri Apr  5 08:27:40 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 05 Apr 2013 08:27:40 -0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
Message-ID: <515EC33C.7090604@cs.oswego.edu>

On 04/03/13 06:35, Doug Lea wrote:
> This was designed to perform best in the case of possibly contended
> updates when the element is absent, by avoiding retraversal, and
> thus minimizing lock hold times in the common case. (When not common,
> it can be guarded by a contains check.) However even in this case,
> it is possible that a retraversal via arraycopy could be faster
> because it can use optimized cheaper writes (fewer card marks).

Yes, by a little.
A simple but reliable performance test is now at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log

The simplest change allowing this (below) also appears to
be among the fastest. Running across various machines and
settings (GC, client/server), it seems to be between 5% and 15%
faster. This is a smaller difference than in Ivan's tests,
that didn't include lock and contention effects.

I committed jsr166 version. We'll need to sync this up with
with openjdk tl someday, but might as well wait until
other updates for Spliterators/streams are ready to integrate.

-Doug

*** CopyOnWriteArrayList.java.~1.100.~	Tue Mar 12 19:59:08 2013
--- CopyOnWriteArrayList.java	Fri Apr  5 08:03:29 2013
***************
*** 579,595 ****
           final ReentrantLock lock = this.lock;
           lock.lock();
           try {
-             // Copy while checking if already present.
-             // This wins in the most common case where it is not present
               Object[] elements = getArray();
               int len = elements.length;
-             Object[] newElements = new Object[len + 1];
               for (int i = 0; i < len; ++i) {
                   if (eq(e, elements[i]))
!                     return false; // exit, throwing away copy
!                 else
!                     newElements[i] = elements[i];
               }
               newElements[len] = e;
               setArray(newElements);
               return true;
--- 579,591 ----
           final ReentrantLock lock = this.lock;
           lock.lock();
           try {
               Object[] elements = getArray();
               int len = elements.length;
               for (int i = 0; i < len; ++i) {
                   if (eq(e, elements[i]))
!                     return false;
               }
+             Object[] newElements = Arrays.copyOf(elements, len + 1);
               newElements[len] = e;
               setArray(newElements);
               return true;


From martinrb at google.com  Fri Apr  5 17:27:53 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 5 Apr 2013 14:27:53 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <515EC33C.7090604@cs.oswego.edu>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
Message-ID: <CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>

I'm still advocating an optimistic approach, that does not hold the lock
during the first traversal of the array snapshot.  This is much faster when
cache hits are the norm (or equals methods are expensive), which I would
hope would be the common case, and only slightly slower when all adds are
cache misses.

     public boolean addIfAbsent(E e) {
        Object[] snapshot = getArray();
        int len = snapshot.length;
        for (int i = 0; i < len; i++)
            if (eq(e, snapshot[i]))
                return false;
        return addIfAbsent(e, snapshot);
    }

    private boolean addIfAbsent(E e, Object[] snapshot) {
        final ReentrantLock lock = this.lock;
        lock.lock();
        try {
            Object[] current = getArray();
            int len = current.length;
            if (snapshot != current) {
                // Optimize for contending with another addIfAbsent
                int common = Math.min(snapshot.length, len);
                for (int i = 0; i < common; i++)
                    if (current[i] != snapshot[i] && eq(e, current[i]))
                        return false;
                for (int i = common; i < len; i++)
                    if (eq(e, current[i]))
                        return false;
            }
            Object[] newElements = Arrays.copyOf(current, len + 1);
            newElements[len] = e;
            setArray(newElements);
            return true;
        } finally {
            lock.unlock();
        }
    }



On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 04/03/13 06:35, Doug Lea wrote:
>
>> This was designed to perform best in the case of possibly contended
>> updates when the element is absent, by avoiding retraversal, and
>> thus minimizing lock hold times in the common case. (When not common,
>> it can be guarded by a contains check.) However even in this case,
>> it is possible that a retraversal via arraycopy could be faster
>> because it can use optimized cheaper writes (fewer card marks).
>>
>
> Yes, by a little.
> A simple but reliable performance test is now at
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
> COWALAddIfAbsentLoops.java?**view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log>
>
> The simplest change allowing this (below) also appears to
> be among the fastest. Running across various machines and
> settings (GC, client/server), it seems to be between 5% and 15%
> faster. This is a smaller difference than in Ivan's tests,
> that didn't include lock and contention effects.
>
> I committed jsr166 version. We'll need to sync this up with
> with openjdk tl someday, but might as well wait until
> other updates for Spliterators/streams are ready to integrate.
>
> -Doug
>
> *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
> --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
> ***************
> *** 579,595 ****
>           final ReentrantLock lock = this.lock;
>           lock.lock();
>           try {
> -             // Copy while checking if already present.
> -             // This wins in the most common case where it is not present
>
>               Object[] elements = getArray();
>               int len = elements.length;
> -             Object[] newElements = new Object[len + 1];
>
>               for (int i = 0; i < len; ++i) {
>                   if (eq(e, elements[i]))
> !                     return false; // exit, throwing away copy
> !                 else
> !                     newElements[i] = elements[i];
>
>               }
>               newElements[len] = e;
>               setArray(newElements);
>               return true;
> --- 579,591 ----
>           final ReentrantLock lock = this.lock;
>           lock.lock();
>           try {
>
>               Object[] elements = getArray();
>               int len = elements.length;
>               for (int i = 0; i < len; ++i) {
>                   if (eq(e, elements[i]))
> !                     return false;
>               }
> +             Object[] newElements = Arrays.copyOf(elements, len + 1);
>
>               newElements[len] = e;
>               setArray(newElements);
>               return true;
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130405/4fca6beb/attachment.html>

From vitalyd at gmail.com  Fri Apr  5 19:35:16 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 5 Apr 2013 19:35:16 -0400
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
Message-ID: <CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>

When you say it's slightly slower for cache misses, what numbers are we
talking about?

Personally, I don't see a reason to further optimize for cache hits - are
we really saying that's the common usage of COWAL? I'd find that hard to
believe.  At some point, it's probably better to simply externalize this
policy via constructor hint or subclass or whatever rather than catering to
both sides in shared code.

Sent from my phone
On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:

> I'm still advocating an optimistic approach, that does not hold the lock
> during the first traversal of the array snapshot.  This is much faster when
> cache hits are the norm (or equals methods are expensive), which I would
> hope would be the common case, and only slightly slower when all adds are
> cache misses.
>
>      public boolean addIfAbsent(E e) {
>         Object[] snapshot = getArray();
>         int len = snapshot.length;
>         for (int i = 0; i < len; i++)
>             if (eq(e, snapshot[i]))
>                 return false;
>         return addIfAbsent(e, snapshot);
>     }
>
>     private boolean addIfAbsent(E e, Object[] snapshot) {
>         final ReentrantLock lock = this.lock;
>         lock.lock();
>         try {
>             Object[] current = getArray();
>             int len = current.length;
>             if (snapshot != current) {
>                 // Optimize for contending with another addIfAbsent
>                 int common = Math.min(snapshot.length, len);
>                 for (int i = 0; i < common; i++)
>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>                         return false;
>                 for (int i = common; i < len; i++)
>                     if (eq(e, current[i]))
>                         return false;
>             }
>             Object[] newElements = Arrays.copyOf(current, len + 1);
>             newElements[len] = e;
>             setArray(newElements);
>             return true;
>         } finally {
>             lock.unlock();
>         }
>     }
>
>
>
> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> > On 04/03/13 06:35, Doug Lea wrote:
> >
> >> This was designed to perform best in the case of possibly contended
> >> updates when the element is absent, by avoiding retraversal, and
> >> thus minimizing lock hold times in the common case. (When not common,
> >> it can be guarded by a contains check.) However even in this case,
> >> it is possible that a retraversal via arraycopy could be faster
> >> because it can use optimized cheaper writes (fewer card marks).
> >>
> >
> > Yes, by a little.
> > A simple but reliable performance test is now at
> >
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
> > COWALAddIfAbsentLoops.java?**view=log<
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
> >
> >
> > The simplest change allowing this (below) also appears to
> > be among the fastest. Running across various machines and
> > settings (GC, client/server), it seems to be between 5% and 15%
> > faster. This is a smaller difference than in Ivan's tests,
> > that didn't include lock and contention effects.
> >
> > I committed jsr166 version. We'll need to sync this up with
> > with openjdk tl someday, but might as well wait until
> > other updates for Spliterators/streams are ready to integrate.
> >
> > -Doug
> >
> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
> > ***************
> > *** 579,595 ****
> >           final ReentrantLock lock = this.lock;
> >           lock.lock();
> >           try {
> > -             // Copy while checking if already present.
> > -             // This wins in the most common case where it is not
> present
> >
> >               Object[] elements = getArray();
> >               int len = elements.length;
> > -             Object[] newElements = new Object[len + 1];
> >
> >               for (int i = 0; i < len; ++i) {
> >                   if (eq(e, elements[i]))
> > !                     return false; // exit, throwing away copy
> > !                 else
> > !                     newElements[i] = elements[i];
> >
> >               }
> >               newElements[len] = e;
> >               setArray(newElements);
> >               return true;
> > --- 579,591 ----
> >           final ReentrantLock lock = this.lock;
> >           lock.lock();
> >           try {
> >
> >               Object[] elements = getArray();
> >               int len = elements.length;
> >               for (int i = 0; i < len; ++i) {
> >                   if (eq(e, elements[i]))
> > !                     return false;
> >               }
> > +             Object[] newElements = Arrays.copyOf(elements, len + 1);
> >
> >               newElements[len] = e;
> >               setArray(newElements);
> >               return true;
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130405/b96f6e48/attachment.html>

From stanimir at riflexo.com  Sat Apr  6 02:31:00 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 6 Apr 2013 09:31:00 +0300
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
Message-ID: <CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>

I was thinking exactly the same as Martin Buhholz however I could not find
any single use in the our code where it'd be profitable to check outside
the lock optimistically. addIfAbsent is fairly used in the form of COWSet.
i.e. adds at the end, random removals and some COWSets may contain a few
thousands entries. However it's virtually guaranteed addIfAbsent to return
true, while remove(E) may be racily called and return false, i.e. an
optimistic approach to remove() might be beneficial.

On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Personally, I don't see a reason to further optimize for cache hits - are
> we really saying that's the common usage of COWAL? I'd find that hard to
> believe.  At some point, it's probably better to simply externalize this
> policy via constructor hint or subclass or whatever rather than catering to
> both sides in shared code.
>
> Having c-tor hint would require to propagate it would COWSet too. IMO,
addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
require the concrete type to be declared rather than the interface.
Also that will not affect the existing code bases and very few would
actually know the proper use case or the use cases would remain the same.
If the element is likely to already exists and the users are aware of the
current implementation it's likely to be guarded by contains(), such cases
require refactoring to take benefit and the code would require new java
version - unlikely to happen for large projects.
If there is higher contention and smaller array-sizes busy waiting w/ some
backoff might be even better.

Stanimir

> Sent from my phone
> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>
>> I'm still advocating an optimistic approach, that does not hold the lock
>> during the first traversal of the array snapshot.  This is much faster
>> when
>> cache hits are the norm (or equals methods are expensive), which I would
>> hope would be the common case, and only slightly slower when all adds are
>> cache misses.
>>
>>      public boolean addIfAbsent(E e) {
>>         Object[] snapshot = getArray();
>>         int len = snapshot.length;
>>         for (int i = 0; i < len; i++)
>>             if (eq(e, snapshot[i]))
>>                 return false;
>>         return addIfAbsent(e, snapshot);
>>     }
>>
>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>         final ReentrantLock lock = this.lock;
>>         lock.lock();
>>         try {
>>             Object[] current = getArray();
>>             int len = current.length;
>>             if (snapshot != current) {
>>                 // Optimize for contending with another addIfAbsent
>>                 int common = Math.min(snapshot.length, len);
>>                 for (int i = 0; i < common; i++)
>>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>>                         return false;
>>                 for (int i = common; i < len; i++)
>>                     if (eq(e, current[i]))
>>                         return false;
>>             }
>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>             newElements[len] = e;
>>             setArray(newElements);
>>             return true;
>>         } finally {
>>             lock.unlock();
>>         }
>>     }
>>
>>
>>
>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> > On 04/03/13 06:35, Doug Lea wrote:
>> >
>> >> This was designed to perform best in the case of possibly contended
>> >> updates when the element is absent, by avoiding retraversal, and
>> >> thus minimizing lock hold times in the common case. (When not common,
>> >> it can be guarded by a contains check.) However even in this case,
>> >> it is possible that a retraversal via arraycopy could be faster
>> >> because it can use optimized cheaper writes (fewer card marks).
>> >>
>> >
>> > Yes, by a little.
>> > A simple but reliable performance test is now at
>> >
>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>> > COWALAddIfAbsentLoops.java?**view=log<
>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>> >
>>
>> >
>> > The simplest change allowing this (below) also appears to
>> > be among the fastest. Running across various machines and
>> > settings (GC, client/server), it seems to be between 5% and 15%
>> > faster. This is a smaller difference than in Ivan's tests,
>> > that didn't include lock and contention effects.
>> >
>> > I committed jsr166 version. We'll need to sync this up with
>> > with openjdk tl someday, but might as well wait until
>> > other updates for Spliterators/streams are ready to integrate.
>> >
>> > -Doug
>> >
>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>
>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>> > ***************
>> > *** 579,595 ****
>> >           final ReentrantLock lock = this.lock;
>> >           lock.lock();
>> >           try {
>> > -             // Copy while checking if already present.
>> > -             // This wins in the most common case where it is not
>> present
>> >
>> >               Object[] elements = getArray();
>> >               int len = elements.length;
>> > -             Object[] newElements = new Object[len + 1];
>> >
>> >               for (int i = 0; i < len; ++i) {
>> >                   if (eq(e, elements[i]))
>> > !                     return false; // exit, throwing away copy
>> > !                 else
>> > !                     newElements[i] = elements[i];
>> >
>> >               }
>> >               newElements[len] = e;
>> >               setArray(newElements);
>> >               return true;
>> > --- 579,591 ----
>> >           final ReentrantLock lock = this.lock;
>> >           lock.lock();
>> >           try {
>> >
>> >               Object[] elements = getArray();
>> >               int len = elements.length;
>> >               for (int i = 0; i < len; ++i) {
>> >                   if (eq(e, elements[i]))
>> > !                     return false;
>> >               }
>> > +             Object[] newElements = Arrays.copyOf(elements, len + 1);
>> >
>> >               newElements[len] = e;
>> >               setArray(newElements);
>> >               return true;
>> >
>> >
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130406/7a0c7c63/attachment-0001.html>

From martinrb at google.com  Sat Apr  6 16:38:05 2013
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 6 Apr 2013 13:38:05 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
Message-ID: <CA+kOe09EN3im9dv0NCY5K97xgs4J_yXsf13WVuup5R0C2WVPcg@mail.gmail.com>

The joys of microbenchmarking - much worse when benchmarking concurrent
code.
Here you have confounding factors of how many cpus you have for real
parallelism, whether biased locking kicks in, and how expensive the equals
method is.

CVS now has a variant of Doug's benchmark, COWALAddIfAbsentStringLoops,
that wins on my proposed addIfAbsent even when the element is never
present, by moving equals method calls out of the lock.

Here's the latest version of my proposed implementation:

    public boolean addIfAbsent(E e) {
        Object[] snapshot = getArray();
        return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
            addIfAbsent(e, snapshot);
    }

    /**
     * A version of addIfAbsent using the strong hint that recent
     * snapshot does not contain e.
     */
    private boolean addIfAbsent(E e, Object[] snapshot) {
        final ReentrantLock lock = this.lock;
        lock.lock();
        try {
            Object[] current = getArray();
            int len = current.length;
            if (snapshot != current) {
                // Optimize for lost race to another addXXX operation
                int common = Math.min(snapshot.length, len);
                for (int i = 0; i < common; i++)
                    if (current[i] != snapshot[i] && eq(e, current[i]))
                        return false;
                if (indexOf(e, current, common, len) >= 0)
                        return false;
            }
            Object[] newElements = Arrays.copyOf(current, len + 1);
            newElements[len] = e;
            setArray(newElements);
            return true;
        } finally {
            lock.unlock();
        }
    }



On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> I was thinking exactly the same as Martin Buhholz however I could not find
> any single use in the our code where it'd be profitable to check outside
> the lock optimistically. addIfAbsent is fairly used in the form of COWSet.
> i.e. adds at the end, random removals and some COWSets may contain a few
> thousands entries. However it's virtually guaranteed addIfAbsent to return
> true, while remove(E) may be racily called and return false, i.e. an
> optimistic approach to remove() might be beneficial.
>
> On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Personally, I don't see a reason to further optimize for cache hits - are
>> we really saying that's the common usage of COWAL? I'd find that hard to
>> believe.  At some point, it's probably better to simply externalize this
>> policy via constructor hint or subclass or whatever rather than catering to
>> both sides in shared code.
>>
>> Having c-tor hint would require to propagate it would COWSet too. IMO,
> addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
> require the concrete type to be declared rather than the interface.
> Also that will not affect the existing code bases and very few would
> actually know the proper use case or the use cases would remain the same.
> If the element is likely to already exists and the users are aware of the
> current implementation it's likely to be guarded by contains(), such cases
> require refactoring to take benefit and the code would require new java
> version - unlikely to happen for large projects.
> If there is higher contention and smaller array-sizes busy waiting w/ some
> backoff might be even better.
>
> Stanimir
>
>> Sent from my phone
>> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>
>>> I'm still advocating an optimistic approach, that does not hold the lock
>>> during the first traversal of the array snapshot.  This is much faster
>>> when
>>> cache hits are the norm (or equals methods are expensive), which I would
>>> hope would be the common case, and only slightly slower when all adds are
>>> cache misses.
>>>
>>>      public boolean addIfAbsent(E e) {
>>>         Object[] snapshot = getArray();
>>>         int len = snapshot.length;
>>>         for (int i = 0; i < len; i++)
>>>             if (eq(e, snapshot[i]))
>>>                 return false;
>>>         return addIfAbsent(e, snapshot);
>>>     }
>>>
>>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>>         final ReentrantLock lock = this.lock;
>>>         lock.lock();
>>>         try {
>>>             Object[] current = getArray();
>>>             int len = current.length;
>>>             if (snapshot != current) {
>>>                 // Optimize for contending with another addIfAbsent
>>>                 int common = Math.min(snapshot.length, len);
>>>                 for (int i = 0; i < common; i++)
>>>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>>>                         return false;
>>>                 for (int i = common; i < len; i++)
>>>                     if (eq(e, current[i]))
>>>                         return false;
>>>             }
>>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>>             newElements[len] = e;
>>>             setArray(newElements);
>>>             return true;
>>>         } finally {
>>>             lock.unlock();
>>>         }
>>>     }
>>>
>>>
>>>
>>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>> > On 04/03/13 06:35, Doug Lea wrote:
>>> >
>>> >> This was designed to perform best in the case of possibly contended
>>> >> updates when the element is absent, by avoiding retraversal, and
>>> >> thus minimizing lock hold times in the common case. (When not common,
>>> >> it can be guarded by a contains check.) However even in this case,
>>> >> it is possible that a retraversal via arraycopy could be faster
>>> >> because it can use optimized cheaper writes (fewer card marks).
>>> >>
>>> >
>>> > Yes, by a little.
>>> > A simple but reliable performance test is now at
>>> >
>>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>>> > COWALAddIfAbsentLoops.java?**view=log<
>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>>> >
>>>
>>> >
>>> > The simplest change allowing this (below) also appears to
>>> > be among the fastest. Running across various machines and
>>> > settings (GC, client/server), it seems to be between 5% and 15%
>>> > faster. This is a smaller difference than in Ivan's tests,
>>> > that didn't include lock and contention effects.
>>> >
>>> > I committed jsr166 version. We'll need to sync this up with
>>> > with openjdk tl someday, but might as well wait until
>>> > other updates for Spliterators/streams are ready to integrate.
>>> >
>>> > -Doug
>>> >
>>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>>
>>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>>> > ***************
>>> > *** 579,595 ****
>>> >           final ReentrantLock lock = this.lock;
>>> >           lock.lock();
>>> >           try {
>>> > -             // Copy while checking if already present.
>>> > -             // This wins in the most common case where it is not
>>> present
>>> >
>>> >               Object[] elements = getArray();
>>> >               int len = elements.length;
>>> > -             Object[] newElements = new Object[len + 1];
>>> >
>>> >               for (int i = 0; i < len; ++i) {
>>> >                   if (eq(e, elements[i]))
>>> > !                     return false; // exit, throwing away copy
>>> > !                 else
>>> > !                     newElements[i] = elements[i];
>>> >
>>> >               }
>>> >               newElements[len] = e;
>>> >               setArray(newElements);
>>> >               return true;
>>> > --- 579,591 ----
>>> >           final ReentrantLock lock = this.lock;
>>> >           lock.lock();
>>> >           try {
>>> >
>>> >               Object[] elements = getArray();
>>> >               int len = elements.length;
>>> >               for (int i = 0; i < len; ++i) {
>>> >                   if (eq(e, elements[i]))
>>> > !                     return false;
>>> >               }
>>> > +             Object[] newElements = Arrays.copyOf(elements, len + 1);
>>> >
>>> >               newElements[len] = e;
>>> >               setArray(newElements);
>>> >               return true;
>>> >
>>> >
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130406/1c9f4c23/attachment.html>

From hallerp at gmail.com  Mon Apr  8 08:47:44 2013
From: hallerp at gmail.com (Philipp Haller)
Date: Mon, 8 Apr 2013 13:47:44 +0100
Subject: [concurrency-interest] Scala Workshop 2013 - Second Call for Papers
Message-ID: <CALEKNwbmXq9O-QPTPJnTgQRryFqD6H0eumm+QDTK2SD=F0FOTg@mail.gmail.com>

========================================================================
                             "Scala 2013"

                   the Fourth Annual Scala Workshop
                      co-located with ECOOP 2013
                         Montpellier, France
                            July 2nd, 2013

                     SECOND CALL FOR PAPERS

                 http://lamp.epfl.ch/~hmiller/scala2013
========================================================================

    Abstract Submission: April 12, 2013
    Paper Submission   : April 19, 2013
    Author Notification: May 17, 2013
    Final Papers Due   : June 1, 2013 (to be confirmed)
    Early Registration : May 31, 2013

========================================================================

    We're happy to announce that, thanks to our sponsors, a limited number
    of accepted student talks will be awarded full ECOOP conference
    registrations!

========================================================================

Scala is a general purpose programming language designed to express common
programming patterns in a concise, elegant, and type-safe way. It smoothly
integrates features of object-oriented and functional languages.

This workshop is a forum for researchers and practitioners to share new
ideas
and results of interest to the Scala community.

We seek papers on topics related to Scala, including (but not limited to):

- Language design and implementation ? language extensions, optimization,
and
  performance evaluation.
- Library design and implementation patterns for extending Scala ? embedded
  domain-specific languages, combining language features, generic and
meta-programming.
- Formal techniques for Scala-like programs ? formalizations of the
language,
  type system, and semantics, formalizing proposed language extensions and
  variants, dependent object types, type and effect systems.
- Concurrent and distributed programming ? libraries, frameworks, language
  extensions, programming paradigms: (Actors, STM, ...), performance
  evaluation, experimental results.
- Safety and reliability ? pluggable type systems, contracts, static
analysis
  and verification, runtime monitoring.
- Tools ? development environments, debuggers, refactoring tools, testing
  frameworks.
- Case studies, experience reports, and pearls.

Submitted papers should describe new ideas, experimental results, or
projects
related to Scala. In order to encourage lively discussion, submitted papers
may describe work in progress. All papers will be judged on a combination of
correctness, significance, novelty, clarity, and interest to the community.

In general, papers should explain their original contributions,
identifying what has been accomplished, explaining why it is
significant, and relating it to previous work (also for other
languages where appropriate). Papers in the last category of the list
above need not necessarily report original research results; they may
instead, for example, report practical experience that will be useful
to others, new Scala idioms, or programming pearls. In all cases, such
a paper must make a contribution which is of interest to the Scala
community, or from which other members of the Scala community can
benefit.

Publications at the Scala Workshop represent works-in-progress and are
not intended to preclude later publication at any of the main
conferences. Though, follow-up submissions do need to conform to the
publication policies of the targeted conference, which typically
equates to significant extension or refinement of the workshop
publication.

KEYWORDS: Library Design and Implementation, Language Design and
Implementation, Applications, Formal Techniques, Parallelism and
Concurrency, Distributed Programming, Tools, Experience Reports,
Empirical Studies


## Student Talks ##

In addition to regular papers and tool demos, we also solicit short
student talks by PhD students. A student talk is not accompanied by
a paper (it is sufficient to submit a short abstract of the talk in
plain text). Student talks are about 5 minutes long, presenting
ongoing or completed research related to Scala, or announcing a
project that would be of interest to the Scala community.


## Proceedings ##

It is planned to publish accepted papers in the ACM Digital Library, unless
the authors choose not to. In case of publication in the ACM Digital
Library,
authors must transfer copyright to ACM upon acceptance (for government work,
to the extent transferable), but retain various rights (see ACM Copyright
Policy. Authors are encouraged to publish auxiliary material with their
paper
(source code, test data, etc.); they retain copyright of auxiliary material.


## Submission Details ##

* Abstract Submission: April 12, 2013
* Paper Submission   : April 19, 2013
* Author Notification: May 17, 2013
* Final Papers Due   : June 1, 2013 (to be confirmed)
* Early Registration : May 31, 2013

Submitted papers should be in portable document format (PDF), formatted
using
the standard ACM SIGPLAN two-column conference style (10pt format). Regular
research papers must not exceed 10 pages, tool demonstration papers and
short
papers must not exceed 4 pages. "Tool Demos" and "Short Papers" should be
marked as such with those words in the title at time of submission.

Student talks are not accompanied by papers. Therefore, it is sufficient to
only submit a plain-text abstract. "Student Talks" should be marked as such
with those words in the title at time of submission.

Submission is via EasyChair:
https://www.easychair.org/conferences/?conf=scala2013


## Program Committee ##

* Marius Eriksen, Twitter
* Viktor Kuncak, EPFL
* Mira Mezini, TU Darmstadt
* Matt Might, University of Utah
* Nate Nystrom, University of Lugano
* Bruno Oliveira, National University of Singapore
* Kunle Olukotun, Stanford University
* Aleksandar Prokopec, EPFL
* David Van Horn, Northeastern University
* Tobias Wrigstad, Uppsala University


## Organizing Committee ##

* Philipp Haller (Chair), Typesafe
* Martin Odersky, EPFL
* Doug Lea, SUNY Oswego
* Heather Miller (Co-Chair), EPFL
* Vojin Jovanovic, EPFL


## Links ##

* The Scala Workshop 2013 web site: http://lamp.epfl.ch/~hmiller/scala2013
* The ECOOP/ECSA/ECMFA 2013 web site:
http://www.lirmm.fr/ec-montpellier-2013
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130408/e17e2470/attachment.html>

From ron.pressler at gmail.com  Mon Apr  8 13:08:41 2013
From: ron.pressler at gmail.com (Ron Pressler)
Date: Mon, 8 Apr 2013 20:08:41 +0300
Subject: [concurrency-interest] jsr166e.ForkJoinPool performance issue
Message-ID: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>

Hi.
We're using ForkJoin extensively for various purposes. First, let me say
that the FJ implementation in jsr166e yields significantly higher
performance than that of JDK7; thanks for that!

Now, we're trying to use FJ for scheduling a message-passing component, and
I've noticed a performance issue that the code below demonstrates. This is
a degenerate case where only one FJTask is active at a time -- a single
task is submitted, which forks a single task and terminates. The new task
subsequently forks another one and so on.

When the parallelism level is set to 1, a run completes in about 35ms on my
machine. When it is set to 4 (I'm running on a 4-core i7, 8 virtual cores),
that duration rises above 120ms. The NetBeans profiler shows the hot-spot
to be ForkJoinPool.WorkQueue.push(), and, in particular, its call to
ForkJoinPool.signalWork. The NetBeans profiler provides far-from-definite
proof, but further testing has shown that this may, in fact, be true. In
our code, when some computation takes place in the tasks, NetBeans
attributes almost 40% of CPU time to WorkQueue.push().

It seems that this should actually be a simple case, with only one non-idle
Worker continually executing each new task as it forks. However, it seems
like in this case (and unlike with our other FJ uses), the pool is
sensitive to the parallelism level, which could be a problem.

Any help would be much appreciated.

Ron

Here's the benchmarking code:

import java.util.concurrent.TimeUnit;
import jsr166e.ForkJoinPool;
import jsr166e.ForkJoinTask;
import jsr166e.RecursiveAction;

public class FJBenchmark {
    static final int PARALLELISM = 4;
    static final int COUNT = 1000000;
    static ForkJoinPool fjPool = new ForkJoinPool(PARALLELISM,
ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);

    public static void main(String[] args) throws Exception {
        for (int i = 0; i < 10; i++)
            run(COUNT);
    }

    static void run(int count) throws Exception {
        RecursiveAction lastTask = new RecursiveAction() {
            protected void compute() {
            }
        };
        final long start = System.nanoTime();
        fjPool.submit(new MyTask(count, lastTask));
        lastTask.get();
        System.out.println("count: " + count + " time: " +
TimeUnit.MILLISECONDS.convert(System.nanoTime() - start,
TimeUnit.NANOSECONDS));
    }

    static class MyTask extends RecursiveAction {
        final int count;
        final ForkJoinTask lastTask;

        public MyTask(int count, ForkJoinTask lastTask) {
            this.count = count;
            this.lastTask = lastTask;
        }

        protected void compute() {
            if (count > 0)
                new MyTask(count - 1, lastTask).fork();
            else
                lastTask.fork();
        }
    }
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130408/ee28fbbb/attachment.html>

From dl at cs.oswego.edu  Mon Apr  8 13:22:35 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Apr 2013 13:22:35 -0400
Subject: [concurrency-interest] jsr166e.ForkJoinPool performance issue
In-Reply-To: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>
References: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>
Message-ID: <5162FCDB.9000709@cs.oswego.edu>

On 04/08/13 13:08, Ron Pressler wrote:
> Now, we're trying to use FJ for scheduling a message-passing component, and I've
> noticed a performance issue that the code below demonstrates. This is a
> degenerate case where only one FJTask is active at a time -- a single task is
> submitted, which forks a single task and terminates. The new task subsequently
> forks another one and so on.
>
> When the parallelism level is set to 1, a run completes in about 35ms on my
> machine. When it is set to 4 (I'm running on a 4-core i7, 8 virtual cores), that
> duration rises above 120ms. The NetBeans profiler shows the hot-spot to be
> ForkJoinPool.WorkQueue.push(), and, in particular, its call to
> ForkJoinPool.signalWork. The NetBeans profiler provides far-from-definite proof,
> but further testing has shown that this may, in fact, be true. In our code, when
> some computation takes place in the tasks, NetBeans attributes almost 40% of CPU
> time to WorkQueue.push().
>
> It seems that this should actually be a simple case, with only one non-idle
> Worker continually executing each new task as it forks. However, it seems like
> in this case (and unlike with our other FJ uses), the pool is sensitive to the
> parallelism level, which could be a problem.
>

The underlying issue is that the pushing task does not know that
the single worker is already available, so activates another.
(It can take a few dozen nanoseconds for workers to rescan before
idling.) So it is not so much parallelism-level as intrinsic raciness.
This turns out to be a common issue when processing small Streams
in upcoming jdk8 support, so I've been working to improve it.
Stay tuned...

-Doug




From aleksey.shipilev at oracle.com  Mon Apr  8 13:22:32 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 08 Apr 2013 21:22:32 +0400
Subject: [concurrency-interest] jsr166e.ForkJoinPool performance issue
In-Reply-To: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>
References: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>
Message-ID: <5162FCD8.6040000@oracle.com>

On 04/08/2013 09:08 PM, Ron Pressler wrote:
> When the parallelism level is set to 1, a run completes in about 35ms on
> my machine. When it is set to 4 (I'm running on a 4-core i7, 8 virtual
> cores), that duration rises above 120ms. The NetBeans profiler shows the
> hot-spot to be ForkJoinPool.WorkQueue.push(), and, in particular, its
> call to ForkJoinPool.signalWork. The NetBeans profiler provides
> far-from-definite proof, but further testing has shown that this may, in
> fact, be true. In our code, when some computation takes place in the
> tasks, NetBeans attributes almost 40% of CPU time to WorkQueue.push(). 

Can you confirm this with [1]? It could contain the outdated version of
FJP though. This looks like the signaling overheads on your particular
system.

-Aleksey.

[1] https://github.com/shipilev/java-forkjoin-trace


From martinrb at google.com  Mon Apr  8 13:45:01 2013
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 8 Apr 2013 10:45:01 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
Message-ID: <CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>

It would be nice if someone somewhere could confirm that addIfAbsent was
frequently called with element present on their codebase.

But even without any such guidance, I think it makes sense to optimize for
operations not needing to update, for both addIfAbsent and remove(Object).
 Operations that do are read only and don't acquire the lock have infinite
scalability, while the penalty for having everything go wrong while having
to do a second traversal is typically small, and at worst a factor of two.

Another way to look at it is that COW data structures should be used in a
read-mostly fashion.  So optimize assuming that operations like addIfAbsent
that may or may not be read-only, are in fact read-only.



On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> I was thinking exactly the same as Martin Buhholz however I could not find
> any single use in the our code where it'd be profitable to check outside
> the lock optimistically. addIfAbsent is fairly used in the form of COWSet.
> i.e. adds at the end, random removals and some COWSets may contain a few
> thousands entries. However it's virtually guaranteed addIfAbsent to return
> true, while remove(E) may be racily called and return false, i.e. an
> optimistic approach to remove() might be beneficial.
>
> On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Personally, I don't see a reason to further optimize for cache hits - are
>> we really saying that's the common usage of COWAL? I'd find that hard to
>> believe.  At some point, it's probably better to simply externalize this
>> policy via constructor hint or subclass or whatever rather than catering to
>> both sides in shared code.
>>
>> Having c-tor hint would require to propagate it would COWSet too. IMO,
> addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
> require the concrete type to be declared rather than the interface.
> Also that will not affect the existing code bases and very few would
> actually know the proper use case or the use cases would remain the same.
> If the element is likely to already exists and the users are aware of the
> current implementation it's likely to be guarded by contains(), such cases
> require refactoring to take benefit and the code would require new java
> version - unlikely to happen for large projects.
> If there is higher contention and smaller array-sizes busy waiting w/ some
> backoff might be even better.
>
> Stanimir
>
>> Sent from my phone
>> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>
>>> I'm still advocating an optimistic approach, that does not hold the lock
>>> during the first traversal of the array snapshot.  This is much faster
>>> when
>>> cache hits are the norm (or equals methods are expensive), which I would
>>> hope would be the common case, and only slightly slower when all adds are
>>> cache misses.
>>>
>>>      public boolean addIfAbsent(E e) {
>>>         Object[] snapshot = getArray();
>>>         int len = snapshot.length;
>>>         for (int i = 0; i < len; i++)
>>>             if (eq(e, snapshot[i]))
>>>                 return false;
>>>         return addIfAbsent(e, snapshot);
>>>     }
>>>
>>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>>         final ReentrantLock lock = this.lock;
>>>         lock.lock();
>>>         try {
>>>             Object[] current = getArray();
>>>             int len = current.length;
>>>             if (snapshot != current) {
>>>                 // Optimize for contending with another addIfAbsent
>>>                 int common = Math.min(snapshot.length, len);
>>>                 for (int i = 0; i < common; i++)
>>>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>>>                         return false;
>>>                 for (int i = common; i < len; i++)
>>>                     if (eq(e, current[i]))
>>>                         return false;
>>>             }
>>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>>             newElements[len] = e;
>>>             setArray(newElements);
>>>             return true;
>>>         } finally {
>>>             lock.unlock();
>>>         }
>>>     }
>>>
>>>
>>>
>>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>> > On 04/03/13 06:35, Doug Lea wrote:
>>> >
>>> >> This was designed to perform best in the case of possibly contended
>>> >> updates when the element is absent, by avoiding retraversal, and
>>> >> thus minimizing lock hold times in the common case. (When not common,
>>> >> it can be guarded by a contains check.) However even in this case,
>>> >> it is possible that a retraversal via arraycopy could be faster
>>> >> because it can use optimized cheaper writes (fewer card marks).
>>> >>
>>> >
>>> > Yes, by a little.
>>> > A simple but reliable performance test is now at
>>> >
>>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>>> > COWALAddIfAbsentLoops.java?**view=log<
>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>>> >
>>>
>>> >
>>> > The simplest change allowing this (below) also appears to
>>> > be among the fastest. Running across various machines and
>>> > settings (GC, client/server), it seems to be between 5% and 15%
>>> > faster. This is a smaller difference than in Ivan's tests,
>>> > that didn't include lock and contention effects.
>>> >
>>> > I committed jsr166 version. We'll need to sync this up with
>>> > with openjdk tl someday, but might as well wait until
>>> > other updates for Spliterators/streams are ready to integrate.
>>> >
>>> > -Doug
>>> >
>>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>>
>>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>>> > ***************
>>> > *** 579,595 ****
>>> >           final ReentrantLock lock = this.lock;
>>> >           lock.lock();
>>> >           try {
>>> > -             // Copy while checking if already present.
>>> > -             // This wins in the most common case where it is not
>>> present
>>> >
>>> >               Object[] elements = getArray();
>>> >               int len = elements.length;
>>> > -             Object[] newElements = new Object[len + 1];
>>> >
>>> >               for (int i = 0; i < len; ++i) {
>>> >                   if (eq(e, elements[i]))
>>> > !                     return false; // exit, throwing away copy
>>> > !                 else
>>> > !                     newElements[i] = elements[i];
>>> >
>>> >               }
>>> >               newElements[len] = e;
>>> >               setArray(newElements);
>>> >               return true;
>>> > --- 579,591 ----
>>> >           final ReentrantLock lock = this.lock;
>>> >           lock.lock();
>>> >           try {
>>> >
>>> >               Object[] elements = getArray();
>>> >               int len = elements.length;
>>> >               for (int i = 0; i < len; ++i) {
>>> >                   if (eq(e, elements[i]))
>>> > !                     return false;
>>> >               }
>>> > +             Object[] newElements = Arrays.copyOf(elements, len + 1);
>>> >
>>> >               newElements[len] = e;
>>> >               setArray(newElements);
>>> >               return true;
>>> >
>>> >
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130408/83475121/attachment-0001.html>

From ron.pressler at gmail.com  Mon Apr  8 14:41:23 2013
From: ron.pressler at gmail.com (Ron Pressler)
Date: Mon, 8 Apr 2013 21:41:23 +0300
Subject: [concurrency-interest] jsr166e.ForkJoinPool performance issue
In-Reply-To: <CABg6-qi82dxjp=tnJrkhhqYyaf5TNzEGmU1iHK-RbWw-qvYUSg@mail.gmail.com>
References: <CABg6-qizUfvUmNC+C8zHTDPqN3Qog3me5cdMmhfhh5TNHd4tJA@mail.gmail.com>
	<5162FCDB.9000709@cs.oswego.edu>
	<CABg6-qi82dxjp=tnJrkhhqYyaf5TNzEGmU1iHK-RbWw-qvYUSg@mail.gmail.com>
Message-ID: <CABg6-qiUsKpxQiWN55jmofbc2zHCW8Jf2721EFwjaJzMzdpDpA@mail.gmail.com>

> Oh, that makes sense (but wouldn't it also result in the creation of a new
> worker in the low parallelism case?)
>
> I guess that if you assume that the task is short lived you could signal
> the workers after it's done, or let them steal the forked task if the
> parent is long-lived. That might have other issues, though.
>
> I'm waiting with bated breath for the fix...
> On 04/08/13 13:08, Ron Pressler wrote:
>
>> Now, we're trying to use FJ for scheduling a message-passing component,
>> and I've
>> noticed a performance issue that the code below demonstrates. This is a
>> degenerate case where only one FJTask is active at a time -- a single
>> task is
>> submitted, which forks a single task and terminates. The new task
>> subsequently
>> forks another one and so on.
>>
>> When the parallelism level is set to 1, a run completes in about 35ms on
>> my
>> machine. When it is set to 4 (I'm running on a 4-core i7, 8 virtual
>> cores), that
>> duration rises above 120ms. The NetBeans profiler shows the hot-spot to be
>> ForkJoinPool.WorkQueue.push(), and, in particular, its call to
>> ForkJoinPool.signalWork. The NetBeans profiler provides far-from-definite
>> proof,
>> but further testing has shown that this may, in fact, be true. In our
>> code, when
>> some computation takes place in the tasks, NetBeans attributes almost 40%
>> of CPU
>> time to WorkQueue.push().
>>
>> It seems that this should actually be a simple case, with only one
>> non-idle
>> Worker continually executing each new task as it forks. However, it seems
>> like
>> in this case (and unlike with our other FJ uses), the pool is sensitive
>> to the
>> parallelism level, which could be a problem.
>>
>>
> The underlying issue is that the pushing task does not know that
> the single worker is already available, so activates another.
> (It can take a few dozen nanoseconds for workers to rescan before
> idling.) So it is not so much parallelism-level as intrinsic raciness.
> This turns out to be a common issue when processing small Streams
> in upcoming jdk8 support, so I've been working to improve it.
> Stay tuned...
>
> -Doug
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130408/8ea90bf5/attachment.html>

From mudit.f2004912 at gmail.com  Tue Apr  9 22:24:14 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 10 Apr 2013 04:24:14 +0200
Subject: [concurrency-interest] An Eventually Consistent highly scalable K-V
	Store/HashMap
Message-ID: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>

Hello All,

I am new to the group. For past few months I have been working on
Concurrent Data Structures for my master thesis. My work is motivated by
the assumption that in future, with growing number of cores per machine, we
might need to give up strong semantics in order to achieve better
performance.

For example, we have come up with a sequentially consistent version of
Queue which is 50-70 times faster than the original linearized
ConcurrentLinkedQueue under very high contention. In our synthetic
benchmarks we have found that in ConcurrentLinkedQueue one operation
(enqueue/dequeue) can take as high as 3,00,000 cycles  where
ThinkTime/Delay between two operations (work done between two ops)  is in
order of 200,000-300,000 cycles.

With the same approach of relaxing the semantics, we want to implement a KV
(hashmap/hashset) which will be designed just keeping in mind the very high
scalability.  Obviously, we are giving up on stronger semantics but we
think that there are applications which can bear with it.

We did some scalability testing on ConcurrentHashMap but I just now found
out that a new implementation is planned for Java 8 which has better
scalability.

We want to use CRDTs (conflict free replicated data types) techniques used
in distributed systems [1] where operations commute (add followed by remove
or remove followed by add converge to same state eventually).

We also think that transactions can be implemented on top KV  where
multiples puts/gets are grouped and applied by workers and have atomic (all
or none) effect on underneath KV store/hashMap.

Since people in this group work actively on Concurrent Data Structures, I
am looking for comments and suggestions on

a. Eventually Consistent HashMaps/KV Stores for multicores.
b. Transactions on top of HashMaps/KV Stores in multicores.

What are your take on these two ideas, will it be useful?  If yes, do you
have some applications in mind?

Thanks,
Mudit Verma
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/3835479d/attachment.html>

From oleksandr.otenko at oracle.com  Wed Apr 10 09:47:50 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 10 Apr 2013 14:47:50 +0100
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
Message-ID: <51656D86.2030907@oracle.com>

I'd start with defining "eventually" and "consistent".

Alex


On 10/04/2013 03:24, Mudit Verma wrote:
> Hello All,
>
> I am new to the group. For past few months I have been working on 
> Concurrent Data Structures for my master thesis. My work is motivated 
> by the assumption that in future, with growing number of cores per 
> machine, we might need to give up strong semantics in order to achieve 
> better performance.
>
> For example, we have come up with a sequentially consistent version of 
> Queue which is 50-70 times faster than the original linearized 
> ConcurrentLinkedQueue under very high contention. In our synthetic 
> benchmarks we have found that in ConcurrentLinkedQueue one operation 
> (enqueue/dequeue) can take as high as 3,00,000 cycles  where 
> ThinkTime/Delay between two operations (work done between two ops)  is 
> in order of 200,000-300,000 cycles.
>
> With the same approach of relaxing the semantics, we want to implement 
> a KV (hashmap/hashset) which will be designed just keeping in mind the 
> very high scalability.  Obviously, we are giving up on stronger 
> semantics but we think that there are applications which can bear with 
> it.
>
> We did some scalability testing on ConcurrentHashMap but I just now 
> found out that a new implementation is planned for Java 8 which has 
> better scalability.
>
> We want to use CRDTs (conflict free replicated data types) techniques 
> used in distributed systems [1] where operations commute (add followed 
> by remove or remove followed by add converge to same state eventually).
>
> We also think that transactions can be implemented on top KV  where 
> multiples puts/gets are grouped and applied by workers and have atomic 
> (all or none) effect on underneath KV store/hashMap.
>
> Since people in this group work actively on Concurrent Data 
> Structures, I am looking for comments and suggestions on
>
> a. Eventually Consistent HashMaps/KV Stores for multicores.
> b. Transactions on top of HashMaps/KV Stores in multicores.
>
> What are your take on these two ideas, will it be useful?  If yes, do 
> you have some applications in mind?
>
> Thanks,
> Mudit Verma
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/394a1e92/attachment.html>

From nathan.reynolds at oracle.com  Wed Apr 10 12:03:55 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Apr 2013 09:03:55 -0700
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
Message-ID: <51658D6B.2040908@oracle.com>

What kind of transactions are you talking about?  Software transactional 
memory?  Hardware transactional memory?  Database-like transactions?

Eventually consistent HashMaps/KV stores could definitely be useful for 
caches.  Application servers have many caches with a lot of threads 
hitting them rapidly.  Simply replacing the lock with a slower 
concurrent path isn't going help.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/9/2013 7:24 PM, Mudit Verma wrote:
> Hello All,
>
> I am new to the group. For past few months I have been working on 
> Concurrent Data Structures for my master thesis. My work is motivated 
> by the assumption that in future, with growing number of cores per 
> machine, we might need to give up strong semantics in order to achieve 
> better performance.
>
> For example, we have come up with a sequentially consistent version of 
> Queue which is 50-70 times faster than the original linearized 
> ConcurrentLinkedQueue under very high contention. In our synthetic 
> benchmarks we have found that in ConcurrentLinkedQueue one operation 
> (enqueue/dequeue) can take as high as 3,00,000 cycles  where 
> ThinkTime/Delay between two operations (work done between two ops)  is 
> in order of 200,000-300,000 cycles.
>
> With the same approach of relaxing the semantics, we want to implement 
> a KV (hashmap/hashset) which will be designed just keeping in mind the 
> very high scalability.  Obviously, we are giving up on stronger 
> semantics but we think that there are applications which can bear with 
> it.
>
> We did some scalability testing on ConcurrentHashMap but I just now 
> found out that a new implementation is planned for Java 8 which has 
> better scalability.
>
> We want to use CRDTs (conflict free replicated data types) techniques 
> used in distributed systems [1] where operations commute (add followed 
> by remove or remove followed by add converge to same state eventually).
>
> We also think that transactions can be implemented on top KV  where 
> multiples puts/gets are grouped and applied by workers and have atomic 
> (all or none) effect on underneath KV store/hashMap.
>
> Since people in this group work actively on Concurrent Data 
> Structures, I am looking for comments and suggestions on
>
> a. Eventually Consistent HashMaps/KV Stores for multicores.
> b. Transactions on top of HashMaps/KV Stores in multicores.
>
> What are your take on these two ideas, will it be useful?  If yes, do 
> you have some applications in mind?
>
> Thanks,
> Mudit Verma
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/93865a94/attachment.html>

From cheremin at gmail.com  Wed Apr 10 12:20:01 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Wed, 10 Apr 2013 20:20:01 +0400
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <51658D6B.2040908@oracle.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
	<51658D6B.2040908@oracle.com>
Message-ID: <CAOwENiJNsYVyPoX-TH9duF52bNRAbnFwoMx7c5X-bYr3_WymRg@mail.gmail.com>

Eventually consistent cache can be easy created based on open-addressing
hash map, and immutable entries -- without any sync-ing, just like
String.hashCode does. It works if value (which cached) evaluated as f(key),
and f() is pure: JMM guarantee for immutable objects, + atomicity of stores
for references, + OoTA guaranteed by JMM makes it work.

 But I doubt it'll be much faster than precisely implemented sych-ed map.
Seems like (at least on x86) for reads sync-ed and EC versions will differ
only in compiler barriers.




2013/4/10 Nathan Reynolds <nathan.reynolds at oracle.com>

>  What kind of transactions are you talking about?  Software transactional
> memory?  Hardware transactional memory?  Database-like transactions?
>
> Eventually consistent HashMaps/KV stores could definitely be useful for
> caches.  Application servers have many caches with a lot of threads hitting
> them rapidly.  Simply replacing the lock with a slower concurrent path
> isn't going help.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/9/2013 7:24 PM, Mudit Verma wrote:
>
>     Hello All,
>
> I am new to the group. For past few months I have been working on
> Concurrent Data Structures for my master thesis. My work is motivated by
> the assumption that in future, with growing number of cores per machine, we
> might need to give up strong semantics in order to achieve better
> performance.
>
>  For example, we have come up with a sequentially consistent version of
> Queue which is 50-70 times faster than the original linearized
> ConcurrentLinkedQueue under very high contention. In our synthetic
> benchmarks we have found that in ConcurrentLinkedQueue one operation
> (enqueue/dequeue) can take as high as 3,00,000 cycles  where
> ThinkTime/Delay between two operations (work done between two ops)  is in
> order of 200,000-300,000 cycles.
>
>  With the same approach of relaxing the semantics, we want to implement a
> KV (hashmap/hashset) which will be designed just keeping in mind the very
> high scalability.  Obviously, we are giving up on stronger semantics but we
> think that there are applications which can bear with it.
>
>  We did some scalability testing on ConcurrentHashMap but I just now found
> out that a new implementation is planned for Java 8 which has better
> scalability.
>
>  We want to use CRDTs (conflict free replicated data types) techniques
> used in distributed systems [1] where operations commute (add followed by
> remove or remove followed by add converge to same state eventually).
>
>  We also think that transactions can be implemented on top KV  where
> multiples puts/gets are grouped and applied by workers and have atomic (all
> or none) effect on underneath KV store/hashMap.
>
> Since people in this group work actively on Concurrent Data Structures, I
> am looking for comments and suggestions on
>
> a. Eventually Consistent HashMaps/KV Stores for multicores.
> b. Transactions on top of HashMaps/KV Stores in multicores.
>
>  What are your take on these two ideas, will it be useful?  If yes, do you
> have some applications in mind?
>
>  Thanks,
> Mudit Verma
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/76253eae/attachment.html>

From mudit.f2004912 at gmail.com  Wed Apr 10 12:36:32 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 10 Apr 2013 18:36:32 +0200
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <51658D6B.2040908@oracle.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
	<51658D6B.2040908@oracle.com>
Message-ID: <CAMM2gsNhj=C-B2HJcfG=JRCRjzUY_J0Jp2-Oiu1swj8qe1iS=w@mail.gmail.com>

Data base like transactions. Main idea is to use community between the
operation. This will allow HashMap to converge eventually to deterministic
state, no matter in which order the operations are applied. Transaction
will give all or none kind of behavior + if a two operation X and Y are
committed in a transaction. Both object should be consistent. If I see the
effect of X, I should be able to see the effect of Y as well.

Conflict free replicated data tyes - CRDT
 pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf

Thanks,
Mudit




On Wed, Apr 10, 2013 at 6:03 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  What kind of transactions are you talking about?  Software transactional
> memory?  Hardware transactional memory?  Database-like transactions?
>
> Eventually consistent HashMaps/KV stores could definitely be useful for
> caches.  Application servers have many caches with a lot of threads hitting
> them rapidly.  Simply replacing the lock with a slower concurrent path
> isn't going help.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/9/2013 7:24 PM, Mudit Verma wrote:
>
>     Hello All,
>
> I am new to the group. For past few months I have been working on
> Concurrent Data Structures for my master thesis. My work is motivated by
> the assumption that in future, with growing number of cores per machine, we
> might need to give up strong semantics in order to achieve better
> performance.
>
>  For example, we have come up with a sequentially consistent version of
> Queue which is 50-70 times faster than the original linearized
> ConcurrentLinkedQueue under very high contention. In our synthetic
> benchmarks we have found that in ConcurrentLinkedQueue one operation
> (enqueue/dequeue) can take as high as 3,00,000 cycles  where
> ThinkTime/Delay between two operations (work done between two ops)  is in
> order of 200,000-300,000 cycles.
>
>  With the same approach of relaxing the semantics, we want to implement a
> KV (hashmap/hashset) which will be designed just keeping in mind the very
> high scalability.  Obviously, we are giving up on stronger semantics but we
> think that there are applications which can bear with it.
>
>  We did some scalability testing on ConcurrentHashMap but I just now found
> out that a new implementation is planned for Java 8 which has better
> scalability.
>
>  We want to use CRDTs (conflict free replicated data types) techniques
> used in distributed systems [1] where operations commute (add followed by
> remove or remove followed by add converge to same state eventually).
>
>  We also think that transactions can be implemented on top KV  where
> multiples puts/gets are grouped and applied by workers and have atomic (all
> or none) effect on underneath KV store/hashMap.
>
> Since people in this group work actively on Concurrent Data Structures, I
> am looking for comments and suggestions on
>
> a. Eventually Consistent HashMaps/KV Stores for multicores.
> b. Transactions on top of HashMaps/KV Stores in multicores.
>
>  What are your take on these two ideas, will it be useful?  If yes, do you
> have some applications in mind?
>
>  Thanks,
> Mudit Verma
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/73ebb401/attachment-0001.html>

From hans.boehm at hp.com  Wed Apr 10 13:40:25 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 10 Apr 2013 17:40:25 +0000
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <CAOwENiJNsYVyPoX-TH9duF52bNRAbnFwoMx7c5X-bYr3_WymRg@mail.gmail.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
	<51658D6B.2040908@oracle.com>
	<CAOwENiJNsYVyPoX-TH9duF52bNRAbnFwoMx7c5X-bYr3_WymRg@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369C745F@G9W0725.americas.hpqcorp.net>

What are you going to put into these stores?  If thread 1 puts in a reference to an object P, and thread 2 retrieves P from the store, is thread 2 guaranteed to be able to see all of P's fields, including the non-final ones changed after construction?  Does communicating the object through the KV store ensure visibility?  If not, this seems like a very limited facility that requires extremely careful documentation.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Ruslan Cheremin
Sent: Wednesday, April 10, 2013 9:20 AM
To: Nathan Reynolds
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] An Eventually Consistent highly scalable K-V Store/HashMap

Eventually consistent cache can be easy created based on open-addressing hash map, and immutable entries -- without any sync-ing, just like String.hashCode does. It works if value (which cached) evaluated as f(key), and f() is pure: JMM guarantee for immutable objects, + atomicity of stores for references, + OoTA guaranteed by JMM makes it work.

 But I doubt it'll be much faster than precisely implemented sych-ed map. Seems like (at least on x86) for reads sync-ed and EC versions will differ only in compiler barriers.



2013/4/10 Nathan Reynolds <nathan.reynolds at oracle.com<mailto:nathan.reynolds at oracle.com>>
What kind of transactions are you talking about?  Software transactional memory?  Hardware transactional memory?  Database-like transactions?

Eventually consistent HashMaps/KV stores could definitely be useful for caches.  Application servers have many caches with a lot of threads hitting them rapidly.  Simply replacing the lock with a slower concurrent path isn't going help.
Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | Architect | 602.333.9091
Oracle PSR Engineering<http://psr.us.oracle.com/> | Server Technology
On 4/9/2013 7:24 PM, Mudit Verma wrote:
Hello All,

I am new to the group. For past few months I have been working on Concurrent Data Structures for my master thesis. My work is motivated by the assumption that in future, with growing number of cores per machine, we might need to give up strong semantics in order to achieve better performance.
For example, we have come up with a sequentially consistent version of Queue which is 50-70 times faster than the original linearized  ConcurrentLinkedQueue under very high contention. In our synthetic benchmarks we have found that in ConcurrentLinkedQueue one operation (enqueue/dequeue) can take as high as 3,00,000 cycles  where ThinkTime/Delay between two operations (work done between two ops)  is in order of 200,000-300,000 cycles.
With the same approach of relaxing the semantics, we want to implement a KV (hashmap/hashset) which will be designed just keeping in mind the very high scalability.  Obviously, we are giving up on stronger semantics but we think that there are applications which can bear with it.
We did some scalability testing on ConcurrentHashMap but I just now found out that a new implementation is planned for Java 8 which has better scalability.
We want to use CRDTs (conflict free replicated data types) techniques used in distributed systems [1] where operations commute (add followed by remove or remove followed by add converge to same state eventually).
We also think that transactions can be implemented on top KV  where multiples puts/gets are grouped and applied by workers and have atomic (all or none) effect on underneath KV store/hashMap.

Since people in this group work actively on Concurrent Data Structures, I am looking for comments and suggestions on

a. Eventually Consistent HashMaps/KV Stores for multicores.
b. Transactions on top of HashMaps/KV Stores in multicores.
What are your take on these two ideas, will it be useful?  If yes, do you have some applications in mind?
Thanks,
Mudit Verma



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/dfa7bd29/attachment.html>

From oleksandr.otenko at oracle.com  Wed Apr 10 13:52:06 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 10 Apr 2013 18:52:06 +0100
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <CAMM2gsNhj=C-B2HJcfG=JRCRjzUY_J0Jp2-Oiu1swj8qe1iS=w@mail.gmail.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
	<51658D6B.2040908@oracle.com>
	<CAMM2gsNhj=C-B2HJcfG=JRCRjzUY_J0Jp2-Oiu1swj8qe1iS=w@mail.gmail.com>
Message-ID: <5165A6C6.3080706@oracle.com>

That is why it is important to start with the definition of consistency.

When you mean the results do not depend on the order of the operations, 
you mean two things:

1. state transition is linearizable (can be represented as a list of 
updates)
2. folding linearized state transitions is, in fact, a monoid 
homomorphism (applying updates is a mapping of free monoid (list of 
updates) to a domain-specific monoid)

You may find it difficult to marry a monoid and happens-before 
requirements of any kind (not even talking about visibility of X 
implying visibility of Y, when Y is outside HashMap).

So you will need to restrict the requirements of where the ordering 
doesn't matter.

Alex


On 10/04/2013 17:36, Mudit Verma wrote:
> Data base like transactions. Main idea is to use community between the 
> operation. This will allow HashMap to converge eventually to 
> deterministic state, no matter in which order the operations are 
> applied. Transaction will give all or none kind of behavior + if a two 
> operation X and Y are committed in a transaction. Both object should 
> be consistent. If I see the effect of X, I should be able to see the 
> effect of Y as well.
>
> Conflict free replicated data tyes - CRDT
> pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf 
> <http://pagesperso-systeme.lip6.fr/Marc.Shapiro/papers/RR-6956.pdf>
>
> Thanks,
> Mudit
>
>
>
>
> On Wed, Apr 10, 2013 at 6:03 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     What kind of transactions are you talking about? Software
>     transactional memory?  Hardware transactional memory? 
>     Database-like transactions?
>
>     Eventually consistent HashMaps/KV stores could definitely be
>     useful for caches.  Application servers have many caches with a
>     lot of threads hitting them rapidly.  Simply replacing the lock
>     with a slower concurrent path isn't going help.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 4/9/2013 7:24 PM, Mudit Verma wrote:
>>     Hello All,
>>
>>     I am new to the group. For past few months I have been working on
>>     Concurrent Data Structures for my master thesis. My work is
>>     motivated by the assumption that in future, with growing number
>>     of cores per machine, we might need to give up strong semantics
>>     in order to achieve better performance.
>>
>>     For example, we have come up with a sequentially consistent
>>     version of Queue which is 50-70 times faster than the original
>>     linearized ConcurrentLinkedQueue under very high contention. In
>>     our synthetic benchmarks we have found that in
>>     ConcurrentLinkedQueue one operation (enqueue/dequeue) can take as
>>     high as 3,00,000 cycles  where ThinkTime/Delay between two
>>     operations (work done between two ops)  is in order of
>>     200,000-300,000 cycles.
>>
>>     With the same approach of relaxing the semantics, we want to
>>     implement a KV (hashmap/hashset) which will be designed just
>>     keeping in mind the very high scalability.  Obviously, we are
>>     giving up on stronger semantics but we think that there are
>>     applications which can bear with it.
>>
>>     We did some scalability testing on ConcurrentHashMap but I just
>>     now found out that a new implementation is planned for Java 8
>>     which has better scalability.
>>
>>     We want to use CRDTs (conflict free replicated data types)
>>     techniques used in distributed systems [1] where operations
>>     commute (add followed by remove or remove followed by add
>>     converge to same state eventually).
>>
>>     We also think that transactions can be implemented on top KV 
>>     where multiples puts/gets are grouped and applied by workers and
>>     have atomic (all or none) effect on underneath KV store/hashMap.
>>
>>     Since people in this group work actively on Concurrent Data
>>     Structures, I am looking for comments and suggestions on
>>
>>     a. Eventually Consistent HashMaps/KV Stores for multicores.
>>     b. Transactions on top of HashMaps/KV Stores in multicores.
>>
>>     What are your take on these two ideas, will it be useful?  If
>>     yes, do you have some applications in mind?
>>
>>     Thanks,
>>     Mudit Verma
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130410/0bee8b10/attachment-0001.html>

From discus at kotek.net  Thu Apr 11 03:08:57 2013
From: discus at kotek.net (Jan Kotek)
Date: Thu, 11 Apr 2013 08:08:57 +0100
Subject: [concurrency-interest] An Eventually Consistent highly scalable
	K-V Store/HashMap
In-Reply-To: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
Message-ID: <41386124.jsafdxjMAY@artemis>

Hi,

not sure how much it is related, but I wrote BTreeMap and HTreeMap which have 
transactions. It uses embedded db to store tree nodes, but could be easily 
altered to use keep node instances in heap, this way it would work as any 
other heap based collection.

https://github.com/jankotek/mapdb

Jan

On Wednesday 10 April 2013 04:24:14 Mudit Verma wrote:

Hello All, 

I am new to the group. For past few months I have been working on Concurrent 
Data Structures for my master thesis. My work is motivated by the assumption 
that in future, with growing number of cores per machine, we might need to 
give up strong semantics in order to achieve better performance. 


For example, we have come up with a sequentially consistent version of Queue 
which is 50-70 times faster than the original linearized  
ConcurrentLinkedQueue under very high contention. In our synthetic benchmarks 
we have found that in ConcurrentLinkedQueue one operation (enqueue/dequeue) 
can take as high as 3,00,000 cycles  where ThinkTime/Delay between two 
operations (work done between two ops)  is in order of 200,000-300,000 cycles. 


With the same approach of relaxing the semantics, we want to implement a KV 
(hashmap/hashset) which will be designed just keeping in mind the very high 
scalability.  Obviously, we are giving up on stronger semantics but we think 
that there are applications which can bear with it. 


We did some scalability testing on ConcurrentHashMap but I just now found out 
that a new implementation is planned for Java 8 which has better scalability. 


We want to use CRDTs (conflict free replicated data types) techniques used in 
distributed systems [1] where operations commute (add followed by remove or 
remove followed by add converge to same state eventually). 


We also think that transactions can be implemented on top KV  where multiples 
puts/gets are grouped and applied by workers and have atomic (all or none) 
effect on underneath KV store/hashMap.  

Since people in this group work actively on Concurrent Data Structures, I am 
looking for comments and suggestions on 


a. Eventually Consistent HashMaps/KV Stores for multicores.
b. Transactions on top of HashMaps/KV Stores in multicores.


What are your take on these two ideas, will it be useful?  If yes, do you have 
some applications in mind? 


Thanks,
Mudit Verma






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130411/e4b46b19/attachment.html>

From thurston at nomagicsoftware.com  Fri Apr 12 14:55:44 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 12 Apr 2013 11:55:44 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
Message-ID: <1365792944576-9408.post@n7.nabble.com>

In thinking about whether code is thread-safe or not, one can attempt to
identify whether it 'contains a data-race'.  If not, you're good.  Else you
need to add an explicit happens-before relationship.

Which begs the question: what exactly constitutes a 'data-race'?  And here
I'm interested in something a little more formal than the famed judicial
judgement of obscenity (I know it when I see it)

If you do a web search, you unfortunately get quite a few divergent
definitions, many of which seem to be inconsistent.  
IIRC, the official JMM defines a data-race as any two conflicting operations
from two or more threads on shared data (where at least one of the two
operations is a write).

Brian Goetz (in his excellent  article
<http://www.ibm.com/developerworks/library/j-jtp03304/>  ) defines data-race
thusly:

"A program is said to have a data race, and therefore not be a "properly
synchronized" program, when there is a variable that is read by more than
one thread, written by at least one thread, and the write and the reads are
not ordered by a happens-before relationship."

But this would mark the following code as a data-race

int shared = 0

Thread 1                  Thread 2                 Thread 3
local = this.shared      this.shared = 10       local = this.shared

This clearly meets his definition, yet I do not consider this a 'data-race'.

I've always relied on traditional database concurrency control theory (I
still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
best), which has a formal definition of 'serializability', viz. that any
transaction log is 'serializable', if and only if, its serialization graph
is acyclic.  Why can we not use this as the basis for a formal definition of
'data-race' (excluding the notion of commit and abort of course):

"A program is said to have a data-race, if any legal (as prescribed by the
MM) execution order produces a serialization graph that is *cyclic*"

It has the advantage of a formal, mathematical model and although it is has
historically been confined to databases (and transactions), it seems
applicable to concurrent execution of any kind?

Hoping that I don't get flamed.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From cheremin at gmail.com  Fri Apr 12 15:12:29 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 12 Apr 2013 23:12:29 +0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365792944576-9408.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
Message-ID: <CAOwENiLtbeectV_+UHYt1c+ubK9XaTxQpnq_LE5wnkLvMP=8oA@mail.gmail.com>

As far, as I understand, notation of data race comes from technical, not
mathematical point of view. Data race, defined as above, is exactly the
case there it is hard for _implementation_ to hide dirty details of
underlaying mechanics -- e.g that stores/loads are not "instant", and may
have "stages", they can be pipelined, so started executing early and
actually finished latter. Hide this details is not impossible, in theory,
but just hard to implement. And because of this, it was decided to put on
programmer the responsibility to "order" such instructions. There are exist
algorithms to find and order such races, but, afaik, they are NP-complete
with N around total instructions in program (i.e. such algorithms require
global program analyze), so not practical today.



> But this would mark the following code as a data-race
>
> int shared = 0
>
> Thread 1                  Thread 2                 Thread 3
> local = this.shared      this.shared = 10       local = this.shared
>
> This clearly meets his definition, yet I do not consider this a
> 'data-race'.
>




> I've always relied on traditional database concurrency control theory (I
> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
> best), which has a formal definition of 'serializability', viz. that any
> transaction log is 'serializable', if and only if, its serialization graph
> is acyclic.  Why can we not use this as the basis for a formal definition
> of
> 'data-race' (excluding the notion of commit and abort of course):
>
> "A program is said to have a data-race, if any legal (as prescribed by the
> MM) execution order produces a serialization graph that is *cyclic*"
>
> It has the advantage of a formal, mathematical model and although it is has
> historically been confined to databases (and transactions), it seems
> applicable to concurrent execution of any kind?
>
> Hoping that I don't get flamed.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130412/67ffa828/attachment.html>

From hans.boehm at hp.com  Fri Apr 12 15:58:29 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 12 Apr 2013 19:58:29 +0000
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAOwENiLtbeectV_+UHYt1c+ubK9XaTxQpnq_LE5wnkLvMP=8oA@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<CAOwENiLtbeectV_+UHYt1c+ubK9XaTxQpnq_LE5wnkLvMP=8oA@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369C899F@G9W0725.americas.hpqcorp.net>

Right.  The absence of a data race, as defined by the JMM guarantees that synchronization-free code sequences appear indivisible.  No other thread can see an intermediate state.  That means that:


-          Programmer no longer has to reason about instruction-level interleaving.  As far as they're concerned, context switches only happen at synchronization operations, even if there is a multiprocessor involved.

-          It no longer matters whether your stores are done a byte or a word at a time.

-          Sync-free library calls to e.g. copying an array, are no different from single byte assignments; they appear to happen at once, and you no longer need to reason about intermediate, half-completed. int assignments behave like long assignments.

-          Compilers can optimize in a fairly conventional way within these sync-free regions, and you can't tell the difference.

These can clearly not hold unless programs like the one below are considered to have a data race.

There are asymptotically efficient (perhaps 10x-100x slowdown) ways to test dynamically whether a data race has occurred.  (And much more efficiently with hardware assist.)  I don't think there are practical ways to statically and fully accurately check non-tiny applications for data races.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Ruslan Cheremin
Sent: Friday, April 12, 2013 12:12 PM
To: thurstonn
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] On A Formal Definition of 'Data-Race'

As far, as I understand, notation of data race comes from technical, not mathematical point of view. Data race, defined as above, is exactly the case there it is hard for _implementation_ to hide dirty details of underlaying mechanics -- e.g that stores/loads are not "instant", and may have "stages", they can be pipelined, so started executing early and actually finished latter. Hide this details is not impossible, in theory, but just hard to implement. And because of this, it was decided to put on programmer the responsibility to "order" such instructions. There are exist algorithms to find and order such races, but, afaik, they are NP-complete with N around total instructions in program (i.e. such algorithms require global program analyze), so not practical today.


But this would mark the following code as a data-race

int shared = 0

Thread 1                  Thread 2                 Thread 3
local = this.shared      this.shared = 10       local = this.shared

This clearly meets his definition, yet I do not consider this a 'data-race'.



I've always relied on traditional database concurrency control theory (I
still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
best), which has a formal definition of 'serializability', viz. that any
transaction log is 'serializable', if and only if, its serialization graph
is acyclic.  Why can we not use this as the basis for a formal definition of
'data-race' (excluding the notion of commit and abort of course):

"A program is said to have a data-race, if any legal (as prescribed by the
MM) execution order produces a serialization graph that is *cyclic*"

It has the advantage of a formal, mathematical model and although it is has
historically been confined to databases (and transactions), it seems
applicable to concurrent execution of any kind?

Hoping that I don't get flamed.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130412/c649be0b/attachment-0001.html>

From martinrb at google.com  Fri Apr 12 16:24:37 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 12 Apr 2013 13:24:37 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
	<CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>
Message-ID: <CA+kOe080J5ixGFa7EPMM7Ls1iB5fUaUsmvnY6WEWskdRF24GuQ@mail.gmail.com>

I persuaded Doug to accept my suggestions for remove(Object) and
addIfAbsent:

--- src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
18:44:19 -0000 1.103
+++ src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
20:20:10 -0000
@@ -498,35 +498,44 @@
      * @return {@code true} if this list contained the specified element
      */
     public boolean remove(Object o) {
+        Object[] snapshot = getArray();
+        int index = indexOf(o, snapshot, 0, snapshot.length);
+        return (index < 0) ? false : remove(o, snapshot, index);
+    }
+
+    /**
+     * A version of remove(Object) using the strong hint that given
+     * recent snapshot contains o at the given index.
+     */
+    private boolean remove(Object o, Object[] snapshot, int index) {
         final ReentrantLock lock = this.lock;
         lock.lock();
         try {
-            Object[] elements = getArray();
-            int len = elements.length;
-            if (len != 0) {
-                // Copy while searching for element to remove
-                // This wins in the normal case of element being present
-                int newlen = len - 1;
-                Object[] newElements = new Object[newlen];
-
-                for (int i = 0; i < newlen; ++i) {
-                    if (eq(o, elements[i])) {
-                        // found one;  copy remaining and exit
-                        for (int k = i + 1; k < len; ++k)
-                            newElements[k-1] = elements[k];
-                        setArray(newElements);
-                        return true;
-                    } else
-                        newElements[i] = elements[i];
-                }
-
-                // special handling for last cell
-                if (eq(o, elements[newlen])) {
-                    setArray(newElements);
-                    return true;
+            Object[] current = getArray();
+            int len = current.length;
+            if (snapshot != current) findIndex: {
+                int prefix = Math.min(index, len);
+                for (int i = 0; i < prefix; i++) {
+                    if (current[i] != snapshot[i] && eq(o, current[i])) {
+                        index = i;
+                        break findIndex;
+                    }
                 }
+                if (index >= len)
+                    return false;
+                if (current[index] == o)
+                    break findIndex;
+                index = indexOf(o, current, index, len);
+                if (index < 0)
+                    return false;
             }
-            return false;
+            Object[] newElements = new Object[len - 1];
+            System.arraycopy(current, 0, newElements, 0, index);
+            System.arraycopy(current, index + 1,
+                             newElements, index,
+                             len - index - 1);
+            setArray(newElements);
+            return true;
         } finally {
             lock.unlock();
         }
@@ -576,16 +585,31 @@
      * @return {@code true} if the element was added
      */
     public boolean addIfAbsent(E e) {
+        Object[] snapshot = getArray();
+        return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
+            addIfAbsent(e, snapshot);
+    }
+
+    /**
+     * A version of addIfAbsent using the strong hint that given
+     * recent snapshot does not contain e.
+     */
+    private boolean addIfAbsent(E e, Object[] snapshot) {
         final ReentrantLock lock = this.lock;
         lock.lock();
         try {
-            Object[] elements = getArray();
-            int len = elements.length;
-            for (int i = 0; i < len; ++i) {
-                if (eq(e, elements[i]))
-                    return false;
+            Object[] current = getArray();
+            int len = current.length;
+            if (snapshot != current) {
+                // Optimize for lost race to another addXXX operation
+                int common = Math.min(snapshot.length, len);
+                for (int i = 0; i < common; i++)
+                    if (current[i] != snapshot[i] && eq(e, current[i]))
+                        return false;
+                if (indexOf(e, current, common, len) >= 0)
+                        return false;
             }
-            Object[] newElements = Arrays.copyOf(elements, len + 1);
+            Object[] newElements = Arrays.copyOf(current, len + 1);
             newElements[len] = e;
             setArray(newElements);
             return true;



On Mon, Apr 8, 2013 at 10:45 AM, Martin Buchholz <martinrb at google.com>wrote:

> It would be nice if someone somewhere could confirm that addIfAbsent was
> frequently called with element present on their codebase.
>
> But even without any such guidance, I think it makes sense to optimize for
> operations not needing to update, for both addIfAbsent and remove(Object).
>  Operations that do are read only and don't acquire the lock have infinite
> scalability, while the penalty for having everything go wrong while having
> to do a second traversal is typically small, and at worst a factor of two.
>
> Another way to look at it is that COW data structures should be used in a
> read-mostly fashion.  So optimize assuming that operations like addIfAbsent
> that may or may not be read-only, are in fact read-only.
>
>
>
> On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> I was thinking exactly the same as Martin Buhholz however I could not
>> find any single use in the our code where it'd be profitable to check
>> outside the lock optimistically. addIfAbsent is fairly used in the form of
>> COWSet. i.e. adds at the end, random removals and some COWSets may contain
>> a few thousands entries. However it's virtually guaranteed addIfAbsent to
>> return true, while remove(E) may be racily called and return false, i.e. an
>> optimistic approach to remove() might be beneficial.
>>
>> On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> Personally, I don't see a reason to further optimize for cache hits -
>>> are we really saying that's the common usage of COWAL? I'd find that hard
>>> to believe.  At some point, it's probably better to simply externalize this
>>> policy via constructor hint or subclass or whatever rather than catering to
>>> both sides in shared code.
>>>
>>> Having c-tor hint would require to propagate it would COWSet too. IMO,
>> addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
>> require the concrete type to be declared rather than the interface.
>> Also that will not affect the existing code bases and very few would
>> actually know the proper use case or the use cases would remain the same.
>> If the element is likely to already exists and the users are aware of the
>> current implementation it's likely to be guarded by contains(), such cases
>> require refactoring to take benefit and the code would require new java
>> version - unlikely to happen for large projects.
>> If there is higher contention and smaller array-sizes busy waiting w/
>> some backoff might be even better.
>>
>> Stanimir
>>
>>>  Sent from my phone
>>> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>>
>>>> I'm still advocating an optimistic approach, that does not hold the lock
>>>> during the first traversal of the array snapshot.  This is much faster
>>>> when
>>>> cache hits are the norm (or equals methods are expensive), which I would
>>>> hope would be the common case, and only slightly slower when all adds
>>>> are
>>>> cache misses.
>>>>
>>>>      public boolean addIfAbsent(E e) {
>>>>         Object[] snapshot = getArray();
>>>>         int len = snapshot.length;
>>>>         for (int i = 0; i < len; i++)
>>>>             if (eq(e, snapshot[i]))
>>>>                 return false;
>>>>         return addIfAbsent(e, snapshot);
>>>>     }
>>>>
>>>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>>>         final ReentrantLock lock = this.lock;
>>>>         lock.lock();
>>>>         try {
>>>>             Object[] current = getArray();
>>>>             int len = current.length;
>>>>             if (snapshot != current) {
>>>>                 // Optimize for contending with another addIfAbsent
>>>>                 int common = Math.min(snapshot.length, len);
>>>>                 for (int i = 0; i < common; i++)
>>>>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>>>>                         return false;
>>>>                 for (int i = common; i < len; i++)
>>>>                     if (eq(e, current[i]))
>>>>                         return false;
>>>>             }
>>>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>>>             newElements[len] = e;
>>>>             setArray(newElements);
>>>>             return true;
>>>>         } finally {
>>>>             lock.unlock();
>>>>         }
>>>>     }
>>>>
>>>>
>>>>
>>>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>>
>>>> > On 04/03/13 06:35, Doug Lea wrote:
>>>> >
>>>> >> This was designed to perform best in the case of possibly contended
>>>> >> updates when the element is absent, by avoiding retraversal, and
>>>> >> thus minimizing lock hold times in the common case. (When not common,
>>>> >> it can be guarded by a contains check.) However even in this case,
>>>> >> it is possible that a retraversal via arraycopy could be faster
>>>> >> because it can use optimized cheaper writes (fewer card marks).
>>>> >>
>>>> >
>>>> > Yes, by a little.
>>>> > A simple but reliable performance test is now at
>>>> >
>>>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>>>> > COWALAddIfAbsentLoops.java?**view=log<
>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>>>> >
>>>>
>>>> >
>>>> > The simplest change allowing this (below) also appears to
>>>> > be among the fastest. Running across various machines and
>>>> > settings (GC, client/server), it seems to be between 5% and 15%
>>>> > faster. This is a smaller difference than in Ivan's tests,
>>>> > that didn't include lock and contention effects.
>>>> >
>>>> > I committed jsr166 version. We'll need to sync this up with
>>>> > with openjdk tl someday, but might as well wait until
>>>> > other updates for Spliterators/streams are ready to integrate.
>>>> >
>>>> > -Doug
>>>> >
>>>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>>>
>>>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>>>> > ***************
>>>> > *** 579,595 ****
>>>> >           final ReentrantLock lock = this.lock;
>>>> >           lock.lock();
>>>> >           try {
>>>> > -             // Copy while checking if already present.
>>>> > -             // This wins in the most common case where it is not
>>>> present
>>>> >
>>>> >               Object[] elements = getArray();
>>>> >               int len = elements.length;
>>>> > -             Object[] newElements = new Object[len + 1];
>>>> >
>>>> >               for (int i = 0; i < len; ++i) {
>>>> >                   if (eq(e, elements[i]))
>>>> > !                     return false; // exit, throwing away copy
>>>> > !                 else
>>>> > !                     newElements[i] = elements[i];
>>>> >
>>>> >               }
>>>> >               newElements[len] = e;
>>>> >               setArray(newElements);
>>>> >               return true;
>>>> > --- 579,591 ----
>>>> >           final ReentrantLock lock = this.lock;
>>>> >           lock.lock();
>>>> >           try {
>>>> >
>>>> >               Object[] elements = getArray();
>>>> >               int len = elements.length;
>>>> >               for (int i = 0; i < len; ++i) {
>>>> >                   if (eq(e, elements[i]))
>>>> > !                     return false;
>>>> >               }
>>>> > +             Object[] newElements = Arrays.copyOf(elements, len + 1);
>>>> >
>>>> >               newElements[len] = e;
>>>> >               setArray(newElements);
>>>> >               return true;
>>>> >
>>>> >
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130412/41c66d6c/attachment-0001.html>

From brian at briangoetz.com  Sat Apr 13 16:04:01 2013
From: brian at briangoetz.com (Brian Goetz)
Date: Sat, 13 Apr 2013 16:04:01 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365792944576-9408.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
Message-ID: <5169BA31.7010903@briangoetz.com>

Consider this similar-looking programing:

Object lock = new Object();
int shared = 0;

Thread 1: synchronized (lock) { local = shared; }
Thread 2: synchronized (lock) ( shared = 10; }
Thread 3: synchronized (lock) { local = shared; }

Is what you're saying "in either my racy program or Brian's non-racy 
program, the effect will be the same -- threads 1 and 3 will each see 
either 0 or 10, and can't predict which because they do not control the 
timing of thread scheduling and lock acquisition", and from there, going 
to "so why do we call one racy and not the other?"

In a properly synchronized program, you only see changes to shared 
variables at synchronization points, whereas in a racy program, they 
happen whenever, and the order in which they appear to happen may be 
different from the perspective of different threads.

Or, maybe you just don't like the happens-before model because its 
different from what you're used to?  HB seems no less well-defined and 
tractable than the other models you seem to like.  The rules are simple:

  - Synchronization actions (lock acquire/release, volatile read/write) 
are totally ordered
  - If two actions A and B are in the same thread, and A precedes B in 
the program order, then A happens-before B
  - Writes of volatile variables happen-before subsequent reads of that 
same variable (we can say "subsequent" because of the first rule above)
  - Releasing a lock happens-before subsequent acquisitions of that lock

There are a few other rules (having to do with starting/joining with 
threads, finalizers, etc) but they rarely come into play.

On 4/12/2013 2:55 PM, thurstonn wrote:
> In thinking about whether code is thread-safe or not, one can attempt to
> identify whether it 'contains a data-race'.  If not, you're good.  Else you
> need to add an explicit happens-before relationship.
>
> Which begs the question: what exactly constitutes a 'data-race'?  And here
> I'm interested in something a little more formal than the famed judicial
> judgement of obscenity (I know it when I see it)
>
> If you do a web search, you unfortunately get quite a few divergent
> definitions, many of which seem to be inconsistent.
> IIRC, the official JMM defines a data-race as any two conflicting operations
> from two or more threads on shared data (where at least one of the two
> operations is a write).
>
> Brian Goetz (in his excellent  article
> <http://www.ibm.com/developerworks/library/j-jtp03304/>  ) defines data-race
> thusly:
>
> "A program is said to have a data race, and therefore not be a "properly
> synchronized" program, when there is a variable that is read by more than
> one thread, written by at least one thread, and the write and the reads are
> not ordered by a happens-before relationship."
>
> But this would mark the following code as a data-race
>
> int shared = 0
>
> Thread 1                  Thread 2                 Thread 3
> local = this.shared      this.shared = 10       local = this.shared
>
> This clearly meets his definition, yet I do not consider this a 'data-race'.
>
> I've always relied on traditional database concurrency control theory (I
> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
> best), which has a formal definition of 'serializability', viz. that any
> transaction log is 'serializable', if and only if, its serialization graph
> is acyclic.  Why can we not use this as the basis for a formal definition of
> 'data-race' (excluding the notion of commit and abort of course):
>
> "A program is said to have a data-race, if any legal (as prescribed by the
> MM) execution order produces a serialization graph that is *cyclic*"
>
> It has the advantage of a formal, mathematical model and although it is has
> historically been confined to databases (and transactions), it seems
> applicable to concurrent execution of any kind?
>
> Hoping that I don't get flamed.
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From thurston at nomagicsoftware.com  Sat Apr 13 21:42:01 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 13 Apr 2013 18:42:01 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <5169BA31.7010903@briangoetz.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
Message-ID: <1365903721280-9413.post@n7.nabble.com>

Before I answer fully, let me ask you about another variant of the program:


Thread 1                     Thread 2
this.shared = 10            local = this.shared

Is this "racy"?  Clearly there is no explicit happens-before.  But, at least
in my reading of the (your) definition that I quoted in my OP, it wouldn't
qualify as a data-race.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From yankee.sierra at gmail.com  Sat Apr 13 23:04:19 2013
From: yankee.sierra at gmail.com (ys)
Date: Sat, 13 Apr 2013 23:04:19 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365903721280-9413.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
Message-ID: <CAE+h5-BLYQQ7R=-=fgVbLo3xYkrmCSBeC5uupHiu1RNkPNmUzw@mail.gmail.com>

Is "this.shared" a non-volatile long or double? If so, how would it not be
racy? And if not, isn't there precedent to declare it a benign race (rather
than not a race at all)?


On Sat, Apr 13, 2013 at 9:42 PM, thurstonn <thurston at nomagicsoftware.com>wrote:

> Before I answer fully, let me ask you about another variant of the program:
>
>
> Thread 1                     Thread 2
> this.shared = 10            local = this.shared
>
> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
> least
> in my reading of the (your) definition that I quoted in my OP, it wouldn't
> qualify as a data-race.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130413/766cc634/attachment.html>

From kirk at kodewerk.com  Sun Apr 14 04:27:17 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Sun, 14 Apr 2013 09:27:17 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365903721280-9413.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
Message-ID: <150D3927-808D-45E2-AAF1-A3712E818660@kodewerk.com>

To be honest, without any context it's hard to determine if any of the behaviours are racy or not. Brian's example looked very racy, this one looks worse... the original is obviously the worst of the bunch... the question is; what do you mean by racy and at some point you have to have some domain context to understand intend so you know what is or isn't allowed in that context. Any less and it's a discussion of semantics to which all I can say is that you're all right and you're all wrong at the same time.

-- Kirk

On 2013-04-14, at 2:42 AM, thurstonn <thurston at nomagicsoftware.com> wrote:

> Before I answer fully, let me ask you about another variant of the program:
> 
> 
> Thread 1                     Thread 2
> this.shared = 10            local = this.shared
> 
> Is this "racy"?  Clearly there is no explicit happens-before.  But, at least
> in my reading of the (your) definition that I quoted in my OP, it wouldn't
> qualify as a data-race.
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From brian at briangoetz.com  Sun Apr 14 13:23:08 2013
From: brian at briangoetz.com (Brian Goetz)
Date: Sun, 14 Apr 2013 13:23:08 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAE+h5-BLYQQ7R=-=fgVbLo3xYkrmCSBeC5uupHiu1RNkPNmUzw@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAE+h5-BLYQQ7R=-=fgVbLo3xYkrmCSBeC5uupHiu1RNkPNmUzw@mail.gmail.com>
Message-ID: <516AE5FC.3010104@briangoetz.com>

The "long or double" part is irrelevant.  People get all torn up about 
word-tearing -- as if it somehow was a case unto itself -- but even if 
long/double reads/writes were guaranteed tear-free, it would *still* be 
a data race if the variable is not volatile.  The word-tearing risk 
simply expands the set of "bad things that can happen in a data race" 
from "could read stale values" to "could read half-stale values."

On 4/13/2013 11:04 PM, ys wrote:
> Is "this.shared" a non-volatile long or double? If so, how would it not
> be racy? And if not, isn't there precedent to declare it a benign race
> (rather than not a race at all)?
>
>
> On Sat, Apr 13, 2013 at 9:42 PM, thurstonn <thurston at nomagicsoftware.com
> <mailto:thurston at nomagicsoftware.com>> wrote:
>
>     Before I answer fully, let me ask you about another variant of the
>     program:
>
>
>     Thread 1                     Thread 2
>     this.shared = 10            local = this.shared
>
>     Is this "racy"?  Clearly there is no explicit happens-before.  But,
>     at least
>     in my reading of the (your) definition that I quoted in my OP, it
>     wouldn't
>     qualify as a data-race.
>
>
>
>     --
>     View this message in context:
>     http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From stanimir at riflexo.com  Sun Apr 14 18:28:46 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 15 Apr 2013 01:28:46 +0300
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe080J5ixGFa7EPMM7Ls1iB5fUaUsmvnY6WEWskdRF24GuQ@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
	<CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>
	<CA+kOe080J5ixGFa7EPMM7Ls1iB5fUaUsmvnY6WEWskdRF24GuQ@mail.gmail.com>
Message-ID: <CAEJX8orLHR9-Scy+DRY17UEOo_zjboCc2m-bvAj+vPNBJY7nsw@mail.gmail.com>

One more, perhaps it's a bit too late but adding a c-tor
public CopyOnWriteArrayList(E firstElement){
  setArray(new Object[]{e});
}
seems quite useful, alternatively change CopyOnWriteArrayList(E[]
toCopyIn)to be varArgs.

Virtually all dynamically created
CopyOnWriteArrayList/CopyOnWriteArraySetwould have a single element
added from get go. Generally I do not like
sugar but the need to wrap the 1st element looks clunky.

Stanimir

On Fri, Apr 12, 2013 at 11:24 PM, Martin Buchholz <martinrb at google.com>wrote:

> I persuaded Doug to accept my suggestions for remove(Object) and
> addIfAbsent:
>
> --- src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
> 18:44:19 -0000 1.103
> +++ src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
> 20:20:10 -0000
> @@ -498,35 +498,44 @@
>       * @return {@code true} if this list contained the specified element
>       */
>      public boolean remove(Object o) {
> +        Object[] snapshot = getArray();
> +        int index = indexOf(o, snapshot, 0, snapshot.length);
> +        return (index < 0) ? false : remove(o, snapshot, index);
> +    }
> +
> +    /**
> +     * A version of remove(Object) using the strong hint that given
> +     * recent snapshot contains o at the given index.
> +     */
> +    private boolean remove(Object o, Object[] snapshot, int index) {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
> -            Object[] elements = getArray();
> -            int len = elements.length;
> -            if (len != 0) {
> -                // Copy while searching for element to remove
> -                // This wins in the normal case of element being present
> -                int newlen = len - 1;
> -                Object[] newElements = new Object[newlen];
> -
> -                for (int i = 0; i < newlen; ++i) {
> -                    if (eq(o, elements[i])) {
> -                        // found one;  copy remaining and exit
> -                        for (int k = i + 1; k < len; ++k)
> -                            newElements[k-1] = elements[k];
> -                        setArray(newElements);
> -                        return true;
> -                    } else
> -                        newElements[i] = elements[i];
> -                }
> -
> -                // special handling for last cell
> -                if (eq(o, elements[newlen])) {
> -                    setArray(newElements);
> -                    return true;
> +            Object[] current = getArray();
> +            int len = current.length;
> +            if (snapshot != current) findIndex: {
> +                int prefix = Math.min(index, len);
> +                for (int i = 0; i < prefix; i++) {
> +                    if (current[i] != snapshot[i] && eq(o, current[i])) {
> +                        index = i;
> +                        break findIndex;
> +                    }
>                  }
> +                if (index >= len)
> +                    return false;
> +                if (current[index] == o)
> +                    break findIndex;
> +                index = indexOf(o, current, index, len);
> +                if (index < 0)
> +                    return false;
>              }
> -            return false;
> +            Object[] newElements = new Object[len - 1];
> +            System.arraycopy(current, 0, newElements, 0, index);
> +            System.arraycopy(current, index + 1,
> +                             newElements, index,
> +                             len - index - 1);
> +            setArray(newElements);
> +            return true;
>          } finally {
>              lock.unlock();
>          }
> @@ -576,16 +585,31 @@
>       * @return {@code true} if the element was added
>       */
>      public boolean addIfAbsent(E e) {
> +        Object[] snapshot = getArray();
> +        return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
> +            addIfAbsent(e, snapshot);
> +    }
> +
> +    /**
> +     * A version of addIfAbsent using the strong hint that given
> +     * recent snapshot does not contain e.
> +     */
> +    private boolean addIfAbsent(E e, Object[] snapshot) {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
> -            Object[] elements = getArray();
> -            int len = elements.length;
> -            for (int i = 0; i < len; ++i) {
> -                if (eq(e, elements[i]))
> -                    return false;
> +            Object[] current = getArray();
> +            int len = current.length;
> +            if (snapshot != current) {
> +                // Optimize for lost race to another addXXX operation
> +                int common = Math.min(snapshot.length, len);
> +                for (int i = 0; i < common; i++)
> +                    if (current[i] != snapshot[i] && eq(e, current[i]))
> +                        return false;
> +                if (indexOf(e, current, common, len) >= 0)
> +                        return false;
>              }
> -            Object[] newElements = Arrays.copyOf(elements, len + 1);
> +            Object[] newElements = Arrays.copyOf(current, len + 1);
>              newElements[len] = e;
>              setArray(newElements);
>              return true;
>
>
>
> On Mon, Apr 8, 2013 at 10:45 AM, Martin Buchholz <martinrb at google.com>wrote:
>
>> It would be nice if someone somewhere could confirm that addIfAbsent was
>> frequently called with element present on their codebase.
>>
>> But even without any such guidance, I think it makes sense to optimize
>> for operations not needing to update, for both addIfAbsent and
>> remove(Object).  Operations that do are read only and don't acquire the
>> lock have infinite scalability, while the penalty for having everything go
>> wrong while having to do a second traversal is typically small, and at
>> worst a factor of two.
>>
>> Another way to look at it is that COW data structures should be used in a
>> read-mostly fashion.  So optimize assuming that operations like addIfAbsent
>> that may or may not be read-only, are in fact read-only.
>>
>>
>>
>> On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff <stanimir at riflexo.com
>> > wrote:
>>
>>> I was thinking exactly the same as Martin Buhholz however I could not
>>> find any single use in the our code where it'd be profitable to check
>>> outside the lock optimistically. addIfAbsent is fairly used in the form of
>>> COWSet. i.e. adds at the end, random removals and some COWSets may contain
>>> a few thousands entries. However it's virtually guaranteed addIfAbsent to
>>> return true, while remove(E) may be racily called and return false, i.e. an
>>> optimistic approach to remove() might be beneficial.
>>>
>>> On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> Personally, I don't see a reason to further optimize for cache hits -
>>>> are we really saying that's the common usage of COWAL? I'd find that hard
>>>> to believe.  At some point, it's probably better to simply externalize this
>>>> policy via constructor hint or subclass or whatever rather than catering to
>>>> both sides in shared code.
>>>>
>>>> Having c-tor hint would require to propagate it would COWSet too. IMO,
>>> addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
>>> require the concrete type to be declared rather than the interface.
>>> Also that will not affect the existing code bases and very few would
>>> actually know the proper use case or the use cases would remain the same.
>>> If the element is likely to already exists and the users are aware of the
>>> current implementation it's likely to be guarded by contains(), such cases
>>> require refactoring to take benefit and the code would require new java
>>> version - unlikely to happen for large projects.
>>> If there is higher contention and smaller array-sizes busy waiting w/
>>> some backoff might be even better.
>>>
>>> Stanimir
>>>
>>>>  Sent from my phone
>>>> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>>>
>>>>> I'm still advocating an optimistic approach, that does not hold the
>>>>> lock
>>>>> during the first traversal of the array snapshot.  This is much faster
>>>>> when
>>>>> cache hits are the norm (or equals methods are expensive), which I
>>>>> would
>>>>> hope would be the common case, and only slightly slower when all adds
>>>>> are
>>>>> cache misses.
>>>>>
>>>>>      public boolean addIfAbsent(E e) {
>>>>>         Object[] snapshot = getArray();
>>>>>         int len = snapshot.length;
>>>>>         for (int i = 0; i < len; i++)
>>>>>             if (eq(e, snapshot[i]))
>>>>>                 return false;
>>>>>         return addIfAbsent(e, snapshot);
>>>>>     }
>>>>>
>>>>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>>>>         final ReentrantLock lock = this.lock;
>>>>>         lock.lock();
>>>>>         try {
>>>>>             Object[] current = getArray();
>>>>>             int len = current.length;
>>>>>             if (snapshot != current) {
>>>>>                 // Optimize for contending with another addIfAbsent
>>>>>                 int common = Math.min(snapshot.length, len);
>>>>>                 for (int i = 0; i < common; i++)
>>>>>                     if (current[i] != snapshot[i] && eq(e, current[i]))
>>>>>                         return false;
>>>>>                 for (int i = common; i < len; i++)
>>>>>                     if (eq(e, current[i]))
>>>>>                         return false;
>>>>>             }
>>>>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>>>>             newElements[len] = e;
>>>>>             setArray(newElements);
>>>>>             return true;
>>>>>         } finally {
>>>>>             lock.unlock();
>>>>>         }
>>>>>     }
>>>>>
>>>>>
>>>>>
>>>>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>>>
>>>>> > On 04/03/13 06:35, Doug Lea wrote:
>>>>> >
>>>>> >> This was designed to perform best in the case of possibly contended
>>>>> >> updates when the element is absent, by avoiding retraversal, and
>>>>> >> thus minimizing lock hold times in the common case. (When not
>>>>> common,
>>>>> >> it can be guarded by a contains check.) However even in this case,
>>>>> >> it is possible that a retraversal via arraycopy could be faster
>>>>> >> because it can use optimized cheaper writes (fewer card marks).
>>>>> >>
>>>>> >
>>>>> > Yes, by a little.
>>>>> > A simple but reliable performance test is now at
>>>>> >
>>>>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>>>>> > COWALAddIfAbsentLoops.java?**view=log<
>>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>>>>> >
>>>>>
>>>>> >
>>>>> > The simplest change allowing this (below) also appears to
>>>>> > be among the fastest. Running across various machines and
>>>>> > settings (GC, client/server), it seems to be between 5% and 15%
>>>>> > faster. This is a smaller difference than in Ivan's tests,
>>>>> > that didn't include lock and contention effects.
>>>>> >
>>>>> > I committed jsr166 version. We'll need to sync this up with
>>>>> > with openjdk tl someday, but might as well wait until
>>>>> > other updates for Spliterators/streams are ready to integrate.
>>>>> >
>>>>> > -Doug
>>>>> >
>>>>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>>>>
>>>>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>>>>> > ***************
>>>>> > *** 579,595 ****
>>>>> >           final ReentrantLock lock = this.lock;
>>>>> >           lock.lock();
>>>>> >           try {
>>>>> > -             // Copy while checking if already present.
>>>>> > -             // This wins in the most common case where it is not
>>>>> present
>>>>> >
>>>>> >               Object[] elements = getArray();
>>>>> >               int len = elements.length;
>>>>> > -             Object[] newElements = new Object[len + 1];
>>>>> >
>>>>> >               for (int i = 0; i < len; ++i) {
>>>>> >                   if (eq(e, elements[i]))
>>>>> > !                     return false; // exit, throwing away copy
>>>>> > !                 else
>>>>> > !                     newElements[i] = elements[i];
>>>>> >
>>>>> >               }
>>>>> >               newElements[len] = e;
>>>>> >               setArray(newElements);
>>>>> >               return true;
>>>>> > --- 579,591 ----
>>>>> >           final ReentrantLock lock = this.lock;
>>>>> >           lock.lock();
>>>>> >           try {
>>>>> >
>>>>> >               Object[] elements = getArray();
>>>>> >               int len = elements.length;
>>>>> >               for (int i = 0; i < len; ++i) {
>>>>> >                   if (eq(e, elements[i]))
>>>>> > !                     return false;
>>>>> >               }
>>>>> > +             Object[] newElements = Arrays.copyOf(elements, len +
>>>>> 1);
>>>>> >
>>>>> >               newElements[len] = e;
>>>>> >               setArray(newElements);
>>>>> >               return true;
>>>>> >
>>>>> >
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/68d61b39/attachment-0001.html>

From gregg at cytetech.com  Mon Apr 15 09:29:21 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 15 Apr 2013 08:29:21 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <5169BA31.7010903@briangoetz.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
Message-ID: <516C00B1.8040001@cytetech.com>

Atomic views of the changes in values is a different issue than the 
Happens-Before model creates/addresses.  Total program order is different from 
thread order.  If you expect total program order to have a specific behavior, 
and you use more than 1 thread, then you've created a situation that you need to 
manage explicitly.

It's like a traffic light.  The light must be honored for safe traffic flow 
guarantees.  But, if someone ignores the light, safe traffic flow is not 
necessarily denied, but must be negotiated.

Happens before is like the case of no traffic lights.

Gregg Wonderly

On 4/13/2013 3:04 PM, Brian Goetz wrote:
> Consider this similar-looking programing:
>
> Object lock = new Object();
> int shared = 0;
>
> Thread 1: synchronized (lock) { local = shared; }
> Thread 2: synchronized (lock) ( shared = 10; }
> Thread 3: synchronized (lock) { local = shared; }
>
> Is what you're saying "in either my racy program or Brian's non-racy program,
> the effect will be the same -- threads 1 and 3 will each see either 0 or 10, and
> can't predict which because they do not control the timing of thread scheduling
> and lock acquisition", and from there, going to "so why do we call one racy and
> not the other?"
>
> In a properly synchronized program, you only see changes to shared variables at
> synchronization points, whereas in a racy program, they happen whenever, and the
> order in which they appear to happen may be different from the perspective of
> different threads.
>
> Or, maybe you just don't like the happens-before model because its different
> from what you're used to?  HB seems no less well-defined and tractable than the
> other models you seem to like.  The rules are simple:
>
>   - Synchronization actions (lock acquire/release, volatile read/write) are
> totally ordered
>   - If two actions A and B are in the same thread, and A precedes B in the
> program order, then A happens-before B
>   - Writes of volatile variables happen-before subsequent reads of that same
> variable (we can say "subsequent" because of the first rule above)
>   - Releasing a lock happens-before subsequent acquisitions of that lock
>
> There are a few other rules (having to do with starting/joining with threads,
> finalizers, etc) but they rarely come into play.
>
> On 4/12/2013 2:55 PM, thurstonn wrote:
>> In thinking about whether code is thread-safe or not, one can attempt to
>> identify whether it 'contains a data-race'.  If not, you're good.  Else you
>> need to add an explicit happens-before relationship.
>>
>> Which begs the question: what exactly constitutes a 'data-race'?  And here
>> I'm interested in something a little more formal than the famed judicial
>> judgement of obscenity (I know it when I see it)
>>
>> If you do a web search, you unfortunately get quite a few divergent
>> definitions, many of which seem to be inconsistent.
>> IIRC, the official JMM defines a data-race as any two conflicting operations
>> from two or more threads on shared data (where at least one of the two
>> operations is a write).
>>
>> Brian Goetz (in his excellent  article
>> <http://www.ibm.com/developerworks/library/j-jtp03304/>  ) defines data-race
>> thusly:
>>
>> "A program is said to have a data race, and therefore not be a "properly
>> synchronized" program, when there is a variable that is read by more than
>> one thread, written by at least one thread, and the write and the reads are
>> not ordered by a happens-before relationship."
>>
>> But this would mark the following code as a data-race
>>
>> int shared = 0
>>
>> Thread 1                  Thread 2                 Thread 3
>> local = this.shared      this.shared = 10       local = this.shared
>>
>> This clearly meets his definition, yet I do not consider this a 'data-race'.
>>
>> I've always relied on traditional database concurrency control theory (I
>> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
>> best), which has a formal definition of 'serializability', viz. that any
>> transaction log is 'serializable', if and only if, its serialization graph
>> is acyclic.  Why can we not use this as the basis for a formal definition of
>> 'data-race' (excluding the notion of commit and abort of course):
>>
>> "A program is said to have a data-race, if any legal (as prescribed by the
>> MM) execution order produces a serialization graph that is *cyclic*"
>>
>> It has the advantage of a formal, mathematical model and although it is has
>> historically been confined to databases (and transactions), it seems
>> applicable to concurrent execution of any kind?
>>
>> Hoping that I don't get flamed.
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
>>
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From unmeshjoshi at gmail.com  Mon Apr 15 09:37:08 2013
From: unmeshjoshi at gmail.com (Unmesh Joshi)
Date: Mon, 15 Apr 2013 09:37:08 -0400
Subject: [concurrency-interest] Default values for maximum thread count in
	JVM web servers
Message-ID: <CAOk+zfdR6hroTWJu2kn7nGzcFjaqMgoVN1KBC37x6pcS4G+9eQ@mail.gmail.com>

Hi,

I was always curious to know about how the default max thread count in Java
web servers are decided. For almost all servers (Tomcat, Weblogic etc..)
the default value is always between 150 to 200.
Is there any specific reason for this number.?

Recently I read a blog explaining why the default max thread count was
changed from 25 to 250 in .NET
http://www.bluebytesoftware.com/blog/PermaLink,guid,ca22a5a8-a3c9-4ee8-9b41-667dbd7d2108.aspx


Is there any similar reason for default count of 150 to 200 in Java web
servers?

Thanks,
Unmesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/0b4f2a53/attachment.html>

From vitalyd at gmail.com  Mon Apr 15 09:41:30 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 15 Apr 2013 09:41:30 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365903721280-9413.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
Message-ID: <CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>

Yes this is racy; it may be benign but that depends on context.  If you
have reader/writer of shared memory with no explicit synch or HB, it's a
race by (informal) definition.
On Apr 13, 2013 9:51 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Before I answer fully, let me ask you about another variant of the program:
>
>
> Thread 1                     Thread 2
> this.shared = 10            local = this.shared
>
> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
> least
> in my reading of the (your) definition that I quoted in my OP, it wouldn't
> qualify as a data-race.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/40a5e122/attachment.html>

From mudit.f2004912 at gmail.com  Mon Apr 15 09:46:37 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Mon, 15 Apr 2013 15:46:37 +0200
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
Message-ID: <CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>

Another question,

Is data race a race even when it is OK to loose updates/writes and read
older stuff while working on shared memory?


On Mon, Apr 15, 2013 at 3:41 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Yes this is racy; it may be benign but that depends on context.  If you
> have reader/writer of shared memory with no explicit synch or HB, it's a
> race by (informal) definition.
> On Apr 13, 2013 9:51 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:
>
>> Before I answer fully, let me ask you about another variant of the
>> program:
>>
>>
>> Thread 1                     Thread 2
>> this.shared = 10            local = this.shared
>>
>> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
>> least
>> in my reading of the (your) definition that I quoted in my OP, it wouldn't
>> qualify as a data-race.
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/2f01f9c0/attachment.html>

From vitalyd at gmail.com  Mon Apr 15 10:13:10 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 15 Apr 2013 10:13:10 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
Message-ID: <CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>

The short of it is that a data race is a data race - it describes shared
memory access pattern.  Whether it yields correct execution is what
determines if it's benign or not, and that's circumstantial.  The classic
example is String.hashCode - it's (deliberately) data racy, but benign.
On Apr 15, 2013 9:46 AM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:

> Another question,
>
> Is data race a race even when it is OK to loose updates/writes and read
> older stuff while working on shared memory?
>
>
> On Mon, Apr 15, 2013 at 3:41 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Yes this is racy; it may be benign but that depends on context.  If you
>> have reader/writer of shared memory with no explicit synch or HB, it's a
>> race by (informal) definition.
>> On Apr 13, 2013 9:51 PM, "thurstonn" <thurston at nomagicsoftware.com>
>> wrote:
>>
>>> Before I answer fully, let me ask you about another variant of the
>>> program:
>>>
>>>
>>> Thread 1                     Thread 2
>>> this.shared = 10            local = this.shared
>>>
>>> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
>>> least
>>> in my reading of the (your) definition that I quoted in my OP, it
>>> wouldn't
>>> qualify as a data-race.
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/b7b6461a/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Apr 15 10:31:50 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Mon, 15 Apr 2013 15:31:50 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365792944576-9408.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
Message-ID: <516C0F56.7020000@oracle.com>

When "data race" means "broken logic", there must be a place where that 
logic is defined.

Literature on linearizability introduces a history of events (not the 
only place where it is done so). If a valid reordering of events 
produces a invalid history, you have a data race. But you need a 
definition of a valid history.

Your argument is that any reordering of instructions in your example is 
a valid history. But in order to state that, we would need to see the 
rest of history. The subsequent use of local will determine the validity 
of history.


Alex


On 12/04/2013 19:55, thurstonn wrote:
> In thinking about whether code is thread-safe or not, one can attempt to
> identify whether it 'contains a data-race'.  If not, you're good.  Else you
> need to add an explicit happens-before relationship.
>
> Which begs the question: what exactly constitutes a 'data-race'?  And here
> I'm interested in something a little more formal than the famed judicial
> judgement of obscenity (I know it when I see it)
>
> If you do a web search, you unfortunately get quite a few divergent
> definitions, many of which seem to be inconsistent.
> IIRC, the official JMM defines a data-race as any two conflicting operations
> from two or more threads on shared data (where at least one of the two
> operations is a write).
>
> Brian Goetz (in his excellent  article
> <http://www.ibm.com/developerworks/library/j-jtp03304/>  ) defines data-race
> thusly:
>
> "A program is said to have a data race, and therefore not be a "properly
> synchronized" program, when there is a variable that is read by more than
> one thread, written by at least one thread, and the write and the reads are
> not ordered by a happens-before relationship."
>
> But this would mark the following code as a data-race
>
> int shared = 0
>
> Thread 1                  Thread 2                 Thread 3
> local = this.shared      this.shared = 10       local = this.shared
>
> This clearly meets his definition, yet I do not consider this a 'data-race'.
>
> I've always relied on traditional database concurrency control theory (I
> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
> best), which has a formal definition of 'serializability', viz. that any
> transaction log is 'serializable', if and only if, its serialization graph
> is acyclic.  Why can we not use this as the basis for a formal definition of
> 'data-race' (excluding the notion of commit and abort of course):
>
> "A program is said to have a data-race, if any legal (as prescribed by the
> MM) execution order produces a serialization graph that is *cyclic*"
>
> It has the advantage of a formal, mathematical model and although it is has
> historically been confined to databases (and transactions), it seems
> applicable to concurrent execution of any kind?
>
> Hoping that I don't get flamed.
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/bb70b483/attachment.html>

From oleksandr.otenko at oracle.com  Mon Apr 15 12:37:02 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Mon, 15 Apr 2013 17:37:02 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1365903721280-9413.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
Message-ID: <516C2CAE.1090407@oracle.com>

happens-before is sooo misunderstood!..

What do you mean - "no explicit happens-before"? happens-before doesn't 
create barriers "between threads"; volatile or not, locked or not, there 
is no order enforced between the first and the second thread. 
happens-before is a *reasoning mode* about visibility of effects from 
another thread.

In this example, you need to show:

this.shared was 0.

this.shared = 10               local = this.shared;

if (local == 10) "this.shared = 10" happens-before "local = this.shared"
else unknown


Alex


On 14/04/2013 02:42, thurstonn wrote:
> Before I answer fully, let me ask you about another variant of the program:
>
>
> Thread 1                     Thread 2
> this.shared = 10            local = this.shared
>
> Is this "racy"?  Clearly there is no explicit happens-before.  But, at least
> in my reading of the (your) definition that I quoted in my OP, it wouldn't
> qualify as a data-race.
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/a971b5f7/attachment.html>

From thurston at nomagicsoftware.com  Mon Apr 15 12:55:59 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 15 Apr 2013 09:55:59 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516C0F56.7020000@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
Message-ID: <1366044959679-9425.post@n7.nabble.com>

oleksandr otenko wrote
> When "data race" means "broken logic", there must be a place where that 
> logic is defined.

Yes, there is an assumption in my OP that "data-race" ==> "incorrect" (or
"broken logic" in your words)
Vitaly does not necessarily equate "raciness" with "incorrectness" (and
probably Brian as well) and that's OK with me

oleksandr otenko wrote
> Literature on linearizability introduces a history of events (not the 
> only place where it is done so). If a valid reordering of events 
> produces a invalid history, you have a data race. But you need a 
> definition of a valid history.

Yes, what I'm in (a quixotic?) search for is a process that does the
following:
given a set of operations that can execute across multiple threads (like the
example in the OP)
-define a set of "execution histories" (your "history of events") that are
possible given the MM in effect and consistent with the original program's
set of operations (of course there will be multiple such histories)
-each "execution history" defines at least a partial ordering among
conflicting operations (r/w or w/w on the same shared data item)
-analyze each execution history for "correctness"
if each possible history is correct, then you're good
else add explicit happens-before relations. Repeat


oleksandr otenko wrote
> Your argument is that any reordering of instructions in your example is 
> a valid history. But in order to state that, we would need to see the 
> rest of history. The subsequent use of local will determine the validity 
> of history.
> Alex

Agreed (the example 'program' is simplistic at best).

What my original post described was exactly the kind of process (viz. an
acyclic serialization graph) that I'm in search of, but is applied to
database concurrency control.  The problems are very similar (turning
concurrent executions into serial, partially-ordered ones; operations are
reads/writes of data items), but they are not exactly the same.  I was
wondering if we could use the same techniques (with a serialization graph
==> "execution graph") to analyze the "correctness" of, e.g. non-locking
concurrent algorithms/data structures.

Thurston



On 12/04/2013 19:55, thurstonn wrote:
> In thinking about whether code is thread-safe or not, one can attempt to
> identify whether it 'contains a data-race'.  If not, you're good.  Else
> you
> need to add an explicit happens-before relationship.
>
> Which begs the question: what exactly constitutes a 'data-race'?  And here
> I'm interested in something a little more formal than the famed judicial
> judgement of obscenity (I know it when I see it)
>
> If you do a web search, you unfortunately get quite a few divergent
> definitions, many of which seem to be inconsistent.
> IIRC, the official JMM defines a data-race as any two conflicting
> operations
> from two or more threads on shared data (where at least one of the two
> operations is a write).
>
> Brian Goetz (in his excellent  article
> &lt;http://www.ibm.com/developerworks/library/j-jtp03304/&gt;  ) defines
> data-race
> thusly:
>
> "A program is said to have a data race, and therefore not be a "properly
> synchronized" program, when there is a variable that is read by more than
> one thread, written by at least one thread, and the write and the reads
> are
> not ordered by a happens-before relationship."
>
> But this would mark the following code as a data-race
>
> int shared = 0
>
> Thread 1                  Thread 2                 Thread 3
> local = this.shared      this.shared = 10       local = this.shared
>
> This clearly meets his definition, yet I do not consider this a
> 'data-race'.
>
> I've always relied on traditional database concurrency control theory (I
> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
> best), which has a formal definition of 'serializability', viz. that any
> transaction log is 'serializable', if and only if, its serialization graph
> is acyclic.  Why can we not use this as the basis for a formal definition
> of
> 'data-race' (excluding the notion of commit and abort of course):
>
> "A program is said to have a data-race, if any legal (as prescribed by the
> MM) execution order produces a serialization graph that is *cyclic*"
>
> It has the advantage of a formal, mathematical model and although it is
> has
> historically been confined to databases (and transactions), it seems
> applicable to concurrent execution of any kind?
>
> Hoping that I don't get flamed.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at .oswego
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Mon Apr 15 13:07:37 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 15 Apr 2013 10:07:37 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516C2CAE.1090407@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<516C2CAE.1090407@oracle.com>
Message-ID: <1366045657906-9426.post@n7.nabble.com>

oleksandr otenko wrote
> happens-before is sooo misunderstood!..
> 
> What do you mean - "no explicit happens-before"? happens-before doesn't 
> create barriers "between threads"; volatile or not, locked or not, there 
> is no order enforced between the first and the second thread. 
> happens-before is a *reasoning mode* about visibility of effects from 
> another thread.
> 
> In this example, you need to show:
> 
> this.shared was 0.
> 
> this.shared = 10               local = this.shared;
> 
> if (local == 10) "this.shared = 10" happens-before "local = this.shared"
> else unknown
> 
> 
> Alex

I do understand happens-before, the difficulty is talking about it.
What I mean is that given the code (no explicit "happens-before"),

local == 0 (one possible outcome) tells you *nothing* about the partial
ordering of this.shared = 10, i.e. the write may or may not have 'happened'

now with an explicit "happens-before", e.g.
volatile int shared = 0
. . .

if local == 0, then there is a partial ordering defined between the writer
and reader threads (the reader < writer)

Perhaps "explicit happens-before" is grating to your eyes, but you shouldn't
presume that it means that I don't understand










On 14/04/2013 02:42, thurstonn wrote:
> Before I answer fully, let me ask you about another variant of the
> program:
>
>
> Thread 1                     Thread 2
> this.shared = 10            local = this.shared
>
> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
> least
> in my reading of the (your) definition that I quoted in my OP, it wouldn't
> qualify as a data-race.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at .oswego
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9426.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Mon Apr 15 13:44:58 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Mon, 15 Apr 2013 18:44:58 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366045657906-9426.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<516C2CAE.1090407@oracle.com>
	<1366045657906-9426.post@n7.nabble.com>
Message-ID: <516C3C9A.1000302@oracle.com>

My example wasn't even showing the actual thing, because unfortunately 
it is based on a trivial one-field access.

Happens-before is more powerful when considered as a thread-local 
property. It is pointless to consider just one write. What is important, 
is the ordering of all other reads and writes preceding the volatile 
store - in the writing thread. Then you can use the value of the last 
stored variable as evidence, when reasoning about the state transition 
that thread undertook.

So, back to your example - if this.shared is not volatile, a more 
powerful observation is that even if local == 10 there is no ordering 
with respect to other effects observed and applied in that other thread. 
And back to my point - when you declare it volatile, you are not 
creating a happens-before between the write and the read in another 
thread; you are creating a happens-before edge between all preceding 
reads and writes in the mutator thread and a store to this.shared.


With regards to "how to express validity". I pepper my code with 
asserts. But not everything can be expressed with simple tests of a few 
variables - sometimes it is necessary to assert a system-wide property.


Alex


On 15/04/2013 18:07, thurstonn wrote:
> oleksandr otenko wrote
>> happens-before is sooo misunderstood!..
>>
>> What do you mean - "no explicit happens-before"? happens-before doesn't
>> create barriers "between threads"; volatile or not, locked or not, there
>> is no order enforced between the first and the second thread.
>> happens-before is a *reasoning mode* about visibility of effects from
>> another thread.
>>
>> In this example, you need to show:
>>
>> this.shared was 0.
>>
>> this.shared = 10               local = this.shared;
>>
>> if (local == 10) "this.shared = 10" happens-before "local = this.shared"
>> else unknown
>>
>>
>> Alex
> I do understand happens-before, the difficulty is talking about it.
> What I mean is that given the code (no explicit "happens-before"),
>
> local == 0 (one possible outcome) tells you *nothing* about the partial
> ordering of this.shared = 10, i.e. the write may or may not have 'happened'
>
> now with an explicit "happens-before", e.g.
> volatile int shared = 0
> . . .
>
> if local == 0, then there is a partial ordering defined between the writer
> and reader threads (the reader < writer)
>
> Perhaps "explicit happens-before" is grating to your eyes, but you shouldn't
> presume that it means that I don't understand
>
>
>
>
>
>
>
>
>
>
> On 14/04/2013 02:42, thurstonn wrote:
>> Before I answer fully, let me ask you about another variant of the
>> program:
>>
>>
>> Thread 1                     Thread 2
>> this.shared = 10            local = this.shared
>>
>> Is this "racy"?  Clearly there is no explicit happens-before.  But, at
>> least
>> in my reading of the (your) definition that I quoted in my OP, it wouldn't
>> qualify as a data-race.
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9413.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9426.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/c91a83eb/attachment.html>

From nathan.reynolds at oracle.com  Mon Apr 15 14:33:47 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 15 Apr 2013 11:33:47 -0700
Subject: [concurrency-interest] Default values for maximum thread count
 in JVM web servers
In-Reply-To: <CAOk+zfdR6hroTWJu2kn7nGzcFjaqMgoVN1KBC37x6pcS4G+9eQ@mail.gmail.com>
References: <CAOk+zfdR6hroTWJu2kn7nGzcFjaqMgoVN1KBC37x6pcS4G+9eQ@mail.gmail.com>
Message-ID: <516C480B.2090401@oracle.com>

Weblogic tries to optimize the throughput by adjusting the number of 
threads.  If the DB connection pool is set to 150, then for DB 
connection constrained workloads, we find the number of threads to hover 
above that (say 175).

However, it really depends upon the work load.  Another work load we run 
on Weblogic maxes out the CPU.  The threads don't block very often.  So, 
the number of threads is usually just above the number of logical 
processors.  For example, on a 24 logical processor machine, the number 
of threads might be around 28.

The article talks about deadlocking due to insufficient threads to do 
all of the parallel processing.  As far as I know (which isn't much), 
Weblogic doesn't do parallel processing.  It does concurrent processing 
of different requests but doesn't use multiple threads to process each 
request... except in some minor cases.  So, having a lot of threads to 
overcome deadlocks isn't necessary.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/15/2013 6:37 AM, Unmesh Joshi wrote:
> Hi,
>
> I was always curious to know about how the default max thread count in 
> Java web servers are decided. For almost all servers (Tomcat, Weblogic 
> etc..) the default value is always between 150 to 200.
> Is there any specific reason for this number.?
>
> Recently I read a blog explaining why the default max thread count was 
> changed from 25 to 250 in .NET 
> http://www.bluebytesoftware.com/blog/PermaLink,guid,ca22a5a8-a3c9-4ee8-9b41-667dbd7d2108.aspx 
>
>
> Is there any similar reason for default count of 150 to 200 in Java 
> web servers?
>
> Thanks,
> Unmesh
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/120d6172/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Apr 15 15:18:29 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Mon, 15 Apr 2013 20:18:29 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366044959679-9425.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
Message-ID: <516C5285.90504@oracle.com>


On 15/04/2013 17:55, thurstonn wrote:
> oleksandr otenko wrote
>> When "data race" means "broken logic", there must be a place where that
>> logic is defined.
> Yes, there is an assumption in my OP that "data-race" ==> "incorrect" (or
> "broken logic" in your words)
> Vitaly does not necessarily equate "raciness" with "incorrectness" (and
> probably Brian as well) and that's OK with me
>
> oleksandr otenko wrote
>> Literature on linearizability introduces a history of events (not the
>> only place where it is done so). If a valid reordering of events
>> produces a invalid history, you have a data race. But you need a
>> definition of a valid history.
> Yes, what I'm in (a quixotic?) search for is a process that does the
> following:
> given a set of operations that can execute across multiple threads (like the
> example in the OP)
> -define a set of "execution histories" (your "history of events") that are
> possible given the MM in effect and consistent with the original program's
> set of operations (of course there will be multiple such histories)
> -each "execution history" defines at least a partial ordering among
> conflicting operations (r/w or w/w on the same shared data item)
> -analyze each execution history for "correctness"
> if each possible history is correct, then you're good
> else add explicit happens-before relations. Repeat
This sounds awfully like Java PathFinder.

Not tractable for less-trivial code. (I couldn't test a semaphore-like 
primitive with more than 4 threads).


Alex


> oleksandr otenko wrote
>> Your argument is that any reordering of instructions in your example is
>> a valid history. But in order to state that, we would need to see the
>> rest of history. The subsequent use of local will determine the validity
>> of history.
>> Alex
> Agreed (the example 'program' is simplistic at best).
>
> What my original post described was exactly the kind of process (viz. an
> acyclic serialization graph) that I'm in search of, but is applied to
> database concurrency control.  The problems are very similar (turning
> concurrent executions into serial, partially-ordered ones; operations are
> reads/writes of data items), but they are not exactly the same.  I was
> wondering if we could use the same techniques (with a serialization graph
> ==> "execution graph") to analyze the "correctness" of, e.g. non-locking
> concurrent algorithms/data structures.
>
> Thurston
>
>
>
> On 12/04/2013 19:55, thurstonn wrote:
>> In thinking about whether code is thread-safe or not, one can attempt to
>> identify whether it 'contains a data-race'.  If not, you're good.  Else
>> you
>> need to add an explicit happens-before relationship.
>>
>> Which begs the question: what exactly constitutes a 'data-race'?  And here
>> I'm interested in something a little more formal than the famed judicial
>> judgement of obscenity (I know it when I see it)
>>
>> If you do a web search, you unfortunately get quite a few divergent
>> definitions, many of which seem to be inconsistent.
>> IIRC, the official JMM defines a data-race as any two conflicting
>> operations
>> from two or more threads on shared data (where at least one of the two
>> operations is a write).
>>
>> Brian Goetz (in his excellent  article
>> &lt;http://www.ibm.com/developerworks/library/j-jtp03304/&gt;  ) defines
>> data-race
>> thusly:
>>
>> "A program is said to have a data race, and therefore not be a "properly
>> synchronized" program, when there is a variable that is read by more than
>> one thread, written by at least one thread, and the write and the reads
>> are
>> not ordered by a happens-before relationship."
>>
>> But this would mark the following code as a data-race
>>
>> int shared = 0
>>
>> Thread 1                  Thread 2                 Thread 3
>> local = this.shared      this.shared = 10       local = this.shared
>>
>> This clearly meets his definition, yet I do not consider this a
>> 'data-race'.
>>
>> I've always relied on traditional database concurrency control theory (I
>> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
>> best), which has a formal definition of 'serializability', viz. that any
>> transaction log is 'serializable', if and only if, its serialization graph
>> is acyclic.  Why can we not use this as the basis for a formal definition
>> of
>> 'data-race' (excluding the notion of commit and abort of course):
>>
>> "A program is said to have a data-race, if any legal (as prescribed by the
>> MM) execution order produces a serialization graph that is *cyclic*"
>>
>> It has the advantage of a formal, mathematical model and although it is
>> has
>> historically been confined to databases (and transactions), it seems
>> applicable to concurrent execution of any kind?
>>
>> Hoping that I don't get flamed.
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Mon Apr 15 16:51:56 2013
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 15 Apr 2013 13:51:56 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CAEJX8orLHR9-Scy+DRY17UEOo_zjboCc2m-bvAj+vPNBJY7nsw@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe09Rn9hDx4_7qKLreC0cwEbT2H0RoEJVCs1E8P6d2q4mhQ@mail.gmail.com>
	<515B41AA.3040703@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
	<CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>
	<CA+kOe080J5ixGFa7EPMM7Ls1iB5fUaUsmvnY6WEWskdRF24GuQ@mail.gmail.com>
	<CAEJX8orLHR9-Scy+DRY17UEOo_zjboCc2m-bvAj+vPNBJY7nsw@mail.gmail.com>
Message-ID: <CA+kOe0-HZk5NqdqQ5u1EV5wZZvqLdQLTJcWAgzuQ+8V8L5n1tA@mail.gmail.com>

This request seems not worth it to me.  I think it's not the common case to
have just the first element in hand when creating the collection.  If we
add this method to one collection class, we should add it to others.  I
think it will be difficult to add the new constructor without breaking
existing programs.  Low utility, low ubiquity.


On Sun, Apr 14, 2013 at 3:28 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> One more, perhaps it's a bit too late but adding a c-tor
> public CopyOnWriteArrayList(E firstElement){
>   setArray(new Object[]{e});
> }
> seems quite useful, alternatively change CopyOnWriteArrayList(E[]
> toCopyIn) to be varArgs.
>
> Virtually all dynamically created  CopyOnWriteArrayList/
> CopyOnWriteArraySet would have a single element added from get go.
> Generally I do not like sugar but the need to wrap the 1st element looks
> clunky.
>
> Stanimir
>
> On Fri, Apr 12, 2013 at 11:24 PM, Martin Buchholz <martinrb at google.com>wrote:
>
>> I persuaded Doug to accept my suggestions for remove(Object) and
>> addIfAbsent:
>>
>> --- src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
>> 18:44:19 -0000 1.103
>> +++ src/main/java/util/concurrent/CopyOnWriteArrayList.java 12 Apr 2013
>> 20:20:10 -0000
>> @@ -498,35 +498,44 @@
>>       * @return {@code true} if this list contained the specified element
>>       */
>>      public boolean remove(Object o) {
>> +        Object[] snapshot = getArray();
>> +        int index = indexOf(o, snapshot, 0, snapshot.length);
>> +        return (index < 0) ? false : remove(o, snapshot, index);
>> +    }
>> +
>> +    /**
>> +     * A version of remove(Object) using the strong hint that given
>> +     * recent snapshot contains o at the given index.
>> +     */
>> +    private boolean remove(Object o, Object[] snapshot, int index) {
>>           final ReentrantLock lock = this.lock;
>>          lock.lock();
>>          try {
>> -            Object[] elements = getArray();
>> -            int len = elements.length;
>>  -            if (len != 0) {
>> -                // Copy while searching for element to remove
>> -                // This wins in the normal case of element being present
>> -                int newlen = len - 1;
>> -                Object[] newElements = new Object[newlen];
>> -
>> -                for (int i = 0; i < newlen; ++i) {
>> -                    if (eq(o, elements[i])) {
>> -                        // found one;  copy remaining and exit
>> -                        for (int k = i + 1; k < len; ++k)
>> -                            newElements[k-1] = elements[k];
>> -                        setArray(newElements);
>> -                        return true;
>>  -                    } else
>> -                        newElements[i] = elements[i];
>> -                }
>> -
>> -                // special handling for last cell
>> -                if (eq(o, elements[newlen])) {
>> -                    setArray(newElements);
>> -                    return true;
>> +            Object[] current = getArray();
>> +            int len = current.length;
>> +            if (snapshot != current) findIndex: {
>> +                int prefix = Math.min(index, len);
>> +                for (int i = 0; i < prefix; i++) {
>> +                    if (current[i] != snapshot[i] && eq(o, current[i])) {
>>  +                        index = i;
>> +                        break findIndex;
>> +                    }
>>                  }
>> +                if (index >= len)
>> +                    return false;
>> +                if (current[index] == o)
>> +                    break findIndex;
>> +                index = indexOf(o, current, index, len);
>> +                if (index < 0)
>> +                    return false;
>>              }
>> -            return false;
>> +            Object[] newElements = new Object[len - 1];
>> +            System.arraycopy(current, 0, newElements, 0, index);
>> +            System.arraycopy(current, index + 1,
>> +                             newElements, index,
>> +                             len - index - 1);
>> +            setArray(newElements);
>> +            return true;
>>          } finally {
>>              lock.unlock();
>>          }
>> @@ -576,16 +585,31 @@
>>       * @return {@code true} if the element was added
>>       */
>>      public boolean addIfAbsent(E e) {
>> +        Object[] snapshot = getArray();
>> +        return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
>> +            addIfAbsent(e, snapshot);
>> +    }
>> +
>> +    /**
>> +     * A version of addIfAbsent using the strong hint that given
>> +     * recent snapshot does not contain e.
>> +     */
>> +    private boolean addIfAbsent(E e, Object[] snapshot) {
>>           final ReentrantLock lock = this.lock;
>>          lock.lock();
>>          try {
>> -            Object[] elements = getArray();
>> -            int len = elements.length;
>> -            for (int i = 0; i < len; ++i) {
>> -                if (eq(e, elements[i]))
>> -                    return false;
>> +            Object[] current = getArray();
>> +            int len = current.length;
>> +            if (snapshot != current) {
>> +                // Optimize for lost race to another addXXX operation
>> +                int common = Math.min(snapshot.length, len);
>> +                for (int i = 0; i < common; i++)
>> +                    if (current[i] != snapshot[i] && eq(e, current[i]))
>> +                        return false;
>> +                if (indexOf(e, current, common, len) >= 0)
>> +                        return false;
>>              }
>> -            Object[] newElements = Arrays.copyOf(elements, len + 1);
>> +            Object[] newElements = Arrays.copyOf(current, len + 1);
>>              newElements[len] = e;
>>              setArray(newElements);
>>              return true;
>>
>>
>>
>> On Mon, Apr 8, 2013 at 10:45 AM, Martin Buchholz <martinrb at google.com>wrote:
>>
>>> It would be nice if someone somewhere could confirm that addIfAbsent was
>>> frequently called with element present on their codebase.
>>>
>>> But even without any such guidance, I think it makes sense to optimize
>>> for operations not needing to update, for both addIfAbsent and
>>> remove(Object).  Operations that do are read only and don't acquire the
>>> lock have infinite scalability, while the penalty for having everything go
>>> wrong while having to do a second traversal is typically small, and at
>>> worst a factor of two.
>>>
>>> Another way to look at it is that COW data structures should be used in
>>> a read-mostly fashion.  So optimize assuming that operations like
>>> addIfAbsent that may or may not be read-only, are in fact read-only.
>>>
>>>
>>>
>>> On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff <
>>> stanimir at riflexo.com> wrote:
>>>
>>>> I was thinking exactly the same as Martin Buhholz however I could not
>>>> find any single use in the our code where it'd be profitable to check
>>>> outside the lock optimistically. addIfAbsent is fairly used in the form of
>>>> COWSet. i.e. adds at the end, random removals and some COWSets may contain
>>>> a few thousands entries. However it's virtually guaranteed addIfAbsent to
>>>> return true, while remove(E) may be racily called and return false, i.e. an
>>>> optimistic approach to remove() might be beneficial.
>>>>
>>>> On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>>
>>>>> Personally, I don't see a reason to further optimize for cache hits -
>>>>> are we really saying that's the common usage of COWAL? I'd find that hard
>>>>> to believe.  At some point, it's probably better to simply externalize this
>>>>> policy via constructor hint or subclass or whatever rather than catering to
>>>>> both sides in shared code.
>>>>>
>>>>> Having c-tor hint would require to propagate it would COWSet too. IMO,
>>>> addIfAbsent is mostly used by COWSet not COWList itself, as it doesn't
>>>> require the concrete type to be declared rather than the interface.
>>>> Also that will not affect the existing code bases and very few would
>>>> actually know the proper use case or the use cases would remain the same.
>>>> If the element is likely to already exists and the users are aware of the
>>>> current implementation it's likely to be guarded by contains(), such cases
>>>> require refactoring to take benefit and the code would require new java
>>>> version - unlikely to happen for large projects.
>>>> If there is higher contention and smaller array-sizes busy waiting w/
>>>> some backoff might be even better.
>>>>
>>>> Stanimir
>>>>
>>>>>  Sent from my phone
>>>>> On Apr 5, 2013 5:28 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>>>>
>>>>>> I'm still advocating an optimistic approach, that does not hold the
>>>>>> lock
>>>>>> during the first traversal of the array snapshot.  This is much
>>>>>> faster when
>>>>>> cache hits are the norm (or equals methods are expensive), which I
>>>>>> would
>>>>>> hope would be the common case, and only slightly slower when all adds
>>>>>> are
>>>>>> cache misses.
>>>>>>
>>>>>>      public boolean addIfAbsent(E e) {
>>>>>>         Object[] snapshot = getArray();
>>>>>>         int len = snapshot.length;
>>>>>>         for (int i = 0; i < len; i++)
>>>>>>             if (eq(e, snapshot[i]))
>>>>>>                 return false;
>>>>>>         return addIfAbsent(e, snapshot);
>>>>>>     }
>>>>>>
>>>>>>     private boolean addIfAbsent(E e, Object[] snapshot) {
>>>>>>         final ReentrantLock lock = this.lock;
>>>>>>         lock.lock();
>>>>>>         try {
>>>>>>             Object[] current = getArray();
>>>>>>             int len = current.length;
>>>>>>             if (snapshot != current) {
>>>>>>                 // Optimize for contending with another addIfAbsent
>>>>>>                 int common = Math.min(snapshot.length, len);
>>>>>>                 for (int i = 0; i < common; i++)
>>>>>>                     if (current[i] != snapshot[i] && eq(e,
>>>>>> current[i]))
>>>>>>                         return false;
>>>>>>                 for (int i = common; i < len; i++)
>>>>>>                     if (eq(e, current[i]))
>>>>>>                         return false;
>>>>>>             }
>>>>>>             Object[] newElements = Arrays.copyOf(current, len + 1);
>>>>>>             newElements[len] = e;
>>>>>>             setArray(newElements);
>>>>>>             return true;
>>>>>>         } finally {
>>>>>>             lock.unlock();
>>>>>>         }
>>>>>>     }
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>>>>
>>>>>> > On 04/03/13 06:35, Doug Lea wrote:
>>>>>> >
>>>>>> >> This was designed to perform best in the case of possibly contended
>>>>>> >> updates when the element is absent, by avoiding retraversal, and
>>>>>> >> thus minimizing lock hold times in the common case. (When not
>>>>>> common,
>>>>>> >> it can be guarded by a contains check.) However even in this case,
>>>>>> >> it is possible that a retraversal via arraycopy could be faster
>>>>>> >> because it can use optimized cheaper writes (fewer card marks).
>>>>>> >>
>>>>>> >
>>>>>> > Yes, by a little.
>>>>>> > A simple but reliable performance test is now at
>>>>>> >
>>>>>> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>>>>>> > COWALAddIfAbsentLoops.java?**view=log<
>>>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log
>>>>>> >
>>>>>>
>>>>>> >
>>>>>> > The simplest change allowing this (below) also appears to
>>>>>> > be among the fastest. Running across various machines and
>>>>>> > settings (GC, client/server), it seems to be between 5% and 15%
>>>>>> > faster. This is a smaller difference than in Ivan's tests,
>>>>>> > that didn't include lock and contention effects.
>>>>>> >
>>>>>> > I committed jsr166 version. We'll need to sync this up with
>>>>>> > with openjdk tl someday, but might as well wait until
>>>>>> > other updates for Spliterators/streams are ready to integrate.
>>>>>> >
>>>>>> > -Doug
>>>>>> >
>>>>>> > *** CopyOnWriteArrayList.java.~1.**100.~  Tue Mar 12 19:59:08 2013
>>>>>>
>>>>>> > --- CopyOnWriteArrayList.java   Fri Apr  5 08:03:29 2013
>>>>>> > ***************
>>>>>> > *** 579,595 ****
>>>>>> >           final ReentrantLock lock = this.lock;
>>>>>> >           lock.lock();
>>>>>> >           try {
>>>>>> > -             // Copy while checking if already present.
>>>>>> > -             // This wins in the most common case where it is not
>>>>>> present
>>>>>> >
>>>>>> >               Object[] elements = getArray();
>>>>>> >               int len = elements.length;
>>>>>> > -             Object[] newElements = new Object[len + 1];
>>>>>> >
>>>>>> >               for (int i = 0; i < len; ++i) {
>>>>>> >                   if (eq(e, elements[i]))
>>>>>> > !                     return false; // exit, throwing away copy
>>>>>> > !                 else
>>>>>> > !                     newElements[i] = elements[i];
>>>>>> >
>>>>>> >               }
>>>>>> >               newElements[len] = e;
>>>>>> >               setArray(newElements);
>>>>>> >               return true;
>>>>>> > --- 579,591 ----
>>>>>> >           final ReentrantLock lock = this.lock;
>>>>>> >           lock.lock();
>>>>>> >           try {
>>>>>> >
>>>>>> >               Object[] elements = getArray();
>>>>>> >               int len = elements.length;
>>>>>> >               for (int i = 0; i < len; ++i) {
>>>>>> >                   if (eq(e, elements[i]))
>>>>>> > !                     return false;
>>>>>> >               }
>>>>>> > +             Object[] newElements = Arrays.copyOf(elements, len +
>>>>>> 1);
>>>>>> >
>>>>>> >               newElements[len] = e;
>>>>>> >               setArray(newElements);
>>>>>> >               return true;
>>>>>> >
>>>>>> >
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/c815e528/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon Apr 15 17:35:15 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 15 Apr 2013 14:35:15 -0700
Subject: [concurrency-interest] RFR [8011215] optimization of
	CopyOnWriteArrayList.addIfAbsent()
In-Reply-To: <CA+kOe0-HZk5NqdqQ5u1EV5wZZvqLdQLTJcWAgzuQ+8V8L5n1tA@mail.gmail.com>
References: <515B2915.60402@oracle.com>
	<CA+kOe092K5+ubOCG_2EHYOCvoD6uxZUKUG2O9dp94MJ5Fb1Brw@mail.gmail.com>
	<515B499E.3090401@oracle.com> <515B4AE5.4060104@CoSoCo.de>
	<515B5F79.9090603@oracle.com>
	<CA+kOe09LU+sfL9Y1fUPn91e+VPMPYfXROZFuCc4r0ViLxrHPTA@mail.gmail.com>
	<CAEJX8or0+f2ct5AHQFYTxF4cP0aoJkJvk_wsHcUmxJGjWQzfdw@mail.gmail.com>
	<CAHjP37GbDZ8hJ6x0yUnCSCxmv4grya4OU+PtYXq6BxOmPT64=g@mail.gmail.com>
	<34160.38.123.136.254.1364985327.squirrel@altair.cs.oswego.edu>
	<515EC33C.7090604@cs.oswego.edu>
	<CA+kOe08_D8G1xvrCVX4rh7tOWDH7x9+5dc+=TcF6sprJDSE6OA@mail.gmail.com>
	<CAHjP37ELZ297i5YdFYNteNiCSTg4Gf2m9Ym8dwj8DoG8qn+3kg@mail.gmail.com>
	<CAEJX8oqDBUcMM630eyfGw8W-T6gbQH6M_DmHzFN88BYu9MO36g@mail.gmail.com>
	<CA+kOe0_zmSBBB-wKKwntTtmHcDKf0Jaj78C9g0K3D4RHH2pq9w@mail.gmail.com>
	<CA+kOe080J5ixGFa7EPMM7Ls1iB5fUaUsmvnY6WEWskdRF24GuQ@mail.gmail.com>
	<CAEJX8orLHR9-Scy+DRY17UEOo_zjboCc2m-bvAj+vPNBJY7nsw@mail.gmail.com>
	<CA+kOe0-HZk5NqdqQ5u1EV5wZZvqLdQLTJcWAgzuQ+8V8L5n1tA@mail.gmail.com>
Message-ID: <516C7293.2010706@oracle.com>

How is the compiler going to know which method to call?  How will the 
JVM know which method to call after type erasure?  Even if we solve 
those problems, it could lead to some code which is very difficult for 
humans to parse.

CopyOnWriteArrayList<Collection<X>> list;
Collection<X> value;

list = new CopyOnWriteArrayList<>(value);

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/15/2013 1:51 PM, Martin Buchholz wrote:
> This request seems not worth it to me.  I think it's not the common 
> case to have just the first element in hand when creating the 
> collection.  If we add this method to one collection class, we should 
> add it to others.  I think it will be difficult to add the new 
> constructor without breaking existing programs.  Low utility, low 
> ubiquity.
>
>
> On Sun, Apr 14, 2013 at 3:28 PM, Stanimir Simeonoff 
> <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>     One more, perhaps it's a bit too late but adding a c-tor
>     public CopyOnWriteArrayList(E firstElement){
>     setArray(new Object[]{e});
>     }
>     seems quite useful, alternatively change CopyOnWriteArrayList(E[]
>     toCopyIn) to be varArgs.
>
>     Virtually all dynamically created
>     CopyOnWriteArrayList/CopyOnWriteArraySetwould have a single
>     element added from get go. Generally I do not like sugar but the
>     need to wrap the 1st element looks clunky.
>
>     Stanimir
>
>     On Fri, Apr 12, 2013 at 11:24 PM, Martin Buchholz
>     <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>
>         I persuaded Doug to accept my suggestions for remove(Object)
>         and addIfAbsent:
>
>         --- src/main/java/util/concurrent/CopyOnWriteArrayList.java12
>         Apr 2013 18:44:19 -00001.103
>         +++ src/main/java/util/concurrent/CopyOnWriteArrayList.java12
>         Apr 2013 20:20:10 -0000
>         @@ -498,35 +498,44 @@
>               * @return {@code true} if this list contained the
>         specified element
>               */
>              public boolean remove(Object o) {
>         +        Object[] snapshot = getArray();
>         +        int index = indexOf(o, snapshot, 0, snapshot.length);
>         +        return (index < 0) ? false : remove(o, snapshot, index);
>         +    }
>         +
>         +    /**
>         +     * A version of remove(Object) using the strong hint that
>         given
>         +     * recent snapshot contains o at the given index.
>         +     */
>         +    private boolean remove(Object o, Object[] snapshot, int
>         index) {
>                  final ReentrantLock lock = this.lock;
>                  lock.lock();
>                  try {
>         -            Object[] elements = getArray();
>         -            int len = elements.length;
>         -            if (len != 0) {
>         -                // Copy while searching for element to remove
>         -                // This wins in the normal case of element
>         being present
>         -                int newlen = len - 1;
>         -                Object[] newElements = new Object[newlen];
>         -
>         -                for (int i = 0; i < newlen; ++i) {
>         -                    if (eq(o, elements[i])) {
>         -                        // found one;  copy remaining and exit
>         -                        for (int k = i + 1; k < len; ++k)
>         -  newElements[k-1] = elements[k];
>         -  setArray(newElements);
>         -                        return true;
>         -                    } else
>         -                        newElements[i] = elements[i];
>         -                }
>         -
>         -                // special handling for last cell
>         -                if (eq(o, elements[newlen])) {
>         -                    setArray(newElements);
>         -                    return true;
>         +            Object[] current = getArray();
>         +            int len = current.length;
>         +            if (snapshot != current) findIndex: {
>         +                int prefix = Math.min(index, len);
>         +                for (int i = 0; i < prefix; i++) {
>         +                    if (current[i] != snapshot[i] && eq(o,
>         current[i])) {
>         +                        index = i;
>         +                        break findIndex;
>         +                    }
>                          }
>         +                if (index >= len)
>         +                    return false;
>         +                if (current[index] == o)
>         +                    break findIndex;
>         +                index = indexOf(o, current, index, len);
>         +                if (index < 0)
>         +                    return false;
>                      }
>         -            return false;
>         +            Object[] newElements = new Object[len - 1];
>         +            System.arraycopy(current, 0, newElements, 0, index);
>         +            System.arraycopy(current, index + 1,
>         +                             newElements, index,
>         +                             len - index - 1);
>         +            setArray(newElements);
>         +            return true;
>                  } finally {
>                      lock.unlock();
>                  }
>         @@ -576,16 +585,31 @@
>               * @return {@code true} if the element was added
>               */
>              public boolean addIfAbsent(E e) {
>         +        Object[] snapshot = getArray();
>         +        return indexOf(e, snapshot, 0, snapshot.length) >= 0
>         ? false :
>         +            addIfAbsent(e, snapshot);
>         +    }
>         +
>         +    /**
>         +     * A version of addIfAbsent using the strong hint that given
>         +     * recent snapshot does not contain e.
>         +     */
>         +    private boolean addIfAbsent(E e, Object[] snapshot) {
>                  final ReentrantLock lock = this.lock;
>                  lock.lock();
>                  try {
>         -            Object[] elements = getArray();
>         -            int len = elements.length;
>         -            for (int i = 0; i < len; ++i) {
>         -                if (eq(e, elements[i]))
>         -                    return false;
>         +            Object[] current = getArray();
>         +            int len = current.length;
>         +            if (snapshot != current) {
>         +                // Optimize for lost race to another addXXX
>         operation
>         +                int common = Math.min(snapshot.length, len);
>         +                for (int i = 0; i < common; i++)
>         +                    if (current[i] != snapshot[i] && eq(e,
>         current[i]))
>         +                        return false;
>         +                if (indexOf(e, current, common, len) >= 0)
>         +                        return false;
>                      }
>         -            Object[] newElements = Arrays.copyOf(elements,
>         len + 1);
>         +            Object[] newElements = Arrays.copyOf(current, len
>         + 1);
>                      newElements[len] = e;
>                      setArray(newElements);
>                      return true;
>
>
>
>         On Mon, Apr 8, 2013 at 10:45 AM, Martin Buchholz
>         <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>
>             It would be nice if someone somewhere could confirm that
>             addIfAbsent was frequently called with element present on
>             their codebase.
>
>             But even without any such guidance, I think it makes sense
>             to optimize for operations not needing to update, for both
>             addIfAbsent and remove(Object).  Operations that do are
>             read only and don't acquire the lock have infinite
>             scalability, while the penalty for having everything go
>             wrong while having to do a second traversal is typically
>             small, and at worst a factor of two.
>
>             Another way to look at it is that COW data structures
>             should be used in a read-mostly fashion.  So optimize
>             assuming that operations like addIfAbsent that may or may
>             not be read-only, are in fact read-only.
>
>
>
>             On Fri, Apr 5, 2013 at 11:31 PM, Stanimir Simeonoff
>             <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>                 I was thinking exactly the same as Martin Buhholz
>                 however I could not find any single use in the our
>                 code where it'd be profitable to check outside the
>                 lock optimistically. addIfAbsent is fairly used in the
>                 form of COWSet. i.e. adds at the end, random removals
>                 and some COWSets may contain a few thousands entries.
>                 However it's virtually guaranteed addIfAbsent to
>                 return true, while remove(E) may be racily called and
>                 return false, i.e. an optimistic approach to remove()
>                 might be beneficial.
>
>                 On Sat, Apr 6, 2013 at 2:35 AM, Vitaly Davidovich
>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>                     Personally, I don't see a reason to further
>                     optimize for cache hits - are we really saying
>                     that's the common usage of COWAL? I'd find that
>                     hard to believe.  At some point, it's probably
>                     better to simply externalize this policy via
>                     constructor hint or subclass or whatever rather
>                     than catering to both sides in shared code. 
>
>                 Having c-tor hint would require to propagate it would
>                 COWSet too. IMO, addIfAbsent is mostly used by COWSet
>                 not COWList itself, as it doesn't require the concrete
>                 type to be declared rather than the interface.
>                 Also that will not affect the existing code bases and
>                 very few would actually know the proper use case or
>                 the use cases would remain the same. If the element is
>                 likely to already exists and the users are aware of
>                 the current implementation it's likely to be guarded
>                 by contains(), such cases require refactoring to take
>                 benefit and the code would require new java version -
>                 unlikely to happen for large projects.
>                 If there is higher contention and smaller array-sizes
>                 busy waiting w/ some backoff might be even better.
>
>                 Stanimir
>
>                     Sent from my phone
>
>                     On Apr 5, 2013 5:28 PM, "Martin Buchholz"
>                     <martinrb at google.com <mailto:martinrb at google.com>>
>                     wrote:
>
>                         I'm still advocating an optimistic approach,
>                         that does not hold the lock
>                         during the first traversal of the array
>                         snapshot.  This is much faster when
>                         cache hits are the norm (or equals methods are
>                         expensive), which I would
>                         hope would be the common case, and only
>                         slightly slower when all adds are
>                         cache misses.
>
>                              public boolean addIfAbsent(E e) {
>                                 Object[] snapshot = getArray();
>                                 int len = snapshot.length;
>                                 for (int i = 0; i < len; i++)
>                                     if (eq(e, snapshot[i]))
>                         return false;
>                                 return addIfAbsent(e, snapshot);
>                             }
>
>                             private boolean addIfAbsent(E e, Object[]
>                         snapshot) {
>                                 final ReentrantLock lock = this.lock;
>                         lock.lock();
>                                 try {
>                         Object[] current = getArray();
>                                     int len = current.length;
>                                     if (snapshot != current) {
>                         // Optimize for contending with another
>                         addIfAbsent
>                         int common = Math.min(snapshot.length, len);
>                         for (int i = 0; i < common; i++)
>                             if (current[i] != snapshot[i] && eq(e,
>                         current[i]))
>                                 return false;
>                         for (int i = common; i < len; i++)
>                             if (eq(e, current[i]))
>                                 return false;
>                                     }
>                         Object[] newElements = Arrays.copyOf(current,
>                         len + 1);
>                         newElements[len] = e;
>                         setArray(newElements);
>                         return true;
>                                 } finally {
>                         lock.unlock();
>                                 }
>                             }
>
>
>
>                         On Fri, Apr 5, 2013 at 5:27 AM, Doug Lea
>                         <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu>>
>                         wrote:
>
>                         > On 04/03/13 06:35, Doug Lea wrote:
>                         >
>                         >> This was designed to perform best in the
>                         case of possibly contended
>                         >> updates when the element is absent, by
>                         avoiding retraversal, and
>                         >> thus minimizing lock hold times in the
>                         common case. (When not common,
>                         >> it can be guarded by a contains check.)
>                         However even in this case,
>                         >> it is possible that a retraversal via
>                         arraycopy could be faster
>                         >> because it can use optimized cheaper writes
>                         (fewer card marks).
>                         >>
>                         >
>                         > Yes, by a little.
>                         > A simple but reliable performance test is now at
>                         >
>                         http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**test/loops/**
>                         >
>                         COWALAddIfAbsentLoops.java?**view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/test/loops/COWALAddIfAbsentLoops.java?view=log>
>
>
>                         >
>                         > The simplest change allowing this (below)
>                         also appears to
>                         > be among the fastest. Running across various
>                         machines and
>                         > settings (GC, client/server), it seems to be
>                         between 5% and 15%
>                         > faster. This is a smaller difference than in
>                         Ivan's tests,
>                         > that didn't include lock and contention effects.
>                         >
>                         > I committed jsr166 version. We'll need to
>                         sync this up with
>                         > with openjdk tl someday, but might as well
>                         wait until
>                         > other updates for Spliterators/streams are
>                         ready to integrate.
>                         >
>                         > -Doug
>                         >
>                         > *** CopyOnWriteArrayList.java.~1.**100.~
>                          Tue Mar 12 19:59:08 2013
>
>                         > --- CopyOnWriteArrayList.java   Fri Apr  5
>                         08:03:29 2013
>                         > ***************
>                         > *** 579,595 ****
>                         > final ReentrantLock lock = this.lock;
>                         > lock.lock();
>                         > try {
>                         > -   // Copy while checking if already present.
>                         > -   // This wins in the most common case
>                         where it is not present
>                         >
>                         >   Object[] elements = getArray();
>                         >   int len = elements.length;
>                         > -   Object[] newElements = new Object[len + 1];
>                         >
>                         >   for (int i = 0; i < len; ++i) {
>                         >       if (eq(e, elements[i]))
>                         > !           return false; // exit, throwing
>                         away copy
>                         > !       else
>                         > ! newElements[i] = elements[i];
>                         >
>                         >   }
>                         > newElements[len] = e;
>                         > setArray(newElements);
>                         >   return true;
>                         > --- 579,591 ----
>                         > final ReentrantLock lock = this.lock;
>                         > lock.lock();
>                         > try {
>                         >
>                         >   Object[] elements = getArray();
>                         >   int len = elements.length;
>                         >   for (int i = 0; i < len; ++i) {
>                         >       if (eq(e, elements[i]))
>                         > !           return false;
>                         >   }
>                         > +   Object[] newElements =
>                         Arrays.copyOf(elements, len + 1);
>                         >
>                         > newElements[len] = e;
>                         > setArray(newElements);
>                         >   return true;
>                         >
>                         >
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/6a4b63d3/attachment-0001.html>

From gregg at cytetech.com  Mon Apr 15 18:43:15 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 15 Apr 2013 17:43:15 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516C5285.90504@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com> <516C5285.90504@oracle.com>
Message-ID: <516C8283.9080307@cytetech.com>

Happens-before doesn't play a part in program=order, only thread-order.  This is 
the problem that a lot of developers seem to struggle with.  If you want the 
history of all possible paths, then the complexity of that possibility is huge. 
  Basically for X unrelated instructions, the possible histories are X^2. 
Because happens-before injects a "barrier", it can appear to simplify the 
complexity, but, the global ordering still is X^2 for all "blocks" between 
happens-before spots.

This is why I keep waving my hands here, and saying "stop that".  It makes it 
extremely difficult for "racy" software to be understood as "racy", because 
"reordering" introduces a whole realm of "unknown compiler reorderings changing 
JLS specified behaviors" that many developers, may not recognize as needing 
their attention.  I still encounter developers who just don't understand that 
the JLS doesn't describe "inter-thread" behaviors, because of the JMM.  People 
expect that they will just see data races, and not programmatic run time data 
corruption such as the Thread.getThread()/.setThread() discussion here revealed.

Many people pick java for the some reason that James Gosling wrote quite some 
time ago, regarding it working correctly.  The JLS and JMM decoupling of 
"behaviors" causes software to break with "corrupted data" (word tearing 
included) and that's something which I don't think is on the radar for as many 
people writing Java software, as seems to be expected by the aggressive demands 
on the JMM being perfectly understood and adhered to.

Gregg

On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>
> On 15/04/2013 17:55, thurstonn wrote:
>> oleksandr otenko wrote
>>> When "data race" means "broken logic", there must be a place where that
>>> logic is defined.
>> Yes, there is an assumption in my OP that "data-race" ==> "incorrect" (or
>> "broken logic" in your words)
>> Vitaly does not necessarily equate "raciness" with "incorrectness" (and
>> probably Brian as well) and that's OK with me
>>
>> oleksandr otenko wrote
>>> Literature on linearizability introduces a history of events (not the
>>> only place where it is done so). If a valid reordering of events
>>> produces a invalid history, you have a data race. But you need a
>>> definition of a valid history.
>> Yes, what I'm in (a quixotic?) search for is a process that does the
>> following:
>> given a set of operations that can execute across multiple threads (like the
>> example in the OP)
>> -define a set of "execution histories" (your "history of events") that are
>> possible given the MM in effect and consistent with the original program's
>> set of operations (of course there will be multiple such histories)
>> -each "execution history" defines at least a partial ordering among
>> conflicting operations (r/w or w/w on the same shared data item)
>> -analyze each execution history for "correctness"
>> if each possible history is correct, then you're good
>> else add explicit happens-before relations. Repeat
> This sounds awfully like Java PathFinder.
>
> Not tractable for less-trivial code. (I couldn't test a semaphore-like primitive
> with more than 4 threads).
>
>
> Alex
>
>
>> oleksandr otenko wrote
>>> Your argument is that any reordering of instructions in your example is
>>> a valid history. But in order to state that, we would need to see the
>>> rest of history. The subsequent use of local will determine the validity
>>> of history.
>>> Alex
>> Agreed (the example 'program' is simplistic at best).
>>
>> What my original post described was exactly the kind of process (viz. an
>> acyclic serialization graph) that I'm in search of, but is applied to
>> database concurrency control.  The problems are very similar (turning
>> concurrent executions into serial, partially-ordered ones; operations are
>> reads/writes of data items), but they are not exactly the same.  I was
>> wondering if we could use the same techniques (with a serialization graph
>> ==> "execution graph") to analyze the "correctness" of, e.g. non-locking
>> concurrent algorithms/data structures.
>>
>> Thurston
>>
>>
>>
>> On 12/04/2013 19:55, thurstonn wrote:
>>> In thinking about whether code is thread-safe or not, one can attempt to
>>> identify whether it 'contains a data-race'.  If not, you're good.  Else
>>> you
>>> need to add an explicit happens-before relationship.
>>>
>>> Which begs the question: what exactly constitutes a 'data-race'?  And here
>>> I'm interested in something a little more formal than the famed judicial
>>> judgement of obscenity (I know it when I see it)
>>>
>>> If you do a web search, you unfortunately get quite a few divergent
>>> definitions, many of which seem to be inconsistent.
>>> IIRC, the official JMM defines a data-race as any two conflicting
>>> operations
>>> from two or more threads on shared data (where at least one of the two
>>> operations is a write).
>>>
>>> Brian Goetz (in his excellent  article
>>> &lt;http://www.ibm.com/developerworks/library/j-jtp03304/&gt;  ) defines
>>> data-race
>>> thusly:
>>>
>>> "A program is said to have a data race, and therefore not be a "properly
>>> synchronized" program, when there is a variable that is read by more than
>>> one thread, written by at least one thread, and the write and the reads
>>> are
>>> not ordered by a happens-before relationship."
>>>
>>> But this would mark the following code as a data-race
>>>
>>> int shared = 0
>>>
>>> Thread 1                  Thread 2                 Thread 3
>>> local = this.shared      this.shared = 10       local = this.shared
>>>
>>> This clearly meets his definition, yet I do not consider this a
>>> 'data-race'.
>>>
>>> I've always relied on traditional database concurrency control theory (I
>>> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
>>> best), which has a formal definition of 'serializability', viz. that any
>>> transaction log is 'serializable', if and only if, its serialization graph
>>> is acyclic.  Why can we not use this as the basis for a formal definition
>>> of
>>> 'data-race' (excluding the notion of commit and abort of course):
>>>
>>> "A program is said to have a data-race, if any legal (as prescribed by the
>>> MM) execution order produces a serialization graph that is *cyclic*"
>>>
>>> It has the advantage of a formal, mathematical model and although it is
>>> has
>>> historically been confined to databases (and transactions), it seems
>>> applicable to concurrent execution of any kind?
>>>
>>> Hoping that I don't get flamed.
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html
>>>
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at .oswego
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html
>>
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From vitalyd at gmail.com  Mon Apr 15 20:12:17 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 15 Apr 2013 20:12:17 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516C8283.9080307@cytetech.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com> <516C5285.90504@oracle.com>
	<516C8283.9080307@cytetech.com>
Message-ID: <CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>

Yes, threading is hard :) There's nothing stopping people from using
different designs to minimize the hairy parts (e.g. message passing,
immutability, etc) in java.  Deliberate data races are an "expert"-only
tool, and last resort at that.  I think this list typically reiterates that
point when the subject comes up.
On Apr 15, 2013 6:46 PM, "Gregg Wonderly" <gregg at cytetech.com> wrote:

> Happens-before doesn't play a part in program=order, only thread-order.
>  This is the problem that a lot of developers seem to struggle with.  If
> you want the history of all possible paths, then the complexity of that
> possibility is huge.  Basically for X unrelated instructions, the possible
> histories are X^2. Because happens-before injects a "barrier", it can
> appear to simplify the complexity, but, the global ordering still is X^2
> for all "blocks" between happens-before spots.
>
> This is why I keep waving my hands here, and saying "stop that".  It makes
> it extremely difficult for "racy" software to be understood as "racy",
> because "reordering" introduces a whole realm of "unknown compiler
> reorderings changing JLS specified behaviors" that many developers, may not
> recognize as needing their attention.  I still encounter developers who
> just don't understand that the JLS doesn't describe "inter-thread"
> behaviors, because of the JMM.  People expect that they will just see data
> races, and not programmatic run time data corruption such as the
> Thread.getThread()/.setThread(**) discussion here revealed.
>
> Many people pick java for the some reason that James Gosling wrote quite
> some time ago, regarding it working correctly.  The JLS and JMM decoupling
> of "behaviors" causes software to break with "corrupted data" (word tearing
> included) and that's something which I don't think is on the radar for as
> many people writing Java software, as seems to be expected by the
> aggressive demands on the JMM being perfectly understood and adhered to.
>
> Gregg
>
> On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>
>>
>> On 15/04/2013 17:55, thurstonn wrote:
>>
>>> oleksandr otenko wrote
>>>
>>>> When "data race" means "broken logic", there must be a place where that
>>>> logic is defined.
>>>>
>>> Yes, there is an assumption in my OP that "data-race" ==> "incorrect" (or
>>> "broken logic" in your words)
>>> Vitaly does not necessarily equate "raciness" with "incorrectness" (and
>>> probably Brian as well) and that's OK with me
>>>
>>> oleksandr otenko wrote
>>>
>>>> Literature on linearizability introduces a history of events (not the
>>>> only place where it is done so). If a valid reordering of events
>>>> produces a invalid history, you have a data race. But you need a
>>>> definition of a valid history.
>>>>
>>> Yes, what I'm in (a quixotic?) search for is a process that does the
>>> following:
>>> given a set of operations that can execute across multiple threads (like
>>> the
>>> example in the OP)
>>> -define a set of "execution histories" (your "history of events") that
>>> are
>>> possible given the MM in effect and consistent with the original
>>> program's
>>> set of operations (of course there will be multiple such histories)
>>> -each "execution history" defines at least a partial ordering among
>>> conflicting operations (r/w or w/w on the same shared data item)
>>> -analyze each execution history for "correctness"
>>> if each possible history is correct, then you're good
>>> else add explicit happens-before relations. Repeat
>>>
>> This sounds awfully like Java PathFinder.
>>
>> Not tractable for less-trivial code. (I couldn't test a semaphore-like
>> primitive
>> with more than 4 threads).
>>
>>
>> Alex
>>
>>
>>  oleksandr otenko wrote
>>>
>>>> Your argument is that any reordering of instructions in your example is
>>>> a valid history. But in order to state that, we would need to see the
>>>> rest of history. The subsequent use of local will determine the validity
>>>> of history.
>>>> Alex
>>>>
>>> Agreed (the example 'program' is simplistic at best).
>>>
>>> What my original post described was exactly the kind of process (viz. an
>>> acyclic serialization graph) that I'm in search of, but is applied to
>>> database concurrency control.  The problems are very similar (turning
>>> concurrent executions into serial, partially-ordered ones; operations are
>>> reads/writes of data items), but they are not exactly the same.  I was
>>> wondering if we could use the same techniques (with a serialization graph
>>> ==> "execution graph") to analyze the "correctness" of, e.g. non-locking
>>> concurrent algorithms/data structures.
>>>
>>> Thurston
>>>
>>>
>>>
>>> On 12/04/2013 19:55, thurstonn wrote:
>>>
>>>> In thinking about whether code is thread-safe or not, one can attempt to
>>>> identify whether it 'contains a data-race'.  If not, you're good.  Else
>>>> you
>>>> need to add an explicit happens-before relationship.
>>>>
>>>> Which begs the question: what exactly constitutes a 'data-race'?  And
>>>> here
>>>> I'm interested in something a little more formal than the famed judicial
>>>> judgement of obscenity (I know it when I see it)
>>>>
>>>> If you do a web search, you unfortunately get quite a few divergent
>>>> definitions, many of which seem to be inconsistent.
>>>> IIRC, the official JMM defines a data-race as any two conflicting
>>>> operations
>>>> from two or more threads on shared data (where at least one of the two
>>>> operations is a write).
>>>>
>>>> Brian Goetz (in his excellent  article
>>>> &lt;http://www.ibm.com/**developerworks/library/j-**jtp03304/&gt<http://www.ibm.com/developerworks/library/j-jtp03304/&gt>;
>>>>  ) defines
>>>> data-race
>>>> thusly:
>>>>
>>>> "A program is said to have a data race, and therefore not be a "properly
>>>> synchronized" program, when there is a variable that is read by more
>>>> than
>>>> one thread, written by at least one thread, and the write and the reads
>>>> are
>>>> not ordered by a happens-before relationship."
>>>>
>>>> But this would mark the following code as a data-race
>>>>
>>>> int shared = 0
>>>>
>>>> Thread 1                  Thread 2                 Thread 3
>>>> local = this.shared      this.shared = 10       local = this.shared
>>>>
>>>> This clearly meets his definition, yet I do not consider this a
>>>> 'data-race'.
>>>>
>>>> I've always relied on traditional database concurrency control theory (I
>>>> still find the treatise by Bernstein, Hadzilacos, and Goodman to be the
>>>> best), which has a formal definition of 'serializability', viz. that any
>>>> transaction log is 'serializable', if and only if, its serialization
>>>> graph
>>>> is acyclic.  Why can we not use this as the basis for a formal
>>>> definition
>>>> of
>>>> 'data-race' (excluding the notion of commit and abort of course):
>>>>
>>>> "A program is said to have a data-race, if any legal (as prescribed by
>>>> the
>>>> MM) execution order produces a serialization graph that is *cyclic*"
>>>>
>>>> It has the advantage of a formal, mathematical model and although it is
>>>> has
>>>> historically been confined to databases (and transactions), it seems
>>>> applicable to concurrent execution of any kind?
>>>>
>>>> Hoping that I don't get flamed.
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://jsr166-concurrency.**10961.n7.nabble.com/On-A-**
>>>> Formal-Definition-of-Data-**Race-tp9408.html<http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html>
>>>>
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> ______________________________**_________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at .oswego
>>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at .oswego
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.**10961.n7.nabble.com/On-A-**
>>> Formal-Definition-of-Data-**Race-tp9408p9425.html<http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html>
>>>
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130415/3434f6a1/attachment-0001.html>

From thurston at nomagicsoftware.com  Tue Apr 16 08:01:54 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 05:01:54 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
Message-ID: <1366113714952-9434.post@n7.nabble.com>

Yes, concurrency is hard.
So is database concurrency control. But there is a formal methodology for
analyzing it (even if it is NP-complete)

It seems to me that the lack of something similar for analyzing
multi-threaded code on SMP systems is a real failure of computer science.  I
mean we have a MM.
Even if you accept the "leave it to the experts" prescription, the point is
that "experts" make mistakes as well (there's a great academic paper (that I
can't find the link to at the moment) that describes some putatively
thread-safe program that ran continuously for 2+ years before it failed)

The "how do you know this program is thread-safe"?
Pause.
"I thought *really* hard about it"

I can't be the only one who finds that deeply unsatisfying






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9434.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From heinz at javaspecialists.eu  Tue Apr 16 08:54:45 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 16 Apr 2013 15:54:45 +0300
Subject: [concurrency-interest] Latency in starting threads
Message-ID: <516D4A15.2090001@javaspecialists.eu>

Good day my fellow concurrency enthusiasts!

Yesterday, whilst teaching my Concurrency Specialist Course, I wanted to 
demonstrate to my class how slow it was starting threads and how much 
better it is to use a FixedThreadPool.  The question that I wanted to 
answer was: How many microseconds does it take on average to start a 
simple thread and what is the maximum time it could take?

We all know that it can take in the milliseconds range to do the following:

Thread t = new Thread(); // even without it actually doing anything
t.start();

This is one of the reasons why the fixed thread pool only starts the 
threads as we submit jobs to it, since the up-front cost might not be 
worth the wait.

But how long do you think the *maximum* was that I had to wait for 
t.start() to return?  100ms?  200ms?

Actually, the longest I had to wait turned out to be about 250 seconds.  
Yes.  That is *seconds*, not *milliseconds*.  Just to start a single thread.

This is most certainly a bug in the OpenJDK on Mac OS X.  We did not see 
this behaviour on Linux nor on Windows 7.

The bug started in OpenJDK 1.7.0_06.  Prior to that it hardly ever took 
longer than 30ms to start a single thread.

java version "1.7.0_05"
heinz$ java ThreadLeakMac2
time = 1, threads = 4
time = 2, threads = 346
time = 4, threads = 7378
time = 7, threads = 9614
time = 12, threads = 10027
time = 14, threads = 10063
time = 17, threads = 26965
time = 38, threads = 27013
time = 39, threads = 452053

java version "1.7.0_06"
heinz$ java ThreadLeakMac2
time = 1, threads = 6
time = 2, threads = 256
time = 6, threads = 373
*snip*
time = 111, threads = 42592
time = 200, threads = 49419
time = 333, threads = 58976
*snip*
time = 3245, threads = 202336
time = 3706, threads = 203702
*snip*
time = 5835, threads = 267872
time = 6455, threads = 269238
time = 9170, threads = 270603

In my code, I make sure that the thread has stopped before creating the 
next one by calling join().

public class ThreadLeakMac2 {
    public static void main(String[] args) throws InterruptedException {
        long threads = 0;
        long max = 0;
        while(true) {
            long time = System.currentTimeMillis();
            Thread thread = new Thread();
            thread.start(); // should finish almost immediately
            time = System.currentTimeMillis() - time;
            thread.join(); // short delay, hopefully
            threads++;
            if (time > max) {
                max = time;
                System.out.println("time = " + time + ", threads = " + 
threads);
            }
        }
    }
}

This would be another nice test case for Alexey's concurrency stress 
test harness.

(I will also post this to the macosx-port-dev list.)

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


From aleksey.shipilev at oracle.com  Tue Apr 16 09:06:53 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 16 Apr 2013 17:06:53 +0400
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516D4A15.2090001@javaspecialists.eu>
References: <516D4A15.2090001@javaspecialists.eu>
Message-ID: <516D4CED.3030201@oracle.com>

On 04/16/2013 04:54 PM, Dr Heinz M. Kabutz wrote:
> The question that I wanted to
> answer was: How many microseconds does it take on average to start a
> simple thread and what is the maximum time it could take?

We even have the related sample for this, as if this is a major
benchmarking pain point in many concurrent benchmarks:

http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-samples/src/main/java/org/openjdk/jmh/samples/JMHSample_07_FixtureLevelInvocation.java

> java version "1.7.0_06"
> heinz$ java ThreadLeakMac2
> time = 1, threads = 6
> time = 2, threads = 256
> time = 6, threads = 373
> *snip*
> time = 111, threads = 42592
> time = 200, threads = 49419
> time = 333, threads = 58976
> *snip*
> time = 3245, threads = 202336
> time = 3706, threads = 203702
> *snip*
> time = 5835, threads = 267872
> time = 6455, threads = 269238
> time = 9170, threads = 270603

This looks suspiciously as if some internal data structure holding the
threads (not cleaned up properly), and we traverse it linearly. I wonder
if that is at OS scheduler side, since Linux is not affected.

-Aleksey.

From oleksandr.otenko at oracle.com  Tue Apr 16 09:21:44 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 14:21:44 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516C8283.9080307@cytetech.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
Message-ID: <516D5068.3020206@oracle.com>

On 15/04/2013 23:43, Gregg Wonderly wrote:
> Happens-before doesn't play a part in program=order, only thread-order. 
This is exactly what I am pointing out.

However, the poset of happens-before has a structure isomorphic to a 
system of propositions with implications. So you can reason about what 
happened-before in another thread by observing atomically obtainable values.

The difficulty with Happens-Before is that the developers don't see the 
propositions following from the system of barriers, and that the 
reasoning system is not what they are used to (and/or/not is understood 
better than implication, because the former is practiced all the time).

Alex

> This is the problem that a lot of developers seem to struggle with.  
> If you want the history of all possible paths, then the complexity of 
> that possibility is huge.  Basically for X unrelated instructions, the 
> possible histories are X^2. Because happens-before injects a 
> "barrier", it can appear to simplify the complexity, but, the global 
> ordering still is X^2 for all "blocks" between happens-before spots.
>
> This is why I keep waving my hands here, and saying "stop that". It 
> makes it extremely difficult for "racy" software to be understood as 
> "racy", because "reordering" introduces a whole realm of "unknown 
> compiler reorderings changing JLS specified behaviors" that many 
> developers, may not recognize as needing their attention.  I still 
> encounter developers who just don't understand that the JLS doesn't 
> describe "inter-thread" behaviors, because of the JMM.  People expect 
> that they will just see data races, and not programmatic run time data 
> corruption such as the Thread.getThread()/.setThread() discussion here 
> revealed.
>
> Many people pick java for the some reason that James Gosling wrote 
> quite some time ago, regarding it working correctly.  The JLS and JMM 
> decoupling of "behaviors" causes software to break with "corrupted 
> data" (word tearing included) and that's something which I don't think 
> is on the radar for as many people writing Java software, as seems to 
> be expected by the aggressive demands on the JMM being perfectly 
> understood and adhered to.
>
> Gregg
>
> On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>>
>> On 15/04/2013 17:55, thurstonn wrote:
>>> oleksandr otenko wrote
>>>> When "data race" means "broken logic", there must be a place where 
>>>> that
>>>> logic is defined.
>>> Yes, there is an assumption in my OP that "data-race" ==> 
>>> "incorrect" (or
>>> "broken logic" in your words)
>>> Vitaly does not necessarily equate "raciness" with "incorrectness" (and
>>> probably Brian as well) and that's OK with me
>>>
>>> oleksandr otenko wrote
>>>> Literature on linearizability introduces a history of events (not the
>>>> only place where it is done so). If a valid reordering of events
>>>> produces a invalid history, you have a data race. But you need a
>>>> definition of a valid history.
>>> Yes, what I'm in (a quixotic?) search for is a process that does the
>>> following:
>>> given a set of operations that can execute across multiple threads 
>>> (like the
>>> example in the OP)
>>> -define a set of "execution histories" (your "history of events") 
>>> that are
>>> possible given the MM in effect and consistent with the original 
>>> program's
>>> set of operations (of course there will be multiple such histories)
>>> -each "execution history" defines at least a partial ordering among
>>> conflicting operations (r/w or w/w on the same shared data item)
>>> -analyze each execution history for "correctness"
>>> if each possible history is correct, then you're good
>>> else add explicit happens-before relations. Repeat
>> This sounds awfully like Java PathFinder.
>>
>> Not tractable for less-trivial code. (I couldn't test a 
>> semaphore-like primitive
>> with more than 4 threads).
>>
>>
>> Alex
>>
>>
>>> oleksandr otenko wrote
>>>> Your argument is that any reordering of instructions in your 
>>>> example is
>>>> a valid history. But in order to state that, we would need to see the
>>>> rest of history. The subsequent use of local will determine the 
>>>> validity
>>>> of history.
>>>> Alex
>>> Agreed (the example 'program' is simplistic at best).
>>>
>>> What my original post described was exactly the kind of process 
>>> (viz. an
>>> acyclic serialization graph) that I'm in search of, but is applied to
>>> database concurrency control.  The problems are very similar (turning
>>> concurrent executions into serial, partially-ordered ones; 
>>> operations are
>>> reads/writes of data items), but they are not exactly the same.  I was
>>> wondering if we could use the same techniques (with a serialization 
>>> graph
>>> ==> "execution graph") to analyze the "correctness" of, e.g. 
>>> non-locking
>>> concurrent algorithms/data structures.
>>>
>>> Thurston
>>>
>>>
>>>
>>> On 12/04/2013 19:55, thurstonn wrote:
>>>> In thinking about whether code is thread-safe or not, one can 
>>>> attempt to
>>>> identify whether it 'contains a data-race'.  If not, you're good.  
>>>> Else
>>>> you
>>>> need to add an explicit happens-before relationship.
>>>>
>>>> Which begs the question: what exactly constitutes a 'data-race'?  
>>>> And here
>>>> I'm interested in something a little more formal than the famed 
>>>> judicial
>>>> judgement of obscenity (I know it when I see it)
>>>>
>>>> If you do a web search, you unfortunately get quite a few divergent
>>>> definitions, many of which seem to be inconsistent.
>>>> IIRC, the official JMM defines a data-race as any two conflicting
>>>> operations
>>>> from two or more threads on shared data (where at least one of the two
>>>> operations is a write).
>>>>
>>>> Brian Goetz (in his excellent  article
>>>> &lt;http://www.ibm.com/developerworks/library/j-jtp03304/&gt; ) 
>>>> defines
>>>> data-race
>>>> thusly:
>>>>
>>>> "A program is said to have a data race, and therefore not be a 
>>>> "properly
>>>> synchronized" program, when there is a variable that is read by 
>>>> more than
>>>> one thread, written by at least one thread, and the write and the 
>>>> reads
>>>> are
>>>> not ordered by a happens-before relationship."
>>>>
>>>> But this would mark the following code as a data-race
>>>>
>>>> int shared = 0
>>>>
>>>> Thread 1                  Thread 2                 Thread 3
>>>> local = this.shared      this.shared = 10       local = this.shared
>>>>
>>>> This clearly meets his definition, yet I do not consider this a
>>>> 'data-race'.
>>>>
>>>> I've always relied on traditional database concurrency control 
>>>> theory (I
>>>> still find the treatise by Bernstein, Hadzilacos, and Goodman to be 
>>>> the
>>>> best), which has a formal definition of 'serializability', viz. 
>>>> that any
>>>> transaction log is 'serializable', if and only if, its 
>>>> serialization graph
>>>> is acyclic.  Why can we not use this as the basis for a formal 
>>>> definition
>>>> of
>>>> 'data-race' (excluding the notion of commit and abort of course):
>>>>
>>>> "A program is said to have a data-race, if any legal (as prescribed 
>>>> by the
>>>> MM) execution order produces a serialization graph that is *cyclic*"
>>>>
>>>> It has the advantage of a formal, mathematical model and although 
>>>> it is
>>>> has
>>>> historically been confined to databases (and transactions), it seems
>>>> applicable to concurrent execution of any kind?
>>>>
>>>> Hoping that I don't get flamed.
>>>>
>>>>
>>>>
>>>> -- 
>>>> View this message in context:
>>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html 
>>>>
>>>>
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at .oswego
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at .oswego
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>> -- 
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html 
>>>
>>>
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


From heinz at javaspecialists.eu  Tue Apr 16 09:30:00 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 16 Apr 2013 16:30:00 +0300
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516D4CED.3030201@oracle.com>
References: <516D4A15.2090001@javaspecialists.eu> <516D4CED.3030201@oracle.com>
Message-ID: <516D5258.5080204@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/fbdaba3b/attachment.html>

From oleksandr.otenko at oracle.com  Tue Apr 16 10:02:09 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 15:02:09 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366113714952-9434.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
Message-ID: <516D59E1.2030802@oracle.com>

Java PathFinder and SMV implement NP-complete validation of concurrent 
code. But like I mentioned before, the state space explodes, so it is 
not useful with more than N threads.

(Someone posted a blog jeering at "how many contenders do you need to 
prove the code is not thread-safe?" As I found out, it depends! If I 
have just 4 contenders, some conditions never happen - because there'd 
be not more than 4 distinct states at any given time)

The problem with SMV also is in mapping the specification into 
implementation. (even if you have a correct SMV model, what will it look 
like in Java?)


So NP-complete validation exists - computer science hasn't failed you 
there - but it is not what you want.


Alex


On 16/04/2013 13:01, thurstonn wrote:
> Yes, concurrency is hard.
> So is database concurrency control. But there is a formal methodology for
> analyzing it (even if it is NP-complete)
>
> It seems to me that the lack of something similar for analyzing
> multi-threaded code on SMP systems is a real failure of computer science.  I
> mean we have a MM.
> Even if you accept the "leave it to the experts" prescription, the point is
> that "experts" make mistakes as well (there's a great academic paper (that I
> can't find the link to at the moment) that describes some putatively
> thread-safe program that ran continuously for 2+ years before it failed)
>
> The "how do you know this program is thread-safe"?
> Pause.
> "I thought *really* hard about it"
>
> I can't be the only one who finds that deeply unsatisfying

From nathan.reynolds at oracle.com  Tue Apr 16 11:02:53 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 08:02:53 -0700
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516D4A15.2090001@javaspecialists.eu>
References: <516D4A15.2090001@javaspecialists.eu>
Message-ID: <516D681D.8040404@oracle.com>

When I started reading your email, I figured you meant the time from 
calling start() to the time the created thread actually started 
running.  I've seen this take a considerable amount of time on Windows 
XP (not 250 seconds though) and has to be considered if one is trying to 
account for latency. I was surprised to find that you meant the time for 
the constructor and start() call itself.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 5:54 AM, Dr Heinz M. Kabutz wrote:
> Good day my fellow concurrency enthusiasts!
>
> Yesterday, whilst teaching my Concurrency Specialist Course, I wanted 
> to demonstrate to my class how slow it was starting threads and how 
> much better it is to use a FixedThreadPool.  The question that I 
> wanted to answer was: How many microseconds does it take on average to 
> start a simple thread and what is the maximum time it could take?
>
> We all know that it can take in the milliseconds range to do the 
> following:
>
> Thread t = new Thread(); // even without it actually doing anything
> t.start();
>
> This is one of the reasons why the fixed thread pool only starts the 
> threads as we submit jobs to it, since the up-front cost might not be 
> worth the wait.
>
> But how long do you think the *maximum* was that I had to wait for 
> t.start() to return?  100ms?  200ms?
>
> Actually, the longest I had to wait turned out to be about 250 
> seconds.  Yes.  That is *seconds*, not *milliseconds*.  Just to start 
> a single thread.
>
> This is most certainly a bug in the OpenJDK on Mac OS X.  We did not 
> see this behaviour on Linux nor on Windows 7.
>
> The bug started in OpenJDK 1.7.0_06.  Prior to that it hardly ever 
> took longer than 30ms to start a single thread.
>
> java version "1.7.0_05"
> heinz$ java ThreadLeakMac2
> time = 1, threads = 4
> time = 2, threads = 346
> time = 4, threads = 7378
> time = 7, threads = 9614
> time = 12, threads = 10027
> time = 14, threads = 10063
> time = 17, threads = 26965
> time = 38, threads = 27013
> time = 39, threads = 452053
>
> java version "1.7.0_06"
> heinz$ java ThreadLeakMac2
> time = 1, threads = 6
> time = 2, threads = 256
> time = 6, threads = 373
> *snip*
> time = 111, threads = 42592
> time = 200, threads = 49419
> time = 333, threads = 58976
> *snip*
> time = 3245, threads = 202336
> time = 3706, threads = 203702
> *snip*
> time = 5835, threads = 267872
> time = 6455, threads = 269238
> time = 9170, threads = 270603
>
> In my code, I make sure that the thread has stopped before creating 
> the next one by calling join().
>
> public class ThreadLeakMac2 {
>    public static void main(String[] args) throws InterruptedException {
>        long threads = 0;
>        long max = 0;
>        while(true) {
>            long time = System.currentTimeMillis();
>            Thread thread = new Thread();
>            thread.start(); // should finish almost immediately
>            time = System.currentTimeMillis() - time;
>            thread.join(); // short delay, hopefully
>            threads++;
>            if (time > max) {
>                max = time;
>                System.out.println("time = " + time + ", threads = " + 
> threads);
>            }
>        }
>    }
> }
>
> This would be another nice test case for Alexey's concurrency stress 
> test harness.
>
> (I will also post this to the macosx-port-dev list.)
>
> Regards
>
> Heinz

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/9f4fc2d8/attachment.html>

From nathan.reynolds at oracle.com  Tue Apr 16 11:16:50 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 08:16:50 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366113714952-9434.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
Message-ID: <516D6B62.6080607@oracle.com>

 > the point is that "experts" make mistakes as well (there's a great 
academic paper (that I can't find the link to at the moment) that 
describes some putatively thread-safe program that ran continuously for 
2+ years before it failed)

One piece of code had a home-grown lock.  The code was 10 years old and 
was left alone.  So, it wasn't suffering from split-brain development.  
However, we would hear about the C++ server rarely crashing in 
production whenever doing an operation related to that lock.

I added lock hygiene to all of our lock implementations (i.e. double 
constructor calls, double deletes, use before constructor, use after 
destructor, destructor called while lock busy, recursive acquires, 
unexpected release, etc).  After doing so, I found a short-cut through 
the home-grown lock which would allow multiple threads to enter the 
critical region concurrently.

This isn't quite the "ran continuously for 2+ years" case, but 
concurrency issues are difficult to figure out.  I am not surprised that 
a program can run that long without crashing.  We never reproduced the 
issue, but the reports of the server crashing went away.  So, there are 
probably a lot more concurrency issues lurking about.  Fortunately with 
lock hygiene, we are more likely to catch them with enough information 
to fix them.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 5:01 AM, thurstonn wrote:
> Yes, concurrency is hard.
> So is database concurrency control. But there is a formal methodology for
> analyzing it (even if it is NP-complete)
>
> It seems to me that the lack of something similar for analyzing
> multi-threaded code on SMP systems is a real failure of computer science.  I
> mean we have a MM.
> Even if you accept the "leave it to the experts" prescription, the point is
> that "experts" make mistakes as well (there's a great academic paper (that I
> can't find the link to at the moment) that describes some putatively
> thread-safe program that ran continuously for 2+ years before it failed)
>
> The "how do you know this program is thread-safe"?
> Pause.
> "I thought *really* hard about it"
>
> I can't be the only one who finds that deeply unsatisfying
>
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9434.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/95313dfc/attachment.html>

From gregg at cytetech.com  Tue Apr 16 11:17:53 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 16 Apr 2013 10:17:53 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
Message-ID: <516D6BA1.5000506@cytetech.com>

The thing I don't believe is completely understood, is how much software was 
written prior to JDK1.5, which explicitly did not synchronized() on reads of 
"shared" data, because it made code faster.  The particular scenario that I've 
seen in various older code, is the "boolean" while loop control variable set to 
"true" by a "shutdown" method.  That happened all over the place, because it was 
so ugly to put a "synchronized" section around a whole loop.

boolean stopping;

public void shutdown() {
	stopping = true;
}


public void run() {
	while(!stopping) {
		// do your work
	}
}

As wrong as this is, it worked just fine prior to JDK1.5.   People didn't seem t 
want/see the need to, write a correct version of this, such as:

public synchronized void shutdown() {
	stopping = true;
}

public void run() {
	while( true ) {
		synchronized( this ) {
			if( stopping )
				break;
		}

		...
	}
}

It's that kind of old code "issue", which I think is leaving around all kinds of 
open cans of worms.  This was an optimization issue.  The compiler would do

	if( !stopping )
		return;
	while( true ) {
		...
	}

Visibility issues get "masked" over by people using "correctly" or "more 
correctly" synchronized code, which seems to trigger cache flushes or otherwise 
update cache lines to cause their "wrong" code to appear to be right.

Tools to look for these kinds of problems are starting to mature and be more and 
more visible and people get bit by these things.  But, overall, I'm just not 
convinced there is any hope that developers are really going to be able to deal 
with these issues in any dependable fashion.  Software stability is going to 
suffer, and that will keep up a barrier for some people in using Java when there 
are bugs visible to them but which have no explainable reason, given the 
"visible in source" order of execution.

We can get all excited about optimizations and speed of execution. But it 
doesn't matter how fast you do the wrong thing, it's still wrong.

Gregg Wonderly

On 4/15/2013 7:12 PM, Vitaly Davidovich wrote:
> Yes, threading is hard :) There's nothing stopping people from using different
> designs to minimize the hairy parts (e.g. message passing, immutability, etc) in
> java.  Deliberate data races are an "expert"-only tool, and last resort at
> that.  I think this list typically reiterates that point when the subject comes up.
>
> On Apr 15, 2013 6:46 PM, "Gregg Wonderly" <gregg at cytetech.com
> <mailto:gregg at cytetech.com>> wrote:
>
>     Happens-before doesn't play a part in program=order, only thread-order.
>       This is the problem that a lot of developers seem to struggle with.  If
>     you want the history of all possible paths, then the complexity of that
>     possibility is huge.  Basically for X unrelated instructions, the possible
>     histories are X^2. Because happens-before injects a "barrier", it can appear
>     to simplify the complexity, but, the global ordering still is X^2 for all
>     "blocks" between happens-before spots.
>
>     This is why I keep waving my hands here, and saying "stop that".  It makes
>     it extremely difficult for "racy" software to be understood as "racy",
>     because "reordering" introduces a whole realm of "unknown compiler
>     reorderings changing JLS specified behaviors" that many developers, may not
>     recognize as needing their attention.  I still encounter developers who just
>     don't understand that the JLS doesn't describe "inter-thread" behaviors,
>     because of the JMM.  People expect that they will just see data races, and
>     not programmatic run time data corruption such as the
>     Thread.getThread()/.setThread(__) discussion here revealed.
>
>     Many people pick java for the some reason that James Gosling wrote quite
>     some time ago, regarding it working correctly.  The JLS and JMM decoupling
>     of "behaviors" causes software to break with "corrupted data" (word tearing
>     included) and that's something which I don't think is on the radar for as
>     many people writing Java software, as seems to be expected by the aggressive
>     demands on the JMM being perfectly understood and adhered to.
>
>     Gregg
>
>     On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>
>
>         On 15/04/2013 17:55, thurstonn wrote:
>
>             oleksandr otenko wrote
>
>                 When "data race" means "broken logic", there must be a place
>                 where that
>                 logic is defined.
>
>             Yes, there is an assumption in my OP that "data-race" ==>
>             "incorrect" (or
>             "broken logic" in your words)
>             Vitaly does not necessarily equate "raciness" with "incorrectness" (and
>             probably Brian as well) and that's OK with me
>
>             oleksandr otenko wrote
>
>                 Literature on linearizability introduces a history of events
>                 (not the
>                 only place where it is done so). If a valid reordering of events
>                 produces a invalid history, you have a data race. But you need a
>                 definition of a valid history.
>
>             Yes, what I'm in (a quixotic?) search for is a process that does the
>             following:
>             given a set of operations that can execute across multiple threads
>             (like the
>             example in the OP)
>             -define a set of "execution histories" (your "history of events")
>             that are
>             possible given the MM in effect and consistent with the original
>             program's
>             set of operations (of course there will be multiple such histories)
>             -each "execution history" defines at least a partial ordering among
>             conflicting operations (r/w or w/w on the same shared data item)
>             -analyze each execution history for "correctness"
>             if each possible history is correct, then you're good
>             else add explicit happens-before relations. Repeat
>
>         This sounds awfully like Java PathFinder.
>
>         Not tractable for less-trivial code. (I couldn't test a semaphore-like
>         primitive
>         with more than 4 threads).
>
>
>         Alex
>
>
>             oleksandr otenko wrote
>
>                 Your argument is that any reordering of instructions in your
>                 example is
>                 a valid history. But in order to state that, we would need to
>                 see the
>                 rest of history. The subsequent use of local will determine the
>                 validity
>                 of history.
>                 Alex
>
>             Agreed (the example 'program' is simplistic at best).
>
>             What my original post described was exactly the kind of process (viz. an
>             acyclic serialization graph) that I'm in search of, but is applied to
>             database concurrency control.  The problems are very similar (turning
>             concurrent executions into serial, partially-ordered ones;
>             operations are
>             reads/writes of data items), but they are not exactly the same.  I was
>             wondering if we could use the same techniques (with a serialization
>             graph
>             ==> "execution graph") to analyze the "correctness" of, e.g. non-locking
>             concurrent algorithms/data structures.
>
>             Thurston
>
>
>
>             On 12/04/2013 19:55, thurstonn wrote:
>
>                 In thinking about whether code is thread-safe or not, one can
>                 attempt to
>                 identify whether it 'contains a data-race'.  If not, you're
>                 good.  Else
>                 you
>                 need to add an explicit happens-before relationship.
>
>                 Which begs the question: what exactly constitutes a 'data-race'?
>                   And here
>                 I'm interested in something a little more formal than the famed
>                 judicial
>                 judgement of obscenity (I know it when I see it)
>
>                 If you do a web search, you unfortunately get quite a few divergent
>                 definitions, many of which seem to be inconsistent.
>                 IIRC, the official JMM defines a data-race as any two conflicting
>                 operations
>                 from two or more threads on shared data (where at least one of
>                 the two
>                 operations is a write).
>
>                 Brian Goetz (in his excellent  article
>                 &lt;http://www.ibm.com/__developerworks/library/j-__jtp03304/&gt
>                 <http://www.ibm.com/developerworks/library/j-jtp03304/&gt>;  )
>                 defines
>                 data-race
>                 thusly:
>
>                 "A program is said to have a data race, and therefore not be a
>                 "properly
>                 synchronized" program, when there is a variable that is read by
>                 more than
>                 one thread, written by at least one thread, and the write and
>                 the reads
>                 are
>                 not ordered by a happens-before relationship."
>
>                 But this would mark the following code as a data-race
>
>                 int shared = 0
>
>                 Thread 1                  Thread 2                 Thread 3
>                 local = this.shared      this.shared = 10       local = this.shared
>
>                 This clearly meets his definition, yet I do not consider this a
>                 'data-race'.
>
>                 I've always relied on traditional database concurrency control
>                 theory (I
>                 still find the treatise by Bernstein, Hadzilacos, and Goodman to
>                 be the
>                 best), which has a formal definition of 'serializability', viz.
>                 that any
>                 transaction log is 'serializable', if and only if, its
>                 serialization graph
>                 is acyclic.  Why can we not use this as the basis for a formal
>                 definition
>                 of
>                 'data-race' (excluding the notion of commit and abort of course):
>
>                 "A program is said to have a data-race, if any legal (as
>                 prescribed by the
>                 MM) execution order produces a serialization graph that is *cyclic*"
>
>                 It has the advantage of a formal, mathematical model and
>                 although it is
>                 has
>                 historically been confined to databases (and transactions), it seems
>                 applicable to concurrent execution of any kind?
>
>                 Hoping that I don't get flamed.
>
>
>
>                 --
>                 View this message in context:
>                 http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408.html
>                 <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html>
>
>                 Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>                 _________________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at .oswego
>                 http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>                 <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>             _________________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at .oswego
>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>
>             --
>             View this message in context:
>             http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408p9425.html
>             <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html>
>
>             Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>             _________________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.__oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>         _________________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.__oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>


From oleksandr.otenko at oracle.com  Tue Apr 16 12:18:06 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 17:18:06 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516D6BA1.5000506@cytetech.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com>
Message-ID: <516D79BE.2060506@oracle.com>

The "pre-1.5" kind of code you quote is not correct in pre-1.5, and in 
other mainstream languages. So not clear who is unclear about 
incorrectness of that code and why you insist on supporting it.

It worked in pre-1.5 by accident. If you set stopping=true in another 
thread, it can stay in write buffers indefinitely because that's how 
modern CPUs work. There is nothing telling when it should be flushed. 
For example, suppose you have stopping=true; while(!stopped); waiting 
for a thread to report it stopped. That's the proof of incorrectness 
even in terms of pre-1.5.

Alex

On 16/04/2013 16:17, Gregg Wonderly wrote:
> The thing I don't believe is completely understood, is how much 
> software was written prior to JDK1.5, which explicitly did not 
> synchronized() on reads of "shared" data, because it made code 
> faster.  The particular scenario that I've seen in various older code, 
> is the "boolean" while loop control variable set to "true" by a 
> "shutdown" method.  That happened all over the place, because it was 
> so ugly to put a "synchronized" section around a whole loop.
>
> boolean stopping;
>
> public void shutdown() {
>     stopping = true;
> }
>
>
> public void run() {
>     while(!stopping) {
>         // do your work
>     }
> }
>
> As wrong as this is, it worked just fine prior to JDK1.5.   People 
> didn't seem t want/see the need to, write a correct version of this, 
> such as:
>
> public synchronized void shutdown() {
>     stopping = true;
> }
>
> public void run() {
>     while( true ) {
>         synchronized( this ) {
>             if( stopping )
>                 break;
>         }
>
>         ...
>     }
> }
>
> It's that kind of old code "issue", which I think is leaving around 
> all kinds of open cans of worms.  This was an optimization issue.  The 
> compiler would do
>
>     if( !stopping )
>         return;
>     while( true ) {
>         ...
>     }
>
> Visibility issues get "masked" over by people using "correctly" or 
> "more correctly" synchronized code, which seems to trigger cache 
> flushes or otherwise update cache lines to cause their "wrong" code to 
> appear to be right.
>
> Tools to look for these kinds of problems are starting to mature and 
> be more and more visible and people get bit by these things. But, 
> overall, I'm just not convinced there is any hope that developers are 
> really going to be able to deal with these issues in any dependable 
> fashion.  Software stability is going to suffer, and that will keep up 
> a barrier for some people in using Java when there are bugs visible to 
> them but which have no explainable reason, given the "visible in 
> source" order of execution.
>
> We can get all excited about optimizations and speed of execution. But 
> it doesn't matter how fast you do the wrong thing, it's still wrong.
>
> Gregg Wonderly
>
> On 4/15/2013 7:12 PM, Vitaly Davidovich wrote:
>> Yes, threading is hard :) There's nothing stopping people from using 
>> different
>> designs to minimize the hairy parts (e.g. message passing, 
>> immutability, etc) in
>> java.  Deliberate data races are an "expert"-only tool, and last 
>> resort at
>> that.  I think this list typically reiterates that point when the 
>> subject comes up.
>>
>> On Apr 15, 2013 6:46 PM, "Gregg Wonderly" <gregg at cytetech.com
>> <mailto:gregg at cytetech.com>> wrote:
>>
>>     Happens-before doesn't play a part in program=order, only 
>> thread-order.
>>       This is the problem that a lot of developers seem to struggle 
>> with.  If
>>     you want the history of all possible paths, then the complexity 
>> of that
>>     possibility is huge.  Basically for X unrelated instructions, the 
>> possible
>>     histories are X^2. Because happens-before injects a "barrier", it 
>> can appear
>>     to simplify the complexity, but, the global ordering still is X^2 
>> for all
>>     "blocks" between happens-before spots.
>>
>>     This is why I keep waving my hands here, and saying "stop that".  
>> It makes
>>     it extremely difficult for "racy" software to be understood as 
>> "racy",
>>     because "reordering" introduces a whole realm of "unknown compiler
>>     reorderings changing JLS specified behaviors" that many 
>> developers, may not
>>     recognize as needing their attention.  I still encounter 
>> developers who just
>>     don't understand that the JLS doesn't describe "inter-thread" 
>> behaviors,
>>     because of the JMM.  People expect that they will just see data 
>> races, and
>>     not programmatic run time data corruption such as the
>>     Thread.getThread()/.setThread(__) discussion here revealed.
>>
>>     Many people pick java for the some reason that James Gosling 
>> wrote quite
>>     some time ago, regarding it working correctly.  The JLS and JMM 
>> decoupling
>>     of "behaviors" causes software to break with "corrupted data" 
>> (word tearing
>>     included) and that's something which I don't think is on the 
>> radar for as
>>     many people writing Java software, as seems to be expected by the 
>> aggressive
>>     demands on the JMM being perfectly understood and adhered to.
>>
>>     Gregg
>>
>>     On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>>
>>
>>         On 15/04/2013 17:55, thurstonn wrote:
>>
>>             oleksandr otenko wrote
>>
>>                 When "data race" means "broken logic", there must be 
>> a place
>>                 where that
>>                 logic is defined.
>>
>>             Yes, there is an assumption in my OP that "data-race" ==>
>>             "incorrect" (or
>>             "broken logic" in your words)
>>             Vitaly does not necessarily equate "raciness" with 
>> "incorrectness" (and
>>             probably Brian as well) and that's OK with me
>>
>>             oleksandr otenko wrote
>>
>>                 Literature on linearizability introduces a history of 
>> events
>>                 (not the
>>                 only place where it is done so). If a valid 
>> reordering of events
>>                 produces a invalid history, you have a data race. But 
>> you need a
>>                 definition of a valid history.
>>
>>             Yes, what I'm in (a quixotic?) search for is a process 
>> that does the
>>             following:
>>             given a set of operations that can execute across 
>> multiple threads
>>             (like the
>>             example in the OP)
>>             -define a set of "execution histories" (your "history of 
>> events")
>>             that are
>>             possible given the MM in effect and consistent with the 
>> original
>>             program's
>>             set of operations (of course there will be multiple such 
>> histories)
>>             -each "execution history" defines at least a partial 
>> ordering among
>>             conflicting operations (r/w or w/w on the same shared 
>> data item)
>>             -analyze each execution history for "correctness"
>>             if each possible history is correct, then you're good
>>             else add explicit happens-before relations. Repeat
>>
>>         This sounds awfully like Java PathFinder.
>>
>>         Not tractable for less-trivial code. (I couldn't test a 
>> semaphore-like
>>         primitive
>>         with more than 4 threads).
>>
>>
>>         Alex
>>
>>
>>             oleksandr otenko wrote
>>
>>                 Your argument is that any reordering of instructions 
>> in your
>>                 example is
>>                 a valid history. But in order to state that, we would 
>> need to
>>                 see the
>>                 rest of history. The subsequent use of local will 
>> determine the
>>                 validity
>>                 of history.
>>                 Alex
>>
>>             Agreed (the example 'program' is simplistic at best).
>>
>>             What my original post described was exactly the kind of 
>> process (viz. an
>>             acyclic serialization graph) that I'm in search of, but 
>> is applied to
>>             database concurrency control.  The problems are very 
>> similar (turning
>>             concurrent executions into serial, partially-ordered ones;
>>             operations are
>>             reads/writes of data items), but they are not exactly the 
>> same.  I was
>>             wondering if we could use the same techniques (with a 
>> serialization
>>             graph
>>             ==> "execution graph") to analyze the "correctness" of, 
>> e.g. non-locking
>>             concurrent algorithms/data structures.
>>
>>             Thurston
>>
>>
>>
>>             On 12/04/2013 19:55, thurstonn wrote:
>>
>>                 In thinking about whether code is thread-safe or not, 
>> one can
>>                 attempt to
>>                 identify whether it 'contains a data-race'.  If not, 
>> you're
>>                 good.  Else
>>                 you
>>                 need to add an explicit happens-before relationship.
>>
>>                 Which begs the question: what exactly constitutes a 
>> 'data-race'?
>>                   And here
>>                 I'm interested in something a little more formal than 
>> the famed
>>                 judicial
>>                 judgement of obscenity (I know it when I see it)
>>
>>                 If you do a web search, you unfortunately get quite a 
>> few divergent
>>                 definitions, many of which seem to be inconsistent.
>>                 IIRC, the official JMM defines a data-race as any two 
>> conflicting
>>                 operations
>>                 from two or more threads on shared data (where at 
>> least one of
>>                 the two
>>                 operations is a write).
>>
>>                 Brian Goetz (in his excellent  article
>> &lt;http://www.ibm.com/__developerworks/library/j-__jtp03304/&gt
>> <http://www.ibm.com/developerworks/library/j-jtp03304/&gt>; )
>>                 defines
>>                 data-race
>>                 thusly:
>>
>>                 "A program is said to have a data race, and therefore 
>> not be a
>>                 "properly
>>                 synchronized" program, when there is a variable that 
>> is read by
>>                 more than
>>                 one thread, written by at least one thread, and the 
>> write and
>>                 the reads
>>                 are
>>                 not ordered by a happens-before relationship."
>>
>>                 But this would mark the following code as a data-race
>>
>>                 int shared = 0
>>
>>                 Thread 1                  Thread 2                 
>> Thread 3
>>                 local = this.shared      this.shared = 10 local = 
>> this.shared
>>
>>                 This clearly meets his definition, yet I do not 
>> consider this a
>>                 'data-race'.
>>
>>                 I've always relied on traditional database 
>> concurrency control
>>                 theory (I
>>                 still find the treatise by Bernstein, Hadzilacos, and 
>> Goodman to
>>                 be the
>>                 best), which has a formal definition of 
>> 'serializability', viz.
>>                 that any
>>                 transaction log is 'serializable', if and only if, its
>>                 serialization graph
>>                 is acyclic.  Why can we not use this as the basis for 
>> a formal
>>                 definition
>>                 of
>>                 'data-race' (excluding the notion of commit and abort 
>> of course):
>>
>>                 "A program is said to have a data-race, if any legal (as
>>                 prescribed by the
>>                 MM) execution order produces a serialization graph 
>> that is *cyclic*"
>>
>>                 It has the advantage of a formal, mathematical model and
>>                 although it is
>>                 has
>>                 historically been confined to databases (and 
>> transactions), it seems
>>                 applicable to concurrent execution of any kind?
>>
>>                 Hoping that I don't get flamed.
>>
>>
>>
>>                 --
>>                 View this message in context:
>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408.html
>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html>
>>
>>                 Sent from the JSR166 Concurrency mailing list archive 
>> at Nabble.com.
>> _________________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>             _________________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>>             --
>>             View this message in context:
>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408p9425.html
>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html>
>>
>>             Sent from the JSR166 Concurrency mailing list archive at 
>> Nabble.com.
>>             _________________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.__oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>         _________________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.__oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>     _________________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.__oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>


From nathan.reynolds at oracle.com  Tue Apr 16 12:58:56 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 09:58:56 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516D79BE.2060506@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
Message-ID: <516D8350.5010800@oracle.com>

Try running this code on JDK 1.4.2 on a processor with a weak memory 
model.  It will perform exactly as Alex has stated.  If you run it on 
x86, then it will behave as you want it to.

That is the beauty of JMM.  It provides a bit of sanity when trying to 
write code for different processors and different versions of JIT (i.e. 
different JVMs or even different versions of the same JVM).  It is 
really difficult to write code in C++ for 4 different processors and 4 
different compilers.  One of them is going to bite you if you try 
anything fancy.  Fortunately, there is some hope in sight for C++.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 9:18 AM, oleksandr otenko wrote:
> The "pre-1.5" kind of code you quote is not correct in pre-1.5, and in 
> other mainstream languages. So not clear who is unclear about 
> incorrectness of that code and why you insist on supporting it.
>
> It worked in pre-1.5 by accident. If you set stopping=true in another 
> thread, it can stay in write buffers indefinitely because that's how 
> modern CPUs work. There is nothing telling when it should be flushed. 
> For example, suppose you have stopping=true; while(!stopped); waiting 
> for a thread to report it stopped. That's the proof of incorrectness 
> even in terms of pre-1.5.
>
> Alex
>
> On 16/04/2013 16:17, Gregg Wonderly wrote:
>> The thing I don't believe is completely understood, is how much 
>> software was written prior to JDK1.5, which explicitly did not 
>> synchronized() on reads of "shared" data, because it made code 
>> faster.  The particular scenario that I've seen in various older 
>> code, is the "boolean" while loop control variable set to "true" by a 
>> "shutdown" method.  That happened all over the place, because it was 
>> so ugly to put a "synchronized" section around a whole loop.
>>
>> boolean stopping;
>>
>> public void shutdown() {
>>     stopping = true;
>> }
>>
>>
>> public void run() {
>>     while(!stopping) {
>>         // do your work
>>     }
>> }
>>
>> As wrong as this is, it worked just fine prior to JDK1.5. People 
>> didn't seem t want/see the need to, write a correct version of this, 
>> such as:
>>
>> public synchronized void shutdown() {
>>     stopping = true;
>> }
>>
>> public void run() {
>>     while( true ) {
>>         synchronized( this ) {
>>             if( stopping )
>>                 break;
>>         }
>>
>>         ...
>>     }
>> }
>>
>> It's that kind of old code "issue", which I think is leaving around 
>> all kinds of open cans of worms.  This was an optimization issue.  
>> The compiler would do
>>
>>     if( !stopping )
>>         return;
>>     while( true ) {
>>         ...
>>     }
>>
>> Visibility issues get "masked" over by people using "correctly" or 
>> "more correctly" synchronized code, which seems to trigger cache 
>> flushes or otherwise update cache lines to cause their "wrong" code 
>> to appear to be right.
>>
>> Tools to look for these kinds of problems are starting to mature and 
>> be more and more visible and people get bit by these things. But, 
>> overall, I'm just not convinced there is any hope that developers are 
>> really going to be able to deal with these issues in any dependable 
>> fashion.  Software stability is going to suffer, and that will keep 
>> up a barrier for some people in using Java when there are bugs 
>> visible to them but which have no explainable reason, given the 
>> "visible in source" order of execution.
>>
>> We can get all excited about optimizations and speed of execution. 
>> But it doesn't matter how fast you do the wrong thing, it's still wrong.
>>
>> Gregg Wonderly
>>
>> On 4/15/2013 7:12 PM, Vitaly Davidovich wrote:
>>> Yes, threading is hard :) There's nothing stopping people from using 
>>> different
>>> designs to minimize the hairy parts (e.g. message passing, 
>>> immutability, etc) in
>>> java.  Deliberate data races are an "expert"-only tool, and last 
>>> resort at
>>> that.  I think this list typically reiterates that point when the 
>>> subject comes up.
>>>
>>> On Apr 15, 2013 6:46 PM, "Gregg Wonderly" <gregg at cytetech.com
>>> <mailto:gregg at cytetech.com>> wrote:
>>>
>>>     Happens-before doesn't play a part in program=order, only 
>>> thread-order.
>>>       This is the problem that a lot of developers seem to struggle 
>>> with.  If
>>>     you want the history of all possible paths, then the complexity 
>>> of that
>>>     possibility is huge.  Basically for X unrelated instructions, 
>>> the possible
>>>     histories are X^2. Because happens-before injects a "barrier", 
>>> it can appear
>>>     to simplify the complexity, but, the global ordering still is 
>>> X^2 for all
>>>     "blocks" between happens-before spots.
>>>
>>>     This is why I keep waving my hands here, and saying "stop 
>>> that".  It makes
>>>     it extremely difficult for "racy" software to be understood as 
>>> "racy",
>>>     because "reordering" introduces a whole realm of "unknown compiler
>>>     reorderings changing JLS specified behaviors" that many 
>>> developers, may not
>>>     recognize as needing their attention.  I still encounter 
>>> developers who just
>>>     don't understand that the JLS doesn't describe "inter-thread" 
>>> behaviors,
>>>     because of the JMM.  People expect that they will just see data 
>>> races, and
>>>     not programmatic run time data corruption such as the
>>>     Thread.getThread()/.setThread(__) discussion here revealed.
>>>
>>>     Many people pick java for the some reason that James Gosling 
>>> wrote quite
>>>     some time ago, regarding it working correctly.  The JLS and JMM 
>>> decoupling
>>>     of "behaviors" causes software to break with "corrupted data" 
>>> (word tearing
>>>     included) and that's something which I don't think is on the 
>>> radar for as
>>>     many people writing Java software, as seems to be expected by 
>>> the aggressive
>>>     demands on the JMM being perfectly understood and adhered to.
>>>
>>>     Gregg
>>>
>>>     On 4/15/2013 2:18 PM, oleksandr otenko wrote:
>>>
>>>
>>>         On 15/04/2013 17:55, thurstonn wrote:
>>>
>>>             oleksandr otenko wrote
>>>
>>>                 When "data race" means "broken logic", there must be 
>>> a place
>>>                 where that
>>>                 logic is defined.
>>>
>>>             Yes, there is an assumption in my OP that "data-race" ==>
>>>             "incorrect" (or
>>>             "broken logic" in your words)
>>>             Vitaly does not necessarily equate "raciness" with 
>>> "incorrectness" (and
>>>             probably Brian as well) and that's OK with me
>>>
>>>             oleksandr otenko wrote
>>>
>>>                 Literature on linearizability introduces a history 
>>> of events
>>>                 (not the
>>>                 only place where it is done so). If a valid 
>>> reordering of events
>>>                 produces a invalid history, you have a data race. 
>>> But you need a
>>>                 definition of a valid history.
>>>
>>>             Yes, what I'm in (a quixotic?) search for is a process 
>>> that does the
>>>             following:
>>>             given a set of operations that can execute across 
>>> multiple threads
>>>             (like the
>>>             example in the OP)
>>>             -define a set of "execution histories" (your "history of 
>>> events")
>>>             that are
>>>             possible given the MM in effect and consistent with the 
>>> original
>>>             program's
>>>             set of operations (of course there will be multiple such 
>>> histories)
>>>             -each "execution history" defines at least a partial 
>>> ordering among
>>>             conflicting operations (r/w or w/w on the same shared 
>>> data item)
>>>             -analyze each execution history for "correctness"
>>>             if each possible history is correct, then you're good
>>>             else add explicit happens-before relations. Repeat
>>>
>>>         This sounds awfully like Java PathFinder.
>>>
>>>         Not tractable for less-trivial code. (I couldn't test a 
>>> semaphore-like
>>>         primitive
>>>         with more than 4 threads).
>>>
>>>
>>>         Alex
>>>
>>>
>>>             oleksandr otenko wrote
>>>
>>>                 Your argument is that any reordering of instructions 
>>> in your
>>>                 example is
>>>                 a valid history. But in order to state that, we 
>>> would need to
>>>                 see the
>>>                 rest of history. The subsequent use of local will 
>>> determine the
>>>                 validity
>>>                 of history.
>>>                 Alex
>>>
>>>             Agreed (the example 'program' is simplistic at best).
>>>
>>>             What my original post described was exactly the kind of 
>>> process (viz. an
>>>             acyclic serialization graph) that I'm in search of, but 
>>> is applied to
>>>             database concurrency control.  The problems are very 
>>> similar (turning
>>>             concurrent executions into serial, partially-ordered ones;
>>>             operations are
>>>             reads/writes of data items), but they are not exactly 
>>> the same.  I was
>>>             wondering if we could use the same techniques (with a 
>>> serialization
>>>             graph
>>>             ==> "execution graph") to analyze the "correctness" of, 
>>> e.g. non-locking
>>>             concurrent algorithms/data structures.
>>>
>>>             Thurston
>>>
>>>
>>>
>>>             On 12/04/2013 19:55, thurstonn wrote:
>>>
>>>                 In thinking about whether code is thread-safe or 
>>> not, one can
>>>                 attempt to
>>>                 identify whether it 'contains a data-race'. If not, 
>>> you're
>>>                 good.  Else
>>>                 you
>>>                 need to add an explicit happens-before relationship.
>>>
>>>                 Which begs the question: what exactly constitutes a 
>>> 'data-race'?
>>>                   And here
>>>                 I'm interested in something a little more formal 
>>> than the famed
>>>                 judicial
>>>                 judgement of obscenity (I know it when I see it)
>>>
>>>                 If you do a web search, you unfortunately get quite 
>>> a few divergent
>>>                 definitions, many of which seem to be inconsistent.
>>>                 IIRC, the official JMM defines a data-race as any 
>>> two conflicting
>>>                 operations
>>>                 from two or more threads on shared data (where at 
>>> least one of
>>>                 the two
>>>                 operations is a write).
>>>
>>>                 Brian Goetz (in his excellent  article
>>> &lt;http://www.ibm.com/__developerworks/library/j-__jtp03304/&gt
>>> <http://www.ibm.com/developerworks/library/j-jtp03304/&gt>; )
>>>                 defines
>>>                 data-race
>>>                 thusly:
>>>
>>>                 "A program is said to have a data race, and 
>>> therefore not be a
>>>                 "properly
>>>                 synchronized" program, when there is a variable that 
>>> is read by
>>>                 more than
>>>                 one thread, written by at least one thread, and the 
>>> write and
>>>                 the reads
>>>                 are
>>>                 not ordered by a happens-before relationship."
>>>
>>>                 But this would mark the following code as a data-race
>>>
>>>                 int shared = 0
>>>
>>>                 Thread 1                  Thread 2                 
>>> Thread 3
>>>                 local = this.shared      this.shared = 10 local = 
>>> this.shared
>>>
>>>                 This clearly meets his definition, yet I do not 
>>> consider this a
>>>                 'data-race'.
>>>
>>>                 I've always relied on traditional database 
>>> concurrency control
>>>                 theory (I
>>>                 still find the treatise by Bernstein, Hadzilacos, 
>>> and Goodman to
>>>                 be the
>>>                 best), which has a formal definition of 
>>> 'serializability', viz.
>>>                 that any
>>>                 transaction log is 'serializable', if and only if, its
>>>                 serialization graph
>>>                 is acyclic.  Why can we not use this as the basis 
>>> for a formal
>>>                 definition
>>>                 of
>>>                 'data-race' (excluding the notion of commit and 
>>> abort of course):
>>>
>>>                 "A program is said to have a data-race, if any legal 
>>> (as
>>>                 prescribed by the
>>>                 MM) execution order produces a serialization graph 
>>> that is *cyclic*"
>>>
>>>                 It has the advantage of a formal, mathematical model 
>>> and
>>>                 although it is
>>>                 has
>>>                 historically been confined to databases (and 
>>> transactions), it seems
>>>                 applicable to concurrent execution of any kind?
>>>
>>>                 Hoping that I don't get flamed.
>>>
>>>
>>>
>>>                 --
>>>                 View this message in context:
>>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408.html 
>>>
>>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html> 
>>>
>>>
>>>                 Sent from the JSR166 Concurrency mailing list 
>>> archive at Nabble.com.
>>> _________________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at .oswego
>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>             _________________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at .oswego
>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>>>             --
>>>             View this message in context:
>>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408p9425.html 
>>>
>>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html> 
>>>
>>>
>>>             Sent from the JSR166 Concurrency mailing list archive at 
>>> Nabble.com.
>>>             _________________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.__oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>         _________________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.__oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>     _________________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.__oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/98199771/attachment-0001.html>

From thurston at nomagicsoftware.com  Tue Apr 16 14:57:09 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 11:57:09 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
Message-ID: <1366138629783-9445.post@n7.nabble.com>

Just curious, how is String#hashCode() racy?
Strings are immutable in java; I looked at the code a bit and I didn't see
anything that looked racy.
The only thing I guess could be:
private char[] value


Although that array is never modified in the String class, so . . .



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9445.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Tue Apr 16 15:22:49 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 16 Apr 2013 15:22:49 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366138629783-9445.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
Message-ID: <CAHjP37H3KKBnUvWoM2aJPAOcNEk6-Eo-MbE_a93EimVDNf-p6Q@mail.gmail.com>

String caches the hashcode in a plain field; multiple threads can race to
compute and store the hashcode, but since string is immutable, this is
fine.  The code is careful to operate only on a local variable to avoid
re-reads of the field (which could theoretically lead to wrong results if,
e.g., JIT reordered code in a certain way).
On Apr 16, 2013 3:12 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Just curious, how is String#hashCode() racy?
> Strings are immutable in java; I looked at the code a bit and I didn't see
> anything that looked racy.
> The only thing I guess could be:
> private char[] value
>
>
> Although that array is never modified in the String class, so . . .
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9445.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/0ef226ab/attachment.html>

From heinz at javaspecialists.eu  Tue Apr 16 15:30:33 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 16 Apr 2013 22:30:33 +0300
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516D681D.8040404@oracle.com>
References: <516D4A15.2090001@javaspecialists.eu> <516D681D.8040404@oracle.com>
Message-ID: <516DA6D9.1040405@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/b395c1a7/attachment.html>

From oleksandr.otenko at oracle.com  Tue Apr 16 15:33:39 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 20:33:39 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366138629783-9445.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
Message-ID: <516DA793.4060606@oracle.com>

Technically, setting hash value is racy. It is the same value, but the 
writes race.

Alex

On 16/04/2013 19:57, thurstonn wrote:
> Just curious, how is String#hashCode() racy?
> Strings are immutable in java; I looked at the code a bit and I didn't see
> anything that looked racy.
> The only thing I guess could be:
> private char[] value
>
>
> Although that array is never modified in the String class, so . . .
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/d9639ecd/attachment.html>

From mike.duigou at oracle.com  Tue Apr 16 15:58:12 2013
From: mike.duigou at oracle.com (Mike Duigou)
Date: Tue, 16 Apr 2013 12:58:12 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DA793.4060606@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com>
Message-ID: <7A68AF86-A05E-4A33-9780-FE03F447B235@oracle.com>

It's been a practice for a while in OpenJDK maintenance to document benign races like String.hash initialization and class statics like Collections.r whenever they are noted.

Whenever you see an uncommented race condition in JDK code as part of a review please request that a comment be added.

If the race condition is surprising or non-obvious and important it's possible to create bug issue/patches just to document it. (Which has been done in at least one case I remember).

Mike

On Apr 16 2013, at 12:33 , oleksandr otenko wrote:

> Technically, setting hash value is racy. It is the same value, but the writes race.
> 
> Alex
> 
> On 16/04/2013 19:57, thurstonn wrote:
>> Just curious, how is String#hashCode() racy?
>> Strings are immutable in java; I looked at the code a bit and I didn't see
>> anything that looked racy.
>> The only thing I guess could be:
>> private char[] value
>> 
>> 
>> Although that array is never modified in the String class, so . . .
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/7c57b139/attachment.html>

From jeffhain at rocketmail.com  Tue Apr 16 16:16:14 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Tue, 16 Apr 2013 21:16:14 +0100 (BST)
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516D4A15.2090001@javaspecialists.eu>
References: <516D4A15.2090001@javaspecialists.eu>
Message-ID: <1366143374.52042.YahooMailNeo@web171705.mail.ir2.yahoo.com>

Hi

>long time = System.currentTimeMillis();


System.currentTimeMillis() might jump around

with system time (but I don't mean it's where
the 250s come from).


You could use use System.nanoTime() instead
when measuring delays (as done in j.u.c):
Drift, if any, should be slow (ok for small benches),

and if it jumps (might happen on some broken configurations)
it should more likely create either obviously crazy or tiny
time shifts.
At least I never experienced timing glitches with it
as I did with System.currentTimeMillis().



-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/4b03a475/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Apr 16 17:16:42 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 14:16:42 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DA793.4060606@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com>
Message-ID: <516DBFBA.7080104@oracle.com>

Let's frame this in terms of ACID. Properly-used locks provide the A, C 
and I of ACID.  A harmful data race can be viewed as a violation of 
atomicity, consistency or isolation.  Many data races can exist without 
violating A, C or I and hence be harmless.  These data races are great 
ways to improve scalability.  Idempotent operations seem to be easiest 
to make racy yet harmless.

Setting the String hash value is racy yet harmless.  It doesn't violate 
atomicity since the field changes without any intermediate values.  The 
field either has 0 or the proper value (assuming word tearing can't 
happen).  It doesn't violate consistency since the field can only be in 
two valid states (i.e. 0 or the proper value).  It doesn't violate 
isolation since "concurrent execution results in a state that would be 
obtained if executed serially" (Wikipedia).

Let's revisit the following example in this framework.

 > Thread 1                     Thread 2
 > this.shared = 10            local = this.shared

 > Is this "racy"?

Both operations on Thread 1 and 2 are atomic (assuming word tearing 
can't happen).

As far as consistency is concerned, that depends upon the value of 
"this.shared" before execution.  Is "this.shared" in a valid state to 
begin with (i.e. is is consistent)?  If so, then Thread 2 will end up in 
a consistent state.  If not, then it is a harmful data race.  For 
example, if an object's reference is published before the constructor 
finishes, then Thread 1 could be executing in the constructor and Thread 
2 is accessing the partially-constructed object.  The object's state is 
inconsistent (i.e. this.shared == 0 is an invalid state).

It has isolation.  If you execute Thread 1 and Thread 2 concurrently, 
you get the same result as if you had executed one first and the other 
second.  Of course, depending upon timing, the system state will end up 
in one of two states: local == 10 or local == /previous value/.  The 
system can't end up in a different third state.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 12:33 PM, oleksandr otenko wrote:
> Technically, setting hash value is racy. It is the same value, but the 
> writes race.
>
> Alex
>
> On 16/04/2013 19:57, thurstonn wrote:
>> Just curious, how is String#hashCode() racy?
>> Strings are immutable in java; I looked at the code a bit and I didn't see
>> anything that looked racy.
>> The only thing I guess could be:
>> private char[] value
>>
>>
>> Although that array is never modified in the String class, so . . .
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/47e9a16d/attachment.html>

From oleksandr.otenko at oracle.com  Tue Apr 16 17:30:38 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 22:30:38 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DBFBA.7080104@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com> <516DBFBA.7080104@oracle.com>
Message-ID: <516DC2FE.6020701@oracle.com>

Yes. I am only saying that technically it is a race, since the question 
was what a race is. But is it harmful? No. That's why early on in the 
discussion some people made a distinction between a race and broken logic.

Alex


On 16/04/2013 22:16, Nathan Reynolds wrote:
> Let's frame this in terms of ACID. Properly-used locks provide the A, 
> C and I of ACID.  A harmful data race can be viewed as a violation of 
> atomicity, consistency or isolation.  Many data races can exist 
> without violating A, C or I and hence be harmless.  These data races 
> are great ways to improve scalability.  Idempotent operations seem to 
> be easiest to make racy yet harmless.
>
> Setting the String hash value is racy yet harmless.  It doesn't 
> violate atomicity since the field changes without any intermediate 
> values.  The field either has 0 or the proper value (assuming word 
> tearing can't happen).  It doesn't violate consistency since the field 
> can only be in two valid states (i.e. 0 or the proper value).  It 
> doesn't violate isolation since "concurrent execution results in a 
> state that would be obtained if executed serially" (Wikipedia).
>
> Let's revisit the following example in this framework.
>
> > Thread 1                     Thread 2
> > this.shared = 10            local = this.shared
>
> > Is this "racy"?
>
> Both operations on Thread 1 and 2 are atomic (assuming word tearing 
> can't happen).
>
> As far as consistency is concerned, that depends upon the value of 
> "this.shared" before execution.  Is "this.shared" in a valid state to 
> begin with (i.e. is is consistent)?  If so, then Thread 2 will end up 
> in a consistent state.  If not, then it is a harmful data race.  For 
> example, if an object's reference is published before the constructor 
> finishes, then Thread 1 could be executing in the constructor and 
> Thread 2 is accessing the partially-constructed object.  The object's 
> state is inconsistent (i.e. this.shared == 0 is an invalid state).
>
> It has isolation.  If you execute Thread 1 and Thread 2 concurrently, 
> you get the same result as if you had executed one first and the other 
> second.  Of course, depending upon timing, the system state will end 
> up in one of two states: local == 10 or local == /previous value/.  
> The system can't end up in a different third state.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 4/16/2013 12:33 PM, oleksandr otenko wrote:
>> Technically, setting hash value is racy. It is the same value, but 
>> the writes race.
>>
>> Alex
>>
>> On 16/04/2013 19:57, thurstonn wrote:
>>> Just curious, how is String#hashCode() racy?
>>> Strings are immutable in java; I looked at the code a bit and I didn't see
>>> anything that looked racy.
>>> The only thing I guess could be:
>>> private char[] value
>>>
>>>
>>> Although that array is never modified in the String class, so . . .
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/ab77fc50/attachment.html>

From stanimir at riflexo.com  Tue Apr 16 17:37:10 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 17 Apr 2013 00:37:10 +0300
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <1366143374.52042.YahooMailNeo@web171705.mail.ir2.yahoo.com>
References: <516D4A15.2090001@javaspecialists.eu>
	<1366143374.52042.YahooMailNeo@web171705.mail.ir2.yahoo.com>
Message-ID: <CAEJX8oopjQuKS6BAr2Mj2r+vZrND==8R9Nv1p+fv2Lqy5KSV4Q@mail.gmail.com>

On Tue, Apr 16, 2013 at 11:16 PM, Jeff Hain <jeffhain at rocketmail.com> wrote
>
>
> System.currentTimeMillis() might jump around
> with system time
>
System.currentTimeMillis() works quite fine if you use NTP daemon. The NPT
slows down or speeds up the system clock frequency very slightly to adjust,
hence no jumps.


Stanimir



On Tue, Apr 16, 2013 at 11:16 PM, Jeff Hain <jeffhain at rocketmail.com> wrote:

> Hi
>
>
> >long time = System.currentTimeMillis();
>
>
> System.currentTimeMillis() might jump around
> with system time (but I don't mean it's where
> the 250s come from).
>
>
> You could use use System.nanoTime() instead
> when measuring delays (as done in j.u.c):
> Drift, if any, should be slow (ok for small benches),
> and if it jumps (might happen on some broken configurations)
> it should more likely create either obviously crazy or tiny
> time shifts.
> At least I never experienced timing glitches with it
> as I did with System.currentTimeMillis().
>
>
> -Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/115c91fb/attachment-0001.html>

From thurston at nomagicsoftware.com  Tue Apr 16 17:49:18 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 14:49:18 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516D8350.5010800@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
Message-ID: <1366148958661-9454.post@n7.nabble.com>

I'm wondering if the stopped-flag code were to run on a modern CPU (i.e. with
cache coherency), would there be any practical difference between declaring:

boolean stopped = false

vs.

volatile boolean stopped  = false

/Note: I'm not advocating the non-volatile, 'broken' version; of course you
should write to the JMM, that's its purpose. I really feel for engineers who
have to write for multiple CPUs/MMs; I don't know how they manage./


But given cache coherency, let's say you were to run the code a gazillion
times and could count the # of loops after setting this.stopped = true.
Should one expect some (even if negligible) difference between the two
versions?  I suppose it would depend on a lot of things (is #stopped cached,
false-sharing effects, etc), but due to cc, the run thread, even in the
non-volatile version,  should eventually see this.stopped == true -- hmm
unless the JIT were to do the kind of code 'optimization' that Greg showed
(hoisting this.stopped out of the loop), which I guess is allowed.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9454.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Tue Apr 16 17:58:49 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 14:58:49 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DBFBA.7080104@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com> <516DBFBA.7080104@oracle.com>
Message-ID: <1366149529007-9455.post@n7.nabble.com>

Nathan Reynolds-2 wrote
> Let's revisit the following example in this framework.
> 
>  > Thread 1                     Thread 2
>  > this.shared = 10            local = this.shared
> 
>  > Is this "racy"?
> 
> Both operations on Thread 1 and 2 are atomic (assuming word tearing 
> can't happen).
> 
> As far as consistency is concerned, that depends upon the value of 
> "this.shared" before execution.  Is "this.shared" in a valid state to 
> begin with (i.e. is is consistent)?  If so, then Thread 2 will end up in 
> a consistent state.  If not, then it is a harmful data race.  For 
> example, if an object's reference is published before the constructor 
> finishes, then Thread 1 could be executing in the constructor and Thread 
> 2 is accessing the partially-constructed object.  The object's state is 
> inconsistent (i.e. this.shared == 0 is an invalid state).
> 
> It has isolation.  If you execute Thread 1 and Thread 2 concurrently, 
> you get the same result as if you had executed one first and the other 
> second.  Of course, depending upon timing, the system state will end up 
> in one of two states: local == 10 or local == /previous value/.  The 
> system can't end up in a different third state.
> 
> Nathan Reynolds 

True.  But if you apply a serialization graph to the two "transactions"
(executions), the SG is acyclic ==> non serializable.  Normally you don't
consider the case where all of the transactions (threads) write the same
value to the shared data item
&lt;http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds&gt; | 
Architect | 602.333.9091
Oracle PSR Engineering &lt;http://psr.us.oracle.com/&gt; | Server Technology
On 4/16/2013 12:33 PM, oleksandr otenko wrote:
> Technically, setting hash value is racy. It is the same value, but the 
> writes race.
>
> Alex
>
> On 16/04/2013 19:57, thurstonn wrote:
>> Just curious, how is String#hashCode() racy?
>> Strings are immutable in java; I looked at the code a bit and I didn't
>> see
>> anything that looked racy.
>> The only thing I guess could be:
>> private char[] value
>>
>>
>> Although that array is never modified in the String class, so . . .
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at .oswego
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9455.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From nathan.reynolds at oracle.com  Tue Apr 16 18:01:52 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 15:01:52 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366148958661-9454.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
Message-ID: <516DCA50.9000702@oracle.com>

All things being equal, reading a volatile and non-volatile field from 
L1/2/3/4 cache/memory has no impact on performance.  The instructions 
are exactly the same (on x86).

Writing a volatile and non-volatile field to cache/memory has an impact 
on performance.  Writing to a volatile field requires a memory fence on 
x86 and many other processors.  This fence is going to take cycles.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 2:49 PM, thurstonn wrote:
> I'm wondering if the stopped-flag code were to run on a modern CPU (i.e. with
> cache coherency), would there be any practical difference between declaring:
>
> boolean stopped = false
>
> vs.
>
> volatile boolean stopped  = false
>
> /Note: I'm not advocating the non-volatile, 'broken' version; of course you
> should write to the JMM, that's its purpose. I really feel for engineers who
> have to write for multiple CPUs/MMs; I don't know how they manage./
>
>
> But given cache coherency, let's say you were to run the code a gazillion
> times and could count the # of loops after setting this.stopped = true.
> Should one expect some (even if negligible) difference between the two
> versions?  I suppose it would depend on a lot of things (is #stopped cached,
> false-sharing effects, etc), but due to cc, the run thread, even in the
> non-volatile version,  should eventually see this.stopped == true -- hmm
> unless the JIT were to do the kind of code 'optimization' that Greg showed
> (hoisting this.stopped out of the loop), which I guess is allowed.
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9454.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/c18e3ed8/attachment.html>

From oleksandr.otenko at oracle.com  Tue Apr 16 18:18:39 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 16 Apr 2013 23:18:39 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366149529007-9455.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com> <516DBFBA.7080104@oracle.com>
	<1366149529007-9455.post@n7.nabble.com>
Message-ID: <516DCE3F.2060309@oracle.com>

Serializability is not the only consistency model.

You need to consider what equality means. If equality is defined as 
identity, two racy writes are always broken logic. If equality is 
defined as isomorphism, some racy writes do not break logic.


Alex


On 16/04/2013 22:58, thurstonn wrote:
> Nathan Reynolds-2 wrote
>> Let's revisit the following example in this framework.
>>
>>   > Thread 1                     Thread 2
>>   > this.shared = 10            local = this.shared
>>
>>   > Is this "racy"?
>>
>> Both operations on Thread 1 and 2 are atomic (assuming word tearing
>> can't happen).
>>
>> As far as consistency is concerned, that depends upon the value of
>> "this.shared" before execution.  Is "this.shared" in a valid state to
>> begin with (i.e. is is consistent)?  If so, then Thread 2 will end up in
>> a consistent state.  If not, then it is a harmful data race.  For
>> example, if an object's reference is published before the constructor
>> finishes, then Thread 1 could be executing in the constructor and Thread
>> 2 is accessing the partially-constructed object.  The object's state is
>> inconsistent (i.e. this.shared == 0 is an invalid state).
>>
>> It has isolation.  If you execute Thread 1 and Thread 2 concurrently,
>> you get the same result as if you had executed one first and the other
>> second.  Of course, depending upon timing, the system state will end up
>> in one of two states: local == 10 or local == /previous value/.  The
>> system can't end up in a different third state.
>>
>> Nathan Reynolds
> True.  But if you apply a serialization graph to the two "transactions"
> (executions), the SG is acyclic ==> non serializable.  Normally you don't
> consider the case where all of the transactions (threads) write the same
> value to the shared data item
> &lt;http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds&gt; |
> Architect | 602.333.9091
> Oracle PSR Engineering &lt;http://psr.us.oracle.com/&gt; | Server Technology
> On 4/16/2013 12:33 PM, oleksandr otenko wrote:
>> Technically, setting hash value is racy. It is the same value, but the
>> writes race.
>>
>> Alex
>>
>> On 16/04/2013 19:57, thurstonn wrote:
>>> Just curious, how is String#hashCode() racy?
>>> Strings are immutable in java; I looked at the code a bit and I didn't
>>> see
>>> anything that looked racy.
>>> The only thing I guess could be:
>>> private char[] value
>>>
>>>
>>> Although that array is never modified in the String class, so . . .
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at .oswego
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at .oswego
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9455.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/9098089a/attachment.html>

From brian at briangoetz.com  Tue Apr 16 18:18:46 2013
From: brian at briangoetz.com (Brian Goetz)
Date: Tue, 16 Apr 2013 18:18:46 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366113714952-9434.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
Message-ID: <516DCE46.4030605@briangoetz.com>

> The "how do you know this program is thread-safe"?
> Pause.
> "I thought *really* hard about it"
>
> I can't be the only one who finds that deeply unsatisfying

Is *this* really your question?

You have been asking about data races, which is a term that has a 
precise meaning, and you got lots of precise answers about data races -- 
none of which were what you were looking for, because really, I think 
what you're asking for is a formal notion of multithreaded correctness. 
  Which, in a system which embraces mutable-by-default, is not all that 
realistic a wish.

Further, data races and thread-safety are not even ordered with respect 
to each other; there are racy programs that are still correct (the 
benign data race in String.hashCode() is a fine example), and there are 
non-thread-safe programs that have no data races.  Data-race-freedom is 
neither necessary nor sufficient for multithreaded correctness -- though 
it is a pretty good start.

We all find the situation deeply unsatisfying, but this is the real 
world, and experiments in taking away mutable-by-default from mainstream 
programmers have not been all that successful.  So we do what we can 
within the world we live in and the systems people choose to program in. 
  (I spent quite a lot of time and effort writing a book trying to 
capture good practices that reduce people's chances of writing bad code. 
  The fact that I couldn't make it impossible for people to write bad 
code, or even force everyone to read it, didn't deter me.)

From thurston at nomagicsoftware.com  Tue Apr 16 18:31:19 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 15:31:19 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DCA50.9000702@oracle.com>
References: <516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
Message-ID: <1366151479112-9459.post@n7.nabble.com>

Nathan Reynolds-2 wrote
> All things being equal, reading a volatile and non-volatile field from 
> L1/2/3/4 cache/memory has no impact on performance.  The instructions 
> are exactly the same (on x86).
> 
> Writing a volatile and non-volatile field to cache/memory has an impact 
> on performance.  Writing to a volatile field requires a memory fence on 
> x86 and many other processors.  This fence is going to take cycles.
> 
> Nathan Reynolds 
> &lt;http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds&gt; | 
> Architect | 602.333.9091
> Oracle PSR Engineering &lt;http://psr.us.oracle.com/&gt; | Server
> Technology

Sure, that's my understanding as well.  I wasn't asking about the 'cost' of
reading #stopped when declared volatile, as you mentioned there isn't one.
My question was about the 'timing' of the visibility of #stopped in the
*non-volatile* case, given cache coherency




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9459.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From nathan.reynolds at oracle.com  Tue Apr 16 19:01:02 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 16:01:02 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366151479112-9459.post@n7.nabble.com>
References: <516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366151479112-9459.post@n7.nabble.com>
Message-ID: <516DD82E.5040100@oracle.com>

On x86, only loads can bypass stores. So, the program can make progress 
even though the store hasn't been made globally visible.

In an extreme case, the store is not made globally visible for a very 
long time.  The load/store buffer is eventually going to be filled with 
other stores (loads will be able to complete).  The core is going to 
stall waiting for the store at the front of the line to complete.

In order for a store to complete, it has to be pushed into the L1 cache 
for the core.  In order to do this, the cache line has to be fetched 
from another core's cache or from RAM.  Then the cache line has to be 
invalidated in all other cores.  Both of these operations can be done in 
a single message sent to all of the cores on the system.

Consider that an L3 cache miss takes 14-38 clocks or 6-66 ns 
(http://www.sisoftware.net/?d=qa&f=ben_mem_latency) on a Sandy Bridge E 
processor.  This means a store can take a long time relatively speaking.

Also, consider that the system could have 8 processor sockets. Some 
processor sockets are not directly connected and must communicate via a 
shared processor socket.  This increases the latency of the messaging 
even further.

Without a memory fence after a non-volatile write, the subsequent loads 
can bypass the store.  These loads could "read" the value being stored 
or "read" values previously stored.  This means there is no 
happens-before relationship between the stores and loads. In other 
words, the loads could happen before the store.

There is no way to know the timing of the visibility of stopped. The 
store could happen very quickly (i.e. 4 clocks) if the cache line is in 
the modified or exclusive state in the core's L1 cache or it could 
happen after the entire system has removed the cache line from all of 
the cores and has acknowledge the invalidation.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 3:31 PM, thurstonn wrote:
> Nathan Reynolds-2 wrote
>> All things being equal, reading a volatile and non-volatile field from
>> L1/2/3/4 cache/memory has no impact on performance.  The instructions
>> are exactly the same (on x86).
>>
>> Writing a volatile and non-volatile field to cache/memory has an impact
>> on performance.  Writing to a volatile field requires a memory fence on
>> x86 and many other processors.  This fence is going to take cycles.
>>
>> Nathan Reynolds
>> &lt;http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds&gt; |
>> Architect | 602.333.9091
>> Oracle PSR Engineering &lt;http://psr.us.oracle.com/&gt; | Server
>> Technology
> Sure, that's my understanding as well.  I wasn't asking about the 'cost' of
> reading #stopped when declared volatile, as you mentioned there isn't one.
> My question was about the 'timing' of the visibility of #stopped in the
> *non-volatile* case, given cache coherency
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9459.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/9333dd9f/attachment.html>

From thurston at nomagicsoftware.com  Tue Apr 16 19:27:06 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 16:27:06 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DCA50.9000702@oracle.com>
References: <516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
Message-ID: <1366154826058-9461.post@n7.nabble.com>

Come to think of it, why don't the JDK authors declare String#hash as
volatile?
Yes, the (generally one-time) write would be more expensive . . .



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9461.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From stanimir at riflexo.com  Tue Apr 16 19:31:42 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 17 Apr 2013 02:31:42 +0300
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366148958661-9454.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com> <516C5285.90504@oracle.com>
	<516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
Message-ID: <CAEJX8op8xWTJ9Fc5cBiFbzFk8wEVxnEKVZnzeOfZg2MWNxjOZw@mail.gmail.com>

Technically, the JVM is free to ignore the write if it considers it may not
visible (according to JMM) to any other thread it's not being read by the
current.
Now, on reading non-volatile it's where it comes tough: while(!stopped){}
can be transformed to if (!stopped) for(;;){}. The latter transformation
happens for real.

I seriously do not get the idea to skip volatiles. They are relatively
cheap in terms of performance and avoiding them can cause serious problems.
It may work on some JVM/CPU but fails on an updated JVM.
Overall unannotated/commented dataraces should be considered a bug.

Stanimir



On Wed, Apr 17, 2013 at 12:49 AM, thurstonn <thurston at nomagicsoftware.com>wrote:

> I'm wondering if the stopped-flag code were to run on a modern CPU (i.e.
> with
> cache coherency), would there be any practical difference between
> declaring:
>
> boolean stopped = false
>
> vs.
>
> volatile boolean stopped  = false
>
> /Note: I'm not advocating the non-volatile, 'broken' version; of course you
> should write to the JMM, that's its purpose. I really feel for engineers
> who
> have to write for multiple CPUs/MMs; I don't know how they manage./
>
>
> But given cache coherency, let's say you were to run the code a gazillion
> times and could count the # of loops after setting this.stopped = true.
> Should one expect some (even if negligible) difference between the two
> versions?  I suppose it would depend on a lot of things (is #stopped
> cached,
> false-sharing effects, etc), but due to cc, the run thread, even in the
> non-volatile version,  should eventually see this.stopped == true -- hmm
> unless the JIT were to do the kind of code 'optimization' that Greg showed
> (hoisting this.stopped out of the loop), which I guess is allowed.
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9454.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/436ee938/attachment.html>

From vitalyd at gmail.com  Tue Apr 16 20:10:32 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 16 Apr 2013 20:10:32 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366154826058-9461.post@n7.nabble.com>
References: <516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com> <516C5285.90504@oracle.com>
	<516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
Message-ID: <CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>

Why though? The code works as-is.  String is too high profile (especially
hashing it) to do the "naive" thing.  Also, some architectures pay a
penalty for volatile loads and you'd incur that each time.
On Apr 16, 2013 7:29 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Come to think of it, why don't the JDK authors declare String#hash as
> volatile?
> Yes, the (generally one-time) write would be more expensive . . .
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9461.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/f17fe75e/attachment.html>

From davidcholmes at aapt.net.au  Tue Apr 16 20:18:45 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 17 Apr 2013 10:18:45 +1000
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516DA6D9.1040405@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECDJMAA.davidcholmes@aapt.net.au>

Heinz,

It is perfected valid for t.start() to start and context switch to the new
thread. That is up to the OS scheduler.

Thread.join only indicates logical termination at the Java level. The native
thread will still have to exit at the VM level and OS level, both of which
can hit bottlenecks and cause unexpected numbers of threads to still be
active. That in turn can impact thread creation - which happens in start().

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
Kabutz
  Sent: Wednesday, 17 April 2013 5:31 AM
  To: Nathan Reynolds
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Latency in starting threads


  Right, I would not have sent a notifyAll() if it had just been 250ms for
the thread to wake up.  However, I was always under the wrong impression
that thread.start() would return pretty much immediately.  That is the slow
part, not the constructor call.  Try it out if you find a Mac OS X machine
with at least 1.7.0_06 on it.

  And just to eliminate any confusion - I waited for each thread to die
before I start the next one.  So we never have an excessively large number
of threads around at any one time.

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz


  Nathan Reynolds wrote:
    When I started reading your email, I figured you meant the time from
calling start() to the time the created thread actually started running.
I've seen this take a considerable amount of time on Windows XP (not 250
seconds though) and has to be considered if one is trying to account for
latency.  I was surprised to find that you meant the time for the
constructor and start() call itself.


    Nathan Reynolds | Architect | 602.333.9091
    Oracle PSR Engineering | Server Technology

    On 4/16/2013 5:54 AM, Dr Heinz M. Kabutz wrote:

      Good day my fellow concurrency enthusiasts!

      Yesterday, whilst teaching my Concurrency Specialist Course, I wanted
to demonstrate to my class how slow it was starting threads and how much
better it is to use a FixedThreadPool.  The question that I wanted to answer
was: How many microseconds does it take on average to start a simple thread
and what is the maximum time it could take?

      We all know that it can take in the milliseconds range to do the
following:

      Thread t = new Thread(); // even without it actually doing anything
      t.start();

      This is one of the reasons why the fixed thread pool only starts the
threads as we submit jobs to it, since the up-front cost might not be worth
the wait.

      But how long do you think the *maximum* was that I had to wait for
t.start() to return?  100ms?  200ms?

      Actually, the longest I had to wait turned out to be about 250
seconds.  Yes.  That is *seconds*, not *milliseconds*.  Just to start a
single thread.

      This is most certainly a bug in the OpenJDK on Mac OS X.  We did not
see this behaviour on Linux nor on Windows 7.

      The bug started in OpenJDK 1.7.0_06.  Prior to that it hardly ever
took longer than 30ms to start a single thread.

      java version "1.7.0_05"
      heinz$ java ThreadLeakMac2
      time = 1, threads = 4
      time = 2, threads = 346
      time = 4, threads = 7378
      time = 7, threads = 9614
      time = 12, threads = 10027
      time = 14, threads = 10063
      time = 17, threads = 26965
      time = 38, threads = 27013
      time = 39, threads = 452053

      java version "1.7.0_06"
      heinz$ java ThreadLeakMac2
      time = 1, threads = 6
      time = 2, threads = 256
      time = 6, threads = 373
      *snip*
      time = 111, threads = 42592
      time = 200, threads = 49419
      time = 333, threads = 58976
      *snip*
      time = 3245, threads = 202336
      time = 3706, threads = 203702
      *snip*
      time = 5835, threads = 267872
      time = 6455, threads = 269238
      time = 9170, threads = 270603

      In my code, I make sure that the thread has stopped before creating
the next one by calling join().

      public class ThreadLeakMac2 {
         public static void main(String[] args) throws InterruptedException
{
             long threads = 0;
             long max = 0;
             while(true) {
                 long time = System.currentTimeMillis();
                 Thread thread = new Thread();
                 thread.start(); // should finish almost immediately
                 time = System.currentTimeMillis() - time;
                 thread.join(); // short delay, hopefully
                 threads++;
                 if (time > max) {
                     max = time;
                     System.out.println("time = " + time + ", threads = " +
threads);
                 }
             }
         }
      }

      This would be another nice test case for Alexey's concurrency stress
test harness.

      (I will also post this to the macosx-port-dev list.)

      Regards

      Heinz



----------------------------------------------------------------------------
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/f82ea16c/attachment-0001.html>

From vgrazi at gmail.com  Tue Apr 16 21:02:15 2013
From: vgrazi at gmail.com (Victor Grazi)
Date: Tue, 16 Apr 2013 21:02:15 -0400
Subject: [concurrency-interest] New version of Java Concurrent Animated
Message-ID: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>

Hi, just a quick note to let everyone know there is a new version of Java
Concurrent Animated on SourceForge.

Some of the new features:
* A new animation showing good old synchronized/wait/notify. Kind of
surprising when you see it in action.
* Thread states are now color coded, and includes a novel state diagram
kind courtesy of Dr. Heinz Kabutz and his Concurrency Specialist
Training<http://www.javaspecialists.eu/courses/concurrency.jsp>course.

Check it out!
Thanks
Victor Grazi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/5e6afc84/attachment.html>

From thurston at nomagicsoftware.com  Tue Apr 16 21:51:59 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 18:51:59 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
Message-ID: <1366163519386-9466.post@n7.nabble.com>

Vitaly Davidovich wrote
> The code works as-is.  

Absolutely.  volatile is not needed for correctness

Vitaly Davidovich wrote
> Why?

Well, for performance reasons given the 'undefined/indefinite' visibility of
#hash to other threads.
At least according to the JMM (which has nothing to say about CPU cache
coherency), it is *possible* that each distinct thread that invoked
#hashCode() *could* result in a recalculation of the hash.
Imagine a long-lived Map<String, ?>; and many threads accessing the map's
keyset and for some unknown reason invoking #hashCode() on each key.
If #hash was declared volatile, although there is no guarantee that #hash
would only be calculated once, it is guaranteed that once a write to main
memory was completed, every *subsequent* (here meaning after the write to
main memory) read no matter from which thread would see #hash != 0 and
therefore skip the calculation.



Vitaly Davidovich wrote
> String is too high profile (especially
> hashing it) to do the "naive" thing.

Nothing wrong with being naive; naive can be charming.  


Vitaly Davidovich wrote
> Also, some architectures pay a
> penalty for volatile loads and you'd incur that each time.

Fair point; the JDK authors only get one shot and they can't assume that
volatile reads are cheap





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From david.dice at gmail.com  Tue Apr 16 23:10:54 2013
From: david.dice at gmail.com (David Dice)
Date: Tue, 16 Apr 2013 23:10:54 -0400
Subject: [concurrency-interest] Subject: Re:  Latency in starting threads
Message-ID: <CANbRUci15S-G3Ep-bQq33+g0uyDuU4zRymtkKXtmnMrj4CdLkw@mail.gmail.com>

Message: 6
> Date: Wed, 17 Apr 2013 10:18:45 +1000
> From: "David Holmes" <davidcholmes at aapt.net.au>
> To: "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Latency in starting threads
> Message-ID: <NFBBKALFDCPFIDBNKAPCAECDJMAA.davidcholmes at aapt.net.au>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Heinz,
>
> It is perfected valid for t.start() to start and context switch to the new
> thread. That is up to the OS scheduler.
>
> Thread.join only indicates logical termination at the Java level. The
> native
> thread will still have to exit at the VM level and OS level, both of which
> can hit bottlenecks and cause unexpected numbers of threads to still be
> active. That in turn can impact thread creation - which happens in start().
>
> Cheers,
> David
>
>
Following up on what David said, it's not uncommon to have the launching
thread preempted by the launchee.

IIRC, on linux it was a good idea to implement fork() such that the spawned
thread would preempt the parent, as there was a good chance the child was
going to exec() anyway, and preempting the parent would cut down on COW
faults and address-space cleaving.  I think the thread subsystem still
borrows a good deal of plumbing and policy from fork(), possibly explaining
the policy on linux.   It's possible the same might apply to MacOS.

Waker preemption by the wakee is also common.   It's a good idea that
notify/notifyAll don't actually wake any thread in the critical section --
we really don't want preemption at that point.   In HotSpot
notify/notifyAll simply move a thread from the waitset to the list of
threads contending for the lock.   At worst we'll wake a notifyee after
having dropped the lock.

The host OS scheduler manages all transitions between 'ready' and 'running'
states, so we're typically at its mercy.

Regards
Dave

https://blogs.oracle.com/dave/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/91ee57f3/attachment.html>

From heinz at javaspecialists.eu  Wed Apr 17 00:42:13 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 17 Apr 2013 07:42:13 +0300
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <CAEJX8oopjQuKS6BAr2Mj2r+vZrND==8R9Nv1p+fv2Lqy5KSV4Q@mail.gmail.com>
References: <516D4A15.2090001@javaspecialists.eu>	<1366143374.52042.YahooMailNeo@web171705.mail.ir2.yahoo.com>
	<CAEJX8oopjQuKS6BAr2Mj2r+vZrND==8R9Nv1p+fv2Lqy5KSV4Q@mail.gmail.com>
Message-ID: <516E2825.4050804@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/a68dbedb/attachment.html>

From heinz at javaspecialists.eu  Wed Apr 17 00:50:43 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 17 Apr 2013 07:50:43 +0300
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECDJMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAECDJMAA.davidcholmes@aapt.net.au>
Message-ID: <516E2A23.1020204@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7cb5c672/attachment-0001.html>

From kirk at kodewerk.com  Wed Apr 17 01:08:38 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Wed, 17 Apr 2013 07:08:38 +0200
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516E2825.4050804@javaspecialists.eu>
References: <516D4A15.2090001@javaspecialists.eu>	<1366143374.52042.YahooMailNeo@web171705.mail.ir2.yahoo.com>
	<CAEJX8oopjQuKS6BAr2Mj2r+vZrND==8R9Nv1p+fv2Lqy5KSV4Q@mail.gmail.com>
	<516E2825.4050804@javaspecialists.eu>
Message-ID: <D8AAF2CD-16F2-47E9-9105-6C0FB4414410@kodewerk.com>

Hi,



> In my experience, currentTimeMillis() gives good results if used correctly.

That is my experience also.

>> 
>> System.currentTimeMillis() might jump around
>> with system time
>> System.currentTimeMillis() works quite fine if you use NTP daemon. The NPT slows down or speeds up the system clock frequency very slightly to adjust, hence no jumps.

From http://www.ntp.org/ntpfaq/NTP-s-algo.htm

In order to keep the right time, xntpd must make adjustments to the system clock. Different operating systems provide different means, but the most popular ones are listed below.

Basically there are four mechanisms (system calls) an NTP implementation can use to discipline the system clock (For details see the different RFCs found in Table 4):


settimeofday(2) to step (set) the time. This method is used if the time if off by more than 128ms.

adjtime(2) to slew (gradually change) the time. Slewing the time means to change the virtual frequency of the software clock to make the clock go faster or slower until the requested correction is achieved. Slewing the clock for a larger amount of time may require some time, too. For example standard Linux adjusts the time with a rate of 0.5ms per second.

ntp_adjtime(2) to control several parameters of the software clock (also known as kernel discipline). Among these parameters are:


Adjust the offset of the software clock, possibly correcting the virtual frequency as well

Adjust the virtual frequency of the software clock directly

Enable or disable PPS event processing

Control processing of leap seconds

Read and set some related characteristic values of the clock


So, I suspect no jumps in this case.

Regards,
Kirk
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/6a38ea6e/attachment.html>

From zhong.j.yu at gmail.com  Wed Apr 17 01:27:05 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 17 Apr 2013 00:27:05 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366163519386-9466.post@n7.nabble.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
Message-ID: <CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>

On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <thurston at nomagicsoftware.com> wrote:
> Vitaly Davidovich wrote
>> The code works as-is.
>
> Absolutely.  volatile is not needed for correctness
>
> Vitaly Davidovich wrote
>> Why?
>
> Well, for performance reasons given the 'undefined/indefinite' visibility of
> #hash to other threads.
> At least according to the JMM (which has nothing to say about CPU cache
> coherency), it is *possible* that each distinct thread that invoked
> #hashCode() *could* result in a recalculation of the hash.

In practice though, application threads contain very frequent
synchronization actions, or other operations that force VM to
flush/reload. So it won't take very long for any non-volatile write in
one thread to become visible to other threads.

> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
> keyset and for some unknown reason invoking #hashCode() on each key.
> If #hash was declared volatile, although there is no guarantee that #hash
> would only be calculated once, it is guaranteed that once a write to main
> memory was completed, every *subsequent* (here meaning after the write to

In JMM though, we cannot even express this guarantee. Say we have
threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
volatile-reads #hash, and if it's 0, calculates and volatile-writes
#hash which takes 100 ns. We can find no guarantee from JMM that
there's only one write; it's legal that every thread sees 0 from the
volatile read.

Zhong Yu

> main memory) read no matter from which thread would see #hash != 0 and
> therefore skip the calculation.
>
>
>
> Vitaly Davidovich wrote
>> String is too high profile (especially
>> hashing it) to do the "naive" thing.
>
> Nothing wrong with being naive; naive can be charming.
>
>
> Vitaly Davidovich wrote
>> Also, some architectures pay a
>> penalty for volatile loads and you'd incur that each time.
>
> Fair point; the JDK authors only get one shot and they can't assume that
> volatile reads are cheap
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From thurston at nomagicsoftware.com  Wed Apr 17 01:54:09 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 16 Apr 2013 22:54:09 -0700 (PDT)
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DCE46.4030605@briangoetz.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
	<516DCE46.4030605@briangoetz.com>
Message-ID: <1366178049325-9472.post@n7.nabble.com>

Brian Goetz-3 wrote
>> The "how do you know this program is thread-safe"?
>> Pause.
>> "I thought *really* hard about it"
>>
>> I can't be the only one who finds that deeply unsatisfying
> 
> Is *this* really your question?

Uh, yes.


Brian Goetz-3 wrote
> You have been asking about data races, which is a term that has a 
> precise meaning, and you got lots of precise answers about data races -- 
> none of which were what you were looking for, because really, I think 
> what you're asking for is a formal notion of multithreaded correctness. 

I addressed this specifically in an earlier response:
"Yes, there is an assumption in my OP that "data-race" ==> "incorrect" (or
"broken logic" in your words)
Vitaly does not necessarily equate "raciness" with "incorrectness" (and
probably Brian as well) and that's OK with me"

I'm not sure how I can be clearer than that.
Although I wouldn't describe what I'm after as a "formal notion of mt
correctness" - but that's probably just a quibble.
I'm not sure that all responses that have defined a data-race have given the
exact same "precise meaning", but regardless I'm *not* hung-up on the term
"data-race", I think it makes sense to use the most conservative definition
(conflicting operations from multiple threads) which allows for the notion
of a "benign" data-race


Brian Goetz-3 wrote
> Further, data races and thread-safety are not even ordered with respect 
> to each other; there are racy programs that are still correct (the 
> benign data race in String.hashCode() is a fine example), 

Hmm, let me quote:
"A program that accesses a mutable variable from multiple threads without
synchronization is a broken program" -- sound familiar?
Clearly String#hashCode() qualifies, and yet all agree that it *is*
"correct"







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9472.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From nathan.reynolds at oracle.com  Wed Apr 17 02:34:30 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 23:34:30 -0700
Subject: [concurrency-interest] New version of Java Concurrent Animated
In-Reply-To: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>
References: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>
Message-ID: <516E4276.90504@oracle.com>

Nice animation!  I noticed a couple of problems.

The ConcurrentHashMap demo fails to draw the threads if you keep hitting 
putIfAbsent.  The threads are drawn beyond the bottom.

If I remember right ReentrantReadWriteLock gives priority to writer 
threads.  If that is the case then the ReadWriteLock demo is not quite 
right.

Consider this scenario.

 1. Thread #1 calls writeLock().lock() and enters the critical region.
 2. Thread #2 calls readLock().lock() and blocks.
 3. Thread #3 calls readLock().lock() and blocks.
 4. Thread #4 calls readLock().lock() and blocks.
 5. Thread #5 calls readLock().lock() and blocks.
 6. Thread #6 calls writeLock().lock() and blocks.
 7. Thread #1 releases the lock.
 8. If writer threads are given priority, then Thread #6 should enter
    the critical region.

In the demo, threads #2 to #5 are given the lock in step 8.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 6:02 PM, Victor Grazi wrote:
> Hi, just a quick note to let everyone know there is a new version of 
> Java Concurrent Animated on SourceForge.
>
> Some of the new features:
> * A new animation showing good old synchronized/wait/notify. Kind of 
> surprising when you see it in action.
> * Thread states are now color coded, and includes a novel state 
> diagram kind courtesy of Dr. Heinz Kabutz and his Concurrency 
> Specialist Training 
> <http://www.javaspecialists.eu/courses/concurrency.jsp> course.
>
> Check it out!
> Thanks
> Victor Grazi
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/1621b972/attachment.html>

From nathan.reynolds at oracle.com  Wed Apr 17 02:38:59 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 16 Apr 2013 23:38:59 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
Message-ID: <516E4383.1020103@oracle.com>

Couldn't JIT hoist the non-volatile writes out of the loop?  For 
example, the following code...

for (i = 0; i < 1_000_000_000; i++)
{
     System.out.println(i);
     shared = 2 * i;
}

... could be transformed into ...

for (i = 0; i < 1_000_000_000; i++)
{
     System.out.println(i);
}

shared = 2 * 1_000_000_000;

... If so, then the non-volatile write may not happen for a very long time.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 10:27 PM, Zhong Yu wrote:
> On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <thurston at nomagicsoftware.com> wrote:
>> Vitaly Davidovich wrote
>>> The code works as-is.
>> Absolutely.  volatile is not needed for correctness
>>
>> Vitaly Davidovich wrote
>>> Why?
>> Well, for performance reasons given the 'undefined/indefinite' visibility of
>> #hash to other threads.
>> At least according to the JMM (which has nothing to say about CPU cache
>> coherency), it is *possible* that each distinct thread that invoked
>> #hashCode() *could* result in a recalculation of the hash.
> In practice though, application threads contain very frequent
> synchronization actions, or other operations that force VM to
> flush/reload. So it won't take very long for any non-volatile write in
> one thread to become visible to other threads.
>> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
>> keyset and for some unknown reason invoking #hashCode() on each key.
>> If #hash was declared volatile, although there is no guarantee that #hash
>> would only be calculated once, it is guaranteed that once a write to main
>> memory was completed, every *subsequent* (here meaning after the write to
> In JMM though, we cannot even express this guarantee. Say we have
> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
> volatile-reads #hash, and if it's 0, calculates and volatile-writes
> #hash which takes 100 ns. We can find no guarantee from JMM that
> there's only one write; it's legal that every thread sees 0 from the
> volatile read.
>
> Zhong Yu
>
>> main memory) read no matter from which thread would see #hash != 0 and
>> therefore skip the calculation.
>>
>>
>>
>> Vitaly Davidovich wrote
>>> String is too high profile (especially
>>> hashing it) to do the "naive" thing.
>> Nothing wrong with being naive; naive can be charming.
>>
>>
>> Vitaly Davidovich wrote
>>> Also, some architectures pay a
>>> penalty for volatile loads and you'd incur that each time.
>> Fair point; the JDK authors only get one shot and they can't assume that
>> volatile reads are cheap
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/d0a91772/attachment-0001.html>

From heinz at javaspecialists.eu  Wed Apr 17 03:06:58 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 17 Apr 2013 10:06:58 +0300
Subject: [concurrency-interest] New version of Java Concurrent Animated
In-Reply-To: <516E4276.90504@oracle.com>
References: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>
	<516E4276.90504@oracle.com>
Message-ID: <516E4A12.1060903@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/aa7afeda/attachment.html>

From davidcholmes at aapt.net.au  Wed Apr 17 03:34:58 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 17 Apr 2013 17:34:58 +1000
Subject: [concurrency-interest] Latency in starting threads
In-Reply-To: <516E2A23.1020204@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECGJMAA.davidcholmes@aapt.net.au>

Starting the native thread actually does involve a handshake that switches
execution between the starter, startee and back. This is done because the
new thread has to run to set some state before it can be considered a valid
thread. So there is actually a lot happening behind start() and a number of
scheduling points.

But this need further investigation if 7u6 introduced the anomaly.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
Kabutz
  Sent: Wednesday, 17 April 2013 2:51 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Latency in starting threads


  Hi David,

  I understand it is valid for t.start() to wait until the new thread has
really started.  I was just surprised, as I expected it to be basically an
asynchronous call that spawns something but does not wait for it to actually
start.

  Also, with the join(), I realize that this is at the end of the Java view
of the thread.  However, I did check that the threads were indeed shutting
down by looking at the process in top and also dumping threads.

  Heinz

  David Holmes wrote:
    Heinz,

    It is perfected valid for t.start() to start and context switch to the
new thread. That is up to the OS scheduler.

    Thread.join only indicates logical termination at the Java level. The
native thread will still have to exit at the VM level and OS level, both of
which can hit bottlenecks and cause unexpected numbers of threads to still
be active. That in turn can impact thread creation - which happens in
start().

    Cheers,
    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M.
Kabutz
      Sent: Wednesday, 17 April 2013 5:31 AM
      To: Nathan Reynolds
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Latency in starting threads


      Right, I would not have sent a notifyAll() if it had just been 250ms
for the thread to wake up.  However, I was always under the wrong impression
that thread.start() would return pretty much immediately.  That is the slow
part, not the constructor call.  Try it out if you find a Mac OS X machine
with at least 1.7.0_06 on it.

      And just to eliminate any confusion - I waited for each thread to die
before I start the next one.  So we never have an excessively large number
of threads around at any one time.

Regards

Heinz
--
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz


      Nathan Reynolds wrote:
        When I started reading your email, I figured you meant the time from
calling start() to the time the created thread actually started running.
I've seen this take a considerable amount of time on Windows XP (not 250
seconds though) and has to be considered if one is trying to account for
latency.  I was surprised to find that you meant the time for the
constructor and start() call itself.


        Nathan Reynolds | Architect | 602.333.9091
        Oracle PSR Engineering | Server Technology

        On 4/16/2013 5:54 AM, Dr Heinz M. Kabutz wrote:

          Good day my fellow concurrency enthusiasts!

          Yesterday, whilst teaching my Concurrency Specialist Course, I
wanted to demonstrate to my class how slow it was starting threads and how
much better it is to use a FixedThreadPool.  The question that I wanted to
answer was: How many microseconds does it take on average to start a simple
thread and what is the maximum time it could take?

          We all know that it can take in the milliseconds range to do the
following:

          Thread t = new Thread(); // even without it actually doing
anything
          t.start();

          This is one of the reasons why the fixed thread pool only starts
the threads as we submit jobs to it, since the up-front cost might not be
worth the wait.

          But how long do you think the *maximum* was that I had to wait for
t.start() to return?  100ms?  200ms?

          Actually, the longest I had to wait turned out to be about 250
seconds.  Yes.  That is *seconds*, not *milliseconds*.  Just to start a
single thread.

          This is most certainly a bug in the OpenJDK on Mac OS X.  We did
not see this behaviour on Linux nor on Windows 7.

          The bug started in OpenJDK 1.7.0_06.  Prior to that it hardly ever
took longer than 30ms to start a single thread.

          java version "1.7.0_05"
          heinz$ java ThreadLeakMac2
          time = 1, threads = 4
          time = 2, threads = 346
          time = 4, threads = 7378
          time = 7, threads = 9614
          time = 12, threads = 10027
          time = 14, threads = 10063
          time = 17, threads = 26965
          time = 38, threads = 27013
          time = 39, threads = 452053

          java version "1.7.0_06"
          heinz$ java ThreadLeakMac2
          time = 1, threads = 6
          time = 2, threads = 256
          time = 6, threads = 373
          *snip*
          time = 111, threads = 42592
          time = 200, threads = 49419
          time = 333, threads = 58976
          *snip*
          time = 3245, threads = 202336
          time = 3706, threads = 203702
          *snip*
          time = 5835, threads = 267872
          time = 6455, threads = 269238
          time = 9170, threads = 270603

          In my code, I make sure that the thread has stopped before
creating the next one by calling join().

          public class ThreadLeakMac2 {
             public static void main(String[] args) throws
InterruptedException {
                 long threads = 0;
                 long max = 0;
                 while(true) {
                     long time = System.currentTimeMillis();
                     Thread thread = new Thread();
                     thread.start(); // should finish almost immediately
                     time = System.currentTimeMillis() - time;
                     thread.join(); // short delay, hopefully
                     threads++;
                     if (time > max) {
                         max = time;
                         System.out.println("time = " + time + ", threads =
" + threads);
                     }
                 }
             }
          }

          This would be another nice test case for Alexey's concurrency
stress test harness.

          (I will also post this to the macosx-port-dev list.)

          Regards

          Heinz



------------------------------------------------------------------------
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/3463066f/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 06:18:56 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 11:18:56 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516E4383.1020103@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
Message-ID: <516E7710.7040507@oracle.com>

You need to prove System.out.println isn't using shared.

Alex

On 17/04/2013 07:38, Nathan Reynolds wrote:
> Couldn't JIT hoist the non-volatile writes out of the loop?  For 
> example, the following code...
>
> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
>     shared = 2 * i;
> }
>
> ... could be transformed into ...
>
> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
> }
>
> shared = 2 * 1_000_000_000;
>
> ... If so, then the non-volatile write may not happen for a very long 
> time.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>> On Tue, Apr 16, 2013 at 8:51 PM, thurstonn<thurston at nomagicsoftware.com>  wrote:
>>> Vitaly Davidovich wrote
>>>> The code works as-is.
>>> Absolutely.  volatile is not needed for correctness
>>>
>>> Vitaly Davidovich wrote
>>>> Why?
>>> Well, for performance reasons given the 'undefined/indefinite' visibility of
>>> #hash to other threads.
>>> At least according to the JMM (which has nothing to say about CPU cache
>>> coherency), it is *possible* that each distinct thread that invoked
>>> #hashCode() *could* result in a recalculation of the hash.
>> In practice though, application threads contain very frequent
>> synchronization actions, or other operations that force VM to
>> flush/reload. So it won't take very long for any non-volatile write in
>> one thread to become visible to other threads.
>>> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
>>> keyset and for some unknown reason invoking #hashCode() on each key.
>>> If #hash was declared volatile, although there is no guarantee that #hash
>>> would only be calculated once, it is guaranteed that once a write to main
>>> memory was completed, every *subsequent* (here meaning after the write to
>> In JMM though, we cannot even express this guarantee. Say we have
>> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>> volatile-reads #hash, and if it's 0, calculates and volatile-writes
>> #hash which takes 100 ns. We can find no guarantee from JMM that
>> there's only one write; it's legal that every thread sees 0 from the
>> volatile read.
>>
>> Zhong Yu
>>
>>> main memory) read no matter from which thread would see #hash != 0 and
>>> therefore skip the calculation.
>>>
>>>
>>>
>>> Vitaly Davidovich wrote
>>>> String is too high profile (especially
>>>> hashing it) to do the "naive" thing.
>>> Nothing wrong with being naive; naive can be charming.
>>>
>>>
>>> Vitaly Davidovich wrote
>>>> Also, some architectures pay a
>>>> penalty for volatile loads and you'd incur that each time.
>>> Fair point; the JDK authors only get one shot and they can't assume that
>>> volatile reads are cheap
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/fc98ca9b/attachment.html>

From mudit.f2004912 at gmail.com  Wed Apr 17 08:56:28 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 17 Apr 2013 14:56:28 +0200
Subject: [concurrency-interest] An Eventually Consistent highly scalable
 K-V Store/HashMap
In-Reply-To: <41386124.jsafdxjMAY@artemis>
References: <CAMM2gsMNNnFdPNxj3XDv4v4TFN68WYQXq6jCwJf59A2-LZg91g@mail.gmail.com>
	<41386124.jsafdxjMAY@artemis>
Message-ID: <CAMM2gsNGGrBh7=4zwF2R2EaLODJRXmWGEiWHrWhzprB3GhBRNQ@mail.gmail.com>

Hello Everyone,

Thanks all for deep insight.  I will soon write in detail about our
approach and rationale behind it. However, one thing is for sure, as you
all mentioned, relaxation would not come for free. We'll have to give up on
some requirements in order to achieve some other such as better
performance.

Mudit


On Thu, Apr 11, 2013 at 9:08 AM, Jan Kotek <discus at kotek.net> wrote:

> **
>
> Hi,
>
>
>
> not sure how much it is related, but I wrote BTreeMap and HTreeMap which
> have transactions. It uses embedded db to store tree nodes, but could be
> easily altered to use keep node instances in heap, this way it would work
> as any other heap based collection.
>
>
>
> https://github.com/jankotek/mapdb
>
>
>
> Jan
>
>
>
> On Wednesday 10 April 2013 04:24:14 Mudit Verma wrote:
>
> Hello All,
>
> I am new to the group. For past few months I have been working on
> Concurrent Data Structures for my master thesis. My work is motivated by
> the assumption that in future, with growing number of cores per machine, we
> might need to give up strong semantics in order to achieve better
> performance.
>
> For example, we have come up with a sequentially consistent version of
> Queue which is 50-70 times faster than the original linearized
> ConcurrentLinkedQueue under very high contention. In our synthetic
> benchmarks we have found that in ConcurrentLinkedQueue one operation
> (enqueue/dequeue) can take as high as 3,00,000 cycles  where
> ThinkTime/Delay between two operations (work done between two ops)  is in
> order of 200,000-300,000 cycles.
>
> With the same approach of relaxing the semantics, we want to implement a
> KV (hashmap/hashset) which will be designed just keeping in mind the very
> high scalability.  Obviously, we are giving up on stronger semantics but we
> think that there are applications which can bear with it.
>
> We did some scalability testing on ConcurrentHashMap but I just now found
> out that a new implementation is planned for Java 8 which has better
> scalability.
>
> We want to use CRDTs (conflict free replicated data types) techniques used
> in distributed systems [1] where operations commute (add followed by remove
> or remove followed by add converge to same state eventually).
>
> We also think that transactions can be implemented on top KV  where
> multiples puts/gets are grouped and applied by workers and have atomic (all
> or none) effect on underneath KV store/hashMap.
>
> Since people in this group work actively on Concurrent Data Structures, I
> am looking for comments and suggestions on
>
>
> a. Eventually Consistent HashMaps/KV Stores for multicores.
> b. Transactions on top of HashMaps/KV Stores in multicores.
>
> What are your take on these two ideas, will it be useful?  If yes, do you
> have some applications in mind?
>
> Thanks,
> Mudit Verma
>
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/5438a17d/attachment.html>

From gregg at cytetech.com  Wed Apr 17 09:25:21 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 17 Apr 2013 08:25:21 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516E7710.7040507@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com> <516E7710.7040507@oracle.com>
Message-ID: <516EA2C1.6020802@cytetech.com>

Actually the compiler doesn't do that for non-volatile access, as I've been 
going on about with the loop exit hoisting example.  This transformation is 
exactly the kind of thing that I'd expect to see happen, and this is the 
"unexplainable behavior" that will create alarm, because mutations of "shared" 
are not visible, except a loop exit.

Gregg Wonderly

On 4/17/2013 5:18 AM, oleksandr otenko wrote:
> You need to prove System.out.println isn't using shared.
>
> Alex
>
> On 17/04/2013 07:38, Nathan Reynolds wrote:
>> Couldn't JIT hoist the non-volatile writes out of the loop?  For example, the
>> following code...
>>
>> for (i = 0; i < 1_000_000_000; i++)
>> {
>>     System.out.println(i);
>>     shared = 2 * i;
>> }
>>
>> ... could be transformed into ...
>>
>> for (i = 0; i < 1_000_000_000; i++)
>> {
>>     System.out.println(i);
>> }
>>
>> shared = 2 * 1_000_000_000;
>>
>> ... If so, then the non-volatile write may not happen for a very long time.
>>
>> Nathan Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>> | Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>> On Tue, Apr 16, 2013 at 8:51 PM, thurstonn<thurston at nomagicsoftware.com>  wrote:
>>>> Vitaly Davidovich wrote
>>>>> The code works as-is.
>>>> Absolutely.  volatile is not needed for correctness
>>>>
>>>> Vitaly Davidovich wrote
>>>>> Why?
>>>> Well, for performance reasons given the 'undefined/indefinite' visibility of
>>>> #hash to other threads.
>>>> At least according to the JMM (which has nothing to say about CPU cache
>>>> coherency), it is *possible* that each distinct thread that invoked
>>>> #hashCode() *could* result in a recalculation of the hash.
>>> In practice though, application threads contain very frequent
>>> synchronization actions, or other operations that force VM to
>>> flush/reload. So it won't take very long for any non-volatile write in
>>> one thread to become visible to other threads.
>>>> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
>>>> keyset and for some unknown reason invoking #hashCode() on each key.
>>>> If #hash was declared volatile, although there is no guarantee that #hash
>>>> would only be calculated once, it is guaranteed that once a write to main
>>>> memory was completed, every *subsequent* (here meaning after the write to
>>> In JMM though, we cannot even express this guarantee. Say we have
>>> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>>> volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>> #hash which takes 100 ns. We can find no guarantee from JMM that
>>> there's only one write; it's legal that every thread sees 0 from the
>>> volatile read.
>>>
>>> Zhong Yu
>>>
>>>> main memory) read no matter from which thread would see #hash != 0 and
>>>> therefore skip the calculation.
>>>>
>>>>
>>>>
>>>> Vitaly Davidovich wrote
>>>>> String is too high profile (especially
>>>>> hashing it) to do the "naive" thing.
>>>> Nothing wrong with being naive; naive can be charming.
>>>>
>>>>
>>>> Vitaly Davidovich wrote
>>>>> Also, some architectures pay a
>>>>> penalty for volatile loads and you'd incur that each time.
>>>> Fair point; the JDK authors only get one shot and they can't assume that
>>>> volatile reads are cheap
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From mudit.f2004912 at gmail.com  Wed Apr 17 09:31:51 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 17 Apr 2013 15:31:51 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior of
	ReentrantLock under contention.
Message-ID: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>

Hi All,

I recently performed a scalability test (very aggressive and may be not
practical, anyhow) on put operation of ConcurrentHashMap.

Test: Each thread is trying to put (Same Key, Random Value) in HashMap in a
tight loop. Therefore, all the threads will hit the same location on
hashMap and will cause contention.

What is more surprising is, when each thread continue to do another put one
after the other, avg time taken in one put operation is lesser than when a
thread do some other work between two put operations.

We continue to see the increase in per operation time by increasing the
work done in between.  This is very counter intuitive. Only after a work of
about 10,000 - 20,000 cycles in between, per op time comes down.

When I read the code, I found out that put op first try to use CAS to
aquire the lock(64 times on multicore), only if it could not acquire the
lock on segment through CASing, it goes for ReentrantLocking (which suspend
threads .. ).

We also tweaked, the number of times CAS (from 0 to 10,000) is used before
actually going for ReentrantLocking.   Attached is the graph.

One interesting thing to note. As we increase the work between two ops,
locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
the spike. Therefore, I assume that, spike comes from ReentractLocking even
when there is a mixture of two (CAS+Lock).

Code Skeleton: For each Thread

While() {
  hashMap.put(K,randomValue);     // K is same for each thread
  ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>

}

 Machine: 48 core NUMA
Threads used:  32 (each one is pinned to a core).
#ops: In total 51200000 (each thread with 160000 ops)

That's like saying, if I do something else in between my operations (upto a
limit), contention will increase.  Very strange.

Does anyone of you know why is it happening?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/b6b7b033/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ConcurrentHashMap-Put.JPG
Type: image/jpeg
Size: 59331 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/b6b7b033/attachment-0001.jpe>

From oleksandr.otenko at oracle.com  Wed Apr 17 09:32:51 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 14:32:51 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516EA2C1.6020802@cytetech.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com> <516E7710.7040507@oracle.com>
	<516EA2C1.6020802@cytetech.com>
Message-ID: <516EA483.1080007@oracle.com>

I am not sure what you are saying.

I am only saying that moving the write out of the loop is possible only 
if we can prove it is not used inside the loop. Multiple writes are 
fused in exactly the same way as multiple reads are fused. Elimination 
is a form of reordering.

Alex

On 17/04/2013 14:25, Gregg Wonderly wrote:
> Actually the compiler doesn't do that for non-volatile access, as I've 
> been going on about with the loop exit hoisting example.  This 
> transformation is exactly the kind of thing that I'd expect to see 
> happen, and this is the "unexplainable behavior" that will create 
> alarm, because mutations of "shared" are not visible, except a loop exit.
>
> Gregg Wonderly
>
> On 4/17/2013 5:18 AM, oleksandr otenko wrote:
>> You need to prove System.out.println isn't using shared.
>>
>> Alex
>>
>> On 17/04/2013 07:38, Nathan Reynolds wrote:
>>> Couldn't JIT hoist the non-volatile writes out of the loop?  For 
>>> example, the
>>> following code...
>>>
>>> for (i = 0; i < 1_000_000_000; i++)
>>> {
>>>     System.out.println(i);
>>>     shared = 2 * i;
>>> }
>>>
>>> ... could be transformed into ...
>>>
>>> for (i = 0; i < 1_000_000_000; i++)
>>> {
>>>     System.out.println(i);
>>> }
>>>
>>> shared = 2 * 1_000_000_000;
>>>
>>> ... If so, then the non-volatile write may not happen for a very 
>>> long time.
>>>
>>> Nathan Reynolds 
>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>> | Architect | 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>> On Tue, Apr 16, 2013 at 8:51 PM, 
>>>> thurstonn<thurston at nomagicsoftware.com>  wrote:
>>>>> Vitaly Davidovich wrote
>>>>>> The code works as-is.
>>>>> Absolutely.  volatile is not needed for correctness
>>>>>
>>>>> Vitaly Davidovich wrote
>>>>>> Why?
>>>>> Well, for performance reasons given the 'undefined/indefinite' 
>>>>> visibility of
>>>>> #hash to other threads.
>>>>> At least according to the JMM (which has nothing to say about CPU 
>>>>> cache
>>>>> coherency), it is *possible* that each distinct thread that invoked
>>>>> #hashCode() *could* result in a recalculation of the hash.
>>>> In practice though, application threads contain very frequent
>>>> synchronization actions, or other operations that force VM to
>>>> flush/reload. So it won't take very long for any non-volatile write in
>>>> one thread to become visible to other threads.
>>>>> Imagine a long-lived Map<String, ?>; and many threads accessing 
>>>>> the map's
>>>>> keyset and for some unknown reason invoking #hashCode() on each key.
>>>>> If #hash was declared volatile, although there is no guarantee 
>>>>> that #hash
>>>>> would only be calculated once, it is guaranteed that once a write 
>>>>> to main
>>>>> memory was completed, every *subsequent* (here meaning after the 
>>>>> write to
>>>> In JMM though, we cannot even express this guarantee. Say we have
>>>> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>>>> volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>>> #hash which takes 100 ns. We can find no guarantee from JMM that
>>>> there's only one write; it's legal that every thread sees 0 from the
>>>> volatile read.
>>>>
>>>> Zhong Yu
>>>>
>>>>> main memory) read no matter from which thread would see #hash != 0 
>>>>> and
>>>>> therefore skip the calculation.
>>>>>
>>>>>
>>>>>
>>>>> Vitaly Davidovich wrote
>>>>>> String is too high profile (especially
>>>>>> hashing it) to do the "naive" thing.
>>>>> Nothing wrong with being naive; naive can be charming.
>>>>>
>>>>>
>>>>> Vitaly Davidovich wrote
>>>>>> Also, some architectures pay a
>>>>>> penalty for volatile loads and you'd incur that each time.
>>>>> Fair point; the JDK authors only get one shot and they can't 
>>>>> assume that
>>>>> volatile reads are cheap
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> -- 
>>>>> View this message in 
>>>>> context:http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From gergg at cox.net  Tue Apr 16 19:53:21 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 16 Apr 2013 18:53:21 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516D8350.5010800@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
Message-ID: <153E1469-E080-4F9C-B231-6FE39B3456AD@cox.net>

I didn't meant to say that it was "correct", only that it worked correctly, and on x86, it probably could not be tested to be "wrong".

I just trying to point out that the complexity of this is much more of a problem than I think is addressed by tooling and language support.

I am grateful that the JMM finally details exactly what to expect.  I'm frustrated by the fact that this also means that the "real rules" never changed (synchronized is important and must be used), but the behavior did.  Now we see, not just "visibility" issues, but logically "weird" behaviors out of racy code.  From my perspective, I really think that the JMM got volatile backwards.  We should of used a different keyword, the reverse of volatile, to be able to say this variable is intra-thread only.   That, for me in all of the network and multi-thread applications that I write, would be the much "less used" keyword.  I rarely need atomicity (I use synchronized or other locks when I need that) near as much as I need visibility, and visibility and "inter-thread" sanity would make it much more likely that people would get correct code first, and could then focus on faster by turning on "intra-thread" access.  That would also allow tools to assert that there was non inter-thread access, and warn about it.

Gregg

On Apr 16, 2013, at 11:58 AM, Nathan Reynolds <nathan.reynolds at oracle.com> wrote:

> Try running this code on JDK 1.4.2 on a processor with a weak memory model.  It will perform exactly as Alex has stated.  If you run it on x86, then it will behave as you want it to.
> 
> That is the beauty of JMM.  It provides a bit of sanity when trying to write code for different processors and different versions of JIT (i.e. different JVMs or even different versions of the same JVM).  It is really difficult to write code in C++ for 4 different processors and 4 different compilers.  One of them is going to bite you if you try anything fancy.  Fortunately, there is some hope in sight for C++.
> 
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 4/16/2013 9:18 AM, oleksandr otenko wrote:
>> The "pre-1.5" kind of code you quote is not correct in pre-1.5, and in other mainstream languages. So not clear who is unclear about incorrectness of that code and why you insist on supporting it. 
>> 
>> It worked in pre-1.5 by accident. If you set stopping=true in another thread, it can stay in write buffers indefinitely because that's how modern CPUs work. There is nothing telling when it should be flushed. For example, suppose you have stopping=true; while(!stopped); waiting for a thread to report it stopped. That's the proof of incorrectness even in terms of pre-1.5. 
>> 
>> Alex 
>> 
>> On 16/04/2013 16:17, Gregg Wonderly wrote: 
>>> The thing I don't believe is completely understood, is how much software was written prior to JDK1.5, which explicitly did not synchronized() on reads of "shared" data, because it made code faster.  The particular scenario that I've seen in various older code, is the "boolean" while loop control variable set to "true" by a "shutdown" method.  That happened all over the place, because it was so ugly to put a "synchronized" section around a whole loop. 
>>> 
>>> boolean stopping; 
>>> 
>>> public void shutdown() { 
>>>     stopping = true; 
>>> } 
>>> 
>>> 
>>> public void run() { 
>>>     while(!stopping) { 
>>>         // do your work 
>>>     } 
>>> } 
>>> 
>>> As wrong as this is, it worked just fine prior to JDK1.5.   People didn't seem t want/see the need to, write a correct version of this, such as: 
>>> 
>>> public synchronized void shutdown() { 
>>>     stopping = true; 
>>> } 
>>> 
>>> public void run() { 
>>>     while( true ) { 
>>>         synchronized( this ) { 
>>>             if( stopping ) 
>>>                 break; 
>>>         } 
>>> 
>>>         ... 
>>>     } 
>>> } 
>>> 
>>> It's that kind of old code "issue", which I think is leaving around all kinds of open cans of worms.  This was an optimization issue.  The compiler would do 
>>> 
>>>     if( !stopping ) 
>>>         return; 
>>>     while( true ) { 
>>>         ... 
>>>     } 
>>> 
>>> Visibility issues get "masked" over by people using "correctly" or "more correctly" synchronized code, which seems to trigger cache flushes or otherwise update cache lines to cause their "wrong" code to appear to be right. 
>>> 
>>> Tools to look for these kinds of problems are starting to mature and be more and more visible and people get bit by these things. But, overall, I'm just not convinced there is any hope that developers are really going to be able to deal with these issues in any dependable fashion.  Software stability is going to suffer, and that will keep up a barrier for some people in using Java when there are bugs visible to them but which have no explainable reason, given the "visible in source" order of execution. 
>>> 
>>> We can get all excited about optimizations and speed of execution. But it doesn't matter how fast you do the wrong thing, it's still wrong. 
>>> 
>>> Gregg Wonderly 
>>> 
>>> On 4/15/2013 7:12 PM, Vitaly Davidovich wrote: 
>>>> Yes, threading is hard :) There's nothing stopping people from using different 
>>>> designs to minimize the hairy parts (e.g. message passing, immutability, etc) in 
>>>> java.  Deliberate data races are an "expert"-only tool, and last resort at 
>>>> that.  I think this list typically reiterates that point when the subject comes up. 
>>>> 
>>>> On Apr 15, 2013 6:46 PM, "Gregg Wonderly" <gregg at cytetech.com 
>>>> <mailto:gregg at cytetech.com>> wrote: 
>>>> 
>>>>     Happens-before doesn't play a part in program=order, only thread-order. 
>>>>       This is the problem that a lot of developers seem to struggle with.  If 
>>>>     you want the history of all possible paths, then the complexity of that 
>>>>     possibility is huge.  Basically for X unrelated instructions, the possible 
>>>>     histories are X^2. Because happens-before injects a "barrier", it can appear 
>>>>     to simplify the complexity, but, the global ordering still is X^2 for all 
>>>>     "blocks" between happens-before spots. 
>>>> 
>>>>     This is why I keep waving my hands here, and saying "stop that".  It makes 
>>>>     it extremely difficult for "racy" software to be understood as "racy", 
>>>>     because "reordering" introduces a whole realm of "unknown compiler 
>>>>     reorderings changing JLS specified behaviors" that many developers, may not 
>>>>     recognize as needing their attention.  I still encounter developers who just 
>>>>     don't understand that the JLS doesn't describe "inter-thread" behaviors, 
>>>>     because of the JMM.  People expect that they will just see data races, and 
>>>>     not programmatic run time data corruption such as the 
>>>>     Thread.getThread()/.setThread(__) discussion here revealed. 
>>>> 
>>>>     Many people pick java for the some reason that James Gosling wrote quite 
>>>>     some time ago, regarding it working correctly.  The JLS and JMM decoupling 
>>>>     of "behaviors" causes software to break with "corrupted data" (word tearing 
>>>>     included) and that's something which I don't think is on the radar for as 
>>>>     many people writing Java software, as seems to be expected by the aggressive 
>>>>     demands on the JMM being perfectly understood and adhered to. 
>>>> 
>>>>     Gregg 
>>>> 
>>>>     On 4/15/2013 2:18 PM, oleksandr otenko wrote: 
>>>> 
>>>> 
>>>>         On 15/04/2013 17:55, thurstonn wrote: 
>>>> 
>>>>             oleksandr otenko wrote 
>>>> 
>>>>                 When "data race" means "broken logic", there must be a place 
>>>>                 where that 
>>>>                 logic is defined. 
>>>> 
>>>>             Yes, there is an assumption in my OP that "data-race" ==> 
>>>>             "incorrect" (or 
>>>>             "broken logic" in your words) 
>>>>             Vitaly does not necessarily equate "raciness" with "incorrectness" (and 
>>>>             probably Brian as well) and that's OK with me 
>>>> 
>>>>             oleksandr otenko wrote 
>>>> 
>>>>                 Literature on linearizability introduces a history of events 
>>>>                 (not the 
>>>>                 only place where it is done so). If a valid reordering of events 
>>>>                 produces a invalid history, you have a data race. But you need a 
>>>>                 definition of a valid history. 
>>>> 
>>>>             Yes, what I'm in (a quixotic?) search for is a process that does the 
>>>>             following: 
>>>>             given a set of operations that can execute across multiple threads 
>>>>             (like the 
>>>>             example in the OP) 
>>>>             -define a set of "execution histories" (your "history of events") 
>>>>             that are 
>>>>             possible given the MM in effect and consistent with the original 
>>>>             program's 
>>>>             set of operations (of course there will be multiple such histories) 
>>>>             -each "execution history" defines at least a partial ordering among 
>>>>             conflicting operations (r/w or w/w on the same shared data item) 
>>>>             -analyze each execution history for "correctness" 
>>>>             if each possible history is correct, then you're good 
>>>>             else add explicit happens-before relations. Repeat 
>>>> 
>>>>         This sounds awfully like Java PathFinder. 
>>>> 
>>>>         Not tractable for less-trivial code. (I couldn't test a semaphore-like 
>>>>         primitive 
>>>>         with more than 4 threads). 
>>>> 
>>>> 
>>>>         Alex 
>>>> 
>>>> 
>>>>             oleksandr otenko wrote 
>>>> 
>>>>                 Your argument is that any reordering of instructions in your 
>>>>                 example is 
>>>>                 a valid history. But in order to state that, we would need to 
>>>>                 see the 
>>>>                 rest of history. The subsequent use of local will determine the 
>>>>                 validity 
>>>>                 of history. 
>>>>                 Alex 
>>>> 
>>>>             Agreed (the example 'program' is simplistic at best). 
>>>> 
>>>>             What my original post described was exactly the kind of process (viz. an 
>>>>             acyclic serialization graph) that I'm in search of, but is applied to 
>>>>             database concurrency control.  The problems are very similar (turning 
>>>>             concurrent executions into serial, partially-ordered ones; 
>>>>             operations are 
>>>>             reads/writes of data items), but they are not exactly the same.  I was 
>>>>             wondering if we could use the same techniques (with a serialization 
>>>>             graph 
>>>>             ==> "execution graph") to analyze the "correctness" of, e.g. non-locking 
>>>>             concurrent algorithms/data structures. 
>>>> 
>>>>             Thurston 
>>>> 
>>>> 
>>>> 
>>>>             On 12/04/2013 19:55, thurstonn wrote: 
>>>> 
>>>>                 In thinking about whether code is thread-safe or not, one can 
>>>>                 attempt to 
>>>>                 identify whether it 'contains a data-race'.  If not, you're 
>>>>                 good.  Else 
>>>>                 you 
>>>>                 need to add an explicit happens-before relationship. 
>>>> 
>>>>                 Which begs the question: what exactly constitutes a 'data-race'? 
>>>>                   And here 
>>>>                 I'm interested in something a little more formal than the famed 
>>>>                 judicial 
>>>>                 judgement of obscenity (I know it when I see it) 
>>>> 
>>>>                 If you do a web search, you unfortunately get quite a few divergent 
>>>>                 definitions, many of which seem to be inconsistent. 
>>>>                 IIRC, the official JMM defines a data-race as any two conflicting 
>>>>                 operations 
>>>>                 from two or more threads on shared data (where at least one of 
>>>>                 the two 
>>>>                 operations is a write). 
>>>> 
>>>>                 Brian Goetz (in his excellent  article 
>>>> &lt;http://www.ibm.com/__developerworks/library/j-__jtp03304/&gt 
>>>> <http://www.ibm.com/developerworks/library/j-jtp03304/&gt>; ) 
>>>>                 defines 
>>>>                 data-race 
>>>>                 thusly: 
>>>> 
>>>>                 "A program is said to have a data race, and therefore not be a 
>>>>                 "properly 
>>>>                 synchronized" program, when there is a variable that is read by 
>>>>                 more than 
>>>>                 one thread, written by at least one thread, and the write and 
>>>>                 the reads 
>>>>                 are 
>>>>                 not ordered by a happens-before relationship." 
>>>> 
>>>>                 But this would mark the following code as a data-race 
>>>> 
>>>>                 int shared = 0 
>>>> 
>>>>                 Thread 1                  Thread 2                 Thread 3 
>>>>                 local = this.shared      this.shared = 10 local = this.shared 
>>>> 
>>>>                 This clearly meets his definition, yet I do not consider this a 
>>>>                 'data-race'. 
>>>> 
>>>>                 I've always relied on traditional database concurrency control 
>>>>                 theory (I 
>>>>                 still find the treatise by Bernstein, Hadzilacos, and Goodman to 
>>>>                 be the 
>>>>                 best), which has a formal definition of 'serializability', viz. 
>>>>                 that any 
>>>>                 transaction log is 'serializable', if and only if, its 
>>>>                 serialization graph 
>>>>                 is acyclic.  Why can we not use this as the basis for a formal 
>>>>                 definition 
>>>>                 of 
>>>>                 'data-race' (excluding the notion of commit and abort of course): 
>>>> 
>>>>                 "A program is said to have a data-race, if any legal (as 
>>>>                 prescribed by the 
>>>>                 MM) execution order produces a serialization graph that is *cyclic*" 
>>>> 
>>>>                 It has the advantage of a formal, mathematical model and 
>>>>                 although it is 
>>>>                 has 
>>>>                 historically been confined to databases (and transactions), it seems 
>>>>                 applicable to concurrent execution of any kind? 
>>>> 
>>>>                 Hoping that I don't get flamed. 
>>>> 
>>>> 
>>>> 
>>>>                 -- 
>>>>                 View this message in context: 
>>>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408.html 
>>>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408.html> 
>>>> 
>>>>                 Sent from the JSR166 Concurrency mailing list archive at Nabble.com. 
>>>> _________________________________________________ 
>>>>                 Concurrency-interest mailing list 
>>>>                 Concurrency-interest at .oswego 
>>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest 
>>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
>>>> 
>>>> 
>>>>             _________________________________________________ 
>>>>             Concurrency-interest mailing list 
>>>>             Concurrency-interest at .oswego 
>>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest 
>>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>>             -- 
>>>>             View this message in context: 
>>>> http://jsr166-concurrency.__10961.n7.nabble.com/On-A-__Formal-Definition-of-Data-__Race-tp9408p9425.html 
>>>> <http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9425.html> 
>>>> 
>>>>             Sent from the JSR166 Concurrency mailing list archive at Nabble.com. 
>>>>             _________________________________________________ 
>>>>             Concurrency-interest mailing list 
>>>>             Concurrency-interest at cs.__oswego.edu 
>>>>             <mailto:Concurrency-interest at cs.oswego.edu> 
>>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest 
>>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
>>>> 
>>>> 
>>>>         _________________________________________________ 
>>>>         Concurrency-interest mailing list 
>>>>         Concurrency-interest at cs.__oswego.edu 
>>>>         <mailto:Concurrency-interest at cs.oswego.edu> 
>>>> http://cs.oswego.edu/mailman/__listinfo/concurrency-interest 
>>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
>>>> 
>>>> 
>>>> 
>>>>     _________________________________________________ 
>>>>     Concurrency-interest mailing list 
>>>>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
>>>>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest 
>>>> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> 
>>>> 
>>> 
>> 
>> _______________________________________________ 
>> Concurrency-interest mailing list 
>> Concurrency-interest at cs.oswego.edu 
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>> 
>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130416/ccd035cc/attachment-0001.html>

From zhong.j.yu at gmail.com  Wed Apr 17 11:29:19 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 17 Apr 2013 10:29:19 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516E4383.1020103@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
Message-ID: <CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>

On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:
> Couldn't JIT hoist the non-volatile writes out of the loop?

Certainly, sorry if my statement sounds too absolute.

> For example, the following code...

But, is this a valid example? Can JMM really reorder around
System.out.println()?

> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
>     shared = 2 * i;
> }
>
> ... could be transformed into ...
>
> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
> }
>
> shared = 2 * 1_000_000_000;
>
> ... If so, then the non-volatile write may not happen for a very long time.
>
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>
> On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <thurston at nomagicsoftware.com>
> wrote:
>
> Vitaly Davidovich wrote
>
> The code works as-is.
>
> Absolutely.  volatile is not needed for correctness
>
> Vitaly Davidovich wrote
>
> Why?
>
> Well, for performance reasons given the 'undefined/indefinite' visibility of
> #hash to other threads.
> At least according to the JMM (which has nothing to say about CPU cache
> coherency), it is *possible* that each distinct thread that invoked
> #hashCode() *could* result in a recalculation of the hash.
>
> In practice though, application threads contain very frequent
> synchronization actions, or other operations that force VM to
> flush/reload. So it won't take very long for any non-volatile write in
> one thread to become visible to other threads.
>
> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
> keyset and for some unknown reason invoking #hashCode() on each key.
> If #hash was declared volatile, although there is no guarantee that #hash
> would only be calculated once, it is guaranteed that once a write to main
> memory was completed, every *subsequent* (here meaning after the write to
>
> In JMM though, we cannot even express this guarantee. Say we have
> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
> volatile-reads #hash, and if it's 0, calculates and volatile-writes
> #hash which takes 100 ns. We can find no guarantee from JMM that
> there's only one write; it's legal that every thread sees 0 from the
> volatile read.
>
> Zhong Yu
>
> main memory) read no matter from which thread would see #hash != 0 and
> therefore skip the calculation.
>
>
>
> Vitaly Davidovich wrote
>
> String is too high profile (especially
> hashing it) to do the "naive" thing.
>
> Nothing wrong with being naive; naive can be charming.
>
>
> Vitaly Davidovich wrote
>
> Also, some architectures pay a
> penalty for volatile loads and you'd incur that each time.
>
> Fair point; the JDK authors only get one shot and they can't assume that
> volatile reads are cheap
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From vitalyd at gmail.com  Wed Apr 17 11:44:17 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 11:44:17 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
Message-ID: <CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>

I actually expect that the optimizer *would* do this transformation on
plain fields (provided it doesn't break intra-thread semantics, of course)
because it's a perf gain.

Don't know how much JIT can see through println as it ultimately calls into
runtime and OS functions, so I'd guess it doesn't know enough or simply
plays conservative here.  However, Nathan's point is still valid even if
example isn't necessarily the best one.  If you had "pure" java code
instead of an I/O call that took significant time to execute, the write
would be delayed.  I'm not sure why that matters though for benign data
races.  Clearly if you need immediate visibility, you code for that
specifically.
On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
> <nathan.reynolds at oracle.com> wrote:
> > Couldn't JIT hoist the non-volatile writes out of the loop?
>
> Certainly, sorry if my statement sounds too absolute.
>
> > For example, the following code...
>
> But, is this a valid example? Can JMM really reorder around
> System.out.println()?
>
> > for (i = 0; i < 1_000_000_000; i++)
> > {
> >     System.out.println(i);
> >     shared = 2 * i;
> > }
> >
> > ... could be transformed into ...
> >
> > for (i = 0; i < 1_000_000_000; i++)
> > {
> >     System.out.println(i);
> > }
> >
> > shared = 2 * 1_000_000_000;
> >
> > ... If so, then the non-volatile write may not happen for a very long
> time.
> >
> > Nathan Reynolds | Architect | 602.333.9091
> > Oracle PSR Engineering | Server Technology
> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
> >
> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <thurston at nomagicsoftware.com
> >
> > wrote:
> >
> > Vitaly Davidovich wrote
> >
> > The code works as-is.
> >
> > Absolutely.  volatile is not needed for correctness
> >
> > Vitaly Davidovich wrote
> >
> > Why?
> >
> > Well, for performance reasons given the 'undefined/indefinite'
> visibility of
> > #hash to other threads.
> > At least according to the JMM (which has nothing to say about CPU cache
> > coherency), it is *possible* that each distinct thread that invoked
> > #hashCode() *could* result in a recalculation of the hash.
> >
> > In practice though, application threads contain very frequent
> > synchronization actions, or other operations that force VM to
> > flush/reload. So it won't take very long for any non-volatile write in
> > one thread to become visible to other threads.
> >
> > Imagine a long-lived Map<String, ?>; and many threads accessing the map's
> > keyset and for some unknown reason invoking #hashCode() on each key.
> > If #hash was declared volatile, although there is no guarantee that #hash
> > would only be calculated once, it is guaranteed that once a write to main
> > memory was completed, every *subsequent* (here meaning after the write to
> >
> > In JMM though, we cannot even express this guarantee. Say we have
> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
> > #hash which takes 100 ns. We can find no guarantee from JMM that
> > there's only one write; it's legal that every thread sees 0 from the
> > volatile read.
> >
> > Zhong Yu
> >
> > main memory) read no matter from which thread would see #hash != 0 and
> > therefore skip the calculation.
> >
> >
> >
> > Vitaly Davidovich wrote
> >
> > String is too high profile (especially
> > hashing it) to do the "naive" thing.
> >
> > Nothing wrong with being naive; naive can be charming.
> >
> >
> > Vitaly Davidovich wrote
> >
> > Also, some architectures pay a
> > penalty for volatile loads and you'd incur that each time.
> >
> > Fair point; the JDK authors only get one shot and they can't assume that
> > volatile reads are cheap
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7176ab07/attachment.html>

From brian at briangoetz.com  Wed Apr 17 11:58:37 2013
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 17 Apr 2013 11:58:37 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <1366178049325-9472.post@n7.nabble.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
	<516DCE46.4030605@briangoetz.com>
	<1366178049325-9472.post@n7.nabble.com>
Message-ID: <516EC6AD.3060005@briangoetz.com>

> Hmm, let me quote:
> "A program that accesses a mutable variable from multiple threads without
> synchronization is a broken program" -- sound familiar?
> Clearly String#hashCode() qualifies, and yet all agree that it *is*
> "correct"

Yes, sometimes when teaching, you have to say things that are only 
99.99% correct so that people will get the message clearly.

The folks on this list live in that remaining .01%.  Mostly so that the 
rest of the world doesn't have to.


From nathan.reynolds at oracle.com  Wed Apr 17 12:15:02 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 17 Apr 2013 09:15:02 -0700
Subject: [concurrency-interest] New version of Java Concurrent Animated
In-Reply-To: <516E4A12.1060903@javaspecialists.eu>
References: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>
	<516E4276.90504@oracle.com> <516E4A12.1060903@javaspecialists.eu>
Message-ID: <516ECA86.3050303@oracle.com>

Shouldn't the code sleep for 250+ seconds to give enough time for 
Thread.start()?  ;)

Thanks for testing this.  This raises an interesting case. Interleave 
threads so that readLock().lock() and writeLock().lock() are alternately 
called.  If this behavior is followed, then the reader will have 
exclusive access to the critical region since it comes between two writers.

Isn't this a throughput problem?  Shouldn't all of the readers be 
allowed to enter the critical region together and thus improve 
throughput through the lock?  This means changing the behavior so that 
writers get high priority and always enter the lock first otherwise they 
will starve.  When all writers are done, all of the readers will be able 
to enter the lock concurrently.  What's the problem with this scheme?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/17/2013 12:06 AM, Dr Heinz M. Kabutz wrote:
> Interesting thing about Victor's animations is that they hook directly 
> into what is happening inside the concurrency constructs.  Thus if you 
> run that particular example in Java 5, you will see how the readers 
> keep on getting in, even if a writer is waiting.  And in Java 6+ it 
> actually does work as Victor's animation shows, as can be also seen if 
> you run this code:
>
> import java.util.concurrent.locks.*;
>
> public class ReentrantReadWriteLockTest {
>     public static void main(String[] args) throws InterruptedException {
>         final ReadWriteLock rwlock = new ReentrantReadWriteLock();
>
>         // Thread #1 calls writeLock().lock() and enters the critical 
> region.
>         waitForWriteLock(rwlock, "t1");
>         Thread.sleep(100);
>
>         // Thread #2 calls readLock().lock() and blocks.
>         waitForReadLock(rwlock, "t2");
>         Thread.sleep(100);
>
>         // Thread #3 calls readLock().lock() and blocks.
>         waitForReadLock(rwlock, "t3");
>         Thread.sleep(100);
>
>         // Thread #4 calls readLock().lock() and blocks.
>         waitForReadLock(rwlock, "t4");
>         Thread.sleep(100);
>
>         // Thread #5 calls readLock().lock() and blocks.
>         waitForReadLock(rwlock, "t5");
>         Thread.sleep(100);
>
>         // Thread#6 calls writeLock ().lock() and blocks.
>         waitForWriteLock(rwlock, "t6");
>
>         // Thread#1 releases the lock.
>     }
>
>     private static void waitForReadLock(final ReadWriteLock rwlock, 
> String name) {
>         new Thread(name) {
>             public void run() {
>                 System.out.println("Thread " + getName() + " waiting 
> for read lock");
>                 rwlock.readLock().lock();
>                 try {
>                     System.out.println("Thread " + getName() + " is in 
> read section");
>                     Thread.sleep(2000);
>                 } catch (InterruptedException e) {
>                     Thread.currentThread().interrupt();
>                 } finally {
>                     rwlock.readLock().unlock();
>                 }
>             }
>         }.start();
>     }
>
>     private static void waitForWriteLock(final ReadWriteLock rwlock, 
> String name) {
>         new Thread(name) {
>             public void run() {
>                 System.out.println("Thread " + getName() + " waiting 
> for write lock");
>                 rwlock.writeLock().lock();
>                 try {
>                     System.out.println("Thread " + getName() + " is in 
> write section");
>                     Thread.sleep(2000);
>                 } catch (InterruptedException e) {
>                     Thread.currentThread().interrupt();
>                 } finally {
>                     rwlock.writeLock().unlock();
>                 }
>             }
>         }.start();
>     }
> }
>
> The output is:
>
> Thread t1 waiting for write lock
> Thread t1 is in write section
> Thread t2 waiting for read lock
> Thread t3 waiting for read lock
> Thread t4 waiting for read lock
> Thread t5 waiting for read lock
> Thread t6 waiting for write lock
> Thread t2 is in read section
> Thread t3 is in read section
> Thread t4 is in read section
> Thread t5 is in read section
> Thread t6 is in write section
>
> Regards
>
> Heinz
> -- 
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
>
>
> Nathan Reynolds wrote:
>> Nice animation!  I noticed a couple of problems.
>>
>> The ConcurrentHashMap demo fails to draw the threads if you keep 
>> hitting putIfAbsent.  The threads are drawn beyond the bottom.
>>
>> If I remember right ReentrantReadWriteLock gives priority to writer 
>> threads.  If that is the case then the ReadWriteLock demo is not 
>> quite right.
>>
>> Consider this scenario.
>>
>>  1. Thread #1 calls writeLock().lock() and enters the critical region.
>>  2. Thread #2 calls readLock().lock() and blocks.
>>  3. Thread #3 calls readLock().lock() and blocks.
>>  4. Thread #4 calls readLock().lock() and blocks.
>>  5. Thread #5 calls readLock().lock() and blocks.
>>  6. Thread #6 calls writeLock().lock() and blocks.
>>  7. Thread #1 releases the lock.
>>  8. If writer threads are given priority, then Thread #6 should enter
>>     the critical region.
>>
>> In the demo, threads #2 to #5 are given the lock in step 8.
>>
>> Nathan Reynolds 
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 4/16/2013 6:02 PM, Victor Grazi wrote:
>>> Hi, just a quick note to let everyone know there is a new version of 
>>> Java Concurrent Animated on SourceForge.
>>>
>>> Some of the new features:
>>> * A new animation showing good old synchronized/wait/notify. Kind of 
>>> surprising when you see it in action.
>>> * Thread states are now color coded, and includes a novel state 
>>> diagram kind courtesy of Dr. Heinz Kabutz and his Concurrency 
>>> Specialist Training 
>>> <http://www.javaspecialists.eu/courses/concurrency.jsp> course.
>>>
>>> Check it out!
>>> Thanks
>>> Victor Grazi
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>      
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>    

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/4332a13a/attachment.html>

From nathan.reynolds at oracle.com  Wed Apr 17 12:19:07 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 17 Apr 2013 09:19:07 -0700
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
Message-ID: <516ECB7B.4030806@oracle.com>

What version of JDK are you using?  JDK 8 has or will have a new version 
of ConcurrentHashMap.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/17/2013 6:31 AM, Mudit Verma wrote:
> Hi All,
>
> I recently performed a scalability test (very aggressive and may be 
> not practical, anyhow) on put operation of ConcurrentHashMap.
>
> Test: Each thread is trying to put (Same Key, Random Value) in HashMap 
> in a tight loop. Therefore, all the threads will hit the same location 
> on hashMap and will cause contention.
>
> What is more surprising is, when each thread continue to do another 
> put one after the other, avg time taken in one put operation is lesser 
> than when a thread do some other work between two put operations.
>
> We continue to see the increase in per operation time by increasing 
> the work done in between.  This is very counter intuitive. Only after 
> a work of about 10,000 - 20,000 cycles in between, per op time comes 
> down.
>
> When I read the code, I found out that put op first try to use CAS to 
> aquire the lock(64 times on multicore), only if it could not acquire 
> the lock on segment through CASing, it goes for ReentrantLocking 
> (which suspend threads .. ).
>
> We also tweaked, the number of times CAS (from 0 to 10,000) is used 
> before actually going for ReentrantLocking.   Attached is the graph.
>
> One interesting thing to note. As we increase the work between two 
> ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst 
> affected with the spike. Therefore, I assume that, spike comes from 
> ReentractLocking even when there is a mixture of two (CAS+Lock).
>
> Code Skeleton: For each Thread
>
> While() {
>   hashMap.put(K,randomValue);     // K is same for each thread
>   ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>
> }
>
>  Machine: 48 core NUMA
> Threads used:  32 (each one is pinned to a core).
> #ops: In total 51200000 (each thread with 160000 ops)
>
> That's like saying, if I do something else in between my operations 
> (upto a limit), contention will increase.  Very strange.
>
> Does anyone of you know why is it happening?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7d94d447/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 12:19:17 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 17:19:17 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
Message-ID: <516ECB85.9060202@oracle.com>

What is this "immediate" anyway?

It is actually "before anything else in this thread that's temporal"

Alex


On 17/04/2013 16:44, Vitaly Davidovich wrote:
>
> I actually expect that the optimizer *would* do this transformation on 
> plain fields (provided it doesn't break intra-thread semantics, of 
> course) because it's a perf gain.
>
> Don't know how much JIT can see through println as it ultimately calls 
> into runtime and OS functions, so I'd guess it doesn't know enough or 
> simply plays conservative here.  However, Nathan's point is still 
> valid even if example isn't necessarily the best one.  If you had 
> "pure" java code instead of an I/O call that took significant time to 
> execute, the write would be delayed.  I'm not sure why that matters 
> though for benign data races.  Clearly if you need immediate 
> visibility, you code for that specifically.
>
> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com 
> <mailto:zhong.j.yu at gmail.com>> wrote:
>
>     On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>     wrote:
>     > Couldn't JIT hoist the non-volatile writes out of the loop?
>
>     Certainly, sorry if my statement sounds too absolute.
>
>     > For example, the following code...
>
>     But, is this a valid example? Can JMM really reorder around
>     System.out.println()?
>
>     > for (i = 0; i < 1_000_000_000; i++)
>     > {
>     >     System.out.println(i);
>     >     shared = 2 * i;
>     > }
>     >
>     > ... could be transformed into ...
>     >
>     > for (i = 0; i < 1_000_000_000; i++)
>     > {
>     >     System.out.println(i);
>     > }
>     >
>     > shared = 2 * 1_000_000_000;
>     >
>     > ... If so, then the non-volatile write may not happen for a very
>     long time.
>     >
>     > Nathan Reynolds | Architect | 602.333.9091
>     > Oracle PSR Engineering | Server Technology
>     > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>     >
>     > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>     <thurston at nomagicsoftware.com <mailto:thurston at nomagicsoftware.com>>
>     > wrote:
>     >
>     > Vitaly Davidovich wrote
>     >
>     > The code works as-is.
>     >
>     > Absolutely.  volatile is not needed for correctness
>     >
>     > Vitaly Davidovich wrote
>     >
>     > Why?
>     >
>     > Well, for performance reasons given the 'undefined/indefinite'
>     visibility of
>     > #hash to other threads.
>     > At least according to the JMM (which has nothing to say about
>     CPU cache
>     > coherency), it is *possible* that each distinct thread that invoked
>     > #hashCode() *could* result in a recalculation of the hash.
>     >
>     > In practice though, application threads contain very frequent
>     > synchronization actions, or other operations that force VM to
>     > flush/reload. So it won't take very long for any non-volatile
>     write in
>     > one thread to become visible to other threads.
>     >
>     > Imagine a long-lived Map<String, ?>; and many threads accessing
>     the map's
>     > keyset and for some unknown reason invoking #hashCode() on each key.
>     > If #hash was declared volatile, although there is no guarantee
>     that #hash
>     > would only be calculated once, it is guaranteed that once a
>     write to main
>     > memory was completed, every *subsequent* (here meaning after the
>     write to
>     >
>     > In JMM though, we cannot even express this guarantee. Say we have
>     > threads T1...Tn, each thread Ti burns `i` seconds CPU time
>     first, then
>     > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>     > #hash which takes 100 ns. We can find no guarantee from JMM that
>     > there's only one write; it's legal that every thread sees 0 from the
>     > volatile read.
>     >
>     > Zhong Yu
>     >
>     > main memory) read no matter from which thread would see #hash !=
>     0 and
>     > therefore skip the calculation.
>     >
>     >
>     >
>     > Vitaly Davidovich wrote
>     >
>     > String is too high profile (especially
>     > hashing it) to do the "naive" thing.
>     >
>     > Nothing wrong with being naive; naive can be charming.
>     >
>     >
>     > Vitaly Davidovich wrote
>     >
>     > Also, some architectures pay a
>     > penalty for volatile loads and you'd incur that each time.
>     >
>     > Fair point; the JDK authors only get one shot and they can't
>     assume that
>     > volatile reads are cheap
>     >
>     >
>     >
>     >
>     >
>     > --
>     > View this message in context:
>     >
>     http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>     > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     >
>     >
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/0cc0185d/attachment.html>

From william.louth at jinspired.com  Wed Apr 17 12:22:30 2013
From: william.louth at jinspired.com (william.louth at jinspired.com)
Date: Wed, 17 Apr 2013 16:22:30 +0000
Subject: [concurrency-interest] Fwd:  : Re:  Latency in starting threads
Message-ID: <W157312960355981366215750@atl4webmail40>

-----Original Message-----
From: william.louth at jinspired.com [mailto:william.louth at jinspired.com]
Sent: Wednesday, April 17, 2013 11:45 AM
To: concurrency-interest-bounces at cs.oswego.edu
Subject: Re: : Re: [concurrency-interest] Latency in starting threads

I did a quick test using the following changed code to shed a little bit more light on the behavior and there is a pattern in all of this looking at the count increments in the table listed here.

http://www.jinspired.com/wp-content/uploads/2013/04/heinz.thread.start_.metering.jpg

Very strange to see the counts (representing threads created between max increments) cluster around 136x. Notice also the min at each new stage is relatively low so the impact is not in constant at an average level but more so outlier. I could turn on distributions if need be but maybe what I have seen can be first repeated on some other machine using another measurement library.

import com.jinspired.jxinsight.probes.Probes;

import java.text.DecimalFormat;
import java.text.NumberFormat;

public class ThreadTest {
  public static void main(String[] args) throws Throwable {
    long threads = 0;
    long max = 0;

    final NumberFormat fmt = new DecimalFormat("000,000");
    final Probes.Context ctx = Probes.context();
    final Probes.Name mn = Probes.parse("clock.time");

    Probes.Probe p = ctx.create(Probes.name(fmt.format(max)));
    Probes.Reading r = p.reading(mn);

    while(true) {

      p.begin();

      final Thread thread = new Thread();
      thread.start(); // should finish almost immediately
      thread.join();// short delay, hopefully

      p.end();

      threads++;

      long time = r.getDelta() / 1000; // microseconds to milliseconds

      if (time > max) {

        max = time;
        System.out.println("time = " + time + ", threads = " + threads);

        r = (p = ctx.create(Probes.name(fmt.format(max)))).reading(mn);

      }

    }
  }
}

-----Original Message-----
To: 'William Louth'
Subject: Fwd: Re: [concurrency-interest] Latency in starting threads

-------------------------------------------------------------
 Hi David,

 I understand it is valid for t.start() to wait until the new thread has really started. I was just surprised, as I expected it to be basically an asynchronous call that spawns something but does not wait for it to actually start.

 Also, with the join(), I realize that this is at the end of the Java view of the thread. However, I did check that the threads were indeed shutting down by looking at the process in top and also dumping threads.

 Heinz

 David Holmes wrote: Heinz,

It is perfected valid for t.start() to start and context switch to the new thread. That is up to the OS scheduler.

Thread.join only indicates logical termination at the Java level. The native thread will still have to exit at the VM level and OS level, both of which can hit bottlenecks and cause unexpected numbers of threads to still be active. That in turn can impact thread creation - which happens in start().

Cheers,
David
-----Original Message-----
From:concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz M. Kabutz
Sent: Wednesday, 17 April 2013 5:31 AM
To: Nathan Reynolds
Cc:concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Latency in starting threads


 Right, I would not have sent a notifyAll() if it had just been 250ms for the thread to wake up. However, I was always under the wrong impression that thread.start() would return pretty much immediately. That is the slow part, not the constructor call. Try it out if you find a Mac OS X machine with at least 1.7.0_06 on it.

 And just to eliminate any confusion - I waited for each thread to die before I start the next one. So we never have an excessively large number of threads around at any one time.
RegardsHeinz-- Dr Heinz M. Kabutz (PhD CompSci)Author of "The Java(tm) Specialists' Newsletter"Sun Java ChampionIEEE Certified Software Development Professionalhttp://www.javaspecialists.euTel: +30 69 75 595 262Skype: kabutz 

 Nathan Reynolds wrote: When I started reading your email, I figured you meant the time from calling start() to the time the created thread actually started running. I've seen this take a considerable amount of time on Windows XP (not 250 seconds though) and has to be considered if one is trying to account for latency. I was surprised to find that you meant the time for the constructor and start() call itself.

Nathan Reynolds | Architect | 602.333.9091
OraclePSR Engineering | Server Technology

 On 4/16/2013 5:54 AM, Dr Heinz M. Kabutz wrote:

Good day my fellow concurrency enthusiasts! 

 Yesterday, whilst teaching my Concurrency Specialist Course, I wanted to demonstrate to my class how slow it was starting threads and how much better it is to use a FixedThreadPool. The question that I wanted to answer was: How many microseconds does it take on average to start a simple thread and what is the maximum time it could take? 

 We all know that it can take in the milliseconds range to do the following: 

 Thread t = new Thread(); // even without it actually doing anything 
 t.start(); 

 This is one of the reasons why the fixed thread pool only starts the threads as we submit jobs to it, since the up-front cost might not be worth the wait. 

 But how long do you think the *maximum* was that I had to wait for t.start() to return? 100ms? 200ms? 

 Actually, the longest I had to wait turned out to be about 250 seconds. Yes. That is *seconds*, not *milliseconds*. Just to start a single thread. 

 This is most certainly a bug in the OpenJDK on Mac OS X. We did not see this behaviour on Linux nor on Windows 7. 

 The bug started in OpenJDK 1.7.0_06. Prior to that it hardly ever took longer than 30ms to start a single thread. 

 java version "1.7.0_05" 
 heinz$ java ThreadLeakMac2 
 time = 1, threads = 4 
 time = 2, threads = 346 
 time = 4, threads = 7378 
 time = 7, threads = 9614 
 time = 12, threads = 10027 
 time = 14, threads = 10063 
 time = 17, threads = 26965 
 time = 38, threads = 27013 
 time = 39, threads = 452053 

 java version "1.7.0_06" 
 heinz$ java ThreadLeakMac2 
 time = 1, threads = 6 
 time = 2, threads = 256 
 time = 6, threads = 373 
 *snip* 
 time = 111, threads = 42592 
 time = 200, threads = 49419 
 time = 333, threads = 58976 
 *snip* 
 time = 3245, threads = 202336 
 time = 3706, threads = 203702 
 *snip* 
 time = 5835, threads = 267872 
 time = 6455, threads = 269238 
 time = 9170, threads = 270603 

 In my code, I make sure that the thread has stopped before creating the next one by calling join(). 

 public class ThreadLeakMac2 { 
 public static void main(String[] args) throws InterruptedException { 
 long threads = 0; 
 long max = 0; 
 while(true) { 
 long time = System.currentTimeMillis(); 
 Thread thread = new Thread(); 
 thread.start(); // should finish almost immediately 
 time = System.currentTimeMillis() - time; 
 thread.join(); // short delay, hopefully 
 threads++; 
 if (time > max) { 
 max = time; 
 System.out.println("time = " + time + ", threads = " + threads); 
 } 
 } 
 } 
 } 

 This would be another nice test case for Alexey's concurrency stress test harness. 

 (I will also post this to the macosx-port-dev list.) 

 Regards 

 Heinz 


------------------------------------------------------------
_______________________________________________Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest








From nathan.reynolds at oracle.com  Wed Apr 17 12:24:08 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 17 Apr 2013 09:24:08 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516EA483.1080007@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com> <516E7710.7040507@oracle.com>
	<516EA2C1.6020802@cytetech.com> <516EA483.1080007@oracle.com>
Message-ID: <516ECCA8.6020500@oracle.com>

If I have my details correct, the Intel x86 processor will fuse 2 
back-to-back stores into 1 store if they are for the same address.  JIT 
has a lot more information about the program than the processor and 
might "fuse" stores to the same field.  The loop hoisting example is one 
such case.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/17/2013 6:32 AM, oleksandr otenko wrote:
> I am not sure what you are saying.
>
> I am only saying that moving the write out of the loop is possible 
> only if we can prove it is not used inside the loop. Multiple writes 
> are fused in exactly the same way as multiple reads are fused. 
> Elimination is a form of reordering.
>
> Alex
>
> On 17/04/2013 14:25, Gregg Wonderly wrote:
>> Actually the compiler doesn't do that for non-volatile access, as 
>> I've been going on about with the loop exit hoisting example.  This 
>> transformation is exactly the kind of thing that I'd expect to see 
>> happen, and this is the "unexplainable behavior" that will create 
>> alarm, because mutations of "shared" are not visible, except a loop 
>> exit.
>>
>> Gregg Wonderly
>>
>> On 4/17/2013 5:18 AM, oleksandr otenko wrote:
>>> You need to prove System.out.println isn't using shared.
>>>
>>> Alex
>>>
>>> On 17/04/2013 07:38, Nathan Reynolds wrote:
>>>> Couldn't JIT hoist the non-volatile writes out of the loop?  For 
>>>> example, the
>>>> following code...
>>>>
>>>> for (i = 0; i < 1_000_000_000; i++)
>>>> {
>>>>     System.out.println(i);
>>>>     shared = 2 * i;
>>>> }
>>>>
>>>> ... could be transformed into ...
>>>>
>>>> for (i = 0; i < 1_000_000_000; i++)
>>>> {
>>>>     System.out.println(i);
>>>> }
>>>>
>>>> shared = 2 * 1_000_000_000;
>>>>
>>>> ... If so, then the non-volatile write may not happen for a very 
>>>> long time.
>>>>
>>>> Nathan Reynolds 
>>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>> | Architect | 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>>> On Tue, Apr 16, 2013 at 8:51 PM, 
>>>>> thurstonn<thurston at nomagicsoftware.com>  wrote:
>>>>>> Vitaly Davidovich wrote
>>>>>>> The code works as-is.
>>>>>> Absolutely.  volatile is not needed for correctness
>>>>>>
>>>>>> Vitaly Davidovich wrote
>>>>>>> Why?
>>>>>> Well, for performance reasons given the 'undefined/indefinite' 
>>>>>> visibility of
>>>>>> #hash to other threads.
>>>>>> At least according to the JMM (which has nothing to say about CPU 
>>>>>> cache
>>>>>> coherency), it is *possible* that each distinct thread that invoked
>>>>>> #hashCode() *could* result in a recalculation of the hash.
>>>>> In practice though, application threads contain very frequent
>>>>> synchronization actions, or other operations that force VM to
>>>>> flush/reload. So it won't take very long for any non-volatile 
>>>>> write in
>>>>> one thread to become visible to other threads.
>>>>>> Imagine a long-lived Map<String, ?>; and many threads accessing 
>>>>>> the map's
>>>>>> keyset and for some unknown reason invoking #hashCode() on each key.
>>>>>> If #hash was declared volatile, although there is no guarantee 
>>>>>> that #hash
>>>>>> would only be calculated once, it is guaranteed that once a write 
>>>>>> to main
>>>>>> memory was completed, every *subsequent* (here meaning after the 
>>>>>> write to
>>>>> In JMM though, we cannot even express this guarantee. Say we have
>>>>> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, 
>>>>> then
>>>>> volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>>>> #hash which takes 100 ns. We can find no guarantee from JMM that
>>>>> there's only one write; it's legal that every thread sees 0 from the
>>>>> volatile read.
>>>>>
>>>>> Zhong Yu
>>>>>
>>>>>> main memory) read no matter from which thread would see #hash != 
>>>>>> 0 and
>>>>>> therefore skip the calculation.
>>>>>>
>>>>>>
>>>>>>
>>>>>> Vitaly Davidovich wrote
>>>>>>> String is too high profile (especially
>>>>>>> hashing it) to do the "naive" thing.
>>>>>> Nothing wrong with being naive; naive can be charming.
>>>>>>
>>>>>>
>>>>>> Vitaly Davidovich wrote
>>>>>>> Also, some architectures pay a
>>>>>>> penalty for volatile loads and you'd incur that each time.
>>>>>> Fair point; the JDK authors only get one shot and they can't 
>>>>>> assume that
>>>>>> volatile reads are cheap
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> -- 
>>>>>> View this message in 
>>>>>> context:http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/4a6417e9/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Apr 17 12:29:48 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 17 Apr 2013 09:29:48 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
Message-ID: <516ECDFC.7010003@oracle.com>

Sorry for the poor choice of method (i.e. println).  I needed something 
that would (1) take enough time to make the loop take a non-trivial 
amount of time and (2) could potentially be proved that it doesn't 
access "shared", (3) didn't have synchronization (or at least we 
wouldn't think of it as having synchronization) and (4) couldn't be 
discarded by JIT because it doesn't change global state.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/17/2013 8:44 AM, Vitaly Davidovich wrote:
>
> I actually expect that the optimizer *would* do this transformation on 
> plain fields (provided it doesn't break intra-thread semantics, of 
> course) because it's a perf gain.
>
> Don't know how much JIT can see through println as it ultimately calls 
> into runtime and OS functions, so I'd guess it doesn't know enough or 
> simply plays conservative here.  However, Nathan's point is still 
> valid even if example isn't necessarily the best one.  If you had 
> "pure" java code instead of an I/O call that took significant time to 
> execute, the write would be delayed.  I'm not sure why that matters 
> though for benign data races.  Clearly if you need immediate 
> visibility, you code for that specifically.
>
> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com 
> <mailto:zhong.j.yu at gmail.com>> wrote:
>
>     On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>     wrote:
>     > Couldn't JIT hoist the non-volatile writes out of the loop?
>
>     Certainly, sorry if my statement sounds too absolute.
>
>     > For example, the following code...
>
>     But, is this a valid example? Can JMM really reorder around
>     System.out.println()?
>
>     > for (i = 0; i < 1_000_000_000; i++)
>     > {
>     >     System.out.println(i);
>     >     shared = 2 * i;
>     > }
>     >
>     > ... could be transformed into ...
>     >
>     > for (i = 0; i < 1_000_000_000; i++)
>     > {
>     >     System.out.println(i);
>     > }
>     >
>     > shared = 2 * 1_000_000_000;
>     >
>     > ... If so, then the non-volatile write may not happen for a very
>     long time.
>     >
>     > Nathan Reynolds | Architect | 602.333.9091
>     > Oracle PSR Engineering | Server Technology
>     > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>     >
>     > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>     <thurston at nomagicsoftware.com <mailto:thurston at nomagicsoftware.com>>
>     > wrote:
>     >
>     > Vitaly Davidovich wrote
>     >
>     > The code works as-is.
>     >
>     > Absolutely.  volatile is not needed for correctness
>     >
>     > Vitaly Davidovich wrote
>     >
>     > Why?
>     >
>     > Well, for performance reasons given the 'undefined/indefinite'
>     visibility of
>     > #hash to other threads.
>     > At least according to the JMM (which has nothing to say about
>     CPU cache
>     > coherency), it is *possible* that each distinct thread that invoked
>     > #hashCode() *could* result in a recalculation of the hash.
>     >
>     > In practice though, application threads contain very frequent
>     > synchronization actions, or other operations that force VM to
>     > flush/reload. So it won't take very long for any non-volatile
>     write in
>     > one thread to become visible to other threads.
>     >
>     > Imagine a long-lived Map<String, ?>; and many threads accessing
>     the map's
>     > keyset and for some unknown reason invoking #hashCode() on each key.
>     > If #hash was declared volatile, although there is no guarantee
>     that #hash
>     > would only be calculated once, it is guaranteed that once a
>     write to main
>     > memory was completed, every *subsequent* (here meaning after the
>     write to
>     >
>     > In JMM though, we cannot even express this guarantee. Say we have
>     > threads T1...Tn, each thread Ti burns `i` seconds CPU time
>     first, then
>     > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>     > #hash which takes 100 ns. We can find no guarantee from JMM that
>     > there's only one write; it's legal that every thread sees 0 from the
>     > volatile read.
>     >
>     > Zhong Yu
>     >
>     > main memory) read no matter from which thread would see #hash !=
>     0 and
>     > therefore skip the calculation.
>     >
>     >
>     >
>     > Vitaly Davidovich wrote
>     >
>     > String is too high profile (especially
>     > hashing it) to do the "naive" thing.
>     >
>     > Nothing wrong with being naive; naive can be charming.
>     >
>     >
>     > Vitaly Davidovich wrote
>     >
>     > Also, some architectures pay a
>     > penalty for volatile loads and you'd incur that each time.
>     >
>     > Fair point; the JDK authors only get one shot and they can't
>     assume that
>     > volatile reads are cheap
>     >
>     >
>     >
>     >
>     >
>     > --
>     > View this message in context:
>     >
>     http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>     > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     >
>     >
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7abcdda5/attachment.html>

From vitalyd at gmail.com  Wed Apr 17 12:43:12 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 12:43:12 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ECB85.9060202@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
Message-ID: <CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>

Immediate means store is made globally visible before subsequent
instructions complete/retire.
On Apr 17, 2013 12:26 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
wrote:

>  What is this "immediate" anyway?
>
> It is actually "before anything else in this thread that's temporal"
>
> Alex
>
>
>  On 17/04/2013 16:44, Vitaly Davidovich wrote:
>
> I actually expect that the optimizer *would* do this transformation on
> plain fields (provided it doesn't break intra-thread semantics, of course)
> because it's a perf gain.
>
> Don't know how much JIT can see through println as it ultimately calls
> into runtime and OS functions, so I'd guess it doesn't know enough or
> simply plays conservative here.  However, Nathan's point is still valid
> even if example isn't necessarily the best one.  If you had "pure" java
> code instead of an I/O call that took significant time to execute, the
> write would be delayed.  I'm not sure why that matters though for benign
> data races.  Clearly if you need immediate visibility, you code for that
> specifically.
> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>
>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>> <nathan.reynolds at oracle.com> wrote:
>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>
>> Certainly, sorry if my statement sounds too absolute.
>>
>> > For example, the following code...
>>
>> But, is this a valid example? Can JMM really reorder around
>> System.out.println()?
>>
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> >     shared = 2 * i;
>> > }
>> >
>> > ... could be transformed into ...
>> >
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> > }
>> >
>> > shared = 2 * 1_000_000_000;
>> >
>> > ... If so, then the non-volatile write may not happen for a very long
>> time.
>> >
>> > Nathan Reynolds | Architect | 602.333.9091
>> > Oracle PSR Engineering | Server Technology
>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>> >
>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <
>> thurston at nomagicsoftware.com>
>> > wrote:
>> >
>> > Vitaly Davidovich wrote
>> >
>> > The code works as-is.
>> >
>> > Absolutely.  volatile is not needed for correctness
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Why?
>> >
>> > Well, for performance reasons given the 'undefined/indefinite'
>> visibility of
>> > #hash to other threads.
>> > At least according to the JMM (which has nothing to say about CPU cache
>> > coherency), it is *possible* that each distinct thread that invoked
>> > #hashCode() *could* result in a recalculation of the hash.
>> >
>> > In practice though, application threads contain very frequent
>> > synchronization actions, or other operations that force VM to
>> > flush/reload. So it won't take very long for any non-volatile write in
>> > one thread to become visible to other threads.
>> >
>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>> map's
>> > keyset and for some unknown reason invoking #hashCode() on each key.
>> > If #hash was declared volatile, although there is no guarantee that
>> #hash
>> > would only be calculated once, it is guaranteed that once a write to
>> main
>> > memory was completed, every *subsequent* (here meaning after the write
>> to
>> >
>> > In JMM though, we cannot even express this guarantee. Say we have
>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>> > there's only one write; it's legal that every thread sees 0 from the
>> > volatile read.
>> >
>> > Zhong Yu
>> >
>> > main memory) read no matter from which thread would see #hash != 0 and
>> > therefore skip the calculation.
>> >
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > String is too high profile (especially
>> > hashing it) to do the "naive" thing.
>> >
>> > Nothing wrong with being naive; naive can be charming.
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Also, some architectures pay a
>> > penalty for volatile loads and you'd incur that each time.
>> >
>> > Fair point; the JDK authors only get one shot and they can't assume that
>> > volatile reads are cheap
>> >
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/28d9fd8f/attachment-0001.html>

From vitalyd at gmail.com  Wed Apr 17 12:48:03 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 12:48:03 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ECDFC.7010003@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECDFC.7010003@oracle.com>
Message-ID: <CAHjP37FvO2BuA0DtLmcMXrQ+HypHY1-WbrckUa9jJmhkVei8MA@mail.gmail.com>

Sure, understood.  Let's pretend the code calls into some crypto or
compression that's pure java with the characteristics you mention instead
of println. :)
On Apr 17, 2013 12:30 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Sorry for the poor choice of method (i.e. println).  I needed something
> that would (1) take enough time to make the loop take a non-trivial amount
> of time and (2) could potentially be proved that it doesn't access
> "shared", (3) didn't have synchronization (or at least we wouldn't think of
> it as having synchronization) and (4) couldn't be discarded by JIT because
> it doesn't change global state.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/17/2013 8:44 AM, Vitaly Davidovich wrote:
>
> I actually expect that the optimizer *would* do this transformation on
> plain fields (provided it doesn't break intra-thread semantics, of course)
> because it's a perf gain.
>
> Don't know how much JIT can see through println as it ultimately calls
> into runtime and OS functions, so I'd guess it doesn't know enough or
> simply plays conservative here.  However, Nathan's point is still valid
> even if example isn't necessarily the best one.  If you had "pure" java
> code instead of an I/O call that took significant time to execute, the
> write would be delayed.  I'm not sure why that matters though for benign
> data races.  Clearly if you need immediate visibility, you code for that
> specifically.
> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>
>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>> <nathan.reynolds at oracle.com> wrote:
>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>
>> Certainly, sorry if my statement sounds too absolute.
>>
>> > For example, the following code...
>>
>> But, is this a valid example? Can JMM really reorder around
>> System.out.println()?
>>
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> >     shared = 2 * i;
>> > }
>> >
>> > ... could be transformed into ...
>> >
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> > }
>> >
>> > shared = 2 * 1_000_000_000;
>> >
>> > ... If so, then the non-volatile write may not happen for a very long
>> time.
>> >
>> > Nathan Reynolds | Architect | 602.333.9091
>> > Oracle PSR Engineering | Server Technology
>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>> >
>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <
>> thurston at nomagicsoftware.com>
>> > wrote:
>> >
>> > Vitaly Davidovich wrote
>> >
>> > The code works as-is.
>> >
>> > Absolutely.  volatile is not needed for correctness
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Why?
>> >
>> > Well, for performance reasons given the 'undefined/indefinite'
>> visibility of
>> > #hash to other threads.
>> > At least according to the JMM (which has nothing to say about CPU cache
>> > coherency), it is *possible* that each distinct thread that invoked
>> > #hashCode() *could* result in a recalculation of the hash.
>> >
>> > In practice though, application threads contain very frequent
>> > synchronization actions, or other operations that force VM to
>> > flush/reload. So it won't take very long for any non-volatile write in
>> > one thread to become visible to other threads.
>> >
>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>> map's
>> > keyset and for some unknown reason invoking #hashCode() on each key.
>> > If #hash was declared volatile, although there is no guarantee that
>> #hash
>> > would only be calculated once, it is guaranteed that once a write to
>> main
>> > memory was completed, every *subsequent* (here meaning after the write
>> to
>> >
>> > In JMM though, we cannot even express this guarantee. Say we have
>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>> > there's only one write; it's legal that every thread sees 0 from the
>> > volatile read.
>> >
>> > Zhong Yu
>> >
>> > main memory) read no matter from which thread would see #hash != 0 and
>> > therefore skip the calculation.
>> >
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > String is too high profile (especially
>> > hashing it) to do the "naive" thing.
>> >
>> > Nothing wrong with being naive; naive can be charming.
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Also, some architectures pay a
>> > penalty for volatile loads and you'd incur that each time.
>> >
>> > Fair point; the JDK authors only get one shot and they can't assume that
>> > volatile reads are cheap
>> >
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/e2d6cabb/attachment.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 12:49:44 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 17:49:44 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
Message-ID: <516ED2A8.1060506@oracle.com>

Yes, but not any instructions - only those that have temporal relationship.

So, for example, in this case even if shared is volatile, then volatile 
stores can still be moved out of the loop and fused into a single 
volatile store.

Alex


On 17/04/2013 17:43, Vitaly Davidovich wrote:
>
> Immediate means store is made globally visible before subsequent 
> instructions complete/retire.
>
> On Apr 17, 2013 12:26 PM, "oleksandr otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     What is this "immediate" anyway?
>
>     It is actually "before anything else in this thread that's temporal"
>
>     Alex
>
>
>     On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>
>>     I actually expect that the optimizer *would* do this
>>     transformation on plain fields (provided it doesn't break
>>     intra-thread semantics, of course) because it's a perf gain.
>>
>>     Don't know how much JIT can see through println as it ultimately
>>     calls into runtime and OS functions, so I'd guess it doesn't know
>>     enough or simply plays conservative here.  However, Nathan's
>>     point is still valid even if example isn't necessarily the best
>>     one.  If you had "pure" java code instead of an I/O call that
>>     took significant time to execute, the write would be delayed. 
>>     I'm not sure why that matters though for benign data races. 
>>     Clearly if you need immediate visibility, you code for that
>>     specifically.
>>
>>     On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com
>>     <mailto:zhong.j.yu at gmail.com>> wrote:
>>
>>         On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>         <nathan.reynolds at oracle.com
>>         <mailto:nathan.reynolds at oracle.com>> wrote:
>>         > Couldn't JIT hoist the non-volatile writes out of the loop?
>>
>>         Certainly, sorry if my statement sounds too absolute.
>>
>>         > For example, the following code...
>>
>>         But, is this a valid example? Can JMM really reorder around
>>         System.out.println()?
>>
>>         > for (i = 0; i < 1_000_000_000; i++)
>>         > {
>>         >     System.out.println(i);
>>         >     shared = 2 * i;
>>         > }
>>         >
>>         > ... could be transformed into ...
>>         >
>>         > for (i = 0; i < 1_000_000_000; i++)
>>         > {
>>         >     System.out.println(i);
>>         > }
>>         >
>>         > shared = 2 * 1_000_000_000;
>>         >
>>         > ... If so, then the non-volatile write may not happen for a
>>         very long time.
>>         >
>>         > Nathan Reynolds | Architect | 602.333.9091 <tel:602.333.9091>
>>         > Oracle PSR Engineering | Server Technology
>>         > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>         >
>>         > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>>         <thurston at nomagicsoftware.com
>>         <mailto:thurston at nomagicsoftware.com>>
>>         > wrote:
>>         >
>>         > Vitaly Davidovich wrote
>>         >
>>         > The code works as-is.
>>         >
>>         > Absolutely.  volatile is not needed for correctness
>>         >
>>         > Vitaly Davidovich wrote
>>         >
>>         > Why?
>>         >
>>         > Well, for performance reasons given the
>>         'undefined/indefinite' visibility of
>>         > #hash to other threads.
>>         > At least according to the JMM (which has nothing to say
>>         about CPU cache
>>         > coherency), it is *possible* that each distinct thread that
>>         invoked
>>         > #hashCode() *could* result in a recalculation of the hash.
>>         >
>>         > In practice though, application threads contain very frequent
>>         > synchronization actions, or other operations that force VM to
>>         > flush/reload. So it won't take very long for any
>>         non-volatile write in
>>         > one thread to become visible to other threads.
>>         >
>>         > Imagine a long-lived Map<String, ?>; and many threads
>>         accessing the map's
>>         > keyset and for some unknown reason invoking #hashCode() on
>>         each key.
>>         > If #hash was declared volatile, although there is no
>>         guarantee that #hash
>>         > would only be calculated once, it is guaranteed that once a
>>         write to main
>>         > memory was completed, every *subsequent* (here meaning
>>         after the write to
>>         >
>>         > In JMM though, we cannot even express this guarantee. Say
>>         we have
>>         > threads T1...Tn, each thread Ti burns `i` seconds CPU time
>>         first, then
>>         > volatile-reads #hash, and if it's 0, calculates and
>>         volatile-writes
>>         > #hash which takes 100 ns. We can find no guarantee from JMM
>>         that
>>         > there's only one write; it's legal that every thread sees 0
>>         from the
>>         > volatile read.
>>         >
>>         > Zhong Yu
>>         >
>>         > main memory) read no matter from which thread would see
>>         #hash != 0 and
>>         > therefore skip the calculation.
>>         >
>>         >
>>         >
>>         > Vitaly Davidovich wrote
>>         >
>>         > String is too high profile (especially
>>         > hashing it) to do the "naive" thing.
>>         >
>>         > Nothing wrong with being naive; naive can be charming.
>>         >
>>         >
>>         > Vitaly Davidovich wrote
>>         >
>>         > Also, some architectures pay a
>>         > penalty for volatile loads and you'd incur that each time.
>>         >
>>         > Fair point; the JDK authors only get one shot and they
>>         can't assume that
>>         > volatile reads are cheap
>>         >
>>         >
>>         >
>>         >
>>         >
>>         > --
>>         > View this message in context:
>>         >
>>         http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>         > Sent from the JSR166 Concurrency mailing list archive at
>>         Nabble.com.
>>         > _______________________________________________
>>         > Concurrency-interest mailing list
>>         > Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>         >
>>         > _______________________________________________
>>         > Concurrency-interest mailing list
>>         > Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>         >
>>         >
>>         >
>>         >
>>         > _______________________________________________
>>         > Concurrency-interest mailing list
>>         > Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>         >
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/8f307023/attachment-0001.html>

From vitalyd at gmail.com  Wed Apr 17 12:55:38 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 12:55:38 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ECCA8.6020500@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com> <516E7710.7040507@oracle.com>
	<516EA2C1.6020802@cytetech.com> <516EA483.1080007@oracle.com>
	<516ECCA8.6020500@oracle.com>
Message-ID: <CAHjP37G4PEAxU17iz5X9ELhKKZ5quM48hFvqOT5vYeN1PuRjRw@mail.gmail.com>

This is called write combining in Intel parlance (maybe not just them, not
sure).  However, in the example shown, it's not just wasteful stores but
the fact that the store is loop invariant in a way - the loop is counted
and no early returns from it.
On Apr 17, 2013 12:27 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  If I have my details correct, the Intel x86 processor will fuse 2
> back-to-back stores into 1 store if they are for the same address.  JIT has
> a lot more information about the program than the processor and might
> "fuse" stores to the same field.  The loop hoisting example is one such
> case.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/17/2013 6:32 AM, oleksandr otenko wrote:
>
> I am not sure what you are saying.
>
> I am only saying that moving the write out of the loop is possible only if
> we can prove it is not used inside the loop. Multiple writes are fused in
> exactly the same way as multiple reads are fused. Elimination is a form of
> reordering.
>
> Alex
>
> On 17/04/2013 14:25, Gregg Wonderly wrote:
>
> Actually the compiler doesn't do that for non-volatile access, as I've
> been going on about with the loop exit hoisting example.  This
> transformation is exactly the kind of thing that I'd expect to see happen,
> and this is the "unexplainable behavior" that will create alarm, because
> mutations of "shared" are not visible, except a loop exit.
>
> Gregg Wonderly
>
> On 4/17/2013 5:18 AM, oleksandr otenko wrote:
>
> You need to prove System.out.println isn't using shared.
>
> Alex
>
> On 17/04/2013 07:38, Nathan Reynolds wrote:
>
> Couldn't JIT hoist the non-volatile writes out of the loop?  For example,
> the
> following code...
>
> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
>     shared = 2 * i;
> }
>
> ... could be transformed into ...
>
> for (i = 0; i < 1_000_000_000; i++)
> {
>     System.out.println(i);
> }
>
> shared = 2 * 1_000_000_000;
>
> ... If so, then the non-volatile write may not happen for a very long
> time.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds><http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
> | Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/><http://psr.us.oracle.com/>| Server Technology
> On 4/16/2013 10:27 PM, Zhong Yu wrote:
>
> On Tue, Apr 16, 2013 at 8:51 PM, thurstonn<thurston at nomagicsoftware.com><thurston at nomagicsoftware.com>
> wrote:
>
> Vitaly Davidovich wrote
>
> The code works as-is.
>
> Absolutely.  volatile is not needed for correctness
>
> Vitaly Davidovich wrote
>
> Why?
>
> Well, for performance reasons given the 'undefined/indefinite' visibility
> of
> #hash to other threads.
> At least according to the JMM (which has nothing to say about CPU cache
> coherency), it is *possible* that each distinct thread that invoked
> #hashCode() *could* result in a recalculation of the hash.
>
> In practice though, application threads contain very frequent
> synchronization actions, or other operations that force VM to
> flush/reload. So it won't take very long for any non-volatile write in
> one thread to become visible to other threads.
>
> Imagine a long-lived Map<String, ?>; and many threads accessing the map's
> keyset and for some unknown reason invoking #hashCode() on each key.
> If #hash was declared volatile, although there is no guarantee that #hash
> would only be calculated once, it is guaranteed that once a write to main
> memory was completed, every *subsequent* (here meaning after the write to
>
> In JMM though, we cannot even express this guarantee. Say we have
> threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
> volatile-reads #hash, and if it's 0, calculates and volatile-writes
> #hash which takes 100 ns. We can find no guarantee from JMM that
> there's only one write; it's legal that every thread sees 0 from the
> volatile read.
>
> Zhong Yu
>
> main memory) read no matter from which thread would see #hash != 0 and
> therefore skip the calculation.
>
>
>
> Vitaly Davidovich wrote
>
> String is too high profile (especially
> hashing it) to do the "naive" thing.
>
> Nothing wrong with being naive; naive can be charming.
>
>
> Vitaly Davidovich wrote
>
> Also, some architectures pay a
> penalty for volatile loads and you'd incur that each time.
>
> Fair point; the JDK authors only get one shot and they can't assume that
> volatile reads are cheap
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7233e9c9/attachment.html>

From vgrazi at gmail.com  Wed Apr 17 12:59:24 2013
From: vgrazi at gmail.com (Victor Grazi)
Date: Wed, 17 Apr 2013 12:59:24 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 99,
	Issue 41
In-Reply-To: <mailman.249.1366215591.2479.concurrency-interest@cs.oswego.edu>
References: <mailman.249.1366215591.2479.concurrency-interest@cs.oswego.edu>
Message-ID: <CA+y1Pu9+UQy+gT2W6OKemJcZJwRA6SFb_ctpLL05_7=YeyykZg@mail.gmail.com>

Thanks Nathan
As Heinz points out this is the correct RW Lock behavior as of Java 6 (it
could also be obtained in java 5 using a Fair Boolean in the Reentrant RW
lock constructor.
As far as the concurrent hash map, all of the animations work that way,
just press the button two or three times to experience the effect.

Regards, Victor

On Wednesday, April 17, 2013, wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu <javascript:;>
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu <javascript:;>
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu <javascript:;>
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Re: New version of Java Concurrent Animated (Nathan Reynolds)
>    2. Re: ConcurrentHashMap-Very Strange behavior of ReentrantLock
>       under contention. (Nathan Reynolds)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 17 Apr 2013 09:15:02 -0700
> From: Nathan Reynolds <nathan.reynolds at oracle.com <javascript:;>>
> To: "Dr Heinz M. Kabutz" <heinz at javaspecialists.eu <javascript:;>>
> Cc: concurrency-interest at cs.oswego.edu <javascript:;>
> Subject: Re: [concurrency-interest] New version of Java Concurrent
>         Animated
> Message-ID: <516ECA86.3050303 at oracle.com <javascript:;>>
> Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"
>
> Shouldn't the code sleep for 250+ seconds to give enough time for
> Thread.start()?  ;)
>
> Thanks for testing this.  This raises an interesting case. Interleave
> threads so that readLock().lock() and writeLock().lock() are alternately
> called.  If this behavior is followed, then the reader will have
> exclusive access to the critical region since it comes between two writers.
>
> Isn't this a throughput problem?  Shouldn't all of the readers be
> allowed to enter the critical region together and thus improve
> throughput through the lock?  This means changing the behavior so that
> writers get high priority and always enter the lock first otherwise they
> will starve.  When all writers are done, all of the readers will be able
> to enter the lock concurrently.  What's the problem with this scheme?
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 4/17/2013 12:06 AM, Dr Heinz M. Kabutz wrote:
> > Interesting thing about Victor's animations is that they hook directly
> > into what is happening inside the concurrency constructs.  Thus if you
> > run that particular example in Java 5, you will see how the readers
> > keep on getting in, even if a writer is waiting.  And in Java 6+ it
> > actually does work as Victor's animation shows, as can be also seen if
> > you run this code:
> >
> > import java.util.concurrent.locks.*;
> >
> > public class ReentrantReadWriteLockTest {
> >     public static void main(String[] args) throws InterruptedException {
> >         final ReadWriteLock rwlock = new ReentrantReadWriteLock();
> >
> >         // Thread #1 calls writeLock().lock() and enters the critical
> > region.
> >         waitForWriteLock(rwlock, "t1");
> >         Thread.sleep(100);
> >
> >         // Thread #2 calls readLock().lock() and blocks.
> >         waitForReadLock(rwlock, "t2");
> >         Thread.sleep(100);
> >
> >         // Thread #3 calls readLock().lock() and blocks.
> >         waitForReadLock(rwlock, "t3");
> >         Thread.sleep(100);
> >
> >         // Thread #4 calls readLock().lock() and blocks.
> >         waitForReadLock(rwlock, "t4");
> >         Thread.sleep(100);
> >
> >         // Thread #5 calls readLock().lock() and blocks.
> >         waitForReadLock(rwlock, "t5");
> >         Thread.sleep(100);
> >
> >         // Thread#6 calls writeLock ().lock() and blocks.
> >         waitForWriteLock(rwlock, "t6");
> >
> >         // Thread#1 releases the lock.
> >     }
> >
> >     private static void waitForReadLock(final ReadWriteLock rwlock,
> > String name) {
> >         new Thread(name) {
> >             public void run() {
> >                 System.out.println("Thread " + getName() + " waiting
> > for read lock");
> >                 rwlock.readLock().lock();
> >                 try {
> >                     System.out.println("Thread " + getName() + " is in
> > read section");
> >                     Thread.sleep(2000);
> >                 } catch (InterruptedException e) {
> >                     Thread.currentThread().interrupt();
> >                 } finally {
> >                     rwlock.readLock().unlock();
> >                 }
> >             }
> >         }.start();
> >     }
> >
> >     private static void waitForWriteLock(final ReadWriteLock rwlock,
> > String name) {
> >         new Thread(name) {
> >             public void run() {
> >                 System.out.println("Thread " + getName() + " waiting
> > for write lock");
> >                 rwlock.writeLock().lock();
> >                 try {
> >                     System.out.println("Thread " + getName() + " is in
> > write section");
> >                     Thread.sleep(2000);
> >                 } catch (InterruptedException e) {
> >                     Thread.currentThread().interrupt();
> >                 } finally {
> >                     rwlock.writeLock().unlock();
> >                 }
> >             }
> >         }.start();
> >     }
> > }
> >
> > The output is:
> >
> > Thread t1 waiting for write lock
> > Thread t1 is in write section
> > Thread t2 waiting for read lock
> > Thread t3 waiting for read lock
> > Thread t4 waiting for read lock
> > Thread t5 waiting for read lock
> > Thread t6 waiting for write lock
> > Thread t2 is in read section
> > Thread t3 is in read section
> > Thread t4 is in read section
> > Thread t5 is in read section
> > Thread t6 is in write section
> >
> > Regards
> >
> > Heinz
> > --
> > Dr Heinz M. Kabutz (PhD CompSci)
> > Author of "The Java(tm) Specialists' Newsletter"
> > Sun Java Champion
> > IEEE Certified Software Development Professional
> > http://www.javaspecialists.eu
> > Tel: +30 69 75 595 262
> > Skype: kabutz
> >
> >
> > Nathan Reynolds wrote:
> >> Nice animation!  I noticed a couple of problems.
> >>
> >> The ConcurrentHashMap demo fails to draw the threads if you keep
> >> hitting putIfAbsent.  The threads are drawn beyond the bottom.
> >>
> >> If I remember right ReentrantReadWriteLock gives priority to writer
> >> threads.  If that is the case then the ReadWriteLock demo is not
> >> quite right.
> >>
> >> Consider this scenario.
> >>
> >>  1. Thread #1 calls writeLock().lock() and enters the critical region.
> >>  2. Thread #2 calls readLock().lock() and blocks.
> >>  3. Thread #3 calls readLock().lock() and blocks.
> >>  4. Thread #4 calls readLock().lock() and blocks.
> >>  5. Thread #5 calls readLock().lock() and blocks.
> >>  6. Thread #6 calls writeLock().lock() and blocks.
> >>  7. Thread #1 releases the lock.
> >>  8. If writer threads are given priority, then Thread #6 should enter
> >>     the critical region.
> >>
> >> In the demo, threads #2 to #5 are given the lock in step 8.
> >>
> >> Nathan Reynolds
> >> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> >> Architect | 602.333.9091
> >> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> >> On 4/16/2013 6:02 PM, Victor Grazi wrote:
> >>> Hi, just a quick note to let everyone know there is a new version of
> >>> Java Concurrent Animated on SourceForge.
> >>>
> >>> Some of the new features:
> >>> * A new animation showing good old synchronized/wait/notify. Kind of
> >>> surprising when you see it in action.
> >>> * Thread states are now color coded, and includes a novel state
> >>> diagram kind courtesy of Dr. Heinz Kabutz and his Concurrency
> >>> Specialist Training
> >>> <http://www.javaspecialists.eu/courses/concurrency.jsp> course.
> >>>
> >>> Check it out!
> >>> Thanks
> >>> Victor Grazi
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu <javascript:;>
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >> ------------------------------------------------------------------------
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu <javascript:;>
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/4332a13a/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Wed, 17 Apr 2013 09:19:07 -0700
> From: Nathan Reynolds <nathan.reynolds at oracle.com <javascript:;>>
> To: concurrency-interest at cs.oswego.edu <javascript:;>
> Subject: Re: [concurrency-interest] ConcurrentHashMap-Very Strange
>         behavior of ReentrantLock under contention.
> Message-ID: <516ECB7B.4030806 at oracle.com <javascript:;>>
> Content-Type: text/plain; charset="iso-8859-1"; Format="flowed"
>
> What version of JDK are you using?  JDK 8 has or will have a new version
> of ConcurrentHashMap.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 4/17/2013 6:31 AM, Mudit Verma wrote:
> > Hi All,
> >
> > I recently performed a scalability test (very aggressive and may be
> > not practical, anyhow) on put operation of ConcurrentHashMap.
> >
> > Test: Each thread is trying to put (Same Key, Random Value) in HashMap
> > in a tight loop. Therefore, all the threads will hit the same location
> > on hashMap and will cause contention.
> >
> > What is more surprising is, when each thread continue to do another
> > put one after the other, avg time taken in one put operation is lesser
> > than when a thread do some other work between two put operations.
> >
> > We continue to see the increase in per operation time by increasing
> > the work done in between.  This is very counter intuitive. Only after
> > a work of about 10,000 - 20,000 cycles in between, per op time comes
> > down.
> >
> > When I read the code, I found out that put op first try to use CAS to
> > aquire the lock(64 times on multicore), only if it could not acquire
> > the lock on segment through CASing, it goes for ReentrantLocking
> > (which suspend threads .. ).
> >
> > We also tweaked, the number of times CAS (from 0 to 10,000) is used
> > before actually going for ReentrantLocking.   Attached is the graph.
> >
> > One interesting thing to note. As we increase the work between two
> > ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst
> > affected with the spike. Therefore, I assume that, spike comes from
> > ReentractLocking even when there is a mixture of two (CAS+Lock).
> >
> > Code Skeleton: For each Thread
> >
> > While() {
> >   hashMap.put(K,randomValue);     // K is same for each thread
> >   ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
> >
> > }
> >
> >  Machine: 48 core NUMA
> > Threads used:  32 (each one is pinned to a core).
> > #ops: In total 51200000 (each thread with 160000 ops)
> >
> > That's like saying, if I do something else in between my operations
> > (upto a limit), contention will increase.  Very strange.
> >
> > Does anyone of you know why is it happening?
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu <javascript:;>
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/7d94d447/attachment.html
> >
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <javascript:;>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 99, Issue 41
> ****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/08e671ab/attachment-0001.html>

From vitalyd at gmail.com  Wed Apr 17 13:01:51 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 13:01:51 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ED2A8.1060506@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
Message-ID: <CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>

Are you sure about fusion if shared is volatile? Maybe theoretically it's
possible but the analysis (and risk) the JVM would have to do to make that
transform is probably impractical.  Intuitively, if I mark something
volatile, I expect almost verbatim code to execute.

By instructions, I mean loads and stores since that's what actually matters
for these scenarios.
On Apr 17, 2013 12:55 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Yes, but not any instructions - only those that have temporal
> relationship.
>
> So, for example, in this case even if shared is volatile, then volatile
> stores can still be moved out of the loop and fused into a single volatile
> store.
>
> Alex
>
>
> On 17/04/2013 17:43, Vitaly Davidovich wrote:
>
> Immediate means store is made globally visible before subsequent
> instructions complete/retire.
> On Apr 17, 2013 12:26 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  What is this "immediate" anyway?
>>
>> It is actually "before anything else in this thread that's temporal"
>>
>> Alex
>>
>>
>>  On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>
>> I actually expect that the optimizer *would* do this transformation on
>> plain fields (provided it doesn't break intra-thread semantics, of course)
>> because it's a perf gain.
>>
>> Don't know how much JIT can see through println as it ultimately calls
>> into runtime and OS functions, so I'd guess it doesn't know enough or
>> simply plays conservative here.  However, Nathan's point is still valid
>> even if example isn't necessarily the best one.  If you had "pure" java
>> code instead of an I/O call that took significant time to execute, the
>> write would be delayed.  I'm not sure why that matters though for benign
>> data races.  Clearly if you need immediate visibility, you code for that
>> specifically.
>> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>>
>>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>> <nathan.reynolds at oracle.com> wrote:
>>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>>
>>> Certainly, sorry if my statement sounds too absolute.
>>>
>>> > For example, the following code...
>>>
>>> But, is this a valid example? Can JMM really reorder around
>>> System.out.println()?
>>>
>>> > for (i = 0; i < 1_000_000_000; i++)
>>> > {
>>> >     System.out.println(i);
>>> >     shared = 2 * i;
>>> > }
>>> >
>>> > ... could be transformed into ...
>>> >
>>> > for (i = 0; i < 1_000_000_000; i++)
>>> > {
>>> >     System.out.println(i);
>>> > }
>>> >
>>> > shared = 2 * 1_000_000_000;
>>> >
>>> > ... If so, then the non-volatile write may not happen for a very long
>>> time.
>>> >
>>> > Nathan Reynolds | Architect | 602.333.9091
>>> > Oracle PSR Engineering | Server Technology
>>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>> >
>>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <
>>> thurston at nomagicsoftware.com>
>>> > wrote:
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > The code works as-is.
>>> >
>>> > Absolutely.  volatile is not needed for correctness
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > Why?
>>> >
>>> > Well, for performance reasons given the 'undefined/indefinite'
>>> visibility of
>>> > #hash to other threads.
>>> > At least according to the JMM (which has nothing to say about CPU cache
>>> > coherency), it is *possible* that each distinct thread that invoked
>>> > #hashCode() *could* result in a recalculation of the hash.
>>> >
>>> > In practice though, application threads contain very frequent
>>> > synchronization actions, or other operations that force VM to
>>> > flush/reload. So it won't take very long for any non-volatile write in
>>> > one thread to become visible to other threads.
>>> >
>>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>>> map's
>>> > keyset and for some unknown reason invoking #hashCode() on each key.
>>> > If #hash was declared volatile, although there is no guarantee that
>>> #hash
>>> > would only be calculated once, it is guaranteed that once a write to
>>> main
>>> > memory was completed, every *subsequent* (here meaning after the write
>>> to
>>> >
>>> > In JMM though, we cannot even express this guarantee. Say we have
>>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>>> > there's only one write; it's legal that every thread sees 0 from the
>>> > volatile read.
>>> >
>>> > Zhong Yu
>>> >
>>> > main memory) read no matter from which thread would see #hash != 0 and
>>> > therefore skip the calculation.
>>> >
>>> >
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > String is too high profile (especially
>>> > hashing it) to do the "naive" thing.
>>> >
>>> > Nothing wrong with being naive; naive can be charming.
>>> >
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > Also, some architectures pay a
>>> > penalty for volatile loads and you'd incur that each time.
>>> >
>>> > Fair point; the JDK authors only get one shot and they can't assume
>>> that
>>> > volatile reads are cheap
>>> >
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> >
>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> >
>>> >
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/f0b5706d/attachment.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 13:06:43 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 18:06:43 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
	<CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
Message-ID: <516ED6A3.3080602@oracle.com>

Yes.

1. Unroll the loop.
2. Reorder non-volatile accesses to go ahead of volatile stores
3. Now all volatile stores from all iterations are one after the other
4. Multiple stores to the same location can be fused, even if they are 
volatile stores

If you want a store in every iteration, need a volatile load somewhere 
between the stores.

Alex


On 17/04/2013 18:01, Vitaly Davidovich wrote:
>
> Are you sure about fusion if shared is volatile? Maybe theoretically 
> it's possible but the analysis (and risk) the JVM would have to do to 
> make that transform is probably impractical.  Intuitively, if I mark 
> something volatile, I expect almost verbatim code to execute.
>
> By instructions, I mean loads and stores since that's what actually 
> matters for these scenarios.
>
> On Apr 17, 2013 12:55 PM, "oleksandr otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Yes, but not any instructions - only those that have temporal
>     relationship.
>
>     So, for example, in this case even if shared is volatile, then
>     volatile stores can still be moved out of the loop and fused into
>     a single volatile store.
>
>     Alex
>
>
>     On 17/04/2013 17:43, Vitaly Davidovich wrote:
>>
>>     Immediate means store is made globally visible before subsequent
>>     instructions complete/retire.
>>
>>     On Apr 17, 2013 12:26 PM, "oleksandr otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         What is this "immediate" anyway?
>>
>>         It is actually "before anything else in this thread that's
>>         temporal"
>>
>>         Alex
>>
>>
>>         On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>>
>>>         I actually expect that the optimizer *would* do this
>>>         transformation on plain fields (provided it doesn't break
>>>         intra-thread semantics, of course) because it's a perf gain.
>>>
>>>         Don't know how much JIT can see through println as it
>>>         ultimately calls into runtime and OS functions, so I'd guess
>>>         it doesn't know enough or simply plays conservative here. 
>>>         However, Nathan's point is still valid even if example isn't
>>>         necessarily the best one. If you had "pure" java code
>>>         instead of an I/O call that took significant time to
>>>         execute, the write would be delayed.  I'm not sure why that
>>>         matters though for benign data races.  Clearly if you need
>>>         immediate visibility, you code for that specifically.
>>>
>>>         On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com
>>>         <mailto:zhong.j.yu at gmail.com>> wrote:
>>>
>>>             On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>>             <nathan.reynolds at oracle.com
>>>             <mailto:nathan.reynolds at oracle.com>> wrote:
>>>             > Couldn't JIT hoist the non-volatile writes out of the
>>>             loop?
>>>
>>>             Certainly, sorry if my statement sounds too absolute.
>>>
>>>             > For example, the following code...
>>>
>>>             But, is this a valid example? Can JMM really reorder around
>>>             System.out.println()?
>>>
>>>             > for (i = 0; i < 1_000_000_000; i++)
>>>             > {
>>>             >     System.out.println(i);
>>>             >     shared = 2 * i;
>>>             > }
>>>             >
>>>             > ... could be transformed into ...
>>>             >
>>>             > for (i = 0; i < 1_000_000_000; i++)
>>>             > {
>>>             >     System.out.println(i);
>>>             > }
>>>             >
>>>             > shared = 2 * 1_000_000_000;
>>>             >
>>>             > ... If so, then the non-volatile write may not happen
>>>             for a very long time.
>>>             >
>>>             > Nathan Reynolds | Architect | 602.333.9091
>>>             <tel:602.333.9091>
>>>             > Oracle PSR Engineering | Server Technology
>>>             > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>             >
>>>             > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>>>             <thurston at nomagicsoftware.com
>>>             <mailto:thurston at nomagicsoftware.com>>
>>>             > wrote:
>>>             >
>>>             > Vitaly Davidovich wrote
>>>             >
>>>             > The code works as-is.
>>>             >
>>>             > Absolutely.  volatile is not needed for correctness
>>>             >
>>>             > Vitaly Davidovich wrote
>>>             >
>>>             > Why?
>>>             >
>>>             > Well, for performance reasons given the
>>>             'undefined/indefinite' visibility of
>>>             > #hash to other threads.
>>>             > At least according to the JMM (which has nothing to
>>>             say about CPU cache
>>>             > coherency), it is *possible* that each distinct thread
>>>             that invoked
>>>             > #hashCode() *could* result in a recalculation of the hash.
>>>             >
>>>             > In practice though, application threads contain very
>>>             frequent
>>>             > synchronization actions, or other operations that
>>>             force VM to
>>>             > flush/reload. So it won't take very long for any
>>>             non-volatile write in
>>>             > one thread to become visible to other threads.
>>>             >
>>>             > Imagine a long-lived Map<String, ?>; and many threads
>>>             accessing the map's
>>>             > keyset and for some unknown reason invoking
>>>             #hashCode() on each key.
>>>             > If #hash was declared volatile, although there is no
>>>             guarantee that #hash
>>>             > would only be calculated once, it is guaranteed that
>>>             once a write to main
>>>             > memory was completed, every *subsequent* (here meaning
>>>             after the write to
>>>             >
>>>             > In JMM though, we cannot even express this guarantee.
>>>             Say we have
>>>             > threads T1...Tn, each thread Ti burns `i` seconds CPU
>>>             time first, then
>>>             > volatile-reads #hash, and if it's 0, calculates and
>>>             volatile-writes
>>>             > #hash which takes 100 ns. We can find no guarantee
>>>             from JMM that
>>>             > there's only one write; it's legal that every thread
>>>             sees 0 from the
>>>             > volatile read.
>>>             >
>>>             > Zhong Yu
>>>             >
>>>             > main memory) read no matter from which thread would
>>>             see #hash != 0 and
>>>             > therefore skip the calculation.
>>>             >
>>>             >
>>>             >
>>>             > Vitaly Davidovich wrote
>>>             >
>>>             > String is too high profile (especially
>>>             > hashing it) to do the "naive" thing.
>>>             >
>>>             > Nothing wrong with being naive; naive can be charming.
>>>             >
>>>             >
>>>             > Vitaly Davidovich wrote
>>>             >
>>>             > Also, some architectures pay a
>>>             > penalty for volatile loads and you'd incur that each time.
>>>             >
>>>             > Fair point; the JDK authors only get one shot and they
>>>             can't assume that
>>>             > volatile reads are cheap
>>>             >
>>>             >
>>>             >
>>>             >
>>>             >
>>>             > --
>>>             > View this message in context:
>>>             >
>>>             http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>             > Sent from the JSR166 Concurrency mailing list archive
>>>             at Nabble.com.
>>>             > _______________________________________________
>>>             > Concurrency-interest mailing list
>>>             > Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>             >
>>>             > _______________________________________________
>>>             > Concurrency-interest mailing list
>>>             > Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>             >
>>>             >
>>>             >
>>>             >
>>>             > _______________________________________________
>>>             > Concurrency-interest mailing list
>>>             > Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>             >
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/6a636870/attachment-0001.html>

From vitalyd at gmail.com  Wed Apr 17 13:11:17 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 13:11:17 -0400
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
Message-ID: <CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>

What exactly are you doing in ThinkTime?
On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:

> Hi All,
>
> I recently performed a scalability test (very aggressive and may be not
> practical, anyhow) on put operation of ConcurrentHashMap.
>
> Test: Each thread is trying to put (Same Key, Random Value) in HashMap in
> a tight loop. Therefore, all the threads will hit the same location on
> hashMap and will cause contention.
>
> What is more surprising is, when each thread continue to do another put
> one after the other, avg time taken in one put operation is lesser than
> when a thread do some other work between two put operations.
>
> We continue to see the increase in per operation time by increasing the
> work done in between.  This is very counter intuitive. Only after a work of
> about 10,000 - 20,000 cycles in between, per op time comes down.
>
> When I read the code, I found out that put op first try to use CAS to
> aquire the lock(64 times on multicore), only if it could not acquire the
> lock on segment through CASing, it goes for ReentrantLocking (which suspend
> threads .. ).
>
> We also tweaked, the number of times CAS (from 0 to 10,000) is used before
> actually going for ReentrantLocking.   Attached is the graph.
>
> One interesting thing to note. As we increase the work between two ops,
> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
> the spike. Therefore, I assume that, spike comes from ReentractLocking even
> when there is a mixture of two (CAS+Lock).
>
> Code Skeleton: For each Thread
>
> While() {
>   hashMap.put(K,randomValue);     // K is same for each thread
>   ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>
> }
>
>  Machine: 48 core NUMA
> Threads used:  32 (each one is pinned to a core).
> #ops: In total 51200000 (each thread with 160000 ops)
>
> That's like saying, if I do something else in between my operations (upto
> a limit), contention will increase.  Very strange.
>
> Does anyone of you know why is it happening?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/321f7192/attachment.html>

From zhong.j.yu at gmail.com  Wed Apr 17 13:15:52 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 17 Apr 2013 12:15:52 -0500
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
Message-ID: <CACuKZqHh0oCW+O--ijJ8v6VUFcWRsYM_gxj_yAFgonaDX3e4rw@mail.gmail.com>

On Wed, Apr 17, 2013 at 10:44 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> I actually expect that the optimizer *would* do this transformation on plain

I do not expect that, since println() interacts with the external
world. Consider this program:

Thread 1
[1]    shared = 1;
[2]    console.print("press enter to continue");

Thread 2
[3]    console.readLine();
[4]    r = shared;

If there is a causality from [2] to [3] in the physics world (the
human user sees the prompt which drives him to press enter), isn't it
reasonable to assume that [4] sees [1]? If we don't have that
guarantee, how can we write programs that appear to be coherent to the
outside world? Note that making [1] and [4] volatile or synchronized
won't help, as long as there is not an edge from [2] to [3].

Zhong Yu

> fields (provided it doesn't break intra-thread semantics, of course) because
> it's a perf gain.
>
> Don't know how much JIT can see through println as it ultimately calls into
> runtime and OS functions, so I'd guess it doesn't know enough or simply
> plays conservative here.  However, Nathan's point is still valid even if
> example isn't necessarily the best one.  If you had "pure" java code instead
> of an I/O call that took significant time to execute, the write would be
> delayed.  I'm not sure why that matters though for benign data races.
> Clearly if you need immediate visibility, you code for that specifically.
>
> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>>
>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>> <nathan.reynolds at oracle.com> wrote:
>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>
>> Certainly, sorry if my statement sounds too absolute.
>>
>> > For example, the following code...
>>
>> But, is this a valid example? Can JMM really reorder around
>> System.out.println()?
>>
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> >     shared = 2 * i;
>> > }
>> >
>> > ... could be transformed into ...
>> >
>> > for (i = 0; i < 1_000_000_000; i++)
>> > {
>> >     System.out.println(i);
>> > }
>> >
>> > shared = 2 * 1_000_000_000;
>> >
>> > ... If so, then the non-volatile write may not happen for a very long
>> > time.
>> >
>> > Nathan Reynolds | Architect | 602.333.9091
>> > Oracle PSR Engineering | Server Technology
>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>> >
>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>> > <thurston at nomagicsoftware.com>
>> > wrote:
>> >
>> > Vitaly Davidovich wrote
>> >
>> > The code works as-is.
>> >
>> > Absolutely.  volatile is not needed for correctness
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Why?
>> >
>> > Well, for performance reasons given the 'undefined/indefinite'
>> > visibility of
>> > #hash to other threads.
>> > At least according to the JMM (which has nothing to say about CPU cache
>> > coherency), it is *possible* that each distinct thread that invoked
>> > #hashCode() *could* result in a recalculation of the hash.
>> >
>> > In practice though, application threads contain very frequent
>> > synchronization actions, or other operations that force VM to
>> > flush/reload. So it won't take very long for any non-volatile write in
>> > one thread to become visible to other threads.
>> >
>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>> > map's
>> > keyset and for some unknown reason invoking #hashCode() on each key.
>> > If #hash was declared volatile, although there is no guarantee that
>> > #hash
>> > would only be calculated once, it is guaranteed that once a write to
>> > main
>> > memory was completed, every *subsequent* (here meaning after the write
>> > to
>> >
>> > In JMM though, we cannot even express this guarantee. Say we have
>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>> > there's only one write; it's legal that every thread sees 0 from the
>> > volatile read.
>> >
>> > Zhong Yu
>> >
>> > main memory) read no matter from which thread would see #hash != 0 and
>> > therefore skip the calculation.
>> >
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > String is too high profile (especially
>> > hashing it) to do the "naive" thing.
>> >
>> > Nothing wrong with being naive; naive can be charming.
>> >
>> >
>> > Vitaly Davidovich wrote
>> >
>> > Also, some architectures pay a
>> > penalty for volatile loads and you'd incur that each time.
>> >
>> > Fair point; the JDK authors only get one shot and they can't assume that
>> > volatile reads are cheap
>> >
>> >
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> >
>> > http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From vitalyd at gmail.com  Wed Apr 17 13:18:39 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 13:18:39 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ED6A3.3080602@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
	<CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
	<516ED6A3.3080602@oracle.com>
Message-ID: <CAHjP37HS70CLh8efBkE2L3n52H86LJxtRqudy2iEKUMO4sRqmA@mail.gmail.com>

Yes, but where does it say that #4 is valid and under what conditions?
Presumably in this example, shared is read somewhere else as well
(different method perhaps) - else why are we calling it shared? Only
writers? That's uninteresting.

Sent from my phone
 Yes.

1. Unroll the loop.
2. Reorder non-volatile accesses to go ahead of volatile stores
3. Now all volatile stores from all iterations are one after the other
4. Multiple stores to the same location can be fused, even if they are
volatile stores

If you want a store in every iteration, need a volatile load somewhere
between the stores.

Alex


On 17/04/2013 18:01, Vitaly Davidovich wrote:

Are you sure about fusion if shared is volatile? Maybe theoretically it's
possible but the analysis (and risk) the JVM would have to do to make that
transform is probably impractical.  Intuitively, if I mark something
volatile, I expect almost verbatim code to execute.

By instructions, I mean loads and stores since that's what actually matters
for these scenarios.
On Apr 17, 2013 12:55 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Yes, but not any instructions - only those that have temporal
> relationship.
>
> So, for example, in this case even if shared is volatile, then volatile
> stores can still be moved out of the loop and fused into a single volatile
> store.
>
> Alex
>
>
> On 17/04/2013 17:43, Vitaly Davidovich wrote:
>
> Immediate means store is made globally visible before subsequent
> instructions complete/retire.
> On Apr 17, 2013 12:26 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  What is this "immediate" anyway?
>>
>> It is actually "before anything else in this thread that's temporal"
>>
>> Alex
>>
>>
>>  On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>
>> I actually expect that the optimizer *would* do this transformation on
>> plain fields (provided it doesn't break intra-thread semantics, of course)
>> because it's a perf gain.
>>
>> Don't know how much JIT can see through println as it ultimately calls
>> into runtime and OS functions, so I'd guess it doesn't know enough or
>> simply plays conservative here.  However, Nathan's point is still valid
>> even if example isn't necessarily the best one.  If you had "pure" java
>> code instead of an I/O call that took significant time to execute, the
>> write would be delayed.  I'm not sure why that matters though for benign
>> data races.  Clearly if you need immediate visibility, you code for that
>> specifically.
>> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>>
>>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>> <nathan.reynolds at oracle.com> wrote:
>>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>>
>>> Certainly, sorry if my statement sounds too absolute.
>>>
>>> > For example, the following code...
>>>
>>> But, is this a valid example? Can JMM really reorder around
>>> System.out.println()?
>>>
>>> > for (i = 0; i < 1_000_000_000; i++)
>>> > {
>>> >     System.out.println(i);
>>> >     shared = 2 * i;
>>> > }
>>> >
>>> > ... could be transformed into ...
>>> >
>>> > for (i = 0; i < 1_000_000_000; i++)
>>> > {
>>> >     System.out.println(i);
>>> > }
>>> >
>>> > shared = 2 * 1_000_000_000;
>>> >
>>> > ... If so, then the non-volatile write may not happen for a very long
>>> time.
>>> >
>>> > Nathan Reynolds | Architect | 602.333.9091
>>> > Oracle PSR Engineering | Server Technology
>>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>> >
>>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <
>>> thurston at nomagicsoftware.com>
>>> > wrote:
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > The code works as-is.
>>> >
>>> > Absolutely.  volatile is not needed for correctness
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > Why?
>>> >
>>> > Well, for performance reasons given the 'undefined/indefinite'
>>> visibility of
>>> > #hash to other threads.
>>> > At least according to the JMM (which has nothing to say about CPU cache
>>> > coherency), it is *possible* that each distinct thread that invoked
>>> > #hashCode() *could* result in a recalculation of the hash.
>>> >
>>> > In practice though, application threads contain very frequent
>>> > synchronization actions, or other operations that force VM to
>>> > flush/reload. So it won't take very long for any non-volatile write in
>>> > one thread to become visible to other threads.
>>> >
>>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>>> map's
>>> > keyset and for some unknown reason invoking #hashCode() on each key.
>>> > If #hash was declared volatile, although there is no guarantee that
>>> #hash
>>> > would only be calculated once, it is guaranteed that once a write to
>>> main
>>> > memory was completed, every *subsequent* (here meaning after the write
>>> to
>>> >
>>> > In JMM though, we cannot even express this guarantee. Say we have
>>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>>> > there's only one write; it's legal that every thread sees 0 from the
>>> > volatile read.
>>> >
>>> > Zhong Yu
>>> >
>>> > main memory) read no matter from which thread would see #hash != 0 and
>>> > therefore skip the calculation.
>>> >
>>> >
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > String is too high profile (especially
>>> > hashing it) to do the "naive" thing.
>>> >
>>> > Nothing wrong with being naive; naive can be charming.
>>> >
>>> >
>>> > Vitaly Davidovich wrote
>>> >
>>> > Also, some architectures pay a
>>> > penalty for volatile loads and you'd incur that each time.
>>> >
>>> > Fair point; the JDK authors only get one shot and they can't assume
>>> that
>>> > volatile reads are cheap
>>> >
>>> >
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> >
>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> >
>>> >
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> >
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/6d8798b0/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 13:19:54 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 18:19:54 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37HS70CLh8efBkE2L3n52H86LJxtRqudy2iEKUMO4sRqmA@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
	<CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
	<516ED6A3.3080602@oracle.com>
	<CAHjP37HS70CLh8efBkE2L3n52H86LJxtRqudy2iEKUMO4sRqmA@mail.gmail.com>
Message-ID: <516ED9BA.50307@oracle.com>

How would a external entity notice that there were multiple writes?


Alex

On 17/04/2013 18:18, Vitaly Davidovich wrote:
>
> Yes, but where does it say that #4 is valid and under what conditions? 
> Presumably in this example, shared is read somewhere else as well 
> (different method perhaps) - else why are we calling it shared? Only 
> writers? That's uninteresting.
>
> Sent from my phone
>
> Yes.
>
> 1. Unroll the loop.
> 2. Reorder non-volatile accesses to go ahead of volatile stores
> 3. Now all volatile stores from all iterations are one after the other
> 4. Multiple stores to the same location can be fused, even if they are 
> volatile stores
>
> If you want a store in every iteration, need a volatile load somewhere 
> between the stores.
>
> Alex
>
>
> On 17/04/2013 18:01, Vitaly Davidovich wrote:
>>
>> Are you sure about fusion if shared is volatile? Maybe theoretically 
>> it's possible but the analysis (and risk) the JVM would have to do to 
>> make that transform is probably impractical.  Intuitively, if I mark 
>> something volatile, I expect almost verbatim code to execute.
>>
>> By instructions, I mean loads and stores since that's what actually 
>> matters for these scenarios.
>>
>> On Apr 17, 2013 12:55 PM, "oleksandr otenko" 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     Yes, but not any instructions - only those that have temporal
>>     relationship.
>>
>>     So, for example, in this case even if shared is volatile, then
>>     volatile stores can still be moved out of the loop and fused into
>>     a single volatile store.
>>
>>     Alex
>>
>>
>>     On 17/04/2013 17:43, Vitaly Davidovich wrote:
>>>
>>>     Immediate means store is made globally visible before subsequent
>>>     instructions complete/retire.
>>>
>>>     On Apr 17, 2013 12:26 PM, "oleksandr otenko"
>>>     <oleksandr.otenko at oracle.com
>>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>         What is this "immediate" anyway?
>>>
>>>         It is actually "before anything else in this thread that's
>>>         temporal"
>>>
>>>         Alex
>>>
>>>
>>>         On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>>>
>>>>         I actually expect that the optimizer *would* do this
>>>>         transformation on plain fields (provided it doesn't break
>>>>         intra-thread semantics, of course) because it's a perf gain.
>>>>
>>>>         Don't know how much JIT can see through println as it
>>>>         ultimately calls into runtime and OS functions, so I'd
>>>>         guess it doesn't know enough or simply plays conservative
>>>>         here.  However, Nathan's point is still valid even if
>>>>         example isn't necessarily the best one. If you had "pure"
>>>>         java code instead of an I/O call that took significant time
>>>>         to execute, the write would be delayed.  I'm not sure why
>>>>         that matters though for benign data races.  Clearly if you
>>>>         need immediate visibility, you code for that specifically.
>>>>
>>>>         On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com
>>>>         <mailto:zhong.j.yu at gmail.com>> wrote:
>>>>
>>>>             On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>>>             <nathan.reynolds at oracle.com
>>>>             <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>             > Couldn't JIT hoist the non-volatile writes out of the
>>>>             loop?
>>>>
>>>>             Certainly, sorry if my statement sounds too absolute.
>>>>
>>>>             > For example, the following code...
>>>>
>>>>             But, is this a valid example? Can JMM really reorder around
>>>>             System.out.println()?
>>>>
>>>>             > for (i = 0; i < 1_000_000_000; i++)
>>>>             > {
>>>>             >     System.out.println(i);
>>>>             >     shared = 2 * i;
>>>>             > }
>>>>             >
>>>>             > ... could be transformed into ...
>>>>             >
>>>>             > for (i = 0; i < 1_000_000_000; i++)
>>>>             > {
>>>>             >     System.out.println(i);
>>>>             > }
>>>>             >
>>>>             > shared = 2 * 1_000_000_000;
>>>>             >
>>>>             > ... If so, then the non-volatile write may not happen
>>>>             for a very long time.
>>>>             >
>>>>             > Nathan Reynolds | Architect | 602.333.9091
>>>>             <tel:602.333.9091>
>>>>             > Oracle PSR Engineering | Server Technology
>>>>             > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>>             >
>>>>             > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>>>>             <thurston at nomagicsoftware.com
>>>>             <mailto:thurston at nomagicsoftware.com>>
>>>>             > wrote:
>>>>             >
>>>>             > Vitaly Davidovich wrote
>>>>             >
>>>>             > The code works as-is.
>>>>             >
>>>>             > Absolutely.  volatile is not needed for correctness
>>>>             >
>>>>             > Vitaly Davidovich wrote
>>>>             >
>>>>             > Why?
>>>>             >
>>>>             > Well, for performance reasons given the
>>>>             'undefined/indefinite' visibility of
>>>>             > #hash to other threads.
>>>>             > At least according to the JMM (which has nothing to
>>>>             say about CPU cache
>>>>             > coherency), it is *possible* that each distinct
>>>>             thread that invoked
>>>>             > #hashCode() *could* result in a recalculation of the
>>>>             hash.
>>>>             >
>>>>             > In practice though, application threads contain very
>>>>             frequent
>>>>             > synchronization actions, or other operations that
>>>>             force VM to
>>>>             > flush/reload. So it won't take very long for any
>>>>             non-volatile write in
>>>>             > one thread to become visible to other threads.
>>>>             >
>>>>             > Imagine a long-lived Map<String, ?>; and many threads
>>>>             accessing the map's
>>>>             > keyset and for some unknown reason invoking
>>>>             #hashCode() on each key.
>>>>             > If #hash was declared volatile, although there is no
>>>>             guarantee that #hash
>>>>             > would only be calculated once, it is guaranteed that
>>>>             once a write to main
>>>>             > memory was completed, every *subsequent* (here
>>>>             meaning after the write to
>>>>             >
>>>>             > In JMM though, we cannot even express this guarantee.
>>>>             Say we have
>>>>             > threads T1...Tn, each thread Ti burns `i` seconds CPU
>>>>             time first, then
>>>>             > volatile-reads #hash, and if it's 0, calculates and
>>>>             volatile-writes
>>>>             > #hash which takes 100 ns. We can find no guarantee
>>>>             from JMM that
>>>>             > there's only one write; it's legal that every thread
>>>>             sees 0 from the
>>>>             > volatile read.
>>>>             >
>>>>             > Zhong Yu
>>>>             >
>>>>             > main memory) read no matter from which thread would
>>>>             see #hash != 0 and
>>>>             > therefore skip the calculation.
>>>>             >
>>>>             >
>>>>             >
>>>>             > Vitaly Davidovich wrote
>>>>             >
>>>>             > String is too high profile (especially
>>>>             > hashing it) to do the "naive" thing.
>>>>             >
>>>>             > Nothing wrong with being naive; naive can be charming.
>>>>             >
>>>>             >
>>>>             > Vitaly Davidovich wrote
>>>>             >
>>>>             > Also, some architectures pay a
>>>>             > penalty for volatile loads and you'd incur that each
>>>>             time.
>>>>             >
>>>>             > Fair point; the JDK authors only get one shot and
>>>>             they can't assume that
>>>>             > volatile reads are cheap
>>>>             >
>>>>             >
>>>>             >
>>>>             >
>>>>             >
>>>>             > --
>>>>             > View this message in context:
>>>>             >
>>>>             http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>>             > Sent from the JSR166 Concurrency mailing list archive
>>>>             at Nabble.com.
>>>>             > _______________________________________________
>>>>             > Concurrency-interest mailing list
>>>>             > Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             >
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>             >
>>>>             > _______________________________________________
>>>>             > Concurrency-interest mailing list
>>>>             > Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             >
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>             >
>>>>             >
>>>>             >
>>>>             >
>>>>             > _______________________________________________
>>>>             > Concurrency-interest mailing list
>>>>             > Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             >
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>             >
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/e71bf3f2/attachment-0001.html>

From vitalyd at gmail.com  Wed Apr 17 13:20:54 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 13:20:54 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CACuKZqHh0oCW+O--ijJ8v6VUFcWRsYM_gxj_yAFgonaDX3e4rw@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<CACuKZqHh0oCW+O--ijJ8v6VUFcWRsYM_gxj_yAFgonaDX3e4rw@mail.gmail.com>
Message-ID: <CAHjP37G2bBZrwnr5kLObhDrEPgKOTOt-HMO+dFwsO2+wWACKgg@mail.gmail.com>

Yes, I don't expect that for println either, but I do in the scenarios that
Nathan and I described.

Sent from my phone
On Apr 17, 2013 1:15 PM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> On Wed, Apr 17, 2013 at 10:44 AM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
> > I actually expect that the optimizer *would* do this transformation on
> plain
>
> I do not expect that, since println() interacts with the external
> world. Consider this program:
>
> Thread 1
> [1]    shared = 1;
> [2]    console.print("press enter to continue");
>
> Thread 2
> [3]    console.readLine();
> [4]    r = shared;
>
> If there is a causality from [2] to [3] in the physics world (the
> human user sees the prompt which drives him to press enter), isn't it
> reasonable to assume that [4] sees [1]? If we don't have that
> guarantee, how can we write programs that appear to be coherent to the
> outside world? Note that making [1] and [4] volatile or synchronized
> won't help, as long as there is not an edge from [2] to [3].
>
> Zhong Yu
>
> > fields (provided it doesn't break intra-thread semantics, of course)
> because
> > it's a perf gain.
> >
> > Don't know how much JIT can see through println as it ultimately calls
> into
> > runtime and OS functions, so I'd guess it doesn't know enough or simply
> > plays conservative here.  However, Nathan's point is still valid even if
> > example isn't necessarily the best one.  If you had "pure" java code
> instead
> > of an I/O call that took significant time to execute, the write would be
> > delayed.  I'm not sure why that matters though for benign data races.
> > Clearly if you need immediate visibility, you code for that specifically.
> >
> > On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
> >>
> >> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
> >> <nathan.reynolds at oracle.com> wrote:
> >> > Couldn't JIT hoist the non-volatile writes out of the loop?
> >>
> >> Certainly, sorry if my statement sounds too absolute.
> >>
> >> > For example, the following code...
> >>
> >> But, is this a valid example? Can JMM really reorder around
> >> System.out.println()?
> >>
> >> > for (i = 0; i < 1_000_000_000; i++)
> >> > {
> >> >     System.out.println(i);
> >> >     shared = 2 * i;
> >> > }
> >> >
> >> > ... could be transformed into ...
> >> >
> >> > for (i = 0; i < 1_000_000_000; i++)
> >> > {
> >> >     System.out.println(i);
> >> > }
> >> >
> >> > shared = 2 * 1_000_000_000;
> >> >
> >> > ... If so, then the non-volatile write may not happen for a very long
> >> > time.
> >> >
> >> > Nathan Reynolds | Architect | 602.333.9091
> >> > Oracle PSR Engineering | Server Technology
> >> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
> >> >
> >> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
> >> > <thurston at nomagicsoftware.com>
> >> > wrote:
> >> >
> >> > Vitaly Davidovich wrote
> >> >
> >> > The code works as-is.
> >> >
> >> > Absolutely.  volatile is not needed for correctness
> >> >
> >> > Vitaly Davidovich wrote
> >> >
> >> > Why?
> >> >
> >> > Well, for performance reasons given the 'undefined/indefinite'
> >> > visibility of
> >> > #hash to other threads.
> >> > At least according to the JMM (which has nothing to say about CPU
> cache
> >> > coherency), it is *possible* that each distinct thread that invoked
> >> > #hashCode() *could* result in a recalculation of the hash.
> >> >
> >> > In practice though, application threads contain very frequent
> >> > synchronization actions, or other operations that force VM to
> >> > flush/reload. So it won't take very long for any non-volatile write in
> >> > one thread to become visible to other threads.
> >> >
> >> > Imagine a long-lived Map<String, ?>; and many threads accessing the
> >> > map's
> >> > keyset and for some unknown reason invoking #hashCode() on each key.
> >> > If #hash was declared volatile, although there is no guarantee that
> >> > #hash
> >> > would only be calculated once, it is guaranteed that once a write to
> >> > main
> >> > memory was completed, every *subsequent* (here meaning after the write
> >> > to
> >> >
> >> > In JMM though, we cannot even express this guarantee. Say we have
> >> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
> >> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
> >> > #hash which takes 100 ns. We can find no guarantee from JMM that
> >> > there's only one write; it's legal that every thread sees 0 from the
> >> > volatile read.
> >> >
> >> > Zhong Yu
> >> >
> >> > main memory) read no matter from which thread would see #hash != 0 and
> >> > therefore skip the calculation.
> >> >
> >> >
> >> >
> >> > Vitaly Davidovich wrote
> >> >
> >> > String is too high profile (especially
> >> > hashing it) to do the "naive" thing.
> >> >
> >> > Nothing wrong with being naive; naive can be charming.
> >> >
> >> >
> >> > Vitaly Davidovich wrote
> >> >
> >> > Also, some architectures pay a
> >> > penalty for volatile loads and you'd incur that each time.
> >> >
> >> > Fair point; the JDK authors only get one shot and they can't assume
> that
> >> > volatile reads are cheap
> >> >
> >> >
> >> >
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >> >
> >> >
> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
> >> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >> >
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/d19170fa/attachment.html>

From vitalyd at gmail.com  Wed Apr 17 13:35:57 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 13:35:57 -0400
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516ED9BA.50307@oracle.com>
References: <516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<516D6BA1.5000506@cytetech.com> <516D79BE.2060506@oracle.com>
	<516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
	<CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
	<516ED6A3.3080602@oracle.com>
	<CAHjP37HS70CLh8efBkE2L3n52H86LJxtRqudy2iEKUMO4sRqmA@mail.gmail.com>
	<516ED9BA.50307@oracle.com>
Message-ID: <CAHjP37FeWh1xWU72kL9hJuCJV6YcY_VfFycm1m8vPEDqU3Vgdg@mail.gmail.com>

I fully realize that it may not notice them even if every write was
issued.  However, if code is written like that then I'd expect the JVM to
not optimize them because the other entity "could" notice it as well.  I
guess my point is that JMM may allow that because it's phrased in a
somewhat abstract manner, but I'd be surprised if JVMs do that
optimization.  Not saying anyone should rely on that, but this is purely
from practical standpoint.

Sent from my phone
On Apr 17, 2013 1:26 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
wrote:

>  How would a external entity notice that there were multiple writes?
>
>
> Alex
>
>  On 17/04/2013 18:18, Vitaly Davidovich wrote:
>
> Yes, but where does it say that #4 is valid and under what conditions?
> Presumably in this example, shared is read somewhere else as well
> (different method perhaps) - else why are we calling it shared? Only
> writers? That's uninteresting.
>
> Sent from my phone
>  Yes.
>
> 1. Unroll the loop.
> 2. Reorder non-volatile accesses to go ahead of volatile stores
> 3. Now all volatile stores from all iterations are one after the other
> 4. Multiple stores to the same location can be fused, even if they are
> volatile stores
>
> If you want a store in every iteration, need a volatile load somewhere
> between the stores.
>
> Alex
>
>
> On 17/04/2013 18:01, Vitaly Davidovich wrote:
>
> Are you sure about fusion if shared is volatile? Maybe theoretically it's
> possible but the analysis (and risk) the JVM would have to do to make that
> transform is probably impractical.  Intuitively, if I mark something
> volatile, I expect almost verbatim code to execute.
>
> By instructions, I mean loads and stores since that's what actually
> matters for these scenarios.
> On Apr 17, 2013 12:55 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  Yes, but not any instructions - only those that have temporal
>> relationship.
>>
>> So, for example, in this case even if shared is volatile, then volatile
>> stores can still be moved out of the loop and fused into a single volatile
>> store.
>>
>> Alex
>>
>>
>> On 17/04/2013 17:43, Vitaly Davidovich wrote:
>>
>> Immediate means store is made globally visible before subsequent
>> instructions complete/retire.
>> On Apr 17, 2013 12:26 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  What is this "immediate" anyway?
>>>
>>> It is actually "before anything else in this thread that's temporal"
>>>
>>> Alex
>>>
>>>
>>>  On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>>
>>> I actually expect that the optimizer *would* do this transformation on
>>> plain fields (provided it doesn't break intra-thread semantics, of course)
>>> because it's a perf gain.
>>>
>>> Don't know how much JIT can see through println as it ultimately calls
>>> into runtime and OS functions, so I'd guess it doesn't know enough or
>>> simply plays conservative here.  However, Nathan's point is still valid
>>> even if example isn't necessarily the best one.  If you had "pure" java
>>> code instead of an I/O call that took significant time to execute, the
>>> write would be delayed.  I'm not sure why that matters though for benign
>>> data races.  Clearly if you need immediate visibility, you code for that
>>> specifically.
>>> On Apr 17, 2013 11:32 AM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>>>
>>>> On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>>> <nathan.reynolds at oracle.com> wrote:
>>>> > Couldn't JIT hoist the non-volatile writes out of the loop?
>>>>
>>>> Certainly, sorry if my statement sounds too absolute.
>>>>
>>>> > For example, the following code...
>>>>
>>>> But, is this a valid example? Can JMM really reorder around
>>>> System.out.println()?
>>>>
>>>> > for (i = 0; i < 1_000_000_000; i++)
>>>> > {
>>>> >     System.out.println(i);
>>>> >     shared = 2 * i;
>>>> > }
>>>> >
>>>> > ... could be transformed into ...
>>>> >
>>>> > for (i = 0; i < 1_000_000_000; i++)
>>>> > {
>>>> >     System.out.println(i);
>>>> > }
>>>> >
>>>> > shared = 2 * 1_000_000_000;
>>>> >
>>>> > ... If so, then the non-volatile write may not happen for a very long
>>>> time.
>>>> >
>>>> > Nathan Reynolds | Architect | 602.333.9091
>>>> > Oracle PSR Engineering | Server Technology
>>>> > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>> >
>>>> > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn <
>>>> thurston at nomagicsoftware.com>
>>>> > wrote:
>>>> >
>>>> > Vitaly Davidovich wrote
>>>> >
>>>> > The code works as-is.
>>>> >
>>>> > Absolutely.  volatile is not needed for correctness
>>>> >
>>>> > Vitaly Davidovich wrote
>>>> >
>>>> > Why?
>>>> >
>>>> > Well, for performance reasons given the 'undefined/indefinite'
>>>> visibility of
>>>> > #hash to other threads.
>>>> > At least according to the JMM (which has nothing to say about CPU
>>>> cache
>>>> > coherency), it is *possible* that each distinct thread that invoked
>>>> > #hashCode() *could* result in a recalculation of the hash.
>>>> >
>>>> > In practice though, application threads contain very frequent
>>>> > synchronization actions, or other operations that force VM to
>>>> > flush/reload. So it won't take very long for any non-volatile write in
>>>> > one thread to become visible to other threads.
>>>> >
>>>> > Imagine a long-lived Map<String, ?>; and many threads accessing the
>>>> map's
>>>> > keyset and for some unknown reason invoking #hashCode() on each key.
>>>> > If #hash was declared volatile, although there is no guarantee that
>>>> #hash
>>>> > would only be calculated once, it is guaranteed that once a write to
>>>> main
>>>> > memory was completed, every *subsequent* (here meaning after the
>>>> write to
>>>> >
>>>> > In JMM though, we cannot even express this guarantee. Say we have
>>>> > threads T1...Tn, each thread Ti burns `i` seconds CPU time first, then
>>>> > volatile-reads #hash, and if it's 0, calculates and volatile-writes
>>>> > #hash which takes 100 ns. We can find no guarantee from JMM that
>>>> > there's only one write; it's legal that every thread sees 0 from the
>>>> > volatile read.
>>>> >
>>>> > Zhong Yu
>>>> >
>>>> > main memory) read no matter from which thread would see #hash != 0 and
>>>> > therefore skip the calculation.
>>>> >
>>>> >
>>>> >
>>>> > Vitaly Davidovich wrote
>>>> >
>>>> > String is too high profile (especially
>>>> > hashing it) to do the "naive" thing.
>>>> >
>>>> > Nothing wrong with being naive; naive can be charming.
>>>> >
>>>> >
>>>> > Vitaly Davidovich wrote
>>>> >
>>>> > Also, some architectures pay a
>>>> > penalty for volatile loads and you'd incur that each time.
>>>> >
>>>> > Fair point; the JDK authors only get one shot and they can't assume
>>>> that
>>>> > volatile reads are cheap
>>>> >
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > View this message in context:
>>>> >
>>>> http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>> > Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> >
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> >
>>>> >
>>>> >
>>>> >
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> >
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/565f08fe/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Apr 17 13:42:26 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 17 Apr 2013 18:42:26 +0100
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37FeWh1xWU72kL9hJuCJV6YcY_VfFycm1m8vPEDqU3Vgdg@mail.gmail.com>
References: <516C5285.90504@oracle.com> <516D6BA1.5000506@cytetech.com>
	<516D79BE.2060506@oracle.com> <516D8350.5010800@oracle.com>
	<1366148958661-9454.post@n7.nabble.com>
	<516DCA50.9000702@oracle.com>
	<1366154826058-9461.post@n7.nabble.com>
	<CAHjP37F9TbfwBB0TpYPnNDUUPOfrCKZunZFFRUCaY231zqseuQ@mail.gmail.com>
	<1366163519386-9466.post@n7.nabble.com>
	<CACuKZqFZjpLYXcV2xYfhDTaQXNAh9RdAun=g4AxOSq2tZCJPtw@mail.gmail.com>
	<516E4383.1020103@oracle.com>
	<CACuKZqFTA2joztR=2sxrbjcv=64wFn7n3FJUqpPN3Sjj=bYMMA@mail.gmail.com>
	<CAHjP37Gw7p=GyCbE9pU1iY0EMedSqOwYqq3WU5mN2xfJcXABZw@mail.gmail.com>
	<516ECB85.9060202@oracle.com>
	<CAHjP37GMs0TjPOni6oYAgYWGcuf+t=LddMBY2MXsfW9bfhJLrA@mail.gmail.com>
	<516ED2A8.1060506@oracle.com>
	<CAHjP37HHn0Bb+gdHZgoFziXHLMfdBA9arNwYf56r9rkrUQ2+4w@mail.gmail.com>
	<516ED6A3.3080602@oracle.com>
	<CAHjP37HS70CLh8efBkE2L3n52H86LJxtRqudy2iEKUMO4sRqmA@mail.gmail.com>
	<516ED9BA.50307@oracle.com>
	<CAHjP37FeWh1xWU72kL9hJuCJV6YcY_VfFycm1m8vPEDqU3Vgdg@mail.gmail.com>
Message-ID: <516EDF02.9090206@oracle.com>

On the contrary, if it can unroll the loops, and if it can reorder 
non-volatile stuff with volatile stores, then you should expect you 
won't notice more than one write.

Alex

On 17/04/2013 18:35, Vitaly Davidovich wrote:
>
> I fully realize that it may not notice them even if every write was 
> issued.  However, if code is written like that then I'd expect the JVM 
> to not optimize them because the other entity "could" notice it as 
> well.  I guess my point is that JMM may allow that because it's 
> phrased in a somewhat abstract manner, but I'd be surprised if JVMs do 
> that optimization.  Not saying anyone should rely on that, but this is 
> purely from practical standpoint.
>
> Sent from my phone
>
> On Apr 17, 2013 1:26 PM, "oleksandr otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     How would a external entity notice that there were multiple writes?
>
>
>     Alex
>
>     On 17/04/2013 18:18, Vitaly Davidovich wrote:
>>
>>     Yes, but where does it say that #4 is valid and under what
>>     conditions? Presumably in this example, shared is read somewhere
>>     else as well (different method perhaps) - else why are we calling
>>     it shared? Only writers? That's uninteresting.
>>
>>     Sent from my phone
>>
>>     Yes.
>>
>>     1. Unroll the loop.
>>     2. Reorder non-volatile accesses to go ahead of volatile stores
>>     3. Now all volatile stores from all iterations are one after the
>>     other
>>     4. Multiple stores to the same location can be fused, even if
>>     they are volatile stores
>>
>>     If you want a store in every iteration, need a volatile load
>>     somewhere between the stores.
>>
>>     Alex
>>
>>
>>     On 17/04/2013 18:01, Vitaly Davidovich wrote:
>>>
>>>     Are you sure about fusion if shared is volatile? Maybe
>>>     theoretically it's possible but the analysis (and risk) the JVM
>>>     would have to do to make that transform is probably impractical.
>>>     Intuitively, if I mark something volatile, I expect almost
>>>     verbatim code to execute.
>>>
>>>     By instructions, I mean loads and stores since that's what
>>>     actually matters for these scenarios.
>>>
>>>     On Apr 17, 2013 12:55 PM, "oleksandr otenko"
>>>     <oleksandr.otenko at oracle.com
>>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>         Yes, but not any instructions - only those that have
>>>         temporal relationship.
>>>
>>>         So, for example, in this case even if shared is volatile,
>>>         then volatile stores can still be moved out of the loop and
>>>         fused into a single volatile store.
>>>
>>>         Alex
>>>
>>>
>>>         On 17/04/2013 17:43, Vitaly Davidovich wrote:
>>>>
>>>>         Immediate means store is made globally visible before
>>>>         subsequent instructions complete/retire.
>>>>
>>>>         On Apr 17, 2013 12:26 PM, "oleksandr otenko"
>>>>         <oleksandr.otenko at oracle.com
>>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>             What is this "immediate" anyway?
>>>>
>>>>             It is actually "before anything else in this thread
>>>>             that's temporal"
>>>>
>>>>             Alex
>>>>
>>>>
>>>>             On 17/04/2013 16:44, Vitaly Davidovich wrote:
>>>>>
>>>>>             I actually expect that the optimizer *would* do this
>>>>>             transformation on plain fields (provided it doesn't
>>>>>             break intra-thread semantics, of course) because it's
>>>>>             a perf gain.
>>>>>
>>>>>             Don't know how much JIT can see through println as it
>>>>>             ultimately calls into runtime and OS functions, so I'd
>>>>>             guess it doesn't know enough or simply plays
>>>>>             conservative here.  However, Nathan's point is still
>>>>>             valid even if example isn't necessarily the best one. 
>>>>>             If you had "pure" java code instead of an I/O call
>>>>>             that took significant time to execute, the write would
>>>>>             be delayed.  I'm not sure why that matters though for
>>>>>             benign data races.  Clearly if you need immediate
>>>>>             visibility, you code for that specifically.
>>>>>
>>>>>             On Apr 17, 2013 11:32 AM, "Zhong Yu"
>>>>>             <zhong.j.yu at gmail.com <mailto:zhong.j.yu at gmail.com>>
>>>>>             wrote:
>>>>>
>>>>>                 On Wed, Apr 17, 2013 at 1:38 AM, Nathan Reynolds
>>>>>                 <nathan.reynolds at oracle.com
>>>>>                 <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>                 > Couldn't JIT hoist the non-volatile writes out
>>>>>                 of the loop?
>>>>>
>>>>>                 Certainly, sorry if my statement sounds too absolute.
>>>>>
>>>>>                 > For example, the following code...
>>>>>
>>>>>                 But, is this a valid example? Can JMM really
>>>>>                 reorder around
>>>>>                 System.out.println()?
>>>>>
>>>>>                 > for (i = 0; i < 1_000_000_000; i++)
>>>>>                 > {
>>>>>                 >     System.out.println(i);
>>>>>                 >     shared = 2 * i;
>>>>>                 > }
>>>>>                 >
>>>>>                 > ... could be transformed into ...
>>>>>                 >
>>>>>                 > for (i = 0; i < 1_000_000_000; i++)
>>>>>                 > {
>>>>>                 >     System.out.println(i);
>>>>>                 > }
>>>>>                 >
>>>>>                 > shared = 2 * 1_000_000_000;
>>>>>                 >
>>>>>                 > ... If so, then the non-volatile write may not
>>>>>                 happen for a very long time.
>>>>>                 >
>>>>>                 > Nathan Reynolds | Architect | 602.333.9091
>>>>>                 <tel:602.333.9091>
>>>>>                 > Oracle PSR Engineering | Server Technology
>>>>>                 > On 4/16/2013 10:27 PM, Zhong Yu wrote:
>>>>>                 >
>>>>>                 > On Tue, Apr 16, 2013 at 8:51 PM, thurstonn
>>>>>                 <thurston at nomagicsoftware.com
>>>>>                 <mailto:thurston at nomagicsoftware.com>>
>>>>>                 > wrote:
>>>>>                 >
>>>>>                 > Vitaly Davidovich wrote
>>>>>                 >
>>>>>                 > The code works as-is.
>>>>>                 >
>>>>>                 > Absolutely.  volatile is not needed for correctness
>>>>>                 >
>>>>>                 > Vitaly Davidovich wrote
>>>>>                 >
>>>>>                 > Why?
>>>>>                 >
>>>>>                 > Well, for performance reasons given the
>>>>>                 'undefined/indefinite' visibility of
>>>>>                 > #hash to other threads.
>>>>>                 > At least according to the JMM (which has nothing
>>>>>                 to say about CPU cache
>>>>>                 > coherency), it is *possible* that each distinct
>>>>>                 thread that invoked
>>>>>                 > #hashCode() *could* result in a recalculation of
>>>>>                 the hash.
>>>>>                 >
>>>>>                 > In practice though, application threads contain
>>>>>                 very frequent
>>>>>                 > synchronization actions, or other operations
>>>>>                 that force VM to
>>>>>                 > flush/reload. So it won't take very long for any
>>>>>                 non-volatile write in
>>>>>                 > one thread to become visible to other threads.
>>>>>                 >
>>>>>                 > Imagine a long-lived Map<String, ?>; and many
>>>>>                 threads accessing the map's
>>>>>                 > keyset and for some unknown reason invoking
>>>>>                 #hashCode() on each key.
>>>>>                 > If #hash was declared volatile, although there
>>>>>                 is no guarantee that #hash
>>>>>                 > would only be calculated once, it is guaranteed
>>>>>                 that once a write to main
>>>>>                 > memory was completed, every *subsequent* (here
>>>>>                 meaning after the write to
>>>>>                 >
>>>>>                 > In JMM though, we cannot even express this
>>>>>                 guarantee. Say we have
>>>>>                 > threads T1...Tn, each thread Ti burns `i`
>>>>>                 seconds CPU time first, then
>>>>>                 > volatile-reads #hash, and if it's 0, calculates
>>>>>                 and volatile-writes
>>>>>                 > #hash which takes 100 ns. We can find no
>>>>>                 guarantee from JMM that
>>>>>                 > there's only one write; it's legal that every
>>>>>                 thread sees 0 from the
>>>>>                 > volatile read.
>>>>>                 >
>>>>>                 > Zhong Yu
>>>>>                 >
>>>>>                 > main memory) read no matter from which thread
>>>>>                 would see #hash != 0 and
>>>>>                 > therefore skip the calculation.
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 > Vitaly Davidovich wrote
>>>>>                 >
>>>>>                 > String is too high profile (especially
>>>>>                 > hashing it) to do the "naive" thing.
>>>>>                 >
>>>>>                 > Nothing wrong with being naive; naive can be
>>>>>                 charming.
>>>>>                 >
>>>>>                 >
>>>>>                 > Vitaly Davidovich wrote
>>>>>                 >
>>>>>                 > Also, some architectures pay a
>>>>>                 > penalty for volatile loads and you'd incur that
>>>>>                 each time.
>>>>>                 >
>>>>>                 > Fair point; the JDK authors only get one shot
>>>>>                 and they can't assume that
>>>>>                 > volatile reads are cheap
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 > --
>>>>>                 > View this message in context:
>>>>>                 >
>>>>>                 http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9466.html
>>>>>                 > Sent from the JSR166 Concurrency mailing list
>>>>>                 archive at Nabble.com.
>>>>>                 > _______________________________________________
>>>>>                 > Concurrency-interest mailing list
>>>>>                 > Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 >
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>                 >
>>>>>                 > _______________________________________________
>>>>>                 > Concurrency-interest mailing list
>>>>>                 > Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 >
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 >
>>>>>                 > _______________________________________________
>>>>>                 > Concurrency-interest mailing list
>>>>>                 > Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 >
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>                 >
>>>>>                 _______________________________________________
>>>>>                 Concurrency-interest mailing list
>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>             _______________________________________________
>>>>>             Concurrency-interest mailing list
>>>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/55ee7cb2/attachment-0001.html>

From heinz at javaspecialists.eu  Wed Apr 17 14:28:45 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 17 Apr 2013 21:28:45 +0300
Subject: [concurrency-interest] New version of Java Concurrent Animated
In-Reply-To: <516ECA86.3050303@oracle.com>
References: <CA+y1Pu9ErOzBeRj4Q+0tUepsEci73pk3fGPwr_YERvVvR1P39Q@mail.gmail.com>
	<516E4276.90504@oracle.com> <516E4A12.1060903@javaspecialists.eu>
	<516ECA86.3050303@oracle.com>
Message-ID: <516EE9DD.6080902@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/0ab987e1/attachment.html>

From mudit.f2004912 at gmail.com  Wed Apr 17 14:32:37 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 17 Apr 2013 20:32:37 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
Message-ID: <CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>

@Nathan,
Yes I am aware that new implementation of ConcurrentHashMap which is
rolling out in Java 8. I am using Java 7. However, I don't think it will
change the behavior because all the threads are hitting the same bucket. To
best of my knowledge, the difference between Java 8 and 7 is that the lock
is acquired per bucket, while in latter case its acquired per segment.
This, I believe should not change the strange behavior I am seeing.
Anyhow, even with the current implementation, its just weird that in low
contention performance deteriorates.


@Vitaly,
ThinkTime is nothing but the a simple loop made to run X number of times.
We are considering one iteration as one cycle. This is not absolutely
correct since one iteration should take more cycles (5-6) including
increasing the counter and terminate condition. But this should not change
the graph. This is only going to shift the graph to the right a bit.

@Kimo,
Thanks for the links.  I'll take a look. But the problem is not with the
CAS.  I guess the issue is with ReentrantLock. Current implemenation try
CASing 64 times and after that it goes for ReentrantLock. Under high
contention most of the times all 64 CAS will fail anyway and hashMap will
have to resort to ReentrantLocking. We are just trying to understand this
strange behavior.


Thanks,
Mudit
Intern, INRIA, Paris


On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> What exactly are you doing in ThinkTime?
> On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>
>> Hi All,
>>
>> I recently performed a scalability test (very aggressive and may be not
>> practical, anyhow) on put operation of ConcurrentHashMap.
>>
>> Test: Each thread is trying to put (Same Key, Random Value) in HashMap in
>> a tight loop. Therefore, all the threads will hit the same location on
>> hashMap and will cause contention.
>>
>> What is more surprising is, when each thread continue to do another put
>> one after the other, avg time taken in one put operation is lesser than
>> when a thread do some other work between two put operations.
>>
>> We continue to see the increase in per operation time by increasing the
>> work done in between.  This is very counter intuitive. Only after a work of
>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>
>> When I read the code, I found out that put op first try to use CAS to
>> aquire the lock(64 times on multicore), only if it could not acquire the
>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>> threads .. ).
>>
>> We also tweaked, the number of times CAS (from 0 to 10,000) is used
>> before actually going for ReentrantLocking.   Attached is the graph.
>>
>> One interesting thing to note. As we increase the work between two ops,
>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>> when there is a mixture of two (CAS+Lock).
>>
>> Code Skeleton: For each Thread
>>
>> While() {
>>   hashMap.put(K,randomValue);     // K is same for each thread
>>   ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>
>> }
>>
>>  Machine: 48 core NUMA
>> Threads used:  32 (each one is pinned to a core).
>> #ops: In total 51200000 (each thread with 160000 ops)
>>
>> That's like saying, if I do something else in between my operations (upto
>> a limit), contention will increase.  Very strange.
>>
>> Does anyone of you know why is it happening?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/8b49209e/attachment.html>

From nathan.reynolds at oracle.com  Wed Apr 17 14:41:26 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 17 Apr 2013 11:41:26 -0700
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
Message-ID: <516EECD6.1020806@oracle.com>

 > lock is acquired per bucket

I wasn't aware of the implementation.  I see your point.  There might 
not be any difference.

 > ThinkTime is nothing but the a simple loop made to run X number of times

As for ThinkTime, are you sure JIT didn't get rid of your loop and now 
ThinkTime runs in very little time?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/17/2013 11:32 AM, Mudit Verma wrote:
> @Nathan,
> Yes I am aware that new implementation of ConcurrentHashMap which is 
> rolling out in Java 8. I am using Java 7. However, I don't think it 
> will change the behavior because all the threads are hitting the same 
> bucket. To best of my knowledge, the difference between Java 8 and 7 
> is that the lock is acquired per bucket, while in latter case its 
> acquired per segment. This, I believe should not change the strange 
> behavior I am seeing.  Anyhow, even with the current implementation, 
> its just weird that in low contention performance deteriorates.
>
>
> @Vitaly,
> ThinkTime is nothing but the a simple loop made to run X number of 
> times. We are considering one iteration as one cycle. This is not 
> absolutely correct since one iteration should take more cycles (5-6) 
> including increasing the counter and terminate condition. But this 
> should not change the graph. This is only going to shift the graph to 
> the right a bit.
>
> @Kimo,
> Thanks for the links.  I'll take a look. But the problem is not with 
> the CAS.  I guess the issue is with ReentrantLock. Current 
> implemenation try CASing 64 times and after that it goes for 
> ReentrantLock. Under high contention most of the times all 64 CAS will 
> fail anyway and hashMap will have to resort to ReentrantLocking. We 
> are just trying to understand this strange behavior.
>
>
> Thanks,
> Mudit
> Intern, INRIA, Paris
>
>
> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     What exactly are you doing in ThinkTime?
>
>     On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com
>     <mailto:mudit.f2004912 at gmail.com>> wrote:
>
>         Hi All,
>
>         I recently performed a scalability test (very aggressive and
>         may be not practical, anyhow) on put operation of
>         ConcurrentHashMap.
>
>         Test: Each thread is trying to put (Same Key, Random Value) in
>         HashMap in a tight loop. Therefore, all the threads will hit
>         the same location on hashMap and will cause contention.
>
>         What is more surprising is, when each thread continue to do
>         another put one after the other, avg time taken in one put
>         operation is lesser than when a thread do some other work
>         between two put operations.
>
>         We continue to see the increase in per operation time by
>         increasing the work done in between.  This is very counter
>         intuitive. Only after a work of about 10,000 - 20,000 cycles
>         in between, per op time comes down.
>
>         When I read the code, I found out that put op first try to use
>         CAS to aquire the lock(64 times on multicore), only if it
>         could not acquire the lock on segment through CASing, it goes
>         for ReentrantLocking (which suspend threads .. ).
>
>         We also tweaked, the number of times CAS (from 0 to 10,000) is
>         used before actually going for ReentrantLocking. Attached is
>         the graph.
>
>         One interesting thing to note. As we increase the work between
>         two ops, locking with 0 CAS (pure ReentrantLocking) seems to
>         be worst affected with the spike. Therefore, I assume that,
>         spike comes from ReentractLocking even when there is a mixture
>         of two (CAS+Lock).
>
>         Code Skeleton: For each Thread
>
>         While() {
>           hashMap.put(K,randomValue);     // K is same for each thread
>           ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>
>         }
>
>          Machine: 48 core NUMA
>         Threads used:  32 (each one is pinned to a core).
>         #ops: In total 51200000 (each thread with 160000 ops)
>
>         That's like saying, if I do something else in between my
>         operations (upto a limit), contention will increase.  Very
>         strange.
>
>         Does anyone of you know why is it happening?
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/496762e3/attachment-0001.html>

From hans.boehm at hp.com  Wed Apr 17 14:52:36 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 17 Apr 2013 18:52:36 +0000
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516DBFBA.7080104@oracle.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>	<516DA793.4060606@oracle.com>
	<516DBFBA.7080104@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369D57FA@G9W0725.americas.hpqcorp.net>

From: Nathan Reynolds

>? Many data races can exist without violating A, C or I and hence be harmless.
> These data races are great ways to improve scalability.
Absolute performance, probably.  Scalability, I doubt it.  Fences on x86 seem to involve just local work which doesn't involve the memory system.  Programs whose performance is limited by fences tend to scale well.  See my RACES 12 workshop paper http://www.hpl.hp.com/techreports/2012/HPL-2012-218.pdf.

> Setting the String hash value is racy yet harmless
Probably.  Aside from the fact that we know that the specified semantics for racy accesses are not correct, and hence are not quite sure what the real rules are.

Hans



From mudit.f2004912 at gmail.com  Wed Apr 17 15:01:30 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Wed, 17 Apr 2013 21:01:30 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <516EECD6.1020806@oracle.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
Message-ID: <CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>

Well, as per JIT. I am not sure. I don't think its the case. As with
increasing ThinkTime, experiment completion time visibly increases.

Also, alternate to counting iterations, we used System.nanoSec call, to
actually make ThinkTime loop wait for X no of nano secs before it
terminates.

With frequency of our machine, 1 nano sec = 2 cycles roughly.

We see same graph in both the cases.


On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  > lock is acquired per bucket
>
> I wasn't aware of the implementation.  I see your point.  There might not
> be any difference.
>
>
> > ThinkTime is nothing but the a simple loop made to run X number of times
>
> As for ThinkTime, are you sure JIT didn't get rid of your loop and now
> ThinkTime runs in very little time?
>
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/17/2013 11:32 AM, Mudit Verma wrote:
>
>    @Nathan,
> Yes I am aware that new implementation of ConcurrentHashMap which is
> rolling out in Java 8. I am using Java 7. However, I don't think it will
> change the behavior because all the threads are hitting the same bucket. To
> best of my knowledge, the difference between Java 8 and 7 is that the lock
> is acquired per bucket, while in latter case its acquired per segment.
> This, I believe should not change the strange behavior I am seeing.
> Anyhow, even with the current implementation, its just weird that in low
> contention performance deteriorates.
>
>
>  @Vitaly,
>  ThinkTime is nothing but the a simple loop made to run X number of times.
> We are considering one iteration as one cycle. This is not absolutely
> correct since one iteration should take more cycles (5-6) including
> increasing the counter and terminate condition. But this should not change
> the graph. This is only going to shift the graph to the right a bit.
>
>  @Kimo,
> Thanks for the links.  I'll take a look. But the problem is not with the
> CAS.  I guess the issue is with ReentrantLock. Current implemenation try
> CASing 64 times and after that it goes for ReentrantLock. Under high
> contention most of the times all 64 CAS will fail anyway and hashMap will
> have to resort to ReentrantLocking. We are just trying to understand this
> strange behavior.
>
>
>  Thanks,
>  Mudit
> Intern, INRIA, Paris
>
>
> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> What exactly are you doing in ThinkTime?
>>  On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>>
>>>     Hi All,
>>>
>>> I recently performed a scalability test (very aggressive and may be not
>>> practical, anyhow) on put operation of ConcurrentHashMap.
>>>
>>>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
>>> in a tight loop. Therefore, all the threads will hit the same location on
>>> hashMap and will cause contention.
>>>
>>>  What is more surprising is, when each thread continue to do another
>>> put one after the other, avg time taken in one put operation is lesser than
>>> when a thread do some other work between two put operations.
>>>
>>> We continue to see the increase in per operation time by increasing the
>>> work done in between.  This is very counter intuitive. Only after a work of
>>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>>
>>>  When I read the code, I found out that put op first try to use CAS to
>>> aquire the lock(64 times on multicore), only if it could not acquire the
>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>> threads .. ).
>>>
>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>
>>> One interesting thing to note. As we increase the work between two ops,
>>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>>> when there is a mixture of two (CAS+Lock).
>>>
>>>  Code Skeleton: For each Thread
>>>
>>> While() {
>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>
>>> }
>>>
>>>  Machine: 48 core NUMA
>>>  Threads used:  32 (each one is pinned to a core).
>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>
>>>  That's like saying, if I do something else in between my operations
>>> (upto a limit), contention will increase.  Very strange.
>>>
>>>  Does anyone of you know why is it happening?
>>>
>>>  _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/f8cbf83f/attachment.html>

From vitalyd at gmail.com  Wed Apr 17 21:02:36 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 21:02:36 -0400
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
Message-ID: <CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>

What's strange in your graph is when think time is very high, op time drops
down even further.  Are you warming up the C2 compiler properly? The drop
at the tail end almost seems like your think it time is being reduced to
almost nothing, which I can see happening if your think time is trivial and
C2 aggressively optimizes it.

Are you also using tiered compilation? If so, turn it off.

Sent from my phone
On Apr 17, 2013 3:04 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:

> Well, as per JIT. I am not sure. I don't think its the case. As with
> increasing ThinkTime, experiment completion time visibly increases.
>
> Also, alternate to counting iterations, we used System.nanoSec call, to
> actually make ThinkTime loop wait for X no of nano secs before it
> terminates.
>
> With frequency of our machine, 1 nano sec = 2 cycles roughly.
>
> We see same graph in both the cases.
>
>
> On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds <
> nathan.reynolds at oracle.com> wrote:
>
>>  > lock is acquired per bucket
>>
>> I wasn't aware of the implementation.  I see your point.  There might not
>> be any difference.
>>
>>
>> > ThinkTime is nothing but the a simple loop made to run X number of times
>>
>> As for ThinkTime, are you sure JIT didn't get rid of your loop and now
>> ThinkTime runs in very little time?
>>
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 4/17/2013 11:32 AM, Mudit Verma wrote:
>>
>>    @Nathan,
>> Yes I am aware that new implementation of ConcurrentHashMap which is
>> rolling out in Java 8. I am using Java 7. However, I don't think it will
>> change the behavior because all the threads are hitting the same bucket. To
>> best of my knowledge, the difference between Java 8 and 7 is that the lock
>> is acquired per bucket, while in latter case its acquired per segment.
>> This, I believe should not change the strange behavior I am seeing.
>> Anyhow, even with the current implementation, its just weird that in low
>> contention performance deteriorates.
>>
>>
>>  @Vitaly,
>>  ThinkTime is nothing but the a simple loop made to run X number of
>> times. We are considering one iteration as one cycle. This is not
>> absolutely correct since one iteration should take more cycles (5-6)
>> including increasing the counter and terminate condition. But this should
>> not change the graph. This is only going to shift the graph to the right a
>> bit.
>>
>>  @Kimo,
>> Thanks for the links.  I'll take a look. But the problem is not with the
>> CAS.  I guess the issue is with ReentrantLock. Current implemenation try
>> CASing 64 times and after that it goes for ReentrantLock. Under high
>> contention most of the times all 64 CAS will fail anyway and hashMap will
>> have to resort to ReentrantLocking. We are just trying to understand this
>> strange behavior.
>>
>>
>>  Thanks,
>>  Mudit
>> Intern, INRIA, Paris
>>
>>
>> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> What exactly are you doing in ThinkTime?
>>>  On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com>
>>> wrote:
>>>
>>>>     Hi All,
>>>>
>>>> I recently performed a scalability test (very aggressive and may be not
>>>> practical, anyhow) on put operation of ConcurrentHashMap.
>>>>
>>>>  Test: Each thread is trying to put (Same Key, Random Value) in
>>>> HashMap in a tight loop. Therefore, all the threads will hit the same
>>>> location on hashMap and will cause contention.
>>>>
>>>>  What is more surprising is, when each thread continue to do another
>>>> put one after the other, avg time taken in one put operation is lesser than
>>>> when a thread do some other work between two put operations.
>>>>
>>>> We continue to see the increase in per operation time by increasing the
>>>> work done in between.  This is very counter intuitive. Only after a work of
>>>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>>>
>>>>  When I read the code, I found out that put op first try to use CAS to
>>>> aquire the lock(64 times on multicore), only if it could not acquire the
>>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>>> threads .. ).
>>>>
>>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>>
>>>> One interesting thing to note. As we increase the work between two ops,
>>>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>>>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>>>> when there is a mixture of two (CAS+Lock).
>>>>
>>>>  Code Skeleton: For each Thread
>>>>
>>>> While() {
>>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>>
>>>> }
>>>>
>>>>  Machine: 48 core NUMA
>>>>  Threads used:  32 (each one is pinned to a core).
>>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>>
>>>>  That's like saying, if I do something else in between my operations
>>>> (upto a limit), contention will increase.  Very strange.
>>>>
>>>>  Does anyone of you know why is it happening?
>>>>
>>>>  _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/80c7bc0f/attachment-0001.html>

From mudit.f2004912 at gmail.com  Wed Apr 17 21:20:19 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 03:20:19 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
Message-ID: <CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>

@Vitaly,

I am sorry I didn't get you correctly.

What's strange in your graph is when think time is very high, op time drops
down even further.

This is expected behavior.  More the think time, lesser and lesser should
be the contention.

What's strange in your graph is when think time is very high, op time drops
down even further.

No,  Why?  op time on Y axis is just the average time a put operation take
(time taken in HashMap.put invocation and return).  It doesn't include
think Time.  Therefore,  at around 1,00,000 cycles of ThinkTime,   threads
are doing enough work in between the operation and enough interleaved that
they don't hit bucket at the same time, and hence very low (almost nothing)
time in put op.


Thanks,
Mudit




On Thu, Apr 18, 2013 at 3:02 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> What's strange in your graph is when think time is very high, op time
> drops down even further.  Are you warming up the C2 compiler properly? The
> drop at the tail end almost seems like your think it time is being reduced
> to almost nothing, which I can see happening if your think time is trivial
> and C2 aggressively optimizes it.
>
> Are you also using tiered compilation? If so, turn it off.
>
> Sent from my phone
> On Apr 17, 2013 3:04 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>
>> Well, as per JIT. I am not sure. I don't think its the case. As with
>> increasing ThinkTime, experiment completion time visibly increases.
>>
>> Also, alternate to counting iterations, we used System.nanoSec call, to
>> actually make ThinkTime loop wait for X no of nano secs before it
>> terminates.
>>
>> With frequency of our machine, 1 nano sec = 2 cycles roughly.
>>
>> We see same graph in both the cases.
>>
>>
>> On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds <
>> nathan.reynolds at oracle.com> wrote:
>>
>>>  > lock is acquired per bucket
>>>
>>> I wasn't aware of the implementation.  I see your point.  There might
>>> not be any difference.
>>>
>>>
>>> > ThinkTime is nothing but the a simple loop made to run X number of
>>> times
>>>
>>> As for ThinkTime, are you sure JIT didn't get rid of your loop and now
>>> ThinkTime runs in very little time?
>>>
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 4/17/2013 11:32 AM, Mudit Verma wrote:
>>>
>>>    @Nathan,
>>> Yes I am aware that new implementation of ConcurrentHashMap which is
>>> rolling out in Java 8. I am using Java 7. However, I don't think it will
>>> change the behavior because all the threads are hitting the same bucket. To
>>> best of my knowledge, the difference between Java 8 and 7 is that the lock
>>> is acquired per bucket, while in latter case its acquired per segment.
>>> This, I believe should not change the strange behavior I am seeing.
>>> Anyhow, even with the current implementation, its just weird that in low
>>> contention performance deteriorates.
>>>
>>>
>>>  @Vitaly,
>>>  ThinkTime is nothing but the a simple loop made to run X number of
>>> times. We are considering one iteration as one cycle. This is not
>>> absolutely correct since one iteration should take more cycles (5-6)
>>> including increasing the counter and terminate condition. But this should
>>> not change the graph. This is only going to shift the graph to the right a
>>> bit.
>>>
>>>  @Kimo,
>>> Thanks for the links.  I'll take a look. But the problem is not with the
>>> CAS.  I guess the issue is with ReentrantLock. Current implemenation try
>>> CASing 64 times and after that it goes for ReentrantLock. Under high
>>> contention most of the times all 64 CAS will fail anyway and hashMap will
>>> have to resort to ReentrantLocking. We are just trying to understand this
>>> strange behavior.
>>>
>>>
>>>  Thanks,
>>>  Mudit
>>> Intern, INRIA, Paris
>>>
>>>
>>> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> What exactly are you doing in ThinkTime?
>>>>  On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com>
>>>> wrote:
>>>>
>>>>>     Hi All,
>>>>>
>>>>> I recently performed a scalability test (very aggressive and may be
>>>>> not practical, anyhow) on put operation of ConcurrentHashMap.
>>>>>
>>>>>  Test: Each thread is trying to put (Same Key, Random Value) in
>>>>> HashMap in a tight loop. Therefore, all the threads will hit the same
>>>>> location on hashMap and will cause contention.
>>>>>
>>>>>  What is more surprising is, when each thread continue to do another
>>>>> put one after the other, avg time taken in one put operation is lesser than
>>>>> when a thread do some other work between two put operations.
>>>>>
>>>>> We continue to see the increase in per operation time by increasing
>>>>> the work done in between.  This is very counter intuitive. Only after a
>>>>> work of about 10,000 - 20,000 cycles in between, per op time comes down.
>>>>>
>>>>>  When I read the code, I found out that put op first try to use CAS
>>>>> to aquire the lock(64 times on multicore), only if it could not acquire the
>>>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>>>> threads .. ).
>>>>>
>>>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>>>
>>>>> One interesting thing to note. As we increase the work between two
>>>>> ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst affected
>>>>> with the spike. Therefore, I assume that, spike comes from ReentractLocking
>>>>> even when there is a mixture of two (CAS+Lock).
>>>>>
>>>>>  Code Skeleton: For each Thread
>>>>>
>>>>> While() {
>>>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>>>
>>>>> }
>>>>>
>>>>>  Machine: 48 core NUMA
>>>>>  Threads used:  32 (each one is pinned to a core).
>>>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>>>
>>>>>  That's like saying, if I do something else in between my operations
>>>>> (upto a limit), contention will increase.  Very strange.
>>>>>
>>>>>  Does anyone of you know why is it happening?
>>>>>
>>>>>  _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/b176dd37/attachment.html>

From vitalyd at gmail.com  Wed Apr 17 22:28:45 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 17 Apr 2013 22:28:45 -0400
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
	<CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
Message-ID: <CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>

Ah sorry, I misread the chart - yes, makes sense.

So what CPU and OS is this on?

Sent from my phone
On Apr 17, 2013 9:20 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:

> @Vitaly,
>
> I am sorry I didn't get you correctly.
>
> What's strange in your graph is when think time is very high, op time
> drops down even further.
>
> This is expected behavior.  More the think time, lesser and lesser should
> be the contention.
>
> What's strange in your graph is when think time is very high, op time
> drops down even further.
>
> No,  Why?  op time on Y axis is just the average time a put operation take
> (time taken in HashMap.put invocation and return).  It doesn't include
> think Time.  Therefore,  at around 1,00,000 cycles of ThinkTime,   threads
> are doing enough work in between the operation and enough interleaved that
> they don't hit bucket at the same time, and hence very low (almost nothing)
> time in put op.
>
>
> Thanks,
> Mudit
>
>
>
>
> On Thu, Apr 18, 2013 at 3:02 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> What's strange in your graph is when think time is very high, op time
>> drops down even further.  Are you warming up the C2 compiler properly? The
>> drop at the tail end almost seems like your think it time is being reduced
>> to almost nothing, which I can see happening if your think time is trivial
>> and C2 aggressively optimizes it.
>>
>> Are you also using tiered compilation? If so, turn it off.
>>
>> Sent from my phone
>> On Apr 17, 2013 3:04 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>>
>>> Well, as per JIT. I am not sure. I don't think its the case. As with
>>> increasing ThinkTime, experiment completion time visibly increases.
>>>
>>> Also, alternate to counting iterations, we used System.nanoSec call, to
>>> actually make ThinkTime loop wait for X no of nano secs before it
>>> terminates.
>>>
>>> With frequency of our machine, 1 nano sec = 2 cycles roughly.
>>>
>>> We see same graph in both the cases.
>>>
>>>
>>> On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds <
>>> nathan.reynolds at oracle.com> wrote:
>>>
>>>>  > lock is acquired per bucket
>>>>
>>>> I wasn't aware of the implementation.  I see your point.  There might
>>>> not be any difference.
>>>>
>>>>
>>>> > ThinkTime is nothing but the a simple loop made to run X number of
>>>> times
>>>>
>>>> As for ThinkTime, are you sure JIT didn't get rid of your loop and now
>>>> ThinkTime runs in very little time?
>>>>
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 4/17/2013 11:32 AM, Mudit Verma wrote:
>>>>
>>>>    @Nathan,
>>>> Yes I am aware that new implementation of ConcurrentHashMap which is
>>>> rolling out in Java 8. I am using Java 7. However, I don't think it will
>>>> change the behavior because all the threads are hitting the same bucket. To
>>>> best of my knowledge, the difference between Java 8 and 7 is that the lock
>>>> is acquired per bucket, while in latter case its acquired per segment.
>>>> This, I believe should not change the strange behavior I am seeing.
>>>> Anyhow, even with the current implementation, its just weird that in low
>>>> contention performance deteriorates.
>>>>
>>>>
>>>>  @Vitaly,
>>>>  ThinkTime is nothing but the a simple loop made to run X number of
>>>> times. We are considering one iteration as one cycle. This is not
>>>> absolutely correct since one iteration should take more cycles (5-6)
>>>> including increasing the counter and terminate condition. But this should
>>>> not change the graph. This is only going to shift the graph to the right a
>>>> bit.
>>>>
>>>>  @Kimo,
>>>> Thanks for the links.  I'll take a look. But the problem is not with
>>>> the CAS.  I guess the issue is with ReentrantLock. Current implemenation
>>>> try CASing 64 times and after that it goes for ReentrantLock. Under high
>>>> contention most of the times all 64 CAS will fail anyway and hashMap will
>>>> have to resort to ReentrantLocking. We are just trying to understand this
>>>> strange behavior.
>>>>
>>>>
>>>>  Thanks,
>>>>  Mudit
>>>> Intern, INRIA, Paris
>>>>
>>>>
>>>> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>>
>>>>> What exactly are you doing in ThinkTime?
>>>>>  On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com>
>>>>> wrote:
>>>>>
>>>>>>     Hi All,
>>>>>>
>>>>>> I recently performed a scalability test (very aggressive and may be
>>>>>> not practical, anyhow) on put operation of ConcurrentHashMap.
>>>>>>
>>>>>>  Test: Each thread is trying to put (Same Key, Random Value) in
>>>>>> HashMap in a tight loop. Therefore, all the threads will hit the same
>>>>>> location on hashMap and will cause contention.
>>>>>>
>>>>>>  What is more surprising is, when each thread continue to do another
>>>>>> put one after the other, avg time taken in one put operation is lesser than
>>>>>> when a thread do some other work between two put operations.
>>>>>>
>>>>>> We continue to see the increase in per operation time by increasing
>>>>>> the work done in between.  This is very counter intuitive. Only after a
>>>>>> work of about 10,000 - 20,000 cycles in between, per op time comes down.
>>>>>>
>>>>>>  When I read the code, I found out that put op first try to use CAS
>>>>>> to aquire the lock(64 times on multicore), only if it could not acquire the
>>>>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>>>>> threads .. ).
>>>>>>
>>>>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>>>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>>>>
>>>>>> One interesting thing to note. As we increase the work between two
>>>>>> ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst affected
>>>>>> with the spike. Therefore, I assume that, spike comes from ReentractLocking
>>>>>> even when there is a mixture of two (CAS+Lock).
>>>>>>
>>>>>>  Code Skeleton: For each Thread
>>>>>>
>>>>>> While() {
>>>>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>>>>
>>>>>> }
>>>>>>
>>>>>>  Machine: 48 core NUMA
>>>>>>  Threads used:  32 (each one is pinned to a core).
>>>>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>>>>
>>>>>>  That's like saying, if I do something else in between my operations
>>>>>> (upto a limit), contention will increase.  Very strange.
>>>>>>
>>>>>>  Does anyone of you know why is it happening?
>>>>>>
>>>>>>  _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130417/ea3b0ff5/attachment-0001.html>

From mudit.f2004912 at gmail.com  Wed Apr 17 22:40:31 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 04:40:31 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
	<CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
	<CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>
Message-ID: <CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>

Its linux on 48 core NUMA machine. I am using 32 threads each one pinned to
a separate core.


On Thu, Apr 18, 2013 at 4:28 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Ah sorry, I misread the chart - yes, makes sense.
>
> So what CPU and OS is this on?
>
> Sent from my phone
> On Apr 17, 2013 9:20 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>
>> @Vitaly,
>>
>> I am sorry I didn't get you correctly.
>>
>> What's strange in your graph is when think time is very high, op time
>> drops down even further.
>>
>> This is expected behavior.  More the think time, lesser and lesser should
>> be the contention.
>>
>> What's strange in your graph is when think time is very high, op time
>> drops down even further.
>>
>> No,  Why?  op time on Y axis is just the average time a put operation
>> take (time taken in HashMap.put invocation and return).  It doesn't include
>> think Time.  Therefore,  at around 1,00,000 cycles of ThinkTime,   threads
>> are doing enough work in between the operation and enough interleaved that
>> they don't hit bucket at the same time, and hence very low (almost nothing)
>> time in put op.
>>
>>
>> Thanks,
>> Mudit
>>
>>
>>
>>
>> On Thu, Apr 18, 2013 at 3:02 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> What's strange in your graph is when think time is very high, op time
>>> drops down even further.  Are you warming up the C2 compiler properly? The
>>> drop at the tail end almost seems like your think it time is being reduced
>>> to almost nothing, which I can see happening if your think time is trivial
>>> and C2 aggressively optimizes it.
>>>
>>> Are you also using tiered compilation? If so, turn it off.
>>>
>>> Sent from my phone
>>> On Apr 17, 2013 3:04 PM, "Mudit Verma" <mudit.f2004912 at gmail.com> wrote:
>>>
>>>> Well, as per JIT. I am not sure. I don't think its the case. As with
>>>> increasing ThinkTime, experiment completion time visibly increases.
>>>>
>>>> Also, alternate to counting iterations, we used System.nanoSec call, to
>>>> actually make ThinkTime loop wait for X no of nano secs before it
>>>> terminates.
>>>>
>>>> With frequency of our machine, 1 nano sec = 2 cycles roughly.
>>>>
>>>> We see same graph in both the cases.
>>>>
>>>>
>>>> On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds <
>>>> nathan.reynolds at oracle.com> wrote:
>>>>
>>>>>  > lock is acquired per bucket
>>>>>
>>>>> I wasn't aware of the implementation.  I see your point.  There might
>>>>> not be any difference.
>>>>>
>>>>>
>>>>> > ThinkTime is nothing but the a simple loop made to run X number of
>>>>> times
>>>>>
>>>>> As for ThinkTime, are you sure JIT didn't get rid of your loop and now
>>>>> ThinkTime runs in very little time?
>>>>>
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 4/17/2013 11:32 AM, Mudit Verma wrote:
>>>>>
>>>>>    @Nathan,
>>>>> Yes I am aware that new implementation of ConcurrentHashMap which is
>>>>> rolling out in Java 8. I am using Java 7. However, I don't think it will
>>>>> change the behavior because all the threads are hitting the same bucket. To
>>>>> best of my knowledge, the difference between Java 8 and 7 is that the lock
>>>>> is acquired per bucket, while in latter case its acquired per segment.
>>>>> This, I believe should not change the strange behavior I am seeing.
>>>>> Anyhow, even with the current implementation, its just weird that in low
>>>>> contention performance deteriorates.
>>>>>
>>>>>
>>>>>  @Vitaly,
>>>>>  ThinkTime is nothing but the a simple loop made to run X number of
>>>>> times. We are considering one iteration as one cycle. This is not
>>>>> absolutely correct since one iteration should take more cycles (5-6)
>>>>> including increasing the counter and terminate condition. But this should
>>>>> not change the graph. This is only going to shift the graph to the right a
>>>>> bit.
>>>>>
>>>>>  @Kimo,
>>>>> Thanks for the links.  I'll take a look. But the problem is not with
>>>>> the CAS.  I guess the issue is with ReentrantLock. Current implemenation
>>>>> try CASing 64 times and after that it goes for ReentrantLock. Under high
>>>>> contention most of the times all 64 CAS will fail anyway and hashMap will
>>>>> have to resort to ReentrantLocking. We are just trying to understand this
>>>>> strange behavior.
>>>>>
>>>>>
>>>>>  Thanks,
>>>>>  Mudit
>>>>> Intern, INRIA, Paris
>>>>>
>>>>>
>>>>> On Wed, Apr 17, 2013 at 7:11 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>>>
>>>>>> What exactly are you doing in ThinkTime?
>>>>>>  On Apr 17, 2013 9:41 AM, "Mudit Verma" <mudit.f2004912 at gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>>     Hi All,
>>>>>>>
>>>>>>> I recently performed a scalability test (very aggressive and may be
>>>>>>> not practical, anyhow) on put operation of ConcurrentHashMap.
>>>>>>>
>>>>>>>  Test: Each thread is trying to put (Same Key, Random Value) in
>>>>>>> HashMap in a tight loop. Therefore, all the threads will hit the same
>>>>>>> location on hashMap and will cause contention.
>>>>>>>
>>>>>>>  What is more surprising is, when each thread continue to do
>>>>>>> another put one after the other, avg time taken in one put operation is
>>>>>>> lesser than when a thread do some other work between two put operations.
>>>>>>>
>>>>>>> We continue to see the increase in per operation time by increasing
>>>>>>> the work done in between.  This is very counter intuitive. Only after a
>>>>>>> work of about 10,000 - 20,000 cycles in between, per op time comes down.
>>>>>>>
>>>>>>>  When I read the code, I found out that put op first try to use CAS
>>>>>>> to aquire the lock(64 times on multicore), only if it could not acquire the
>>>>>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>>>>>> threads .. ).
>>>>>>>
>>>>>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is
>>>>>>> used before actually going for ReentrantLocking.   Attached is the graph.
>>>>>>>
>>>>>>> One interesting thing to note. As we increase the work between two
>>>>>>> ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst affected
>>>>>>> with the spike. Therefore, I assume that, spike comes from ReentractLocking
>>>>>>> even when there is a mixture of two (CAS+Lock).
>>>>>>>
>>>>>>>  Code Skeleton: For each Thread
>>>>>>>
>>>>>>> While() {
>>>>>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>>>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>>>>>
>>>>>>> }
>>>>>>>
>>>>>>>  Machine: 48 core NUMA
>>>>>>>  Threads used:  32 (each one is pinned to a core).
>>>>>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>>>>>
>>>>>>>  That's like saying, if I do something else in between my operations
>>>>>>> (upto a limit), contention will increase.  Very strange.
>>>>>>>
>>>>>>>  Does anyone of you know why is it happening?
>>>>>>>
>>>>>>>  _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/ad0f64ac/attachment.html>

From peter.levart at gmail.com  Thu Apr 18 03:23:17 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 18 Apr 2013 09:23:17 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
	<CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
	<CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>
	<CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>
Message-ID: <516F9F65.1050106@gmail.com>

Hi Mudit,

Have you measured the distribution of put operation time at various 
think-time points? It might shed some light on what's happening.

Regards, Peter

On 04/18/2013 04:40 AM, Mudit Verma wrote:
> Its linux on 48 core NUMA machine. I am using 32 threads each one 
> pinned to a separate core.
>
>
> On Thu, Apr 18, 2013 at 4:28 AM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     Ah sorry, I misread the chart - yes, makes sense.
>
>     So what CPU and OS is this on?
>
>     Sent from my phone
>
>     On Apr 17, 2013 9:20 PM, "Mudit Verma" <mudit.f2004912 at gmail.com
>     <mailto:mudit.f2004912 at gmail.com>> wrote:
>
>         @Vitaly,
>
>         I am sorry I didn't get you correctly.
>
>         What's strange in your graph is when think time is very high,
>         op time drops down even further.
>
>         This is expected behavior.  More the think time, lesser and
>         lesser should be the contention.
>
>         What's strange in your graph is when think time is very high,
>         op time drops down even further.
>
>         No,  Why?  op time on Y axis is just the average time a put
>         operation take (time taken in HashMap.put invocation and
>         return).  It doesn't include think Time.  Therefore,  at
>         around 1,00,000 cycles of ThinkTime,   threads are doing
>         enough work in between the operation and enough interleaved
>         that they don't hit bucket at the same time, and hence very
>         low (almost nothing) time in put op.
>
>
>         Thanks,
>         Mudit
>
>
>
>
>         On Thu, Apr 18, 2013 at 3:02 AM, Vitaly Davidovich
>         <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>             What's strange in your graph is when think time is very
>             high, op time drops down even further.  Are you warming up
>             the C2 compiler properly? The drop at the tail end almost
>             seems like your think it time is being reduced to almost
>             nothing, which I can see happening if your think time is
>             trivial and C2 aggressively optimizes it.
>
>             Are you also using tiered compilation? If so, turn it off.
>
>             Sent from my phone
>
>             On Apr 17, 2013 3:04 PM, "Mudit Verma"
>             <mudit.f2004912 at gmail.com
>             <mailto:mudit.f2004912 at gmail.com>> wrote:
>
>                 Well, as per JIT. I am not sure. I don't think its the
>                 case. As with increasing ThinkTime, experiment
>                 completion time visibly increases.
>
>                 Also, alternate to counting iterations, we used
>                 System.nanoSec call, to actually make ThinkTime loop
>                 wait for X no of nano secs before it terminates.
>
>                 With frequency of our machine, 1 nano sec = 2 cycles
>                 roughly.
>
>                 We see same graph in both the cases.
>
>
>                 On Wed, Apr 17, 2013 at 8:41 PM, Nathan Reynolds
>                 <nathan.reynolds at oracle.com
>                 <mailto:nathan.reynolds at oracle.com>> wrote:
>
>                     > lock is acquired per bucket
>
>                     I wasn't aware of the implementation.  I see your
>                     point.  There might not be any difference.
>
>
>                     > ThinkTime is nothing but the a simple loop made
>                     to run X number of times
>
>                     As for ThinkTime, are you sure JIT didn't get rid
>                     of your loop and now ThinkTime runs in very little
>                     time?
>
>
>                     Nathan Reynolds
>                     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>                     | Architect | 602.333.9091 <tel:602.333.9091>
>                     Oracle PSR Engineering <http://psr.us.oracle.com/>
>                     | Server Technology
>                     On 4/17/2013 11:32 AM, Mudit Verma wrote:
>>                     @Nathan,
>>                     Yes I am aware that new implementation of
>>                     ConcurrentHashMap which is rolling out in Java 8.
>>                     I am using Java 7. However, I don't think it will
>>                     change the behavior because all the threads are
>>                     hitting the same bucket. To best of my knowledge,
>>                     the difference between Java 8 and 7 is that the
>>                     lock is acquired per bucket, while in latter case
>>                     its acquired per segment. This, I believe should
>>                     not change the strange behavior I am seeing.
>>                     Anyhow, even with the current implementation, its
>>                     just weird that in low contention performance
>>                     deteriorates.
>>
>>
>>                     @Vitaly,
>>                     ThinkTime is nothing but the a simple loop made
>>                     to run X number of times. We are considering one
>>                     iteration as one cycle. This is not absolutely
>>                     correct since one iteration should take more
>>                     cycles (5-6) including increasing the counter and
>>                     terminate condition. But this should not change
>>                     the graph. This is only going to shift the graph
>>                     to the right a bit.
>>
>>                     @Kimo,
>>                     Thanks for the links.  I'll take a look. But the
>>                     problem is not with the CAS.  I guess the issue
>>                     is with ReentrantLock. Current implemenation try
>>                     CASing 64 times and after that it goes for
>>                     ReentrantLock. Under high contention most of the
>>                     times all 64 CAS will fail anyway and hashMap
>>                     will have to resort to ReentrantLocking. We are
>>                     just trying to understand this strange behavior.
>>
>>
>>                     Thanks,
>>                     Mudit
>>                     Intern, INRIA, Paris
>>
>>
>>                     On Wed, Apr 17, 2013 at 7:11 PM, Vitaly
>>                     Davidovich <vitalyd at gmail.com
>>                     <mailto:vitalyd at gmail.com>> wrote:
>>
>>                         What exactly are you doing in ThinkTime?
>>
>>                         On Apr 17, 2013 9:41 AM, "Mudit Verma"
>>                         <mudit.f2004912 at gmail.com
>>                         <mailto:mudit.f2004912 at gmail.com>> wrote:
>>
>>                             Hi All,
>>
>>                             I recently performed a scalability test
>>                             (very aggressive and may be not
>>                             practical, anyhow) on put operation of
>>                             ConcurrentHashMap.
>>
>>                             Test: Each thread is trying to put (Same
>>                             Key, Random Value) in HashMap in a tight
>>                             loop. Therefore, all the threads will hit
>>                             the same location on hashMap and will
>>                             cause contention.
>>
>>                             What is more surprising is, when each
>>                             thread continue to do another put one
>>                             after the other, avg time taken in one
>>                             put operation is lesser than when a
>>                             thread do some other work between two put
>>                             operations.
>>
>>                             We continue to see the increase in per
>>                             operation time by increasing the work
>>                             done in between.  This is very counter
>>                             intuitive. Only after a work of about
>>                             10,000 - 20,000 cycles in between, per op
>>                             time comes down.
>>
>>                             When I read the code, I found out that
>>                             put op first try to use CAS to aquire the
>>                             lock(64 times on multicore), only if it
>>                             could not acquire the lock on segment
>>                             through CASing, it goes for
>>                             ReentrantLocking (which suspend threads
>>                             .. ).
>>
>>                             We also tweaked, the number of times CAS
>>                             (from 0 to 10,000) is used before
>>                             actually going for ReentrantLocking.
>>                             Attached is the graph.
>>
>>                             One interesting thing to note. As we
>>                             increase the work between two ops,
>>                             locking with 0 CAS (pure
>>                             ReentrantLocking) seems to be worst
>>                             affected with the spike. Therefore, I
>>                             assume that, spike comes from
>>                             ReentractLocking even when there is a
>>                             mixture of two (CAS+Lock).
>>
>>                             Code Skeleton: For each Thread
>>
>>                             While() {
>>                             hashMap.put(K,randomValue); // K is same
>>                             for each thread
>>                             ThinkTime(); < Ranging from 0 Cycles to 1
>>                             million Cycles>
>>
>>                             }
>>
>>                              Machine: 48 core NUMA
>>                             Threads used: 32 (each one is pinned to a
>>                             core).
>>                             #ops: In total 51200000 (each thread with
>>                             160000 ops)
>>
>>                             That's like saying, if I do something
>>                             else in between my operations (upto a
>>                             limit), contention will increase. Very
>>                             strange.
>>
>>                             Does anyone of you know why is it happening?
>>
>>                             _______________________________________________
>>                             Concurrency-interest mailing list
>>                             Concurrency-interest at cs.oswego.edu
>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/246a0e7d/attachment-0001.html>

From kirk at kodewerk.com  Thu Apr 18 05:30:27 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Thu, 18 Apr 2013 11:30:27 +0200
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <516EC6AD.3060005@briangoetz.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<516C0F56.7020000@oracle.com>
	<1366044959679-9425.post@n7.nabble.com>
	<516C5285.90504@oracle.com> <516C8283.9080307@cytetech.com>
	<CAHjP37EaDzZS7C9sVHOs11SZ=CnYPR20SrfJTB1TE8UsV+c4jA@mail.gmail.com>
	<1366113714952-9434.post@n7.nabble.com>
	<516DCE46.4030605@briangoetz.com>
	<1366178049325-9472.post@n7.nabble.com>
	<516EC6AD.3060005@briangoetz.com>
Message-ID: <487754D8-1686-4BD4-B95A-4FC9F2DBF55B@kodewerk.com>

I get this... I do it also... I simply tell them that I'm lying up front.. but then I add, it's a useful lie to help with understanding.

On 2013-04-17, at 5:58 PM, Brian Goetz <brian at briangoetz.com> wrote:

>> Hmm, let me quote:
>> "A program that accesses a mutable variable from multiple threads without
>> synchronization is a broken program" -- sound familiar?
>> Clearly String#hashCode() qualifies, and yet all agree that it *is*
>> "correct"
> 
> Yes, sometimes when teaching, you have to say things that are only 99.99% correct so that people will get the message clearly.
> 
> The folks on this list live in that remaining .01%.  Mostly so that the rest of the world doesn't have to.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From oleksandr.otenko at oracle.com  Thu Apr 18 05:43:07 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 18 Apr 2013 10:43:07 +0100
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
Message-ID: <516FC02B.8080301@oracle.com>

How is fairness doing?

If the threads spin faster, threads on the same socket may have an 
advantage over the other threads:

- lock release invalidates lock state in all caches.
- lock acquire will work faster from the same socket, because some 
caches are shared.

As you increase think time, you increase the odds of lock acquire to 
cross socket boundary.


To test this theory, repeat the test with threads pinned to one socket.
Then the same number of threads, but pinned across the sockets.

Alex


On 17/04/2013 14:31, Mudit Verma wrote:
> Hi All,
>
> I recently performed a scalability test (very aggressive and may be 
> not practical, anyhow) on put operation of ConcurrentHashMap.
>
> Test: Each thread is trying to put (Same Key, Random Value) in HashMap 
> in a tight loop. Therefore, all the threads will hit the same location 
> on hashMap and will cause contention.
>
> What is more surprising is, when each thread continue to do another 
> put one after the other, avg time taken in one put operation is lesser 
> than when a thread do some other work between two put operations.
>
> We continue to see the increase in per operation time by increasing 
> the work done in between.  This is very counter intuitive. Only after 
> a work of about 10,000 - 20,000 cycles in between, per op time comes 
> down.
>
> When I read the code, I found out that put op first try to use CAS to 
> aquire the lock(64 times on multicore), only if it could not acquire 
> the lock on segment through CASing, it goes for ReentrantLocking 
> (which suspend threads .. ).
>
> We also tweaked, the number of times CAS (from 0 to 10,000) is used 
> before actually going for ReentrantLocking.   Attached is the graph.
>
> One interesting thing to note. As we increase the work between two 
> ops, locking with 0 CAS (pure ReentrantLocking) seems to be worst 
> affected with the spike. Therefore, I assume that, spike comes from 
> ReentractLocking even when there is a mixture of two (CAS+Lock).
>
> Code Skeleton: For each Thread
>
> While() {
>   hashMap.put(K,randomValue);     // K is same for each thread
>   ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>
> }
>
>  Machine: 48 core NUMA
> Threads used:  32 (each one is pinned to a core).
> #ops: In total 51200000 (each thread with 160000 ops)
>
> That's like saying, if I do something else in between my operations 
> (upto a limit), contention will increase.  Very strange.
>
> Does anyone of you know why is it happening?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/74b36580/attachment.html>

From mudit.f2004912 at gmail.com  Thu Apr 18 07:25:41 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 13:25:41 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <516FC02B.8080301@oracle.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
Message-ID: <CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>

I'd say threads are doing pretty fair, across the sockets.  I have done
this testing with Pure Reentrant Locking where the increase in per
operation time with increase in ThinkTime is worst affected.

Attached are threads completion time with  ThinkTime 0 and ThinkTime 10000
(where per op time is highest).


On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  How is fairness doing?
>
> If the threads spin faster, threads on the same socket may have an
> advantage over the other threads:
>
> - lock release invalidates lock state in all caches.
> - lock acquire will work faster from the same socket, because some caches
> are shared.
>
> As you increase think time, you increase the odds of lock acquire to cross
> socket boundary.
>
>
> To test this theory, repeat the test with threads pinned to one socket.
> Then the same number of threads, but pinned across the sockets.
>
> Alex
>
>
>
> On 17/04/2013 14:31, Mudit Verma wrote:
>
>    Hi All,
>
> I recently performed a scalability test (very aggressive and may be not
> practical, anyhow) on put operation of ConcurrentHashMap.
>
>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
> in a tight loop. Therefore, all the threads will hit the same location on
> hashMap and will cause contention.
>
>  What is more surprising is, when each thread continue to do another put
> one after the other, avg time taken in one put operation is lesser than
> when a thread do some other work between two put operations.
>
> We continue to see the increase in per operation time by increasing the
> work done in between.  This is very counter intuitive. Only after a work of
> about 10,000 - 20,000 cycles in between, per op time comes down.
>
>  When I read the code, I found out that put op first try to use CAS to
> aquire the lock(64 times on multicore), only if it could not acquire the
> lock on segment through CASing, it goes for ReentrantLocking (which suspend
> threads .. ).
>
>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
> before actually going for ReentrantLocking.   Attached is the graph.
>
> One interesting thing to note. As we increase the work between two ops,
> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
> the spike. Therefore, I assume that, spike comes from ReentractLocking even
> when there is a mixture of two (CAS+Lock).
>
>  Code Skeleton: For each Thread
>
> While() {
>    hashMap.put(K,randomValue);     // K is same for each thread
>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>
> }
>
>  Machine: 48 core NUMA
>  Threads used:  32 (each one is pinned to a core).
>  #ops: In total 51200000 (each thread with 160000 ops)
>
>  That's like saying, if I do something else in between my operations (upto
> a limit), contention will increase.  Very strange.
>
>  Does anyone of you know why is it happening?
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/6385681b/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fairness Think Time 10000.JPG
Type: image/jpeg
Size: 45960 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/6385681b/attachment-0002.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fairness Think Time 0.JPG
Type: image/jpeg
Size: 37084 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/6385681b/attachment-0003.jpe>

From mudit.f2004912 at gmail.com  Thu Apr 18 07:31:32 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 13:31:32 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
Message-ID: <CAMM2gsNUvE8dJV0jj6k_R+-MC5rW91WP1SomfatSJB3y5E+Tfw@mail.gmail.com>

A correction. In graph, X axis is the avg per operation completion time for
each individual thread.

Rest of my explanation remain same.


On Thu, Apr 18, 2013 at 1:25 PM, Mudit Verma <mudit.f2004912 at gmail.com>wrote:

> I'd say threads are doing pretty fair, across the sockets.  I have done
> this testing with Pure Reentrant Locking where the increase in per
> operation time with increase in ThinkTime is worst affected.
>
> Attached are threads completion time with  ThinkTime 0 and ThinkTime 10000
> (where per op time is highest).
>
>
> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  How is fairness doing?
>>
>> If the threads spin faster, threads on the same socket may have an
>> advantage over the other threads:
>>
>> - lock release invalidates lock state in all caches.
>> - lock acquire will work faster from the same socket, because some caches
>> are shared.
>>
>> As you increase think time, you increase the odds of lock acquire to
>> cross socket boundary.
>>
>>
>> To test this theory, repeat the test with threads pinned to one socket.
>> Then the same number of threads, but pinned across the sockets.
>>
>> Alex
>>
>>
>>
>> On 17/04/2013 14:31, Mudit Verma wrote:
>>
>>    Hi All,
>>
>> I recently performed a scalability test (very aggressive and may be not
>> practical, anyhow) on put operation of ConcurrentHashMap.
>>
>>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
>> in a tight loop. Therefore, all the threads will hit the same location on
>> hashMap and will cause contention.
>>
>>  What is more surprising is, when each thread continue to do another put
>> one after the other, avg time taken in one put operation is lesser than
>> when a thread do some other work between two put operations.
>>
>> We continue to see the increase in per operation time by increasing the
>> work done in between.  This is very counter intuitive. Only after a work of
>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>
>>  When I read the code, I found out that put op first try to use CAS to
>> aquire the lock(64 times on multicore), only if it could not acquire the
>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>> threads .. ).
>>
>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>> before actually going for ReentrantLocking.   Attached is the graph.
>>
>> One interesting thing to note. As we increase the work between two ops,
>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>> when there is a mixture of two (CAS+Lock).
>>
>>  Code Skeleton: For each Thread
>>
>> While() {
>>    hashMap.put(K,randomValue);     // K is same for each thread
>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>
>> }
>>
>>  Machine: 48 core NUMA
>>  Threads used:  32 (each one is pinned to a core).
>>  #ops: In total 51200000 (each thread with 160000 ops)
>>
>>  That's like saying, if I do something else in between my operations
>> (upto a limit), contention will increase.  Very strange.
>>
>>  Does anyone of you know why is it happening?
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/f45860df/attachment.html>

From oleksandr.otenko at oracle.com  Thu Apr 18 07:37:22 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 18 Apr 2013 12:37:22 +0100
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
Message-ID: <516FDAF2.8080804@oracle.com>

How do you conclude "pretty fair"?

When we tested synchronization primitives, we tested threads on the same 
socket pinned to Hyper-threaded siblings, same socket distinct physical 
cores, and cross-socket. Obviously, we get different throughput because 
of latency of CAS.

At this scale, it is sufficient for lock acquire to only bias towards 
same socket. If you have smaller think time, more threads on the same 
socket are ready to acquire the lock, and the lock "tick-tocks" between 
sockets less frequently - but it does move around. At macro level it 
will look fair, but at nano level it is biased.

As you increase think time, you reduce the number of threads on the same 
socket ready to acquire the lock. At the same time, the other socket may 
have a higher chance to steal the lock, as the threads there will have 
waited longer, and accumulated more threads ready to acquire. So the 
lock "tick-tocks" between sockets more frequently. At macro level 
fairness looks the same, but at nano level more lock acquires are across 
the socket, and inflate the CAS time.

The cross-socket traffic is not only for the lock, but for the whole 
HashMap bucket, so this also inflates the time spent /*in*/ put, making 
the put time even longer.


It seems very easy to test.

Alex


On 18/04/2013 12:25, Mudit Verma wrote:
> I'd say threads are doing pretty fair, across the sockets. I have done 
> this testing with Pure Reentrant Locking where the increase in per 
> operation time with increase in ThinkTime is worst affected.
>
> Attached are threads completion time with  ThinkTime 0 and ThinkTime 
> 10000 (where per op time is highest).
>
>
> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     How is fairness doing?
>
>     If the threads spin faster, threads on the same socket may have an
>     advantage over the other threads:
>
>     - lock release invalidates lock state in all caches.
>     - lock acquire will work faster from the same socket, because some
>     caches are shared.
>
>     As you increase think time, you increase the odds of lock acquire
>     to cross socket boundary.
>
>
>     To test this theory, repeat the test with threads pinned to one
>     socket.
>     Then the same number of threads, but pinned across the sockets.
>
>     Alex
>
>
>
>     On 17/04/2013 14:31, Mudit Verma wrote:
>>     Hi All,
>>
>>     I recently performed a scalability test (very aggressive and may
>>     be not practical, anyhow) on put operation of ConcurrentHashMap.
>>
>>     Test: Each thread is trying to put (Same Key, Random Value) in
>>     HashMap in a tight loop. Therefore, all the threads will hit the
>>     same location on hashMap and will cause contention.
>>
>>     What is more surprising is, when each thread continue to do
>>     another put one after the other, avg time taken in one put
>>     operation is lesser than when a thread do some other work between
>>     two put operations.
>>
>>     We continue to see the increase in per operation time by
>>     increasing the work done in between.  This is very counter
>>     intuitive. Only after a work of about 10,000 - 20,000 cycles in
>>     between, per op time comes down.
>>
>>     When I read the code, I found out that put op first try to use
>>     CAS to aquire the lock(64 times on multicore), only if it could
>>     not acquire the lock on segment through CASing, it goes for
>>     ReentrantLocking (which suspend threads .. ).
>>
>>     We also tweaked, the number of times CAS (from 0 to 10,000) is
>>     used before actually going for ReentrantLocking. Attached is the
>>     graph.
>>
>>     One interesting thing to note. As we increase the work between
>>     two ops, locking with 0 CAS (pure ReentrantLocking) seems to be
>>     worst affected with the spike. Therefore, I assume that, spike
>>     comes from ReentractLocking even when there is a mixture of two
>>     (CAS+Lock).
>>
>>     Code Skeleton: For each Thread
>>
>>     While() {
>>       hashMap.put(K,randomValue);     // K is same for each thread
>>       ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>
>>     }
>>
>>      Machine: 48 core NUMA
>>     Threads used:  32 (each one is pinned to a core).
>>     #ops: In total 51200000 (each thread with 160000 ops)
>>
>>     That's like saying, if I do something else in between my
>>     operations (upto a limit), contention will increase.  Very strange.
>>
>>     Does anyone of you know why is it happening?
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/3299b870/attachment-0001.html>

From mudit.f2004912 at gmail.com  Thu Apr 18 07:54:05 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 13:54:05 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <516FDAF2.8080804@oracle.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
	<516FDAF2.8080804@oracle.com>
Message-ID: <CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>

I agree with what you say, but I'm not talking about Lock acquisition
through CAS.

I mentioned in previous mail that the issue is most seen when we don't use
CASing and just rely on ReentrantLocking which suspends the threads and
grants locks in order the lock requests came. AFAIK by seeing the internal
code, this reentrant lock maintains an internal queue of lock requests by
different threads.  Much like Posix lock.


However, you are perfectly right with your theory, if we just use CASing to
acquire the lock, results are not fair at all.  Attached are the results.
I


On Thu, Apr 18, 2013 at 1:37 PM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  How do you conclude "pretty fair"?
>
> When we tested synchronization primitives, we tested threads on the same
> socket pinned to Hyper-threaded siblings, same socket distinct physical
> cores, and cross-socket. Obviously, we get different throughput because of
> latency of CAS.
>
> At this scale, it is sufficient for lock acquire to only bias towards same
> socket. If you have smaller think time, more threads on the same socket are
> ready to acquire the lock, and the lock "tick-tocks" between sockets less
> frequently - but it does move around. At macro level it will look fair, but
> at nano level it is biased.
>
> As you increase think time, you reduce the number of threads on the same
> socket ready to acquire the lock. At the same time, the other socket may
> have a higher chance to steal the lock, as the threads there will have
> waited longer, and accumulated more threads ready to acquire. So the lock
> "tick-tocks" between sockets more frequently. At macro level fairness looks
> the same, but at nano level more lock acquires are across the socket, and
> inflate the CAS time.
>
> The cross-socket traffic is not only for the lock, but for the whole
> HashMap bucket, so this also inflates the time spent *in* put, making the
> put time even longer.
>
>
> It seems very easy to test.
>
> Alex
>
>
>
> On 18/04/2013 12:25, Mudit Verma wrote:
>
>  I'd say threads are doing pretty fair, across the sockets.  I have done
> this testing with Pure Reentrant Locking where the increase in per
> operation time with increase in ThinkTime is worst affected.
>
>  Attached are threads completion time with  ThinkTime 0 and ThinkTime
> 10000 (where per op time is highest).
>
>
> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  How is fairness doing?
>>
>> If the threads spin faster, threads on the same socket may have an
>> advantage over the other threads:
>>
>> - lock release invalidates lock state in all caches.
>> - lock acquire will work faster from the same socket, because some caches
>> are shared.
>>
>> As you increase think time, you increase the odds of lock acquire to
>> cross socket boundary.
>>
>>
>> To test this theory, repeat the test with threads pinned to one socket.
>> Then the same number of threads, but pinned across the sockets.
>>
>> Alex
>>
>>
>>
>> On 17/04/2013 14:31, Mudit Verma wrote:
>>
>>     Hi All,
>>
>> I recently performed a scalability test (very aggressive and may be not
>> practical, anyhow) on put operation of ConcurrentHashMap.
>>
>>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
>> in a tight loop. Therefore, all the threads will hit the same location on
>> hashMap and will cause contention.
>>
>>  What is more surprising is, when each thread continue to do another put
>> one after the other, avg time taken in one put operation is lesser than
>> when a thread do some other work between two put operations.
>>
>> We continue to see the increase in per operation time by increasing the
>> work done in between.  This is very counter intuitive. Only after a work of
>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>
>>  When I read the code, I found out that put op first try to use CAS to
>> aquire the lock(64 times on multicore), only if it could not acquire the
>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>> threads .. ).
>>
>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>> before actually going for ReentrantLocking.   Attached is the graph.
>>
>> One interesting thing to note. As we increase the work between two ops,
>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>> when there is a mixture of two (CAS+Lock).
>>
>>  Code Skeleton: For each Thread
>>
>> While() {
>>    hashMap.put(K,randomValue);     // K is same for each thread
>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>
>> }
>>
>>  Machine: 48 core NUMA
>>  Threads used:  32 (each one is pinned to a core).
>>  #ops: In total 51200000 (each thread with 160000 ops)
>>
>>  That's like saying, if I do something else in between my operations
>> (upto a limit), contention will increase.  Very strange.
>>
>>  Does anyone of you know why is it happening?
>>
>>
>>   _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/6f530b3c/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Fairness Only Casing.JPG
Type: image/jpeg
Size: 35212 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/6f530b3c/attachment-0001.jpe>

From mudit.f2004912 at gmail.com  Thu Apr 18 07:58:27 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 13:58:27 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
	<516FDAF2.8080804@oracle.com>
	<CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
Message-ID: <CAMM2gsMPDCH4c4SYd2_pXQ=4nEi9HzCDj-3_W65qT_osvDx0xw@mail.gmail.com>

Anyway, I will do further testing with your suggestion. :)


On Thu, Apr 18, 2013 at 1:54 PM, Mudit Verma <mudit.f2004912 at gmail.com>wrote:

> I agree with what you say, but I'm not talking about Lock acquisition
> through CAS.
>
> I mentioned in previous mail that the issue is most seen when we don't use
> CASing and just rely on ReentrantLocking which suspends the threads and
> grants locks in order the lock requests came. AFAIK by seeing the internal
> code, this reentrant lock maintains an internal queue of lock requests by
> different threads.  Much like Posix lock.
>
>
> However, you are perfectly right with your theory, if we just use CASing
> to acquire the lock, results are not fair at all.  Attached are the
> results.
> I
>
>
> On Thu, Apr 18, 2013 at 1:37 PM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  How do you conclude "pretty fair"?
>>
>> When we tested synchronization primitives, we tested threads on the same
>> socket pinned to Hyper-threaded siblings, same socket distinct physical
>> cores, and cross-socket. Obviously, we get different throughput because of
>> latency of CAS.
>>
>> At this scale, it is sufficient for lock acquire to only bias towards
>> same socket. If you have smaller think time, more threads on the same
>> socket are ready to acquire the lock, and the lock "tick-tocks" between
>> sockets less frequently - but it does move around. At macro level it will
>> look fair, but at nano level it is biased.
>>
>> As you increase think time, you reduce the number of threads on the same
>> socket ready to acquire the lock. At the same time, the other socket may
>> have a higher chance to steal the lock, as the threads there will have
>> waited longer, and accumulated more threads ready to acquire. So the lock
>> "tick-tocks" between sockets more frequently. At macro level fairness looks
>> the same, but at nano level more lock acquires are across the socket, and
>> inflate the CAS time.
>>
>> The cross-socket traffic is not only for the lock, but for the whole
>> HashMap bucket, so this also inflates the time spent *in* put, making
>> the put time even longer.
>>
>>
>> It seems very easy to test.
>>
>> Alex
>>
>>
>>
>> On 18/04/2013 12:25, Mudit Verma wrote:
>>
>>  I'd say threads are doing pretty fair, across the sockets.  I have done
>> this testing with Pure Reentrant Locking where the increase in per
>> operation time with increase in ThinkTime is worst affected.
>>
>>  Attached are threads completion time with  ThinkTime 0 and ThinkTime
>> 10000 (where per op time is highest).
>>
>>
>> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  How is fairness doing?
>>>
>>> If the threads spin faster, threads on the same socket may have an
>>> advantage over the other threads:
>>>
>>> - lock release invalidates lock state in all caches.
>>> - lock acquire will work faster from the same socket, because some
>>> caches are shared.
>>>
>>> As you increase think time, you increase the odds of lock acquire to
>>> cross socket boundary.
>>>
>>>
>>> To test this theory, repeat the test with threads pinned to one socket.
>>> Then the same number of threads, but pinned across the sockets.
>>>
>>> Alex
>>>
>>>
>>>
>>> On 17/04/2013 14:31, Mudit Verma wrote:
>>>
>>>     Hi All,
>>>
>>> I recently performed a scalability test (very aggressive and may be not
>>> practical, anyhow) on put operation of ConcurrentHashMap.
>>>
>>>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
>>> in a tight loop. Therefore, all the threads will hit the same location on
>>> hashMap and will cause contention.
>>>
>>>  What is more surprising is, when each thread continue to do another
>>> put one after the other, avg time taken in one put operation is lesser than
>>> when a thread do some other work between two put operations.
>>>
>>> We continue to see the increase in per operation time by increasing the
>>> work done in between.  This is very counter intuitive. Only after a work of
>>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>>
>>>  When I read the code, I found out that put op first try to use CAS to
>>> aquire the lock(64 times on multicore), only if it could not acquire the
>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>> threads .. ).
>>>
>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>
>>> One interesting thing to note. As we increase the work between two ops,
>>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>>> when there is a mixture of two (CAS+Lock).
>>>
>>>  Code Skeleton: For each Thread
>>>
>>> While() {
>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>
>>> }
>>>
>>>  Machine: 48 core NUMA
>>>  Threads used:  32 (each one is pinned to a core).
>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>
>>>  That's like saying, if I do something else in between my operations
>>> (upto a limit), contention will increase.  Very strange.
>>>
>>>  Does anyone of you know why is it happening?
>>>
>>>
>>>   _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/26bd3427/attachment.html>

From oleksandr.otenko at oracle.com  Thu Apr 18 07:59:13 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 18 Apr 2013 12:59:13 +0100
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
	<516FDAF2.8080804@oracle.com>
	<CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
Message-ID: <516FE011.70200@oracle.com>

That is a good point. However, what is that "Thread.park" in this test.

If you do not have contention for CPU, it is a condvar wait with similar 
properties - who do you schedule to run next? Of course, if you have 
threads waiting to run on the same socket, bias towards those.

Alex

On 18/04/2013 12:54, Mudit Verma wrote:
> I agree with what you say, but I'm not talking about Lock acquisition 
> through CAS.
>
> I mentioned in previous mail that the issue is most seen when we don't 
> use CASing and just rely on ReentrantLocking which suspends the 
> threads and grants locks in order the lock requests came. AFAIK by 
> seeing the internal code, this reentrant lock maintains an internal 
> queue of lock requests by different threads.  Much like Posix lock.
>
>
> However, you are perfectly right with your theory, if we just use 
> CASing to acquire the lock, results are not fair at all. Attached are 
> the results.
> I
>
>
> On Thu, Apr 18, 2013 at 1:37 PM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     How do you conclude "pretty fair"?
>
>     When we tested synchronization primitives, we tested threads on
>     the same socket pinned to Hyper-threaded siblings, same socket
>     distinct physical cores, and cross-socket. Obviously, we get
>     different throughput because of latency of CAS.
>
>     At this scale, it is sufficient for lock acquire to only bias
>     towards same socket. If you have smaller think time, more threads
>     on the same socket are ready to acquire the lock, and the lock
>     "tick-tocks" between sockets less frequently - but it does move
>     around. At macro level it will look fair, but at nano level it is
>     biased.
>
>     As you increase think time, you reduce the number of threads on
>     the same socket ready to acquire the lock. At the same time, the
>     other socket may have a higher chance to steal the lock, as the
>     threads there will have waited longer, and accumulated more
>     threads ready to acquire. So the lock "tick-tocks" between sockets
>     more frequently. At macro level fairness looks the same, but at
>     nano level more lock acquires are across the socket, and inflate
>     the CAS time.
>
>     The cross-socket traffic is not only for the lock, but for the
>     whole HashMap bucket, so this also inflates the time spent /*in*/
>     put, making the put time even longer.
>
>
>     It seems very easy to test.
>
>     Alex
>
>
>
>     On 18/04/2013 12:25, Mudit Verma wrote:
>>     I'd say threads are doing pretty fair, across the sockets.  I
>>     have done this testing with Pure Reentrant Locking where the
>>     increase in per operation time with increase in ThinkTime is
>>     worst affected.
>>
>>     Attached are threads completion time with ThinkTime 0 and
>>     ThinkTime 10000 (where per op time is highest).
>>
>>
>>     On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         How is fairness doing?
>>
>>         If the threads spin faster, threads on the same socket may
>>         have an advantage over the other threads:
>>
>>         - lock release invalidates lock state in all caches.
>>         - lock acquire will work faster from the same socket, because
>>         some caches are shared.
>>
>>         As you increase think time, you increase the odds of lock
>>         acquire to cross socket boundary.
>>
>>
>>         To test this theory, repeat the test with threads pinned to
>>         one socket.
>>         Then the same number of threads, but pinned across the sockets.
>>
>>         Alex
>>
>>
>>
>>         On 17/04/2013 14:31, Mudit Verma wrote:
>>>         Hi All,
>>>
>>>         I recently performed a scalability test (very aggressive and
>>>         may be not practical, anyhow) on put operation of
>>>         ConcurrentHashMap.
>>>
>>>         Test: Each thread is trying to put (Same Key, Random Value)
>>>         in HashMap in a tight loop. Therefore, all the threads will
>>>         hit the same location on hashMap and will cause contention.
>>>
>>>         What is more surprising is, when each thread continue to do
>>>         another put one after the other, avg time taken in one put
>>>         operation is lesser than when a thread do some other work
>>>         between two put operations.
>>>
>>>         We continue to see the increase in per operation time by
>>>         increasing the work done in between. This is very counter
>>>         intuitive. Only after a work of about 10,000 - 20,000 cycles
>>>         in between, per op time comes down.
>>>
>>>         When I read the code, I found out that put op first try to
>>>         use CAS to aquire the lock(64 times on multicore), only if
>>>         it could not acquire the lock on segment through CASing, it
>>>         goes for ReentrantLocking (which suspend threads .. ).
>>>
>>>         We also tweaked, the number of times CAS (from 0 to 10,000)
>>>         is used before actually going for ReentrantLocking. Attached
>>>         is the graph.
>>>
>>>         One interesting thing to note. As we increase the work
>>>         between two ops, locking with 0 CAS (pure ReentrantLocking)
>>>         seems to be worst affected with the spike. Therefore, I
>>>         assume that, spike comes from ReentractLocking even when
>>>         there is a mixture of two (CAS+Lock).
>>>
>>>         Code Skeleton: For each Thread
>>>
>>>         While() {
>>>         hashMap.put(K,randomValue); // K is same for each thread
>>>           ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>
>>>         }
>>>
>>>          Machine: 48 core NUMA
>>>         Threads used:  32 (each one is pinned to a core).
>>>         #ops: In total 51200000 (each thread with 160000 ops)
>>>
>>>         That's like saying, if I do something else in between my
>>>         operations (upto a limit), contention will increase.  Very
>>>         strange.
>>>
>>>         Does anyone of you know why is it happening?
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/ce7b69b3/attachment-0001.html>

From mudit.f2004912 at gmail.com  Thu Apr 18 08:43:30 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 14:43:30 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <516FE011.70200@oracle.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
	<516FDAF2.8080804@oracle.com>
	<CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
	<516FE011.70200@oracle.com>
Message-ID: <CAMM2gsMQbDc2Z7pm6Ph4ZieuR88aH7rE6Xx5f0bH9=TuTQZCwA@mail.gmail.com>

Thread.Park ?  What is that? I'm not aware of it.


On Thu, Apr 18, 2013 at 1:59 PM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  That is a good point. However, what is that "Thread.park" in this test.
>
> If you do not have contention for CPU, it is a condvar wait with similar
> properties - who do you schedule to run next? Of course, if you have
> threads waiting to run on the same socket, bias towards those.
>
> Alex
>
>  On 18/04/2013 12:54, Mudit Verma wrote:
>
> I agree with what you say, but I'm not talking about Lock acquisition
> through CAS.
>
> I mentioned in previous mail that the issue is most seen when we don't use
> CASing and just rely on ReentrantLocking which suspends the threads and
> grants locks in order the lock requests came. AFAIK by seeing the internal
> code, this reentrant lock maintains an internal queue of lock requests by
> different threads.  Much like Posix lock.
>
>
> However, you are perfectly right with your theory, if we just use CASing
> to acquire the lock, results are not fair at all.  Attached are the
> results.
> I
>
>
>  On Thu, Apr 18, 2013 at 1:37 PM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  How do you conclude "pretty fair"?
>>
>> When we tested synchronization primitives, we tested threads on the same
>> socket pinned to Hyper-threaded siblings, same socket distinct physical
>> cores, and cross-socket. Obviously, we get different throughput because of
>> latency of CAS.
>>
>> At this scale, it is sufficient for lock acquire to only bias towards
>> same socket. If you have smaller think time, more threads on the same
>> socket are ready to acquire the lock, and the lock "tick-tocks" between
>> sockets less frequently - but it does move around. At macro level it will
>> look fair, but at nano level it is biased.
>>
>> As you increase think time, you reduce the number of threads on the same
>> socket ready to acquire the lock. At the same time, the other socket may
>> have a higher chance to steal the lock, as the threads there will have
>> waited longer, and accumulated more threads ready to acquire. So the lock
>> "tick-tocks" between sockets more frequently. At macro level fairness looks
>> the same, but at nano level more lock acquires are across the socket, and
>> inflate the CAS time.
>>
>> The cross-socket traffic is not only for the lock, but for the whole
>> HashMap bucket, so this also inflates the time spent *in* put, making
>> the put time even longer.
>>
>>
>> It seems very easy to test.
>>
>> Alex
>>
>>
>>
>> On 18/04/2013 12:25, Mudit Verma wrote:
>>
>>  I'd say threads are doing pretty fair, across the sockets.  I have done
>> this testing with Pure Reentrant Locking where the increase in per
>> operation time with increase in ThinkTime is worst affected.
>>
>>  Attached are threads completion time with  ThinkTime 0 and ThinkTime
>> 10000 (where per op time is highest).
>>
>>
>> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  How is fairness doing?
>>>
>>> If the threads spin faster, threads on the same socket may have an
>>> advantage over the other threads:
>>>
>>> - lock release invalidates lock state in all caches.
>>> - lock acquire will work faster from the same socket, because some
>>> caches are shared.
>>>
>>> As you increase think time, you increase the odds of lock acquire to
>>> cross socket boundary.
>>>
>>>
>>> To test this theory, repeat the test with threads pinned to one socket.
>>> Then the same number of threads, but pinned across the sockets.
>>>
>>> Alex
>>>
>>>
>>>
>>> On 17/04/2013 14:31, Mudit Verma wrote:
>>>
>>>     Hi All,
>>>
>>> I recently performed a scalability test (very aggressive and may be not
>>> practical, anyhow) on put operation of ConcurrentHashMap.
>>>
>>>  Test: Each thread is trying to put (Same Key, Random Value) in HashMap
>>> in a tight loop. Therefore, all the threads will hit the same location on
>>> hashMap and will cause contention.
>>>
>>>  What is more surprising is, when each thread continue to do another
>>> put one after the other, avg time taken in one put operation is lesser than
>>> when a thread do some other work between two put operations.
>>>
>>> We continue to see the increase in per operation time by increasing the
>>> work done in between.  This is very counter intuitive. Only after a work of
>>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>>
>>>  When I read the code, I found out that put op first try to use CAS to
>>> aquire the lock(64 times on multicore), only if it could not acquire the
>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>> threads .. ).
>>>
>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>
>>> One interesting thing to note. As we increase the work between two ops,
>>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>>> when there is a mixture of two (CAS+Lock).
>>>
>>>  Code Skeleton: For each Thread
>>>
>>> While() {
>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>
>>> }
>>>
>>>  Machine: 48 core NUMA
>>>  Threads used:  32 (each one is pinned to a core).
>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>
>>>  That's like saying, if I do something else in between my operations
>>> (upto a limit), contention will increase.  Very strange.
>>>
>>>  Does anyone of you know why is it happening?
>>>
>>>
>>>   _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/16ccc72a/attachment.html>

From mudit.f2004912 at gmail.com  Thu Apr 18 08:58:41 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 14:58:41 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsMQbDc2Z7pm6Ph4ZieuR88aH7rE6Xx5f0bH9=TuTQZCwA@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<516FC02B.8080301@oracle.com>
	<CAMM2gsNS2omUKa6f6FLJvdu6xdCi3szNdxKh9ut7TtY-Es0bPg@mail.gmail.com>
	<516FDAF2.8080804@oracle.com>
	<CAMM2gsP4-tCC9oXH+6wL5x3TwgW58A=XW=S_eKT8PejmLAP9xA@mail.gmail.com>
	<516FE011.70200@oracle.com>
	<CAMM2gsMQbDc2Z7pm6Ph4ZieuR88aH7rE6Xx5f0bH9=TuTQZCwA@mail.gmail.com>
Message-ID: <CAMM2gsN1KW3cA_ccoy2QMCG4qN_9gBWuKtJf9cbPtPRbR4jE5g@mail.gmail.com>

I got it now. I didn't check that. I will check.

 By the way, one interesting thing, the same behavior was seen in posix
locks in C by a different person while working on another problem. The
results are also published.   There also,  with the increase in ThinkTime,
somehow per operation time increases.



On Thu, Apr 18, 2013 at 2:43 PM, Mudit Verma <mudit.f2004912 at gmail.com>wrote:

> Thread.Park ?  What is that? I'm not aware of it.
>
>
> On Thu, Apr 18, 2013 at 1:59 PM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  That is a good point. However, what is that "Thread.park" in this test.
>>
>> If you do not have contention for CPU, it is a condvar wait with similar
>> properties - who do you schedule to run next? Of course, if you have
>> threads waiting to run on the same socket, bias towards those.
>>
>> Alex
>>
>>  On 18/04/2013 12:54, Mudit Verma wrote:
>>
>> I agree with what you say, but I'm not talking about Lock acquisition
>> through CAS.
>>
>> I mentioned in previous mail that the issue is most seen when we don't
>> use CASing and just rely on ReentrantLocking which suspends the threads and
>> grants locks in order the lock requests came. AFAIK by seeing the internal
>> code, this reentrant lock maintains an internal queue of lock requests by
>> different threads.  Much like Posix lock.
>>
>>
>> However, you are perfectly right with your theory, if we just use CASing
>> to acquire the lock, results are not fair at all.  Attached are the
>> results.
>> I
>>
>>
>>  On Thu, Apr 18, 2013 at 1:37 PM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  How do you conclude "pretty fair"?
>>>
>>> When we tested synchronization primitives, we tested threads on the same
>>> socket pinned to Hyper-threaded siblings, same socket distinct physical
>>> cores, and cross-socket. Obviously, we get different throughput because of
>>> latency of CAS.
>>>
>>> At this scale, it is sufficient for lock acquire to only bias towards
>>> same socket. If you have smaller think time, more threads on the same
>>> socket are ready to acquire the lock, and the lock "tick-tocks" between
>>> sockets less frequently - but it does move around. At macro level it will
>>> look fair, but at nano level it is biased.
>>>
>>> As you increase think time, you reduce the number of threads on the same
>>> socket ready to acquire the lock. At the same time, the other socket may
>>> have a higher chance to steal the lock, as the threads there will have
>>> waited longer, and accumulated more threads ready to acquire. So the lock
>>> "tick-tocks" between sockets more frequently. At macro level fairness looks
>>> the same, but at nano level more lock acquires are across the socket, and
>>> inflate the CAS time.
>>>
>>> The cross-socket traffic is not only for the lock, but for the whole
>>> HashMap bucket, so this also inflates the time spent *in* put, making
>>> the put time even longer.
>>>
>>>
>>> It seems very easy to test.
>>>
>>> Alex
>>>
>>>
>>>
>>> On 18/04/2013 12:25, Mudit Verma wrote:
>>>
>>>  I'd say threads are doing pretty fair, across the sockets.  I have
>>> done this testing with Pure Reentrant Locking where the increase in per
>>> operation time with increase in ThinkTime is worst affected.
>>>
>>>  Attached are threads completion time with  ThinkTime 0 and ThinkTime
>>> 10000 (where per op time is highest).
>>>
>>>
>>> On Thu, Apr 18, 2013 at 11:43 AM, oleksandr otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>>  How is fairness doing?
>>>>
>>>> If the threads spin faster, threads on the same socket may have an
>>>> advantage over the other threads:
>>>>
>>>> - lock release invalidates lock state in all caches.
>>>> - lock acquire will work faster from the same socket, because some
>>>> caches are shared.
>>>>
>>>> As you increase think time, you increase the odds of lock acquire to
>>>> cross socket boundary.
>>>>
>>>>
>>>> To test this theory, repeat the test with threads pinned to one socket.
>>>> Then the same number of threads, but pinned across the sockets.
>>>>
>>>> Alex
>>>>
>>>>
>>>>
>>>> On 17/04/2013 14:31, Mudit Verma wrote:
>>>>
>>>>     Hi All,
>>>>
>>>> I recently performed a scalability test (very aggressive and may be not
>>>> practical, anyhow) on put operation of ConcurrentHashMap.
>>>>
>>>>  Test: Each thread is trying to put (Same Key, Random Value) in
>>>> HashMap in a tight loop. Therefore, all the threads will hit the same
>>>> location on hashMap and will cause contention.
>>>>
>>>>  What is more surprising is, when each thread continue to do another
>>>> put one after the other, avg time taken in one put operation is lesser than
>>>> when a thread do some other work between two put operations.
>>>>
>>>> We continue to see the increase in per operation time by increasing the
>>>> work done in between.  This is very counter intuitive. Only after a work of
>>>> about 10,000 - 20,000 cycles in between, per op time comes down.
>>>>
>>>>  When I read the code, I found out that put op first try to use CAS to
>>>> aquire the lock(64 times on multicore), only if it could not acquire the
>>>> lock on segment through CASing, it goes for ReentrantLocking (which suspend
>>>> threads .. ).
>>>>
>>>>  We also tweaked, the number of times CAS (from 0 to 10,000) is used
>>>> before actually going for ReentrantLocking.   Attached is the graph.
>>>>
>>>> One interesting thing to note. As we increase the work between two ops,
>>>> locking with 0 CAS (pure ReentrantLocking) seems to be worst affected with
>>>> the spike. Therefore, I assume that, spike comes from ReentractLocking even
>>>> when there is a mixture of two (CAS+Lock).
>>>>
>>>>  Code Skeleton: For each Thread
>>>>
>>>> While() {
>>>>    hashMap.put(K,randomValue);     // K is same for each thread
>>>>    ThinkTime();    < Ranging from 0 Cycles to 1 million Cycles>
>>>>
>>>> }
>>>>
>>>>  Machine: 48 core NUMA
>>>>  Threads used:  32 (each one is pinned to a core).
>>>>  #ops: In total 51200000 (each thread with 160000 ops)
>>>>
>>>>  That's like saying, if I do something else in between my operations
>>>> (upto a limit), contention will increase.  Very strange.
>>>>
>>>>  Does anyone of you know why is it happening?
>>>>
>>>>
>>>>   _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/3bebb92d/attachment-0001.html>

From thurston at nomagicsoftware.com  Thu Apr 18 09:54:01 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 18 Apr 2013 06:54:01 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
	<CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
	<CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>
	<CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>
Message-ID: <1366293241814-9526.post@n7.nabble.com>

Just curious, how were you pinning each thread to a core?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/ConcurrentHashMap-Very-Strange-behavior-of-ReentrantLock-under-contention-tp9481p9526.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From mudit.f2004912 at gmail.com  Thu Apr 18 10:01:07 2013
From: mudit.f2004912 at gmail.com (Mudit Verma)
Date: Thu, 18 Apr 2013 16:01:07 +0200
Subject: [concurrency-interest] ConcurrentHashMap-Very Strange behavior
 of ReentrantLock under contention.
In-Reply-To: <1366293241814-9526.post@n7.nabble.com>
References: <CAMM2gsNtHBLNQuS=nujrJpPYqhF7Sp+qaS4oTJ8oXj6JQvdNJQ@mail.gmail.com>
	<CAHjP37E9BR1xoXLYBJMrmtXuKpVvidQgXmDuZWE0ngtJxHqmaw@mail.gmail.com>
	<CAMM2gsPABfMON6xwD14hgMrUKcZirLiucLK5S7CTRmr5L_JMcg@mail.gmail.com>
	<516EECD6.1020806@oracle.com>
	<CAMM2gsPiUh8G2zxbeb6xEhZcVz=GGB_Rwta14j2TN8OE4ydpqQ@mail.gmail.com>
	<CAHjP37EikbCHj5Oty8x8tkqcTd=s_1g71o3ov=Vgy4ivDP6PiQ@mail.gmail.com>
	<CAMM2gsPLagyhp1R0EbXYTPHTjE1pAwYDKi1OJznrp=bmA9sgPg@mail.gmail.com>
	<CAHjP37GaQSxWGeK0=ahvoB0jVGgXgH22fOe3TgTRRe-cJaYAVg@mail.gmail.com>
	<CAMM2gsO+Lj+FxdUymtegSswN6=PTQr+osa7_9pu1GS6K7YET2A@mail.gmail.com>
	<1366293241814-9526.post@n7.nabble.com>
Message-ID: <CAMM2gsMXUXvDQfFRCjYd1cUYwJ3=yS=_fy+yEwRS=QgmhH5xkg@mail.gmail.com>

using Java native interface.  Otherwise, there is this library which uses
JNI internally.

http://vanillajava.blogspot.fr/2011/12/thread-affinity-library-for-java.html



On Thu, Apr 18, 2013 at 3:54 PM, thurstonn <thurston at nomagicsoftware.com>wrote:

> Just curious, how were you pinning each thread to a core?
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/ConcurrentHashMap-Very-Strange-behavior-of-ReentrantLock-under-contention-tp9481p9526.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/0762b157/attachment.html>

From nathan.reynolds at oracle.com  Thu Apr 18 13:05:10 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 18 Apr 2013 10:05:10 -0700
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <CAHjP37H3KKBnUvWoM2aJPAOcNEk6-Eo-MbE_a93EimVDNf-p6Q@mail.gmail.com>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<CAHjP37H3KKBnUvWoM2aJPAOcNEk6-Eo-MbE_a93EimVDNf-p6Q@mail.gmail.com>
Message-ID: <517027C6.6030700@oracle.com>

In the future, HotSpot will probably use Intel TSX for synchronized 
blocks.  Harmless data races such as String.hashCode() can now cause 
transaction aborts.  For example, all of the threads/cores compute the 
value, but only 1 thread sets the field.  The rest of the threads/cores 
see the field set and have to roll back their transactions and start 
over.  This is going to waste all of the cycles spent executing from the 
beginning of the synchronized block to the point of calling 
String.hashCode().  Wrapping String.hashCode() in its own synchronized 
block isn't going to work since the abort goes to the start of the outer 
most transaction.  The only solution I can think of is calling 
String.hashCode() just before the synchronized block.  But, I would only 
recommend this if enough cycles are wasted from calling String.hashCode().

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/16/2013 12:22 PM, Vitaly Davidovich wrote:
>
> String caches the hashcode in a plain field; multiple threads can race 
> to compute and store the hashcode, but since string is immutable, this 
> is fine.  The code is careful to operate only on a local variable to 
> avoid re-reads of the field (which could theoretically lead to wrong 
> results if, e.g., JIT reordered code in a certain way).
>
> On Apr 16, 2013 3:12 PM, "thurstonn" <thurston at nomagicsoftware.com 
> <mailto:thurston at nomagicsoftware.com>> wrote:
>
>     Just curious, how is String#hashCode() racy?
>     Strings are immutable in java; I looked at the code a bit and I
>     didn't see
>     anything that looked racy.
>     The only thing I guess could be:
>     private char[] value
>
>
>     Although that array is never modified in the String class, so . . .
>
>
>
>     --
>     View this message in context:
>     http://jsr166-concurrency.10961.n7.nabble.com/On-A-Formal-Definition-of-Data-Race-tp9408p9445.html
>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130418/8705aa99/attachment.html>

From reachbach at gmail.com  Fri Apr 19 05:28:22 2013
From: reachbach at gmail.com (Bharath Ravi Kumar)
Date: Fri, 19 Apr 2013 14:58:22 +0530
Subject: [concurrency-interest] On A Formal Definition of 'Data-Race'
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369D57FA@G9W0725.americas.hpqcorp.net>
References: <1365792944576-9408.post@n7.nabble.com>
	<5169BA31.7010903@briangoetz.com>
	<1365903721280-9413.post@n7.nabble.com>
	<CAHjP37F=omERqETxvW0kb+WUQtdjMEYaATbkxk6tobrk50YCTw@mail.gmail.com>
	<CAMM2gsNKDzX0Fx=c97EufARp_3sCeskCxfUS41PgbR_BMkYnng@mail.gmail.com>
	<CAHjP37HomN5xDUi91rt9xJbk97WrbZtuJkheJ9E543Ej0VOO+Q@mail.gmail.com>
	<1366138629783-9445.post@n7.nabble.com>
	<516DA793.4060606@oracle.com> <516DBFBA.7080104@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369D57FA@G9W0725.americas.hpqcorp.net>
Message-ID: <CALxMP-AsRP_KQWJVdOQtEXH6y31NmDQQQsr-ubTZ5dBvhhXsDw@mail.gmail.com>

On Thu, Apr 18, 2013 at 12:22 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

> > Absolute performance, probably.  Scalability, I doubt it.  Fences on x86
> seem to involve just local work which doesn't involve the memory system.
>  Programs whose performance is limited by fences tend to scale well.  See
> my RACES 12 workshop paper
> http://www.hpl.hp.com/techreports/2012/HPL-2012-218.pdf.
>

The section on race & scalability reads: "In a sense, the synchronized
pro-gram scales better than the racy version. I conjecture that this is due
to memory bandwidth limitations. There are many locks, but due to the
mapping scheme, locks are reused manytimes before being evicted from the
cache" What mapping scheme is being referred to? I did read the footnote on
the mapping scheme, but it doesn't explain lock reuse before cache
eviction. Is the mere presence of the locks in the cacheline helping the
correctly synchronized version? What'd prevent the racy code from
benefiting similarly without the use of an actual lock? Could you please
explain further?  Thanks.

-Bharath
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130419/0b4120e9/attachment.html>

From hallorant at gmail.com  Mon Apr 29 16:27:09 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Mon, 29 Apr 2013 16:27:09 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and
	long : Android
Message-ID: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>

Does anyone know if Android's Dalvik goes with the recommendation in JLS
17.7 and avoids splitting 64-bit values. I'm pretty sure most other VMs do.

After some searching I couldn't find any information and it seems pretty
hard to write code to "test" for this on the VM.

Many thanks and best regards, Tim Halloran
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/b31da042/attachment.html>

From aleksey.shipilev at oracle.com  Mon Apr 29 16:42:30 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 30 Apr 2013 00:42:30 +0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
Message-ID: <517EDB36.1090508@oracle.com>

On 04/30/2013 12:27 AM, Tim Halloran wrote:
> Does anyone know if Android's Dalvik goes with the recommendation in JLS
> 17.7 and avoids splitting 64-bit values. I'm pretty sure most other VMs do.

You might safely assume ARM does not have full-width 64-bit reads/writes
(it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
sure non-volatile longs are not atomic on most mobile platforms.

> After some searching I couldn't find any information and it seems pretty
> hard to write code to "test" for this on the VM.

It is actually one of the simplest tests for concurrency, and simple
enough to reproduce in the wild.

-Aleksey.


From nathan.reynolds at oracle.com  Mon Apr 29 16:46:53 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 29 Apr 2013 13:46:53 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
Message-ID: <517EDC3D.4060601@oracle.com>

If Dalvik can print the assembly code from JIT, then this should be too 
hard to check to see that a 64-bit value is loaded/stored with a single 
instruction.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/29/2013 1:27 PM, Tim Halloran wrote:
> Does anyone know if Android's Dalvik goes with the recommendation in 
> JLS 17.7 and avoids splitting 64-bit values. I'm pretty sure most 
> other VMs do.
>
> After some searching I couldn't find any information and it seems 
> pretty hard to write code to "test" for this on the VM.
>
> Many thanks and best regards, Tim Halloran
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/eaf680f7/attachment.html>

From hallorant at gmail.com  Mon Apr 29 16:54:53 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Mon, 29 Apr 2013 16:54:53 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517EDB36.1090508@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
Message-ID: <CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>

Thanks Aleksey, can you elaborate on how to reproduce this in the wild?

Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
pattern/long value) and x00000000_0000000 (another known bit pattern/long
value) from two different threads and seeing if you get x00000000_FFFFFFFF
or xFFFFFFFF_00000000 -- or did you have another scheme in mind.  Best.


On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 04/30/2013 12:27 AM, Tim Halloran wrote:
> > Does anyone know if Android's Dalvik goes with the recommendation in JLS
> > 17.7 and avoids splitting 64-bit values. I'm pretty sure most other VMs
> do.
>
> You might safely assume ARM does not have full-width 64-bit reads/writes
> (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
> sure non-volatile longs are not atomic on most mobile platforms.
>
> > After some searching I couldn't find any information and it seems pretty
> > hard to write code to "test" for this on the VM.
>
> It is actually one of the simplest tests for concurrency, and simple
> enough to reproduce in the wild.
>
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/ce0696bf/attachment.html>

From aleksey.shipilev at oracle.com  Mon Apr 29 16:59:58 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 30 Apr 2013 00:59:58 +0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
Message-ID: <517EDF4E.3040302@oracle.com>

Yes, that's exactly what I had in mind:
 a. Declare "long a"
 b. Ramp up two threads.
 c. Make thread 1 write 0L and -1L over and over to field $a
 d. Make thread 2 observe the field a, and count the observed values
 e. ...
 f. PROFIT!

P.S. It is important to do some action on value read in thread 2, so
that it does not hoisted from the loop, since $a is not supposed to be
volatile.

-Aleksey.

On 04/30/2013 12:54 AM, Tim Halloran wrote:
> Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
> 
> Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
> pattern/long value) and x00000000_0000000 (another known bit
> pattern/long value) from two different threads and seeing if you get
> x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
> scheme in mind.  Best.
> 
> 
> On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>     > Does anyone know if Android's Dalvik goes with the recommendation
>     in JLS
>     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>     other VMs do.
> 
>     You might safely assume ARM does not have full-width 64-bit reads/writes
>     (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
>     sure non-volatile longs are not atomic on most mobile platforms.
> 
>     > After some searching I couldn't find any information and it seems
>     pretty
>     > hard to write code to "test" for this on the VM.
> 
>     It is actually one of the simplest tests for concurrency, and simple
>     enough to reproduce in the wild.
> 
>     -Aleksey.
> 
> 


From yankee.sierra at gmail.com  Mon Apr 29 17:16:47 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Mon, 29 Apr 2013 17:16:47 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517EDF4E.3040302@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
Message-ID: <CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>

If the action in thread 2 doesn't update $a, isn't the JVM still within its
rights to hoist it from the loop? And even if thread 2 does write to $a
within the loop, isn't the JVM within its rights to update that in some
register and never write it to main memory where it could interact with the
writes from thread 1? In other words, in the absence of any HB (since $a
isn't volatile), can't the JVM just pretend they're entirely separate
variables?


On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> Yes, that's exactly what I had in mind:
>  a. Declare "long a"
>  b. Ramp up two threads.
>  c. Make thread 1 write 0L and -1L over and over to field $a
>  d. Make thread 2 observe the field a, and count the observed values
>  e. ...
>  f. PROFIT!
>
> P.S. It is important to do some action on value read in thread 2, so
> that it does not hoisted from the loop, since $a is not supposed to be
> volatile.
>
> -Aleksey.
>
> On 04/30/2013 12:54 AM, Tim Halloran wrote:
> > Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
> >
> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
> > pattern/long value) and x00000000_0000000 (another known bit
> > pattern/long value) from two different threads and seeing if you get
> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
> > scheme in mind.  Best.
> >
> >
> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
> wrote:
> >
> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
> >     > Does anyone know if Android's Dalvik goes with the recommendation
> >     in JLS
> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
> >     other VMs do.
> >
> >     You might safely assume ARM does not have full-width 64-bit
> reads/writes
> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
> >     sure non-volatile longs are not atomic on most mobile platforms.
> >
> >     > After some searching I couldn't find any information and it seems
> >     pretty
> >     > hard to write code to "test" for this on the VM.
> >
> >     It is actually one of the simplest tests for concurrency, and simple
> >     enough to reproduce in the wild.
> >
> >     -Aleksey.
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/8046f059/attachment-0001.html>

From aleksey.shipilev at oracle.com  Mon Apr 29 17:23:14 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 30 Apr 2013 01:23:14 +0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
Message-ID: <517EE4C2.9020300@oracle.com>

Well, yeah, it gets complicated.

But many *practical* cases would still yield the desired result,
probably by coincidence (i.e. you might percieve the "false" acquire
fence from Thread.interrupted() ;)). So then, if that test shows about
the same frequency of 0L and -1L in thread 2, you're fine, otherwise,
fix it by writing/reading the volatile boolean flag in thread 1 / thread
2, rinse and repeat.

-Aleksey.

On 04/30/2013 01:16 AM, Yuval Shavit wrote:
> If the action in thread 2 doesn't update $a, isn't the JVM still within
> its rights to hoist it from the loop? And even if thread 2 does write to
> $a within the loop, isn't the JVM within its rights to update that in
> some register and never write it to main memory where it could interact
> with the writes from thread 1? In other words, in the absence of any HB
> (since $a isn't volatile), can't the JVM just pretend they're entirely
> separate variables?
> 
> 
> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     Yes, that's exactly what I had in mind:
>      a. Declare "long a"
>      b. Ramp up two threads.
>      c. Make thread 1 write 0L and -1L over and over to field $a
>      d. Make thread 2 observe the field a, and count the observed values
>      e. ...
>      f. PROFIT!
> 
>     P.S. It is important to do some action on value read in thread 2, so
>     that it does not hoisted from the loop, since $a is not supposed to be
>     volatile.
> 
>     -Aleksey.
> 
>     On 04/30/2013 12:54 AM, Tim Halloran wrote:
>     > Thanks Aleksey, can you elaborate on how to reproduce this in the
>     wild?
>     >
>     > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
>     > pattern/long value) and x00000000_0000000 (another known bit
>     > pattern/long value) from two different threads and seeing if you get
>     > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
>     > scheme in mind.  Best.
>     >
>     >
>     > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
>     > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>
>     <mailto:aleksey.shipilev at oracle.com
>     <mailto:aleksey.shipilev at oracle.com>>> wrote:
>     >
>     >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>     >     > Does anyone know if Android's Dalvik goes with the
>     recommendation
>     >     in JLS
>     >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>     >     other VMs do.
>     >
>     >     You might safely assume ARM does not have full-width 64-bit
>     reads/writes
>     >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm
>     pretty
>     >     sure non-volatile longs are not atomic on most mobile platforms.
>     >
>     >     > After some searching I couldn't find any information and it
>     seems
>     >     pretty
>     >     > hard to write code to "test" for this on the VM.
>     >
>     >     It is actually one of the simplest tests for concurrency, and
>     simple
>     >     enough to reproduce in the wild.
>     >
>     >     -Aleksey.
>     >
>     >
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From hallorant at gmail.com  Mon Apr 29 17:27:41 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Mon, 29 Apr 2013 17:27:41 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
Message-ID: <CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>

Yuval,

Dalvik won't hoist the expression. I'm sure of that.  For example,
the snippet below:

boolean flag;

In one thread:
void run() {
  while (!flag) {
    // todo
  }
}

In another thread (later on):
flag = true;

On JIT it never terminates due to hoisting inside the JIT (the compiler is
not doing the optimization) (Windows, Mac OS, Linux). Today, on Android
this program works. (At least on a whole lot of Android devices I've tried
it on anyway.)

Sadly, if they add JIT to Dalvik a whole lot of Android apps are going to
break. :-(  I've found several open source examples that rely upon this
broken code behaving as it does on Dalvik today.


On Mon, Apr 29, 2013 at 5:16 PM, Yuval Shavit <yankee.sierra at gmail.com>wrote:

> If the action in thread 2 doesn't update $a, isn't the JVM still within
> its rights to hoist it from the loop? And even if thread 2 does write to $a
> within the loop, isn't the JVM within its rights to update that in some
> register and never write it to main memory where it could interact with the
> writes from thread 1? In other words, in the absence of any HB (since $a
> isn't volatile), can't the JVM just pretend they're entirely separate
> variables?
>
>
> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> Yes, that's exactly what I had in mind:
>>  a. Declare "long a"
>>  b. Ramp up two threads.
>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>  d. Make thread 2 observe the field a, and count the observed values
>>  e. ...
>>  f. PROFIT!
>>
>> P.S. It is important to do some action on value read in thread 2, so
>> that it does not hoisted from the loop, since $a is not supposed to be
>> volatile.
>>
>> -Aleksey.
>>
>> On 04/30/2013 12:54 AM, Tim Halloran wrote:
>> > Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
>> >
>> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
>> > pattern/long value) and x00000000_0000000 (another known bit
>> > pattern/long value) from two different threads and seeing if you get
>> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
>> > scheme in mind.  Best.
>> >
>> >
>> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
>> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>> wrote:
>> >
>> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>> >     > Does anyone know if Android's Dalvik goes with the recommendation
>> >     in JLS
>> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>> >     other VMs do.
>> >
>> >     You might safely assume ARM does not have full-width 64-bit
>> reads/writes
>> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
>> >     sure non-volatile longs are not atomic on most mobile platforms.
>> >
>> >     > After some searching I couldn't find any information and it seems
>> >     pretty
>> >     > hard to write code to "test" for this on the VM.
>> >
>> >     It is actually one of the simplest tests for concurrency, and simple
>> >     enough to reproduce in the wild.
>> >
>> >     -Aleksey.
>> >
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/590f0425/attachment.html>

From gergg at cox.net  Mon Apr 29 18:10:13 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 29 Apr 2013 17:10:13 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
In-Reply-To: <CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
	<CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
Message-ID: <14319DA3-97A1-4668-9C86-8F63D27CBC76@cox.net>

This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.  

Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.

That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.

Gregg

On Apr 29, 2013, at 4:27 PM, Tim Halloran <hallorant at gmail.com> wrote:

> Yuval,
> 
> Dalvik won't hoist the expression. I'm sure of that.  For example, the snippet below:
> 
> boolean flag;
> 
> In one thread:
> void run() {
>   while (!flag) {
>     // todo
>   }
> }
> 
> In another thread (later on):
> flag = true;
> 
> On JIT it never terminates due to hoisting inside the JIT (the compiler is not doing the optimization) (Windows, Mac OS, Linux). Today, on Android this program works. (At least on a whole lot of Android devices I've tried it on anyway.)
> 
> Sadly, if they add JIT to Dalvik a whole lot of Android apps are going to break. :-(  I've found several open source examples that rely upon this broken code behaving as it does on Dalvik today.
> 
> 
> On Mon, Apr 29, 2013 at 5:16 PM, Yuval Shavit <yankee.sierra at gmail.com> wrote:
> If the action in thread 2 doesn't update $a, isn't the JVM still within its rights to hoist it from the loop? And even if thread 2 does write to $a within the loop, isn't the JVM within its rights to update that in some register and never write it to main memory where it could interact with the writes from thread 1? In other words, in the absence of any HB (since $a isn't volatile), can't the JVM just pretend they're entirely separate variables?
> 
> 
> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:
> Yes, that's exactly what I had in mind:
>  a. Declare "long a"
>  b. Ramp up two threads.
>  c. Make thread 1 write 0L and -1L over and over to field $a
>  d. Make thread 2 observe the field a, and count the observed values
>  e. ...
>  f. PROFIT!
> 
> P.S. It is important to do some action on value read in thread 2, so
> that it does not hoisted from the loop, since $a is not supposed to be
> volatile.
> 
> -Aleksey.
> 
> On 04/30/2013 12:54 AM, Tim Halloran wrote:
> > Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
> >
> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
> > pattern/long value) and x00000000_0000000 (another known bit
> > pattern/long value) from two different threads and seeing if you get
> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
> > scheme in mind.  Best.
> >
> >
> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> >
> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
> >     > Does anyone know if Android's Dalvik goes with the recommendation
> >     in JLS
> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
> >     other VMs do.
> >
> >     You might safely assume ARM does not have full-width 64-bit reads/writes
> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
> >     sure non-volatile longs are not atomic on most mobile platforms.
> >
> >     > After some searching I couldn't find any information and it seems
> >     pretty
> >     > hard to write code to "test" for this on the VM.
> >
> >     It is actually one of the simplest tests for concurrency, and simple
> >     enough to reproduce in the wild.
> >
> >     -Aleksey.
> >
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/b09a79fa/attachment-0001.html>

From joe.bowbeer at gmail.com  Mon Apr 29 18:12:33 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 29 Apr 2013 15:12:33 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
	<CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
Message-ID: <CAHzJPEpM-uFchCZTS4X1Ffp0Zk=BggMYvqRKvGpMmm1xpSPkOQ@mail.gmail.com>

A JIT was added to Android a couple three years ago:

http://android-developers.blogspot.com/2010/05/dalvik-jit.html


On Mon, Apr 29, 2013 at 2:27 PM, Tim Halloran <hallorant at gmail.com> wrote:

> Yuval,
>
> Dalvik won't hoist the expression. I'm sure of that.  For example,
> the snippet below:
>
> boolean flag;
>
> In one thread:
> void run() {
>   while (!flag) {
>     // todo
>   }
> }
>
> In another thread (later on):
> flag = true;
>
> On JIT it never terminates due to hoisting inside the JIT (the compiler is
> not doing the optimization) (Windows, Mac OS, Linux). Today, on Android
> this program works. (At least on a whole lot of Android devices I've tried
> it on anyway.)
>
> Sadly, if they add JIT to Dalvik a whole lot of Android apps are going to
> break. :-(  I've found several open source examples that rely upon this
> broken code behaving as it does on Dalvik today.
>
>
> On Mon, Apr 29, 2013 at 5:16 PM, Yuval Shavit <yankee.sierra at gmail.com>wrote:
>
>> If the action in thread 2 doesn't update $a, isn't the JVM still within
>> its rights to hoist it from the loop? And even if thread 2 does write to $a
>> within the loop, isn't the JVM within its rights to update that in some
>> register and never write it to main memory where it could interact with the
>> writes from thread 1? In other words, in the absence of any HB (since $a
>> isn't volatile), can't the JVM just pretend they're entirely separate
>> variables?
>>
>>
>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>> aleksey.shipilev at oracle.com> wrote:
>>
>>> Yes, that's exactly what I had in mind:
>>>  a. Declare "long a"
>>>  b. Ramp up two threads.
>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>  d. Make thread 2 observe the field a, and count the observed values
>>>  e. ...
>>>  f. PROFIT!
>>>
>>> P.S. It is important to do some action on value read in thread 2, so
>>> that it does not hoisted from the loop, since $a is not supposed to be
>>> volatile.
>>>
>>> -Aleksey.
>>>
>>> On 04/30/2013 12:54 AM, Tim Halloran wrote:
>>> > Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
>>> >
>>> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
>>> > pattern/long value) and x00000000_0000000 (another known bit
>>> > pattern/long value) from two different threads and seeing if you get
>>> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
>>> > scheme in mind.  Best.
>>> >
>>> >
>>> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
>>> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>>> wrote:
>>> >
>>> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>>> >     > Does anyone know if Android's Dalvik goes with the recommendation
>>> >     in JLS
>>> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>>> >     other VMs do.
>>> >
>>> >     You might safely assume ARM does not have full-width 64-bit
>>> reads/writes
>>> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm
>>> pretty
>>> >     sure non-volatile longs are not atomic on most mobile platforms.
>>> >
>>> >     > After some searching I couldn't find any information and it seems
>>> >     pretty
>>> >     > hard to write code to "test" for this on the VM.
>>> >
>>> >     It is actually one of the simplest tests for concurrency, and
>>> simple
>>> >     enough to reproduce in the wild.
>>> >
>>> >     -Aleksey.
>>> >
>>> >
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/e13707eb/attachment.html>

From vitalyd at gmail.com  Mon Apr 29 19:34:37 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 29 Apr 2013 19:34:37 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHzJPEpM-uFchCZTS4X1Ffp0Zk=BggMYvqRKvGpMmm1xpSPkOQ@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
	<CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
	<CAHzJPEpM-uFchCZTS4X1Ffp0Zk=BggMYvqRKvGpMmm1xpSPkOQ@mail.gmail.com>
Message-ID: <CAHjP37EMXQb=T6Hect53Kz=eKUwmrdaX4CK9ZKQb17QVcv+R2Q@mail.gmail.com>

It's not the JIT, of course, that matters but whether the optimizer coming
with it actually does this code motion.  Most likely reason for this type
of thing working is the Dalvik optimizer is still rudimentary and doesn't
hoist cases like this.

Sent from my phone
On Apr 29, 2013 6:15 PM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:

> A JIT was added to Android a couple three years ago:
>
> http://android-developers.blogspot.com/2010/05/dalvik-jit.html
>
>
> On Mon, Apr 29, 2013 at 2:27 PM, Tim Halloran <hallorant at gmail.com> wrote:
>
>> Yuval,
>>
>> Dalvik won't hoist the expression. I'm sure of that.  For example,
>> the snippet below:
>>
>> boolean flag;
>>
>> In one thread:
>> void run() {
>>   while (!flag) {
>>     // todo
>>   }
>> }
>>
>> In another thread (later on):
>> flag = true;
>>
>> On JIT it never terminates due to hoisting inside the JIT (the compiler
>> is not doing the optimization) (Windows, Mac OS, Linux). Today, on Android
>> this program works. (At least on a whole lot of Android devices I've tried
>> it on anyway.)
>>
>> Sadly, if they add JIT to Dalvik a whole lot of Android apps are going to
>> break. :-(  I've found several open source examples that rely upon this
>> broken code behaving as it does on Dalvik today.
>>
>>
>> On Mon, Apr 29, 2013 at 5:16 PM, Yuval Shavit <yankee.sierra at gmail.com>wrote:
>>
>>> If the action in thread 2 doesn't update $a, isn't the JVM still within
>>> its rights to hoist it from the loop? And even if thread 2 does write to $a
>>> within the loop, isn't the JVM within its rights to update that in some
>>> register and never write it to main memory where it could interact with the
>>> writes from thread 1? In other words, in the absence of any HB (since $a
>>> isn't volatile), can't the JVM just pretend they're entirely separate
>>> variables?
>>>
>>>
>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>> aleksey.shipilev at oracle.com> wrote:
>>>
>>>> Yes, that's exactly what I had in mind:
>>>>  a. Declare "long a"
>>>>  b. Ramp up two threads.
>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>  e. ...
>>>>  f. PROFIT!
>>>>
>>>> P.S. It is important to do some action on value read in thread 2, so
>>>> that it does not hoisted from the loop, since $a is not supposed to be
>>>> volatile.
>>>>
>>>> -Aleksey.
>>>>
>>>> On 04/30/2013 12:54 AM, Tim Halloran wrote:
>>>> > Thanks Aleksey, can you elaborate on how to reproduce this in the
>>>> wild?
>>>> >
>>>> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
>>>> > pattern/long value) and x00000000_0000000 (another known bit
>>>> > pattern/long value) from two different threads and seeing if you get
>>>> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
>>>> > scheme in mind.  Best.
>>>> >
>>>> >
>>>> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
>>>> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
>>>> wrote:
>>>> >
>>>> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>>>> >     > Does anyone know if Android's Dalvik goes with the
>>>> recommendation
>>>> >     in JLS
>>>> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>>>> >     other VMs do.
>>>> >
>>>> >     You might safely assume ARM does not have full-width 64-bit
>>>> reads/writes
>>>> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm
>>>> pretty
>>>> >     sure non-volatile longs are not atomic on most mobile platforms.
>>>> >
>>>> >     > After some searching I couldn't find any information and it
>>>> seems
>>>> >     pretty
>>>> >     > hard to write code to "test" for this on the VM.
>>>> >
>>>> >     It is actually one of the simplest tests for concurrency, and
>>>> simple
>>>> >     enough to reproduce in the wild.
>>>> >
>>>> >     -Aleksey.
>>>> >
>>>> >
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/10d39dfe/attachment.html>

From vitalyd at gmail.com  Mon Apr 29 19:41:04 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 29 Apr 2013 19:41:04 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517EDB36.1090508@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
Message-ID: <CAHjP37Gs6ZFF3hWKDeU0d7dCNA2x_2Q_VMYC+c7AU9+-0GW5bg@mail.gmail.com>

No different than x86.  I suppose story will be different with armv8 and
dalvik support for it.

Sent from my phone
On Apr 29, 2013 4:45 PM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> On 04/30/2013 12:27 AM, Tim Halloran wrote:
> > Does anyone know if Android's Dalvik goes with the recommendation in JLS
> > 17.7 and avoids splitting 64-bit values. I'm pretty sure most other VMs
> do.
>
> You might safely assume ARM does not have full-width 64-bit reads/writes
> (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
> sure non-volatile longs are not atomic on most mobile platforms.
>
> > After some searching I couldn't find any information and it seems pretty
> > hard to write code to "test" for this on the VM.
>
> It is actually one of the simplest tests for concurrency, and simple
> enough to reproduce in the wild.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130429/048ab1b7/attachment-0001.html>

From hallorant at gmail.com  Tue Apr 30 00:07:28 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Tue, 30 Apr 2013 00:07:28 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37Gs6ZFF3hWKDeU0d7dCNA2x_2Q_VMYC+c7AU9+-0GW5bg@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAHjP37Gs6ZFF3hWKDeU0d7dCNA2x_2Q_VMYC+c7AU9+-0GW5bg@mail.gmail.com>
Message-ID: <CAMyLHFyRyG+pHGGGf=6Fc34-VhQsjJ7tb54j3MCoxx=rmaF=6A@mail.gmail.com>

Thanks for the Dalvik JIT pointer.

I should have said, as Vitaly corrected:

Sadly, if they add *a loop invariant hoisting optimization* to Dalvik a
> whole lot of Android apps are going to break. :-(  I've found several open
> source examples that rely upon this broken code behaving as it does on
> Dalvik today.


This optimization seems to be less dependent on the chip (ARM vs Intel)
than how much analysis the JIT optimizer is willing to spend time trying to
do. I guess it is dependent upon how good the CPU is at allowing cycles for
optimization, but beyond that it is a portable optimization.

(pie in the sky thought) This all makes me ponder that it might be ideal to
make the memory model implementation as close to the JMM specification as
possible, ideally in a way that makes "broken" programs behave in a
obviously broken manner. The more conservative (toward sequential
consistency, I guess) a platform implementation is (HotSpot on Intel,
Android on ARM) the more likely code depends upon behavior that exists in
the platform implementation that isn't specified by the JMM. In
Brooks-speak I'm wondering if the platform implementations are accidental
or essential complexity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/55781384/attachment.html>

From elizarov at devexperts.com  Tue Apr 30 00:21:59 2013
From: elizarov at devexperts.com (Roman Elizarov)
Date: Tue, 30 Apr 2013 04:21:59 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFyRyG+pHGGGf=6Fc34-VhQsjJ7tb54j3MCoxx=rmaF=6A@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAHjP37Gs6ZFF3hWKDeU0d7dCNA2x_2Q_VMYC+c7AU9+-0GW5bg@mail.gmail.com>
	<CAMyLHFyRyG+pHGGGf=6Fc34-VhQsjJ7tb54j3MCoxx=rmaF=6A@mail.gmail.com>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A824B049DF@RAVEN.office.devexperts.com>

See "Adversarial Memory for Detecting Destructive Races"
http://users.soe.ucsc.edu/~cormac/papers/pldi10.pdf
It will catch the aforementioned bug on the first run.

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Tim Halloran
Sent: Tuesday, April 30, 2013 8:07 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

Thanks for the Dalvik JIT pointer.

I should have said, as Vitaly corrected:

Sadly, if they add a loop invariant hoisting optimization to Dalvik a whole lot of Android apps are going to break. :-(  I've found several open source examples that rely upon this broken code behaving as it does on Dalvik today.

This optimization seems to be less dependent on the chip (ARM vs Intel) than how much analysis the JIT optimizer is willing to spend time trying to do. I guess it is dependent upon how good the CPU is at allowing cycles for optimization, but beyond that it is a portable optimization.

(pie in the sky thought) This all makes me ponder that it might be ideal to make the memory model implementation as close to the JMM specification as possible, ideally in a way that makes "broken" programs behave in a obviously broken manner. The more conservative (toward sequential consistency, I guess) a platform implementation is (HotSpot on Intel, Android on ARM) the more likely code depends upon behavior that exists in the platform implementation that isn't specified by the JMM. In Brooks-speak I'm wondering if the platform implementations are accidental or essential complexity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/30a4044f/attachment.html>

From kirk at kodewerk.com  Tue Apr 30 01:38:01 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 30 Apr 2013 07:38:01 +0200
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
In-Reply-To: <14319DA3-97A1-4668-9C86-8F63D27CBC76@cox.net>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAE+h5-CDh-6KTFhHx-X02Q=0w53c_G+FgaDeeXxohN4TWgS5Ow@mail.gmail.com>
	<CAMyLHFx-3M1DVHbJD9HgXrXAstzN8qUjnEi4u-r=g_e0pr+hoA@mail.gmail.com>
	<14319DA3-97A1-4668-9C86-8F63D27CBC76@cox.net>
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3@kodewerk.com>

Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!

Regards,
Kirk

On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:

> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.  
> 
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
> 
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
> 
> Gregg
> 
> On Apr 29, 2013, at 4:27 PM, Tim Halloran <hallorant at gmail.com> wrote:
> 
>> Yuval,
>> 
>> Dalvik won't hoist the expression. I'm sure of that.  For example, the snippet below:
>> 
>> boolean flag;
>> 
>> In one thread:
>> void run() {
>>   while (!flag) {
>>     // todo
>>   }
>> }
>> 
>> In another thread (later on):
>> flag = true;
>> 
>> On JIT it never terminates due to hoisting inside the JIT (the compiler is not doing the optimization) (Windows, Mac OS, Linux). Today, on Android this program works. (At least on a whole lot of Android devices I've tried it on anyway.)
>> 
>> Sadly, if they add JIT to Dalvik a whole lot of Android apps are going to break. :-(  I've found several open source examples that rely upon this broken code behaving as it does on Dalvik today.
>> 
>> 
>> On Mon, Apr 29, 2013 at 5:16 PM, Yuval Shavit <yankee.sierra at gmail.com> wrote:
>> If the action in thread 2 doesn't update $a, isn't the JVM still within its rights to hoist it from the loop? And even if thread 2 does write to $a within the loop, isn't the JVM within its rights to update that in some register and never write it to main memory where it could interact with the writes from thread 1? In other words, in the absence of any HB (since $a isn't volatile), can't the JVM just pretend they're entirely separate variables?
>> 
>> 
>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:
>> Yes, that's exactly what I had in mind:
>>  a. Declare "long a"
>>  b. Ramp up two threads.
>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>  d. Make thread 2 observe the field a, and count the observed values
>>  e. ...
>>  f. PROFIT!
>> 
>> P.S. It is important to do some action on value read in thread 2, so
>> that it does not hoisted from the loop, since $a is not supposed to be
>> volatile.
>> 
>> -Aleksey.
>> 
>> On 04/30/2013 12:54 AM, Tim Halloran wrote:
>> > Thanks Aleksey, can you elaborate on how to reproduce this in the wild?
>> >
>> > Are you thinking of writing xFFFFFFFF_FFFFFFFF (or some known bit
>> > pattern/long value) and x00000000_0000000 (another known bit
>> > pattern/long value) from two different threads and seeing if you get
>> > x00000000_FFFFFFFF or xFFFFFFFF_00000000 -- or did you have another
>> > scheme in mind.  Best.
>> >
>> >
>> > On Mon, Apr 29, 2013 at 4:42 PM, Aleksey Shipilev
>> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>> >
>> >     On 04/30/2013 12:27 AM, Tim Halloran wrote:
>> >     > Does anyone know if Android's Dalvik goes with the recommendation
>> >     in JLS
>> >     > 17.7 and avoids splitting 64-bit values. I'm pretty sure most
>> >     other VMs do.
>> >
>> >     You might safely assume ARM does not have full-width 64-bit reads/writes
>> >     (it can emulate atomic read/write with LL/SC-ed loop), so I'm pretty
>> >     sure non-volatile longs are not atomic on most mobile platforms.
>> >
>> >     > After some searching I couldn't find any information and it seems
>> >     pretty
>> >     > hard to write code to "test" for this on the VM.
>> >
>> >     It is actually one of the simplest tests for concurrency, and simple
>> >     enough to reproduce in the wild.
>> >
>> >     -Aleksey.
>> >
>> >
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/802825f5/attachment-0001.html>

From mjpt777 at gmail.com  Tue Apr 30 08:57:04 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Tue, 30 Apr 2013 13:57:04 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
Message-ID: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>

I agree with Kirk here and would take it further.  By default the vast
majority of code should be single threaded and concurrent programming is
only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler
optimizations would be defeated.  A default position that all code is
concurrent is sending the industry the wrong way in my view.  It makes a
more sense to explicitly define the regions of data exchange in our
programs and therefore what ordering semantics are required in those
regions.

Martin...

------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com>
To: Gregg Wonderly <gergg at cox.net>
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do.
Code wouldn't be a bit slower, it would be a lot slower and then you'd end
up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> This code exists everywhere on the Java JVM now, because no one expects
the loop hoist?   People are living with it, or eventually declaring the
loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and
developers should of needed to add non-volatile declarations via
annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up
what you were doing, because the added verbosity would only be tolerated
when it actually accomplished a performance improvement.  Today, code is
"Faster" without "volatile".   If everything was "volatile" by default,
then code would be slower to start with, and proper "concurrency
programming" would then make your code faster, as concurrency should.
>
> Gregg
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/24fb6bd7/attachment.html>

From hallorant at gmail.com  Tue Apr 30 10:36:59 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Tue, 30 Apr 2013 10:36:59 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517EDF4E.3040302@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
Message-ID: <CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>

On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> Yes, that's exactly what I had in mind:
>  a. Declare "long a"
>  b. Ramp up two threads.
>  c. Make thread 1 write 0L and -1L over and over to field $a
>  d. Make thread 2 observe the field a, and count the observed values
>  e. ...
>  f. PROFIT!
>
> P.S. It is important to do some action on value read in thread 2, so
> that it does not hoisted from the loop, since $a is not supposed to be
> volatile.
>
> -Aleksey.
>
>
This discussion is getting a bit far afield, I guess, but to get back onto
the topic. I followed Aleksey's advice. And wrote an implementation that
tests this.  I used two separate threads to write 0L and -1L into the long
field "a" but that is the only real change I made. (I already had some
scaffolding code to run things on Android or desktop Java).

*Android: splits writes to longs into two parts.*

On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with Android
4.2.2 I saw non-atomic treatment of long. The value -4294967296
(xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).

So looks like Android does not follow the (albeit optional) advice in the
Java language specification about this.

*JDK: DOES NOT split writes to longs into two parts (even 32-bit
implementations)*

Of course we couldn't get this to happen on any 64-bit JVM, but we tried it
out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The 32-bit
JVM implementations follow the recommendation of the Java language
specification.

An interesting curio. I wonder how many crashes in "working" Java code
moved from desktop Java onto Android programmers are going to lose sleep
tracking down this one.

Best regards,
Tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/2d5a92d6/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: device-2013-04-30-102546.png
Type: image/png
Size: 153306 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/2d5a92d6/attachment-0001.png>

From aleksey.shipilev at oracle.com  Tue Apr 30 10:59:59 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 30 Apr 2013 18:59:59 +0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
Message-ID: <517FDC6F.5070704@oracle.com>

On 04/30/2013 06:36 PM, Tim Halloran wrote:
> *JDK: DOES NOT split writes to longs into two parts (even 32-bit
> implementations)*
> 
> Of course we couldn't get this to happen on any 64-bit JVM, but we tried
> it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
> 32-bit JVM implementations follow the recommendation of the Java
> language specification.

This is where you should be careful about the interpretations.

You can claim the splitting occurs once you see one, but if you see no
splitting, your can not claim it does not occur. In my own internal
tests, every 32-bit x86 OpenJDK-derived VM experiences the long splits.

-Aleksey.

From mthornton at optrak.com  Tue Apr 30 11:26:56 2013
From: mthornton at optrak.com (Mark Thornton)
Date: Tue, 30 Apr 2013 16:26:56 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
Message-ID: <517FE2C0.7010101@optrak.com>

On 30/04/13 15:36, Tim Halloran wrote:
> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev 
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>
>     Yes, that's exactly what I had in mind:
>      a. Declare "long a"
>      b. Ramp up two threads.
>      c. Make thread 1 write 0L and -1L over and over to field $a
>      d. Make thread 2 observe the field a, and count the observed values
>      e. ...
>      f. PROFIT!
>
>     P.S. It is important to do some action on value read in thread 2, so
>     that it does not hoisted from the loop, since $a is not supposed to be
>     volatile.
>
>     -Aleksey.
>
>
> This discussion is getting a bit far afield, I guess, but to get back 
> onto the topic. I followed Aleksey's advice. And wrote an 
> implementation that tests this.  I used two separate threads to write 
> 0L and -1L into the long field "a" but that is the only real change I 
> made. (I already had some scaffolding code to run things on Android or 
> desktop Java).
>
> *Android: splits writes to longs into two parts.*
>
> On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with 
> Android 4.2.2 I saw non-atomic treatment of long. The value 
> -4294967296 (xFFFFFFFF00000000) showed up as well as 4294967295 
> (x00000000FFFFFFFF).
>
> So looks like Android does not follow the (albeit optional) advice in 
> the Java language specification about this.
>
> *JDK: DOES NOT split writes to longs into two parts (even 32-bit 
> implementations)*
>
> Of course we couldn't get this to happen on any 64-bit JVM, but we 
> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT 
> happen. The 32-bit JVM implementations follow the recommendation of 
> the Java language specification.
>
> An interesting curio. I wonder how many crashes in "working" Java code 
> moved from desktop Java onto Android programmers are going to lose 
> sleep tracking down this one.
>
>

Last time I tried this sort of test, a split write would be observed in 
under a second on a true dual processor. However, with only one 
processor available, it would typically take around 20 minutes. So you 
might have to run a very long test to have any real confidence in the 
lack of splitting.

Mark Thornton

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/f6c50b6b/attachment.html>

From zhong.j.yu at gmail.com  Tue Apr 30 11:29:42 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 30 Apr 2013 10:29:42 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
Message-ID: <CACuKZqFGfVqVLoe8QtJKO1scSiTa0a_R5GJu31HmTNjGwn+b9Q@mail.gmail.com>

Is it feasible for threads to observe some MESI-like protocol, so that
non-shared variables can be identified and optimized. The memory model can
claim that all variables are volatile without exception, but execution can
treat most variables as non-volatile.

Zhong Yu



On Tue, Apr 30, 2013 at 7:57 AM, Martin Thompson <mjpt777 at gmail.com> wrote:

> I agree with Kirk here and would take it further.  By default the vast
> majority of code should be single threaded and concurrent programming is
> only utilized in regions of data exchange.
>
> If all code was sequentially consistent then most hardware and compiler
> optimizations would be defeated.  A default position that all code is
> concurrent is sending the industry the wrong way in my view.  It makes a
> more sense to explicitly define the regions of data exchange in our
> programs and therefore what ordering semantics are required in those
> regions.
>
> Martin...
>
> ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine <kirk at kodewerk.com>
> To: Gregg Wonderly <gergg at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double  and long : Android
> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
> Content-Type: text/plain; charset="windows-1252"
>
> Sorry but making thing volatile by default would be a horrible thing to
> do. Code wouldn't be a bit slower, it would be a lot slower and then you'd
> end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> > This code exists everywhere on the Java JVM now, because no one expects
> the loop hoist?   People are living with it, or eventually declaring the
> loop variable volatile after finding these discussions.
> >
> > Java, by default, should of used nothing but volatile variables, and
> developers should of needed to add non-volatile declarations via
> annotations, without the 'volatile' keyword being used, at all.
> >
> > That would of made it hard to "break" code without actually looking up
> what you were doing, because the added verbosity would only be tolerated
> when it actually accomplished a performance improvement.  Today, code is
> "Faster" without "volatile".   If everything was "volatile" by default,
> then code would be slower to start with, and proper "concurrency
> programming" would then make your code faster, as concurrency should.
> >
> > Gregg
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/bc3d617e/attachment.html>

From hallorant at gmail.com  Tue Apr 30 11:48:34 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Tue, 30 Apr 2013 11:48:34 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517FE2C0.7010101@optrak.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
Message-ID: <CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>

Aleksey, correct -- more trials show what you predicted. Thanks for the
nudge.

Mark,

Very helpful, in fact, we are seeing quick failures except for the
dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
have yet to get a failure.  The two programs attached are what I'm running.
 I stripped out my benchmark framework (so they are easy to run on OpenJDK
but not on Android).  The difference is that one uses two threads (one
writer one reader) the other three (two writers one reader) -- both seem to
produce similar results.

With one processor, OpenJDK 1.6.0_27 I see the split write almost
immediatly. Dual we can't get a failure, yet, we get more failures as the
processor count goes up -- but after a few failures, we don't get any more
(they program tries to get 10 to happen)...we can't get to 10.

It seems that while this can happen on OpenJDK it is rarer than on Android
where ten failures takes less than a second to happen.

Best, Tim



On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:

>  On 30/04/13 15:36, Tim Halloran wrote:
>
> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> Yes, that's exactly what I had in mind:
>>  a. Declare "long a"
>>  b. Ramp up two threads.
>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>  d. Make thread 2 observe the field a, and count the observed values
>>  e. ...
>>  f. PROFIT!
>>
>> P.S. It is important to do some action on value read in thread 2, so
>> that it does not hoisted from the loop, since $a is not supposed to be
>> volatile.
>>
>> -Aleksey.
>>
>>
>  This discussion is getting a bit far afield, I guess, but to get back
> onto the topic. I followed Aleksey's advice. And wrote an implementation
> that tests this.  I used two separate threads to write 0L and -1L into the
> long field "a" but that is the only real change I made. (I already had some
> scaffolding code to run things on Android or desktop Java).
>
>  *Android: splits writes to longs into two parts.*
>
>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with Android
> 4.2.2 I saw non-atomic treatment of long. The value -4294967296
> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>
>  So looks like Android does not follow the (albeit optional) advice in
> the Java language specification about this.
>
>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
> implementations)*
>
>  Of course we couldn't get this to happen on any 64-bit JVM, but we tried
> it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
> 32-bit JVM implementations follow the recommendation of the Java language
> specification.
>
>  An interesting curio. I wonder how many crashes in "working" Java code
> moved from desktop Java onto Android programmers are going to lose sleep
> tracking down this one.
>
>
>
> Last time I tried this sort of test, a split write would be observed in
> under a second on a true dual processor. However, with only one processor
> available, it would typically take around 20 minutes. So you might have to
> run a very long test to have any real confidence in the lack of splitting.
>
> Mark Thornton
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/6e9f9763/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: SplitWritesToLongMain.java
Type: application/octet-stream
Size: 1585 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/6e9f9763/attachment-0002.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: SplitWritesToLongMain2.java
Type: application/octet-stream
Size: 1418 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/6e9f9763/attachment-0003.obj>

From vitalyd at gmail.com  Tue Apr 30 11:56:01 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 11:56:01 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
Message-ID: <CAHjP37H81-uT=jwRmS2C1UYL3+unPkOgG-N53p5n9aQn88jK7g@mail.gmail.com>

I don't see how 32bit Hotspot can *not * split 64 bit values given
registers are not wide enough.

Easiest would be to hook up hsdis to hotspot and look at the disassembly.

Sent from my phone
On Apr 30, 2013 10:42 AM, "Tim Halloran" <hallorant at gmail.com> wrote:

> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> Yes, that's exactly what I had in mind:
>>  a. Declare "long a"
>>  b. Ramp up two threads.
>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>  d. Make thread 2 observe the field a, and count the observed values
>>  e. ...
>>  f. PROFIT!
>>
>> P.S. It is important to do some action on value read in thread 2, so
>> that it does not hoisted from the loop, since $a is not supposed to be
>> volatile.
>>
>> -Aleksey.
>>
>>
> This discussion is getting a bit far afield, I guess, but to get back onto
> the topic. I followed Aleksey's advice. And wrote an implementation that
> tests this.  I used two separate threads to write 0L and -1L into the long
> field "a" but that is the only real change I made. (I already had some
> scaffolding code to run things on Android or desktop Java).
>
> *Android: splits writes to longs into two parts.*
>
> On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with Android
> 4.2.2 I saw non-atomic treatment of long. The value -4294967296
> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>
> So looks like Android does not follow the (albeit optional) advice in the
> Java language specification about this.
>
> *JDK: DOES NOT split writes to longs into two parts (even 32-bit
> implementations)*
>
> Of course we couldn't get this to happen on any 64-bit JVM, but we tried
> it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
> 32-bit JVM implementations follow the recommendation of the Java language
> specification.
>
> An interesting curio. I wonder how many crashes in "working" Java code
> moved from desktop Java onto Android programmers are going to lose sleep
> tracking down this one.
>
> Best regards,
> Tim
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/c64edcac/attachment.html>

From nathan.reynolds at oracle.com  Tue Apr 30 12:17:45 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Apr 2013 09:17:45 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
Message-ID: <517FEEA9.7080403@oracle.com>

You might want to print the assembly using HotSpot (and OpenJDK?).  If 
the assembly, uses 1 instruction to do the write, then no splitting can 
ever happen (because alignment takes care of cache line splits).  If the 
assembly, uses 2 instructions to do the write, then it is only a matter 
of timing.

With a single processor system, you are waiting for the thread's quantum 
to end right after the first instruction but before the second 
instruction.  This will allow the other thread to see the split write.

With a dual processor system, the reader thread simply has to get a copy 
of the cache line after the first write and before the second write.  
This is much easier to do.

HotSpot will do a lot of optimizations on single processor systems.  For 
example, it gets rid of the "lock" prefix in front of atomic 
instructions since the instruction's execution can't be split.  It also 
doesn't output memory fences.  Both of these give good performance 
boosts.  I wonder if with one processor, OpenJDK is using 2 instructions 
to do the write whereas with multiple processors it plays it safe and 
uses 1 instruction.

Note: If you disable all of the processors but 1 and then start HotSpot, 
HotSpot will start in single processor mode.  If you then enable those 
processors while HotSpot is running, a lot of things break and the JVM 
will crash.  Because single processor systems are rare, the default 
might be changed to assume multiple processors unless the command line 
specifies 1 processor.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/30/2013 8:48 AM, Tim Halloran wrote:
> Aleksey, correct -- more trials show what you predicted. Thanks for 
> the nudge.
>
> Mark,
>
> Very helpful, in fact, we are seeing quick failures except for the 
> dual-processor case -- on a dual processor hardware or VM (Virtual 
> Box) we have yet to get a failure.  The two programs attached are what 
> I'm running.  I stripped out my benchmark framework (so they are easy 
> to run on OpenJDK but not on Android).  The difference is that one 
> uses two threads (one writer one reader) the other three (two writers 
> one reader) -- both seem to produce similar results.
>
> With one processor, OpenJDK 1.6.0_27 I see the split write almost 
> immediatly. Dual we can't get a failure, yet, we get more failures as 
> the processor count goes up -- but after a few failures, we don't get 
> any more (they program tries to get 10 to happen)...we can't get to 10.
>
> It seems that while this can happen on OpenJDK it is rarer than on 
> Android where ten failures takes less than a second to happen.
>
> Best, Tim
>
>
>
> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com 
> <mailto:mthornton at optrak.com>> wrote:
>
>     On 30/04/13 15:36, Tim Halloran wrote:
>>     On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>     <aleksey.shipilev at oracle.com
>>     <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>>         Yes, that's exactly what I had in mind:
>>          a. Declare "long a"
>>          b. Ramp up two threads.
>>          c. Make thread 1 write 0L and -1L over and over to field $a
>>          d. Make thread 2 observe the field a, and count the observed
>>         values
>>          e. ...
>>          f. PROFIT!
>>
>>         P.S. It is important to do some action on value read in
>>         thread 2, so
>>         that it does not hoisted from the loop, since $a is not
>>         supposed to be
>>         volatile.
>>
>>         -Aleksey.
>>
>>
>>     This discussion is getting a bit far afield, I guess, but to get
>>     back onto the topic. I followed Aleksey's advice. And wrote an
>>     implementation that tests this.  I used two separate threads to
>>     write 0L and -1L into the long field "a" but that is the only
>>     real change I made. (I already had some scaffolding code to run
>>     things on Android or desktop Java).
>>
>>     *Android: splits writes to longs into two parts.*
>>
>>     On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>     Android 4.2.2 I saw non-atomic treatment of long. The value
>>     -4294967296 (xFFFFFFFF00000000) showed up as well as 4294967295
>>     (x00000000FFFFFFFF).
>>
>>     So looks like Android does not follow the (albeit optional)
>>     advice in the Java language specification about this.
>>
>>     *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>     implementations)*
>>
>>     Of course we couldn't get this to happen on any 64-bit JVM, but
>>     we tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does
>>     NOT happen. The 32-bit JVM implementations follow the
>>     recommendation of the Java language specification.
>>
>>     An interesting curio. I wonder how many crashes in "working" Java
>>     code moved from desktop Java onto Android programmers are going
>>     to lose sleep tracking down this one.
>>
>>
>
>     Last time I tried this sort of test, a split write would be
>     observed in under a second on a true dual processor. However, with
>     only one processor available, it would typically take around 20
>     minutes. So you might have to run a very long test to have any
>     real confidence in the lack of splitting.
>
>     Mark Thornton
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/44abcadd/attachment.html>

From vitalyd at gmail.com  Tue Apr 30 12:37:20 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 12:37:20 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517FEEA9.7080403@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
Message-ID: <CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>

Curious how x86 would move a long in 1 instruction? There's no memory to
memory mov so has to go through register, and thus needs 2 registers (and
hence split).  Am I missing something?

Sent from my phone
On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  You might want to print the assembly using HotSpot (and OpenJDK?).  If
> the assembly, uses 1 instruction to do the write, then no splitting can
> ever happen (because alignment takes care of cache line splits).  If the
> assembly, uses 2 instructions to do the write, then it is only a matter of
> timing.
>
> With a single processor system, you are waiting for the thread's quantum
> to end right after the first instruction but before the second
> instruction.  This will allow the other thread to see the split write.
>
> With a dual processor system, the reader thread simply has to get a copy
> of the cache line after the first write and before the second write.  This
> is much easier to do.
>
> HotSpot will do a lot of optimizations on single processor systems.  For
> example, it gets rid of the "lock" prefix in front of atomic instructions
> since the instruction's execution can't be split.  It also doesn't output
> memory fences.  Both of these give good performance boosts.  I wonder if
> with one processor, OpenJDK is using 2 instructions to do the write whereas
> with multiple processors it plays it safe and uses 1 instruction.
>
> Note: If you disable all of the processors but 1 and then start HotSpot,
> HotSpot will start in single processor mode.  If you then enable those
> processors while HotSpot is running, a lot of things break and the JVM will
> crash.  Because single processor systems are rare, the default might be
> changed to assume multiple processors unless the command line specifies 1
> processor.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>
>  Aleksey, correct -- more trials show what you predicted. Thanks for the
> nudge.
>
>  Mark,
>
>  Very helpful, in fact, we are seeing quick failures except for the
> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
> have yet to get a failure.  The two programs attached are what I'm running.
>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
> but not on Android).  The difference is that one uses two threads (one
> writer one reader) the other three (two writers one reader) -- both seem to
> produce similar results.
>
>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
> immediatly. Dual we can't get a failure, yet, we get more failures as the
> processor count goes up -- but after a few failures, we don't get any more
> (they program tries to get 10 to happen)...we can't get to 10.
>
>  It seems that while this can happen on OpenJDK it is rarer than on
> Android where ten failures takes less than a second to happen.
>
>  Best, Tim
>
>
>
> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>
>>   On 30/04/13 15:36, Tim Halloran wrote:
>>
>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>> aleksey.shipilev at oracle.com> wrote:
>>
>>> Yes, that's exactly what I had in mind:
>>>  a. Declare "long a"
>>>  b. Ramp up two threads.
>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>  d. Make thread 2 observe the field a, and count the observed values
>>>  e. ...
>>>  f. PROFIT!
>>>
>>> P.S. It is important to do some action on value read in thread 2, so
>>> that it does not hoisted from the loop, since $a is not supposed to be
>>> volatile.
>>>
>>> -Aleksey.
>>>
>>>
>>  This discussion is getting a bit far afield, I guess, but to get back
>> onto the topic. I followed Aleksey's advice. And wrote an implementation
>> that tests this.  I used two separate threads to write 0L and -1L into the
>> long field "a" but that is the only real change I made. (I already had some
>> scaffolding code to run things on Android or desktop Java).
>>
>>  *Android: splits writes to longs into two parts.*
>>
>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with Android
>> 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>
>>  So looks like Android does not follow the (albeit optional) advice in
>> the Java language specification about this.
>>
>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>> implementations)*
>>
>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>> 32-bit JVM implementations follow the recommendation of the Java language
>> specification.
>>
>>  An interesting curio. I wonder how many crashes in "working" Java code
>> moved from desktop Java onto Android programmers are going to lose sleep
>> tracking down this one.
>>
>>
>>
>>  Last time I tried this sort of test, a split write would be observed in
>> under a second on a true dual processor. However, with only one processor
>> available, it would typically take around 20 minutes. So you might have to
>> run a very long test to have any real confidence in the lack of splitting.
>>
>> Mark Thornton
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/41a1d219/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Apr 30 12:45:45 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Apr 2013 09:45:45 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
Message-ID: <517FF539.5000303@oracle.com>

On 32-bit x86, the cmpxchg8b can be used to write a long in 1 
instruction.  This instruction has been "present on most post-80486 
processors" (Wikipedia).  There might be cheaper ways to write a long 
but there is at least 1 way.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>
> Curious how x86 would move a long in 1 instruction? There's no memory 
> to memory mov so has to go through register, and thus needs 2 
> registers (and hence split).  Am I missing something?
>
> Sent from my phone
>
> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     You might want to print the assembly using HotSpot (and
>     OpenJDK?).  If the assembly, uses 1 instruction to do the write,
>     then no splitting can ever happen (because alignment takes care of
>     cache line splits).  If the assembly, uses 2 instructions to do
>     the write, then it is only a matter of timing.
>
>     With a single processor system, you are waiting for the thread's
>     quantum to end right after the first instruction but before the
>     second instruction.  This will allow the other thread to see the
>     split write.
>
>     With a dual processor system, the reader thread simply has to get
>     a copy of the cache line after the first write and before the
>     second write.  This is much easier to do.
>
>     HotSpot will do a lot of optimizations on single processor
>     systems.  For example, it gets rid of the "lock" prefix in front
>     of atomic instructions since the instruction's execution can't be
>     split.  It also doesn't output memory fences.  Both of these give
>     good performance boosts.  I wonder if with one processor, OpenJDK
>     is using 2 instructions to do the write whereas with multiple
>     processors it plays it safe and uses 1 instruction.
>
>     Note: If you disable all of the processors but 1 and then start
>     HotSpot, HotSpot will start in single processor mode.  If you then
>     enable those processors while HotSpot is running, a lot of things
>     break and the JVM will crash. Because single processor systems are
>     rare, the default might be changed to assume multiple processors
>     unless the command line specifies 1 processor.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>     Aleksey, correct -- more trials show what you predicted. Thanks
>>     for the nudge.
>>
>>     Mark,
>>
>>     Very helpful, in fact, we are seeing quick failures except for
>>     the dual-processor case -- on a dual processor hardware or VM
>>     (Virtual Box) we have yet to get a failure.  The two programs
>>     attached are what I'm running.  I stripped out my benchmark
>>     framework (so they are easy to run on OpenJDK but not on
>>     Android).  The difference is that one uses two threads (one
>>     writer one reader) the other three (two writers one reader) --
>>     both seem to produce similar results.
>>
>>     With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>     immediatly. Dual we can't get a failure, yet, we get more
>>     failures as the processor count goes up -- but after a few
>>     failures, we don't get any more (they program tries to get 10 to
>>     happen)...we can't get to 10.
>>
>>     It seems that while this can happen on OpenJDK it is rarer than
>>     on Android where ten failures takes less than a second to happen.
>>
>>     Best, Tim
>>
>>
>>
>>     On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton
>>     <mthornton at optrak.com <mailto:mthornton at optrak.com>> wrote:
>>
>>         On 30/04/13 15:36, Tim Halloran wrote:
>>>         On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>>         <aleksey.shipilev at oracle.com
>>>         <mailto:aleksey.shipilev at oracle.com>> wrote:
>>>
>>>             Yes, that's exactly what I had in mind:
>>>              a. Declare "long a"
>>>              b. Ramp up two threads.
>>>              c. Make thread 1 write 0L and -1L over and over to field $a
>>>              d. Make thread 2 observe the field a, and count the
>>>             observed values
>>>              e. ...
>>>              f. PROFIT!
>>>
>>>             P.S. It is important to do some action on value read in
>>>             thread 2, so
>>>             that it does not hoisted from the loop, since $a is not
>>>             supposed to be
>>>             volatile.
>>>
>>>             -Aleksey.
>>>
>>>
>>>         This discussion is getting a bit far afield, I guess, but to
>>>         get back onto the topic. I followed Aleksey's advice. And
>>>         wrote an implementation that tests this.  I used two
>>>         separate threads to write 0L and -1L into the long field "a"
>>>         but that is the only real change I made. (I already had some
>>>         scaffolding code to run things on Android or desktop Java).
>>>
>>>         *Android: splits writes to longs into two parts.*
>>>
>>>         On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone
>>>         with Android 4.2.2 I saw non-atomic treatment of long. The
>>>         value -4294967296 (xFFFFFFFF00000000) showed up as well as
>>>         4294967295 (x00000000FFFFFFFF).
>>>
>>>         So looks like Android does not follow the (albeit optional)
>>>         advice in the Java language specification about this.
>>>
>>>         *JDK: DOES NOT split writes to longs into two parts (even
>>>         32-bit implementations)*
>>>
>>>         Of course we couldn't get this to happen on any 64-bit JVM,
>>>         but we tried it out under Linux on 32-bit OpenJDK 1.7.0_21
>>>         it does NOT happen. The 32-bit JVM implementations follow
>>>         the recommendation of the Java language specification.
>>>
>>>         An interesting curio. I wonder how many crashes in "working"
>>>         Java code moved from desktop Java onto Android programmers
>>>         are going to lose sleep tracking down this one.
>>>
>>>
>>
>>         Last time I tried this sort of test, a split write would be
>>         observed in under a second on a true dual processor. However,
>>         with only one processor available, it would typically take
>>         around 20 minutes. So you might have to run a very long test
>>         to have any real confidence in the lack of splitting.
>>
>>         Mark Thornton
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/84233090/attachment.html>

From vitalyd at gmail.com  Tue Apr 30 12:53:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 12:53:00 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517FF539.5000303@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
Message-ID: <CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>

But this requires the src value to be in ecx:ebx so how would you load it
there without two loads (and possibly observe tearing) in the first place?

Sent from my phone
On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
> instruction.  This instruction has been "present on most post-80486
> processors" (Wikipedia).  There might be cheaper ways to write a long but
> there is at least 1 way.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>
> Curious how x86 would move a long in 1 instruction? There's no memory to
> memory mov so has to go through register, and thus needs 2 registers (and
> hence split).  Am I missing something?
>
> Sent from my phone
> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  You might want to print the assembly using HotSpot (and OpenJDK?).  If
>> the assembly, uses 1 instruction to do the write, then no splitting can
>> ever happen (because alignment takes care of cache line splits).  If the
>> assembly, uses 2 instructions to do the write, then it is only a matter of
>> timing.
>>
>> With a single processor system, you are waiting for the thread's quantum
>> to end right after the first instruction but before the second
>> instruction.  This will allow the other thread to see the split write.
>>
>> With a dual processor system, the reader thread simply has to get a copy
>> of the cache line after the first write and before the second write.  This
>> is much easier to do.
>>
>> HotSpot will do a lot of optimizations on single processor systems.  For
>> example, it gets rid of the "lock" prefix in front of atomic instructions
>> since the instruction's execution can't be split.  It also doesn't output
>> memory fences.  Both of these give good performance boosts.  I wonder if
>> with one processor, OpenJDK is using 2 instructions to do the write whereas
>> with multiple processors it plays it safe and uses 1 instruction.
>>
>> Note: If you disable all of the processors but 1 and then start HotSpot,
>> HotSpot will start in single processor mode.  If you then enable those
>> processors while HotSpot is running, a lot of things break and the JVM will
>> crash.  Because single processor systems are rare, the default might be
>> changed to assume multiple processors unless the command line specifies 1
>> processor.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>
>>  Aleksey, correct -- more trials show what you predicted. Thanks for the
>> nudge.
>>
>>  Mark,
>>
>>  Very helpful, in fact, we are seeing quick failures except for the
>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>> have yet to get a failure.  The two programs attached are what I'm running.
>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>> but not on Android).  The difference is that one uses two threads (one
>> writer one reader) the other three (two writers one reader) -- both seem to
>> produce similar results.
>>
>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>> processor count goes up -- but after a few failures, we don't get any more
>> (they program tries to get 10 to happen)...we can't get to 10.
>>
>>  It seems that while this can happen on OpenJDK it is rarer than on
>> Android where ten failures takes less than a second to happen.
>>
>>  Best, Tim
>>
>>
>>
>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>>
>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>
>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>> aleksey.shipilev at oracle.com> wrote:
>>>
>>>> Yes, that's exactly what I had in mind:
>>>>  a. Declare "long a"
>>>>  b. Ramp up two threads.
>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>  e. ...
>>>>  f. PROFIT!
>>>>
>>>> P.S. It is important to do some action on value read in thread 2, so
>>>> that it does not hoisted from the loop, since $a is not supposed to be
>>>> volatile.
>>>>
>>>> -Aleksey.
>>>>
>>>>
>>>  This discussion is getting a bit far afield, I guess, but to get back
>>> onto the topic. I followed Aleksey's advice. And wrote an implementation
>>> that tests this.  I used two separate threads to write 0L and -1L into the
>>> long field "a" but that is the only real change I made. (I already had some
>>> scaffolding code to run things on Android or desktop Java).
>>>
>>>  *Android: splits writes to longs into two parts.*
>>>
>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>
>>>  So looks like Android does not follow the (albeit optional) advice in
>>> the Java language specification about this.
>>>
>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>> implementations)*
>>>
>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>> 32-bit JVM implementations follow the recommendation of the Java language
>>> specification.
>>>
>>>  An interesting curio. I wonder how many crashes in "working" Java code
>>> moved from desktop Java onto Android programmers are going to lose sleep
>>> tracking down this one.
>>>
>>>
>>>
>>>  Last time I tried this sort of test, a split write would be observed
>>> in under a second on a true dual processor. However, with only one
>>> processor available, it would typically take around 20 minutes. So you
>>> might have to run a very long test to have any real confidence in the lack
>>> of splitting.
>>>
>>> Mark Thornton
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/281f0c90/attachment-0001.html>

From stanimir at riflexo.com  Tue Apr 30 12:57:26 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 30 Apr 2013 19:57:26 +0300
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
Message-ID: <CAEJX8opd=r809qHk3_Nv+295S7L7ZfvSj6M02dvr=HTLahLYxw@mail.gmail.com>

On Tue, Apr 30, 2013 at 7:37 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Curious how x86 would move a long in 1 instruction? There's no memory to
> memory mov so has to go through register, and thus needs 2 registers (and
> hence split).  Am I missing something?
>
> It uses ab SSE instruction, they are wider.

Stanimir

> Sent from my phone
> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  You might want to print the assembly using HotSpot (and OpenJDK?).  If
>> the assembly, uses 1 instruction to do the write, then no splitting can
>> ever happen (because alignment takes care of cache line splits).  If the
>> assembly, uses 2 instructions to do the write, then it is only a matter of
>> timing.
>>
>> With a single processor system, you are waiting for the thread's quantum
>> to end right after the first instruction but before the second
>> instruction.  This will allow the other thread to see the split write.
>>
>> With a dual processor system, the reader thread simply has to get a copy
>> of the cache line after the first write and before the second write.  This
>> is much easier to do.
>>
>> HotSpot will do a lot of optimizations on single processor systems.  For
>> example, it gets rid of the "lock" prefix in front of atomic instructions
>> since the instruction's execution can't be split.  It also doesn't output
>> memory fences.  Both of these give good performance boosts.  I wonder if
>> with one processor, OpenJDK is using 2 instructions to do the write whereas
>> with multiple processors it plays it safe and uses 1 instruction.
>>
>> Note: If you disable all of the processors but 1 and then start HotSpot,
>> HotSpot will start in single processor mode.  If you then enable those
>> processors while HotSpot is running, a lot of things break and the JVM will
>> crash.  Because single processor systems are rare, the default might be
>> changed to assume multiple processors unless the command line specifies 1
>> processor.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>
>>  Aleksey, correct -- more trials show what you predicted. Thanks for the
>> nudge.
>>
>>  Mark,
>>
>>  Very helpful, in fact, we are seeing quick failures except for the
>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>> have yet to get a failure.  The two programs attached are what I'm running.
>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>> but not on Android).  The difference is that one uses two threads (one
>> writer one reader) the other three (two writers one reader) -- both seem to
>> produce similar results.
>>
>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>> processor count goes up -- but after a few failures, we don't get any more
>> (they program tries to get 10 to happen)...we can't get to 10.
>>
>>  It seems that while this can happen on OpenJDK it is rarer than on
>> Android where ten failures takes less than a second to happen.
>>
>>  Best, Tim
>>
>>
>>
>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>>
>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>
>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>> aleksey.shipilev at oracle.com> wrote:
>>>
>>>> Yes, that's exactly what I had in mind:
>>>>  a. Declare "long a"
>>>>  b. Ramp up two threads.
>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>  e. ...
>>>>  f. PROFIT!
>>>>
>>>> P.S. It is important to do some action on value read in thread 2, so
>>>> that it does not hoisted from the loop, since $a is not supposed to be
>>>> volatile.
>>>>
>>>> -Aleksey.
>>>>
>>>>
>>>  This discussion is getting a bit far afield, I guess, but to get back
>>> onto the topic. I followed Aleksey's advice. And wrote an implementation
>>> that tests this.  I used two separate threads to write 0L and -1L into the
>>> long field "a" but that is the only real change I made. (I already had some
>>> scaffolding code to run things on Android or desktop Java).
>>>
>>>  *Android: splits writes to longs into two parts.*
>>>
>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>
>>>  So looks like Android does not follow the (albeit optional) advice in
>>> the Java language specification about this.
>>>
>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>> implementations)*
>>>
>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>> 32-bit JVM implementations follow the recommendation of the Java language
>>> specification.
>>>
>>>  An interesting curio. I wonder how many crashes in "working" Java code
>>> moved from desktop Java onto Android programmers are going to lose sleep
>>> tracking down this one.
>>>
>>>
>>>
>>>  Last time I tried this sort of test, a split write would be observed
>>> in under a second on a true dual processor. However, with only one
>>> processor available, it would typically take around 20 minutes. So you
>>> might have to run a very long test to have any real confidence in the lack
>>> of splitting.
>>>
>>> Mark Thornton
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/2312fcb2/attachment.html>

From nathan.reynolds at oracle.com  Tue Apr 30 12:58:16 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Apr 2013 09:58:16 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
Message-ID: <517FF828.20103@oracle.com>

The processor can do whatever it wants in registers without other 
threads being able to see intermediate values.  Registers are private to 
the hardware thread.  So, we can use multiple instructions to load the 
ecx:ebx registers and then execute the cmpxchg8b to do a single write to 
globally visible cache.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>
> But this requires the src value to be in ecx:ebx so how would you load 
> it there without two loads (and possibly observe tearing) in the first 
> place?
>
> Sent from my phone
>
> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>     instruction. This instruction has been "present on most post-80486
>     processors" (Wikipedia).  There might be cheaper ways to write a
>     long but there is at least 1 way.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>
>>     Curious how x86 would move a long in 1 instruction? There's no
>>     memory to memory mov so has to go through register, and thus
>>     needs 2 registers (and hence split).  Am I missing something?
>>
>>     Sent from my phone
>>
>>     On Apr 30, 2013 12:23 PM, "Nathan Reynolds"
>>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>>     wrote:
>>
>>         You might want to print the assembly using HotSpot (and
>>         OpenJDK?).  If the assembly, uses 1 instruction to do the
>>         write, then no splitting can ever happen (because alignment
>>         takes care of cache line splits).  If the assembly, uses 2
>>         instructions to do the write, then it is only a matter of timing.
>>
>>         With a single processor system, you are waiting for the
>>         thread's quantum to end right after the first instruction but
>>         before the second instruction.  This will allow the other
>>         thread to see the split write.
>>
>>         With a dual processor system, the reader thread simply has to
>>         get a copy of the cache line after the first write and before
>>         the second write.  This is much easier to do.
>>
>>         HotSpot will do a lot of optimizations on single processor
>>         systems.  For example, it gets rid of the "lock" prefix in
>>         front of atomic instructions since the instruction's
>>         execution can't be split. It also doesn't output memory
>>         fences.  Both of these give good performance boosts.  I
>>         wonder if with one processor, OpenJDK is using 2 instructions
>>         to do the write whereas with multiple processors it plays it
>>         safe and uses 1 instruction.
>>
>>         Note: If you disable all of the processors but 1 and then
>>         start HotSpot, HotSpot will start in single processor mode. 
>>         If you then enable those processors while HotSpot is running,
>>         a lot of things break and the JVM will crash.  Because single
>>         processor systems are rare, the default might be changed to
>>         assume multiple processors unless the command line specifies
>>         1 processor.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>         Architect | 602.333.9091 <tel:602.333.9091>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>         Technology
>>         On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>         Aleksey, correct -- more trials show what you predicted.
>>>         Thanks for the nudge.
>>>
>>>         Mark,
>>>
>>>         Very helpful, in fact, we are seeing quick failures except
>>>         for the dual-processor case -- on a dual processor hardware
>>>         or VM (Virtual Box) we have yet to get a failure.  The two
>>>         programs attached are what I'm running.  I stripped out my
>>>         benchmark framework (so they are easy to run on OpenJDK but
>>>         not on Android).  The difference is that one uses two
>>>         threads (one writer one reader) the other three (two writers
>>>         one reader) -- both seem to produce similar results.
>>>
>>>         With one processor, OpenJDK 1.6.0_27 I see the split write
>>>         almost immediatly. Dual we can't get a failure, yet, we get
>>>         more failures as the processor count goes up -- but after a
>>>         few failures, we don't get any more (they program tries to
>>>         get 10 to happen)...we can't get to 10.
>>>
>>>         It seems that while this can happen on OpenJDK it is rarer
>>>         than on Android where ten failures takes less than a second
>>>         to happen.
>>>
>>>         Best, Tim
>>>
>>>
>>>
>>>         On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton
>>>         <mthornton at optrak.com <mailto:mthornton at optrak.com>> wrote:
>>>
>>>             On 30/04/13 15:36, Tim Halloran wrote:
>>>>             On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>>>             <aleksey.shipilev at oracle.com
>>>>             <mailto:aleksey.shipilev at oracle.com>> wrote:
>>>>
>>>>                 Yes, that's exactly what I had in mind:
>>>>                  a. Declare "long a"
>>>>                  b. Ramp up two threads.
>>>>                  c. Make thread 1 write 0L and -1L over and over to
>>>>                 field $a
>>>>                  d. Make thread 2 observe the field a, and count
>>>>                 the observed values
>>>>                  e. ...
>>>>                  f. PROFIT!
>>>>
>>>>                 P.S. It is important to do some action on value
>>>>                 read in thread 2, so
>>>>                 that it does not hoisted from the loop, since $a is
>>>>                 not supposed to be
>>>>                 volatile.
>>>>
>>>>                 -Aleksey.
>>>>
>>>>
>>>>             This discussion is getting a bit far afield, I guess,
>>>>             but to get back onto the topic. I followed Aleksey's
>>>>             advice. And wrote an implementation that tests this.  I
>>>>             used two separate threads to write 0L and -1L into the
>>>>             long field "a" but that is the only real change I made.
>>>>             (I already had some scaffolding code to run things on
>>>>             Android or desktop Java).
>>>>
>>>>             *Android: splits writes to longs into two parts.*
>>>>
>>>>             On a Samsung Galaxy II with Android 4.0.4  a Nexus 4
>>>>             phone with Android 4.2.2 I saw non-atomic treatment of
>>>>             long. The value -4294967296 (xFFFFFFFF00000000) showed
>>>>             up as well as 4294967295 (x00000000FFFFFFFF).
>>>>
>>>>             So looks like Android does not follow the (albeit
>>>>             optional) advice in the Java language specification
>>>>             about this.
>>>>
>>>>             *JDK: DOES NOT split writes to longs into two parts
>>>>             (even 32-bit implementations)*
>>>>
>>>>             Of course we couldn't get this to happen on any 64-bit
>>>>             JVM, but we tried it out under Linux on 32-bit OpenJDK
>>>>             1.7.0_21 it does NOT happen. The 32-bit JVM
>>>>             implementations follow the recommendation of the Java
>>>>             language specification.
>>>>
>>>>             An interesting curio. I wonder how many crashes in
>>>>             "working" Java code moved from desktop Java onto
>>>>             Android programmers are going to lose sleep tracking
>>>>             down this one.
>>>>
>>>>
>>>
>>>             Last time I tried this sort of test, a split write would
>>>             be observed in under a second on a true dual processor.
>>>             However, with only one processor available, it would
>>>             typically take around 20 minutes. So you might have to
>>>             run a very long test to have any real confidence in the
>>>             lack of splitting.
>>>
>>>             Mark Thornton
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/986d1ca6/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Apr 30 12:59:42 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Apr 2013 09:59:42 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAEJX8opd=r809qHk3_Nv+295S7L7ZfvSj6M02dvr=HTLahLYxw@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<CAEJX8opd=r809qHk3_Nv+295S7L7ZfvSj6M02dvr=HTLahLYxw@mail.gmail.com>
Message-ID: <517FF87E.7070909@oracle.com>

Thanks for reminding me of SSE.  SSE would be much faster than cmpxchg8b.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/30/2013 9:57 AM, Stanimir Simeonoff wrote:
>
>
> On Tue, Apr 30, 2013 at 7:37 PM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     Curious how x86 would move a long in 1 instruction? There's no
>     memory to memory mov so has to go through register, and thus needs
>     2 registers (and hence split).  Am I missing something?
>
> It uses ab SSE instruction, they are wider.
>
> Stanimir
>
>     Sent from my phone
>
>     On Apr 30, 2013 12:23 PM, "Nathan Reynolds"
>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>     wrote:
>
>         You might want to print the assembly using HotSpot (and
>         OpenJDK?).  If the assembly, uses 1 instruction to do the
>         write, then no splitting can ever happen (because alignment
>         takes care of cache line splits).  If the assembly, uses 2
>         instructions to do the write, then it is only a matter of timing.
>
>         With a single processor system, you are waiting for the
>         thread's quantum to end right after the first instruction but
>         before the second instruction.  This will allow the other
>         thread to see the split write.
>
>         With a dual processor system, the reader thread simply has to
>         get a copy of the cache line after the first write and before
>         the second write.  This is much easier to do.
>
>         HotSpot will do a lot of optimizations on single processor
>         systems.  For example, it gets rid of the "lock" prefix in
>         front of atomic instructions since the instruction's execution
>         can't be split. It also doesn't output memory fences.  Both of
>         these give good performance boosts.  I wonder if with one
>         processor, OpenJDK is using 2 instructions to do the write
>         whereas with multiple processors it plays it safe and uses 1
>         instruction.
>
>         Note: If you disable all of the processors but 1 and then
>         start HotSpot, HotSpot will start in single processor mode. 
>         If you then enable those processors while HotSpot is running,
>         a lot of things break and the JVM will crash.  Because single
>         processor systems are rare, the default might be changed to
>         assume multiple processors unless the command line specifies 1
>         processor.
>
>         Nathan Reynolds
>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>         | Architect | 602.333.9091 <tel:602.333.9091>
>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>         Technology
>         On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>         Aleksey, correct -- more trials show what you predicted.
>>         Thanks for the nudge.
>>
>>         Mark,
>>
>>         Very helpful, in fact, we are seeing quick failures except
>>         for the dual-processor case -- on a dual processor hardware
>>         or VM (Virtual Box) we have yet to get a failure.  The two
>>         programs attached are what I'm running.  I stripped out my
>>         benchmark framework (so they are easy to run on OpenJDK but
>>         not on Android).  The difference is that one uses two threads
>>         (one writer one reader) the other three (two writers one
>>         reader) -- both seem to produce similar results.
>>
>>         With one processor, OpenJDK 1.6.0_27 I see the split write
>>         almost immediatly. Dual we can't get a failure, yet, we get
>>         more failures as the processor count goes up -- but after a
>>         few failures, we don't get any more (they program tries to
>>         get 10 to happen)...we can't get to 10.
>>
>>         It seems that while this can happen on OpenJDK it is rarer
>>         than on Android where ten failures takes less than a second
>>         to happen.
>>
>>         Best, Tim
>>
>>
>>
>>         On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton
>>         <mthornton at optrak.com <mailto:mthornton at optrak.com>> wrote:
>>
>>             On 30/04/13 15:36, Tim Halloran wrote:
>>>             On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>>             <aleksey.shipilev at oracle.com
>>>             <mailto:aleksey.shipilev at oracle.com>> wrote:
>>>
>>>                 Yes, that's exactly what I had in mind:
>>>                  a. Declare "long a"
>>>                  b. Ramp up two threads.
>>>                  c. Make thread 1 write 0L and -1L over and over to
>>>                 field $a
>>>                  d. Make thread 2 observe the field a, and count the
>>>                 observed values
>>>                  e. ...
>>>                  f. PROFIT!
>>>
>>>                 P.S. It is important to do some action on value read
>>>                 in thread 2, so
>>>                 that it does not hoisted from the loop, since $a is
>>>                 not supposed to be
>>>                 volatile.
>>>
>>>                 -Aleksey.
>>>
>>>
>>>             This discussion is getting a bit far afield, I guess,
>>>             but to get back onto the topic. I followed Aleksey's
>>>             advice. And wrote an implementation that tests this.  I
>>>             used two separate threads to write 0L and -1L into the
>>>             long field "a" but that is the only real change I made.
>>>             (I already had some scaffolding code to run things on
>>>             Android or desktop Java).
>>>
>>>             *Android: splits writes to longs into two parts.*
>>>
>>>             On a Samsung Galaxy II with Android 4.0.4  a Nexus 4
>>>             phone with Android 4.2.2 I saw non-atomic treatment of
>>>             long. The value -4294967296 (xFFFFFFFF00000000) showed
>>>             up as well as 4294967295 (x00000000FFFFFFFF).
>>>
>>>             So looks like Android does not follow the (albeit
>>>             optional) advice in the Java language specification
>>>             about this.
>>>
>>>             *JDK: DOES NOT split writes to longs into two parts
>>>             (even 32-bit implementations)*
>>>
>>>             Of course we couldn't get this to happen on any 64-bit
>>>             JVM, but we tried it out under Linux on 32-bit OpenJDK
>>>             1.7.0_21 it does NOT happen. The 32-bit JVM
>>>             implementations follow the recommendation of the Java
>>>             language specification.
>>>
>>>             An interesting curio. I wonder how many crashes in
>>>             "working" Java code moved from desktop Java onto Android
>>>             programmers are going to lose sleep tracking down this one.
>>>
>>>
>>
>>             Last time I tried this sort of test, a split write would
>>             be observed in under a second on a true dual processor.
>>             However, with only one processor available, it would
>>             typically take around 20 minutes. So you might have to
>>             run a very long test to have any real confidence in the
>>             lack of splitting.
>>
>>             Mark Thornton
>>
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/0c4b1817/attachment-0001.html>

From vitalyd at gmail.com  Tue Apr 30 13:05:03 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 13:05:03 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <517FF828.20103@oracle.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
Message-ID: <CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>

Right, writes would be atomic but not reads; read one half, another core
updates the value, read 2nd half from different value now.

As for SSE, yeah it's possible, but is that true? JIT skips integer
registers for scalar long operations? I find that hard to believe as it
would miss out on large register file/renaming opportunities.

Sent from my phone
On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  The processor can do whatever it wants in registers without other threads
> being able to see intermediate values.  Registers are private to the
> hardware thread.  So, we can use multiple instructions to load the ecx:ebx
> registers and then execute the cmpxchg8b to do a single write to globally
> visible cache.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>
> But this requires the src value to be in ecx:ebx so how would you load it
> there without two loads (and possibly observe tearing) in the first place?
>
> Sent from my phone
> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>> instruction.  This instruction has been "present on most post-80486
>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>> there is at least 1 way.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>
>> Curious how x86 would move a long in 1 instruction? There's no memory to
>> memory mov so has to go through register, and thus needs 2 registers (and
>> hence split).  Am I missing something?
>>
>> Sent from my phone
>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> wrote:
>>
>>>  You might want to print the assembly using HotSpot (and OpenJDK?).  If
>>> the assembly, uses 1 instruction to do the write, then no splitting can
>>> ever happen (because alignment takes care of cache line splits).  If the
>>> assembly, uses 2 instructions to do the write, then it is only a matter of
>>> timing.
>>>
>>> With a single processor system, you are waiting for the thread's quantum
>>> to end right after the first instruction but before the second
>>> instruction.  This will allow the other thread to see the split write.
>>>
>>> With a dual processor system, the reader thread simply has to get a copy
>>> of the cache line after the first write and before the second write.  This
>>> is much easier to do.
>>>
>>> HotSpot will do a lot of optimizations on single processor systems.  For
>>> example, it gets rid of the "lock" prefix in front of atomic instructions
>>> since the instruction's execution can't be split.  It also doesn't output
>>> memory fences.  Both of these give good performance boosts.  I wonder if
>>> with one processor, OpenJDK is using 2 instructions to do the write whereas
>>> with multiple processors it plays it safe and uses 1 instruction.
>>>
>>> Note: If you disable all of the processors but 1 and then start HotSpot,
>>> HotSpot will start in single processor mode.  If you then enable those
>>> processors while HotSpot is running, a lot of things break and the JVM will
>>> crash.  Because single processor systems are rare, the default might be
>>> changed to assume multiple processors unless the command line specifies 1
>>> processor.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>
>>>  Aleksey, correct -- more trials show what you predicted. Thanks for
>>> the nudge.
>>>
>>>  Mark,
>>>
>>>  Very helpful, in fact, we are seeing quick failures except for the
>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>> but not on Android).  The difference is that one uses two threads (one
>>> writer one reader) the other three (two writers one reader) -- both seem to
>>> produce similar results.
>>>
>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>> processor count goes up -- but after a few failures, we don't get any more
>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>
>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>> Android where ten failures takes less than a second to happen.
>>>
>>>  Best, Tim
>>>
>>>
>>>
>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>>>
>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>
>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>> aleksey.shipilev at oracle.com> wrote:
>>>>
>>>>> Yes, that's exactly what I had in mind:
>>>>>  a. Declare "long a"
>>>>>  b. Ramp up two threads.
>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>>  e. ...
>>>>>  f. PROFIT!
>>>>>
>>>>> P.S. It is important to do some action on value read in thread 2, so
>>>>> that it does not hoisted from the loop, since $a is not supposed to be
>>>>> volatile.
>>>>>
>>>>> -Aleksey.
>>>>>
>>>>>
>>>>  This discussion is getting a bit far afield, I guess, but to get back
>>>> onto the topic. I followed Aleksey's advice. And wrote an implementation
>>>> that tests this.  I used two separate threads to write 0L and -1L into the
>>>> long field "a" but that is the only real change I made. (I already had some
>>>> scaffolding code to run things on Android or desktop Java).
>>>>
>>>>  *Android: splits writes to longs into two parts.*
>>>>
>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>
>>>>  So looks like Android does not follow the (albeit optional) advice in
>>>> the Java language specification about this.
>>>>
>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>> implementations)*
>>>>
>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>> 32-bit JVM implementations follow the recommendation of the Java language
>>>> specification.
>>>>
>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>> code moved from desktop Java onto Android programmers are going to lose
>>>> sleep tracking down this one.
>>>>
>>>>
>>>>
>>>>  Last time I tried this sort of test, a split write would be observed
>>>> in under a second on a true dual processor. However, with only one
>>>> processor available, it would typically take around 20 minutes. So you
>>>> might have to run a very long test to have any real confidence in the lack
>>>> of splitting.
>>>>
>>>> Mark Thornton
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/2cb89b9f/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Apr 30 13:05:46 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 30 Apr 2013 18:05:46 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
Message-ID: <517FF9EA.1000603@oracle.com>

Coprocessor, XMM and others.

Alex

On 30/04/2013 17:37, Vitaly Davidovich wrote:
>
> Curious how x86 would move a long in 1 instruction? There's no memory 
> to memory mov so has to go through register, and thus needs 2 
> registers (and hence split).  Am I missing something?
>
> Sent from my phone
>
> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     You might want to print the assembly using HotSpot (and
>     OpenJDK?).  If the assembly, uses 1 instruction to do the write,
>     then no splitting can ever happen (because alignment takes care of
>     cache line splits).  If the assembly, uses 2 instructions to do
>     the write, then it is only a matter of timing.
>
>     With a single processor system, you are waiting for the thread's
>     quantum to end right after the first instruction but before the
>     second instruction.  This will allow the other thread to see the
>     split write.
>
>     With a dual processor system, the reader thread simply has to get
>     a copy of the cache line after the first write and before the
>     second write.  This is much easier to do.
>
>     HotSpot will do a lot of optimizations on single processor
>     systems.  For example, it gets rid of the "lock" prefix in front
>     of atomic instructions since the instruction's execution can't be
>     split.  It also doesn't output memory fences.  Both of these give
>     good performance boosts.  I wonder if with one processor, OpenJDK
>     is using 2 instructions to do the write whereas with multiple
>     processors it plays it safe and uses 1 instruction.
>
>     Note: If you disable all of the processors but 1 and then start
>     HotSpot, HotSpot will start in single processor mode.  If you then
>     enable those processors while HotSpot is running, a lot of things
>     break and the JVM will crash. Because single processor systems are
>     rare, the default might be changed to assume multiple processors
>     unless the command line specifies 1 processor.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>     Aleksey, correct -- more trials show what you predicted. Thanks
>>     for the nudge.
>>
>>     Mark,
>>
>>     Very helpful, in fact, we are seeing quick failures except for
>>     the dual-processor case -- on a dual processor hardware or VM
>>     (Virtual Box) we have yet to get a failure.  The two programs
>>     attached are what I'm running.  I stripped out my benchmark
>>     framework (so they are easy to run on OpenJDK but not on
>>     Android).  The difference is that one uses two threads (one
>>     writer one reader) the other three (two writers one reader) --
>>     both seem to produce similar results.
>>
>>     With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>     immediatly. Dual we can't get a failure, yet, we get more
>>     failures as the processor count goes up -- but after a few
>>     failures, we don't get any more (they program tries to get 10 to
>>     happen)...we can't get to 10.
>>
>>     It seems that while this can happen on OpenJDK it is rarer than
>>     on Android where ten failures takes less than a second to happen.
>>
>>     Best, Tim
>>
>>
>>
>>     On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton
>>     <mthornton at optrak.com <mailto:mthornton at optrak.com>> wrote:
>>
>>         On 30/04/13 15:36, Tim Halloran wrote:
>>>         On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>>         <aleksey.shipilev at oracle.com
>>>         <mailto:aleksey.shipilev at oracle.com>> wrote:
>>>
>>>             Yes, that's exactly what I had in mind:
>>>              a. Declare "long a"
>>>              b. Ramp up two threads.
>>>              c. Make thread 1 write 0L and -1L over and over to field $a
>>>              d. Make thread 2 observe the field a, and count the
>>>             observed values
>>>              e. ...
>>>              f. PROFIT!
>>>
>>>             P.S. It is important to do some action on value read in
>>>             thread 2, so
>>>             that it does not hoisted from the loop, since $a is not
>>>             supposed to be
>>>             volatile.
>>>
>>>             -Aleksey.
>>>
>>>
>>>         This discussion is getting a bit far afield, I guess, but to
>>>         get back onto the topic. I followed Aleksey's advice. And
>>>         wrote an implementation that tests this.  I used two
>>>         separate threads to write 0L and -1L into the long field "a"
>>>         but that is the only real change I made. (I already had some
>>>         scaffolding code to run things on Android or desktop Java).
>>>
>>>         *Android: splits writes to longs into two parts.*
>>>
>>>         On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone
>>>         with Android 4.2.2 I saw non-atomic treatment of long. The
>>>         value -4294967296 (xFFFFFFFF00000000) showed up as well as
>>>         4294967295 (x00000000FFFFFFFF).
>>>
>>>         So looks like Android does not follow the (albeit optional)
>>>         advice in the Java language specification about this.
>>>
>>>         *JDK: DOES NOT split writes to longs into two parts (even
>>>         32-bit implementations)*
>>>
>>>         Of course we couldn't get this to happen on any 64-bit JVM,
>>>         but we tried it out under Linux on 32-bit OpenJDK 1.7.0_21
>>>         it does NOT happen. The 32-bit JVM implementations follow
>>>         the recommendation of the Java language specification.
>>>
>>>         An interesting curio. I wonder how many crashes in "working"
>>>         Java code moved from desktop Java onto Android programmers
>>>         are going to lose sleep tracking down this one.
>>>
>>>
>>
>>         Last time I tried this sort of test, a split write would be
>>         observed in under a second on a true dual processor. However,
>>         with only one processor available, it would typically take
>>         around 20 minutes. So you might have to run a very long test
>>         to have any real confidence in the lack of splitting.
>>
>>         Mark Thornton
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/7266e600/attachment.html>

From stanimir at riflexo.com  Tue Apr 30 13:15:06 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 30 Apr 2013 20:15:06 +0300
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
Message-ID: <CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>

.

> As for SSE, yeah it's possible, but is that true? JIT skips integer
> registers for scalar long operations? I find that hard to believe as it
> would miss out on large register file/renaming opportunities.
>
> I know that by looking at the assembly. I can still check w/ the current
version.

Stanimir



> Sent from my phone
> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  The processor can do whatever it wants in registers without other
>> threads being able to see intermediate values.  Registers are private to
>> the hardware thread.  So, we can use multiple instructions to load the
>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>> globally visible cache.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>
>> But this requires the src value to be in ecx:ebx so how would you load it
>> there without two loads (and possibly observe tearing) in the first place?
>>
>> Sent from my phone
>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> wrote:
>>
>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>> instruction.  This instruction has been "present on most post-80486
>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>> there is at least 1 way.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>
>>> Curious how x86 would move a long in 1 instruction? There's no memory to
>>> memory mov so has to go through register, and thus needs 2 registers (and
>>> hence split).  Am I missing something?
>>>
>>> Sent from my phone
>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>> wrote:
>>>
>>>>  You might want to print the assembly using HotSpot (and OpenJDK?).
>>>> If the assembly, uses 1 instruction to do the write, then no splitting can
>>>> ever happen (because alignment takes care of cache line splits).  If the
>>>> assembly, uses 2 instructions to do the write, then it is only a matter of
>>>> timing.
>>>>
>>>> With a single processor system, you are waiting for the thread's
>>>> quantum to end right after the first instruction but before the second
>>>> instruction.  This will allow the other thread to see the split write.
>>>>
>>>> With a dual processor system, the reader thread simply has to get a
>>>> copy of the cache line after the first write and before the second write.
>>>> This is much easier to do.
>>>>
>>>> HotSpot will do a lot of optimizations on single processor systems.
>>>> For example, it gets rid of the "lock" prefix in front of atomic
>>>> instructions since the instruction's execution can't be split.  It also
>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>> write whereas with multiple processors it plays it safe and uses 1
>>>> instruction.
>>>>
>>>> Note: If you disable all of the processors but 1 and then start
>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>> those processors while HotSpot is running, a lot of things break and the
>>>> JVM will crash.  Because single processor systems are rare, the default
>>>> might be changed to assume multiple processors unless the command line
>>>> specifies 1 processor.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>
>>>>  Aleksey, correct -- more trials show what you predicted. Thanks for
>>>> the nudge.
>>>>
>>>>  Mark,
>>>>
>>>>  Very helpful, in fact, we are seeing quick failures except for the
>>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>>> but not on Android).  The difference is that one uses two threads (one
>>>> writer one reader) the other three (two writers one reader) -- both seem to
>>>> produce similar results.
>>>>
>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>> processor count goes up -- but after a few failures, we don't get any more
>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>
>>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>>> Android where ten failures takes less than a second to happen.
>>>>
>>>>  Best, Tim
>>>>
>>>>
>>>>
>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>>>>
>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>
>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>
>>>>>> Yes, that's exactly what I had in mind:
>>>>>>  a. Declare "long a"
>>>>>>  b. Ramp up two threads.
>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>>>  e. ...
>>>>>>  f. PROFIT!
>>>>>>
>>>>>> P.S. It is important to do some action on value read in thread 2, so
>>>>>> that it does not hoisted from the loop, since $a is not supposed to be
>>>>>> volatile.
>>>>>>
>>>>>> -Aleksey.
>>>>>>
>>>>>>
>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>
>>>>>  *Android: splits writes to longs into two parts.*
>>>>>
>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>
>>>>>  So looks like Android does not follow the (albeit optional) advice
>>>>> in the Java language specification about this.
>>>>>
>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>> implementations)*
>>>>>
>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>>> 32-bit JVM implementations follow the recommendation of the Java language
>>>>> specification.
>>>>>
>>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>>> code moved from desktop Java onto Android programmers are going to lose
>>>>> sleep tracking down this one.
>>>>>
>>>>>
>>>>>
>>>>>  Last time I tried this sort of test, a split write would be observed
>>>>> in under a second on a true dual processor. However, with only one
>>>>> processor available, it would typically take around 20 minutes. So you
>>>>> might have to run a very long test to have any real confidence in the lack
>>>>> of splitting.
>>>>>
>>>>> Mark Thornton
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/5e2d6c26/attachment-0001.html>

From stanimir at riflexo.com  Tue Apr 30 13:31:29 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 30 Apr 2013 20:31:29 +0300
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
	<CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
Message-ID: <CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>

Here is some proof.

Stanimir
-----------
package t1;

public class TearLong {
    private volatile long x;
    long test(){
        for (int i=0;i<20000;i++){
            long n=x;
            n+=System.currentTimeMillis()&0xff;
            x=n;
        }
        return x;
    }
    public static void main(String[] args) {
        System.out.println(new TearLong().test());
        System.out.println(new TearLong().test());
    }
}

Decoding compiled method 0x00938a08:
Code:
[Disassembling for mach='i386']
[Entry Point]
[Verified Entry Point]
[Constants]
  # {method} 'test' '()J' in 't1/TearLong'
  0x00938b00: int3
  0x00938b01: xchg   %ax,%ax
  0x00938b04: mov    %eax,0xffffd000(%esp)
  0x00938b0b: push   %ebp
  0x00938b0c: sub    $0x18,%esp
  0x00938b12: mov    0x8(%ecx),%ebx
  0x00938b15: mov    0xc(%ecx),%esi
  0x00938b18: mov    %ecx,(%esp)
  0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
  0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches to
0x00938bdd
  0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
  0x00938b29: jne    0x00938bcb         ;*aload_0
                                        ; - t1.TearLong::test at 5 (line 7)
  0x00938b2f: inc    %ebx               ;*iinc
                                        ; - t1.TearLong::test at 25 (line 6)
  0x00938b30: movsd  0x8(%esi),%xmm0
  0x00938b35: movd   %xmm0,%ebp
  0x00938b39: psrlq  $0x20,%xmm0
  0x00938b3e: movd   %xmm0,%edi         ;*getfield x
                                        ; - t1.TearLong::test at 6 (line 7)
  0x00938b42: call   0x6dce22f0         ;   {runtime_call}
  0x00938b47: and    $0xff,%eax
  0x00938b4d: and    $0x0,%edx
  0x00938b50: add    %ebp,%eax
  0x00938b52: adc    %edi,%edx
  0x00938b54: cmp    0x8(%esi),%eax
  0x00938b57: movd   %eax,%xmm1
  0x00938b5b: movd   %edx,%xmm0
  0x00938b5f: punpckldq %xmm0,%xmm1
  0x00938b63: movsd  %xmm1,0x8(%esi)
  0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
                                        ; - t1.TearLong::test at 22 (line 9)
  0x00938b6d: jmp    0x00938b9c
  0x00938b6f: nop                       ;*getfield x
                                        ; - t1.TearLong::test at 6 (line 7)
  0x00938b70: call   0x6dce22f0         ;*putfield x
                                        ; - t1.TearLong::test at 22 (line 9)
                                        ;   {runtime_call}
  0x00938b75: inc    %ebx               ;*iinc
                                        ; - t1.TearLong::test at 25 (line 6)
  0x00938b76: and    $0xff,%eax
  0x00938b7c: and    $0x0,%edx
  0x00938b7f: add    %ebp,%eax
  0x00938b81: adc    %edi,%edx
  0x00938b83: cmp    0x8(%esi),%eax
  0x00938b86: movd   %eax,%xmm1
  0x00938b8a: movd   %edx,%xmm0
  0x00938b8e: punpckldq %xmm0,%xmm1
  0x00938b92: movsd  %xmm1,0x8(%esi)
  0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
                                        ;*if_icmplt
                                        ; - t1.TearLong::test at 32 (line 6)
  0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
                                        ; - t1.TearLong::test at 32 (line 6)
                                        ;   {poll}
  0x00938ba2: movsd  0x8(%esi),%xmm0
  0x00938ba7: movd   %xmm0,%ebp
  0x00938bab: psrlq  $0x20,%xmm0
  0x00938bb0: movd   %xmm0,%edi
  0x00938bb4: cmp    $0x4e20,%ebx
  0x00938bba: jl     0x00938b70         ;*getfield x
                                        ; - t1.TearLong::test at 36 (line 11)
  0x00938bbc: mov    %ebp,%eax
  0x00938bbe: mov    %edi,%edx
  0x00938bc0: add    $0x18,%esp
  0x00938bc3: pop    %ebp
  0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
  0x00938bca: ret
  0x00938bcb: mov    $0xffffffad,%ecx
  0x00938bd0: mov    %esi,%ebp
  0x00938bd2: mov    %ebx,0x4(%esp)
  0x00938bd6: nop
  0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
                                        ;*aload_0
                                        ; - t1.TearLong::test at 5 (line 7)
                                        ;   {runtime_call}
  0x00938bdc: int3                      ;*getfield x
                                        ; - t1.TearLong::test at 6 (line 7)
  0x00938bdd: mov    $0xfffffff6,%ecx
  0x00938be2: nop
  0x00938be3: call   0x0091c700         ; OopMap{off=232}
                                        ;*getfield x
                                        ; - t1.TearLong::test at 6 (line 7)
                                        ;   {runtime_call}
  0x00938be8: int3                      ;*getfield x
                                        ; - t1.TearLong::test at 6 (line 7)
....



On Tue, Apr 30, 2013 at 8:15 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> .
>
>> As for SSE, yeah it's possible, but is that true? JIT skips integer
>> registers for scalar long operations? I find that hard to believe as it
>> would miss out on large register file/renaming opportunities.
>>
>> I know that by looking at the assembly. I can still check w/ the current
> version.
>
> Stanimir
>
>
>
>> Sent from my phone
>> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> wrote:
>>
>>>  The processor can do whatever it wants in registers without other
>>> threads being able to see intermediate values.  Registers are private to
>>> the hardware thread.  So, we can use multiple instructions to load the
>>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>>> globally visible cache.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>>
>>> But this requires the src value to be in ecx:ebx so how would you load
>>> it there without two loads (and possibly observe tearing) in the first
>>> place?
>>>
>>> Sent from my phone
>>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>> wrote:
>>>
>>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>>> instruction.  This instruction has been "present on most post-80486
>>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>>> there is at least 1 way.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>>
>>>> Curious how x86 would move a long in 1 instruction? There's no memory
>>>> to memory mov so has to go through register, and thus needs 2 registers
>>>> (and hence split).  Am I missing something?
>>>>
>>>> Sent from my phone
>>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>> wrote:
>>>>
>>>>>  You might want to print the assembly using HotSpot (and OpenJDK?).
>>>>> If the assembly, uses 1 instruction to do the write, then no splitting can
>>>>> ever happen (because alignment takes care of cache line splits).  If the
>>>>> assembly, uses 2 instructions to do the write, then it is only a matter of
>>>>> timing.
>>>>>
>>>>> With a single processor system, you are waiting for the thread's
>>>>> quantum to end right after the first instruction but before the second
>>>>> instruction.  This will allow the other thread to see the split write.
>>>>>
>>>>> With a dual processor system, the reader thread simply has to get a
>>>>> copy of the cache line after the first write and before the second write.
>>>>> This is much easier to do.
>>>>>
>>>>> HotSpot will do a lot of optimizations on single processor systems.
>>>>> For example, it gets rid of the "lock" prefix in front of atomic
>>>>> instructions since the instruction's execution can't be split.  It also
>>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>>> write whereas with multiple processors it plays it safe and uses 1
>>>>> instruction.
>>>>>
>>>>> Note: If you disable all of the processors but 1 and then start
>>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>>> those processors while HotSpot is running, a lot of things break and the
>>>>> JVM will crash.  Because single processor systems are rare, the default
>>>>> might be changed to assume multiple processors unless the command line
>>>>> specifies 1 processor.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>>
>>>>>  Aleksey, correct -- more trials show what you predicted. Thanks for
>>>>> the nudge.
>>>>>
>>>>>  Mark,
>>>>>
>>>>>  Very helpful, in fact, we are seeing quick failures except for the
>>>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>>>> but not on Android).  The difference is that one uses two threads (one
>>>>> writer one reader) the other three (two writers one reader) -- both seem to
>>>>> produce similar results.
>>>>>
>>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>>> processor count goes up -- but after a few failures, we don't get any more
>>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>>
>>>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>>>> Android where ten failures takes less than a second to happen.
>>>>>
>>>>>  Best, Tim
>>>>>
>>>>>
>>>>>
>>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com>wrote:
>>>>>
>>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>>
>>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>>
>>>>>>> Yes, that's exactly what I had in mind:
>>>>>>>  a. Declare "long a"
>>>>>>>  b. Ramp up two threads.
>>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>>>>  e. ...
>>>>>>>  f. PROFIT!
>>>>>>>
>>>>>>> P.S. It is important to do some action on value read in thread 2, so
>>>>>>> that it does not hoisted from the loop, since $a is not supposed to
>>>>>>> be
>>>>>>> volatile.
>>>>>>>
>>>>>>> -Aleksey.
>>>>>>>
>>>>>>>
>>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>>
>>>>>>  *Android: splits writes to longs into two parts.*
>>>>>>
>>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>>
>>>>>>  So looks like Android does not follow the (albeit optional) advice
>>>>>> in the Java language specification about this.
>>>>>>
>>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>>> implementations)*
>>>>>>
>>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>>>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>>>> 32-bit JVM implementations follow the recommendation of the Java language
>>>>>> specification.
>>>>>>
>>>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>>>> code moved from desktop Java onto Android programmers are going to lose
>>>>>> sleep tracking down this one.
>>>>>>
>>>>>>
>>>>>>
>>>>>>  Last time I tried this sort of test, a split write would be
>>>>>> observed in under a second on a true dual processor. However, with only one
>>>>>> processor available, it would typically take around 20 minutes. So you
>>>>>> might have to run a very long test to have any real confidence in the lack
>>>>>> of splitting.
>>>>>>
>>>>>> Mark Thornton
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/0c795f98/attachment-0001.html>

From vitalyd at gmail.com  Tue Apr 30 13:45:19 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 13:45:19 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
	<CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
	<CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
Message-ID: <CAHjP37HefjkBc4f5MSkgUXqbJqj_CEz0t0B-5SBqQjNN1ASPxg@mail.gmail.com>

Interesting, thanks.  I would've expected it to be treated like the return
value from System.currentTimeMillis, where it splits it between eax:edx
registers.  Moving into xmm register before storing the value seems
wasteful, but I guess it really is avoiding "short" writes.

Sent from my phone
On Apr 30, 2013 1:31 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

> Here is some proof.
>
> Stanimir
> -----------
> package t1;
>
> public class TearLong {
>     private volatile long x;
>     long test(){
>         for (int i=0;i<20000;i++){
>             long n=x;
>             n+=System.currentTimeMillis()&0xff;
>             x=n;
>         }
>         return x;
>     }
>     public static void main(String[] args) {
>         System.out.println(new TearLong().test());
>         System.out.println(new TearLong().test());
>     }
> }
>
> Decoding compiled method 0x00938a08:
> Code:
> [Disassembling for mach='i386']
> [Entry Point]
> [Verified Entry Point]
> [Constants]
>   # {method} 'test' '()J' in 't1/TearLong'
>   0x00938b00: int3
>   0x00938b01: xchg   %ax,%ax
>   0x00938b04: mov    %eax,0xffffd000(%esp)
>   0x00938b0b: push   %ebp
>   0x00938b0c: sub    $0x18,%esp
>   0x00938b12: mov    0x8(%ecx),%ebx
>   0x00938b15: mov    0xc(%ecx),%esi
>   0x00938b18: mov    %ecx,(%esp)
>   0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
>   0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches
> to 0x00938bdd
>   0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
>   0x00938b29: jne    0x00938bcb         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>   0x00938b2f: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b30: movsd  0x8(%esi),%xmm0
>   0x00938b35: movd   %xmm0,%ebp
>   0x00938b39: psrlq  $0x20,%xmm0
>   0x00938b3e: movd   %xmm0,%edi         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b42: call   0x6dce22f0         ;   {runtime_call}
>   0x00938b47: and    $0xff,%eax
>   0x00938b4d: and    $0x0,%edx
>   0x00938b50: add    %ebp,%eax
>   0x00938b52: adc    %edi,%edx
>   0x00938b54: cmp    0x8(%esi),%eax
>   0x00938b57: movd   %eax,%xmm1
>   0x00938b5b: movd   %edx,%xmm0
>   0x00938b5f: punpckldq %xmm0,%xmm1
>   0x00938b63: movsd  %xmm1,0x8(%esi)
>   0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>   0x00938b6d: jmp    0x00938b9c
>   0x00938b6f: nop                       ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b70: call   0x6dce22f0         ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>                                         ;   {runtime_call}
>   0x00938b75: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b76: and    $0xff,%eax
>   0x00938b7c: and    $0x0,%edx
>   0x00938b7f: add    %ebp,%eax
>   0x00938b81: adc    %edi,%edx
>   0x00938b83: cmp    0x8(%esi),%eax
>   0x00938b86: movd   %eax,%xmm1
>   0x00938b8a: movd   %edx,%xmm0
>   0x00938b8e: punpckldq %xmm0,%xmm1
>   0x00938b92: movsd  %xmm1,0x8(%esi)
>   0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
>                                         ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>   0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>                                         ;   {poll}
>   0x00938ba2: movsd  0x8(%esi),%xmm0
>   0x00938ba7: movd   %xmm0,%ebp
>   0x00938bab: psrlq  $0x20,%xmm0
>   0x00938bb0: movd   %xmm0,%edi
>   0x00938bb4: cmp    $0x4e20,%ebx
>   0x00938bba: jl     0x00938b70         ;*getfield x
>                                         ; - t1.TearLong::test at 36 (line 11)
>   0x00938bbc: mov    %ebp,%eax
>   0x00938bbe: mov    %edi,%edx
>   0x00938bc0: add    $0x18,%esp
>   0x00938bc3: pop    %ebp
>   0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
>   0x00938bca: ret
>   0x00938bcb: mov    $0xffffffad,%ecx
>   0x00938bd0: mov    %esi,%ebp
>   0x00938bd2: mov    %ebx,0x4(%esp)
>   0x00938bd6: nop
>   0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
>                                         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>                                         ;   {runtime_call}
>   0x00938bdc: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938bdd: mov    $0xfffffff6,%ecx
>   0x00938be2: nop
>   0x00938be3: call   0x0091c700         ; OopMap{off=232}
>                                         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>                                         ;   {runtime_call}
>   0x00938be8: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
> ....
>
>
>
> On Tue, Apr 30, 2013 at 8:15 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> .
>>
>>> As for SSE, yeah it's possible, but is that true? JIT skips integer
>>> registers for scalar long operations? I find that hard to believe as it
>>> would miss out on large register file/renaming opportunities.
>>>
>>> I know that by looking at the assembly. I can still check w/ the current
>> version.
>>
>> Stanimir
>>
>>
>>
>>> Sent from my phone
>>> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>> wrote:
>>>
>>>>  The processor can do whatever it wants in registers without other
>>>> threads being able to see intermediate values.  Registers are private to
>>>> the hardware thread.  So, we can use multiple instructions to load the
>>>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>>>> globally visible cache.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>>>
>>>> But this requires the src value to be in ecx:ebx so how would you load
>>>> it there without two loads (and possibly observe tearing) in the first
>>>> place?
>>>>
>>>> Sent from my phone
>>>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>> wrote:
>>>>
>>>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>>>> instruction.  This instruction has been "present on most post-80486
>>>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>>>> there is at least 1 way.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>>>
>>>>> Curious how x86 would move a long in 1 instruction? There's no memory
>>>>> to memory mov so has to go through register, and thus needs 2 registers
>>>>> (and hence split).  Am I missing something?
>>>>>
>>>>> Sent from my phone
>>>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <
>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>
>>>>>>  You might want to print the assembly using HotSpot (and OpenJDK?).
>>>>>> If the assembly, uses 1 instruction to do the write, then no splitting can
>>>>>> ever happen (because alignment takes care of cache line splits).  If the
>>>>>> assembly, uses 2 instructions to do the write, then it is only a matter of
>>>>>> timing.
>>>>>>
>>>>>> With a single processor system, you are waiting for the thread's
>>>>>> quantum to end right after the first instruction but before the second
>>>>>> instruction.  This will allow the other thread to see the split write.
>>>>>>
>>>>>> With a dual processor system, the reader thread simply has to get a
>>>>>> copy of the cache line after the first write and before the second write.
>>>>>> This is much easier to do.
>>>>>>
>>>>>> HotSpot will do a lot of optimizations on single processor systems.
>>>>>> For example, it gets rid of the "lock" prefix in front of atomic
>>>>>> instructions since the instruction's execution can't be split.  It also
>>>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>>>> write whereas with multiple processors it plays it safe and uses 1
>>>>>> instruction.
>>>>>>
>>>>>> Note: If you disable all of the processors but 1 and then start
>>>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>>>> those processors while HotSpot is running, a lot of things break and the
>>>>>> JVM will crash.  Because single processor systems are rare, the default
>>>>>> might be changed to assume multiple processors unless the command line
>>>>>> specifies 1 processor.
>>>>>>
>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>> 602.333.9091
>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>> Technology
>>>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>>>
>>>>>>  Aleksey, correct -- more trials show what you predicted. Thanks for
>>>>>> the nudge.
>>>>>>
>>>>>>  Mark,
>>>>>>
>>>>>>  Very helpful, in fact, we are seeing quick failures except for the
>>>>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>>>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>>>>> but not on Android).  The difference is that one uses two threads (one
>>>>>> writer one reader) the other three (two writers one reader) -- both seem to
>>>>>> produce similar results.
>>>>>>
>>>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>>>> processor count goes up -- but after a few failures, we don't get any more
>>>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>>>
>>>>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>>>>> Android where ten failures takes less than a second to happen.
>>>>>>
>>>>>>  Best, Tim
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com
>>>>>> > wrote:
>>>>>>
>>>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>>>
>>>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>>>
>>>>>>>> Yes, that's exactly what I had in mind:
>>>>>>>>  a. Declare "long a"
>>>>>>>>  b. Ramp up two threads.
>>>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>>>>>  e. ...
>>>>>>>>  f. PROFIT!
>>>>>>>>
>>>>>>>> P.S. It is important to do some action on value read in thread 2, so
>>>>>>>> that it does not hoisted from the loop, since $a is not supposed to
>>>>>>>> be
>>>>>>>> volatile.
>>>>>>>>
>>>>>>>> -Aleksey.
>>>>>>>>
>>>>>>>>
>>>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>>>
>>>>>>>  *Android: splits writes to longs into two parts.*
>>>>>>>
>>>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>>>
>>>>>>>  So looks like Android does not follow the (albeit optional) advice
>>>>>>> in the Java language specification about this.
>>>>>>>
>>>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>>>> implementations)*
>>>>>>>
>>>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>>>>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>>>>> 32-bit JVM implementations follow the recommendation of the Java language
>>>>>>> specification.
>>>>>>>
>>>>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>>>>> code moved from desktop Java onto Android programmers are going to lose
>>>>>>> sleep tracking down this one.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>  Last time I tried this sort of test, a split write would be
>>>>>>> observed in under a second on a true dual processor. However, with only one
>>>>>>> processor available, it would typically take around 20 minutes. So you
>>>>>>> might have to run a very long test to have any real confidence in the lack
>>>>>>> of splitting.
>>>>>>>
>>>>>>> Mark Thornton
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/85367981/attachment-0001.html>

From david.dice at gmail.com  Tue Apr 30 13:51:40 2013
From: david.dice at gmail.com (David Dice)
Date: Tue, 30 Apr 2013 13:51:40 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
Message-ID: <CANbRUci6d4y_SvS-KOFaHFFL=RNk9obUb2fmuvg89tTVdrQd1A@mail.gmail.com>

Regarding 64-bit volatile long accesses for IA32 mode, long ago hotspot
used the x87 FPU.   We'd load values atomically via floating point load
instructions and then deposit the value into a temp on the stack, and then
re-load into integer registers.   We've since switched to XMM which is
somewhat cleaner.

At the time, there were no formal guarantees that the floating point
accesses would actually be atomic.  I discussed this with various vendors,
and all noted that the references would be atomic in their implementations
as long as natural alignment was provided.

-Dave
https://blogs.oracle.com/dave/




Message: 1
> Date: Tue, 30 Apr 2013 20:31:29 +0300
> From: Stanimir Simeonoff <stanimir at riflexo.com>
> To: Vitaly Davidovich <vitalyd at gmail.com>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double and long : Android
> Message-ID:
>         <
> CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Here is some proof.
>
> Stanimir
> -----------
> package t1;
>
> public class TearLong {
>     private volatile long x;
>     long test(){
>         for (int i=0;i<20000;i++){
>             long n=x;
>             n+=System.currentTimeMillis()&0xff;
>             x=n;
>         }
>         return x;
>     }
>     public static void main(String[] args) {
>         System.out.println(new TearLong().test());
>         System.out.println(new TearLong().test());
>     }
> }
>
> Decoding compiled method 0x00938a08:
> Code:
> [Disassembling for mach='i386']
> [Entry Point]
> [Verified Entry Point]
> [Constants]
>   # {method} 'test' '()J' in 't1/TearLong'
>   0x00938b00: int3
>   0x00938b01: xchg   %ax,%ax
>   0x00938b04: mov    %eax,0xffffd000(%esp)
>   0x00938b0b: push   %ebp
>   0x00938b0c: sub    $0x18,%esp
>   0x00938b12: mov    0x8(%ecx),%ebx
>   0x00938b15: mov    0xc(%ecx),%esi
>   0x00938b18: mov    %ecx,(%esp)
>   0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
>   0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches to
> 0x00938bdd
>   0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
>   0x00938b29: jne    0x00938bcb         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>   0x00938b2f: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b30: movsd  0x8(%esi),%xmm0
>   0x00938b35: movd   %xmm0,%ebp
>   0x00938b39: psrlq  $0x20,%xmm0
>   0x00938b3e: movd   %xmm0,%edi         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b42: call   0x6dce22f0         ;   {runtime_call}
>   0x00938b47: and    $0xff,%eax
>   0x00938b4d: and    $0x0,%edx
>   0x00938b50: add    %ebp,%eax
>   0x00938b52: adc    %edi,%edx
>   0x00938b54: cmp    0x8(%esi),%eax
>   0x00938b57: movd   %eax,%xmm1
>   0x00938b5b: movd   %edx,%xmm0
>   0x00938b5f: punpckldq %xmm0,%xmm1
>   0x00938b63: movsd  %xmm1,0x8(%esi)
>   0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>   0x00938b6d: jmp    0x00938b9c
>   0x00938b6f: nop                       ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b70: call   0x6dce22f0         ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>                                         ;   {runtime_call}
>   0x00938b75: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b76: and    $0xff,%eax
>   0x00938b7c: and    $0x0,%edx
>   0x00938b7f: add    %ebp,%eax
>   0x00938b81: adc    %edi,%edx
>   0x00938b83: cmp    0x8(%esi),%eax
>   0x00938b86: movd   %eax,%xmm1
>   0x00938b8a: movd   %edx,%xmm0
>   0x00938b8e: punpckldq %xmm0,%xmm1
>   0x00938b92: movsd  %xmm1,0x8(%esi)
>   0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
>                                         ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>   0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>                                         ;   {poll}
>   0x00938ba2: movsd  0x8(%esi),%xmm0
>   0x00938ba7: movd   %xmm0,%ebp
>   0x00938bab: psrlq  $0x20,%xmm0
>   0x00938bb0: movd   %xmm0,%edi
>   0x00938bb4: cmp    $0x4e20,%ebx
>   0x00938bba: jl     0x00938b70         ;*getfield x
>                                         ; - t1.TearLong::test at 36 (line 11)
>   0x00938bbc: mov    %ebp,%eax
>   0x00938bbe: mov    %edi,%edx
>   0x00938bc0: add    $0x18,%esp
>   0x00938bc3: pop    %ebp
>   0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
>   0x00938bca: ret
>   0x00938bcb: mov    $0xffffffad,%ecx
>   0x00938bd0: mov    %esi,%ebp
>   0x00938bd2: mov    %ebx,0x4(%esp)
>   0x00938bd6: nop
>   0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
>                                         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>                                         ;   {runtime_call}
>   0x00938bdc: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938bdd: mov    $0xfffffff6,%ecx
>   0x00938be2: nop
>   0x00938be3: call   0x0091c700         ; OopMap{off=232}
>                                         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>                                         ;   {runtime_call}
>   0x00938be8: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
> ....
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/8cb0175a/attachment.html>

From vitalyd at gmail.com  Tue Apr 30 13:51:54 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 13:51:54 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
	<CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
	<CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
Message-ID: <CAHjP37HTQjhDqOKkok+eOG3-=TKwSz_SNH72f2gwFx-3oyR-sw@mail.gmail.com>

By the way, if you make x non-volatile, what changes? Just lock addl
fencing instructions go away?

Sent from my phone
On Apr 30, 2013 1:31 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

> Here is some proof.
>
> Stanimir
> -----------
> package t1;
>
> public class TearLong {
>     private volatile long x;
>     long test(){
>         for (int i=0;i<20000;i++){
>             long n=x;
>             n+=System.currentTimeMillis()&0xff;
>             x=n;
>         }
>         return x;
>     }
>     public static void main(String[] args) {
>         System.out.println(new TearLong().test());
>         System.out.println(new TearLong().test());
>     }
> }
>
> Decoding compiled method 0x00938a08:
> Code:
> [Disassembling for mach='i386']
> [Entry Point]
> [Verified Entry Point]
> [Constants]
>   # {method} 'test' '()J' in 't1/TearLong'
>   0x00938b00: int3
>   0x00938b01: xchg   %ax,%ax
>   0x00938b04: mov    %eax,0xffffd000(%esp)
>   0x00938b0b: push   %ebp
>   0x00938b0c: sub    $0x18,%esp
>   0x00938b12: mov    0x8(%ecx),%ebx
>   0x00938b15: mov    0xc(%ecx),%esi
>   0x00938b18: mov    %ecx,(%esp)
>   0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
>   0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches
> to 0x00938bdd
>   0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
>   0x00938b29: jne    0x00938bcb         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>   0x00938b2f: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b30: movsd  0x8(%esi),%xmm0
>   0x00938b35: movd   %xmm0,%ebp
>   0x00938b39: psrlq  $0x20,%xmm0
>   0x00938b3e: movd   %xmm0,%edi         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b42: call   0x6dce22f0         ;   {runtime_call}
>   0x00938b47: and    $0xff,%eax
>   0x00938b4d: and    $0x0,%edx
>   0x00938b50: add    %ebp,%eax
>   0x00938b52: adc    %edi,%edx
>   0x00938b54: cmp    0x8(%esi),%eax
>   0x00938b57: movd   %eax,%xmm1
>   0x00938b5b: movd   %edx,%xmm0
>   0x00938b5f: punpckldq %xmm0,%xmm1
>   0x00938b63: movsd  %xmm1,0x8(%esi)
>   0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>   0x00938b6d: jmp    0x00938b9c
>   0x00938b6f: nop                       ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938b70: call   0x6dce22f0         ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>                                         ;   {runtime_call}
>   0x00938b75: inc    %ebx               ;*iinc
>                                         ; - t1.TearLong::test at 25 (line 6)
>   0x00938b76: and    $0xff,%eax
>   0x00938b7c: and    $0x0,%edx
>   0x00938b7f: add    %ebp,%eax
>   0x00938b81: adc    %edi,%edx
>   0x00938b83: cmp    0x8(%esi),%eax
>   0x00938b86: movd   %eax,%xmm1
>   0x00938b8a: movd   %edx,%xmm0
>   0x00938b8e: punpckldq %xmm0,%xmm1
>   0x00938b92: movsd  %xmm1,0x8(%esi)
>   0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
>                                         ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>   0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
>                                         ; - t1.TearLong::test at 32 (line 6)
>                                         ;   {poll}
>   0x00938ba2: movsd  0x8(%esi),%xmm0
>   0x00938ba7: movd   %xmm0,%ebp
>   0x00938bab: psrlq  $0x20,%xmm0
>   0x00938bb0: movd   %xmm0,%edi
>   0x00938bb4: cmp    $0x4e20,%ebx
>   0x00938bba: jl     0x00938b70         ;*getfield x
>                                         ; - t1.TearLong::test at 36 (line 11)
>   0x00938bbc: mov    %ebp,%eax
>   0x00938bbe: mov    %edi,%edx
>   0x00938bc0: add    $0x18,%esp
>   0x00938bc3: pop    %ebp
>   0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
>   0x00938bca: ret
>   0x00938bcb: mov    $0xffffffad,%ecx
>   0x00938bd0: mov    %esi,%ebp
>   0x00938bd2: mov    %ebx,0x4(%esp)
>   0x00938bd6: nop
>   0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
>                                         ;*aload_0
>                                         ; - t1.TearLong::test at 5 (line 7)
>                                         ;   {runtime_call}
>   0x00938bdc: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>   0x00938bdd: mov    $0xfffffff6,%ecx
>   0x00938be2: nop
>   0x00938be3: call   0x0091c700         ; OopMap{off=232}
>                                         ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
>                                         ;   {runtime_call}
>   0x00938be8: int3                      ;*getfield x
>                                         ; - t1.TearLong::test at 6 (line 7)
> ....
>
>
>
> On Tue, Apr 30, 2013 at 8:15 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> .
>>
>>> As for SSE, yeah it's possible, but is that true? JIT skips integer
>>> registers for scalar long operations? I find that hard to believe as it
>>> would miss out on large register file/renaming opportunities.
>>>
>>> I know that by looking at the assembly. I can still check w/ the current
>> version.
>>
>> Stanimir
>>
>>
>>
>>> Sent from my phone
>>> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>> wrote:
>>>
>>>>  The processor can do whatever it wants in registers without other
>>>> threads being able to see intermediate values.  Registers are private to
>>>> the hardware thread.  So, we can use multiple instructions to load the
>>>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>>>> globally visible cache.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>>>
>>>> But this requires the src value to be in ecx:ebx so how would you load
>>>> it there without two loads (and possibly observe tearing) in the first
>>>> place?
>>>>
>>>> Sent from my phone
>>>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>> wrote:
>>>>
>>>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>>>> instruction.  This instruction has been "present on most post-80486
>>>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>>>> there is at least 1 way.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>>>
>>>>> Curious how x86 would move a long in 1 instruction? There's no memory
>>>>> to memory mov so has to go through register, and thus needs 2 registers
>>>>> (and hence split).  Am I missing something?
>>>>>
>>>>> Sent from my phone
>>>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <
>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>
>>>>>>  You might want to print the assembly using HotSpot (and OpenJDK?).
>>>>>> If the assembly, uses 1 instruction to do the write, then no splitting can
>>>>>> ever happen (because alignment takes care of cache line splits).  If the
>>>>>> assembly, uses 2 instructions to do the write, then it is only a matter of
>>>>>> timing.
>>>>>>
>>>>>> With a single processor system, you are waiting for the thread's
>>>>>> quantum to end right after the first instruction but before the second
>>>>>> instruction.  This will allow the other thread to see the split write.
>>>>>>
>>>>>> With a dual processor system, the reader thread simply has to get a
>>>>>> copy of the cache line after the first write and before the second write.
>>>>>> This is much easier to do.
>>>>>>
>>>>>> HotSpot will do a lot of optimizations on single processor systems.
>>>>>> For example, it gets rid of the "lock" prefix in front of atomic
>>>>>> instructions since the instruction's execution can't be split.  It also
>>>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>>>> write whereas with multiple processors it plays it safe and uses 1
>>>>>> instruction.
>>>>>>
>>>>>> Note: If you disable all of the processors but 1 and then start
>>>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>>>> those processors while HotSpot is running, a lot of things break and the
>>>>>> JVM will crash.  Because single processor systems are rare, the default
>>>>>> might be changed to assume multiple processors unless the command line
>>>>>> specifies 1 processor.
>>>>>>
>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>> 602.333.9091
>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>> Technology
>>>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>>>
>>>>>>  Aleksey, correct -- more trials show what you predicted. Thanks for
>>>>>> the nudge.
>>>>>>
>>>>>>  Mark,
>>>>>>
>>>>>>  Very helpful, in fact, we are seeing quick failures except for the
>>>>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>>>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>>>>> but not on Android).  The difference is that one uses two threads (one
>>>>>> writer one reader) the other three (two writers one reader) -- both seem to
>>>>>> produce similar results.
>>>>>>
>>>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>>>> processor count goes up -- but after a few failures, we don't get any more
>>>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>>>
>>>>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>>>>> Android where ten failures takes less than a second to happen.
>>>>>>
>>>>>>  Best, Tim
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <mthornton at optrak.com
>>>>>> > wrote:
>>>>>>
>>>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>>>
>>>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>>>
>>>>>>>> Yes, that's exactly what I had in mind:
>>>>>>>>  a. Declare "long a"
>>>>>>>>  b. Ramp up two threads.
>>>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>>>  d. Make thread 2 observe the field a, and count the observed values
>>>>>>>>  e. ...
>>>>>>>>  f. PROFIT!
>>>>>>>>
>>>>>>>> P.S. It is important to do some action on value read in thread 2, so
>>>>>>>> that it does not hoisted from the loop, since $a is not supposed to
>>>>>>>> be
>>>>>>>> volatile.
>>>>>>>>
>>>>>>>> -Aleksey.
>>>>>>>>
>>>>>>>>
>>>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>>>
>>>>>>>  *Android: splits writes to longs into two parts.*
>>>>>>>
>>>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>>>
>>>>>>>  So looks like Android does not follow the (albeit optional) advice
>>>>>>> in the Java language specification about this.
>>>>>>>
>>>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>>>> implementations)*
>>>>>>>
>>>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but we
>>>>>>> tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>>>>> 32-bit JVM implementations follow the recommendation of the Java language
>>>>>>> specification.
>>>>>>>
>>>>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>>>>> code moved from desktop Java onto Android programmers are going to lose
>>>>>>> sleep tracking down this one.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>  Last time I tried this sort of test, a split write would be
>>>>>>> observed in under a second on a true dual processor. However, with only one
>>>>>>> processor available, it would typically take around 20 minutes. So you
>>>>>>> might have to run a very long test to have any real confidence in the lack
>>>>>>> of splitting.
>>>>>>>
>>>>>>> Mark Thornton
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/01407776/attachment-0001.html>

From stanimir at riflexo.com  Tue Apr 30 14:07:01 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 30 Apr 2013 21:07:01 +0300
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37HTQjhDqOKkok+eOG3-=TKwSz_SNH72f2gwFx-3oyR-sw@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
	<CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
	<CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
	<CAHjP37HTQjhDqOKkok+eOG3-=TKwSz_SNH72f2gwFx-3oyR-sw@mail.gmail.com>
Message-ID: <CAEJX8opVrFR1ZkUvRZ5UTCLcFP74eVUa63v-N+4cJ=F9h_BWuA@mail.gmail.com>

On Tue, Apr 30, 2013 at 8:51 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> By the way, if you make x non-volatile, what changes? Just lock addl
> fencing instructions go away?
>
No, no. It uses standard x86:

  0x00938b7e: add    0x8(%esi),%ecx
  0x00938b81: adc    0xc(%esi),%ebx     ;*ladd
                                        ; - t1.TearLong::test at 18 (line 8)
  0x00938b84: mov    %ecx,0x8(%esi)
  0x00938b87: mov    %ebx,0xc(%esi)     ;*putfield x
                                        ; - t1.TearLong::test at 22 (line 9)

Like I've told I know that by experience.


> Sent from my phone
> On Apr 30, 2013 1:31 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
> wrote:
>
>> Here is some proof.
>>
>> Stanimir
>> -----------
>> package t1;
>>
>> public class TearLong {
>>     private volatile long x;
>>     long test(){
>>         for (int i=0;i<20000;i++){
>>             long n=x;
>>             n+=System.currentTimeMillis()&0xff;
>>             x=n;
>>         }
>>         return x;
>>     }
>>     public static void main(String[] args) {
>>         System.out.println(new TearLong().test());
>>         System.out.println(new TearLong().test());
>>     }
>> }
>>
>> Decoding compiled method 0x00938a08:
>> Code:
>> [Disassembling for mach='i386']
>> [Entry Point]
>> [Verified Entry Point]
>> [Constants]
>>   # {method} 'test' '()J' in 't1/TearLong'
>>   0x00938b00: int3
>>   0x00938b01: xchg   %ax,%ax
>>   0x00938b04: mov    %eax,0xffffd000(%esp)
>>   0x00938b0b: push   %ebp
>>   0x00938b0c: sub    $0x18,%esp
>>   0x00938b12: mov    0x8(%ecx),%ebx
>>   0x00938b15: mov    0xc(%ecx),%esi
>>   0x00938b18: mov    %ecx,(%esp)
>>   0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
>>   0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches
>> to 0x00938bdd
>>   0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
>>   0x00938b29: jne    0x00938bcb         ;*aload_0
>>                                         ; - t1.TearLong::test at 5 (line 7)
>>   0x00938b2f: inc    %ebx               ;*iinc
>>                                         ; - t1.TearLong::test at 25 (line 6)
>>   0x00938b30: movsd  0x8(%esi),%xmm0
>>   0x00938b35: movd   %xmm0,%ebp
>>   0x00938b39: psrlq  $0x20,%xmm0
>>   0x00938b3e: movd   %xmm0,%edi         ;*getfield x
>>                                         ; - t1.TearLong::test at 6 (line 7)
>>   0x00938b42: call   0x6dce22f0         ;   {runtime_call}
>>   0x00938b47: and    $0xff,%eax
>>   0x00938b4d: and    $0x0,%edx
>>   0x00938b50: add    %ebp,%eax
>>   0x00938b52: adc    %edi,%edx
>>   0x00938b54: cmp    0x8(%esi),%eax
>>   0x00938b57: movd   %eax,%xmm1
>>   0x00938b5b: movd   %edx,%xmm0
>>   0x00938b5f: punpckldq %xmm0,%xmm1
>>   0x00938b63: movsd  %xmm1,0x8(%esi)
>>   0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
>>                                         ; - t1.TearLong::test at 22 (line 9)
>>   0x00938b6d: jmp    0x00938b9c
>>   0x00938b6f: nop                       ;*getfield x
>>                                         ; - t1.TearLong::test at 6 (line 7)
>>   0x00938b70: call   0x6dce22f0         ;*putfield x
>>                                         ; - t1.TearLong::test at 22 (line 9)
>>                                         ;   {runtime_call}
>>   0x00938b75: inc    %ebx               ;*iinc
>>                                         ; - t1.TearLong::test at 25 (line 6)
>>   0x00938b76: and    $0xff,%eax
>>   0x00938b7c: and    $0x0,%edx
>>   0x00938b7f: add    %ebp,%eax
>>   0x00938b81: adc    %edi,%edx
>>   0x00938b83: cmp    0x8(%esi),%eax
>>   0x00938b86: movd   %eax,%xmm1
>>   0x00938b8a: movd   %edx,%xmm0
>>   0x00938b8e: punpckldq %xmm0,%xmm1
>>   0x00938b92: movsd  %xmm1,0x8(%esi)
>>   0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
>>                                         ;*if_icmplt
>>                                         ; - t1.TearLong::test at 32 (line 6)
>>   0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
>>                                         ; - t1.TearLong::test at 32 (line 6)
>>                                         ;   {poll}
>>   0x00938ba2: movsd  0x8(%esi),%xmm0
>>   0x00938ba7: movd   %xmm0,%ebp
>>   0x00938bab: psrlq  $0x20,%xmm0
>>   0x00938bb0: movd   %xmm0,%edi
>>   0x00938bb4: cmp    $0x4e20,%ebx
>>   0x00938bba: jl     0x00938b70         ;*getfield x
>>                                         ; - t1.TearLong::test at 36 (line
>> 11)
>>   0x00938bbc: mov    %ebp,%eax
>>   0x00938bbe: mov    %edi,%edx
>>   0x00938bc0: add    $0x18,%esp
>>   0x00938bc3: pop    %ebp
>>   0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
>>   0x00938bca: ret
>>   0x00938bcb: mov    $0xffffffad,%ecx
>>   0x00938bd0: mov    %esi,%ebp
>>   0x00938bd2: mov    %ebx,0x4(%esp)
>>   0x00938bd6: nop
>>   0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
>>                                         ;*aload_0
>>                                         ; - t1.TearLong::test at 5 (line 7)
>>                                         ;   {runtime_call}
>>   0x00938bdc: int3                      ;*getfield x
>>                                         ; - t1.TearLong::test at 6 (line 7)
>>   0x00938bdd: mov    $0xfffffff6,%ecx
>>   0x00938be2: nop
>>   0x00938be3: call   0x0091c700         ; OopMap{off=232}
>>                                         ;*getfield x
>>                                         ; - t1.TearLong::test at 6 (line 7)
>>                                         ;   {runtime_call}
>>   0x00938be8: int3                      ;*getfield x
>>                                         ; - t1.TearLong::test at 6 (line 7)
>> ....
>>
>>
>>
>> On Tue, Apr 30, 2013 at 8:15 PM, Stanimir Simeonoff <stanimir at riflexo.com
>> > wrote:
>>
>>> .
>>>
>>>> As for SSE, yeah it's possible, but is that true? JIT skips integer
>>>> registers for scalar long operations? I find that hard to believe as it
>>>> would miss out on large register file/renaming opportunities.
>>>>
>>>> I know that by looking at the assembly. I can still check w/ the
>>> current version.
>>>
>>> Stanimir
>>>
>>>
>>>
>>>> Sent from my phone
>>>> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>> wrote:
>>>>
>>>>>  The processor can do whatever it wants in registers without other
>>>>> threads being able to see intermediate values.  Registers are private to
>>>>> the hardware thread.  So, we can use multiple instructions to load the
>>>>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>>>>> globally visible cache.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>>>>
>>>>> But this requires the src value to be in ecx:ebx so how would you load
>>>>> it there without two loads (and possibly observe tearing) in the first
>>>>> place?
>>>>>
>>>>> Sent from my phone
>>>>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <
>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>
>>>>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>>>>> instruction.  This instruction has been "present on most post-80486
>>>>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>>>>> there is at least 1 way.
>>>>>>
>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>> 602.333.9091
>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>> Technology
>>>>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>>>>
>>>>>> Curious how x86 would move a long in 1 instruction? There's no memory
>>>>>> to memory mov so has to go through register, and thus needs 2 registers
>>>>>> (and hence split).  Am I missing something?
>>>>>>
>>>>>> Sent from my phone
>>>>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <
>>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>>
>>>>>>>  You might want to print the assembly using HotSpot (and
>>>>>>> OpenJDK?).  If the assembly, uses 1 instruction to do the write, then no
>>>>>>> splitting can ever happen (because alignment takes care of cache line
>>>>>>> splits).  If the assembly, uses 2 instructions to do the write, then it is
>>>>>>> only a matter of timing.
>>>>>>>
>>>>>>> With a single processor system, you are waiting for the thread's
>>>>>>> quantum to end right after the first instruction but before the second
>>>>>>> instruction.  This will allow the other thread to see the split write.
>>>>>>>
>>>>>>> With a dual processor system, the reader thread simply has to get a
>>>>>>> copy of the cache line after the first write and before the second write.
>>>>>>> This is much easier to do.
>>>>>>>
>>>>>>> HotSpot will do a lot of optimizations on single processor systems.
>>>>>>> For example, it gets rid of the "lock" prefix in front of atomic
>>>>>>> instructions since the instruction's execution can't be split.  It also
>>>>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>>>>> write whereas with multiple processors it plays it safe and uses 1
>>>>>>> instruction.
>>>>>>>
>>>>>>> Note: If you disable all of the processors but 1 and then start
>>>>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>>>>> those processors while HotSpot is running, a lot of things break and the
>>>>>>> JVM will crash.  Because single processor systems are rare, the default
>>>>>>> might be changed to assume multiple processors unless the command line
>>>>>>> specifies 1 processor.
>>>>>>>
>>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>>> 602.333.9091
>>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>>> Technology
>>>>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>>>>
>>>>>>>  Aleksey, correct -- more trials show what you predicted. Thanks
>>>>>>> for the nudge.
>>>>>>>
>>>>>>>  Mark,
>>>>>>>
>>>>>>>  Very helpful, in fact, we are seeing quick failures except for the
>>>>>>> dual-processor case -- on a dual processor hardware or VM (Virtual Box) we
>>>>>>> have yet to get a failure.  The two programs attached are what I'm running.
>>>>>>>  I stripped out my benchmark framework (so they are easy to run on OpenJDK
>>>>>>> but not on Android).  The difference is that one uses two threads (one
>>>>>>> writer one reader) the other three (two writers one reader) -- both seem to
>>>>>>> produce similar results.
>>>>>>>
>>>>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>>>>> processor count goes up -- but after a few failures, we don't get any more
>>>>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>>>>
>>>>>>>  It seems that while this can happen on OpenJDK it is rarer than on
>>>>>>> Android where ten failures takes less than a second to happen.
>>>>>>>
>>>>>>>  Best, Tim
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <
>>>>>>> mthornton at optrak.com> wrote:
>>>>>>>
>>>>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>>>>
>>>>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>>>>
>>>>>>>>> Yes, that's exactly what I had in mind:
>>>>>>>>>  a. Declare "long a"
>>>>>>>>>  b. Ramp up two threads.
>>>>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>>>>  d. Make thread 2 observe the field a, and count the observed
>>>>>>>>> values
>>>>>>>>>  e. ...
>>>>>>>>>  f. PROFIT!
>>>>>>>>>
>>>>>>>>> P.S. It is important to do some action on value read in thread 2,
>>>>>>>>> so
>>>>>>>>> that it does not hoisted from the loop, since $a is not supposed
>>>>>>>>> to be
>>>>>>>>> volatile.
>>>>>>>>>
>>>>>>>>> -Aleksey.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>>>>
>>>>>>>>  *Android: splits writes to longs into two parts.*
>>>>>>>>
>>>>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>>>>
>>>>>>>>  So looks like Android does not follow the (albeit optional)
>>>>>>>> advice in the Java language specification about this.
>>>>>>>>
>>>>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>>>>> implementations)*
>>>>>>>>
>>>>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but
>>>>>>>> we tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen.
>>>>>>>> The 32-bit JVM implementations follow the recommendation of the Java
>>>>>>>> language specification.
>>>>>>>>
>>>>>>>>  An interesting curio. I wonder how many crashes in "working" Java
>>>>>>>> code moved from desktop Java onto Android programmers are going to lose
>>>>>>>> sleep tracking down this one.
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>  Last time I tried this sort of test, a split write would be
>>>>>>>> observed in under a second on a true dual processor. However, with only one
>>>>>>>> processor available, it would typically take around 20 minutes. So you
>>>>>>>> might have to run a very long test to have any real confidence in the lack
>>>>>>>> of splitting.
>>>>>>>>
>>>>>>>> Mark Thornton
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/88882c52/attachment-0001.html>

From vitalyd at gmail.com  Tue Apr 30 14:17:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 30 Apr 2013 14:17:00 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAEJX8opVrFR1ZkUvRZ5UTCLcFP74eVUa63v-N+4cJ=F9h_BWuA@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
	<CAEJX8or+vnt5tL7C1TndaizAG8fDbPeJbnAnrfSEv_kF2r16EQ@mail.gmail.com>
	<CAEJX8oqZA6d1u_4QxLaagr_kZTGzxStseFJqrYntoo+w-hTy7g@mail.gmail.com>
	<CAHjP37HTQjhDqOKkok+eOG3-=TKwSz_SNH72f2gwFx-3oyR-sw@mail.gmail.com>
	<CAEJX8opVrFR1ZkUvRZ5UTCLcFP74eVUa63v-N+4cJ=F9h_BWuA@mail.gmail.com>
Message-ID: <CAHjP37EdLE-SDg9AgeZm+ghnDgQTgdrLca9UmempR1Q4hOr=tg@mail.gmail.com>

OK well I've been talking (and I thought others too) about non volatile
cases, where I'd find this hoop jumping to be costly.

Sent from my phone
On Apr 30, 2013 2:07 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

>
>
> On Tue, Apr 30, 2013 at 8:51 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> By the way, if you make x non-volatile, what changes? Just lock addl
>> fencing instructions go away?
>>
> No, no. It uses standard x86:
>
>   0x00938b7e: add    0x8(%esi),%ecx
>   0x00938b81: adc    0xc(%esi),%ebx     ;*ladd
>                                         ; - t1.TearLong::test at 18 (line 8)
>   0x00938b84: mov    %ecx,0x8(%esi)
>   0x00938b87: mov    %ebx,0xc(%esi)     ;*putfield x
>                                         ; - t1.TearLong::test at 22 (line 9)
>
> Like I've told I know that by experience.
>
>
>> Sent from my phone
>> On Apr 30, 2013 1:31 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
>> wrote:
>>
>>> Here is some proof.
>>>
>>> Stanimir
>>> -----------
>>> package t1;
>>>
>>> public class TearLong {
>>>     private volatile long x;
>>>     long test(){
>>>         for (int i=0;i<20000;i++){
>>>             long n=x;
>>>             n+=System.currentTimeMillis()&0xff;
>>>             x=n;
>>>         }
>>>         return x;
>>>     }
>>>     public static void main(String[] args) {
>>>         System.out.println(new TearLong().test());
>>>         System.out.println(new TearLong().test());
>>>     }
>>> }
>>>
>>> Decoding compiled method 0x00938a08:
>>> Code:
>>> [Disassembling for mach='i386']
>>> [Entry Point]
>>> [Verified Entry Point]
>>> [Constants]
>>>   # {method} 'test' '()J' in 't1/TearLong'
>>>   0x00938b00: int3
>>>   0x00938b01: xchg   %ax,%ax
>>>   0x00938b04: mov    %eax,0xffffd000(%esp)
>>>   0x00938b0b: push   %ebp
>>>   0x00938b0c: sub    $0x18,%esp
>>>   0x00938b12: mov    0x8(%ecx),%ebx
>>>   0x00938b15: mov    0xc(%ecx),%esi
>>>   0x00938b18: mov    %ecx,(%esp)
>>>   0x00938b1b: call   0x6dbeed90         ;   {runtime_call}
>>>   0x00938b20: mov    0x4(%esi),%ebp     ; implicit exception: dispatches
>>> to 0x00938bdd
>>>   0x00938b23: cmp    $0x3b6bd38,%ebp    ;   {oop('t1/TearLong')}
>>>   0x00938b29: jne    0x00938bcb         ;*aload_0
>>>                                         ; - t1.TearLong::test at 5 (line 7)
>>>   0x00938b2f: inc    %ebx               ;*iinc
>>>                                         ; - t1.TearLong::test at 25 (line
>>> 6)
>>>   0x00938b30: movsd  0x8(%esi),%xmm0
>>>   0x00938b35: movd   %xmm0,%ebp
>>>   0x00938b39: psrlq  $0x20,%xmm0
>>>   0x00938b3e: movd   %xmm0,%edi         ;*getfield x
>>>                                         ; - t1.TearLong::test at 6 (line 7)
>>>   0x00938b42: call   0x6dce22f0         ;   {runtime_call}
>>>   0x00938b47: and    $0xff,%eax
>>>   0x00938b4d: and    $0x0,%edx
>>>   0x00938b50: add    %ebp,%eax
>>>   0x00938b52: adc    %edi,%edx
>>>   0x00938b54: cmp    0x8(%esi),%eax
>>>   0x00938b57: movd   %eax,%xmm1
>>>   0x00938b5b: movd   %edx,%xmm0
>>>   0x00938b5f: punpckldq %xmm0,%xmm1
>>>   0x00938b63: movsd  %xmm1,0x8(%esi)
>>>   0x00938b68: lock addl $0x0,(%esp)     ;*putfield x
>>>                                         ; - t1.TearLong::test at 22 (line
>>> 9)
>>>   0x00938b6d: jmp    0x00938b9c
>>>   0x00938b6f: nop                       ;*getfield x
>>>                                         ; - t1.TearLong::test at 6 (line 7)
>>>   0x00938b70: call   0x6dce22f0         ;*putfield x
>>>                                         ; - t1.TearLong::test at 22 (line
>>> 9)
>>>                                         ;   {runtime_call}
>>>   0x00938b75: inc    %ebx               ;*iinc
>>>                                         ; - t1.TearLong::test at 25 (line
>>> 6)
>>>   0x00938b76: and    $0xff,%eax
>>>   0x00938b7c: and    $0x0,%edx
>>>   0x00938b7f: add    %ebp,%eax
>>>   0x00938b81: adc    %edi,%edx
>>>   0x00938b83: cmp    0x8(%esi),%eax
>>>   0x00938b86: movd   %eax,%xmm1
>>>   0x00938b8a: movd   %edx,%xmm0
>>>   0x00938b8e: punpckldq %xmm0,%xmm1
>>>   0x00938b92: movsd  %xmm1,0x8(%esi)
>>>   0x00938b97: lock addl $0x0,(%esp)     ; OopMap{esi=Oop off=156}
>>>                                         ;*if_icmplt
>>>                                         ; - t1.TearLong::test at 32 (line
>>> 6)
>>>   0x00938b9c: test   %edi,0x8c0000      ;*if_icmplt
>>>                                         ; - t1.TearLong::test at 32 (line
>>> 6)
>>>                                         ;   {poll}
>>>   0x00938ba2: movsd  0x8(%esi),%xmm0
>>>   0x00938ba7: movd   %xmm0,%ebp
>>>   0x00938bab: psrlq  $0x20,%xmm0
>>>   0x00938bb0: movd   %xmm0,%edi
>>>   0x00938bb4: cmp    $0x4e20,%ebx
>>>   0x00938bba: jl     0x00938b70         ;*getfield x
>>>                                         ; - t1.TearLong::test at 36 (line
>>> 11)
>>>   0x00938bbc: mov    %ebp,%eax
>>>   0x00938bbe: mov    %edi,%edx
>>>   0x00938bc0: add    $0x18,%esp
>>>   0x00938bc3: pop    %ebp
>>>   0x00938bc4: test   %eax,0x8c0000      ;   {poll_return}
>>>   0x00938bca: ret
>>>   0x00938bcb: mov    $0xffffffad,%ecx
>>>   0x00938bd0: mov    %esi,%ebp
>>>   0x00938bd2: mov    %ebx,0x4(%esp)
>>>   0x00938bd6: nop
>>>   0x00938bd7: call   0x0091c700         ; OopMap{ebp=Oop off=220}
>>>                                         ;*aload_0
>>>                                         ; - t1.TearLong::test at 5 (line 7)
>>>                                         ;   {runtime_call}
>>>   0x00938bdc: int3                      ;*getfield x
>>>                                         ; - t1.TearLong::test at 6 (line 7)
>>>   0x00938bdd: mov    $0xfffffff6,%ecx
>>>   0x00938be2: nop
>>>   0x00938be3: call   0x0091c700         ; OopMap{off=232}
>>>                                         ;*getfield x
>>>                                         ; - t1.TearLong::test at 6 (line 7)
>>>                                         ;   {runtime_call}
>>>   0x00938be8: int3                      ;*getfield x
>>>                                         ; - t1.TearLong::test at 6 (line 7)
>>> ....
>>>
>>>
>>>
>>> On Tue, Apr 30, 2013 at 8:15 PM, Stanimir Simeonoff <
>>> stanimir at riflexo.com> wrote:
>>>
>>>> .
>>>>
>>>>> As for SSE, yeah it's possible, but is that true? JIT skips integer
>>>>> registers for scalar long operations? I find that hard to believe as it
>>>>> would miss out on large register file/renaming opportunities.
>>>>>
>>>>> I know that by looking at the assembly. I can still check w/ the
>>>> current version.
>>>>
>>>> Stanimir
>>>>
>>>>
>>>>
>>>>> Sent from my phone
>>>>> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" <
>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>
>>>>>>  The processor can do whatever it wants in registers without other
>>>>>> threads being able to see intermediate values.  Registers are private to
>>>>>> the hardware thread.  So, we can use multiple instructions to load the
>>>>>> ecx:ebx registers and then execute the cmpxchg8b to do a single write to
>>>>>> globally visible cache.
>>>>>>
>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>> 602.333.9091
>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>> Technology
>>>>>>  On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>>>>>
>>>>>> But this requires the src value to be in ecx:ebx so how would you
>>>>>> load it there without two loads (and possibly observe tearing) in the first
>>>>>> place?
>>>>>>
>>>>>> Sent from my phone
>>>>>> On Apr 30, 2013 12:45 PM, "Nathan Reynolds" <
>>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>>
>>>>>>>  On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>>>>>> instruction.  This instruction has been "present on most post-80486
>>>>>>> processors" (Wikipedia).  There might be cheaper ways to write a long but
>>>>>>> there is at least 1 way.
>>>>>>>
>>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>>> 602.333.9091
>>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>>> Technology
>>>>>>>  On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>>>>>
>>>>>>> Curious how x86 would move a long in 1 instruction? There's no
>>>>>>> memory to memory mov so has to go through register, and thus needs 2
>>>>>>> registers (and hence split).  Am I missing something?
>>>>>>>
>>>>>>> Sent from my phone
>>>>>>> On Apr 30, 2013 12:23 PM, "Nathan Reynolds" <
>>>>>>> nathan.reynolds at oracle.com> wrote:
>>>>>>>
>>>>>>>>  You might want to print the assembly using HotSpot (and
>>>>>>>> OpenJDK?).  If the assembly, uses 1 instruction to do the write, then no
>>>>>>>> splitting can ever happen (because alignment takes care of cache line
>>>>>>>> splits).  If the assembly, uses 2 instructions to do the write, then it is
>>>>>>>> only a matter of timing.
>>>>>>>>
>>>>>>>> With a single processor system, you are waiting for the thread's
>>>>>>>> quantum to end right after the first instruction but before the second
>>>>>>>> instruction.  This will allow the other thread to see the split write.
>>>>>>>>
>>>>>>>> With a dual processor system, the reader thread simply has to get a
>>>>>>>> copy of the cache line after the first write and before the second write.
>>>>>>>> This is much easier to do.
>>>>>>>>
>>>>>>>> HotSpot will do a lot of optimizations on single processor
>>>>>>>> systems.  For example, it gets rid of the "lock" prefix in front of atomic
>>>>>>>> instructions since the instruction's execution can't be split.  It also
>>>>>>>> doesn't output memory fences.  Both of these give good performance boosts.
>>>>>>>> I wonder if with one processor, OpenJDK is using 2 instructions to do the
>>>>>>>> write whereas with multiple processors it plays it safe and uses 1
>>>>>>>> instruction.
>>>>>>>>
>>>>>>>> Note: If you disable all of the processors but 1 and then start
>>>>>>>> HotSpot, HotSpot will start in single processor mode.  If you then enable
>>>>>>>> those processors while HotSpot is running, a lot of things break and the
>>>>>>>> JVM will crash.  Because single processor systems are rare, the default
>>>>>>>> might be changed to assume multiple processors unless the command line
>>>>>>>> specifies 1 processor.
>>>>>>>>
>>>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>>>> 602.333.9091
>>>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>>>> Technology
>>>>>>>>  On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>>>>>
>>>>>>>>  Aleksey, correct -- more trials show what you predicted. Thanks
>>>>>>>> for the nudge.
>>>>>>>>
>>>>>>>>  Mark,
>>>>>>>>
>>>>>>>>  Very helpful, in fact, we are seeing quick failures except for
>>>>>>>> the dual-processor case -- on a dual processor hardware or VM (Virtual Box)
>>>>>>>> we have yet to get a failure.  The two programs attached are what I'm
>>>>>>>> running.  I stripped out my benchmark framework (so they are easy to run on
>>>>>>>> OpenJDK but not on Android).  The difference is that one uses two threads
>>>>>>>> (one writer one reader) the other three (two writers one reader) -- both
>>>>>>>> seem to produce similar results.
>>>>>>>>
>>>>>>>>  With one processor, OpenJDK 1.6.0_27 I see the split write almost
>>>>>>>> immediatly. Dual we can't get a failure, yet, we get more failures as the
>>>>>>>> processor count goes up -- but after a few failures, we don't get any more
>>>>>>>> (they program tries to get 10 to happen)...we can't get to 10.
>>>>>>>>
>>>>>>>>  It seems that while this can happen on OpenJDK it is rarer than
>>>>>>>> on Android where ten failures takes less than a second to happen.
>>>>>>>>
>>>>>>>>  Best, Tim
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton <
>>>>>>>> mthornton at optrak.com> wrote:
>>>>>>>>
>>>>>>>>>   On 30/04/13 15:36, Tim Halloran wrote:
>>>>>>>>>
>>>>>>>>> On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev <
>>>>>>>>> aleksey.shipilev at oracle.com> wrote:
>>>>>>>>>
>>>>>>>>>> Yes, that's exactly what I had in mind:
>>>>>>>>>>  a. Declare "long a"
>>>>>>>>>>  b. Ramp up two threads.
>>>>>>>>>>  c. Make thread 1 write 0L and -1L over and over to field $a
>>>>>>>>>>  d. Make thread 2 observe the field a, and count the observed
>>>>>>>>>> values
>>>>>>>>>>  e. ...
>>>>>>>>>>  f. PROFIT!
>>>>>>>>>>
>>>>>>>>>> P.S. It is important to do some action on value read in thread 2,
>>>>>>>>>> so
>>>>>>>>>> that it does not hoisted from the loop, since $a is not supposed
>>>>>>>>>> to be
>>>>>>>>>> volatile.
>>>>>>>>>>
>>>>>>>>>> -Aleksey.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>  This discussion is getting a bit far afield, I guess, but to get
>>>>>>>>> back onto the topic. I followed Aleksey's advice. And wrote an
>>>>>>>>> implementation that tests this.  I used two separate threads to write 0L
>>>>>>>>> and -1L into the long field "a" but that is the only real change I made. (I
>>>>>>>>> already had some scaffolding code to run things on Android or desktop Java).
>>>>>>>>>
>>>>>>>>>  *Android: splits writes to longs into two parts.*
>>>>>>>>>
>>>>>>>>>  On a Samsung Galaxy II with Android 4.0.4  a Nexus 4 phone with
>>>>>>>>> Android 4.2.2 I saw non-atomic treatment of long. The value -4294967296
>>>>>>>>> (xFFFFFFFF00000000) showed up as well as 4294967295 (x00000000FFFFFFFF).
>>>>>>>>>
>>>>>>>>>  So looks like Android does not follow the (albeit optional)
>>>>>>>>> advice in the Java language specification about this.
>>>>>>>>>
>>>>>>>>>  *JDK: DOES NOT split writes to longs into two parts (even 32-bit
>>>>>>>>> implementations)*
>>>>>>>>>
>>>>>>>>>  Of course we couldn't get this to happen on any 64-bit JVM, but
>>>>>>>>> we tried it out under Linux on 32-bit OpenJDK 1.7.0_21 it does NOT happen.
>>>>>>>>> The 32-bit JVM implementations follow the recommendation of the Java
>>>>>>>>> language specification.
>>>>>>>>>
>>>>>>>>>  An interesting curio. I wonder how many crashes in "working"
>>>>>>>>> Java code moved from desktop Java onto Android programmers are going to
>>>>>>>>> lose sleep tracking down this one.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>  Last time I tried this sort of test, a split write would be
>>>>>>>>> observed in under a second on a true dual processor. However, with only one
>>>>>>>>> processor available, it would typically take around 20 minutes. So you
>>>>>>>>> might have to run a very long test to have any real confidence in the lack
>>>>>>>>> of splitting.
>>>>>>>>>
>>>>>>>>> Mark Thornton
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/eca11b33/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Apr 30 14:52:14 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 30 Apr 2013 11:52:14 -0700
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<517FE2C0.7010101@optrak.com>
	<CAMyLHFwN_e7tYfhjdF1iB=YrmNAE+z1KCWEuQWCMqf2VQA-_ag@mail.gmail.com>
	<517FEEA9.7080403@oracle.com>
	<CAHjP37EetFNwPnYw2xLGxP_Dd-rMT==8-cuoD5QqiZsq3ULNYw@mail.gmail.com>
	<517FF539.5000303@oracle.com>
	<CAHjP37G=OAibY9f-2RdJVigBSKoBUcU=yzjCHkSNNwR=QO262g@mail.gmail.com>
	<517FF828.20103@oracle.com>
	<CAHjP37GmMJfxWTN99pLN1a3d5yoUGX3zYm3j7hdgCXgqNOiZ2w@mail.gmail.com>
Message-ID: <518012DE.1030100@oracle.com>

You could use cmpxchg8b for reading as well since it returns the 
original value in the memory location.  Simply set the expected and 
update values to be the same so that the memory location is not changed.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 4/30/2013 10:05 AM, Vitaly Davidovich wrote:
>
> Right, writes would be atomic but not reads; read one half, another 
> core updates the value, read 2nd half from different value now.
>
> As for SSE, yeah it's possible, but is that true? JIT skips integer 
> registers for scalar long operations? I find that hard to believe as 
> it would miss out on large register file/renaming opportunities.
>
> Sent from my phone
>
> On Apr 30, 2013 12:58 PM, "Nathan Reynolds" 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     The processor can do whatever it wants in registers without other
>     threads being able to see intermediate values.  Registers are
>     private to the hardware thread.  So, we can use multiple
>     instructions to load the ecx:ebx registers and then execute the
>     cmpxchg8b to do a single write to globally visible cache.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 4/30/2013 9:53 AM, Vitaly Davidovich wrote:
>>
>>     But this requires the src value to be in ecx:ebx so how would you
>>     load it there without two loads (and possibly observe tearing) in
>>     the first place?
>>
>>     Sent from my phone
>>
>>     On Apr 30, 2013 12:45 PM, "Nathan Reynolds"
>>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>>     wrote:
>>
>>         On 32-bit x86, the cmpxchg8b can be used to write a long in 1
>>         instruction.  This instruction has been "present on most
>>         post-80486 processors" (Wikipedia).  There might be cheaper
>>         ways to write a long but there is at least 1 way.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>         Architect | 602.333.9091 <tel:602.333.9091>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>         Technology
>>         On 4/30/2013 9:37 AM, Vitaly Davidovich wrote:
>>>
>>>         Curious how x86 would move a long in 1 instruction? There's
>>>         no memory to memory mov so has to go through register, and
>>>         thus needs 2 registers (and hence split).  Am I missing
>>>         something?
>>>
>>>         Sent from my phone
>>>
>>>         On Apr 30, 2013 12:23 PM, "Nathan Reynolds"
>>>         <nathan.reynolds at oracle.com
>>>         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>
>>>             You might want to print the assembly using HotSpot (and
>>>             OpenJDK?).  If the assembly, uses 1 instruction to do
>>>             the write, then no splitting can ever happen (because
>>>             alignment takes care of cache line splits).  If the
>>>             assembly, uses 2 instructions to do the write, then it
>>>             is only a matter of timing.
>>>
>>>             With a single processor system, you are waiting for the
>>>             thread's quantum to end right after the first
>>>             instruction but before the second instruction.  This
>>>             will allow the other thread to see the split write.
>>>
>>>             With a dual processor system, the reader thread simply
>>>             has to get a copy of the cache line after the first
>>>             write and before the second write.  This is much easier
>>>             to do.
>>>
>>>             HotSpot will do a lot of optimizations on single
>>>             processor systems.  For example, it gets rid of the
>>>             "lock" prefix in front of atomic instructions since the
>>>             instruction's execution can't be split. It also doesn't
>>>             output memory fences. Both of these give good
>>>             performance boosts.  I wonder if with one processor,
>>>             OpenJDK is using 2 instructions to do the write whereas
>>>             with multiple processors it plays it safe and uses 1
>>>             instruction.
>>>
>>>             Note: If you disable all of the processors but 1 and
>>>             then start HotSpot, HotSpot will start in single
>>>             processor mode.  If you then enable those processors
>>>             while HotSpot is running, a lot of things break and the
>>>             JVM will crash.  Because single processor systems are
>>>             rare, the default might be changed to assume multiple
>>>             processors unless the command line specifies 1 processor.
>>>
>>>             Nathan Reynolds
>>>             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>             | Architect | 602.333.9091 <tel:602.333.9091>
>>>             Oracle PSR Engineering <http://psr.us.oracle.com/> |
>>>             Server Technology
>>>             On 4/30/2013 8:48 AM, Tim Halloran wrote:
>>>>             Aleksey, correct -- more trials show what you
>>>>             predicted. Thanks for the nudge.
>>>>
>>>>             Mark,
>>>>
>>>>             Very helpful, in fact, we are seeing quick failures
>>>>             except for the dual-processor case -- on a dual
>>>>             processor hardware or VM (Virtual Box) we have yet to
>>>>             get a failure.  The two programs attached are what I'm
>>>>             running.  I stripped out my benchmark framework (so
>>>>             they are easy to run on OpenJDK but not on Android).
>>>>              The difference is that one uses two threads (one
>>>>             writer one reader) the other three (two writers one
>>>>             reader) -- both seem to produce similar results.
>>>>
>>>>             With one processor, OpenJDK 1.6.0_27 I see the split
>>>>             write almost immediatly. Dual we can't get a failure,
>>>>             yet, we get more failures as the processor count goes
>>>>             up -- but after a few failures, we don't get any more
>>>>             (they program tries to get 10 to happen)...we can't get
>>>>             to 10.
>>>>
>>>>             It seems that while this can happen on OpenJDK it is
>>>>             rarer than on Android where ten failures takes less
>>>>             than a second to happen.
>>>>
>>>>             Best, Tim
>>>>
>>>>
>>>>
>>>>             On Tue, Apr 30, 2013 at 11:26 AM, Mark Thornton
>>>>             <mthornton at optrak.com <mailto:mthornton at optrak.com>> wrote:
>>>>
>>>>                 On 30/04/13 15:36, Tim Halloran wrote:
>>>>>                 On Mon, Apr 29, 2013 at 4:59 PM, Aleksey Shipilev
>>>>>                 <aleksey.shipilev at oracle.com
>>>>>                 <mailto:aleksey.shipilev at oracle.com>> wrote:
>>>>>
>>>>>                     Yes, that's exactly what I had in mind:
>>>>>                      a. Declare "long a"
>>>>>                      b. Ramp up two threads.
>>>>>                      c. Make thread 1 write 0L and -1L over and
>>>>>                     over to field $a
>>>>>                      d. Make thread 2 observe the field a, and
>>>>>                     count the observed values
>>>>>                      e. ...
>>>>>                      f. PROFIT!
>>>>>
>>>>>                     P.S. It is important to do some action on
>>>>>                     value read in thread 2, so
>>>>>                     that it does not hoisted from the loop, since
>>>>>                     $a is not supposed to be
>>>>>                     volatile.
>>>>>
>>>>>                     -Aleksey.
>>>>>
>>>>>
>>>>>                 This discussion is getting a bit far afield, I
>>>>>                 guess, but to get back onto the topic. I followed
>>>>>                 Aleksey's advice. And wrote an implementation that
>>>>>                 tests this.  I used two separate threads to write
>>>>>                 0L and -1L into the long field "a" but that is the
>>>>>                 only real change I made. (I already had some
>>>>>                 scaffolding code to run things on Android or
>>>>>                 desktop Java).
>>>>>
>>>>>                 *Android: splits writes to longs into two parts.*
>>>>>
>>>>>                 On a Samsung Galaxy II with Android 4.0.4  a Nexus
>>>>>                 4 phone with Android 4.2.2 I saw non-atomic
>>>>>                 treatment of long. The value -4294967296
>>>>>                 (xFFFFFFFF00000000) showed up as well as
>>>>>                 4294967295 (x00000000FFFFFFFF).
>>>>>
>>>>>                 So looks like Android does not follow the (albeit
>>>>>                 optional) advice in the Java language
>>>>>                 specification about this.
>>>>>
>>>>>                 *JDK: DOES NOT split writes to longs into two
>>>>>                 parts (even 32-bit implementations)*
>>>>>
>>>>>                 Of course we couldn't get this to happen on any
>>>>>                 64-bit JVM, but we tried it out under Linux on
>>>>>                 32-bit OpenJDK 1.7.0_21 it does NOT happen. The
>>>>>                 32-bit JVM implementations follow the
>>>>>                 recommendation of the Java language specification.
>>>>>
>>>>>                 An interesting curio. I wonder how many crashes in
>>>>>                 "working" Java code moved from desktop Java onto
>>>>>                 Android programmers are going to lose sleep
>>>>>                 tracking down this one.
>>>>>
>>>>>
>>>>
>>>>                 Last time I tried this sort of test, a split write
>>>>                 would be observed in under a second on a true dual
>>>>                 processor. However, with only one processor
>>>>                 available, it would typically take around 20
>>>>                 minutes. So you might have to run a very long test
>>>>                 to have any real confidence in the lack of splitting.
>>>>
>>>>                 Mark Thornton
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/3f2d7994/attachment-0001.html>

From hans.boehm at hp.com  Tue Apr 30 20:48:01 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 1 May 2013 00:48:01 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>

A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.

Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
Sent: Tuesday, April 30, 2013 5:57 AM
To: Kirk Pepperdine
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.

Martin...
------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
To: Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>>
Cc: concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>> wrote:
> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>
> Gregg
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/fcdf203a/attachment.html>

From gergg at cox.net  Tue Apr 30 22:21:02 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 30 Apr 2013 21:21:02 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
Message-ID: <7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>

The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.  

If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.

Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.

Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?

The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.

This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.

Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.

Gregg Wonderly

On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.
>  
> Applications such as this are naturally data-race-free, thus there is no issue with the compiler ?breaking? code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it?s unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.
>  
> Hans
>  
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
> Sent: Tuesday, April 30, 2013 5:57 AM
> To: Kirk Pepperdine
> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>  
> I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.
>  
> If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.
>  
> Martin...
> ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine <kirk at kodewerk.com>
> To: Gregg Wonderly <gergg at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double  and long : Android
> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
> Content-Type: text/plain; charset="windows-1252"
> Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> > This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
> >
> > Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
> >
> > That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
> >
> > Gregg
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/e1ffd03e/attachment-0001.html>

