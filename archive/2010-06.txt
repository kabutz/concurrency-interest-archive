From brian.brooks at acm.org  Wed Jun  2 04:58:43 2010
From: brian.brooks at acm.org (BBrooks)
Date: Wed, 2 Jun 2010 04:58:43 -0400
Subject: [concurrency-interest] Profiling java.util.concurrent locks - InfoQ
	Article - May 27, 2010
Message-ID: <AANLkTikSizTR8wbLAhuqx1px6fzCPBa0C2HUUvXV99Ih@mail.gmail.com>

Here's an interesting article about an IBM Developerworks tool for
profiling java.util.concurrent locks

Yao Qi, Raja Das, and Zhi Da Luo. May 27, 2010. "Profiling
java.util.concurrent locks"
http://www.infoq.com/articles/jucprofiler

The tool best profiling support is on Linux 64-bit.

The tool is licensed under IBM's very restrictive DeveloperWorks
license.  It's free for evaluation until IBM decides to expire the
license.

-- 
brian.brooks at acm.org

From qiyaoltc at gmail.com  Wed Jun  2 05:22:34 2010
From: qiyaoltc at gmail.com (Yao Qi)
Date: Wed, 2 Jun 2010 17:22:34 +0800
Subject: [concurrency-interest] Profiling java.util.concurrent locks -
	InfoQ Article - May 27, 2010
In-Reply-To: <AANLkTikSizTR8wbLAhuqx1px6fzCPBa0C2HUUvXV99Ih@mail.gmail.com>
References: <AANLkTikSizTR8wbLAhuqx1px6fzCPBa0C2HUUvXV99Ih@mail.gmail.com>
Message-ID: <AANLkTilQkzshJ_JbGKYVg-fn7FxsG-J03luBn2TULeBD@mail.gmail.com>

On Wed, Jun 2, 2010 at 4:58 PM, BBrooks <brian.brooks at acm.org> wrote:
> Here's an interesting article about an IBM Developerworks tool for
> profiling java.util.concurrent locks
>
> Yao Qi, Raja Das, and Zhi Da Luo. May 27, 2010. "Profiling
> java.util.concurrent locks"
> http://www.infoq.com/articles/jucprofiler
>
> The tool best profiling support is on Linux 64-bit.
>
> The tool is licensed under IBM's very restrictive DeveloperWorks
> license. ?It's free for evaluation until IBM decides to expire the
> license.

jucprofiler is part of MulticoreSDK on alphaWorks
(http://www.alphaworks.ibm.com/tech/msdk).  Users can download it and
evaluate it.

-- 
Yao Qi <qiyaoltc AT gmail DOT com>
http://sites.google.com/site/duewayqi/


From stuff at kai.meder.info  Wed Jun  2 12:57:18 2010
From: stuff at kai.meder.info (Kai Meder)
Date: Wed, 02 Jun 2010 18:57:18 +0200
Subject: [concurrency-interest] General Question on queue-size
In-Reply-To: <4C03FE36.50603@cs.oswego.edu>
References: <4C03CCF7.5010306@kai.meder.info> <4C03FE36.50603@cs.oswego.edu>
Message-ID: <4C068D6E.5070709@kai.meder.info>

On 31.05.2010 20:21, Doug Lea wrote:
> On 05/31/10 10:51, Kai Meder wrote:
>> However is the following not possible either?
>> - offer increments an offerCounter
>> - poll increments a pollCounter
>> - estimatedSize = offerCounter - pollCounter
>>
>> Is this practicable?
>
> Not especially, although it depends on expected producer and
> consumer loads.
>
> Usually the best solution along these lines is to include a
> serial number in each node, and arrange serial for each node is
> one plus its predecessor (recomputing on missed attempts to link). You
> can then determine length as last.serial - head.serial.

What is the difference between offerCounter - pollCounter vs last.serial
- head.serial?

Is the "serial" the better solution because it is easier to compute in
constrast to the counters, which must be written in an atomic
poll/offer-block ?

Thanks a lot!

From martinrb at google.com  Wed Jun  2 14:45:01 2010
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 2 Jun 2010 11:45:01 -0700
Subject: [concurrency-interest] General Question on queue-size
In-Reply-To: <4C068D6E.5070709@kai.meder.info>
References: <4C03CCF7.5010306@kai.meder.info> <4C03FE36.50603@cs.oswego.edu>
	<4C068D6E.5070709@kai.meder.info>
Message-ID: <AANLkTinJG3bZo2ONKajKTkninAW0uaKpe2_kPbtEF6Ky@mail.gmail.com>

On Wed, Jun 2, 2010 at 09:57, Kai Meder <stuff at kai.meder.info> wrote:
> On 31.05.2010 20:21, Doug Lea wrote:
>> On 05/31/10 10:51, Kai Meder wrote:
>>> However is the following not possible either?
>>> - offer increments an offerCounter
>>> - poll increments a pollCounter
>>> - estimatedSize = offerCounter - pollCounter
>>>
>>> Is this practicable?
>>
>> Not especially, although it depends on expected producer and
>> consumer loads.
>>
>> Usually the best solution along these lines is to include a
>> serial number in each node, and arrange serial for each node is
>> one plus its predecessor (recomputing on missed attempts to link). You
>> can then determine length as last.serial - head.serial.
>
> What is the difference between offerCounter - pollCounter vs last.serial
> - head.serial?
>
> Is the "serial" the better solution because it is easier to compute in
> constrast to the counters, which must be written in an atomic
> poll/offer-block ?

It's cheaper to maintain.  You can write the serial field
using a non-volatile write, which is much cheaper than
updating an atomic integer.

In either case you must also account for interior deletions,
e.g. with remove(Object).

Martin

> Thanks a lot!
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From stuff at kai.meder.info  Wed Jun  2 20:39:23 2010
From: stuff at kai.meder.info (Kai Meder)
Date: Thu, 03 Jun 2010 02:39:23 +0200
Subject: [concurrency-interest] General Question on queue-size
In-Reply-To: <AANLkTinJG3bZo2ONKajKTkninAW0uaKpe2_kPbtEF6Ky@mail.gmail.com>
References: <4C03CCF7.5010306@kai.meder.info>	<4C03FE36.50603@cs.oswego.edu>	<4C068D6E.5070709@kai.meder.info>
	<AANLkTinJG3bZo2ONKajKTkninAW0uaKpe2_kPbtEF6Ky@mail.gmail.com>
Message-ID: <4C06F9BB.6000909@kai.meder.info>

>> What is the difference between offerCounter - pollCounter vs last.serial
>> - head.serial?
>>
>> Is the "serial" the better solution because it is easier to compute in
>> constrast to the counters, which must be written in an atomic
>> poll/offer-block ?
> 
> It's cheaper to maintain.  You can write the serial field
> using a non-volatile write, which is much cheaper than
> updating an atomic integer.
> 
> In either case you must also account for interior deletions,
> e.g. with remove(Object).

What if I implement a Queue with the "Two-Lock Concurrent Queue
Algorithm" [1], using two locks, instead of the CAS-algorithm.

Is keeping the serials still better? At least I do not need to account
for interior inserts/deletions.

And generally, what about the serial overflowing as an Integer?

Very general question: Is there any good literature regarding this
discussion of concurrent data-structures?

[1]
http://www.cs.rochester.edu/research/synchronization/pseudocode/queues.html

From martinrb at google.com  Wed Jun  2 20:52:56 2010
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 2 Jun 2010 17:52:56 -0700
Subject: [concurrency-interest] General Question on queue-size
In-Reply-To: <4C06F9BB.6000909@kai.meder.info>
References: <4C03CCF7.5010306@kai.meder.info> <4C03FE36.50603@cs.oswego.edu>
	<4C068D6E.5070709@kai.meder.info>
	<AANLkTinJG3bZo2ONKajKTkninAW0uaKpe2_kPbtEF6Ky@mail.gmail.com>
	<4C06F9BB.6000909@kai.meder.info>
Message-ID: <AANLkTinK5eAtjODMZsyGw0svkY1TAAOy1GDdjscQYcR8@mail.gmail.com>

On Wed, Jun 2, 2010 at 17:39, Kai Meder <stuff at kai.meder.info> wrote:
>>> What is the difference between offerCounter - pollCounter vs last.serial
>>> - head.serial?
>>>
>>> Is the "serial" the better solution because it is easier to compute in
>>> constrast to the counters, which must be written in an atomic
>>> poll/offer-block ?
>>
>> It's cheaper to maintain. ?You can write the serial field
>> using a non-volatile write, which is much cheaper than
>> updating an atomic integer.
>>
>> In either case you must also account for interior deletions,
>> e.g. with remove(Object).
>
> What if I implement a Queue with the "Two-Lock Concurrent Queue
> Algorithm" [1], using two locks, instead of the CAS-algorithm.

That's already been done for you, in LinkedBlockingQueue.
And that one has a size field with a O(1) size method.

> Is keeping the serials still better? At least I do not need to account
> for interior inserts/deletions.
>
> And generally, what about the serial overflowing as an Integer?

You *embrace* overflow, and only ever use a difference of two serials.

Or use a long instead of int, and don't support users who do nothing
but hammer your poor little queue for 292 years.

Martin


From stuff at kai.meder.info  Wed Jun  2 21:23:43 2010
From: stuff at kai.meder.info (Kai Meder)
Date: Thu, 03 Jun 2010 03:23:43 +0200
Subject: [concurrency-interest] General Question on queue-size
In-Reply-To: <AANLkTinK5eAtjODMZsyGw0svkY1TAAOy1GDdjscQYcR8@mail.gmail.com>
References: <4C03CCF7.5010306@kai.meder.info>	<4C03FE36.50603@cs.oswego.edu>	<4C068D6E.5070709@kai.meder.info>	<AANLkTinJG3bZo2ONKajKTkninAW0uaKpe2_kPbtEF6Ky@mail.gmail.com>	<4C06F9BB.6000909@kai.meder.info>
	<AANLkTinK5eAtjODMZsyGw0svkY1TAAOy1GDdjscQYcR8@mail.gmail.com>
Message-ID: <4C07041F.7090305@kai.meder.info>

On 03.06.2010 02:52, Martin Buchholz wrote:
>> What if I implement a Queue with the "Two-Lock Concurrent Queue
>> Algorithm" [1], using two locks, instead of the CAS-algorithm.
> 
> That's already been done for you, in LinkedBlockingQueue.
> And that one has a size field with a O(1) size method.
I have to implement a custom Queue for use in a
Continuation-Passing-Style-Context in Scala.
That's why I am not able to simply use the existing juc-classes. Thanks
for mentioning it however, I will have a deeper look into it.

> You *embrace* overflow, and only ever use a difference of two serials.
> 
> Or use a long instead of int, and don't support users who do nothing
> but hammer your poor little queue for 292 years.
made my day ;)

Thanks a lot!
 Kai

From avoss at cs.st-andrews.ac.uk  Fri Jun  4 02:59:31 2010
From: avoss at cs.st-andrews.ac.uk (Alex Voss)
Date: Fri, 4 Jun 2010 07:59:31 +0100
Subject: [concurrency-interest] lower than expected level of concurrency
Message-ID: <4C08A453.4020900@cs.st-andrews.ac.uk>


Dear all,

a colleague and I are writing code to parallelise a social simulation 
and am using Executors.newFixedThreadPool(noThreads) and code that 
creates tasks that independently work on each of a partition of a data 
structure. Now, the code seems to work fine except that if we look at 
the execution with a profiler it seems that at any point in time only 
two of the threads in the pool pick up tasks while the others are 
waiting on the task queue (see stack trace below).

Any ideas why? I include the relevant snippets of code below and can 
make the whole lot available is if anyone is interested to have a closer 
look.

Oh, we tested this on a number of machines (Intel Linux and Mac) and a 
number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a 
ThreadInfoDumper that dumps out thread information regularly and 
confirms what the JProfiler was telling us.

Best wishes,

Alex

Here is the example stack trace:
--8<--
sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
java.lang.Thread.run(Thread.java:637)
--8<--

Here are the relevant sections of code:

--8<--
private static final int DEFAULT_NO_THREADS = 16;
private static ExecutorService exec = null;

DemographicsContext(String name, int noThreads) {
   super(name);
   exec = Executors.newFixedThreadPool(noThreads);
}

public void step() {
   ArrayList<Task> tasks = new ArrayList<Task>();
   for(Iterator<Person> i: getIterators()) {
     Task task = new Task(i);
     tasks.add(task);						
   }
   try {
     exec.invokeAll(tasks);
   } catch(InterruptedException e) {}
}

class Task implements Callable<Integer>  {
   Iterator<Person> i = null;
	
   Task(Iterator<Person> i) {
     this.i = i;
   }
	
   public Integer call() {
     while(i.hasNext()) {	
       // do stuff()
     }
   }
}

-- 
Alexander Voss
SICSA Advanced Research Fellow
School of Computer Science
University of St Andrews
http://www.cs.st-andrews.ac.uk/~avoss
The University of St Andrews is a charity registered in Scotland : No 
SC013532

From andrew.gomilko at gmail.com  Fri Jun  4 03:49:35 2010
From: andrew.gomilko at gmail.com (Andrew Gomilko)
Date: Fri, 4 Jun 2010 10:49:35 +0300
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <4C08A453.4020900@cs.st-andrews.ac.uk>
References: <4C08A453.4020900@cs.st-andrews.ac.uk>
Message-ID: <AANLkTilogKRDl4rWTMsZDueTvEt9Vr0yqojVJUU4iz_k@mail.gmail.com>

Hi Alex,

I am not sure, but have found here
http://www.docjar.com/docs/api/java/util/concurrent/ThreadPoolExecutor.html
this statement:

Unbounded queues. Using an unbounded queue (for example a
LinkedBlockingQueue without a predefined capacity) will cause new
tasks to wait in the queue when all corePoolSize threads are busy.
Thus, no more than corePoolSize threads will ever be created. (And the
value of the maximumPoolSize therefore doesn't have any effect.)

Maybe it is worth checking passed corePoolSize  value?

On Fri, Jun 4, 2010 at 9:59 AM, Alex Voss <avoss at cs.st-andrews.ac.uk> wrote:
>
> Dear all,
>
> a colleague and I are writing code to parallelise a social simulation and am
> using Executors.newFixedThreadPool(noThreads) and code that creates tasks
> that independently work on each of a partition of a data structure. Now, the
> code seems to work fine except that if we look at the execution with a
> profiler it seems that at any point in time only two of the threads in the
> pool pick up tasks while the others are waiting on the task queue (see stack
> trace below).
>
> Any ideas why? I include the relevant snippets of code below and can make
> the whole lot available is if anyone is interested to have a closer look.
>
> Oh, we tested this on a number of machines (Intel Linux and Mac) and a
> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a
> ThreadInfoDumper that dumps out thread information regularly and confirms
> what the JProfiler was telling us.
>
> Best wishes,
>
> Alex
>
> Here is the example stack trace:
> --8<--
> sun.misc.Unsafe.park(Native Method)
> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
> java.lang.Thread.run(Thread.java:637)
> --8<--
>
> Here are the relevant sections of code:
>
> --8<--
> private static final int DEFAULT_NO_THREADS = 16;
> private static ExecutorService exec = null;
>
> DemographicsContext(String name, int noThreads) {
> ?super(name);
> ?exec = Executors.newFixedThreadPool(noThreads);
> }
>
> public void step() {
> ?ArrayList<Task> tasks = new ArrayList<Task>();
> ?for(Iterator<Person> i: getIterators()) {
> ? ?Task task = new Task(i);
> ? ?tasks.add(task);
> ?}
> ?try {
> ? ?exec.invokeAll(tasks);
> ?} catch(InterruptedException e) {}
> }
>
> class Task implements Callable<Integer> ?{
> ?Iterator<Person> i = null;
>
> ?Task(Iterator<Person> i) {
> ? ?this.i = i;
> ?}
>
> ?public Integer call() {
> ? ?while(i.hasNext()) {
> ? ? ?// do stuff()
> ? ?}
> ?}
> }
>
> --
> Alexander Voss
> SICSA Advanced Research Fellow
> School of Computer Science
> University of St Andrews
> http://www.cs.st-andrews.ac.uk/~avoss
> The University of St Andrews is a charity registered in Scotland : No
> SC013532
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Fri Jun  4 04:07:44 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 4 Jun 2010 18:07:44 +1000
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <4C08A453.4020900@cs.st-andrews.ac.uk>
Message-ID: <NFBBKALFDCPFIDBNKAPCCENAIGAA.davidcholmes@aapt.net.au>

Alex,

How many processors are available? 

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
> Voss
> Sent: Friday, 4 June 2010 5:00 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] lower than expected level of concurrency
> 
> 
> 
> Dear all,
> 
> a colleague and I are writing code to parallelise a social simulation 
> and am using Executors.newFixedThreadPool(noThreads) and code that 
> creates tasks that independently work on each of a partition of a data 
> structure. Now, the code seems to work fine except that if we look at 
> the execution with a profiler it seems that at any point in time only 
> two of the threads in the pool pick up tasks while the others are 
> waiting on the task queue (see stack trace below).
> 
> Any ideas why? I include the relevant snippets of code below and can 
> make the whole lot available is if anyone is interested to have a closer 
> look.
> 
> Oh, we tested this on a number of machines (Intel Linux and Mac) and a 
> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a 
> ThreadInfoDumper that dumps out thread information regularly and 
> confirms what the JProfiler was telling us.
> 
> Best wishes,
> 
> Alex
> 
> Here is the example stack trace:
> --8<--
> sun.misc.Unsafe.park(Native Method)
> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj
> ect.await(AbstractQueuedSynchronizer.java:1925)
> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.
> java:399)
> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor
> .java:947)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecu
> tor.java:907)
> java.lang.Thread.run(Thread.java:637)
> --8<--
> 
> Here are the relevant sections of code:
> 
> --8<--
> private static final int DEFAULT_NO_THREADS = 16;
> private static ExecutorService exec = null;
> 
> DemographicsContext(String name, int noThreads) {
>    super(name);
>    exec = Executors.newFixedThreadPool(noThreads);
> }
> 
> public void step() {
>    ArrayList<Task> tasks = new ArrayList<Task>();
>    for(Iterator<Person> i: getIterators()) {
>      Task task = new Task(i);
>      tasks.add(task);						
>    }
>    try {
>      exec.invokeAll(tasks);
>    } catch(InterruptedException e) {}
> }
> 
> class Task implements Callable<Integer>  {
>    Iterator<Person> i = null;
> 	
>    Task(Iterator<Person> i) {
>      this.i = i;
>    }
> 	
>    public Integer call() {
>      while(i.hasNext()) {	
>        // do stuff()
>      }
>    }
> }
> 
> -- 
> Alexander Voss
> SICSA Advanced Research Fellow
> School of Computer Science
> University of St Andrews
> http://www.cs.st-andrews.ac.uk/~avoss
> The University of St Andrews is a charity registered in Scotland : No 
> SC013532
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From avoss at cs.st-andrews.ac.uk  Fri Jun  4 04:07:57 2010
From: avoss at cs.st-andrews.ac.uk (Alex Voss)
Date: Fri, 4 Jun 2010 09:07:57 +0100
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <AANLkTilogKRDl4rWTMsZDueTvEt9Vr0yqojVJUU4iz_k@mail.gmail.com>
References: <4C08A453.4020900@cs.st-andrews.ac.uk>
	<AANLkTilogKRDl4rWTMsZDueTvEt9Vr0yqojVJUU4iz_k@mail.gmail.com>
Message-ID: <4C08B45D.3000303@cs.st-andrews.ac.uk>


Dear Andrew,

thanks for the prompt reply. The Executors.newFixedThreadPool(int 
nThreads) method creates a ThreadPoolExecutor thus:

--8<--
ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS,
new LinkedBlockingQueue<Runnable>());
--8<--

So corePoolSize == maximumPoolSize, in my case both are 16. Seems 
something else is going on. I am wondering if I should create the 
ThreadPoolExecutor myself and use another implementation of a 
BlockingQueue to see if that changes things since it is where the 
threads seem to be waiting?

Hope I am not getting the wrong end of the stick here...

Cheers,

Alex

On 04/06/2010 08:49, Andrew Gomilko wrote:
> Hi Alex,
>
> I am not sure, but have found here
> http://www.docjar.com/docs/api/java/util/concurrent/ThreadPoolExecutor.html
> this statement:
>
> Unbounded queues. Using an unbounded queue (for example a
> LinkedBlockingQueue without a predefined capacity) will cause new
> tasks to wait in the queue when all corePoolSize threads are busy.
> Thus, no more than corePoolSize threads will ever be created. (And the
> value of the maximumPoolSize therefore doesn't have any effect.)
>
> Maybe it is worth checking passed corePoolSize  value?
>
> On Fri, Jun 4, 2010 at 9:59 AM, Alex Voss<avoss at cs.st-andrews.ac.uk>  wrote:
>>
>> Dear all,
>>
>> a colleague and I are writing code to parallelise a social simulation and am
>> using Executors.newFixedThreadPool(noThreads) and code that creates tasks
>> that independently work on each of a partition of a data structure. Now, the
>> code seems to work fine except that if we look at the execution with a
>> profiler it seems that at any point in time only two of the threads in the
>> pool pick up tasks while the others are waiting on the task queue (see stack
>> trace below).
>>
>> Any ideas why? I include the relevant snippets of code below and can make
>> the whole lot available is if anyone is interested to have a closer look.
>>
>> Oh, we tested this on a number of machines (Intel Linux and Mac) and a
>> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a
>> ThreadInfoDumper that dumps out thread information regularly and confirms
>> what the JProfiler was telling us.
>>
>> Best wishes,
>>
>> Alex
>>
>> Here is the example stack trace:
>> --8<--
>> sun.misc.Unsafe.park(Native Method)
>> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
>> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
>> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
>> java.lang.Thread.run(Thread.java:637)
>> --8<--
>>
>> Here are the relevant sections of code:
>>
>> --8<--
>> private static final int DEFAULT_NO_THREADS = 16;
>> private static ExecutorService exec = null;
>>
>> DemographicsContext(String name, int noThreads) {
>>   super(name);
>>   exec = Executors.newFixedThreadPool(noThreads);
>> }
>>
>> public void step() {
>>   ArrayList<Task>  tasks = new ArrayList<Task>();
>>   for(Iterator<Person>  i: getIterators()) {
>>     Task task = new Task(i);
>>     tasks.add(task);
>>   }
>>   try {
>>     exec.invokeAll(tasks);
>>   } catch(InterruptedException e) {}
>> }
>>
>> class Task implements Callable<Integer>    {
>>   Iterator<Person>  i = null;
>>
>>   Task(Iterator<Person>  i) {
>>     this.i = i;
>>   }
>>
>>   public Integer call() {
>>     while(i.hasNext()) {
>>       // do stuff()
>>     }
>>   }
>> }
>>
>> --
>> Alexander Voss
>> SICSA Advanced Research Fellow
>> School of Computer Science
>> University of St Andrews
>> http://www.cs.st-andrews.ac.uk/~avoss
>> The University of St Andrews is a charity registered in Scotland : No
>> SC013532
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

-- 
Alexander Voss
SICSA Advanced Research Fellow
School of Computer Science
University of St Andrews
http://www.cs.st-andrews.ac.uk/~avoss
The University of St Andrews is a charity registered in Scotland : No 
SC013532

From avoss at cs.st-andrews.ac.uk  Fri Jun  4 04:20:21 2010
From: avoss at cs.st-andrews.ac.uk (Alex Voss)
Date: Fri, 4 Jun 2010 09:20:21 +0100
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCENAIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCENAIGAA.davidcholmes@aapt.net.au>
Message-ID: <4C08B745.8040402@cs.st-andrews.ac.uk>


Dear David,

the largest we have is a dual processor quad core (Xeon E5504). Also 
tried on single processor quad core and Core-2 Macbook.

Cheers,

Alex

On 04/06/2010 09:07, David Holmes wrote:
> Alex,
>
> How many processors are available?
>
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
>> Voss
>> Sent: Friday, 4 June 2010 5:00 PM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] lower than expected level of concurrency
>>
>>
>>
>> Dear all,
>>
>> a colleague and I are writing code to parallelise a social simulation
>> and am using Executors.newFixedThreadPool(noThreads) and code that
>> creates tasks that independently work on each of a partition of a data
>> structure. Now, the code seems to work fine except that if we look at
>> the execution with a profiler it seems that at any point in time only
>> two of the threads in the pool pick up tasks while the others are
>> waiting on the task queue (see stack trace below).
>>
>> Any ideas why? I include the relevant snippets of code below and can
>> make the whole lot available is if anyone is interested to have a closer
>> look.
>>
>> Oh, we tested this on a number of machines (Intel Linux and Mac) and a
>> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a
>> ThreadInfoDumper that dumps out thread information regularly and
>> confirms what the JProfiler was telling us.
>>
>> Best wishes,
>>
>> Alex
>>
>> Here is the example stack trace:
>> --8<--
>> sun.misc.Unsafe.park(Native Method)
>> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj
>> ect.await(AbstractQueuedSynchronizer.java:1925)
>> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.
>> java:399)
>> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor
>> .java:947)
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecu
>> tor.java:907)
>> java.lang.Thread.run(Thread.java:637)
>> --8<--
>>
>> Here are the relevant sections of code:
>>
>> --8<--
>> private static final int DEFAULT_NO_THREADS = 16;
>> private static ExecutorService exec = null;
>>
>> DemographicsContext(String name, int noThreads) {
>>     super(name);
>>     exec = Executors.newFixedThreadPool(noThreads);
>> }
>>
>> public void step() {
>>     ArrayList<Task>  tasks = new ArrayList<Task>();
>>     for(Iterator<Person>  i: getIterators()) {
>>       Task task = new Task(i);
>>       tasks.add(task);						
>>     }
>>     try {
>>       exec.invokeAll(tasks);
>>     } catch(InterruptedException e) {}
>> }
>>
>> class Task implements Callable<Integer>   {
>>     Iterator<Person>  i = null;
>> 	
>>     Task(Iterator<Person>  i) {
>>       this.i = i;
>>     }
>> 	
>>     public Integer call() {
>>       while(i.hasNext()) {	
>>         // do stuff()
>>       }
>>     }
>> }
>>
>> --
>> Alexander Voss
>> SICSA Advanced Research Fellow
>> School of Computer Science
>> University of St Andrews
>> http://www.cs.st-andrews.ac.uk/~avoss
>> The University of St Andrews is a charity registered in Scotland : No
>> SC013532
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Alexander Voss
SICSA Advanced Research Fellow
School of Computer Science
University of St Andrews
http://www.cs.st-andrews.ac.uk/~avoss
The University of St Andrews is a charity registered in Scotland : No 
SC013532

From davidcholmes at aapt.net.au  Fri Jun  4 06:13:32 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 4 Jun 2010 20:13:32 +1000
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <4C08B745.8040402@cs.st-andrews.ac.uk>
Message-ID: <NFBBKALFDCPFIDBNKAPCCENBIGAA.davidcholmes@aapt.net.au>

> the largest we have is a dual processor quad core (Xeon E5504). Also
> tried on single processor quad core and Core-2 Macbook.

So how many active threads do you expect to see? How long are your tasks?
How are they submitted to the pool?

Depending on all these things and more there are various patterns that can
occur for the execution. For example, you have one thread doing the
submission, after the first submission a worker can start working, if you
have two processors the main thread can submit another task and start a new
worker - but now you've used up all processors, one thread has to exhaust
its quantum or block before the other will get a chance to run. So let the
worker thread finish the first task and go back to pool - it finds no work
and waits. This lets the next submission happen, and so the worker can then
resume working again. Meanwhile the second worker completes its task and
waits. If you have enough processors the submitter might keep a small group
of workers going, depending on the task size.

Actual scheduling patterns can often be very surprising in practice.

Cheers,
David Holmes

> Cheers,
>
> Alex
>
> On 04/06/2010 09:07, David Holmes wrote:
> > Alex,
> >
> > How many processors are available?
> >
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
> >> Voss
> >> Sent: Friday, 4 June 2010 5:00 PM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] lower than expected level of
> concurrency
> >>
> >>
> >>
> >> Dear all,
> >>
> >> a colleague and I are writing code to parallelise a social simulation
> >> and am using Executors.newFixedThreadPool(noThreads) and code that
> >> creates tasks that independently work on each of a partition of a data
> >> structure. Now, the code seems to work fine except that if we look at
> >> the execution with a profiler it seems that at any point in time only
> >> two of the threads in the pool pick up tasks while the others are
> >> waiting on the task queue (see stack trace below).
> >>
> >> Any ideas why? I include the relevant snippets of code below and can
> >> make the whole lot available is if anyone is interested to
> have a closer
> >> look.
> >>
> >> Oh, we tested this on a number of machines (Intel Linux and Mac) and a
> >> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a
> >> ThreadInfoDumper that dumps out thread information regularly and
> >> confirms what the JProfiler was telling us.
> >>
> >> Best wishes,
> >>
> >> Alex
> >>
> >> Here is the example stack trace:
> >> --8<--
> >> sun.misc.Unsafe.park(Native Method)
> >> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
> >> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj
> >> ect.await(AbstractQueuedSynchronizer.java:1925)
> >> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.
> >> java:399)
> >> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor
> >> .java:947)
> >> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecu
> >> tor.java:907)
> >> java.lang.Thread.run(Thread.java:637)
> >> --8<--
> >>
> >> Here are the relevant sections of code:
> >>
> >> --8<--
> >> private static final int DEFAULT_NO_THREADS = 16;
> >> private static ExecutorService exec = null;
> >>
> >> DemographicsContext(String name, int noThreads) {
> >>     super(name);
> >>     exec = Executors.newFixedThreadPool(noThreads);
> >> }
> >>
> >> public void step() {
> >>     ArrayList<Task>  tasks = new ArrayList<Task>();
> >>     for(Iterator<Person>  i: getIterators()) {
> >>       Task task = new Task(i);
> >>       tasks.add(task);
> >>     }
> >>     try {
> >>       exec.invokeAll(tasks);
> >>     } catch(InterruptedException e) {}
> >> }
> >>
> >> class Task implements Callable<Integer>   {
> >>     Iterator<Person>  i = null;
> >>
> >>     Task(Iterator<Person>  i) {
> >>       this.i = i;
> >>     }
> >>
> >>     public Integer call() {
> >>       while(i.hasNext()) {
> >>         // do stuff()
> >>       }
> >>     }
> >> }
> >>
> >> --
> >> Alexander Voss
> >> SICSA Advanced Research Fellow
> >> School of Computer Science
> >> University of St Andrews
> >> http://www.cs.st-andrews.ac.uk/~avoss
> >> The University of St Andrews is a charity registered in Scotland : No
> >> SC013532
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Alexander Voss
> SICSA Advanced Research Fellow
> School of Computer Science
> University of St Andrews
> http://www.cs.st-andrews.ac.uk/~avoss
> The University of St Andrews is a charity registered in Scotland : No
> SC013532


From avoss at cs.st-andrews.ac.uk  Fri Jun  4 08:51:00 2010
From: avoss at cs.st-andrews.ac.uk (Alex Voss)
Date: Fri, 4 Jun 2010 13:51:00 +0100
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCENBIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCENBIGAA.davidcholmes@aapt.net.au>
Message-ID: <4C08F6B4.9020603@cs.st-andrews.ac.uk>


Dear David,

thanks, I think you have given me the right hint. I artifically 
increased the running time of each task by a signficant amount and the 
picture looks a lot brighter now. The threads are now all picking up 
tasks and except for the off lock contention in my simulation code are 
making good progress. It seems that before this a thread would 
monopolise the task queue. Not sure why this is but am happy that the 
problem turned out that my code was executing too fast ;o)

Cheers,

Alex

On 04/06/2010 11:13, David Holmes wrote:
>> the largest we have is a dual processor quad core (Xeon E5504). Also
>> tried on single processor quad core and Core-2 Macbook.
>
> So how many active threads do you expect to see? How long are your tasks?
> How are they submitted to the pool?
>
> Depending on all these things and more there are various patterns that can
> occur for the execution. For example, you have one thread doing the
> submission, after the first submission a worker can start working, if you
> have two processors the main thread can submit another task and start a new
> worker - but now you've used up all processors, one thread has to exhaust
> its quantum or block before the other will get a chance to run. So let the
> worker thread finish the first task and go back to pool - it finds no work
> and waits. This lets the next submission happen, and so the worker can then
> resume working again. Meanwhile the second worker completes its task and
> waits. If you have enough processors the submitter might keep a small group
> of workers going, depending on the task size.
>
> Actual scheduling patterns can often be very surprising in practice.
>
> Cheers,
> David Holmes
>
>> Cheers,
>>
>> Alex
>>
>> On 04/06/2010 09:07, David Holmes wrote:
>>> Alex,
>>>
>>> How many processors are available?
>>>
>>> David Holmes
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Alex
>>>> Voss
>>>> Sent: Friday, 4 June 2010 5:00 PM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] lower than expected level of
>> concurrency
>>>>
>>>>
>>>>
>>>> Dear all,
>>>>
>>>> a colleague and I are writing code to parallelise a social simulation
>>>> and am using Executors.newFixedThreadPool(noThreads) and code that
>>>> creates tasks that independently work on each of a partition of a data
>>>> structure. Now, the code seems to work fine except that if we look at
>>>> the execution with a profiler it seems that at any point in time only
>>>> two of the threads in the pool pick up tasks while the others are
>>>> waiting on the task queue (see stack trace below).
>>>>
>>>> Any ideas why? I include the relevant snippets of code below and can
>>>> make the whole lot available is if anyone is interested to
>> have a closer
>>>> look.
>>>>
>>>> Oh, we tested this on a number of machines (Intel Linux and Mac) and a
>>>> number of JVMs (Sun, OpenJDK, Apple, both 1.5 and 1.6), also wrote a
>>>> ThreadInfoDumper that dumps out thread information regularly and
>>>> confirms what the JProfiler was telling us.
>>>>
>>>> Best wishes,
>>>>
>>>> Alex
>>>>
>>>> Here is the example stack trace:
>>>> --8<--
>>>> sun.misc.Unsafe.park(Native Method)
>>>> java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
>>>> java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObj
>>>> ect.await(AbstractQueuedSynchronizer.java:1925)
>>>> java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.
>>>> java:399)
>>>> java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor
>>>> .java:947)
>>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecu
>>>> tor.java:907)
>>>> java.lang.Thread.run(Thread.java:637)
>>>> --8<--
>>>>
>>>> Here are the relevant sections of code:
>>>>
>>>> --8<--
>>>> private static final int DEFAULT_NO_THREADS = 16;
>>>> private static ExecutorService exec = null;
>>>>
>>>> DemographicsContext(String name, int noThreads) {
>>>>      super(name);
>>>>      exec = Executors.newFixedThreadPool(noThreads);
>>>> }
>>>>
>>>> public void step() {
>>>>      ArrayList<Task>   tasks = new ArrayList<Task>();
>>>>      for(Iterator<Person>   i: getIterators()) {
>>>>        Task task = new Task(i);
>>>>        tasks.add(task);
>>>>      }
>>>>      try {
>>>>        exec.invokeAll(tasks);
>>>>      } catch(InterruptedException e) {}
>>>> }
>>>>
>>>> class Task implements Callable<Integer>    {
>>>>      Iterator<Person>   i = null;
>>>>
>>>>      Task(Iterator<Person>   i) {
>>>>        this.i = i;
>>>>      }
>>>>
>>>>      public Integer call() {
>>>>        while(i.hasNext()) {
>>>>          // do stuff()
>>>>        }
>>>>      }
>>>> }
>>>>
>>>> --
>>>> Alexander Voss
>>>> SICSA Advanced Research Fellow
>>>> School of Computer Science
>>>> University of St Andrews
>>>> http://www.cs.st-andrews.ac.uk/~avoss
>>>> The University of St Andrews is a charity registered in Scotland : No
>>>> SC013532
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> --
>> Alexander Voss
>> SICSA Advanced Research Fellow
>> School of Computer Science
>> University of St Andrews
>> http://www.cs.st-andrews.ac.uk/~avoss
>> The University of St Andrews is a charity registered in Scotland : No
>> SC013532
>

-- 
Alexander Voss
SICSA Advanced Research Fellow
School of Computer Science
University of St Andrews
http://www.cs.st-andrews.ac.uk/~avoss
The University of St Andrews is a charity registered in Scotland : No 
SC013532

From andrew.gomilko at gmail.com  Fri Jun  4 13:55:04 2010
From: andrew.gomilko at gmail.com (Andrew Gomilko)
Date: Fri, 4 Jun 2010 20:55:04 +0300
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <4C08F6B4.9020603@cs.st-andrews.ac.uk>
References: <NFBBKALFDCPFIDBNKAPCCENBIGAA.davidcholmes@aapt.net.au>
	<4C08F6B4.9020603@cs.st-andrews.ac.uk>
Message-ID: <AANLkTinscxxSPzhUYa79Q898YwDM8MKWTfe2ajjSJXBs@mail.gmail.com>

Hi Alex,

Probably it is worthless after you have fixed your problem, but I did
a little experiment on this issue.
I wanted to reproduce your problem, but any configuration I tried with
feeding ThreadPoolExecutor
didn't get me to the empty queue. Only thing I realized is a fact that
workers which 'sleeps' at least
for a while increases overall performance.

To test queue size I used ThreadPoolExecutor.getQueue().size(), but it
almost newer was empty.
In case you are interested how I tested it, code is here:
http://pastebin.com/WSWCYnGf with some
debug performance info http://pastebin.com/Zn9C0qFr


On Fri, Jun 4, 2010 at 3:51 PM, Alex Voss <avoss at cs.st-andrews.ac.uk> wrote:
>
> Dear David,
>
> thanks, I think you have given me the right hint. I artifically increased
> the running time of each task by a signficant amount and the picture looks a
> lot brighter now. The threads are now all picking up tasks and except for
> the off lock contention in my simulation code are making good progress. It
> seems that before this a thread would monopolise the task queue. Not sure
> why this is but am happy that the problem turned out that my code was
> executing too fast ;o)
>
> Cheers,
>

From gregg at cytetech.com  Fri Jun  4 15:39:32 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 04 Jun 2010 14:39:32 -0500
Subject: [concurrency-interest] lower than expected level of concurrency
In-Reply-To: <AANLkTinscxxSPzhUYa79Q898YwDM8MKWTfe2ajjSJXBs@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCCENBIGAA.davidcholmes@aapt.net.au>
	<4C08F6B4.9020603@cs.st-andrews.ac.uk>
	<AANLkTinscxxSPzhUYa79Q898YwDM8MKWTfe2ajjSJXBs@mail.gmail.com>
Message-ID: <4C095674.6080309@cytetech.com>

As I tried to convey in an earlier post, my experience with multi-threaded 
programming and work items of fairly constant compute time with minimal "delays" 
will cause threads to congregate around those "delays".  If they are locks, 
contention and single threaded behavior abounds.  If they are more random, such 
as I/O, threads will hover in this area, but if the delay creating operation is 
a spread out resource, such as separate I/O endpoints, then throughput is great.

You really have to focus on how locking and other single resource contention 
impacts your application.  Loading up some test interfaces with way too many 
threads with more longer "contention" times, can help you see how throughput is 
effected.  If you can measure throughput, then starting with one thread, and 
increasing to roughly 2x the number of cores when I/O is the contention should 
show you improved throughput.  After that, you may not see too much additional 
throughput except where external resources, such as databases are injecting huge 
latency that is uncontended.

Gregg Wonderly

Andrew Gomilko wrote:
> Hi Alex,
> 
> Probably it is worthless after you have fixed your problem, but I did
> a little experiment on this issue.
> I wanted to reproduce your problem, but any configuration I tried with
> feeding ThreadPoolExecutor
> didn't get me to the empty queue. Only thing I realized is a fact that
> workers which 'sleeps' at least
> for a while increases overall performance.
> 
> To test queue size I used ThreadPoolExecutor.getQueue().size(), but it
> almost newer was empty.
> In case you are interested how I tested it, code is here:
> http://pastebin.com/WSWCYnGf with some
> debug performance info http://pastebin.com/Zn9C0qFr
> 
> 
> On Fri, Jun 4, 2010 at 3:51 PM, Alex Voss <avoss at cs.st-andrews.ac.uk> wrote:
>> Dear David,
>>
>> thanks, I think you have given me the right hint. I artifically increased
>> the running time of each task by a signficant amount and the picture looks a
>> lot brighter now. The threads are now all picking up tasks and except for
>> the off lock contention in my simulation code are making good progress. It
>> seems that before this a thread would monopolise the task queue. Not sure
>> why this is but am happy that the problem turned out that my code was
>> executing too fast ;o)
>>
>> Cheers,
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From jeffhain at rocketmail.com  Sat Jun  5 17:45:16 2010
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sat, 5 Jun 2010 21:45:16 +0000 (GMT)
Subject: [concurrency-interest] parallelization interfaces
Message-ID: <156651.68272.qm@web29207.mail.ird.yahoo.com>

?? Hello.
?
?
?? I've been doing some parallel processing lately, and ended up?with some parallelization interfaces
(see attached file), for treatments to be agnostic about the underlying parallelization engine.
?
?? I designed them with genericity in mind, but I'm afraid they might be "more paralyzing than parallelizing"
for too manycases, so I'm curious what some of you could think about them and their genericity, maybe
how to complete them, etc.
?

Regards,
?
Jeff



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100605/3ddbae99/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ODK_PRL_INTERFACES.zip
Type: application/x-zip-compressed
Size: 4921 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100605/3ddbae99/attachment.zip>

From alarmnummer at gmail.com  Sun Jun  6 09:02:47 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 6 Jun 2010 15:02:47 +0200
Subject: [concurrency-interest] Performance of AtomicLong.compareAndSet
Message-ID: <AANLkTilpooTeXSHsOk2ecZd9Gc4vjwOxF9113nC2z7e9@mail.gmail.com>

Hi,

I have a question about the AtomicLong.compareAndSet performance.

My big question is: is the performance dominated by fences or the actual
compare and set operation? If you use a compareAndSet, it also provides a
happens before relation between all state modified before the write and the
reads on that state after the AtomicLong.get. But in some cases this is a
high price to pay if you don't need it.

I also looked at the weakCompareAndSet, but it calls the same unsafe logic
the compareAndSet does.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100606/61814358/attachment.html>

From davidcholmes at aapt.net.au  Sun Jun  6 19:50:33 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 7 Jun 2010 09:50:33 +1000
Subject: [concurrency-interest] Performance of AtomicLong.compareAndSet
In-Reply-To: <AANLkTilpooTeXSHsOk2ecZd9Gc4vjwOxF9113nC2z7e9@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMENFIGAA.davidcholmes@aapt.net.au>

Peter,

It depends on the OS and CPU combination, but in general for x86 and sparc
there are no explicit fences/memory-barriers only the underlying hardware
cmpxchg8b/cas instruction. The assumption is that those instructions embody
all the necessary "fences" to ensure the volatile semantics are honoured.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
Veentjer
  Sent: Sunday, 6 June 2010 11:03 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Performance of AtomicLong.compareAndSet


  Hi,

  I have a question about the AtomicLong.compareAndSet performance.

  My big question is: is the performance dominated by fences or the actual
compare and set operation? If you use a compareAndSet, it also provides a
happens before relation between all state modified before the write and the
reads on that state after the AtomicLong.get. But in some cases this is a
high price to pay if you don't need it.

  I also looked at the weakCompareAndSet, but it calls the same unsafe logic
the compareAndSet does.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100607/879c8bbf/attachment.html>

From gregg at wonderly.org  Thu Jun 10 02:11:52 2010
From: gregg at wonderly.org (Gregg Wonderly)
Date: Thu, 10 Jun 2010 01:11:52 -0500
Subject: [concurrency-interest] TPE.execute() doesn't seem to really use
	maxPoolSize effectively
Message-ID: <4C108228.6080102@wonderly.org>

The way that TPE.execute() logic just uses "!workQueue.offer(command)" to 
trigger calls to "addIfUnderMaximumPoolSize(command)" doesn't cause maxPoolSize 
to be used for open ended queues.  This seems to be a pretty significant 
functionality issue.  Shouldn't it really start up to maxPoolSize threads 
anytime that ( corePoolSize < poolSize < maxPoolSize ) so that the additional 
threads always have an opportunity to help in emptying the queue?

Gregg Wonderly

From davidcholmes at aapt.net.au  Thu Jun 10 02:21:51 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 10 Jun 2010 16:21:51 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C108228.6080102@wonderly.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEONIGAA.davidcholmes@aapt.net.au>

Gregg,

I have a real sense of deja-vu here :) There have been many threads on
this - and of course it is part of the docs.

> The way that TPE.execute() logic just uses "!workQueue.offer(command)" to
> trigger calls to "addIfUnderMaximumPoolSize(command)" doesn't
> cause maxPoolSize
> to be used for open ended queues.  This seems to be a pretty significant
> functionality issue.  Shouldn't it really start up to maxPoolSize threads
> anytime that ( corePoolSize < poolSize < maxPoolSize ) so that
> the additional
> threads always have an opportunity to help in emptying the queue?

That's not the way it works. TPE will create up to corepoolsize threads on
job submission and after that will offer to the queue. If the queue is full
then it will increase the threads up to maxPoolSize.

The maxPoolSize only has an effect for bounded queues. If you use an
unbounded queue then you never grow above corepoolsize.

Cheers,
David Holmes


From gregg at wonderly.org  Thu Jun 10 02:57:11 2010
From: gregg at wonderly.org (Gregg Wonderly)
Date: Thu, 10 Jun 2010 01:57:11 -0500
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
 usemaxPoolSize effectively
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEONIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEONIGAA.davidcholmes@aapt.net.au>
Message-ID: <4C108CC7.2070508@wonderly.org>

David Holmes wrote:
> Gregg,
> 
> I have a real sense of deja-vu here :) There have been many threads on
> this - and of course it is part of the docs.
> 
>> The way that TPE.execute() logic just uses "!workQueue.offer(command)" to
>> trigger calls to "addIfUnderMaximumPoolSize(command)" doesn't
>> cause maxPoolSize
>> to be used for open ended queues.  This seems to be a pretty significant
>> functionality issue.  Shouldn't it really start up to maxPoolSize threads
>> anytime that ( corePoolSize < poolSize < maxPoolSize ) so that
>> the additional
>> threads always have an opportunity to help in emptying the queue?
> 
> That's not the way it works. TPE will create up to corepoolsize threads on
> job submission and after that will offer to the queue. If the queue is full
> then it will increase the threads up to maxPoolSize.
> 
> The maxPoolSize only has an effect for bounded queues. If you use an
> unbounded queue then you never grow above corepoolsize.

I've read the javadocs and do recall many of these discussions.  What I trying 
to suggest, is that the behavior of TPE for unbounded queues seems "ineffective" 
regarding the existance of maxPoolSize in the case of unbounded queues.  Having 
JavaDocs discuss unbounded BlockingQueue implementations does provide a way for 
the developer to discover what the current implementation is.

It seems that it really should allow maxPoolSize to help add additional threads 
that can help diminish the extra work, as it does for bounded queues.

Is there a specific part of the current behavior that seems advantageous for 
unbounded queues?

Gregg Wonderly

From davidcholmes at aapt.net.au  Thu Jun 10 03:10:22 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 10 Jun 2010 17:10:22 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C108CC7.2070508@wonderly.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEOOIGAA.davidcholmes@aapt.net.au>

Gregg,

> I've read the javadocs and do recall many of these discussions.
> What I trying to suggest, is that the behavior of TPE for
> unbounded queues seems "ineffective" regarding the existance of
> maxPoolSize in the case of unbounded queues.

I've really nothing to add from those past discussion. The pool is intended
to work under normal load at corePoolSize (to which it ramps up unless
pre-start is used). When an overload condition occurs (defined by there
being more pending/in-process tasks than workers) we use the queue to act as
a buffer - with the expectation that the normal workload will be restored in
the near future. If we're worried about excessive overload then we can use a
bounded queue, and that says "if the queue fills up add more workers up to
maxPoolSize". If we use an unbounded queue we saying we don't expect (or
else don't care about) excessive overload.

The aim is to balance the ability to handle the expected workload, even
under transient overloads, without having excessive thread creation and
without too much thread churn (ie create-work-die-create).

Other designs are possible and this design doesn't allow all possible
policies that people may want, though as discussed in the past there are
ways to achieve some of these through creative TPE chaining.

David

  Having
> JavaDocs discuss unbounded BlockingQueue implementations does
> provide a way for
> the developer to discover what the current implementation is.
>
> It seems that it really should allow maxPoolSize to help add
> additional threads
> that can help diminish the extra work, as it does for bounded queues.
>
> Is there a specific part of the current behavior that seems
> advantageous for unbounded queues?
>
> Gregg Wonderly


From kasper at kav.dk  Thu Jun 10 03:18:42 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 10 Jun 2010 09:18:42 +0200
Subject: [concurrency-interest] Question about ConcurrentHashMap
Message-ID: <4C1091D2.6000408@kav.dk>

Hi,

A quick question about the use of the count field in 
ConcurrentHashMap.Segment. Why do methods get/containsKey/readingValue 
read the field before doing anything. Is it to avoid doing any redundant 
work when count=0 or is it because of memory ordering/visibility issues. 
If it is the latter how come HashIterator  only needs to volatile-read 
the count field when it advances to the next segment?

Also, shouldn?t ValueIterator resort to using readValueUnderLock() if it 
ever encounters a null value?

Cheers
   Kasper

From concurrency-interest at majumdar.org.uk  Thu Jun 10 04:09:09 2010
From: concurrency-interest at majumdar.org.uk (Dibyendu Majumdar)
Date: Thu, 10 Jun 2010 09:09:09 +0100
Subject: [concurrency-interest] Any port of synchronizer framework to other
	langages
Message-ID: <AANLkTikqkBR8n81hOMSDj-wJMjajaAd3jnlA7yEypJeH@mail.gmail.com>

Hi,

I was wondering if anyone has tried porting the synchronizer framework to
languages such as C or C++.

Regards
Dibyendu
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100610/53baf7a7/attachment.html>

From oztalip at gmail.com  Thu Jun 10 04:35:28 2010
From: oztalip at gmail.com (Talip Ozturk)
Date: Thu, 10 Jun 2010 11:35:28 +0300
Subject: [concurrency-interest] ThreadPoolExecutor thread pool is not growing
Message-ID: <AANLkTimPwKW6BM9g14qV8Sg1j0ztIB_0wXZR3mPFzUbS@mail.gmail.com>

Hi guys,

Here is what I am trying to do:
1. If threads are idle for 60 seconds, then the pool should shrink down to zero.
2. if we have tasks in the underlying queue, we should have up to 20
concurrent execution
3. no task should be rejected as we are using LinkedBlockingQueue.

Please see the code below: I would expect 20 of them running
concurrently; not one by one...

1. Is this behavior normal?
2. How can I achieve my own goal?

public static void main(String[] args) throws Exception {
       ThreadPoolExecutor executor = new ThreadPoolExecutor(0, 20,
                                     60L, TimeUnit.SECONDS,
                                     new LinkedBlockingQueue<Runnable>());
       for (int i = 0; i < 100; i++) {
           final int a = i;
           executor.execute(new Runnable() {
               public void run() {
                   System.out.println("running.." + a);
                   try {
                       Thread.sleep(5000);
                   } catch (InterruptedException ignored) {
                   }
               }
           });
       }
}

Thanks,
-talip

From kasper at kav.dk  Thu Jun 10 10:31:14 2010
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 10 Jun 2010 16:31:14 +0200
Subject: [concurrency-interest] ThreadPoolExecutor thread pool is not
 growing
In-Reply-To: <AANLkTimPwKW6BM9g14qV8Sg1j0ztIB_0wXZR3mPFzUbS@mail.gmail.com>
References: <AANLkTimPwKW6BM9g14qV8Sg1j0ztIB_0wXZR3mPFzUbS@mail.gmail.com>
Message-ID: <4C10F732.1000205@kav.dk>

On 10/6/2010 35:10, Talip Ozturk wrote:
> Hi guys,
>
> Here is what I am trying to do:
> 1. If threads are idle for 60 seconds, then the pool should shrink down to zero.
> 2. if we have tasks in the underlying queue, we should have up to 20
> concurrent execution
> 3. no task should be rejected as we are using LinkedBlockingQueue.
>
> Please see the code below: I would expect 20 of them running
> concurrently; not one by one...
>
> 1. Is this behavior normal?
Yes, try reading the javadoc for ThreadPoolExecutor under Queuing.
There has been many questions regarding this behavior. Try, for example, 
to look at the post "TPE.execute() doesn't seem to really use 
maxPoolSize effectively" posted earlier today.

> 2. How can I achieve my own goal?
I believe you are looking for something like:
ThreadPoolExecutor executor = new ThreadPoolExecutor(20, 20,
60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
tpe.allowCoreThreadTimeOut(true);

The last method is only available in Java 6 and upwards though.


Cheers
   Kasper

From gregg at cytetech.com  Thu Jun 10 11:26:28 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 10 Jun 2010 10:26:28 -0500
Subject: [concurrency-interest] ThreadPoolExecutor thread pool is not
 growing
In-Reply-To: <4C10F732.1000205@kav.dk>
References: <AANLkTimPwKW6BM9g14qV8Sg1j0ztIB_0wXZR3mPFzUbS@mail.gmail.com>
	<4C10F732.1000205@kav.dk>
Message-ID: <4C110424.50607@cytetech.com>

As we've discussed here before, TPE's thread starting mechanism matches few 
peoples expectations based on how existing thread pools have been created and 
used.  It really is important, I think, to understand how the mismatch of 
expectations and design repeatedly surprises people.  Software APIs should 
default to "least surprise", and TPE is full of surprises because, while it's 
design is appropriate for how it is documented to behave, expectations are very 
often much different.

As we've seen here, and as I've discussed with others in other places, people 
expect that at most corePoolSize threads would be idle, and up to maxPoolSize 
threads would be added anytime there are more than corePoolSize tasks queued.

The creation of threads from corePoolSize toward maxPoolSize is also weakly 
managed from the perspective that the Queue and the TPE are separate entities.

The user providing the queue to the constructor provides a controlling behavior 
for fixed size vs open ended queues, yet the constructor arguments are not 
appropriately limited or managed based on that.

There could be a "FixedSized" interface it seems to me, and the constructors 
should have specific versions related to this issue so that people get proper 
IDE code completion control of how they use the constructors based on the queue 
behaviors.

Sure there is lots of flexibility in what exists.  It it just full of a lot of 
surprises compared to what many people expect it seems to me.

Gregg Wonderly

Kasper Nielsen wrote:
> On 10/6/2010 35:10, Talip Ozturk wrote:
>> Hi guys,
>>
>> Here is what I am trying to do:
>> 1. If threads are idle for 60 seconds, then the pool should shrink 
>> down to zero.
>> 2. if we have tasks in the underlying queue, we should have up to 20
>> concurrent execution
>> 3. no task should be rejected as we are using LinkedBlockingQueue.
>>
>> Please see the code below: I would expect 20 of them running
>> concurrently; not one by one...
>>
>> 1. Is this behavior normal?
> Yes, try reading the javadoc for ThreadPoolExecutor under Queuing.
> There has been many questions regarding this behavior. Try, for example, 
> to look at the post "TPE.execute() doesn't seem to really use 
> maxPoolSize effectively" posted earlier today.
> 
>> 2. How can I achieve my own goal?
> I believe you are looking for something like:
> ThreadPoolExecutor executor = new ThreadPoolExecutor(20, 20,
> 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
> tpe.allowCoreThreadTimeOut(true);
> 
> The last method is only available in Java 6 and upwards though.
> 
> 
> Cheers
>   Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From barnett.thomas at gmail.com  Thu Jun 10 13:22:48 2010
From: barnett.thomas at gmail.com (Thomas Barnett)
Date: Thu, 10 Jun 2010 18:22:48 +0100
Subject: [concurrency-interest] A question about synchronization piggybacking
Message-ID: <AANLkTin3mkQPsF3_LoXbSSecmGbUW0npq5GGwMLy9Kbg@mail.gmail.com>

Goetz et al. [1] explain piggybacking on synchronization in JCiP,
section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
Avoid excessive synchronization" which I've replicated at [3]. Is it
correct to say that there is an example of piggybacking in that code
listing; the visibility of the variable stopped is piggybacked on the
synchronization of the variable queue? Thanks


[1] Goetz, B. et al., 2006. Java Concurrency in Practice.
Addison-Wesley Professional
[2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
ed. Addison-Wesley, London
[3] http://pastebin.com/46Z2uShJ

From barnett.thomas at gmail.com  Thu Jun 10 13:27:57 2010
From: barnett.thomas at gmail.com (Thomas Barnett)
Date: Thu, 10 Jun 2010 18:27:57 +0100
Subject: [concurrency-interest] A question about synchronization
	piggybacking
In-Reply-To: <AANLkTin3mkQPsF3_LoXbSSecmGbUW0npq5GGwMLy9Kbg@mail.gmail.com>
References: <AANLkTin3mkQPsF3_LoXbSSecmGbUW0npq5GGwMLy9Kbg@mail.gmail.com>
Message-ID: <AANLkTikdJdXlAhSoZ9X3UDMIdZqOo4hshKgsdOLAPV5G@mail.gmail.com>

sorry the link for [3] does not seem to work any more. Below is the
correct link.
http://pastebin.com/PmV5S34J

On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
> Goetz et al. [1] explain piggybacking on synchronization in JCiP,
> section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
> Avoid excessive synchronization" which I've replicated at [3]. Is it
> correct to say that there is an example of piggybacking in that code
> listing; the visibility of the variable stopped is piggybacked on the
> synchronization of the variable queue? Thanks
>
>
> [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
> Addison-Wesley Professional
> [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
> ed. Addison-Wesley, London
> [3] http://pastebin.com/46Z2uShJ
>

From alarmnummer at gmail.com  Thu Jun 10 15:15:51 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 10 Jun 2010 21:15:51 +0200
Subject: [concurrency-interest] A question about synchronization
	piggybacking
In-Reply-To: <AANLkTikdJdXlAhSoZ9X3UDMIdZqOo4hshKgsdOLAPV5G@mail.gmail.com>
References: <AANLkTin3mkQPsF3_LoXbSSecmGbUW0npq5GGwMLy9Kbg@mail.gmail.com>
	<AANLkTikdJdXlAhSoZ9X3UDMIdZqOo4hshKgsdOLAPV5G@mail.gmail.com>
Message-ID: <AANLkTim8VkUk6yokyofORamu0qRaayZzawDN90Z5-2aM@mail.gmail.com>

Hi Thomas,

yes.. there is potential piggybacking going on. And it isn't the stopped
variable that is causing it, but the locks you acquire and release.

Because:
1) There is a happens before relation between construction of item and
placement (program order rule)
2) There is a happens before relation between placement of item and taking
item (monitor lock rule)
3) There is a happens before relation between taking of item and using item
(program order rule)

That is why there is a happens before relation between 1 and 3.

So essentially there is piggybacking going on twice. Between 1->2 and 2->3
since piggybacking involves the program order rule with one of the other
synchronization rules (in this case monitor lock rule).


On Thu, Jun 10, 2010 at 7:27 PM, Thomas Barnett <barnett.thomas at gmail.com>wrote:

> sorry the link for [3] does not seem to work any more. Below is the
> correct link.
> http://pastebin.com/PmV5S34J
>
> On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
> > Goetz et al. [1] explain piggybacking on synchronization in JCiP,
> > section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
> > Avoid excessive synchronization" which I've replicated at [3]. Is it
> > correct to say that there is an example of piggybacking in that code
> > listing; the visibility of the variable stopped is piggybacked on the
> > synchronization of the variable queue? Thanks
> >
> >
> > [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
> > Addison-Wesley Professional
> > [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
> > ed. Addison-Wesley, London
> > [3] http://pastebin.com/46Z2uShJ
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100610/a43b4373/attachment.html>

From alarmnummer at gmail.com  Thu Jun 10 15:31:21 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 10 Jun 2010 21:31:21 +0200
Subject: [concurrency-interest] A question about synchronization
	piggybacking
In-Reply-To: <AANLkTim8VkUk6yokyofORamu0qRaayZzawDN90Z5-2aM@mail.gmail.com>
References: <AANLkTin3mkQPsF3_LoXbSSecmGbUW0npq5GGwMLy9Kbg@mail.gmail.com>
	<AANLkTikdJdXlAhSoZ9X3UDMIdZqOo4hshKgsdOLAPV5G@mail.gmail.com>
	<AANLkTim8VkUk6yokyofORamu0qRaayZzawDN90Z5-2aM@mail.gmail.com>
Message-ID: <AANLkTilz8slkyh2-30nwNCxhmWBLc64UltPmxgE9ht7Y@mail.gmail.com>

PS:
What you essentially are building is a ThreadPoolExecutor. So instead of
getting this example up and running (especially stopping the workqueue is
more complicated), I would advice you to use the ThreadPoolExecutor or at
least have a look at it how it is solved there.

On Thu, Jun 10, 2010 at 9:15 PM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi Thomas,
>
> yes.. there is potential piggybacking going on. And it isn't the stopped
> variable that is causing it, but the locks you acquire and release.
>
> Because:
> 1) There is a happens before relation between construction of item and
> placement (program order rule)
> 2) There is a happens before relation between placement of item and taking
> item (monitor lock rule)
> 3) There is a happens before relation between taking of item and using item
> (program order rule)
>
> That is why there is a happens before relation between 1 and 3.
>
> So essentially there is piggybacking going on twice. Between 1->2 and 2->3
> since piggybacking involves the program order rule with one of the other
> synchronization rules (in this case monitor lock rule).
>
>
>
> On Thu, Jun 10, 2010 at 7:27 PM, Thomas Barnett <barnett.thomas at gmail.com>wrote:
>
>> sorry the link for [3] does not seem to work any more. Below is the
>> correct link.
>> http://pastebin.com/PmV5S34J
>>
>> On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
>> > Goetz et al. [1] explain piggybacking on synchronization in JCiP,
>> > section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
>> > Avoid excessive synchronization" which I've replicated at [3]. Is it
>> > correct to say that there is an example of piggybacking in that code
>> > listing; the visibility of the variable stopped is piggybacked on the
>> > synchronization of the variable queue? Thanks
>> >
>> >
>> > [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
>> > Addison-Wesley Professional
>> > [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
>> > ed. Addison-Wesley, London
>> > [3] http://pastebin.com/46Z2uShJ
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100610/792d301c/attachment.html>

From chriskessel at verizon.net  Thu Jun 10 17:53:03 2010
From: chriskessel at verizon.net (Chris Kessel/Lou Doherty)
Date: Thu, 10 Jun 2010 14:53:03 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to
	really	usemaxPoolSize effectively
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEOOIGAA.davidcholmes@aapt.net.au>
References: <4C108CC7.2070508@wonderly.org>
	<NFBBKALFDCPFIDBNKAPCKEOOIGAA.davidcholmes@aapt.net.au>
Message-ID: <004601cb08e7$4ddfc7d0$e99f5770$@net>

Given I just tried to set up thread pooling yesterday, this is timely.

I simply used Executors.newCachedThreadPool() and from the Javadoc it sounds
like it spawns threads whenever a job would exceed the current number of
threads available. Is that because it uses the SynchronousQueue and thus
essentially the queue is full (and thus the TPE should potentially spawn
another thread) every time a task gets submitted to the TPE?

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
Holmes
Sent: Thursday, June 10, 2010 12:10 AM
To: Gregg Wonderly
Cc: concurrency-interest
Subject: Re: [concurrency-interest] TPE.execute() doesn't seem to really
usemaxPoolSize effectively

Gregg,

> I've read the javadocs and do recall many of these discussions.
> What I trying to suggest, is that the behavior of TPE for
> unbounded queues seems "ineffective" regarding the existance of
> maxPoolSize in the case of unbounded queues.

I've really nothing to add from those past discussion. The pool is intended
to work under normal load at corePoolSize (to which it ramps up unless
pre-start is used). When an overload condition occurs (defined by there
being more pending/in-process tasks than workers) we use the queue to act as
a buffer - with the expectation that the normal workload will be restored in
the near future. If we're worried about excessive overload then we can use a
bounded queue, and that says "if the queue fills up add more workers up to
maxPoolSize". If we use an unbounded queue we saying we don't expect (or
else don't care about) excessive overload.

The aim is to balance the ability to handle the expected workload, even
under transient overloads, without having excessive thread creation and
without too much thread churn (ie create-work-die-create).

Other designs are possible and this design doesn't allow all possible
policies that people may want, though as discussed in the past there are
ways to achieve some of these through creative TPE chaining.

David

  Having
> JavaDocs discuss unbounded BlockingQueue implementations does
> provide a way for
> the developer to discover what the current implementation is.
>
> It seems that it really should allow maxPoolSize to help add
> additional threads
> that can help diminish the extra work, as it does for bounded queues.
>
> Is there a specific part of the current behavior that seems
> advantageous for unbounded queues?
>
> Gregg Wonderly

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

__________ Information from ESET NOD32 Antivirus, version of virus signature
database 5187 (20100610) __________

The message was checked by ESET NOD32 Antivirus.

http://www.eset.com


 

__________ Information from ESET NOD32 Antivirus, version of virus signature
database 5188 (20100610) __________

The message was checked by ESET NOD32 Antivirus.

http://www.eset.com
 


From joe.bowbeer at gmail.com  Thu Jun 10 18:22:40 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 10 Jun 2010 15:22:40 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <004601cb08e7$4ddfc7d0$e99f5770$@net>
References: <4C108CC7.2070508@wonderly.org>
	<NFBBKALFDCPFIDBNKAPCKEOOIGAA.davidcholmes@aapt.net.au>
	<004601cb08e7$4ddfc7d0$e99f5770$@net>
Message-ID: <AANLkTili3SgDjCH4RSIX3T5a_8oQH77cFW2tvqrvnpAk@mail.gmail.com>

On Thu, Jun 10, 2010 at 2:53 PM, Chris Kessel/Lou Doherty wrote:

> Given I just tried to set up thread pooling yesterday, this is timely.
>
> I simply used Executors.newCachedThreadPool() and from the Javadoc it
> sounds
> like it spawns threads whenever a job would exceed the current number of
> threads available. Is that because it uses the SynchronousQueue and thus
> essentially the queue is full (and thus the TPE should potentially spawn
> another thread) every time a task gets submitted to the TPE?
>

Precisely.

We hoped that the Executors tool class would provide an easy on-ramp for
most would-be TPE users.

Can we have avoid more head-scratching by adding another factory method or
two to Executors?  If so, what would these methods look like?

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100610/bd90a8f6/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jun 10 19:17:18 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 11 Jun 2010 09:17:18 +1000
Subject: [concurrency-interest] A question about
	synchronizationpiggybacking
In-Reply-To: <AANLkTikdJdXlAhSoZ9X3UDMIdZqOo4hshKgsdOLAPV5G@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEPBIGAA.davidcholmes@aapt.net.au>

Thomas,

No that is not piggy-backing. All of your WorkQueue state is accessed only
under synchronization, there are no un-synchronized accesses that need to
piggy-back off anything.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Thomas
> Barnett
> Sent: Friday, 11 June 2010 3:28 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] A question about
> synchronizationpiggybacking
>
>
> sorry the link for [3] does not seem to work any more. Below is the
> correct link.
> http://pastebin.com/PmV5S34J
>
> On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
> > Goetz et al. [1] explain piggybacking on synchronization in JCiP,
> > section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
> > Avoid excessive synchronization" which I've replicated at [3]. Is it
> > correct to say that there is an example of piggybacking in that code
> > listing; the visibility of the variable stopped is piggybacked on the
> > synchronization of the variable queue? Thanks
> >
> >
> > [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
> > Addison-Wesley Professional
> > [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
> > ed. Addison-Wesley, London
> > [3] http://pastebin.com/46Z2uShJ
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Thu Jun 10 19:10:40 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 11 Jun 2010 09:10:40 +1000
Subject: [concurrency-interest] ThreadPoolExecutor thread pool is not
	growing
In-Reply-To: <4C110424.50607@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEPAIGAA.davidcholmes@aapt.net.au>

Gregg,

> As we've discussed here before, TPE's thread starting mechanism
> matches few peoples expectations based on how existing thread pools
> have been created and used.

Really? How did you come to that conclusion? People who have a
misunderastanding post to the list; people who don't have a misunderstanding
don't post to the list - so how can you quantify this?

And which "existing thread pools" are you referring to? TPE was pre-dated by
Doug's library by many years.

Maybe we should have use the term "core" and "overload" rather than "core"
and "max", but even then you wouldn't know what overload means unless you
actually read the docs.

David Holmes
------------

> As we've seen here, and as I've discussed with others in other
> places, people
> expect that at most corePoolSize threads would be idle, and up to
> maxPoolSize
> threads would be added anytime there are more than corePoolSize
> tasks queued.

If you simply create threads until maxPoolSize threads exist then what
purpose does corePoolSize serve? A low-water-mark perhaps
> The creation of threads from corePoolSize toward maxPoolSize is
> also weakly
> managed from the perspective that the Queue and the TPE are
> separate entities.
>
> The user providing the queue to the constructor provides a
> controlling behavior
> for fixed size vs open ended queues, yet the constructor
> arguments are not
> appropriately limited or managed based on that.
>
> There could be a "FixedSized" interface it seems to me, and the
> constructors
> should have specific versions related to this issue so that
> people get proper
> IDE code completion control of how they use the constructors
> based on the queue
> behaviors.
>
> Sure there is lots of flexibility in what exists.  It it just
> full of a lot of
> surprises compared to what many people expect it seems to me.
>
> Gregg Wonderly
>
> Kasper Nielsen wrote:
> > On 10/6/2010 35:10, Talip Ozturk wrote:
> >> Hi guys,
> >>
> >> Here is what I am trying to do:
> >> 1. If threads are idle for 60 seconds, then the pool should shrink
> >> down to zero.
> >> 2. if we have tasks in the underlying queue, we should have up to 20
> >> concurrent execution
> >> 3. no task should be rejected as we are using LinkedBlockingQueue.
> >>
> >> Please see the code below: I would expect 20 of them running
> >> concurrently; not one by one...
> >>
> >> 1. Is this behavior normal?
> > Yes, try reading the javadoc for ThreadPoolExecutor under Queuing.
> > There has been many questions regarding this behavior. Try, for
> example,
> > to look at the post "TPE.execute() doesn't seem to really use
> > maxPoolSize effectively" posted earlier today.
> >
> >> 2. How can I achieve my own goal?
> > I believe you are looking for something like:
> > ThreadPoolExecutor executor = new ThreadPoolExecutor(20, 20,
> > 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
> > tpe.allowCoreThreadTimeOut(true);
> >
> > The last method is only available in Java 6 and upwards though.
> >
> >
> > Cheers
> >   Kasper
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Thu Jun 10 19:34:03 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 11 Jun 2010 09:34:03 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem toreally
	usemaxPoolSize effectively
In-Reply-To: <004601cb08e7$4ddfc7d0$e99f5770$@net>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEPCIGAA.davidcholmes@aapt.net.au>

That's exactly right Chris:
- core size of 0
- max size of MAX_VALUE
- queue that's always "full"

So a cachedTPE always uses new threads to handle additional workload rather
than doing any buffering. These threads have a default idle timeout of 60
seconds.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Chris
> Kessel/Lou Doherty
> Sent: Friday, 11 June 2010 7:53 AM
> To: 'concurrency-interest'
> Subject: Re: [concurrency-interest] TPE.execute() doesn't seem toreally
> usemaxPoolSize effectively
>
>
> Given I just tried to set up thread pooling yesterday, this is timely.
>
> I simply used Executors.newCachedThreadPool() and from the
> Javadoc it sounds
> like it spawns threads whenever a job would exceed the current number of
> threads available. Is that because it uses the SynchronousQueue and thus
> essentially the queue is full (and thus the TPE should potentially spawn
> another thread) every time a task gets submitted to the TPE?
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
> Holmes
> Sent: Thursday, June 10, 2010 12:10 AM
> To: Gregg Wonderly
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] TPE.execute() doesn't seem to really
> usemaxPoolSize effectively
>
> Gregg,
>
> > I've read the javadocs and do recall many of these discussions.
> > What I trying to suggest, is that the behavior of TPE for
> > unbounded queues seems "ineffective" regarding the existance of
> > maxPoolSize in the case of unbounded queues.
>
> I've really nothing to add from those past discussion. The pool
> is intended
> to work under normal load at corePoolSize (to which it ramps up unless
> pre-start is used). When an overload condition occurs (defined by there
> being more pending/in-process tasks than workers) we use the
> queue to act as
> a buffer - with the expectation that the normal workload will be
> restored in
> the near future. If we're worried about excessive overload then
> we can use a
> bounded queue, and that says "if the queue fills up add more workers up to
> maxPoolSize". If we use an unbounded queue we saying we don't expect (or
> else don't care about) excessive overload.
>
> The aim is to balance the ability to handle the expected workload, even
> under transient overloads, without having excessive thread creation and
> without too much thread churn (ie create-work-die-create).
>
> Other designs are possible and this design doesn't allow all possible
> policies that people may want, though as discussed in the past there are
> ways to achieve some of these through creative TPE chaining.
>
> David
>
>   Having
> > JavaDocs discuss unbounded BlockingQueue implementations does
> > provide a way for
> > the developer to discover what the current implementation is.
> >
> > It seems that it really should allow maxPoolSize to help add
> > additional threads
> > that can help diminish the extra work, as it does for bounded queues.
> >
> > Is there a specific part of the current behavior that seems
> > advantageous for unbounded queues?
> >
> > Gregg Wonderly
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> __________ Information from ESET NOD32 Antivirus, version of
> virus signature
> database 5187 (20100610) __________
>
> The message was checked by ESET NOD32 Antivirus.
>
> http://www.eset.com
>
>
>
>
> __________ Information from ESET NOD32 Antivirus, version of
> virus signature
> database 5188 (20100610) __________
>
> The message was checked by ESET NOD32 Antivirus.
>
> http://www.eset.com
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Fri Jun 11 03:08:21 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 11 Jun 2010 17:08:21 +1000
Subject: [concurrency-interest] A question about
	synchronizationpiggybacking
In-Reply-To: <AANLkTim8VkUk6yokyofORamu0qRaayZzawDN90Z5-2aM@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEPEIGAA.davidcholmes@aapt.net.au>

Peter is correct, there is piggy-backing for the "safe publication" of the
items passing through the queue.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
Veentjer
  Sent: Friday, 11 June 2010 5:16 AM
  To: Thomas Barnett
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] A question about
synchronizationpiggybacking


  Hi Thomas,

  yes.. there is potential piggybacking going on. And it isn't the stopped
variable that is causing it, but the locks you acquire and release.

  Because:
  1) There is a happens before relation between construction of item and
placement (program order rule)
  2) There is a happens before relation between placement of item and taking
item (monitor lock rule)
  3) There is a happens before relation between taking of item and using
item (program order rule)

  That is why there is a happens before relation between 1 and 3.

  So essentially there is piggybacking going on twice. Between 1->2 and 2->3
since piggybacking involves the program order rule with one of the other
synchronization rules (in this case monitor lock rule).



  On Thu, Jun 10, 2010 at 7:27 PM, Thomas Barnett <barnett.thomas at gmail.com>
wrote:

    sorry the link for [3] does not seem to work any more. Below is the
    correct link.
    http://pastebin.com/PmV5S34J


    On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
    > Goetz et al. [1] explain piggybacking on synchronization in JCiP,
    > section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
    > Avoid excessive synchronization" which I've replicated at [3]. Is it
    > correct to say that there is an example of piggybacking in that code
    > listing; the visibility of the variable stopped is piggybacked on the
    > synchronization of the variable queue? Thanks
    >
    >
    > [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
    > Addison-Wesley Professional
    > [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
    > ed. Addison-Wesley, London
    > [3] http://pastebin.com/46Z2uShJ
    >
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100611/1a7aa52c/attachment.html>

From gregg at cytetech.com  Fri Jun 11 11:46:52 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 11 Jun 2010 10:46:52 -0500
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
 usemaxPoolSize effectively
In-Reply-To: <004601cb08e7$4ddfc7d0$e99f5770$@net>
References: <4C108CC7.2070508@wonderly.org>
	<NFBBKALFDCPFIDBNKAPCKEOOIGAA.davidcholmes@aapt.net.au>
	<004601cb08e7$4ddfc7d0$e99f5770$@net>
Message-ID: <4C125A6C.80705@cytetech.com>

Yes.  This mode of operation is possible to achieve when there is a bounded 
queue and maxPoolSize is Integer.MAX_VALUE.  What I am saying is that I want 
that mode of operation when there is an unbounded queue and maxPoolSize is 
limited.  TPE only considers maxPoolSize when there is a bounded queue.

Gregg Wonderly

Chris Kessel/Lou Doherty wrote:
> Given I just tried to set up thread pooling yesterday, this is timely.
> 
> I simply used Executors.newCachedThreadPool() and from the Javadoc it sounds
> like it spawns threads whenever a job would exceed the current number of
> threads available. Is that because it uses the SynchronousQueue and thus
> essentially the queue is full (and thus the TPE should potentially spawn
> another thread) every time a task gets submitted to the TPE?
> 
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David
> Holmes
> Sent: Thursday, June 10, 2010 12:10 AM
> To: Gregg Wonderly
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] TPE.execute() doesn't seem to really
> usemaxPoolSize effectively
> 
> Gregg,
> 
>> I've read the javadocs and do recall many of these discussions.
>> What I trying to suggest, is that the behavior of TPE for
>> unbounded queues seems "ineffective" regarding the existance of
>> maxPoolSize in the case of unbounded queues.
> 
> I've really nothing to add from those past discussion. The pool is intended
> to work under normal load at corePoolSize (to which it ramps up unless
> pre-start is used). When an overload condition occurs (defined by there
> being more pending/in-process tasks than workers) we use the queue to act as
> a buffer - with the expectation that the normal workload will be restored in
> the near future. If we're worried about excessive overload then we can use a
> bounded queue, and that says "if the queue fills up add more workers up to
> maxPoolSize". If we use an unbounded queue we saying we don't expect (or
> else don't care about) excessive overload.
> 
> The aim is to balance the ability to handle the expected workload, even
> under transient overloads, without having excessive thread creation and
> without too much thread churn (ie create-work-die-create).
> 
> Other designs are possible and this design doesn't allow all possible
> policies that people may want, though as discussed in the past there are
> ways to achieve some of these through creative TPE chaining.
> 
> David
> 
>   Having
>> JavaDocs discuss unbounded BlockingQueue implementations does
>> provide a way for
>> the developer to discover what the current implementation is.
>>
>> It seems that it really should allow maxPoolSize to help add
>> additional threads
>> that can help diminish the extra work, as it does for bounded queues.
>>
>> Is there a specific part of the current behavior that seems
>> advantageous for unbounded queues?
>>
>> Gregg Wonderly
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> __________ Information from ESET NOD32 Antivirus, version of virus signature
> database 5187 (20100610) __________
> 
> The message was checked by ESET NOD32 Antivirus.
> 
> http://www.eset.com
> 
> 
>  
> 
> __________ Information from ESET NOD32 Antivirus, version of virus signature
> database 5188 (20100610) __________
> 
> The message was checked by ESET NOD32 Antivirus.
> 
> http://www.eset.com
>  
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From davidcholmes at aapt.net.au  Fri Jun 11 18:01:50 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 12 Jun 2010 08:01:50 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C125A6C.80705@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>

Gregg Wonderly writes:
> What I am saying is that I want that mode of operation when there is an
> unbounded queue and maxPoolSize is limited.  TPE only considers
> maxPoolSize when there is a bounded queue.

Can you explain the role of coreSize and maxSize in this scenario? I'm
assuming the coreSize acts as a "low-water mark" of some kind. If core and
max serve the same basic role then you only need one of them - which one
depends on the nature of the queue.

David


From unmesh_joshi at hotmail.com  Fri Jun 11 21:01:54 2010
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Sat, 12 Jun 2010 01:01:54 +0000
Subject: [concurrency-interest] Global intepreter lock and JVM
In-Reply-To: <mailman.1.1276272000.18392.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1276272000.18392.concurrency-interest@cs.oswego.edu>
Message-ID: <BAY140-W17243BB834CB69CD2AD831EFDA0@phx.gbl>


Hi,
I just started reading about Ruby interpreter and its threading. it was interesting to read about Global Interpreter Lock. Considering Java and JVM were invented in  1990s, and running Java on multicore and multi processor machines was distant future then, why did java not require GIL? Was it a conscious decision taken at that time?
Thanks,Unmesh   		 	   		  
_________________________________________________________________
See the news as it happens on MSN videos
http://video.in.msn.com/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100612/1f3547c0/attachment.html>

From davidcholmes at aapt.net.au  Sat Jun 12 06:18:42 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 12 Jun 2010 20:18:42 +1000
Subject: [concurrency-interest] Global intepreter lock and JVM
In-Reply-To: <BAY140-W17243BB834CB69CD2AD831EFDA0@phx.gbl>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEPHIGAA.davidcholmes@aapt.net.au>

Unmesh joshi [unmesh_joshi at hotmail.com] wrote:
> I just started reading about Ruby interpreter and its threading.
> it was interesting to read about Global Interpreter Lock.
> Considering Java and JVM were invented in  1990s, and running
> Java on multicore and multi processor machines was distant future
> then, why did java not require GIL? Was it a conscious decision
> taken at that time?

Multi-core may not have been around (was hyper-threading?) but
multi-processor systems certainly were and Sun Microsystems produced
multi-processor hardware that ran a multi-threaded operating system -
Solaris. So defining Java as a language/platform to support multi-threaded
programming was most certainly a very conscious decision taken at the time.

That said the early versions of the Java VM (at least on some platforms)
used a user-level threading model (aka Green Threads) that didn't utilize
multiple processors - all context switching was under full control of the VM
which ran on a single native OS thread (more or less).

I'm sure google could turn up a dozen interviews with James Gosling that
discuss these early design decisions.

The above of course is just my perspective on this.

Cheers,
David Holmes



From barnett.thomas at gmail.com  Sat Jun 12 14:20:58 2010
From: barnett.thomas at gmail.com (Thomas Barnett)
Date: Sat, 12 Jun 2010 19:20:58 +0100
Subject: [concurrency-interest] A question about
	synchronizationpiggybacking
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEPEIGAA.davidcholmes@aapt.net.au>
References: <AANLkTim8VkUk6yokyofORamu0qRaayZzawDN90Z5-2aM@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCKEPEIGAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTilA2jU3duHJFpYYNMbtq_WR-xrr0d_7VzWpdmsK@mail.gmail.com>

Thanks. Seems that I will need to re-read the relevant chapter.

On 6/11/10, David Holmes <davidcholmes at aapt.net.au> wrote:
> Peter is correct, there is piggy-backing for the "safe publication" of the
> items passing through the queue.
>
> David Holmes
>   -----Original Message-----
>   From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Veentjer
>   Sent: Friday, 11 June 2010 5:16 AM
>   To: Thomas Barnett
>   Cc: concurrency-interest at cs.oswego.edu
>   Subject: Re: [concurrency-interest] A question about
> synchronizationpiggybacking
>
>
>   Hi Thomas,
>
>   yes.. there is potential piggybacking going on. And it isn't the stopped
> variable that is causing it, but the locks you acquire and release.
>
>   Because:
>   1) There is a happens before relation between construction of item and
> placement (program order rule)
>   2) There is a happens before relation between placement of item and taking
> item (monitor lock rule)
>   3) There is a happens before relation between taking of item and using
> item (program order rule)
>
>   That is why there is a happens before relation between 1 and 3.
>
>   So essentially there is piggybacking going on twice. Between 1->2 and 2->3
> since piggybacking involves the program order rule with one of the other
> synchronization rules (in this case monitor lock rule).
>
>
>
>   On Thu, Jun 10, 2010 at 7:27 PM, Thomas Barnett <barnett.thomas at gmail.com>
> wrote:
>
>     sorry the link for [3] does not seem to work any more. Below is the
>     correct link.
>     http://pastebin.com/PmV5S34J
>
>
>     On 6/10/10, Thomas Barnett <barnett.thomas at gmail.com> wrote:
>     > Goetz et al. [1] explain piggybacking on synchronization in JCiP,
>     > section 16.1.4. Further, Bloch [2] gives a code listing in "Item 49:
>     > Avoid excessive synchronization" which I've replicated at [3]. Is it
>     > correct to say that there is an example of piggybacking in that code
>     > listing; the visibility of the variable stopped is piggybacked on the
>     > synchronization of the variable queue? Thanks
>     >
>     >
>     > [1] Goetz, B. et al., 2006. Java Concurrency in Practice.
>     > Addison-Wesley Professional
>     > [2] Bloch, J., 2001. Effective Java Programming Language Guide. 1st
>     > ed. Addison-Wesley, London
>     > [3] http://pastebin.com/46Z2uShJ
>     >
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From joe.bowbeer at gmail.com  Sat Jun 12 16:07:32 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 12 Jun 2010 13:07:32 -0700
Subject: [concurrency-interest] Global intepreter lock and JVM
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEPHIGAA.davidcholmes@aapt.net.au>
References: <BAY140-W17243BB834CB69CD2AD831EFDA0@phx.gbl>
	<NFBBKALFDCPFIDBNKAPCAEPHIGAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTilG8mm2lRZHo-zzGNkLDuvNwM409554hiMISez5@mail.gmail.com>

There were some threading dissenters at the time, even inside Sun.  Most
notable is John Ousterhout, inventor or Tcl, who argued that threads were a
bad idea for most purposes, in an Invited Talk at the 1996 USENIX Technical
Conference:

http://home.pacbell.net/ouster/threads.pdf

Ousterhout does make a distinction between scripting languages, such as Tcl
(and now Ruby), and system languages such as Java.

I wonder if Ousterhout's views were known at the time Java's threading model
was designed.  It would be have been interesting to have been a fly on the
wall when the decision to implement a lock per object was made.  In any
event, I think that once the decision to embrace concurrency using threads
in Java was made, everything else followed.  And now Ruby users can leverage
native threads by using JRuby...

Joe

PS - I wonder if it's worth comparing Ruby's GIL to Transactional
Memory's One Big Lock?  Everything old is new again?

On Sat, Jun 12, 2010 at 3:18 AM, David Holmes wrote:

> Unmesh joshi wrote:
> > I just started reading about Ruby interpreter and its threading.
> > it was interesting to read about Global Interpreter Lock.
> > Considering Java and JVM were invented in  1990s, and running
> > Java on multicore and multi processor machines was distant future
> > then, why did java not require GIL? Was it a conscious decision
> > taken at that time?
>
> Multi-core may not have been around (was hyper-threading?) but
> multi-processor systems certainly were and Sun Microsystems produced
> multi-processor hardware that ran a multi-threaded operating system -
> Solaris. So defining Java as a language/platform to support multi-threaded
> programming was most certainly a very conscious decision taken at the time.
>
> That said the early versions of the Java VM (at least on some platforms)
> used a user-level threading model (aka Green Threads) that didn't utilize
> multiple processors - all context switching was under full control of the
> VM
> which ran on a single native OS thread (more or less).
>
> I'm sure google could turn up a dozen interviews with James Gosling that
> discuss these early design decisions.
>
> The above of course is just my perspective on this.
>
> Cheers,
> David Holmes
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100612/924676dd/attachment.html>

From jodoreps at gmail.com  Sun Jun 13 19:40:25 2010
From: jodoreps at gmail.com (jodoreps asddasda)
Date: Mon, 14 Jun 2010 03:40:25 +0400
Subject: [concurrency-interest] Acheter medicaments generiques sans
	ordonnance
Message-ID: <AANLkTincB7XeN3dCmVqbBXG9qdnMH3fUF1n-mvuNgnax@mail.gmail.com>

Acheter medicaments generiques sans ordonnance<http://www.pharmacieenfrance.fr/>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100614/0fa08162/attachment.html>

From gregg at cytetech.com  Tue Jun 15 14:33:26 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 15 Jun 2010 13:33:26 -0500
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
 usemaxPoolSize effectively
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>
Message-ID: <4C17C776.7070906@cytetech.com>

David Holmes wrote:
> Gregg Wonderly writes:
>> What I am saying is that I want that mode of operation when there is an
>> unbounded queue and maxPoolSize is limited.  TPE only considers
>> maxPoolSize when there is a bounded queue.
> 
> Can you explain the role of coreSize and maxSize in this scenario? I'm
> assuming the coreSize acts as a "low-water mark" of some kind. If core and
> max serve the same basic role then you only need one of them - which one
> depends on the nature of the queue.

I guess I am confused.  corePoolSize is a low water mark the represents the 
number of threads that will always be waiting for work.  In that sense, it is a 
low-water mark on the number of threads that could be running.

Currently, with a fixed sized queue, if you exceed the size of the queue, then, 
for each additional overflowing item, an additional thread is started, up to the 
maxPoolSize limit.  So, if I create a TPE with corePoolSize=2, maxPoolSize=10 
and use a queue of size 5, then we see that 2 threads will always be running up 
to the point of there being 7 tasks in the system at the same time (2 running, 5 
queued).  Then, for the next 10 tasks, which are added and cause overflow, 10 
more threads will be started.

Once we get to that point, the next overflow will cause rejection processing.

With an unlimited queue, I'd like this same behavior, except that the last phase 
where we transition into rejection processing never occurs.

Instead, I'd like the ability to, from the outside, cancel tasks via other 
statistics than just pending task count.

Network applications can be bursty and hard to manage load swells because you 
aren't able to throttle peoples use of the system.  Also, interactions with 
other components of the system are not predictable.

It just makes sense to me, that TPE should be using maxPoolSize by "counting" 
pending work when a queue is unbounded.  Should I guess that the synchronization 
for counting the items in the unbounded queue is the main reason why this was 
not done to begin with?  For me, this is one obvious place that queue size is 
needed.

Gregg Wonderly

From martinrb at google.com  Tue Jun 15 14:55:23 2010
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 15 Jun 2010 11:55:23 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C17C776.7070906@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>
	<4C17C776.7070906@cytetech.com>
Message-ID: <AANLkTinZEO74Rlx-eEiVabRJqHg0VEcH2_iaxhPpzPr3@mail.gmail.com>

No one is entirely happy with the current ThreadPoolExecutor.
We are sure there's a better one that can be built,
but it's a tough job, especially if you want to make existing users happy.

The low-hanging fruit would be to never create a new thread
if an existing thread is waiting for work, which has been on my
not-getting-done list for a long time now.

Martin

On Tue, Jun 15, 2010 at 11:33, Gregg Wonderly <gregg at cytetech.com> wrote:
> David Holmes wrote:
>>
>> Gregg Wonderly writes:
>>>
>>> What I am saying is that I want that mode of operation when there is an
>>> unbounded queue and maxPoolSize is limited. ?TPE only considers
>>> maxPoolSize when there is a bounded queue.
>>
>> Can you explain the role of coreSize and maxSize in this scenario? I'm
>> assuming the coreSize acts as a "low-water mark" of some kind. If core and
>> max serve the same basic role then you only need one of them - which one
>> depends on the nature of the queue.
>
> I guess I am confused. ?corePoolSize is a low water mark the represents the
> number of threads that will always be waiting for work. ?In that sense, it
> is a low-water mark on the number of threads that could be running.
>
> Currently, with a fixed sized queue, if you exceed the size of the queue,
> then, for each additional overflowing item, an additional thread is started,
> up to the maxPoolSize limit. ?So, if I create a TPE with corePoolSize=2,
> maxPoolSize=10 and use a queue of size 5, then we see that 2 threads will
> always be running up to the point of there being 7 tasks in the system at
> the same time (2 running, 5 queued). ?Then, for the next 10 tasks, which are
> added and cause overflow, 10 more threads will be started.
>
> Once we get to that point, the next overflow will cause rejection
> processing.
>
> With an unlimited queue, I'd like this same behavior, except that the last
> phase where we transition into rejection processing never occurs.
>
> Instead, I'd like the ability to, from the outside, cancel tasks via other
> statistics than just pending task count.
>
> Network applications can be bursty and hard to manage load swells because
> you aren't able to throttle peoples use of the system. ?Also, interactions
> with other components of the system are not predictable.
>
> It just makes sense to me, that TPE should be using maxPoolSize by
> "counting" pending work when a queue is unbounded. ?Should I guess that the
> synchronization for counting the items in the unbounded queue is the main
> reason why this was not done to begin with? ?For me, this is one obvious
> place that queue size is needed.
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From joe.bowbeer at gmail.com  Tue Jun 15 15:18:15 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 15 Jun 2010 12:18:15 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <AANLkTinZEO74Rlx-eEiVabRJqHg0VEcH2_iaxhPpzPr3@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>
	<4C17C776.7070906@cytetech.com>
	<AANLkTinZEO74Rlx-eEiVabRJqHg0VEcH2_iaxhPpzPr3@mail.gmail.com>
Message-ID: <AANLkTilhoJBlfIU8NFK04jRbrIO9C8ZAfgS-gO5IWjcC@mail.gmail.com>

I think Gregg's use case is the other common complaint, if not the most
common one.

If we have a simple (composite) way to create an ExecutorService for this
case then we can consider adding it to the samples in the javadoc, or even
adding it via the Executors tool class.

In Gregg's case, I think the desired ES would be constructed from a couple
of TPE's?

On Tue, Jun 15, 2010 at 11:55 AM, Martin Buchholz wrote:

> No one is entirely happy with the current ThreadPoolExecutor.
> We are sure there's a better one that can be built,
> but it's a tough job, especially if you want to make existing users happy.
>
> The low-hanging fruit would be to never create a new thread
> if an existing thread is waiting for work, which has been on my
> not-getting-done list for a long time now.
>
> Martin
>
> On Tue, Jun 15, 2010 at 11:33, Gregg Wonderly wrote:
> > David Holmes wrote:
> >>
> >> Gregg Wonderly writes:
> >>>
> >>> What I am saying is that I want that mode of operation when there is an
> >>> unbounded queue and maxPoolSize is limited.  TPE only considers
> >>> maxPoolSize when there is a bounded queue.
> >>
> >> Can you explain the role of coreSize and maxSize in this scenario? I'm
> >> assuming the coreSize acts as a "low-water mark" of some kind. If core
> and
> >> max serve the same basic role then you only need one of them - which one
> >> depends on the nature of the queue.
> >
> > I guess I am confused.  corePoolSize is a low water mark the represents
> the
> > number of threads that will always be waiting for work.  In that sense,
> it
> > is a low-water mark on the number of threads that could be running.
> >
> > Currently, with a fixed sized queue, if you exceed the size of the queue,
> > then, for each additional overflowing item, an additional thread is
> started,
> > up to the maxPoolSize limit.  So, if I create a TPE with corePoolSize=2,
> > maxPoolSize=10 and use a queue of size 5, then we see that 2 threads will
> > always be running up to the point of there being 7 tasks in the system at
> > the same time (2 running, 5 queued).  Then, for the next 10 tasks, which
> are
> > added and cause overflow, 10 more threads will be started.
> >
> > Once we get to that point, the next overflow will cause rejection
> > processing.
> >
> > With an unlimited queue, I'd like this same behavior, except that the
> last
> > phase where we transition into rejection processing never occurs.
> >
> > Instead, I'd like the ability to, from the outside, cancel tasks via
> other
> > statistics than just pending task count.
> >
> > Network applications can be bursty and hard to manage load swells because
> > you aren't able to throttle peoples use of the system.  Also,
> interactions
> > with other components of the system are not predictable.
> >
> > It just makes sense to me, that TPE should be using maxPoolSize by
> > "counting" pending work when a queue is unbounded.  Should I guess that
> the
> > synchronization for counting the items in the unbounded queue is the main
> > reason why this was not done to begin with?  For me, this is one obvious
> > place that queue size is needed.
> >
> > Gregg Wonderly
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100615/fc6ff11b/attachment.html>

From holger.hoffstaette at googlemail.com  Tue Jun 15 16:14:04 2010
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 15 Jun 2010 22:14:04 +0200
Subject: [concurrency-interest] TPE.execute() doesn't seem to
 really	usemaxPoolSize effectively
In-Reply-To: <AANLkTilhoJBlfIU8NFK04jRbrIO9C8ZAfgS-gO5IWjcC@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>	<4C17C776.7070906@cytetech.com>	<AANLkTinZEO74Rlx-eEiVabRJqHg0VEcH2_iaxhPpzPr3@mail.gmail.com>
	<AANLkTilhoJBlfIU8NFK04jRbrIO9C8ZAfgS-gO5IWjcC@mail.gmail.com>
Message-ID: <4C17DF0C.3020601@googlemail.com>

Joe Bowbeer wrote:
> I think Gregg's use case is the other common complaint, if not the most
> common one.

yes.

> If we have a simple (composite) way to create an ExecutorService for
> this case then we can consider adding it to the samples in the javadoc,
> or even adding it via the Executors tool class.

http://www.kimchy.org/juc-executorservice-gotcha/

I have no idea how well it works; Shay is probably reading the list and
can comment best.

regards
Holger

From peger at automotive.com  Tue Jun 15 18:07:09 2010
From: peger at automotive.com (Eger, Patrick)
Date: Tue, 15 Jun 2010 15:07:09 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C17C776.7070906@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCOEPFIGAA.davidcholmes@aapt.net.au>
	<4C17C776.7070906@cytetech.com>
Message-ID: <1CFD7891521AAB4E8201FB7A78C9D36F05C2B8BD@mail-001.corp.automotive.com>

Yeah, I myself have been complaining (though offering no code =) since
the first versions of j.u.c. It's impossible to do this extremely common
thing with the current classes (or even by extending). I find myself
running into it over and over again. Most all JDBC pools (and object
pools) have precisely the semantics for limited-resource management
(min,max,spare,increment) that I desire. The j.u.c classes are all very
flexible, but pay for that flexibility with actual usefulness IMHO. The
old oswego classes were better in this regard.

On a similar topic, block-on-acquire (until a thread is available) is
another very useful feature that was lost...

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Gregg
Wonderly
Sent: Tuesday, June 15, 2010 11:33 AM
To: dholmes at ieee.org
Cc: 'concurrency-interest'; gregg.wonderly at pobox.com
Subject: Re: [concurrency-interest] TPE.execute() doesn't seem to really
usemaxPoolSize effectively

David Holmes wrote:
> Gregg Wonderly writes:
>> What I am saying is that I want that mode of operation when there is
an
>> unbounded queue and maxPoolSize is limited.  TPE only considers
>> maxPoolSize when there is a bounded queue.
> 
> Can you explain the role of coreSize and maxSize in this scenario? I'm
> assuming the coreSize acts as a "low-water mark" of some kind. If core
and
> max serve the same basic role then you only need one of them - which
one
> depends on the nature of the queue.

I guess I am confused.  corePoolSize is a low water mark the represents
the 
number of threads that will always be waiting for work.  In that sense,
it is a 
low-water mark on the number of threads that could be running.

Currently, with a fixed sized queue, if you exceed the size of the
queue, then, 
for each additional overflowing item, an additional thread is started,
up to the 
maxPoolSize limit.  So, if I create a TPE with corePoolSize=2,
maxPoolSize=10 
and use a queue of size 5, then we see that 2 threads will always be
running up 
to the point of there being 7 tasks in the system at the same time (2
running, 5 
queued).  Then, for the next 10 tasks, which are added and cause
overflow, 10 
more threads will be started.

Once we get to that point, the next overflow will cause rejection
processing.

With an unlimited queue, I'd like this same behavior, except that the
last phase 
where we transition into rejection processing never occurs.

Instead, I'd like the ability to, from the outside, cancel tasks via
other 
statistics than just pending task count.

Network applications can be bursty and hard to manage load swells
because you 
aren't able to throttle peoples use of the system.  Also, interactions
with 
other components of the system are not predictable.

It just makes sense to me, that TPE should be using maxPoolSize by
"counting" 
pending work when a queue is unbounded.  Should I guess that the
synchronization 
for counting the items in the unbounded queue is the main reason why
this was 
not done to begin with?  For me, this is one obvious place that queue
size is 
needed.

Gregg Wonderly
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Tue Jun 15 18:43:57 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 16 Jun 2010 08:43:57 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C17C776.7070906@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>

Gregg,

Thanks for clarifying. So to summarise what you would like is a way to set a
limit on the queue size so that once N items are queued we start creating
threads up to maxPoolSize, but beyond that we just continue queuing.

As others have noted this can be emulated by chaining two TPEs eg by using
the rejected-execution from the first to submit to the second.

Ignoring the vaguaries associated with the size() operation on
BlockingQueues this might be possible within the current framework. Though
things can get even more complicated when you try to minimize thread
creation as Martin described.

David

> -----Original Message-----
> From: Gregg Wonderly [mailto:gregg at cytetech.com]
> Sent: Wednesday, 16 June 2010 4:33 AM
> To: dholmes at ieee.org
> Cc: gregg.wonderly at pobox.com; 'concurrency-interest'
> Subject: Re: [concurrency-interest] TPE.execute() doesn't seem to really
> usemaxPoolSize effectively
>
>
> David Holmes wrote:
> > Gregg Wonderly writes:
> >> What I am saying is that I want that mode of operation when there is an
> >> unbounded queue and maxPoolSize is limited.  TPE only considers
> >> maxPoolSize when there is a bounded queue.
> >
> > Can you explain the role of coreSize and maxSize in this scenario? I'm
> > assuming the coreSize acts as a "low-water mark" of some kind.
> If core and
> > max serve the same basic role then you only need one of them - which one
> > depends on the nature of the queue.
>
> I guess I am confused.  corePoolSize is a low water mark the
> represents the
> number of threads that will always be waiting for work.  In that
> sense, it is a
> low-water mark on the number of threads that could be running.
>
> Currently, with a fixed sized queue, if you exceed the size of
> the queue, then,
> for each additional overflowing item, an additional thread is
> started, up to the
> maxPoolSize limit.  So, if I create a TPE with corePoolSize=2,
> maxPoolSize=10
> and use a queue of size 5, then we see that 2 threads will always
> be running up
> to the point of there being 7 tasks in the system at the same
> time (2 running, 5
> queued).  Then, for the next 10 tasks, which are added and cause
> overflow, 10
> more threads will be started.
>
> Once we get to that point, the next overflow will cause rejection
> processing.
>
> With an unlimited queue, I'd like this same behavior, except that
> the last phase
> where we transition into rejection processing never occurs.
>
> Instead, I'd like the ability to, from the outside, cancel tasks
> via other
> statistics than just pending task count.
>
> Network applications can be bursty and hard to manage load swells
> because you
> aren't able to throttle peoples use of the system.  Also,
> interactions with
> other components of the system are not predictable.
>
> It just makes sense to me, that TPE should be using maxPoolSize
> by "counting"
> pending work when a queue is unbounded.  Should I guess that the
> synchronization
> for counting the items in the unbounded queue is the main reason
> why this was
> not done to begin with?  For me, this is one obvious place that
> queue size is
> needed.
>
> Gregg Wonderly


From joe.bowbeer at gmail.com  Wed Jun 16 00:12:08 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 15 Jun 2010 21:12:08 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
References: <4C17C776.7070906@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>

Gregg Wonderly writes:

> What I am saying is that I want that mode of operation when there is an
> unbounded queue and maxPoolSize is limited.  TPE only considers
> maxPoolSize when there is a bounded queue.
>


The SerialExecutor sample in the Executor javadoc can be extended to create
an executor that utilizes a maxPoolSize with an unbounded queue.

http://java.sun.com/javase/6/docs/api/index.html?java/util/concurrent/Executor.html

public class BoundedExecutor implements Executor {

     final Queue<Runnable> tasks = new ArrayDeque<Runnable>();
     final Executor executor;
     final int maxActive;
     int numActive;

     BoundedExecutor(Executor executor, int maxActive) {
         this.executor = executor;
         this.maxActive = maxActive;
     }

     public synchronized void execute(final Runnable r) {
         tasks.offer(new Runnable() {
             public void run() {
                 try {
                     r.run();
                 } finally {
                     scheduleNext(r);
                 }
             }
         });
         if (numActive < maxActive)
             scheduleNext(null);
     }

     protected synchronized void scheduleNext(Runnable previous) {
         if (previous != null)
             --numActive;
         Runnable task = tasks.poll();
         if (task != null) {
             executor.execute(task);
             numActive++;
         }
     }
}

Sample use:

ExecutorService es = Executors.newCachedThreadPool();
Executor exec = new BoundedExecutor(es, 10);
for (int i = 0; i < 1000; i++)
    exec.execute(new Task(i));


There's another BoundedExecutor in the JCiP samples, which uses a semaphore
to implement something similar to missing WaitPolicy:

http://jcip.net/listings/BoundedExecutor.java

Also see:

[concurrency-interest] WaitPolicy for ThreadPoolExecutor ?
http://cs.oswego.edu/pipermail/concurrency-interest/2009-February/005854.html

Joe

On Tue, Jun 15, 2010 at 3:43 PM, David Holmes wrote:

> Gregg,
>
> Thanks for clarifying. So to summarise what you would like is a way to set
> a
> limit on the queue size so that once N items are queued we start creating
> threads up to maxPoolSize, but beyond that we just continue queuing.
>
> As others have noted this can be emulated by chaining two TPEs eg by using
> the rejected-execution from the first to submit to the second.
>
> Ignoring the vaguaries associated with the size() operation on
> BlockingQueues this might be possible within the current framework. Though
> things can get even more complicated when you try to minimize thread
> creation as Martin described.
>
> David
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100615/46a07982/attachment.html>

From tim at peierls.net  Wed Jun 16 09:16:32 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 16 Jun 2010 09:16:32 -0400
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
References: <4C17C776.7070906@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
	<AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
Message-ID: <AANLkTikdY9fpD7cHY3AgMy3z4bFMk0YI0jfWINw_y2je@mail.gmail.com>

On Wed, Jun 16, 2010 at 12:12 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> There's another BoundedExecutor in the JCiP samples, which uses a semaphore
> to implement something similar to missing WaitPolicy:
>
> http://jcip.net/listings/BoundedExecutor.java


This "BoundedExecutor" doesn't implement Executor, but it could by restoring
the interrupt and exiting immediately when catching InterruptedException
from Semaphore.acquire():

    public void execute(final Runnable command) {
        try {
            semaphore.acquire();
        } catch (InterruptedException ex) {
            Thread.currentThread().interrupt(); // restore
            return; // silent failure to execute, with interrupt status set
        }
        try {
            exec.execute(new Runnable() {
                public void run() {
                    try {
                        command.run();
                    } finally {
                        semaphore.release();
                    }
                }
            });
        } catch (RejectedExecutionException e) {
            semaphore.release();
        }
    }

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100616/fc09e02f/attachment.html>

From jwesleysmith at atlassian.com  Wed Jun 16 20:59:32 2010
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Thu, 17 Jun 2010 10:59:32 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
 usemaxPoolSize effectively
In-Reply-To: <AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
References: <4C17C776.7070906@cytetech.com>	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
	<AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
Message-ID: <4C197374.7000505@atlassian.com>

Just for fun I spiked a test of a TPE usage that grows from core to max 
by keeping a number of active threads count, and if (a) there are no 
threads waiting for work and (b) # of active threads is < max then the 
runnable queue rejects the offer.

This approach works (for a fairly basic definition of works, there are a 
lot of concurrency edge cases I haven't come close to proving) by 
providing a special BlockingQueue that wraps the one passed in with an 
impl that checks the above, and a custom threadFactory that counts the 
number of active threads. To handle the case where less than max threads 
are around but some are idle it uses a SynchronousQueue as an 
elimination queue.

I may keep working on this as the idea seems to have some legs. The main 
problem is that it is fairly closely tied to the implementation details 
of TPE.

cheers,
jed.


From joe.bowbeer at gmail.com  Wed Jun 16 21:36:15 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 16 Jun 2010 18:36:15 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <4C197374.7000505@atlassian.com>
References: <4C17C776.7070906@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
	<AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
	<4C197374.7000505@atlassian.com>
Message-ID: <AANLkTikx12R1qyS8MeN-xR2dC5Yn3MDSlplvVR9swrfS@mail.gmail.com>

Would you get similar behavior using BoundedExecutor as follows?

ExecutorService es = Executors.newCachedThreadPool();
es.setCorePoolSize(corePoolSize);
Executor exec = new BoundedExecutor(es, maximumPoolSize);

On Wed, Jun 16, 2010 at 5:59 PM, Jed Wesley-Smith wrote:

> Just for fun I spiked a test of a TPE usage that grows from core to max by
> keeping a number of active threads count, and if (a) there are no threads
> waiting for work and (b) # of active threads is < max then the runnable
> queue rejects the offer.
>
> This approach works (for a fairly basic definition of works, there are a
> lot of concurrency edge cases I haven't come close to proving) by providing
> a special BlockingQueue that wraps the one passed in with an impl that
> checks the above, and a custom threadFactory that counts the number of
> active threads. To handle the case where less than max threads are around
> but some are idle it uses a SynchronousQueue as an elimination queue.
>
> I may keep working on this as the idea seems to have some legs. The main
> problem is that it is fairly closely tied to the implementation details of
> TPE.
>
> cheers,
> jed.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100616/3a821163/attachment.html>

From joe.bowbeer at gmail.com  Wed Jun 16 21:43:39 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 16 Jun 2010 18:43:39 -0700
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
	usemaxPoolSize effectively
In-Reply-To: <AANLkTikx12R1qyS8MeN-xR2dC5Yn3MDSlplvVR9swrfS@mail.gmail.com>
References: <4C17C776.7070906@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>
	<AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>
	<4C197374.7000505@atlassian.com>
	<AANLkTikx12R1qyS8MeN-xR2dC5Yn3MDSlplvVR9swrfS@mail.gmail.com>
Message-ID: <AANLkTikW-jjdEbnKaeLwYcEm1FeOLN1rrqdBKsgt6xc1@mail.gmail.com>

Correction:

ThreadPoolExecutor tpe = new ThreadPoolExecutor(
    corePoolSize, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS,
    new SynchronousQueue<Runnable>());
Executor exec = new BoundedExecutor(tpe, maximumPoolSize);


On Wed, Jun 16, 2010 at 6:36 PM, Joe Bowbeer wrote:

> Would you get similar behavior using BoundedExecutor as follows?
>
> ExecutorService es = Executors.newCachedThreadPool();
> es.setCorePoolSize(corePoolSize);
> Executor exec = new BoundedExecutor(es, maximumPoolSize);
>
> On Wed, Jun 16, 2010 at 5:59 PM, Jed Wesley-Smith wrote:
>
>> Just for fun I spiked a test of a TPE usage that grows from core to max by
>> keeping a number of active threads count, and if (a) there are no threads
>> waiting for work and (b) # of active threads is < max then the runnable
>> queue rejects the offer.
>>
>> This approach works (for a fairly basic definition of works, there are a
>> lot of concurrency edge cases I haven't come close to proving) by providing
>> a special BlockingQueue that wraps the one passed in with an impl that
>> checks the above, and a custom threadFactory that counts the number of
>> active threads. To handle the case where less than max threads are around
>> but some are idle it uses a SynchronousQueue as an elimination queue.
>>
>> I may keep working on this as the idea seems to have some legs. The main
>> problem is that it is fairly closely tied to the implementation details of
>> TPE.
>>
>> cheers,
>> jed.
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100616/b3cade2b/attachment.html>

From jwesleysmith at atlassian.com  Thu Jun 17 00:09:17 2010
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Thu, 17 Jun 2010 14:09:17 +1000
Subject: [concurrency-interest] TPE.execute() doesn't seem to really
 usemaxPoolSize effectively
In-Reply-To: <AANLkTikW-jjdEbnKaeLwYcEm1FeOLN1rrqdBKsgt6xc1@mail.gmail.com>
References: <4C17C776.7070906@cytetech.com>	<NFBBKALFDCPFIDBNKAPCEEADIHAA.davidcholmes@aapt.net.au>	<AANLkTimufESLrvTriOGV1N7UcBSWpry3zqEdDEfdq0ay@mail.gmail.com>	<4C197374.7000505@atlassian.com>	<AANLkTikx12R1qyS8MeN-xR2dC5Yn3MDSlplvVR9swrfS@mail.gmail.com>
	<AANLkTikW-jjdEbnKaeLwYcEm1FeOLN1rrqdBKsgt6xc1@mail.gmail.com>
Message-ID: <4C199FED.9070608@atlassian.com>

It is different in that it preserves the semantics of the passed in 
BlockingQueue and RejectedExecutionHandler. BoundedExecutor is unbound 
and therefore the RejectedExecutionHandler is moot as nothing will ever 
be rejected. If you pass in a bounded queue to mine then the number of 
waiting tasks is bound to that, and the REH is called once the queue is 
full. Also, it doesn't need to lock for task submission (although it 
does depend on the TPE's internal locking details atm.).

cheers,
jed.

Joe Bowbeer wrote:
> Correction:
>
> ThreadPoolExecutor tpe = new ThreadPoolExecutor(
>     corePoolSize, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS,
>     new SynchronousQueue<Runnable>());
> Executor exec = new BoundedExecutor(tpe, maximumPoolSize);
>
>
> On Wed, Jun 16, 2010 at 6:36 PM, Joe Bowbeer wrote:
>
>     Would you get similar behavior using BoundedExecutor as follows?
>
>     ExecutorService es = Executors.newCachedThreadPool();
>     es.setCorePoolSize(corePoolSize);
>     Executor exec = new BoundedExecutor(es, maximumPoolSize);
>
>     On Wed, Jun 16, 2010 at 5:59 PM, Jed Wesley-Smith wrote:
>
>         Just for fun I spiked a test of a TPE usage that grows from
>         core to max by keeping a number of active threads count, and
>         if (a) there are no threads waiting for work and (b) # of
>         active threads is < max then the runnable queue rejects the offer.
>
>         This approach works (for a fairly basic definition of works,
>         there are a lot of concurrency edge cases I haven't come close
>         to proving) by providing a special BlockingQueue that wraps
>         the one passed in with an impl that checks the above, and a
>         custom threadFactory that counts the number of active threads.
>         To handle the case where less than max threads are around but
>         some are idle it uses a SynchronousQueue as an elimination queue.
>
>         I may keep working on this as the idea seems to have some
>         legs. The main problem is that it is fairly closely tied to
>         the implementation details of TPE.
>

From alarmnummer at gmail.com  Fri Jun 18 14:10:43 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 18 Jun 2010 20:10:43 +0200
Subject: [concurrency-interest] Final local variable vs final field
Message-ID: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>

Hi All,

one of the things I'm wondering myself while looking at some concurrent code
from the JDK, is that often a field is copied to a local variable. E.g. from
the ThreadPoolExecutor:

private boolean addIfUnderCorePoolSize(Runnable firstTask) {
        Thread t = null;
        final ReentrantLock mainLock = this.mainLock;
        mainLock.lock();
        try {
            if (poolSize < corePoolSize && runState == RUNNING)
                t = addThread(firstTask);
        } finally {
            mainLock.unlock();
        }
        if (t == null)
            return false;
        t.start();
        return true;
    }

The ThreadPoolExecutor.mainLock field is final, so from a functional point
of view copying it to a local variable should not make a difference. I guess
it is related to that it is cheaper to access a local variable than a member
field or that the JIT can do better optimizations (even though an additional
GET_FIELD/ASTORE is needed and the size of the stackframe increases). Can
someone explain why this is done and when and when not to use this
technique?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/49a294c0/attachment.html>

From holger.hoffstaette at googlemail.com  Fri Jun 18 14:40:14 2010
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Fri, 18 Jun 2010 20:40:14 +0200
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
Message-ID: <4C1BBD8E.1080308@googlemail.com>

Peter Veentjer wrote:
> one of the things I'm wondering myself while looking at some concurrent
> code from the JDK, is that often a field is copied to a local variable.

Some time back it was said that this was indeed found to be (marginally?)
faster in certain cases. The only other reason I can think of is for when
you don't want to/cannot re-read e.g. a volatile ivar several times in a
method; taking "the state" and running with it can be much easier than
having to check for intermediate changes at every access and
cancelling/restarting loops etc.

-h


From kedar.mhaswade at gmail.com  Fri Jun 18 14:46:02 2010
From: kedar.mhaswade at gmail.com (kedar mhaswade)
Date: Fri, 18 Jun 2010 11:46:02 -0700
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
Message-ID: <AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>

+1. I am interested to know as well. AFAIK, this patterns is followed
elsewhere as well. For example, java.lang.String.hashCode().

-Kedar

On Fri, Jun 18, 2010 at 11:10 AM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi All,
>
> one of the things I'm wondering myself while looking at some concurrent
> code from the JDK, is that often a field is copied to a local variable. E.g.
> from the ThreadPoolExecutor:
>
> private boolean addIfUnderCorePoolSize(Runnable firstTask) {
>         Thread t = null;
>         final ReentrantLock mainLock = this.mainLock;
>         mainLock.lock();
>         try {
>             if (poolSize < corePoolSize && runState == RUNNING)
>                 t = addThread(firstTask);
>         } finally {
>             mainLock.unlock();
>         }
>         if (t == null)
>             return false;
>         t.start();
>         return true;
>     }
>
> The ThreadPoolExecutor.mainLock field is final, so from a functional point
> of view copying it to a local variable should not make a difference. I guess
> it is related to that it is cheaper to access a local variable than a member
> field or that the JIT can do better optimizations (even though an additional
> GET_FIELD/ASTORE is needed and the size of the stackframe increases). Can
> someone explain why this is done and when and when not to use this
> technique?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/699a415a/attachment.html>

From opinali at gmail.com  Fri Jun 18 14:59:21 2010
From: opinali at gmail.com (Osvaldo Doederlein)
Date: Fri, 18 Jun 2010 15:59:21 -0300
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
Message-ID: <AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>

Covered recently, start here where the hostpot devs start replying:

http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2010-May/003231.html

A+
Osvaldo

2010/6/18 kedar mhaswade <kedar.mhaswade at gmail.com>

> +1. I am interested to know as well. AFAIK, this patterns is followed
> elsewhere as well. For example, java.lang.String.hashCode().
>
> -Kedar
>
> On Fri, Jun 18, 2010 at 11:10 AM, Peter Veentjer <alarmnummer at gmail.com>wrote:
>
>> Hi All,
>>
>> one of the things I'm wondering myself while looking at some concurrent
>> code from the JDK, is that often a field is copied to a local variable. E.g.
>> from the ThreadPoolExecutor:
>>
>> private boolean addIfUnderCorePoolSize(Runnable firstTask) {
>>         Thread t = null;
>>         final ReentrantLock mainLock = this.mainLock;
>>         mainLock.lock();
>>         try {
>>             if (poolSize < corePoolSize && runState == RUNNING)
>>                 t = addThread(firstTask);
>>         } finally {
>>             mainLock.unlock();
>>         }
>>         if (t == null)
>>             return false;
>>         t.start();
>>         return true;
>>     }
>>
>> The ThreadPoolExecutor.mainLock field is final, so from a functional point
>> of view copying it to a local variable should not make a difference. I guess
>> it is related to that it is cheaper to access a local variable than a member
>> field or that the JIT can do better optimizations (even though an additional
>> GET_FIELD/ASTORE is needed and the size of the stackframe increases). Can
>> someone explain why this is done and when and when not to use this
>> technique?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/97cfbd39/attachment.html>

From hallorant at gmail.com  Fri Jun 18 15:28:35 2010
From: hallorant at gmail.com (Tim Halloran)
Date: Fri, 18 Jun 2010 15:28:35 -0400
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
Message-ID: <AANLkTimDEDH34_8pPe5c1fNfSS8aMwi_Tlq_2ZwISt1E@mail.gmail.com>

On Fri, Jun 18, 2010 at 2:59 PM, Osvaldo Doederlein <opinali at gmail.com>wrote:

> Covered recently, start here where the hostpot devs start replying:
>
>
> http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2010-May/003231.html
>
>
It is not clear how that post addresses the question -- there is no "hot
loop" involved in the example.


> A+
> Osvaldo
>
> 2010/6/18 kedar mhaswade <kedar.mhaswade at gmail.com>
>
> +1. I am interested to know as well. AFAIK, this patterns is followed
>> elsewhere as well. For example, java.lang.String.hashCode().
>>
>> -Kedar
>>
>> On Fri, Jun 18, 2010 at 11:10 AM, Peter Veentjer <alarmnummer at gmail.com>wrote:
>>
>>> Hi All,
>>>
>>> one of the things I'm wondering myself while looking at some concurrent
>>> code from the JDK, is that often a field is copied to a local variable. E.g.
>>> from the ThreadPoolExecutor:
>>>
>>> private boolean addIfUnderCorePoolSize(Runnable firstTask) {
>>>         Thread t = null;
>>>         final ReentrantLock mainLock = this.mainLock;
>>>         mainLock.lock();
>>>         try {
>>>             if (poolSize < corePoolSize && runState == RUNNING)
>>>                 t = addThread(firstTask);
>>>         } finally {
>>>             mainLock.unlock();
>>>         }
>>>         if (t == null)
>>>             return false;
>>>         t.start();
>>>         return true;
>>>     }
>>>
>>> The ThreadPoolExecutor.mainLock field is final, so from a functional
>>> point of view copying it to a local variable should not make a difference. I
>>> guess it is related to that it is cheaper to access a local variable than a
>>> member field or that the JIT can do better optimizations (even though an
>>> additional GET_FIELD/ASTORE is needed and the size of the stackframe
>>> increases). Can someone explain why this is done and when and when not to
>>> use this technique?
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/46d6cd2a/attachment-0001.html>

From dl at cs.oswego.edu  Fri Jun 18 15:43:06 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 18 Jun 2010 15:43:06 -0400
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTimDEDH34_8pPe5c1fNfSS8aMwi_Tlq_2ZwISt1E@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
	<AANLkTimDEDH34_8pPe5c1fNfSS8aMwi_Tlq_2ZwISt1E@mail.gmail.com>
Message-ID: <4C1BCC4A.6020201@cs.oswego.edu>

On 06/18/10 15:28, Tim Halloran wrote:
> On Fri, Jun 18, 2010 at 2:59 PM, Osvaldo Doederlein <opinali at gmail.com
> <mailto:opinali at gmail.com>> wrote:
>
>     Covered recently, start here where the hostpot devs start replying:
>
>     http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2010-May/003231.html
>
>
> It is not clear how that post addresses the question -- there is no "hot
> loop" involved in the example.
>

Many if not all JVMs sometimes have trouble understanding
that final fields do not need to be reloaded across locks/volatiles.
This is ultimately mainly due to the fact that the JVM rules for
final fields are weaker than the Java Language rules.

While I don't particularly recommend this style for ordinary
use, I don't discourage it either, especially for non-final
fields. As a matter of practice, it doesn't hurt to enforce
any code logic that relies on using a single read of a possible
changing variable.

-Doug



>     A+
>     Osvaldo
>
>     2010/6/18 kedar mhaswade <kedar.mhaswade at gmail.com
>     <mailto:kedar.mhaswade at gmail.com>>
>
>         +1. I am interested to know as well. AFAIK, this patterns is
>         followed elsewhere as well. For example,
>         java.lang.String.hashCode().
>
>         -Kedar
>
>         On Fri, Jun 18, 2010 at 11:10 AM, Peter Veentjer
>         <alarmnummer at gmail.com <mailto:alarmnummer at gmail.com>> wrote:
>
>             Hi All,
>
>             one of the things I'm wondering myself while looking at some
>             concurrent code from the JDK, is that often a field is
>             copied to a local variable. E.g. from the ThreadPoolExecutor:
>
>             private boolean addIfUnderCorePoolSize(Runnable firstTask) {
>                      Thread t = null;
>                      final ReentrantLock mainLock = this.mainLock;
>                      mainLock.lock();
>                      try {
>                          if (poolSize < corePoolSize && runState == RUNNING)
>                              t = addThread(firstTask);
>                      } finally {
>                          mainLock.unlock();
>                      }
>                      if (t == null)
>                          return false;
>                      t.start();
>                      return true;
>                  }
>
>             The ThreadPoolExecutor.mainLock field is final, so from a
>             functional point of view copying it to a local variable
>             should not make a difference. I guess it is related to that
>             it is cheaper to access a local variable than a member field
>             or that the JIT can do better optimizations (even though an
>             additional GET_FIELD/ASTORE is needed and the size of the
>             stackframe increases). Can someone explain why this is done
>             and when and when not to use this technique?
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From brian at briangoetz.com  Fri Jun 18 15:45:33 2010
From: brian at briangoetz.com (Brian Goetz)
Date: Fri, 18 Jun 2010 15:45:33 -0400
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
Message-ID: <4C1BCCDD.4090908@briangoetz.com>

Note that this is only talking about C1, the client compiler, which is used in 
the minority of performance-critical applications.  So my usual advice 
applies: don't worry.

On 6/18/2010 2:59 PM, Osvaldo Doederlein wrote:
> Covered recently, start here where the hostpot devs start replying:
>
> http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2010-May/003231.html
>
> A+
> Osvaldo
>
> 2010/6/18 kedar mhaswade <kedar.mhaswade at gmail.com
> <mailto:kedar.mhaswade at gmail.com>>
>
>     +1. I am interested to know as well. AFAIK, this patterns is
>     followed elsewhere as well. For example, java.lang.String.hashCode().
>
>     -Kedar
>
>     On Fri, Jun 18, 2010 at 11:10 AM, Peter Veentjer
>     <alarmnummer at gmail.com <mailto:alarmnummer at gmail.com>> wrote:
>
>         Hi All,
>
>         one of the things I'm wondering myself while looking at some
>         concurrent code from the JDK, is that often a field is copied to
>         a local variable. E.g. from the ThreadPoolExecutor:
>
>         private boolean addIfUnderCorePoolSize(Runnable firstTask) {
>                  Thread t = null;
>                  final ReentrantLock mainLock = this.mainLock;
>                  mainLock.lock();
>                  try {
>                      if (poolSize < corePoolSize && runState == RUNNING)
>                          t = addThread(firstTask);
>                  } finally {
>                      mainLock.unlock();
>                  }
>                  if (t == null)
>                      return false;
>                  t.start();
>                  return true;
>              }
>
>         The ThreadPoolExecutor.mainLock field is final, so from a
>         functional point of view copying it to a local variable should
>         not make a difference. I guess it is related to that it is
>         cheaper to access a local variable than a member field or that
>         the JIT can do better optimizations (even though an additional
>         GET_FIELD/ASTORE is needed and the size of the stackframe
>         increases). Can someone explain why this is done and when and
>         when not to use this technique?
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From alexdmiller at yahoo.com  Fri Jun 18 15:49:42 2010
From: alexdmiller at yahoo.com (Alex Miller)
Date: Fri, 18 Jun 2010 12:49:42 -0700 (PDT)
Subject: [concurrency-interest] Final local variable vs final     field
Message-ID: <879287.56072.qm@web32503.mail.mud.yahoo.com>

> +1. I am interested to know as well. AFAIK, this patterns is followed
> elsewhere as well. For example, java.lang.String.hashCode().>
> -Kedar

The String case is actually different in that the hash field is NOT volatile.  This is an example of the racy single check pattern, which is discussed in Effective Java (with this as the example).  It's fairly important to copy the value locally to be isolated from changes potentially happening in other threads on the hash. 

http://books.google.com/books?id=ka2VUBqHiWkC&lpg=PA284&ots=yXJlQnn_U_&dq=racy%20single%20check&pg=PA284#v=onepage&q=racy%20single%20check&f=false

From joe.bowbeer at gmail.com  Fri Jun 18 16:38:50 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 18 Jun 2010 13:38:50 -0700
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <4C1BCCDD.4090908@briangoetz.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
	<4C1BCCDD.4090908@briangoetz.com>
Message-ID: <AANLkTimmL-G2M8V1T67wcxx01cmGHImgzQovnfArM7Z-@mail.gmail.com>

On Fri, Jun 18, 2010 at 12:45 PM, Brian Goetz wrote:

> Note that this is only talking about C1, the client compiler, which is used
> in the minority of performance-critical applications.  So my usual advice
> applies: don't worry.


But if you do, consider using a new name for the local variable in order to
avoid warnings from the static analyzer:

  *Local variable declaration hides another field or variable*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/fb350534/attachment.html>

From forax at univ-mlv.fr  Fri Jun 18 17:10:59 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 18 Jun 2010 23:10:59 +0200
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <AANLkTimmL-G2M8V1T67wcxx01cmGHImgzQovnfArM7Z-@mail.gmail.com>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>	<4C1BCCDD.4090908@briangoetz.com>
	<AANLkTimmL-G2M8V1T67wcxx01cmGHImgzQovnfArM7Z-@mail.gmail.com>
Message-ID: <4C1BE0E3.6060100@univ-mlv.fr>

Le 18/06/2010 22:38, Joe Bowbeer a ?crit :
> On Fri, Jun 18, 2010 at 12:45 PM, Brian Goetz wrote:
>
>     Note that this is only talking about C1, the client compiler,
>     which is used in the minority of performance-critical
>     applications.  So my usual advice applies: don't worry.
>
>
> But if you do, consider using a new name for the local variable in 
> order to avoid warnings from the static analyzer:
>
> /Local variable declaration hides another field or variable/

or tweak the analyzer to accept the pattern this.foo = foo;

R?mi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/7683ad77/attachment.html>

From joe.bowbeer at gmail.com  Fri Jun 18 18:31:14 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 18 Jun 2010 15:31:14 -0700
Subject: [concurrency-interest] Final local variable vs final field
In-Reply-To: <4C1BE0E3.6060100@univ-mlv.fr>
References: <AANLkTinDhsZrEQVXv4eYQseiqa9_-FfPUT5cP33BYAsJ@mail.gmail.com>
	<AANLkTiklMcPhhvVaFM46BJDmItGXe0VrCG0exSt-i74p@mail.gmail.com>
	<AANLkTiliVALMfV-OlfV6at42P-jwjmbqtUtloxCQT5Og@mail.gmail.com>
	<4C1BCCDD.4090908@briangoetz.com>
	<AANLkTimmL-G2M8V1T67wcxx01cmGHImgzQovnfArM7Z-@mail.gmail.com>
	<4C1BE0E3.6060100@univ-mlv.fr>
Message-ID: <AANLkTilePvjlgx5hg90WdJN1FCcGUoIBRsOCUQYbAWZZ@mail.gmail.com>

The Eclipse analyzer already accepts constructor-style assignments.

  this.foo = foo;

and I assume the reverse is also true, that is:

  foo = this.foo;

but uses of unqualified "foo" (!) outside of these assignments will be
flagged if this warning is enabled.


On Fri, Jun 18, 2010 at 2:10 PM, R?mi Forax wrote:

>  Le 18/06/2010 22:38, Joe Bowbeer a ?crit :
>
> On Fri, Jun 18, 2010 at 12:45 PM, Brian Goetz wrote:
>
>> Note that this is only talking about C1, the client compiler, which is
>> used in the minority of performance-critical applications.  So my usual
>> advice applies: don't worry.
>
>
> But if you do, consider using a new name for the local variable in order to
> avoid warnings from the static analyzer:
>
>   *Local variable declaration hides another field or variable*
>
>
> or tweak the analyzer to accept the pattern this.foo = foo;
>
> R?mi
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100618/95b7add5/attachment.html>

From stuff at kai.meder.info  Sat Jun 19 12:04:26 2010
From: stuff at kai.meder.info (stuff at kai.meder.info)
Date: Sat, 19 Jun 2010 18:04:26 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
Message-ID: <4C1CEA8A.5030205@kai.meder.info>

Hello

Currently I'm using a Semaphore(1) as a lock because I need a
locking-mechanism which may be acquired by one Thread but released by
another.
This does not seem to be possible with a ReentrantLock.

Is there any major performance difference between these two?
Are there other, more lightweight Constructs for locking access to
variables/resources with the ability to unlock from an different Thread?

Thanks

From alarmnummer at gmail.com  Sun Jun 20 08:16:28 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 20 Jun 2010 14:16:28 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1CEA8A.5030205@kai.meder.info>
References: <4C1CEA8A.5030205@kai.meder.info>
Message-ID: <AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>

You can always cook up something yourself, e.g.:

class CheapLock{
    private volatile boolean isLocked = false;

    public void lock(){
        synchronized(this){
            if(isLocked){
                throw new IllegalMonitorStateException();
            }
            isLocked = true;
        }
    }

    public void unlock(){
        synchronized(this){
            if(!isLocked){
                throw new IllegalMonitorStateException();
            }
            isLocked = false;
        }
    }

    public boolean isLocked(){
        return isLockded;
    }
}

Or use it in combination with a ReentrantLock/Condition if you want to have
more control on timeouts, interruptibility etc.

And if you don't need the blocking part, you can even rely on a cas.

class NonBlockingLock{
    private final AtomicBoolean locked = new AtomicBoolean(false);

    public boolean lock(){
        return locked.compareAndSet(false,true);
    }

    public void unlock(){
        return locked.compareAndSet(true,false);
    }

    public boolean isLocked(){
        return locked.get();
    }
}

But both examples really should be used with a lot of care (a lock can be
stolen and can case race problems and other unexpected behaviour).


On Sat, Jun 19, 2010 at 6:04 PM, <stuff at kai.meder.info> wrote:

> Hello
>
> Currently I'm using a Semaphore(1) as a lock because I need a
> locking-mechanism which may be acquired by one Thread but released by
> another.
> This does not seem to be possible with a ReentrantLock.
>
> Is there any major performance difference between these two?
> Are there other, more lightweight Constructs for locking access to
> variables/resources with the ability to unlock from an different Thread?
>
> Thanks
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/9900ff8f/attachment.html>

From stuff at kai.meder.info  Sun Jun 20 10:10:33 2010
From: stuff at kai.meder.info (stuff at kai.meder.info)
Date: Sun, 20 Jun 2010 16:10:33 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
Message-ID: <4C1E2159.2090806@kai.meder.info>

Thank you.
However, I'm limited in the use of synchronized as I work in a
CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.

So I need a blocking lock-mechanism, which may be unlocked by a thread
different to the acquiring thread.
Does a semaphore "cost" more than a ReentrantLock? Are there other
constructs for this use?

On 20.06.2010 14:16, Peter Veentjer wrote:
> You can always cook up something yourself, e.g.:

From tim at peierls.net  Sun Jun 20 10:25:39 2010
From: tim at peierls.net (Tim Peierls)
Date: Sun, 20 Jun 2010 10:25:39 -0400
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1E2159.2090806@kai.meder.info>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
	<4C1E2159.2090806@kai.meder.info>
Message-ID: <AANLkTilaSrq6AwwMgwTeDEWYGCYqFgQE0eQ1cRWbwaSQ@mail.gmail.com>

Both Semaphore and ReentrantLock are extremely likely to be implemented in
terms of AbstractQueuedSynchronizer, so if those are your only choices, pick
the one that fits your usage best.

--tim

On Sun, Jun 20, 2010 at 10:10 AM, <stuff at kai.meder.info> wrote:

> Thank you.
> However, I'm limited in the use of synchronized as I work in a
> CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.
>
> So I need a blocking lock-mechanism, which may be unlocked by a thread
> different to the acquiring thread.
> Does a semaphore "cost" more than a ReentrantLock? Are there other
> constructs for this use?
>
> On 20.06.2010 14:16, Peter Veentjer wrote:
> > You can always cook up something yourself, e.g.:
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/2e9ab8e6/attachment.html>

From alarmnummer at gmail.com  Sun Jun 20 10:30:15 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 20 Jun 2010 16:30:15 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1E2159.2090806@kai.meder.info>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
	<4C1E2159.2090806@kai.meder.info>
Message-ID: <AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>

On Sun, Jun 20, 2010 at 4:10 PM, <stuff at kai.meder.info> wrote:

> Thank you.
> However, I'm limited in the use of synchronized as I work in a
> CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.
>

What always can be done is exposing the MONITOR_ENTER and MONITOR_EXIT
bytecode instructions behind some generated methods. So you can use these
instructions as a basis for constructing your desired lock (important to
realise that these instructions have the same limitation as the
ReentrantLock; the release needs to be done by the same thread as the
acquire.

I think an intrinsic lock is cheaper (memory wise) than a ReentrantLock
since less objects need to be created (although I don't know what happens
behind the screens). And an intrinsic lock also benefits
from features like biased locking. Afaik this is not available for the
ReentrantLock.

But.. if you need timeouts or interruptible behavior.. the intrinsic lock is
not your friend.

So you can rewrite it using a ReentrantLock.

class Lock{
    private volatile boolean isLocked = false;
    private ReentrantLock lock = new ReentrantLock();
    private Condition condition = lock.newCondition();

    public void lock(){
        lock.lock();
        try{
            while(isLocked){
                condition.await();
            }
            isLocked = true;
        }finally{
            lock.unlock();
        }
    }

    public void unlock(){
        if(!isLocked)throw new IllegalMonitorStateException();
        try{
            if(!isLocked)throw new IllegalMonitorStateException();
            isLocked = true;
            condition.signallAll();
        }finally{
            lock.unlock();
        }
    }

    public boolean isLocked(){
        return isLocked;
    }
}

You can add the timed version yourself


> So I need a blocking lock-mechanism, which may be unlocked by a thread
> different to the acquiring thread.
> Does a semaphore "cost" more than a ReentrantLock?


It depends how the semaphore is constructed. A semaphore based on an
intrinsic lock I expect is cheaper than a semaphore based on
ReentrantLock/Condition since more objects are needed. To make sure, you
need to create some kind of benchmark and see which one is faster (although
creating such a benchmark can be a challenge in itself).


> Are there other
> constructs for this use?
>
> On 20.06.2010 14:16, Peter Veentjer wrote:
> > You can always cook up something yourself, e.g.:
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/9a3ca9ee/attachment.html>

From stuff at kai.meder.info  Sun Jun 20 10:10:33 2010
From: stuff at kai.meder.info (stuff at kai.meder.info)
Date: Sun, 20 Jun 2010 16:10:33 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
Message-ID: <4C1E2159.2090806@kai.meder.info>

Thank you.
However, I'm limited in the use of synchronized as I work in a
CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.

So I need a blocking lock-mechanism, which may be unlocked by a thread
different to the acquiring thread.
Does a semaphore "cost" more than a ReentrantLock? Are there other
constructs for this use?

On 20.06.2010 14:16, Peter Veentjer wrote:
> You can always cook up something yourself, e.g.:


From forax at univ-mlv.fr  Sun Jun 20 10:51:01 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sun, 20 Jun 2010 16:51:01 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
References: <4C1CEA8A.5030205@kai.meder.info>	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>	<4C1E2159.2090806@kai.meder.info>
	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
Message-ID: <4C1E2AD5.6000307@univ-mlv.fr>

Le 20/06/2010 16:30, Peter Veentjer a ?crit :
> On Sun, Jun 20, 2010 at 4:10 PM, <stuff at kai.meder.info 
> <mailto:stuff at kai.meder.info>> wrote:
>
>     Thank you.
>     However, I'm limited in the use of synchronized as I work in a
>     CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.
>
>
> What always can be done is exposing the MONITOR_ENTER and MONITOR_EXIT 
> bytecode instructions behind some generated methods. So you can use 
> these instructions as a basis for constructing your desired lock 
> (important to realise that these instructions have the same limitation 
> as the ReentrantLock; the release needs to be done by the same thread 
> as the acquire.
>
> I think an intrinsic lock is cheaper (memory wise) than a 
> ReentrantLock since less objects need to be created (although I don't 
> know what happens behind the screens). And an intrinsic lock also 
> benefits
> from features like biased locking. Afaik this is not available for the 
> ReentrantLock.
>
> But.. if you need timeouts or interruptible behavior.. the intrinsic 
> lock is not your friend.
>
> So you can rewrite it using a ReentrantLock.
>
> class Lock{
>     private volatile boolean isLocked = false;
>     private ReentrantLock lock = new ReentrantLock();
>     private Condition condition = lock.newCondition();

         ^--->  final & final

>
>     public void lock(){
>         lock.lock();
>         try{
>             while(isLocked){
>                 condition.await();
>             }
>             isLocked = true;
>         }finally{
>             lock.unlock();
>         }
>     }
>
>     public void unlock(){
>         if(!isLocked)throw new IllegalMonitorStateException();

             ^--------> boooom

>         try{
>             if(!isLocked)throw new IllegalMonitorStateException();
>             isLocked = true;
>             condition.signallAll();
>         }finally{
>             lock.unlock();
>         }
>     }
>
>     public boolean isLocked(){
>         return isLocked;

                         ^------>   booom

>     }
> }
>

isLocked must be read between lock() and unlock().

R?mi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/2c302af1/attachment-0001.html>

From alarmnummer at gmail.com  Sun Jun 20 11:12:06 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 20 Jun 2010 17:12:06 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1E2AD5.6000307@univ-mlv.fr>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
	<4C1E2159.2090806@kai.meder.info>
	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
	<4C1E2AD5.6000307@univ-mlv.fr>
Message-ID: <AANLkTikLwydI4fsSfmAbU-RPWjiPjo75kQf_mENAh4ED@mail.gmail.com>

On Sun, Jun 20, 2010 at 4:51 PM, R?mi Forax <forax at univ-mlv.fr> wrote:

>  Le 20/06/2010 16:30, Peter Veentjer a ?crit :
>
>
>     private volatile boolean isLocked = false;
>     private ReentrantLock lock = new ReentrantLock();
>     private Condition condition = lock.newCondition();
>  class Lock{
>
>
>         ^--->  final & final
>
>
>
>From a correctness point of view it is better to make them final. But it
won't cause JMM issues since the Lock object itself needs to be published
safely to be used by different threads. This publications makes sure that
all writes done before the publication are visible after usage.

PS: I make them final as well.


>
>     public void lock(){
>         lock.lock();
>         try{
>             while(isLocked){
>                 condition.await();
>             }
>             isLocked = true;
>         }finally{
>             lock.unlock();
>         }
>     }
>
>     public void unlock(){
>         if(!isLocked)throw new IllegalMonitorStateException();
>
>
>             ^--------> boooom
>
>
That is not correct. isLocked is volatile so nothing bad can happen.


>
>          try{
>             if(!isLocked)throw new IllegalMonitorStateException();
>             isLocked = true;
>             condition.signallAll();
>         }finally{
>             lock.unlock();
>         }
>     }
>
>     public boolean isLocked(){
>         return isLocked;
>
>
>                         ^------>   booom
>
>      }
> }
>
>
>
I think you missed the volatile access modifier in the islocked field


> isLocked must be read between lock() and unlock().
>

Not needed when using the volatile access modifier.


>
> R?mi
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/b3d9428a/attachment.html>

From forax at univ-mlv.fr  Sun Jun 20 11:38:54 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sun, 20 Jun 2010 17:38:54 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <AANLkTikLwydI4fsSfmAbU-RPWjiPjo75kQf_mENAh4ED@mail.gmail.com>
References: <4C1CEA8A.5030205@kai.meder.info>	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>	<4C1E2159.2090806@kai.meder.info>	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>	<4C1E2AD5.6000307@univ-mlv.fr>
	<AANLkTikLwydI4fsSfmAbU-RPWjiPjo75kQf_mENAh4ED@mail.gmail.com>
Message-ID: <4C1E360E.8030407@univ-mlv.fr>

Le 20/06/2010 17:12, Peter Veentjer a ?crit :
>
>
> On Sun, Jun 20, 2010 at 4:51 PM, R?mi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     Le 20/06/2010 16:30, Peter Veentjer a ?crit :
>>
>>         private volatile boolean isLocked = false;
>>         private ReentrantLock lock = new ReentrantLock();
>>         private Condition condition = lock.newCondition();
>>     class Lock{
>
>             ^--->  final & final
>
>
>
> From a correctness point of view it is better to make them final. But 
> it won't cause JMM issues since the Lock object itself needs to be 
> published safely to be used by different threads. This publications 
> makes sure that all writes done before the publication are visible 
> after usage.
>
> PS: I make them final as well.
>
>>
>>         public void lock(){
>>             lock.lock();
>>             try{
>>                 while(isLocked){
>>                     condition.await();
>>                 }
>>                 isLocked = true;
>>             }finally{
>>                 lock.unlock();
>>             }
>>         }
>>
>>         public void unlock(){
>>             if(!isLocked)throw new IllegalMonitorStateException();
>
>                 ^--------> boooom
>
>
> That is not correct. isLocked is volatile so nothing bad can happen.
>
>
>>             try{
>>                 if(!isLocked)throw new IllegalMonitorStateException();
>>                 isLocked = true;
>>                 condition.signallAll();
>>             }finally{
>>                 lock.unlock();
>>             }
>>         }
>>
>>         public boolean isLocked(){
>>             return isLocked;
>
>                             ^------>   booom
>
>>         }
>>     }
>>
>
>
> I think you missed the volatile access modifier in the islocked field
>
>     isLocked must be read between lock() and unlock().
>
>
> Not needed when using the volatile access modifier.

Sorry, I've miss the volatile field. Why don't you declare it volatile.
In my opinion, it's better to remove isLocked() because the information
returned by this method is useless and remove the first test in
unlock because you will test it after and optimizing for a case
that should never happen is useless.

In that case, you can remove the 'volatile'.

R?mi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/98a3f141/attachment.html>

From jdmarshall at gmail.com  Sun Jun 20 16:55:37 2010
From: jdmarshall at gmail.com (J Marshall)
Date: Sun, 20 Jun 2010 13:55:37 -0700
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1E360E.8030407@univ-mlv.fr>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
	<4C1E2159.2090806@kai.meder.info>
	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
	<4C1E2AD5.6000307@univ-mlv.fr>
	<AANLkTikLwydI4fsSfmAbU-RPWjiPjo75kQf_mENAh4ED@mail.gmail.com>
	<4C1E360E.8030407@univ-mlv.fr>
Message-ID: <BDDF8FC5-EF89-4456-A262-DAEEBD8BAC75@gmail.com>

Given that isLocked is a public nonfinal method, I would tend to  
agree.  You're only JMM correct until someone extends the code.



On Jun 20, 2010, at 8:38 AM, R?mi Forax <forax at univ-mlv.fr> wrote:

> Le 20/06/2010 17:12, Peter Veentjer a ?crit :
>>
>>
>>
>> On Sun, Jun 20, 2010 at 4:51 PM, R?mi Forax <forax at univ-mlv.fr> wr 
>> ote:
>> Le 20/06/2010 16:30, Peter Veentjer a ?crit :
>>>
>>>     private volatile boolean isLocked = false;
>>>     private ReentrantLock lock = new ReentrantLock();
>>>     private Condition condition = lock.newCondition();
>>> class Lock{
>>
>>         ^--->  final & final
>>
>>
>>
>> >From a correctness point of view it is better to make them final.  
>> But it won't cause JMM issues since the Lock object itself needs to  
>> be published safely to be used by different threads. This  
>> publications makes sure that all writes done before the publication  
>> are visible after usage.
>>
>> PS: I make them final as well.
>>
>>>
>>>     public void lock(){
>>>         lock.lock();
>>>         try{
>>>             while(isLocked){
>>>                 condition.await();
>>>             }
>>>             isLocked = true;
>>>         }finally{
>>>             lock.unlock();
>>>         }
>>>     }
>>>
>>>     public void unlock(){
>>>         if(!isLocked)throw new IllegalMonitorStateException();
>>
>>             ^--------> boooom
>>
>>
>> That is not correct. isLocked is volatile so nothing bad can happen.
>>
>>
>>>         try{
>>>             if(!isLocked)throw new IllegalMonitorStateException();
>>>             isLocked = true;
>>>             condition.signallAll();
>>>         }finally{
>>>             lock.unlock();
>>>         }
>>>     }
>>>
>>>     public boolean isLocked(){
>>>         return isLocked;
>>
>>                         ^------>   booom
>>
>>>     }
>>> }
>>>
>>
>>
>> I think you missed the volatile access modifier in the islocked field
>>
>> isLocked must be read between lock() and unlock().
>>
>> Not needed when using the volatile access modifier.
>
> Sorry, I've miss the volatile field. Why don't you declare it  
> volatile.
> In my opinion, it's better to remove isLocked() because the  
> information
> returned by this method is useless and remove the first test in
> unlock because you will test it after and optimizing for a case
> that should never happen is useless.
>
> In that case, you can remove the 'volatile'.
>
> R?mi
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100620/94405ed5/attachment.html>

From david.lloyd at redhat.com  Mon Jun 21 09:33:03 2010
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Mon, 21 Jun 2010 08:33:03 -0500
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
References: <4C1CEA8A.5030205@kai.meder.info>	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>	<4C1E2159.2090806@kai.meder.info>
	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
Message-ID: <4C1F6A0F.6070900@redhat.com>

On 06/20/2010 09:30 AM, Peter Veentjer wrote:
> On Sun, Jun 20, 2010 at 4:10 PM, <stuff at kai.meder.info
> <mailto:stuff at kai.meder.info>> wrote:
>
>     Thank you.
>     However, I'm limited in the use of synchronized as I work in a
>     CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.
>
>
> What always can be done is exposing the MONITOR_ENTER and MONITOR_EXIT
> bytecode instructions behind some generated methods. So you can use
> these instructions as a basis for constructing your desired lock
> (important to realise that these instructions have the same limitation
> as the ReentrantLock; the release needs to be done by the same thread as
> the acquire.

I don't think so - I think the verifier will barf if you don't have a 
MONITOR_EXIT in a spot corresponding to MONITOR_ENTER in the same method. 
I don't remember for sure though, might be a fun experiment...
-- 
- DML ?

From forax at univ-mlv.fr  Mon Jun 21 10:59:18 2010
From: forax at univ-mlv.fr (=?UTF-8?B?UsOpbWkgRm9yYXg=?=)
Date: Mon, 21 Jun 2010 16:59:18 +0200
Subject: [concurrency-interest] Semaphore vs. ReentrantLock
In-Reply-To: <4C1F6A0F.6070900@redhat.com>
References: <4C1CEA8A.5030205@kai.meder.info>	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>	<4C1E2159.2090806@kai.meder.info>	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
	<4C1F6A0F.6070900@redhat.com>
Message-ID: <4C1F7E46.6020202@univ-mlv.fr>

Le 21/06/2010 15:33, David M. Lloyd a ?crit :
> On 06/20/2010 09:30 AM, Peter Veentjer wrote:
>> On Sun, Jun 20, 2010 at 4:10 PM, <stuff at kai.meder.info
>> <mailto:stuff at kai.meder.info>> wrote:
>>
>>     Thank you.
>>     However, I'm limited in the use of synchronized as I work in a
>>     CPS-Setting constructed by the Delimited-CPS scala-compiler-plugin.
>>
>>
>> What always can be done is exposing the MONITOR_ENTER and MONITOR_EXIT
>> bytecode instructions behind some generated methods. So you can use
>> these instructions as a basis for constructing your desired lock
>> (important to realise that these instructions have the same limitation
>> as the ReentrantLock; the release needs to be done by the same thread as
>> the acquire.
>
> I don't think so - I think the verifier will barf if you don't have a 
> MONITOR_EXIT in a spot corresponding to MONITOR_ENTER in the same 
> method. I don't remember for sure though, might be a fun experiment...

As a workaround, you can use unsafe.monitorEnter()/unsafe.monitorExit().
there is also a tryMonitorEnter which is a tryLock() on monitor.

http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/tip/src/share/classes/sun/misc/Unsafe.java

R?mi
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100621/e6992109/attachment.html>

From bryan at systap.com  Fri Jun 25 09:29:54 2010
From: bryan at systap.com (Bryan Thompson)
Date: Fri, 25 Jun 2010 08:29:54 -0500
Subject: [concurrency-interest] Threads waiting on a ReentrantLock not held
	by any thread?
In-Reply-To: <4C1F7E46.6020202@univ-mlv.fr>
References: <4C1CEA8A.5030205@kai.meder.info>
	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>
	<4C1E2159.2090806@kai.meder.info>
	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>
	<4C1F6A0F.6070900@redhat.com> <4C1F7E46.6020202@univ-mlv.fr>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44D685E0AEC@AUSP01VMBX06.collaborationhost.net>

Hello,

I am hoping that someone can offer me some insight into problem in which a process is failing to make progress because several threads are waiting on a lock which, based on a thread dump, is not help by any thread in the JVM.  This is JDK 1.6.0_17.  Command line options include: -server -ea -showversion  -Xmx22G -XX:+UseParallelOldGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:jvm_gc.log.

A sample stack trace is inline below.  The ReentrantLock in question is a private final field of the Latch class.  While there are a number of threads waiting on that lock, I can not identify any thread in the stack trace which actually holds the lock (no stack traces pass through any code in which the lock is held).

I can provide a full thread dump, which was obtained using jstack, but mainly I would like to know if there are known reasons why a lock might be held but not displayed in a thread dump.  Or, alternatively, if there were known problems with ReentrantLock for this JVM version, interactions with the garbage collection, etc.

Any pointers would be very much appreciated.

Thanks,
Bryan

"com.bigdata.service.jini.JiniFederation.executorService424" daemon prio=10 tid=0x00002ab07366b800 nid=0x724d waiting on condition [0x0000000060188000]
   java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for  <0x00002aaab43b4cf0> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
 at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:747)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:778)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1114)
 at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
 at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
 at com.bigdata.util.concurrent.Latch.dec(Latch.java:206)
 at com.bigdata.service.ndx.pipeline.KVOC.done(KVOC.java:60)
 at com.bigdata.service.ndx.pipeline.IndexPartitionWriteTask.handleChunk(IndexPartitionWriteTask.java:285)
 at com.bigdata.service.ndx.pipeline.IndexPartitionWriteTask.handleChunk(IndexPartitionWriteTask.java:53)
 at com.bigdata.service.ndx.pipeline.AbstractSubtask.call(AbstractSubtask.java:182)
 at com.bigdata.service.ndx.pipeline.AbstractSubtask.call(AbstractSubtask.java:66)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:619)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100625/419a32b2/attachment.html>

From ariel at weisberg.ws  Fri Jun 25 09:48:24 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Fri, 25 Jun 2010 09:48:24 -0400
Subject: [concurrency-interest] Threads waiting on a ReentrantLock not
 held by any thread?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D685E0AEC@AUSP01VMBX06.collaborationhost.net>
References: <4C1CEA8A.5030205@kai.meder.info><AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com><4C1E2159.2090806@kai.meder.info><AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com><4C1F6A0F.6070900@redhat.com>
	<4C1F7E46.6020202@univ-mlv.fr>
	<DE10B00CCE0DC54883734F3060AC9ED44D685E0AEC@AUSP01VMBX06.collaborationhost.net>
Message-ID: <1277473704.18106.1381877275@webmail.messagingengine.com>

Hi,

That sounds like it might be similar to
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6822370 Maybe
try JDK 1.6.0_18?

Hope this helps,
Ariel Weisberg
On Fri, 25 Jun 2010 08:29 -0500, "Bryan Thompson"
<bryan at systap.com> wrote:

Hello,

I am hoping that someone can offer me some insight into problem
in which a process is failing to make progress because several
threads are waiting on a lock which, based on a thread dump, is
not help by any thread in the JVM.  This is JDK 1.6.0_17.
Command line options include: -server -ea -showversion  -Xmx22G
-XX:+UseParallelOldGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
-Xloggc:jvm_gc.log.

A sample stack trace is inline below.  The ReentrantLock in
question is a private final field of the Latch class.  While
there are a number of threads waiting on that lock, I can not
identify any thread in the stack trace which actually holds the
lock (no stack traces pass through any code in which the lock is
held).

I can provide a full thread dump, which was obtained using
jstack, but mainly I would like to know if there are known
reasons why a lock might be held but not displayed in a thread
dump.  Or, alternatively, if there were known problems with
ReentrantLock for this JVM version, interactions with the garbage
collection, etc.

Any pointers would be very much appreciated.

Thanks,
Bryan

"com.bigdata.service.jini.JiniFederation.executorService424"
daemon prio=10 tid=0x00002ab07366b800 nid=0x724d waiting on
condition [0x0000000060188000]
   java.lang.Thread.State: WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for  <0x00002aaab43b4cf0> (a
java.util.concurrent.locks.ReentrantLock$NonfairSync)
 at
java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
 at
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndChec
kInterrupt(AbstractQueuedSynchronizer.java:747)
 at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueu
ed(AbstractQueuedSynchronizer.java:778)
 at
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Abs
tractQueuedSynchronizer.java:1114)
 at
java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(Reentra
ntLock.java:186)
 at
java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:
262)
 at com.bigdata.util.concurrent.Latch.dec(Latch.java:206)
 at com.bigdata.service.ndx.pipeline.KVOC.done(KVOC.java:60)
 at
com.bigdata.service.ndx.pipeline.IndexPartitionWriteTask.handleCh
unk(IndexPartitionWriteTask.java:285)
 at
com.bigdata.service.ndx.pipeline.IndexPartitionWriteTask.handleCh
unk(IndexPartitionWriteTask.java:53)
 at
com.bigdata.service.ndx.pipeline.AbstractSubtask.call(AbstractSub
task.java:182)
 at
com.bigdata.service.ndx.pipeline.AbstractSubtask.call(AbstractSub
task.java:66)
 at
java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303
)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPool
Executor.java:886)
 at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExec
utor.java:908)
 at java.lang.Thread.run(Thread.java:619)
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100625/0888e106/attachment-0001.html>

From aph at redhat.com  Fri Jun 25 09:51:09 2010
From: aph at redhat.com (Andrew Haley)
Date: Fri, 25 Jun 2010 14:51:09 +0100
Subject: [concurrency-interest] Threads waiting on a ReentrantLock not
 held	by any thread?
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44D685E0AEC@AUSP01VMBX06.collaborationhost.net>
References: <4C1CEA8A.5030205@kai.meder.info>	<AANLkTik7_TahtqCKlGjN7q8E-okGDtIGvk24xAOYhETf@mail.gmail.com>	<4C1E2159.2090806@kai.meder.info>	<AANLkTikY2ldCPXsJJB5XImr7XbKqZKg_4F31DbKBixiZ@mail.gmail.com>	<4C1F6A0F.6070900@redhat.com>
	<4C1F7E46.6020202@univ-mlv.fr>
	<DE10B00CCE0DC54883734F3060AC9ED44D685E0AEC@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4C24B44D.1030209@redhat.com>

On 06/25/2010 02:29 PM, Bryan Thompson wrote:
> Hello,
> 
> I am hoping that someone can offer me some insight into problem in
> which a process is failing to make progress because several threads
> are waiting on a lock which, based on a thread dump, is not help by
> any thread in the JVM.  This is JDK 1.6.0_17.  Command line options
> include: -server -ea -showversion -Xmx22G -XX:+UseParallelOldGC
> -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:jvm_gc.log.
> 
> A sample stack trace is inline below.  The ReentrantLock in question
> is a private final field of the Latch class.  While there are a
> number of threads waiting on that lock, I can not identify any
> thread in the stack trace which actually holds the lock (no stack
> traces pass through any code in which the lock is held).
> 
> I can provide a full thread dump, which was obtained using jstack,
> but mainly I would like to know if there are known reasons why a
> lock might be held but not displayed in a thread dump.  Or,
> alternatively, if there were known problems with ReentrantLock for
> this JVM version, interactions with the garbage collection, etc.
> 
> Any pointers would be very much appreciated.

You can attach a debugger and look at the ReentrantLock's owner field.

Andrew.

From i30817 at gmail.com  Fri Jun 25 13:55:51 2010
From: i30817 at gmail.com (Paulo Levi)
Date: Fri, 25 Jun 2010 18:55:51 +0100
Subject: [concurrency-interest] Lock hiding.
Message-ID: <AANLkTikBiTWZPHEXXYP6aM-sR5qh0POaZPjqYCdJMKE_@mail.gmail.com>

Hi. I have a performance question about some locks. In this case they are
glazed list library locks, but i am interested in the answer for java locks
if you can't comment on those.

I have a simple concurrent data structure that hides its lock internally.
However i wanted to allow a user to compose the public operations
exclusively for those cases.

I cooked up this function:
public void withLock(Runable computation), that just acquires the lock, runs
the computation and releases the lock.

My question is: if the lock is already acquired, do the hidden lock
operations inside the the other functions of the public interface delay the
computation as if the outer lock wasn't acquired?

Or put another way : Is the delay short-circuited in some way or do the
inner lock operations delay the computation even with the lack of concurrent
access?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100625/19465796/attachment.html>

From mike at exchange.uark.edu  Fri Jun 25 14:55:25 2010
From: mike at exchange.uark.edu (E. Michael Akerman)
Date: Fri, 25 Jun 2010 13:55:25 -0500
Subject: [concurrency-interest] Background execution of JDBC Queries
Message-ID: <0DB846EAF4B0406E82A126F963C2EEF6@uark.edu>

I apologize if this question is too basic for the forum...  I'm not certain of the correct place to ask.

I have a small class which is my first usage of java.util.concurrent, and I have some thread-safety and performance concerns that the JavaDocs didn't seem to clarify.

Given the following code:
    http://pastebin.com/u5eRW72E

  a.. Is it inefficient to create a newFixedThreadPool for only one thread and then immediately discard it?
  b.. Would a "static ExecutorService es = Executors.newCachedThreadPool()" be more efficient and/or threadsafe?  Would I need to synchronize access to it?
  c.. Do I need an explicit call to shutdown()?  Can I just let the ExecutorService be garbage collected, should "shutdown()" be in a finally block?


Michael Akerman
Systems Analyst
University IT Services
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100625/7b02117b/attachment.html>

From tim at peierls.net  Fri Jun 25 15:32:06 2010
From: tim at peierls.net (Tim Peierls)
Date: Fri, 25 Jun 2010 15:32:06 -0400
Subject: [concurrency-interest] Background execution of JDBC Queries
In-Reply-To: <0DB846EAF4B0406E82A126F963C2EEF6@uark.edu>
References: <0DB846EAF4B0406E82A126F963C2EEF6@uark.edu>
Message-ID: <AANLkTin7A4dzLTb2AD5OWHseOMDPxb50LzVFAbasWKCv@mail.gmail.com>

On Fri, Jun 25, 2010 at 2:55 PM, E. Michael Akerman
<mike at exchange.uark.edu>wrote:

>  <concurrency-interest at cs.oswego.edu>
> I apologize if this question is too basic for the forum...  I'm not certain
> of the correct place to ask.
>
> I have a small class which is my *first usage* of java.util.concurrent,
> and I have some thread-safety and performance concerns that the JavaDocs
> didn't seem to clarify.
>
> Given the following code:
>     http://pastebin.com/u5eRW72E
>
>
>    - Is it inefficient to create a newFixedThreadPool for only one thread
>    and then immediately discard it?
>
> Yes.


>
>    - Would a "static ExecutorService es =
>    Executors.newCachedThreadPool()" be more efficient and/or threadsafe?
>
> Yes, but make it static final. (More efficient, no difference in
thread-safety.)


>
>    - Would I need to synchronize access to it?
>
> No, all standard ExecutorService implementations are thread-safe.


>
>    - Do I need an explicit call to shutdown()?  Can I just let the
>    ExecutorService be garbage collected, should "shutdown()" be in a finally
>    block?
>
> If it's in a static field, it won't be garbage collected. If you use
newCachedThreadPool(), its threads will eventually be reaped (after 60
seconds of idling), so it's not strictly necessary to call shutdown(), but
in general you should shutdown an ExecutorService when you aren't going to
use it any more, e.g., at program exit. A finally block doesn't make sense
for this, since the lifetime of the thread pool presumably spans multiple
calls.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100625/9a66c9d3/attachment.html>

From dl at cs.oswego.edu  Tue Jun 29 11:01:49 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 29 Jun 2010 11:01:49 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
Message-ID: <4C2A0ADD.60002@cs.oswego.edu>


Over the past month I've talked (mainly at various conferences)
to a bunch of people (including some of you on this list)
who are currently using or layering other frameworks on top of ForkJoin.
One of my conclusions is that I should simplify some of
the APIs, mainly by removing some methods; at the same time
reworking some of the internals to provide more uniformly good
performance without the need to experiment with various
tunings and special case methods. By removing some of the
visible controls, we can continue to improve base
algorithms without being constrained in ways that may become
obsolete.

Unless I hear some good arguments otherwise, I plan
the following changes, that should be ready to try out within
a week or so. A mock up of revised APIs is at
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/

1. Disable dynamic reconfiguration of ForkJoinPool
and provide fewer configuration parameters. The main rationale
is that we know of no compelling use cases where these are
helpful and several where they are harmful.
This entails removing methods setParallelism,
setMaintainsParallelism, setMaximumPoolSize, etc.
The only on-construction parameters left are parallelism,
factory, uncaughtExceptionHandler, and async mode. This also
entails removing "maintainParallelism" arguments from a
few other methods.

2. Recast ForkJoinPool.invoke to be equivalent to ForkJoinTask.invoke
when issued by a task in the current pool. While these methods
must be distinct (because the former can invoke a task in a
different pool), the semantic differences are otherwise subtle
and error-prone. Also, improve documentation about the various
execute vs submit vs invoke vs fork methods.

4. Remove ForkJoinTask.helpJoin() and variants. Instead, plain join()
conservatively, safely mixes helping- vs spare- based joins internally.

Any further suggestions for ways to remove, combine, or simplify
functionality would be welcome.

-Doug

From sergio.bossa at gmail.com  Tue Jun 29 13:26:30 2010
From: sergio.bossa at gmail.com (Sergio Bossa)
Date: Tue, 29 Jun 2010 19:26:30 +0200
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <4C2A0ADD.60002@cs.oswego.edu>
References: <4C2A0ADD.60002@cs.oswego.edu>
Message-ID: <AANLkTikc96NKs4Fl3OPWyAGtfxAeVNjmLrrd_nT710e0@mail.gmail.com>

On Tue, Jun 29, 2010 at 5:01 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> This entails removing methods setParallelism,
> setMaintainsParallelism, setMaximumPoolSize, etc.

I think it's actually useful to change the parallelism level after
construction, for example to create a FJP with a default parallelism
level and later (re-)configure it on-the-fly (which is the actual way
I use it in some places).

What do you think?

-- 
Sergio Bossa
http://www.linkedin.com/in/sergiob

From David.Biesack at sas.com  Tue Jun 29 13:37:08 2010
From: David.Biesack at sas.com (David J. Biesack)
Date: Tue, 29 Jun 2010 13:37:08 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <mailman.3.1277827200.17937.concurrency-interest@cs.oswego.edu>
	(concurrency-interest-request@cs.oswego.edu)
References: <mailman.3.1277827200.17937.concurrency-interest@cs.oswego.edu>
Message-ID: <ytb7hlhd9kr.fsf@sas.com>


I like the simplifications.

As is usual for me :-) I have some suggestions for more consist method names,
even though you really only asked for functionality suggestions.

ForkJoinTask :

   quietlyInvoke() -> invokeQuietly()
   quietlyJoin() -> joinQuietly()

   Rather than a mix of <verb><adverb> or <adverb><verb>, I suggest
   consistently using <verb><adverb> (like completeExceptionally() or
   awaitAdvanceInterruptibly()). This way, related methods will be listed
   together alphabetically in each class and in the Index.

   (Though of course names like quietlyJoin have been around for a very,
   very long time in FJ. Better late than never.)

   adapt(*) -> toTask(*) -- similar to java.util.List.toArray() and
   most other conversion/adapter methods in the Java API.

   I think completeExceptionally(Throwable ex) could be simply
   complete(Throwable ex) or if that is too subtle, completeWith(Throwable ex)

   Two terms, "done" and "complete", are used when one might be better.
   In fact, the doc for isDone says "Returns true if this task
   completed."

   Perhaps:

   isCompletedNormally()   -> isDoneNormally()
   isCompletedAbnormally() -> isDoneAbnormally()

   complete(Y result) is trickier. In an earlier release of FJ/jsr166y it
   was finish(Y result). Since completion status is tested with isDone,
   perhaps it can be setDone(Y result). Since Future and FutureTask
   already use isDone, that sets the precedent for "done" over "complete".

ForkJoinPool :

  boolean getAsyncMode() ->  boolean isAsynchronous()
  managedBlock(ManagedBlocker)  -> block(ManagedBlocker) 
  newTaskFor(*)  -> toTask(*)

RecursiveAction, ForJoinTask, RecursiveTask:

  exec() -> execute() (See ForkJoinPool.execute(task); Executor.execute(Runnable), 
                       etc. With the exception of Runtime, the rest of the Java 
                       API spells out "execute")

Phaser :

  getArrivedParties() -> getArrivedPartyCount()
  getUnarrivedParties() -> getUnarrivedPartyCount()
  getRegisteredParties() -> getRegisteredPartyCount()

The current name sound like they return collections. ForkJoinPool has several
get*Count() methods.
 
  bulkRegister(int parties) -> register(int parties)
  forceTermination -> terminate() (see isTerminated)

Sorry that these suggestions come so late in the game. Perhaps with
a large scale change to the API, some name changes are fair game.

djb

> Date: Tue, 29 Jun 2010 11:01:49 -0400
> From: Doug Lea <dl at cs.oswego.edu>
> Subject: [concurrency-interest] Upcoming FJ Simplifications
> To: "Concurrency-interest at cs.oswego.edu" <Concurrency-interest at cs.oswego.edu>
> Message-ID: <4C2A0ADD.60002 at cs.oswego.edu>
>
> Unless I hear some good arguments otherwise, I plan
> the following changes, that should be ready to try out within
> a week or so. A mock up of revised APIs is at
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
> 
> Any further suggestions for ways to remove, combine, or simplify
> functionality would be welcome.
> 
> -Doug

-- 
David J. Biesack, SAS
SAS Campus Dr. Cary, NC 27513
www.sas.com    (919) 531-7771

From dl at cs.oswego.edu  Tue Jun 29 13:44:22 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 29 Jun 2010 13:44:22 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <AANLkTikc96NKs4Fl3OPWyAGtfxAeVNjmLrrd_nT710e0@mail.gmail.com>
References: <4C2A0ADD.60002@cs.oswego.edu>
	<AANLkTikc96NKs4Fl3OPWyAGtfxAeVNjmLrrd_nT710e0@mail.gmail.com>
Message-ID: <4C2A30F6.6060902@cs.oswego.edu>

On 06/29/10 13:26, Sergio Bossa wrote:
> On Tue, Jun 29, 2010 at 5:01 PM, Doug Lea<dl at cs.oswego.edu>  wrote:
>
>> This entails removing methods setParallelism,
>> setMaintainsParallelism, setMaximumPoolSize, etc.
>
> I think it's actually useful to change the parallelism level after
> construction, for example to create a FJP with a default parallelism
> level and later (re-)configure it on-the-fly (which is the actual way
> I use it in some places).
>
> What do you think?
>

I think I'd like to know why you do this :-)
Note that ForkJoinPool.parallelism acts differently
than do pool size settings in other ExecutorServices
(like ThreadPoolExecutor). It sets a target number of threads
that should be available/active to work on submitted
tasks. The actual number of threads may differ dynamically
around this target as tasks block waiting for others etc.
And given the potentially surprising interactions of trying to
change the target while the pool is dynamically adapting to the
target, I'm no longer convinced that allowing it is a good idea.

-Doug

From sergio.bossa at gmail.com  Tue Jun 29 14:54:43 2010
From: sergio.bossa at gmail.com (Sergio Bossa)
Date: Tue, 29 Jun 2010 20:54:43 +0200
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <4C2A30F6.6060902@cs.oswego.edu>
References: <4C2A0ADD.60002@cs.oswego.edu>
	<AANLkTikc96NKs4Fl3OPWyAGtfxAeVNjmLrrd_nT710e0@mail.gmail.com>
	<4C2A30F6.6060902@cs.oswego.edu>
Message-ID: <AANLkTimYNVT2mcNaTWLWi-6FHhWTI2PnryRLnPv8qe5t@mail.gmail.com>

On Tue, Jun 29, 2010 at 7:44 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> I think I'd like to know why you do this :-)

Sure :)
It's just that I don't know the user-specified parallelism level when
the pool is created, so I actually create it and then configure the
parallelism level as requested by the user.
I can overcome the removal of the setParallelism() in other ways, so
no worries ... just wondering if there were any actual problems with
the set method.

> Note that ForkJoinPool.parallelism acts differently
> than do pool size settings in other ExecutorServices
> (like ThreadPoolExecutor). It sets a target number of threads
> that should be available/active to work on submitted
> tasks. The actual number of threads may differ dynamically
> around this target as tasks block waiting for others etc.

Yep, I know this one :)

> And given the potentially surprising interactions of trying to
> change the target while the pool is dynamically adapting to the
> target, I'm no longer convinced that allowing it is a good idea.

I didn't know of any actual problems (didn't have the time to look at
the source code), so again, feel free to remove the method if it makes
for a better implementation :)

Cheers,

Sergio B.

-- 
Sergio Bossa
http://www.linkedin.com/in/sergiob

From dl at cs.oswego.edu  Wed Jun 30 11:52:58 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 30 Jun 2010 11:52:58 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <ytb7hlhd9kr.fsf@sas.com>
References: <mailman.3.1277827200.17937.concurrency-interest@cs.oswego.edu>
	<ytb7hlhd9kr.fsf@sas.com>
Message-ID: <4C2B685A.8080100@cs.oswego.edu>

On 06/29/10 13:37, David J. Biesack wrote:

> As is usual for me :-) I have some suggestions for more consist method names,
> even though you really only asked for functionality suggestions.

Thanks for the thoughtful suggestions. Here's why I hope
to reject most of them :-)

>     quietlyInvoke() ->  invokeQuietly()
>     quietlyJoin() ->  joinQuietly()
>
>     Rather than a mix of<verb><adverb>  or<adverb><verb>,

But English mixes these too, according to emphasis. To me, in these
cases quietlyX sounds better. In other cases not though.

>
>     adapt(*) ->  toTask(*) -- similar to java.util.List.toArray() and
>     most other conversion/adapter methods in the Java API.

I think I tried a bunch of these choices a few years ago
and people stopped complaining when I arrived at plain "adapt".
Well, they almost stopped :-)

>
>     I think completeExceptionally(Throwable ex) could be simply
>     complete(Throwable ex) or if that is too subtle, completeWith(Throwable ex)

Can't do just "complete" because it is conceivable to define
tasks with throwables as their return values.

>
>     Two terms, "done" and "complete", are used when one might be better.

Maybe so. We need the "done" version to match Future, but ...

>
>     isCompletedNormally()   ->  isDoneNormally()

... someone objected to this because it can be mentally misparsed
to mean something like "is to be processed in the normal way"

>    boolean getAsyncMode() ->   boolean isAsynchronous()

I'd rather have a nichy funny-looking method name for this nichy method.

>    newTaskFor(*)  ->  toTask(*)

No can do -- newTaskFor is in AbstractExecutorService.

>
>    exec() ->  execute() (See ForkJoinPool.execute(task); Executor.execute(Runnable),
>                         etc. With the exception of Runtime, the rest of the Java
>                         API spells out "execute")
>

Maybe better, "rawExec"? or something similar to indicate that
this is template-method-design-pattern style extensibility hook
that you never want to directly call except perhaps in subclasses.

>    getArrivedParties() ->  getArrivedPartyCount()
>    getUnarrivedParties() ->  getUnarrivedPartyCount()
>    getRegisteredParties() ->  getRegisteredPartyCount()

Good idea. Will probably do it.

> Sorry that these suggestions come so late in the game. Perhaps with
> a large scale change to the API, some name changes are fair game.
>

FWIW, my take is that those other simpliflications mainly
disrupt people building layered frameworks or mapping other languages
(for whom most of these changes are in response to anyway)
plus only a handful of others.

-Doug

From tim at peierls.net  Wed Jun 30 17:49:49 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 30 Jun 2010 17:49:49 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <4C2B685A.8080100@cs.oswego.edu>
References: <mailman.3.1277827200.17937.concurrency-interest@cs.oswego.edu>
	<ytb7hlhd9kr.fsf@sas.com> <4C2B685A.8080100@cs.oswego.edu>
Message-ID: <AANLkTimT-qK2mG__K-vUPm0Z3RRk_xYKFqUKjX_q78Uv@mail.gmail.com>

On Wed, Jun 30, 2010 at 11:52 AM, Doug Lea <dl at cs.oswego.edu> wrote:

>
>>    quietlyInvoke() ->  invokeQuietly()
>>    quietlyJoin() ->  joinQuietly()
>>
>>    Rather than a mix of<verb><adverb>  or<adverb><verb>,
>>
>
> But English mixes these too, according to emphasis. To me, in these
> cases quietlyX sounds better. In other cases not though.


Maybe there are <adverb><verb> combinations that sound better than
<verb><adverb>, but I don't think these are strong examples. Invoke quietly,
join quietly -- much clearer that they are invoking and joining in some way
than quietlyBlah and quietlyBleah. It might be different if "join" were
being used transitively, but it isn't.



>    I think completeExceptionally(Throwable ex) could be simply
>
>    complete(Throwable ex) or if that is too subtle, completeWith(Throwable
>> ex)
>>
>
> Can't do just "complete" because it is conceivable to define
> tasks with throwables as their return values.


(Conceivable but guaranteed to confuse readers.)

completeWith reads nicely. (completeByThrowing?)



>   boolean getAsyncMode() ->   boolean isAsynchronous()
>
>
> I'd rather have a nichy funny-looking method name for this nichy method.
>

OK, but then return enum AsyncMode { ASYNC, SYNC } or something? Or boolean
isAsynchMode? It's just weird to have getXyz -> boolean.



>   newTaskFor(*)  ->  toTask(*)
>>
>
> No can do -- newTaskFor is in AbstractExecutorService.


Here's a weak excuse for that: This was a Java 6 addition, so the method
name had to be gnarly to minimize the risk of breaking existing AES
extensions (of which there were probably none at the time).



>   exec() ->  execute() (See ForkJoinPool.execute(task);
>> Executor.execute(Runnable),
>>                        etc. With the exception of Runtime, the rest of the
>> Java
>>                        API spells out "execute")
>>
>>
> Maybe better, "rawExec"? or something similar to indicate that
> this is template-method-design-pattern style extensibility hook
> that you never want to directly call except perhaps in subclasses.


exec is weird enough. rawExec would just have me asking where the regular
cooked exec was.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100630/b15eca2a/attachment.html>

