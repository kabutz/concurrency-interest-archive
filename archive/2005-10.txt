From peger at automotive.com  Wed Oct  5 15:24:18 2005
From: peger at automotive.com (Patrick Eger)
Date: Wed Oct  5 15:27:43 2005
Subject: [concurrency-interest] ThreadPoolExecutor
Message-ID: <4BAE344F80BC5D4B961906C7001729DF01B18E72@mail-006.w2k.automotive.com>

Hi, great work on the util.concurrent package, we have converted from to
Oswego package entirely, however there remains one missing piece that I
could not figure out how to do. Basically, we need a ThreadPoolExecutor
with the following properties:
 
1) {infinite | bounded} linked list request queue, where the bound is
really high to avoid OOM (100000+)
2) MIN of 0 threads in pool
3) MAX # of threads in pool, configurable on pool create
4) reuse existing threads if available
5) allow pool to shrink to zero threads if there are no outstanding
requests
 

This was possible in the old Oswego package, but now in the JDK, these
all seem possible except for #4, which conflicts with the documentation
(and class ThreadPoolExecutor) here:

"When a new task is submitted in method execute(java.lang.Runnable)
<http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ThreadPool
Executor.html#execute%28java.lang.Runnable%29> , and fewer than
corePoolSize threads are running, a new thread is created to handle the
request, even if other worker threads are idle. If there are more than
corePoolSize but less than maximumPoolSize threads running, a new thread
will be created only if the queue is full"



Because LinkedBlockingQueue is never full (or when it is its already
HUGE), no combination of corePoolSize, maximumPoolSize seems to allow
this.

Basically, the problem is:

If corePoolSize == MAX, idle threads are ignored, and we will always
climb up to MAX threads very quickly, even though we only wan MAX
threads under heavy load
If corePoolSize < MAX, with an infinite (or effectively infinite)
request queue, we have effectively reduced MAX to corePoolSize.
 

Currently we have a hacked-up JDK 1.6 ThreadPoolExecutor to give us #4
(with corePoolSize==MAX and allowCoreThreadTimeout==true), IE reuse
existing idle threads before trying to create new ones. I'm not certain
if our implementation is correct, it is most certainly ugly:


Hacked into "public void execute(Runnable command)" right after the "if
(runState != RUNNING)" block:
------------------------------------------------------------------------
------------------
//NOTE: Added this for less thread-crazy behaviour
if (workersBlocked.get() > 0) {
	if(workQueue.offer(command)) {
		//HACK: this should protect against the pool shrinking,
should be very rare... 
		if (poolSize == 0)
			addIfUnderCorePoolSize(new Runnable(){ public
void run() {} });

		return;
	}
}
------------------------------------------------------------------------
------------------
 
 

Everything is working correctly as per the docs AFAIK, just seemingly
counterintuitively. It seems quite pointless and lower performant to be
creating new threads while existing ones are idle and available to
process work. Is this just a bad interaction between ThreadPoolExecutor
and LinkedBlockingQueue?  Is there another queue type that will work for
me or thread pool option I am missing?

 

Thanks in advance and please let me know if I am off base or incorrect
here.


Best regards,

Patrick


From joe.bowbeer at gmail.com  Wed Oct  5 16:38:26 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed Oct  5 16:38:49 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E72@mail-006.w2k.automotive.com>
References: <4BAE344F80BC5D4B961906C7001729DF01B18E72@mail-006.w2k.automotive.com>
Message-ID: <31f2a7bd0510051338g22b9ffax46dad20c8a3bd180@mail.gmail.com>

I've probably missed something but it occurs to me that you want to be using
maxPoolSize instead of corePoolSize.

In other words, have you tried the following?

corePoolSize = 0
maxPoolSize = MAX
keepAliveTime = tuned so that pool shrinks to 0 when executor is idle


On 10/5/05, Patrick Eger <peger@automotive.com> wrote:
>
> Hi, great work on the util.concurrent package, we have converted from to
> Oswego package entirely, however there remains one missing piece that I
> could not figure out how to do. Basically, we need a ThreadPoolExecutor
> with the following properties:
>
> 1) {infinite | bounded} linked list request queue, where the bound is
> really high to avoid OOM (100000+)
> 2) MIN of 0 threads in pool
> 3) MAX # of threads in pool, configurable on pool create
> 4) reuse existing threads if available
> 5) allow pool to shrink to zero threads if there are no outstanding
> requests
>
>
> This was possible in the old Oswego package, but now in the JDK, these
> all seem possible except for #4, which conflicts with the documentation
> (and class ThreadPoolExecutor) here:
>
> "When a new task is submitted in method execute(java.lang.Runnable)
> <http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ThreadPool
> Executor.html#execute%28java.lang.Runnable%29> , and fewer than
> corePoolSize threads are running, a new thread is created to handle the
> request, even if other worker threads are idle. If there are more than
> corePoolSize but less than maximumPoolSize threads running, a new thread
> will be created only if the queue is full"
>
>
>
> Because LinkedBlockingQueue is never full (or when it is its already
> HUGE), no combination of corePoolSize, maximumPoolSize seems to allow
> this.
>
> Basically, the problem is:
>
> If corePoolSize == MAX, idle threads are ignored, and we will always
> climb up to MAX threads very quickly, even though we only wan MAX
> threads under heavy load
> If corePoolSize < MAX, with an infinite (or effectively infinite)
> request queue, we have effectively reduced MAX to corePoolSize.
>
>
> Currently we have a hacked-up JDK 1.6 ThreadPoolExecutor to give us #4
> (with corePoolSize==MAX and allowCoreThreadTimeout==true), IE reuse
> existing idle threads before trying to create new ones. I'm not certain
> if our implementation is correct, it is most certainly ugly:
>
>
> Hacked into "public void execute(Runnable command)" right after the "if
> (runState != RUNNING)" block:
> ------------------------------------------------------------------------
> ------------------
> //NOTE: Added this for less thread-crazy behaviour
> if (workersBlocked.get() > 0) {
> if(workQueue.offer(command)) {
> //HACK: this should protect against the pool shrinking,
> should be very rare...
> if (poolSize == 0)
> addIfUnderCorePoolSize(new Runnable(){ public
> void run() {} });
>
> return;
> }
> }
> ------------------------------------------------------------------------
> ------------------
>
>
>
> Everything is working correctly as per the docs AFAIK, just seemingly
> counterintuitively. It seems quite pointless and lower performant to be
> creating new threads while existing ones are idle and available to
> process work. Is this just a bad interaction between ThreadPoolExecutor
> and LinkedBlockingQueue? Is there another queue type that will work for
> me or thread pool option I am missing?
>
>
>
> Thanks in advance and please let me know if I am off base or incorrect
> here.
>
>
> Best regards,
>
> Patrick
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051005/5366ac24/attachment.htm
From joe.bowbeer at gmail.com  Wed Oct  5 17:22:31 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed Oct  5 17:22:53 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E74@mail-006.w2k.automotive.com>
References: <4BAE344F80BC5D4B961906C7001729DF01B18E74@mail-006.w2k.automotive.com>
Message-ID: <31f2a7bd0510051422o24c00b9evaeca9e270a28535e@mail.gmail.com>

Patrick Eger <peger@automotive.com> wrote:
>
> This code is basically the first thing that is run with the  below settings.

Scratch those settings...

Though I think workQueue.offer is executed when corePoolSize == 0.

  if (poolSize < corePoolSize && addIfUnderCorePoolSize(command))
      return;
  if (workQueue.offer(command))
      return;
  int status = addIfUnderMaximumPoolSize(command);
  if (status > 0)      // created new thread
      return;
  if (status == 0) {   // failed to create thread
      reject(command);
      return;
  }

SO you want a decent core pool size, and maxPoolSize is mostly
irrelevant because the queue is effectively unbounded.

If you want the thread pool to shrink to 0 when idle, then you can use

allowCoreThreadTimeOut(true)

which was added in 1.6.


On 10/5/05, Patrick Eger <peger@automotive.com> wrote:
>
> Thank you for the quick response.
> Looking at the code  for ThreadPoolExecutor (from JDK 1.6):
>
> public void execute(Runnable command), line  ~876
> -------------------------------------------------
> Runnable r =  addIfUnderMaximumPoolSize(command);
> -------------------------------------------------
>
> This code is basically the first thing that is run with the below settings.
> So basically it will operate the same way it seems, IE adding a
> new thread unconditionally and not reusing existing idle threads.
> I will  write up a quick test to confirm this behaviour but i'm pretty
> sure we explored  this option as well.
>
> Thanks again for your help.
>
> Best regards,
>
> Patrick
>
>  ________________________________
 From: Joe Bowbeer [mailto:joe.bowbeer@gmail.com]
> Sent: Wednesday, October 05, 2005 1:38 PM
> To: Patrick  Eger
> Cc: concurrency-interest@altair.cs.oswego.edu
> Subject:  Re: [concurrency-interest] ThreadPoolExecutor
>
> I've probably missed something but it occurs to me that you want to
> be using maxPoolSize instead of corePoolSize.
>
> In other words, have you  tried the following?
>
> corePoolSize = 0
> maxPoolSize =  MAX
> keepAliveTime = tuned so that pool shrinks to 0 when executor is  idle
>
> On 10/5/05, Patrick  Eger <peger@automotive.com>  wrote:
> Basically, we need a ThreadPoolExecutor with the following properties:
> >
> > 1) {infinite | bounded} linked list request    queue, where the bound is
> > really high to avoid OOM (100000+)
> > 2) MIN of 0    threads in pool
> > 3) MAX # of threads in pool, configurable on pool create
> > 4) reuse existing threads if available
> > 5) allow pool to shrink to zero    threads if there are no outstanding
> > requests
> >
> >

From dawidk at mathcs.emory.edu  Wed Oct  5 18:17:14 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed Oct  5 18:17:52 2005
Subject: [concurrency-interest] Thread safe or not ?
In-Reply-To: <ca53c8f805092306131d0ad9b3@mail.gmail.com>
References: <ca53c8f8050923002368fa1db2@mail.gmail.com><NFBBKALFDCPFIDBNKAPC
	OEPHGDAA.dholmes@dltech.com.au> 
	<ca53c8f805092306131d0ad9b3@mail.gmail.com>
Message-ID: <434450EA.5050606@mathcs.emory.edu>

Hanson Char wrote:

> That's exactly right - very astute observation as usual.  It's an 
> outline of a pattern for initializing, in a thread-safe manner, 
> multiple final static constants via (optionally) some wiring of 
> external configuration.
>
> Clients of the final static constants don't need to worry about the 
> external configuration.  The wiring framework (such as Spring) doesn't 
> need to concern about the external configuration actually ends up in a 
> bunch of static final constants.
>
> In the trivial example I gave there is only one static final constant, 
> K.  However, imagine we have multiple system constants.  A 
> SystemParameterBean instance can be initialized once, and can then be 
> used to initialize all the static final constants in SystemParameter, 
> which are then accessed by clients in a simple way.  Like 
> SystemParameter.K, SystemParameter.J, etc.
>
> In such multiple-constant scenario, we can avoid init().getK() and 
> init().getJ(), etc. but simply bean.getK() and bean.getJ().  The 
> init() method implementation, which may incur additional overheads, 
> should be done only once and not linear to the number of system constants.
>
I apologize for such a delay in responding, but, if you move init() to 
the static initializer, you can make the bean a local variable, 
obviating the need to store it in a static field even though you access 
it more than once:

public class SystemParameter {
    public static final long K;
    public static final long L;
    ...
    static {
        SystemParameterBean bean = init();
        K = bean.getK();
        L = bean.getL();
        ...
    }


Regards,
Dawid

From peger at automotive.com  Wed Oct  5 16:51:32 2005
From: peger at automotive.com (Patrick Eger)
Date: Wed Oct  5 18:43:18 2005
Subject: [concurrency-interest] ThreadPoolExecutor
Message-ID: <4BAE344F80BC5D4B961906C7001729DF01B18E74@mail-006.w2k.automotive.com>

Thank you for the quick response.  Looking at the code for
ThreadPoolExecutor (from JDK 1.6):
 
public void execute(Runnable command), line ~876
-------------------------------------------------
Runnable r = addIfUnderMaximumPoolSize(command);
-------------------------------------------------
 
This code is basically the first thing that is run with the below
settings. So basically it will operate the same way it seems, IE adding
a new thread unconditionally and not reusing existing idle threads.  I
will write up a quick test to confirm this behaviour but i'm pretty sure
we explored this option as well.
 
Thanks again for your help.
 
 
 
Best regards,
 
Patrick

________________________________

From: Joe Bowbeer [mailto:joe.bowbeer@gmail.com] 
Sent: Wednesday, October 05, 2005 1:38 PM
To: Patrick Eger
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] ThreadPoolExecutor


I've probably missed something but it occurs to me that you want to be
using maxPoolSize instead of corePoolSize.

In other words, have you tried the following?

corePoolSize = 0
maxPoolSize = MAX
keepAliveTime = tuned so that pool shrinks to 0 when executor is idle



On 10/5/05, Patrick Eger <peger@automotive.com> wrote: 

	Hi, great work on the util.concurrent package, we have converted
from to
	Oswego package entirely, however there remains one missing piece
that I
	could not figure out how to do. Basically, we need a
ThreadPoolExecutor 
	with the following properties:
	
	1) {infinite | bounded} linked list request queue, where the
bound is
	really high to avoid OOM (100000+)
	2) MIN of 0 threads in pool
	3) MAX # of threads in pool, configurable on pool create 
	4) reuse existing threads if available
	5) allow pool to shrink to zero threads if there are no
outstanding
	requests
	
	
	This was possible in the old Oswego package, but now in the JDK,
these
	all seem possible except for #4, which conflicts with the
documentation 
	(and class ThreadPoolExecutor) here:
	
	"When a new task is submitted in method
execute(java.lang.Runnable)
	
<http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ThreadPool

	Executor.html#execute%28java.lang.Runnable%29> , and fewer than
	corePoolSize threads are running, a new thread is created to
handle the
	request, even if other worker threads are idle. If there are
more than 
	corePoolSize but less than maximumPoolSize threads running, a
new thread
	will be created only if the queue is full"
	
	
	
	Because LinkedBlockingQueue is never full (or when it is its
already
	HUGE), no combination of corePoolSize, maximumPoolSize seems to
allow 
	this.
	
	Basically, the problem is:
	
	If corePoolSize == MAX, idle threads are ignored, and we will
always
	climb up to MAX threads very quickly, even though we only wan
MAX
	threads under heavy load
	If corePoolSize < MAX, with an infinite (or effectively
infinite) 
	request queue, we have effectively reduced MAX to corePoolSize.
	
	
	Currently we have a hacked-up JDK 1.6 ThreadPoolExecutor to give
us #4
	(with corePoolSize==MAX and allowCoreThreadTimeout==true), IE
reuse
	existing idle threads before trying to create new ones. I'm not
certain
	if our implementation is correct, it is most certainly ugly:
	
	
	Hacked into "public void execute(Runnable command)" right after
the "if 
	(runState != RUNNING)" block:
	
------------------------------------------------------------------------
	------------------
	//NOTE: Added this for less thread-crazy behaviour
	if (workersBlocked.get() > 0) { 
	        if(workQueue.offer(command)) {
	                //HACK: this should protect against the pool
shrinking,
	should be very rare...
	                if (poolSize == 0)
	                        addIfUnderCorePoolSize(new Runnable(){
public
	void run() {} });
	
	                return;
	        }
	}
	
------------------------------------------------------------------------
	------------------
	
	
	
	Everything is working correctly as per the docs AFAIK, just
seemingly 
	counterintuitively. It seems quite pointless and lower
performant to be
	creating new threads while existing ones are idle and available
to
	process work. Is this just a bad interaction between
ThreadPoolExecutor
	and LinkedBlockingQueue?  Is there another queue type that will
work for
	me or thread pool option I am missing?
	
	
	
	Thanks in advance and please let me know if I am off base or
incorrect
	here.
	
	
	Best regards,
	
	Patrick
	
	
	


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051005/2aac7b48/attachment-0001.htm
From dholmes at dltech.com.au  Wed Oct  5 20:17:37 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct  5 20:18:12 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E72@mail-006.w2k.automotive.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECMGFAA.dholmes@dltech.com.au>

Patrick,

Your requirements can't be met directly by the current implementation. The
current design works, as you know, by having a core-pool that can either be
pre-started or lazily created, and which always stays around (in 1.6 idle
core threads can timeout but that just takes you back to the lazy startup
mode). Once you have your core pool you only create more threads (up to max)
if your queue fills up - if the queue never fills then no more threads will
be created. If you reach the maximum size with a full queue the task is
rejected.

> 1) {infinite | bounded} linked list request queue, where the bound is
> really high to avoid OOM (100000+)
> 2) MIN of 0 threads in pool
> 3) MAX # of threads in pool, configurable on pool create
> 4) reuse existing threads if available
> 5) allow pool to shrink to zero threads if there are no outstanding
> requests

Requirement (4) requires a way to detect if threads are "available". But
what does this mean? A worker thread is either executing a task or blocked
waiting for a task to appear in the BlockingQueue. If it is blocked then it
is "available", but to use it you have to submit your task to the queue. To
know if it is blocked you need to keep track of it, which is presumably what
this code is doing:

> //NOTE: Added this for less thread-crazy behaviour
> if (workersBlocked.get() > 0) {
> 	if(workQueue.offer(command)) {
> 		//HACK: this should protect against the pool shrinking,
> should be very rare...
> 		if (poolSize == 0)
> 			addIfUnderCorePoolSize(new Runnable(){ public
> void run() {} });
>
> 		return;
> 	}
> }

However this sort of check requires atomicity that isn't normally present in
the ThreadPoolExecutor. So to do this right requires additional locking
otherwise two incoming tasks can see one available worker and assume the
worker will run their task, when it fact one task will remain in the queue
and the pool could have queued tasks but less than max (or even core)
threads.

So if you really want this you have to pay a price to get it.

> Everything is working correctly as per the docs AFAIK, just seemingly
> counterintuitively. It seems quite pointless and lower performant to be
> creating new threads while existing ones are idle and available to
> process work.

The assumption is that the core pool will be quite steady so if you don't
create a core thread this time, the expected usage means you are going to
create it very soon anyway. If you pre-start the core then you don't create
new threads until your queue is full.

> Is this just a bad interaction between ThreadPoolExecutor
> and LinkedBlockingQueue?  Is there another queue type that will work for
> me or thread pool option I am missing?

It seems to me - and I could be misunderstanding things - that what you want
is a "dual-mode" queue. Set the core size at zero and what you want is to
submit the task to the queue, if a thread is waiting, else create a thread.
This is where you need a synchronous queue - it has no capacity, so if no
thread is waiting then offer() will fail and from the pool's perspective the
queue is "full" and so a new thread will be created if under max. But when
max is reached you now want a queue that has capacity. You could use the
RejectedExecutionHandler to then make your queue (you'd need to define a
custom queue for this) switch to act as a (finite) linked blocking queue. If
the normal queue detects it is empty then it switches back to synchronous
mode. I *think* that would meet your requirements *but* I don't know if what
I just described can actually be implemented. Interesting to think about it
anyway :-)

Cheers,
David Holmes

From dholmes at dltech.com.au  Wed Oct  5 23:44:31 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct  5 23:44:57 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E7E@mail-006.w2k.automotive.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEDEGFAA.dholmes@dltech.com.au>

Patrick,

> There are definite race issues here as well, but it think it is safe in
> that the only problem would be if the pool shrinks to zero size after we
> offer it to the queue.  "if (poolSize == 0)" clause below protects
> against this case I think...

The approximation involved in using the workersBlocked count would
invalidate one of the basic guarantees regarding the creation of core
threads - ie that one will always be created if needed. Without this, for
example, barrier designs that use a pool wouldn't work because tasks may be
left in the queue with all existing workers blocked on the barrier waiting
for the other tasks to arrive - which they won't because they are in the
queue and less than coreSize threads have been created. You would have to
guarantee atomicity using locks.

People have enough trouble understanding the relationship between coreSize,
queue length and maxSize, as it is! Adding another variable to the mix would
be a recipe for potential disaster. That's not to say that there couldn't be
a different TPE that worked as you would like, but it seems unlikely to be a
j.u.c class. I wish there were a clean way to abstract these policy choices
out so that subclasses could modify them, but I can't see a way of doing
this. So cut-paste-modify seems the simplest solution.

> Your queue approach is also interesting but sound much more complex and
> error prone, plus it would really complexify the (currently simple)
> interface between the thread pool and the queue.

Hmmm. There wouldn't be any change in the interface between them - as far as
I can see. You give the pool your special queue and install the queue's
rejected execution handler, then everything "just works". I'm not sure how
the task that triggers the switch from synchronous to non-synchronous mode
gets resubmitted: maybe a simple MyQueue.this.put(), or if necessary hook
the queue to the TPE and let the handler reinvoke execute. I don't *think*
it is that complicated but I won't know unless I try to implement it.

> P.S. I have confirmed via a small test case that a corePoolSize of zero
> will result in submitted tasks *never* executing.

As you would expect - threads beyond the core size only get created when the
queue is full. If the queue can't fill then no threads ever get created. Do
you think this combination should be detected and prohibited?

> P.P.S. Am I still confused and not knowing what I want?  I assumed this
> behaviour is what most people would want for a dynamic work queue
> (0-1000 threads) with bursty request patterns (0-1 million+ on the queue
> ant any given point), but I cannot find much in the archives...

The current design assumes that the core is set such that the expected load
is accommodated, with room for increasing the worker set under heavy load,
by limiting the buffering done by the queue. Having large numbers of threads
in the pool is generally detrimental to overall performce, but may be
necessary if the tasks are expected to block for reasonable periods.
Otherwise with a small core size you would not generally be concerned about
the startup overhead of these threads - whether done eagerly or on demand.

Do you really expect 1000 active threads? How many CPU's are you running on?
Even on a 384-way Azul box I wouldn't expect to need a pool that large. :)

Cheers,
David Holmes

PS. Doug Lea is obviously incommunicado right now or else I'm certain he
would have chipped in - and I expect he will when he can.

From hanson.char at gmail.com  Thu Oct  6 00:51:21 2005
From: hanson.char at gmail.com (Hanson Char)
Date: Thu Oct  6 00:51:45 2005
Subject: [concurrency-interest] Thread safe or not ?
In-Reply-To: <434450EA.5050606@mathcs.emory.edu>
References: <ca53c8f8050923002368fa1db2@mail.gmail.com>
	<ca53c8f805092306131d0ad9b3@mail.gmail.com>
	<434450EA.5050606@mathcs.emory.edu>
Message-ID: <ca53c8f80510052151r7126b0dbvc139d48249bd3035@mail.gmail.com>

Yes, thanks. Indeed the better solution you point out has also been pointed
out by Joe Bowbeer in an earlier response to this thread.

Hanson

On 10/5/05, Dawid Kurzyniec <dawidk@mathcs.emory.edu> wrote:
>
> Hanson Char wrote:
>
> > That's exactly right - very astute observation as usual. It's an
> > outline of a pattern for initializing, in a thread-safe manner,
> > multiple final static constants via (optionally) some wiring of
> > external configuration.
> >
> > Clients of the final static constants don't need to worry about the
> > external configuration. The wiring framework (such as Spring) doesn't
> > need to concern about the external configuration actually ends up in a
> > bunch of static final constants.
> >
> > In the trivial example I gave there is only one static final constant,
> > K. However, imagine we have multiple system constants. A
> > SystemParameterBean instance can be initialized once, and can then be
> > used to initialize all the static final constants in SystemParameter,
> > which are then accessed by clients in a simple way. Like
> > SystemParameter.K, SystemParameter.J, etc.
> >
> > In such multiple-constant scenario, we can avoid init().getK() and
> > init().getJ(), etc. but simply bean.getK() and bean.getJ(). The
> > init() method implementation, which may incur additional overheads,
> > should be done only once and not linear to the number of system
> constants.
> >
> I apologize for such a delay in responding, but, if you move init() to
> the static initializer, you can make the bean a local variable,
> obviating the need to store it in a static field even though you access
> it more than once:
>
> public class SystemParameter {
> public static final long K;
> public static final long L;
> ...
> static {
> SystemParameterBean bean = init();
> K = bean.getK();
> L = bean.getL();
> ...
> }
>
>
> Regards,
> Dawid
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051005/fecff49b/attachment.htm
From peger at automotive.com  Wed Oct  5 23:14:40 2005
From: peger at automotive.com (Patrick Eger)
Date: Thu Oct  6 06:56:55 2005
Subject: [concurrency-interest] ThreadPoolExecutor
Message-ID: <4BAE344F80BC5D4B961906C7001729DF01B18E7E@mail-006.w2k.automotive.com>

Hello and thank you for your response.  I forgot {to mention} the extra
"workersBlocked" variable I added to keep track of the # of threads
waiting for a task to execute.  This obviously adds the extra expense of
the atomic inc/dec pair for every execute() (told you it was hacky :-).
There are definite race issues here as well, but it think it is safe in
that the only problem would be if the pool shrinks to zero size after we
offer it to the queue.  "if (poolSize == 0)" clause below protects
against this case I think...

....
private final AtomicInteger workersBlocked = new AtomicInteger(0);
....
workersBlocked.incrementAndGet();
try {
	// untimed wait if core and not allowing core timeout
	if (poolSize <= corePoolSize && !allowCoreThreadTimeOut)
      	return workQueue.take();
    
      long timeout = keepAliveTime;
	if (timeout <= 0) // die immediately for 0 timeout
      	return null;
      Runnable r = workQueue.poll(timeout, TimeUnit.NANOSECONDS);
	if (r != null)
      	return r;
	if (poolSize > corePoolSize || allowCoreThreadTimeOut)
      	return null; // timed out
      // Else, after timeout, the pool shrank. Retry
     	break;	
} finally {
	workersBlocked.decrementAndGet();
}
....


It uses the "workersBlocked" as an estimate of whether or not a thread
is immediately available to execute the Runnable, though this will be
incorrect under heavily concurrent calls to execute().  I would lkove to
find a better way to do this, maybe an atomic decrement semaphore-style
"reservation" would allow correctness in the face of multiple concurrent
executes()? This would still be an additional atomic dec/inc pair (one
in execute(), one in the executor thread when it picks the task up).

Your queue approach is also interesting but sound much more complex and
error prone, plus it would really complexify the (currently simple)
interface between the thread pool and the queue. 

I would gladly pay this inc/dec cost (as a configurable option, off by
default for backwards compatibility sake and so should not effect
existing users) for the better (IMO of course) thread creation
behaviour.

Do you guys see this as generally useful?  Would a patch using the
(configurable, off by default and so would only add a cachable read to
the fastpath) inc/dec reservation-style approach be considered for
review (the above is a bit of a hack and probably has some bad
characteristics under heavy concurrent use as David has pointed out)? Or
should I pursue the queue-based approach as suggested?

Thanks again for everyone's help!

P.S. I have confirmed via a small test case that a corePoolSize of zero
will result in submitted tasks *never* executing.  They are infinitely
offer()ed up to the queue but there are no threads available to process.
In effect the corePoolSize becomes the "max" pool size with an infinite
queue, which does not give me the behaviour I desire.

P.P.S. Am I still confused and not knowing what I want?  I assumed this
behaviour is what most people would want for a dynamic work queue
(0-1000 threads) with bursty request patterns (0-1 million+ on the queue
ant any given point), but I cannot find much in the archives...

Best regards,

Patrick


-----Original Message-----
From: David Holmes [mailto:dholmes@dltech.com.au] 
Sent: Wednesday, October 05, 2005 5:18 PM
To: Patrick Eger; concurrency-interest@altair.cs.oswego.edu
Subject: RE: [concurrency-interest] ThreadPoolExecutor

Patrick,

Your requirements can't be met directly by the current implementation.
The current design works, as you know, by having a core-pool that can
either be pre-started or lazily created, and which always stays around
(in 1.6 idle core threads can timeout but that just takes you back to
the lazy startup mode). Once you have your core pool you only create
more threads (up to max) if your queue fills up - if the queue never
fills then no more threads will be created. If you reach the maximum
size with a full queue the task is rejected.

> 1) {infinite | bounded} linked list request queue, where the bound is 
> really high to avoid OOM (100000+)
> 2) MIN of 0 threads in pool
> 3) MAX # of threads in pool, configurable on pool create
> 4) reuse existing threads if available
> 5) allow pool to shrink to zero threads if there are no outstanding 
> requests

Requirement (4) requires a way to detect if threads are "available". But
what does this mean? A worker thread is either executing a task or
blocked waiting for a task to appear in the BlockingQueue. If it is
blocked then it is "available", but to use it you have to submit your
task to the queue. To know if it is blocked you need to keep track of
it, which is presumably what this code is doing:

> //NOTE: Added this for less thread-crazy behaviour if 
> (workersBlocked.get() > 0) {
> 	if(workQueue.offer(command)) {
> 		//HACK: this should protect against the pool shrinking,
should be 
> very rare...
> 		if (poolSize == 0)
> 			addIfUnderCorePoolSize(new Runnable(){ public
void run() {} });
>
> 		return;
> 	}
> }

However this sort of check requires atomicity that isn't normally
present in the ThreadPoolExecutor. So to do this right requires
additional locking otherwise two incoming tasks can see one available
worker and assume the worker will run their task, when it fact one task
will remain in the queue and the pool could have queued tasks but less
than max (or even core) threads.

So if you really want this you have to pay a price to get it.

> Everything is working correctly as per the docs AFAIK, just seemingly 
> counterintuitively. It seems quite pointless and lower performant to 
> be creating new threads while existing ones are idle and available to 
> process work.

The assumption is that the core pool will be quite steady so if you
don't create a core thread this time, the expected usage means you are
going to create it very soon anyway. If you pre-start the core then you
don't create new threads until your queue is full.

> Is this just a bad interaction between ThreadPoolExecutor and 
> LinkedBlockingQueue?  Is there another queue type that will work for 
> me or thread pool option I am missing?

It seems to me - and I could be misunderstanding things - that what you
want is a "dual-mode" queue. Set the core size at zero and what you want
is to submit the task to the queue, if a thread is waiting, else create
a thread.
This is where you need a synchronous queue - it has no capacity, so if
no thread is waiting then offer() will fail and from the pool's
perspective the queue is "full" and so a new thread will be created if
under max. But when max is reached you now want a queue that has
capacity. You could use the RejectedExecutionHandler to then make your
queue (you'd need to define a custom queue for this) switch to act as a
(finite) linked blocking queue. If the normal queue detects it is empty
then it switches back to synchronous mode. I *think* that would meet
your requirements *but* I don't know if what I just described can
actually be implemented. Interesting to think about it anyway :-)

Cheers,
David Holmes



From dholmes at dltech.com.au  Thu Oct  6 18:57:00 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Oct  6 18:57:58 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E87@mail-006.w2k.automotive.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEEKGFAA.dholmes@dltech.com.au>

> Just a little surprising is all, though I'm not sure how the
> ThreadPoolExecutor could know that a queue is non-synchronous?

A check of BlockingQueue.remainingCapacity would tell whether the queue was
bounded or not - but it would be subjective as to how big a value
"unbounded" really is.

> Actually isn't corePoolSize==0 an invalid usage pattern if the queue has
> *any* capacity at all?

With an infinite queue, or a queue you don't expect to fill then it is a
problem. However, you could use a zero core size and a small queue to
effectively "batch" tasks.

ThreadPoolExecutor is quite flexible but a side effect of that is that some
combinations of options make little or no sense.

> What we have is lots of threads active at a time doing things like
> waiting for a database or network response so our pools need to be
> pretty big to accommodate all these IO blocking calls.  I'd love to move
> to an event driven non-blocking IO model but unfortunately key
> technologies such as JDBC don't provide any such options.

I see. At this stage "fixing" the pool is probably a better option than
redesigning the core application architecture.

> I'll just maintain a modified version for myself (and anyone else if
> interested, just let me know).

I'm intrigued enough to try and implement that dual queue set up. :)

Cheers,
David Holmes

From peger at automotive.com  Thu Oct  6 17:34:02 2005
From: peger at automotive.com (Patrick Eger)
Date: Thu Oct  6 19:01:21 2005
Subject: [concurrency-interest] ThreadPoolExecutor
Message-ID: <4BAE344F80BC5D4B961906C7001729DF01B18E87@mail-006.w2k.automotive.com>

Sorry forgot to include the list


... Snipped
> 
> > P.S. I have confirmed via a small test case that a corePoolSize of 
> > zero will result in submitted tasks *never* executing.
> 
> As you would expect - threads beyond the core size only get created 
> when the queue is full. If the queue can't fill then no threads ever 
> get created. Do you think this combination should be detected and 
> prohibited?
> 

Just a little surprising is all, though I'm not sure how the
ThreadPoolExecutor could know that a queue is non-synchronous?  Actually
isn't corePoolSize==0 an invalid usage pattern if the queue has *any*
capacity at all?  Otherwise the risk of tasks sitting on the queue never
being executed (or later being executed after a possibly lengthy wait)
is very high as I see it.  Once the queue overflows though (ie offer()
returns false), a core thread will be created and will flush out the
queue.  Not sure if this has bitten anyone but perhaps it has and no one
has noticed...


> > P.P.S. Am I still confused and not knowing what I want?  I assumed 
> > this behaviour is what most people would want for a dynamic
> work queue
> > (0-1000 threads) with bursty request patterns (0-1 million+ on the 
> > queue ant any given point), but I cannot find much in the
> archives...
> 
> The current design assumes that the core is set such that the expected

> load is accommodated, with room for increasing the worker set under 
> heavy load, by limiting the buffering done by the queue. Having large 
> numbers of threads in the pool is generally detrimental to overall 
> performce, but may be necessary if the tasks are expected to block for

> reasonable periods.
> Otherwise with a small core size you would not generally be concerned 
> about the startup overhead of these threads - whether done eagerly or 
> on demand.
> 
> Do you really expect 1000 active threads? How many CPU's are you 
> running on?
> Even on a 384-way Azul box I wouldn't expect to need a pool that 
> large. :)

Would love to play with such a beast but unfortunately no, we're working
on bog-standard ia32 & amd64 ;-)

What we have is lots of threads active at a time doing things like
waiting for a database or network response so our pools need to be
pretty big to accommodate all these IO blocking calls.  I'd love to move
to an event driven non-blocking IO model but unfortunately key
technologies such as JDBC don't provide any such options.  Hence my only
real method of not artificially limiting throughput is lots of threads.
Problem with the current threadpool is that the first thousand (for
example, with corePoolSize=1000) requests will each get their own new
thread, regardless of whether these requests ever had the need to
execute concurrently or not.  IE I'm wasting a bunch of threads that
potentially other request queues in the system will need (we have
multiple pools per instance).  We'd like the multiple pools within the
system to be reasonably bounded by the # of concurrent requests (with a
little leeway for thread keepalives of course), while still maintaining
maximum concurrency in the face of a large spike of requests or a
slowdown in our dependent IO operations.

I hope this all made sense & thanks for all the help.  If you don't see
generic usefulness in the core libs its no problem, I'll just maintain a
modified version for myself (and anyone else if interested, just let me
know).

> 
> Cheers,
> David Holmes
> 
> PS. Doug Lea is obviously incommunicado right now or else I'm certain 
> he would have chipped in - and I expect he will when he can.
> 
> 

Thanks again!


Best regards,

Patrick


From dl at cs.oswego.edu  Thu Oct  6 19:14:11 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu Oct  6 19:16:08 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E87@mail-006.w2k.automotive.com>
References: <4BAE344F80BC5D4B961906C7001729DF01B18E87@mail-006.w2k.automotive.com>
Message-ID: <4345AFC3.6090603@cs.oswego.edu>

Patrick Eger wrote:
> 
>>PS. Doug Lea is obviously incommunicado right now .
>>

No, but Joe and David have handled this so well that I have nothing to
add :-)

... except to say that ThreadPoolExecutor handles as many variants
as we can support without hurting performance or adding too much for any
given mode of use. It sounds like your application might escape
the boundaries. Except that I'd be surprised if a simple
cahchedThreadPool was measurably worse that what you are doing.
Bounding to thousands of threads on a 2 or 4way machine
is not much different than not bounding at all.

-Doug

From peger at automotive.com  Thu Oct  6 20:08:33 2005
From: peger at automotive.com (Patrick Eger)
Date: Thu Oct  6 20:09:00 2005
Subject: [concurrency-interest] ThreadPoolExecutor
Message-ID: <4BAE344F80BC5D4B961906C7001729DF01B18E89@mail-006.w2k.automotive.com>

> Patrick Eger wrote:
> > 
> >>PS. Doug Lea is obviously incommunicado right now .
> >>
> 
> No, but Joe and David have handled this so well that I have 
> nothing to add :-)
> 
> ... except to say that ThreadPoolExecutor handles as many 
> variants as we can support without hurting performance or 
> adding too much for any given mode of use. It sounds like 
> your application might escape the boundaries. Except that I'd 
> be surprised if a simple cahchedThreadPool was measurably 
> worse that what you are doing.

The problem with cachedThreadPool() for our use was that the synchronous
handoff / lack of queuing makes us unable to smoothe out transient
request bursts, and lack of MAX size on the pool means potential
resource exhaustion. I couldn't see a way around the SynchronousQueue
used there...

> Bounding to thousands of threads on a 2 or 4way machine is 
> not much different than not bounding at all.
> 
> -Doug
> 
> 

Unbounded would likely work as well if threads were cheaper than they
are.  Alas, we are only able to allocate a few thousand threads per JVM
maximum, and we need it to be shared dynamically among 10-20 separate
pools.


PS As an aside, are there any known issues with the old "oswego" package
thread pools (since the new JDK5 memory model)?  I seem to remember
these giving us the desired charateristics, though i'd hate to switch
back and will more likely attempt some modification of the Mustang
sources as previously mentioned.


Again, thanks a bunch for everybody's help on this.


Best regards,

Patrick

From dholmes at dltech.com.au  Fri Oct  7 01:03:56 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Fri Oct  7 01:04:19 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEEKGFAA.dholmes@dltech.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEFAGFAA.dholmes@dltech.com.au>

I wrote:
> I'm intrigued enough to try and implement that dual queue set up. :)

Well the basic idea seems sound but the details are a bit tricky. To do the
queue switching you need atomicity otherwise the possible states are too
numerous for me to work out how to handle them. :) But atomicity at the
dual-queue level conflicts with the normal synchronization of the
SynchronousQueue and LinkedBlockingQueue (imagine trying to add a
synchronized wrapper to them: you hold the outer lock while blocked waiting
for space/elements). So the queue would have to be fully custom implemented,
rather than wrapping existing queue implementations. :(

Cheers,
David Holmes


From cnmaclean at hotmail.com  Fri Oct  7 08:08:45 2005
From: cnmaclean at hotmail.com (Calum MacLean)
Date: Fri Oct  7 08:59:47 2005
Subject: [concurrency-interest] Unbounded blocking queues - memory
	consumption
Message-ID: <di5ogf$mkg$1@sea.gmane.org>

Hi

My application is producing a large unbounded quantity of objects which are 
being put into a BlockingQueue for consumption by some other thread.
I'm currently using LinkedBlockingQueue, as it's unbounded.
However, I'm seeing that the LinkedBlockingQueue.Node objects take up a fair 
bit of memory - around about 16 bytes each according to my profiler.
So, while I'm not 100% sure, I'm a bit worried about the memory consumption, 
as it's a large number of objects which are being produced.

Are there any other alternatives for unbounded BlockingQueue implementations 
which maybe take up less memory than LinkedBlockingQueue?

Thanks for your help,
Calum 



From dl at cs.oswego.edu  Fri Oct  7 09:23:06 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri Oct  7 09:24:53 2005
Subject: [concurrency-interest] Unbounded blocking queues -
	memory	consumption
In-Reply-To: <di5ogf$mkg$1@sea.gmane.org>
References: <di5ogf$mkg$1@sea.gmane.org>
Message-ID: <434676BA.7090500@cs.oswego.edu>

Calum MacLean wrote:
> Hi
> 
> My application is producing a large unbounded quantity of objects which are 
> being put into a BlockingQueue for consumption by some other thread.
> I'm currently using LinkedBlockingQueue, as it's unbounded.
> However, I'm seeing that the LinkedBlockingQueue.Node objects take up a fair 
> bit of memory - around about 16 bytes each according to my profiler.
> So, while I'm not 100% sure, I'm a bit worried about the memory consumption, 
> as it's a large number of objects which are being produced.
> 
> Are there any other alternatives for unbounded BlockingQueue implementations 
> which maybe take up less memory than LinkedBlockingQueue?
> 

I don't believe there is any way to reduce overhead for any
linked structure down and further -- the nodes only have item
and next fields; plus the usual Java per-Object header etc.
If you really need the space and are in full control of the
kinds of elements, you might be able to make a custom version.
If for example, each element is of class Element, you can add
a "next" link to the Element class, used only by the queue, and
then copy-paste-hack LinkedBlockingQueue to directly use it
rather than wrapping each item in a Node. This is not recommended
unless you are desparate though.

-Doug



From dl at cs.oswego.edu  Fri Oct  7 09:40:15 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri Oct  7 09:42:02 2005
Subject: [concurrency-interest] ThreadPoolExecutor
In-Reply-To: <4BAE344F80BC5D4B961906C7001729DF01B18E89@mail-006.w2k.automotive.com>
References: <4BAE344F80BC5D4B961906C7001729DF01B18E89@mail-006.w2k.automotive.com>
Message-ID: <43467ABF.5050900@cs.oswego.edu>

Patrick Eger wrote:
>
> 
> The problem with cachedThreadPool() for our use was that the synchronous
> handoff / lack of queuing makes us unable to smoothe out transient
> request bursts, 

One way to deal with this is to create a custom ThreadFactory that
creates threads in batches.

> and lack of MAX size on the pool means potential
> resource exhaustion. 

You can still use a max expecially if you can live with a
CallerRuns or Discard policy on saturation. Which
I'd guess that you probably need to do anyway in this
case.

> 
> PS As an aside, are there any known issues with the old "oswego" package
> thread pools (since the new JDK5 memory model)?  

None that I'm aware of. That package was pretty conservative
about assumptions about JVMs.

-Doug


From cnmaclean at hotmail.com  Fri Oct  7 09:37:51 2005
From: cnmaclean at hotmail.com (Calum MacLean)
Date: Fri Oct  7 09:45:41 2005
Subject: [concurrency-interest] Re: Unbounded blocking queues -memory
	consumption
References: <di5ogf$mkg$1@sea.gmane.org> <434676BA.7090500@cs.oswego.edu>
Message-ID: <di5tnk$8vp$1@sea.gmane.org>

Hi Doug
I was thinking of implementations other than linked-list style.  For 
example, there's an ArrayBlockingQueue, which would presumably take up less 
memory, but that's fixed capacity.
I'm wondering if there's any middle ground - unbounded, but minimal memory.
Thanks,
Calum


"Doug Lea" <dl@cs.oswego.edu> wrote in message 
news:434676BA.7090500@cs.oswego.edu...
> Calum MacLean wrote:
>> Hi
>>
>> My application is producing a large unbounded quantity of objects which 
>> are being put into a BlockingQueue for consumption by some other thread.
>> I'm currently using LinkedBlockingQueue, as it's unbounded.
>> However, I'm seeing that the LinkedBlockingQueue.Node objects take up a 
>> fair bit of memory - around about 16 bytes each according to my profiler.
>> So, while I'm not 100% sure, I'm a bit worried about the memory 
>> consumption, as it's a large number of objects which are being produced.
>>
>> Are there any other alternatives for unbounded BlockingQueue 
>> implementations which maybe take up less memory than LinkedBlockingQueue?
>>
>
> I don't believe there is any way to reduce overhead for any
> linked structure down and further -- the nodes only have item
> and next fields; plus the usual Java per-Object header etc.
> If you really need the space and are in full control of the
> kinds of elements, you might be able to make a custom version.
> If for example, each element is of class Element, you can add
> a "next" link to the Element class, used only by the queue, and
> then copy-paste-hack LinkedBlockingQueue to directly use it
> rather than wrapping each item in a Node. This is not recommended
> unless you are desparate though.
>
> -Doug 



From dl at cs.oswego.edu  Sat Oct  8 09:25:59 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat Oct  8 09:27:45 2005
Subject: [concurrency-interest] Re: Unbounded blocking queues -memory
	consumption
In-Reply-To: <di5tnk$8vp$1@sea.gmane.org>
References: <di5ogf$mkg$1@sea.gmane.org> <434676BA.7090500@cs.oswego.edu>
	<di5tnk$8vp$1@sea.gmane.org>
Message-ID: <4347C8E7.6020707@cs.oswego.edu>

Calum MacLean wrote:
> Hi Doug
> I was thinking of implementations other than linked-list style.  For 
> example, there's an ArrayBlockingQueue, which would presumably take up less 
> memory, but that's fixed capacity.
> I'm wondering if there's any middle ground - unbounded, but minimal memory.
> 


I think that Dawid Kurzyniec might have once put together something
along these lines but I don't see it anywhere offhand.

But it is on average unlikely that an array will save memory:

On a 32 bit machine, an array will have 4bytes per
POTENTIAL item, while a Node { E item, Node next; }
will have minimally 8bytes, possibly 12bytes (one word
object header), and often in practice (because of alignment
or multiword header) 16 bytes per ACTUAL item. This means that an array
will take less space only when it is more than 1/2, 1/3, or 1/4
full, depending on the exact node size.

Statistically (read up on queuing theory for details),
unbounded BlockingQueues used in producer-consumer designs
are likely to be almost-always nearly empty. If they weren't, then they
would on average eventually grow without bound and exhaust all memory.
This means that in practice, an array-based version is likely
to be less than even 1/4 full. Sometimes it will transiently
have many more elements. But in that case, it is usually better
to stick with a linked version that can release nodes and let GC
reclaim space rather than keeping so many unused array slots around.

There are various special cases where you might know more
about usage patterns to do better than this, but I don't
think that they are very common, and I think that most of them are so
special that you would end up writing a custom implementation
anyway if you really needed to minimize footprint.

-Doug
From yechielf at gigaspaces.com  Sun Oct  9 08:30:54 2005
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Sun Oct  9 07:23:34 2005
Subject: [concurrency-interest] propper way to use CAS ? 
Message-ID: <D166C96F43D1D611B8E3000255A0C48C6BE574@OFFICESRV>

Hi,
in many concurrent classes when compare-and-set (cas) is used- it is an
endless loop of competing threads trying to fight over an object (,say, a
queue's tail in order to connect a new element (reference)) .
Wouldn't it be benefitial to perform a thread.Yield() after N
(pre-configured) unsuccessful CAS retries ? wouldnt it give better overall
performance ?

Regrds,
Yechiel Fefer     
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051009/7ef21908/attachment.htm
From brian at quiotix.com  Sun Oct  9 11:18:09 2005
From: brian at quiotix.com (Brian Goetz)
Date: Sun Oct  9 11:18:36 2005
Subject: [concurrency-interest] propper way to use CAS ?
In-Reply-To: <D166C96F43D1D611B8E3000255A0C48C6BE574@OFFICESRV>
References: <D166C96F43D1D611B8E3000255A0C48C6BE574@OFFICESRV>
Message-ID: <434934B1.4090109@quiotix.com>

> in many concurrent classes when compare-and-set (cas) is used- it is an 
> endless loop of competing threads trying to fight over an object (,say, 
> a queue's tail in order to connect a new element (reference)) .
> 
> Wouldn't it be benefitial to perform a thread.Yield() after N 
> (pre-configured) unsuccessful CAS retries ? wouldnt it give better 
> overall performance ?

It depends on the degree of contention you expect to experience.

In most cases, the standard approach (retry continuously) is best.  Only 
when you are expecting very heavy contention, where you expect most CAS 
attempts to fail, is a more sophisticated backoff strategy beneficial.

When there is no contention for a CAS, it always succeeds.  When 
multiple threads compete for a CAS, one will always win and make 
progress.  So the question is, what percent of CAS attempts will lose 
and need to be retried?  Only if this ratio is high will a better 
contention management strategy be a win.  This is a function of how many 
threads are involved, and the ratio of CAS to "other work".

If your contention is so high that most CAS attempts fail, you might be 
better off with a lock, as locking automatically addresses the problem 
you raise fairly well, by blocking so as not to create more contention.

From mike.skells at ebizz-consulting.com  Mon Oct 10 09:48:30 2005
From: mike.skells at ebizz-consulting.com (Mike Skells)
Date: Mon Oct 10 09:49:05 2005
Subject: [concurrency-interest] propper way to use CAS ?
In-Reply-To: <434934B1.4090109@quiotix.com>
Message-ID: <012701c5cda1$4d3c3fb0$0a01a8c0@MikeLaptop>

Hi,
Doesnt this presuppose that you are running on a system the is pre-emptive,
So if you are running on a non pre-emptive system then the unsuccessful CAS
retries will just continue wont they?

Mike

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu 
> [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
> Of Brian Goetz
> Sent: 09 October 2005 16:18
> To: Yechiel Feffer
> Cc: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] propper way to use CAS ?
> 
> > in many concurrent classes when compare-and-set (cas) is 
> used- it is 
> > an endless loop of competing threads trying to fight over an object 
> > (,say, a queue's tail in order to connect a new element 
> (reference)) .
> > 
> > Wouldn't it be benefitial to perform a thread.Yield() after N
> > (pre-configured) unsuccessful CAS retries ? wouldnt it give better 
> > overall performance ?
> 
> It depends on the degree of contention you expect to experience.
> 
> In most cases, the standard approach (retry continuously) is 
> best.  Only when you are expecting very heavy contention, 
> where you expect most CAS attempts to fail, is a more 
> sophisticated backoff strategy beneficial.
> 
> When there is no contention for a CAS, it always succeeds.  
> When multiple threads compete for a CAS, one will always win 
> and make progress.  So the question is, what percent of CAS 
> attempts will lose and need to be retried?  Only if this 
> ratio is high will a better contention management strategy be 
> a win.  This is a function of how many threads are involved, 
> and the ratio of CAS to "other work".
> 
> If your contention is so high that most CAS attempts fail, 
> you might be better off with a lock, as locking automatically 
> addresses the problem you raise fairly well, by blocking so 
> as not to create more contention.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From brian at quiotix.com  Mon Oct 10 11:22:45 2005
From: brian at quiotix.com (Brian Goetz)
Date: Mon Oct 10 11:23:12 2005
Subject: [concurrency-interest] propper way to use CAS ?
In-Reply-To: <012701c5cda1$4d3c3fb0$0a01a8c0@MikeLaptop>
References: <012701c5cda1$4d3c3fb0$0a01a8c0@MikeLaptop>
Message-ID: <434A8745.8010702@quiotix.com>

> Doesnt this presuppose that you are running on a system the is pre-emptive,
> So if you are running on a non pre-emptive system then the unsuccessful CAS
> retries will just continue wont they?

Until they eventually succeed.  But the point is, under real-world 
conditions, they _will_ eventually succeed.

An uncontended CAS is always successful.  In every contended CAS, one 
thread always succeeds.  Some thread always makes progress.

In order for a thread to continue retrying a CAS forever, it means that 
other threads must be pounding really hard on that same atomic variable, 
and always winning.  But each thread has an unbiased chance of winning 
any given CAS.  On a system with N processors, that chance should be 
close to 1/N.  So the expected number of retrys _in the absolute 
unrealistic worst case, where every processing is doing nothing but 
pounding on one atomic variable_, is N.  A far cry from "retrying 
forever".

But in reality, you will never see contention like that, because 
programs do other work besides pounding on a given memory location with 
CAS.

It's not like locking, where one thread can do this:

   synchronized (globalLock) {
     Thread.sleep(Long.MAX_VALUE);
   }

and deny access to the lock to other threads.  In a CAS, someone always 
makes progress, and in reality, every thread makes progress within a few 
retries.

From chris.purcell.39 at gmail.com  Mon Oct 10 11:39:07 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Mon Oct 10 12:00:38 2005
Subject: [concurrency-interest] propper way to use CAS ?
In-Reply-To: <434A8745.8010702@quiotix.com>
References: <012701c5cda1$4d3c3fb0$0a01a8c0@MikeLaptop>
	<434A8745.8010702@quiotix.com>
Message-ID: <c6419ae56b96cfdbb3304e82014e3556@gmail.com>

> But each thread has an unbiased chance of winning any given CAS.

Actually, this probably doesn't hold up on all processors. Biasing 
success towards the current owner of the cache-line just makes good 
sense, as it gives an increase in throughput.

Chris

From dawidk at mathcs.emory.edu  Mon Oct 10 13:08:59 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Oct 10 13:09:26 2005
Subject: [concurrency-interest] Re: Unbounded blocking queues 
	-memoryconsumption
In-Reply-To: <4347C8E7.6020707@cs.oswego.edu>
References: <di5ogf$mkg$1@sea.gmane.org> 
	<434676BA.7090500@cs.oswego.edu><di5tnk$8vp$1@sea.gmane.org> 
	<4347C8E7.6020707@cs.oswego.edu>
Message-ID: <434AA02B.6010805@mathcs.emory.edu>

Doug Lea wrote:

> Calum MacLean wrote:
>
>> Hi Doug
>> I was thinking of implementations other than linked-list style.  For 
>> example, there's an ArrayBlockingQueue, which would presumably take 
>> up less memory, but that's fixed capacity.
>> I'm wondering if there's any middle ground - unbounded, but minimal 
>> memory.
>>
>
>
> I think that Dawid Kurzyniec might have once put together something
> along these lines but I don't see it anywhere offhand.
>
dynamically growing array-based blocking queue, DynamicArrayBlockingQueue:

http://dcl.mathcs.emory.edu/util/#concurrent

http://dcl.mathcs.emory.edu/cgi-bin/viewcvs.cgi/software/util/src/edu/emory/mathcs/util/concurrent/DynamicArrayBlockingQueue.java?rev=1.6&view=markup

> But it is on average unlikely that an array will save memory:
> (...)

This is true, but the array-based implementation can save the day if you 
care only about the peek memory usage, not the average one. For 
instance, if you have occassional bursts and you want to avert 
catastrophic OutOfMemoryErrors.

Also, it is possible to modify the DynamicArrayBlockingQueue so that it 
shrinks back when emptied; this requires reasonable application-specific 
heuristics though so I didn't put that functionality in the general 
version. Perhaps the "compact()" method would do?

DynamicArrayBlockingQueue does not allow concurrent reads and writes, so 
it is more prone to congestion than LinkedBlockingQueue. I am guessing 
this should not happen a lot in a single-producer-single-consumer 
scenario though, since the accessor methods are very brief. But I did 
not measure this.

Both LinkedBlockingQueue and DynamicArrayBlockingQueue have a practical 
capacity limit of 2^31-1. Current implementation of 
DynamicArrayBlockingQueue may not handle overflow past that value 
gracefully. (Although you would almost surely see OutOfMemoryError first).

Regards,
Dawid

From Vishal.Bhasin at sabre-holdings.com  Mon Oct 10 17:55:44 2005
From: Vishal.Bhasin at sabre-holdings.com (Bhasin, Vishal)
Date: Mon Oct 10 18:48:01 2005
Subject: [concurrency-interest] 
	java.util.concurrent.locks.Condition.await(1000,
	TIME.MILLISECONDS) issue
Message-ID: <F1B223E9EE316F49814546D9019E27CB03C244A1@sgtulmsp01.Global.ad.sabre.com>

I noticed that using java.util.concurrent.locks.Condition.await(1000,
TIME.MILLISECONDS) causes the application to hang for a few seconds
(9-10 secs) every so often while running a load test. However, this
doesn't happen when I use boolean and use
java.util.concurrent.locks.Condition.await() in a while loop until this
is set to false. I wonder if anyone has seen this before. 

This one causes the lag to happen -

public abstract class BaseSyncService
{

    /** Timeout for process method, default 10 seconds */
    protected int _timeOut = 10 * 1000;
    final Lock lock = new ReentrantLock();
    final Condition notDone  = lock.newCondition(); 

    protected void releaseMutex()
    {
        lock.lock();
        try {
          notDone.signalAll();
        } finally {
          lock.unlock();
        }
    }

    public Object waitForResponse(int timeout)
        throws InterruptedException
    {
        lock.lock();

        try {
              notDone.await(timeout, TimeUnit.MILLISECONDS);
          } finally {
            lock.unlock();
          }

          return responseObject;
    }
}


The following code works fine



public abstract class BaseSyncService
{

    final Lock lock = new ReentrantLock();
    final Condition notDone  = lock.newCondition(); 

    protected void releaseMutex()
    {
        lock.lock();
        try {
            waitFlag = false;
          notDone.signalAll();
        } finally {
          lock.unlock();
        }
    }

    boolean waitFlag = true;

    public Object waitForResponse(int timeout)
        throws InterruptedException
    {
        lock.lock();

        try {
            while(waitFlag)
            {
                notDone.await();
            }

          } finally {
            lock.unlock();
          }

          return responseObject;
    }
}

I'd appreciate any suggestions..

Thanks,

Vishal Bhasin

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051010/266da589/attachment.htm
From dholmes at dltech.com.au  Tue Oct 11 19:54:29 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Tue Oct 11 19:55:13 2005
Subject: [concurrency-interest]
	java.util.concurrent.locks.Condition.await(1000,
	TIME.MILLISECONDS) issue
In-Reply-To: <F1B223E9EE316F49814546D9019E27CB03C244A1@sgtulmsp01.Global.ad.sabre.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEMLGFAA.dholmes@dltech.com.au>

java.util.concurrent.locks.Condition.await(1000, TIME.MILLISECONDS)
issueVishal,

I'm not at all clear on exactly what you are reporting. If I understand you
right, the code that waits until the condition is signalled works fine, but
the code that uses a timeout (and which never actually checks for being
signalled) occasionally hangs for 9-10 seconds. Without seeing the code that
uses the BaseSyncService it is very hard to get an idea of what might be
happening. And I'm assuming that all else is "equal" when you run the two
different versions.

Can you post more code?

David Holmes


  -----Original Message-----
  From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Bhasin,
Vishal
  Sent: Tuesday, 11 October 2005 7:56 AM
  To: Doug Lea; concurrency-interest@altair.cs.oswego.edu
  Subject: [concurrency-interest]
java.util.concurrent.locks.Condition.await(1000,TIME.MILLISECONDS) issue


  I noticed that using java.util.concurrent.locks.Condition.await(1000,
TIME.MILLISECONDS) causes the application to hang for a few seconds (9-10
secs) every so often while running a load test. However, this doesn't happen
when I use boolean and use java.util.concurrent.locks.Condition.await() in a
while loop until this is set to false. I wonder if anyone has seen this
before.

  This one causes the lag to happen -

  public abstract class BaseSyncService

  {

      /** Timeout for process method, default 10 seconds */

      protected int _timeOut = 10 * 1000;

      final Lock lock = new ReentrantLock();

      final Condition notDone  = lock.newCondition();

      protected void releaseMutex()

      {

          lock.lock();

          try {

            notDone.signalAll();

          } finally {

            lock.unlock();

          }

      }

      public Object waitForResponse(int timeout)

          throws InterruptedException

      {

          lock.lock();

          try {

                notDone.await(timeout, TimeUnit.MILLISECONDS);

            } finally {

              lock.unlock();

            }

            return responseObject;

      }

  }



  The following code works fine




  public abstract class BaseSyncService

  {

      final Lock lock = new ReentrantLock();

      final Condition notDone  = lock.newCondition();

      protected void releaseMutex()

      {

          lock.lock();

          try {

              waitFlag = false;

            notDone.signalAll();

          } finally {

            lock.unlock();

          }

      }

      boolean waitFlag = true;

      public Object waitForResponse(int timeout)

          throws InterruptedException

      {

          lock.lock();

          try {

              while(waitFlag)

              {

                  notDone.await();

              }

            } finally {

              lock.unlock();

            }

            return responseObject;

      }

  }

  I'd appreciate any suggestions..

  Thanks,

  Vishal Bhasin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051012/9353335d/attachment.htm
From pugh at cs.umd.edu  Tue Oct 11 20:37:55 2005
From: pugh at cs.umd.edu (Bill Pugh)
Date: Tue Oct 11 20:38:36 2005
Subject: [concurrency-interest]
In-Reply-To: <F1B223E9EE316F49814546D9019E27CB03C244A1@sgtulmsp01.Global.ad.sabre.com>
References: <F1B223E9EE316F49814546D9019E27CB03C244A1@sgtulmsp01.Global.ad.sabre.com>
Message-ID: <85DD004C-849A-4B57-8687-A8059EDC3354@cs.umd.edu>

The code below is broken.

For one thing, if the releaseMutex call occurs before the call to   
occurs before the
call to waitForResponse, the signal will be missed.

Also, await is allowed to return spuriously, so waitForResponse might  
return
even though no signal or timeout has occurred.

Use the boolean variable.

     Bill

On Oct 10, 2005, at 5:55 PM, Bhasin, Vishal wrote:

> I noticed that using java.util.concurrent.locks.Condition.await 
> (1000, TIME.MILLISECONDS) causes the application to hang for a few  
> seconds (9-10 secs) every so often while running a load test.  
> However, this doesn't happen when I use boolean and use  
> java.util.concurrent.locks.Condition.await() in a while loop until  
> this is set to false. I wonder if anyone has seen this before.
>
> This one causes the lag to happen -
>
> public abstract class BaseSyncService
>
> {
>
>     /** Timeout for process method, default 10 seconds */
>
>     protected int _timeOut = 10 * 1000;
>
>     final Lock lock = new ReentrantLock();
>
>     final Condition notDone  = lock.newCondition();
>
>     protected void releaseMutex()
>
>     {
>
>         lock.lock();
>
>         try {
>
>           notDone.signalAll();
>
>         } finally {
>
>           lock.unlock();
>
>         }
>
>     }
>
>     public Object waitForResponse(int timeout)
>
>         throws InterruptedException
>
>     {
>
>         lock.lock();
>
>         try {
>
>               notDone.await(timeout, TimeUnit.MILLISECONDS);
>
>           } finally {
>
>             lock.unlock();
>
>           }
>
>           return responseObject;
>
>     }
>
> }
>
>
From Ryan.LeCompte at pangonetworks.com  Wed Oct 12 07:59:42 2005
From: Ryan.LeCompte at pangonetworks.com (Ryan LeCompte)
Date: Wed Oct 12 08:00:14 2005
Subject: [concurrency-interest] Assignment of references atomic?
Message-ID: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>

Hello all,

I know that assignment of doubles are potentially unsafe since they are 64 bits, but I was wonder if assignment of object references are atomic in J2SE 1.4.2 and J2SE 5.0? For example, is it safe to do the following without synchronizing the methods:

Object getObject() {

   return object;

}

 

void setObject(Object o) {

   callAnotherMethod();

   if (someCondition == true) {

      callAnotherMethod();

      this.object = o;

   }

}

Thanks,

Ryan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051012/5af705f7/attachment.htm
From joe.bowbeer at gmail.com  Wed Oct 12 09:19:29 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed Oct 12 09:19:49 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
Message-ID: <31f2a7bd0510120619x7ee1a37dt2e894733e865e831@mail.gmail.com>

On 10/12/05, Ryan LeCompte <Ryan.LeCompte@pangonetworks.com> wrote:
>
> I know that assignment of doubles are potentially unsafe since they are 64
> bits, but I was wonder if assignment of object references are atomic in J2SE
> 1.4.2 and J2SE 5.0?
>

The JMM chapter of the JLS says writes and reads of object references
are always atomic.

"Writes to and reads of references are always atomic, regardless of
whether they are implemented as 32 or 64 bit values."

http://java.sun.com/docs/books/jls/third_edition/html/memory.html

Whether unsync'd writes and reads are "safe" is another matter..

From tim at peierls.net  Wed Oct 12 09:25:30 2005
From: tim at peierls.net (Tim Peierls)
Date: Wed Oct 12 09:26:15 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
Message-ID: <434D0ECA.30806@peierls.net>

Short answer: No, this is not safe, but not for the reason you think.

Longer answer: The JVM is under no obligation to make the new value of this.object (o) visible to 
other threads. While it is true that any value other threads see is guaranteed to be *some* value 
that was written by some thread at some point, it will not necessarily be the most *recent* value. 
The value seen by other threads might be a "stale" value; in fact, it might be the field's initial 
value.

The situation with doubles is even worse: not only will threads not necessarily see the most 
recent value, but the value seen might not be a value that was ever written by any thread, a 
so-called "out of thin air" value.

Unless you have confined all getObject/setObject calls to a single thread, or your program can 
tolerate arbitrarily stale values (which is a highly unusual property and one that should be 
documented carefully if you rely on it), you need to guard all accesses to the field with some 
kind of locking. The simplest way to do this is, as you implied, to add the synchronized keyword 
to methods that access this field.

--tim

Ryan LeCompte wrote:
> I know that assignment of doubles are potentially unsafe since they are 
> 64 bits, but I was wonder if assignment of object references are atomic 
> in J2SE 1.4.2 and J2SE 5.0? For example, is it safe to do the following 
> without synchronizing the methods:
> 
> Object getObject() {
>    return object;
> }
> 
> void setObject(Object o) {
>    callAnotherMethod();
>    if (someCondition == true) {
>       callAnotherMethod();
>       this.object = o;
>    }
> }

From ryan.lecompte at pangonetworks.com  Wed Oct 12 09:40:56 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Wed Oct 12 09:41:22 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <434D0ECA.30806@peierls.net>
Message-ID: <PANGOSERVERN46s2apP00000912@pangonetworks.com>

Tim,

Thank you for the lengthy response. This particular part of the code can
tolerate stale values that are returned from the getObject() method.
However, I want to be able to rely on the fact that the getObject() method
will EVENTUALLY return a recent update to the this.object (o) field. Am I
safe to make that assumption? Or is it possible for the JVM to always return
a stale value even if the value gets updated one more time? What about
declaring the variable volatile? 

Ryan

-----Original Message-----
From: Tim Peierls [mailto:tim@peierls.net] 
Sent: Wednesday, October 12, 2005 9:26 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Assignment of references atomic?

Short answer: No, this is not safe, but not for the reason you think.

Longer answer: The JVM is under no obligation to make the new value of
this.object (o) visible to 
other threads. While it is true that any value other threads see is
guaranteed to be *some* value 
that was written by some thread at some point, it will not necessarily be
the most *recent* value. 
The value seen by other threads might be a "stale" value; in fact, it might
be the field's initial 
value.

The situation with doubles is even worse: not only will threads not
necessarily see the most 
recent value, but the value seen might not be a value that was ever written
by any thread, a 
so-called "out of thin air" value.

Unless you have confined all getObject/setObject calls to a single thread,
or your program can 
tolerate arbitrarily stale values (which is a highly unusual property and
one that should be 
documented carefully if you rely on it), you need to guard all accesses to
the field with some 
kind of locking. The simplest way to do this is, as you implied, to add the
synchronized keyword 
to methods that access this field.

--tim

Ryan LeCompte wrote:
> I know that assignment of doubles are potentially unsafe since they are 
> 64 bits, but I was wonder if assignment of object references are atomic 
> in J2SE 1.4.2 and J2SE 5.0? For example, is it safe to do the following 
> without synchronizing the methods:
> 
> Object getObject() {
>    return object;
> }
> 
> void setObject(Object o) {
>    callAnotherMethod();
>    if (someCondition == true) {
>       callAnotherMethod();
>       this.object = o;
>    }
> }

From tim at peierls.net  Wed Oct 12 09:58:10 2005
From: tim at peierls.net (Tim Peierls)
Date: Wed Oct 12 09:58:37 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <PANGOSERVERN46s2apP00000912@pangonetworks.com>
References: <PANGOSERVERN46s2apP00000912@pangonetworks.com>
Message-ID: <434D1672.2090508@peierls.net>

Ryan LeCompte wrote:
> Thank you for the lengthy response. This particular part of the code can tolerate stale values
> that are returned from the getObject() method. However, I want to be able to rely on the fact
> that the getObject() method will EVENTUALLY return a recent update to the this.object (o)
> field. Am I safe to make that assumption? Or is it possible for the JVM to always return a
> stale value even if the value gets updated one more time?

That assumption is not safe. No matter how many times a thread updates the variable, other threads 
will see arbitrarily stale values.


> What about declaring the variable volatile?

Ah! Yes, volatile *does* work, and is the most appropriate mechanism. I should have mentioned that 
first, sorry.

The only reason you might *not* want to use a volatile is if there is a class invariant involving 
more than one field, including the object field. I saw the use of a variable named someCondition 
and assumed this was the case.

--tim

From ryan.lecompte at pangonetworks.com  Wed Oct 12 10:02:17 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Wed Oct 12 10:02:42 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <434D1672.2090508@peierls.net>
Message-ID: <PANGOSERVER9THwIX2Q00000920@pangonetworks.com>

I'm not sure I completely understood your last reason for *not* using
volatile. Forgetting about the code sample from earlier, I basically have
two variables.. one is a primitive (boolean) and the other is a regular
Object reference. Is it safe to declare both of these as volatile? I want to
ensure that other threads see the most recent values of these two variables.

Ryan

-----Original Message-----
From: Tim Peierls [mailto:tim@peierls.net] 
Sent: Wednesday, October 12, 2005 9:58 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Assignment of references atomic?

Ryan LeCompte wrote:
> Thank you for the lengthy response. This particular part of the code can
tolerate stale values
> that are returned from the getObject() method. However, I want to be able
to rely on the fact
> that the getObject() method will EVENTUALLY return a recent update to the
this.object (o)
> field. Am I safe to make that assumption? Or is it possible for the JVM to
always return a
> stale value even if the value gets updated one more time?

That assumption is not safe. No matter how many times a thread updates the
variable, other threads 
will see arbitrarily stale values.


> What about declaring the variable volatile?

Ah! Yes, volatile *does* work, and is the most appropriate mechanism. I
should have mentioned that 
first, sorry.

The only reason you might *not* want to use a volatile is if there is a
class invariant involving 
more than one field, including the object field. I saw the use of a variable
named someCondition 
and assumed this was the case.

--tim

From mike.skells at ebizz-consulting.com  Wed Oct 12 10:11:30 2005
From: mike.skells at ebizz-consulting.com (Mike Skells)
Date: Wed Oct 12 10:12:04 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
Message-ID: <022b01c5cf36$d8264ee0$0a01a8c0@MikeLaptop>

Hi,
 
Without doing something it is certainly unsafe unless the object is
volatile, as there is nothing to force the object to be written or read from
the global address space, so the object references could be assigning and
rading data from the thread cache only
 
Object references are I believe atomic according the the JLS, bu the that
does not make it threadsafe.
 
I think that you need to explain in a little more detail what you expect the
bahavior to be
 
Mike


  _____  

From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf Of Ryan
LeCompte
Sent: 12 October 2005 13:00
To: concurrency-interest@altair.cs.oswego.edu
Subject: [concurrency-interest] Assignment of references atomic?


Hello all,



I know that assignment of doubles are potentially unsafe since they are 64
bits, but I was wonder if assignment of object references are atomic in J2SE
1.4.2 and J2SE 5.0? For example, is it safe to do the following without
synchronizing the methods:



Object getObject() {

   return object;

}

 

void setObject(Object o) {

   callAnotherMethod();

   if (someCondition == true) {

      callAnotherMethod();

      this.object = o;

   }

}



Thanks,

Ryan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051012/f6efa124/attachment.htm
From brian at quiotix.com  Wed Oct 12 10:27:50 2005
From: brian at quiotix.com (Brian Goetz)
Date: Wed Oct 12 10:28:20 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
Message-ID: <434D1D66.1020106@quiotix.com>

There are two things going on here.

The assignment of the _reference_ is atomic -- other threads will either 
see the old value, or the new value, but not gibberish.  But in the 
general case, this program is still broken.  Just atomically assigning 
the reference is not enough to ensure thread-safety -- you still have to 
ensure that the state of the referred-to object is also properly 
published.

It is possible for a thread calling getObject in this case to see a 
partially constructed object; in other words, see an up-to-date value of 
'object' but a stale/inconsistent/invalid version of the object's state, 
because object was not properly published.

In short, no, you cannot do what you want to do.  You have to synchronize.

> Object getObject() {
> 
>    return object;
> 
> }
> 
> void setObject(Object o) {
>    callAnotherMethod();
>    if (someCondition == true) {
>       callAnotherMethod();
>       this.object = o;
>    }
> }

From gregg at cytetech.com  Wed Oct 12 11:35:41 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed Oct 12 11:36:28 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
Message-ID: <434D2D4D.3050605@cytetech.com>



Ryan LeCompte wrote:
> Hello all,
> 
> I know that assignment of doubles are potentially unsafe since they are 64 bits, but I was wonder if assignment of object references are atomic in J2SE 1.4.2 and J2SE 5.0? For example, is it safe to do the following without synchronizing the methods:
> 
> Object getObject() {
>    return object;
> }
> 
> void setObject(Object o) {
>    callAnotherMethod();
> 
>    if (someCondition == true) {
>       callAnotherMethod();
>       this.object = o;
> 
>    }
> }

The other replies so far are good information.  The only interesting point, I 
think is that if this.object is not volatile, then there is no guarentee of 
when, exactly, the effects of "this.object = o" will be visible to other 
threads.  You will never see an inconsistant value, but you may not ever/always 
see the "correct" value in other threads.  Either setObject should be 
synchronized, or this.object should be volatile to make sure that the assignment 
is "consistant" with any other sideeffects of calling setObject().

Gregg Wonderly
From tim at peierls.net  Wed Oct 12 12:38:45 2005
From: tim at peierls.net (Tim Peierls)
Date: Wed Oct 12 12:39:45 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <PANGOSERVER9THwIX2Q00000920@pangonetworks.com>
References: <PANGOSERVER9THwIX2Q00000920@pangonetworks.com>
Message-ID: <434D3C15.3090205@peierls.net>

I can't tell from your code sample whether it is safe. The relevant code looked something like this:

   void setObject(Object o) {
       ...
       if (someCondition) {
           ...
           this.object = o;
       }
   }

If your intent is that "object" only be given a new value when "someCondition" is true -- that's 
what I meant by "class invariant" -- then it is *not* enough to make someCondition and object 
volatiles, because the read of someCondition and the subsequent write to object do not happen 
atomically. Volatiles provide visibility but not atomicity. In that case, you must use 
synchronization when accessing the fields, and there is no point in making the fields volatile.

--tim


Ryan LeCompte wrote:
> I'm not sure I completely understood your last reason for *not* using
> volatile. Forgetting about the code sample from earlier, I basically have
> two variables.. one is a primitive (boolean) and the other is a regular
> Object reference. Is it safe to declare both of these as volatile? I want to
> ensure that other threads see the most recent values of these two variables.
> 
> Ryan
> 
> -----Original Message-----
> From: Tim Peierls [mailto:tim@peierls.net] 
> 
> The only reason you might *not* want to use a volatile is if there is a
> class invariant involving more than one field, including the object field. 
> I saw the use of a variable named someCondition and assumed this was the case.

From joe.bowbeer at gmail.com  Wed Oct 12 14:54:38 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed Oct 12 14:54:59 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <434D0ECA.30806@peierls.net>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
	<434D0ECA.30806@peierls.net>
Message-ID: <31f2a7bd0510121154m763108a1q2656492f55fc8340@mail.gmail.com>

Tim Peierls <tim@peierls.net> wrote:
>
> The situation with doubles is even worse: not only will threads not necessarily see
> the most recent value, but the value seen might not be a value that was ever written
> by any thread, a so-called "out of thin air" value.
>

Minor point: I think "out of thin air" is too strong a term to use for
the corruption that is permitted for (64-bit) longs and doubles.

http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.7

"For the purposes of the Java programming language memory model, a
single write to a non-volatile long or double value is treated as two
separate writes: one to each 32-bit half. This can result in a
situation where a thread sees the first 32 bits of a 64 bit value from
one write, and the second 32 bits from another write."

From jmanson at cs.purdue.edu  Wed Oct 12 16:07:47 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 12 16:08:18 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <31f2a7bd0510121154m763108a1q2656492f55fc8340@mail.gmail.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>	<434D0ECA.30806@peierls.net>
	<31f2a7bd0510121154m763108a1q2656492f55fc8340@mail.gmail.com>
Message-ID: <434D6D13.308@cs.purdue.edu>

Joe Bowbeer wrote:
> Tim Peierls <tim@peierls.net> wrote:
> 
>>The situation with doubles is even worse: not only will threads not necessarily see
>>the most recent value, but the value seen might not be a value that was ever written
>>by any thread, a so-called "out of thin air" value.
> 
> Minor point: I think "out of thin air" is too strong a term to use for
> the corruption that is permitted for (64-bit) longs and doubles.
> 
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.7
> 
> "For the purposes of the Java programming language memory model, a
> single write to a non-volatile long or double value is treated as two
> separate writes: one to each 32-bit half. This can result in a
> situation where a thread sees the first 32 bits of a 64 bit value from
> one write, and the second 32 bits from another write."
> 

It isn't actually out of thin air, which we reserve specifically for 
causality discussions.  However, from the perspective of the programmer, 
it might as well be...

					Jeremy
From nathani.vijay at gmail.com  Wed Oct 12 01:07:06 2005
From: nathani.vijay at gmail.com (Vijay)
Date: Wed Oct 12 16:36:12 2005
Subject: [concurrency-interest] Synchronization automatic with Lock in JVM
	5.0?
Message-ID: <000e01c5ceea$cf282170$9cc94bca@toronto.redknee.com>

I am using Sun JVM 5.0.
The javadoc for Lock interface says
 "All Lock implementations must enforce the same memory synchronization semantics as provided by the built-in monitor lock: 
  a.. A successful lock operation acts like a successful monitorEnter action 
  b.. A successful unlock operation acts like a successful monitorExit action "
So does that mean that acquiring a lock is as good as entering a synchronization block i.e. will all the buffered variables be discarded?
Also does that mean that releasing a lock is as good as leaving a synchronization block i.e. will all the buffered changed variables be flushed to RAM?

My apologies if this doubt is too basic.
Thanks.......Vijay.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051012/327d65dd/attachment.htm
From joe.bowbeer at gmail.com  Wed Oct 12 18:05:43 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed Oct 12 18:06:03 2005
Subject: [concurrency-interest] JavaOne 2005 sessions now online
Message-ID: <31f2a7bd0510121505x3211eca5w5d851acf6ad51a7c@mail.gmail.com>

=> http://developers.sun.com/learning/javaoneonline/

"JavaOne 2004 and 2005 technical sessions are available in static PDF
format without registration while the more robust multimedia sessions
with audio and transcription can be viewed by registering with the Sun
Developer Network (SDN). Registration to SDN is free and only takes a
minute."


Simpler, Faster, Better: Concurrency Utilities in JDK Software Version 5.0
http://developers.sun.com/learning/javaoneonline/2005/coreplatform/TS-5807.html

Concurrency Utilities in Practice
http://developers.sun.com/learning/javaoneonline/2005/coreplatform/TS-3423.html

From chris.purcell.39 at gmail.com  Wed Oct 12 18:52:33 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 12 18:53:36 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <434D1D66.1020106@quiotix.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C5@pangoserver.pangonetworks.com>
	<434D1D66.1020106@quiotix.com>
Message-ID: <13e1fc333666320feb3be5df45bdfb8a@gmail.com>

> It is possible for a thread calling getObject in this case to see a 
> partially constructed object; in other words, see an up-to-date value 
> of 'object' but a stale/inconsistent/invalid version of the object's 
> state, because object was not properly published.
>
> In short, no, you cannot do what you want to do.  You have to 
> synchronize.

Just to clarify here: you can synchronize by using the "volatile" 
keyword, right? You do not need to use the "synchronized" keyword?

Cheers,
Chris Purcell

From dholmes at dltech.com.au  Wed Oct 12 19:00:14 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 12 19:00:57 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <022b01c5cf36$d8264ee0$0a01a8c0@MikeLaptop>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEOGGFAA.dholmes@dltech.com.au>

Mike Skells wrote:
> Without doing something it is certainly unsafe unless the object is
> volatile, as there is nothing to force the object to be written or read
> from the global address space, so the object references could be assigning
> and rading data from the thread cache only

This is not directed at Mike personally.

I'd like to try and dissuade people from explaining JMM issues this way. The
old JMM did indeed use a conceptual thread-cache, or working memory, and was
defined in terms of when things had to move between working memory and main
memory.

The new JMM says nothing about caches because that is only one potential
part of the problem - the bigger part being the compiler and the reorderings
that can occur in both hardware and software. The problem with talking about
caches is that people turn round and say "well the cache on my hardware
doesn't work like that, so I don't have to worry about this". And indeed on
some architectures you are essentially guaranteed that all values in cache
make their way to main memory. It is easier to make an argument that a
variable might be placed in a register by the compiler. :)

Cheers,
David Holmes

From dholmes at dltech.com.au  Wed Oct 12 19:09:16 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 12 19:09:41 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <13e1fc333666320feb3be5df45bdfb8a@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEOHGFAA.dholmes@dltech.com.au>

> Chris Purcell writes:
> Just to clarify here: you can synchronize by using the "volatile"
> keyword, right? You do not need to use the "synchronized" keyword?

You can use the volatile keyword to ensure that things are visible and that
the right ordering is maintained.

For example, this is perfectly safe:

  volatile boolean dataReady = false;
  Object theObject;

  Thread-1                             Thread-2..n
  theObject = makeObject();            if (dataReady)
  dataReady = true;                        use(theObject);


Here there is a single writer and many potential readers. As long as the
readers see dataReady is true then they are guaranteed to see everything
that happened to theObject during makeObject().

But volatile in itself is not sufficient *if* there is a need for atomicity
between deciding to update the object and performing the update.

David Holmes

From dholmes at dltech.com.au  Wed Oct 12 19:19:56 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 12 19:20:31 2005
Subject: [concurrency-interest] Synchronization automatic with Lock in
	JVM5.0?
In-Reply-To: <000e01c5ceea$cf282170$9cc94bca@toronto.redknee.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEOHGFAA.dholmes@dltech.com.au>

Vijay,

Yes acquiring a Lock is equivalent in memory model terms to entering a
synchronized block; and similarly for releasing a Lock and leaving a
synchronized block.

The new memory model doesn't talk about buffering and caches and main
memory, because there is much more to it than just that. Rather it defines a
happens-before relationship between actions in a thread. If you can
establish that a write to a variable happens-before a read of that variable,
then the read will see the value written. Similarly happens-before can
establish a relative ordering between two actions.

So when you say "all buffered variables will be discarded" you need to be
careful about what you mean. What the JMM says is that unlock() of a lock X
happens-before a subsequent lock() of X. This means that only actions
performed by the thread that does the unlock() will be visible to the thread
that does the lock(). Actions in other threads where X has not been lock()ed
or unlock()ed need not be visible.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Vijay
  Sent: Wednesday, 12 October 2005 3:07 PM
  To: concurrency-interest@altair.cs.oswego.edu
  Subject: [concurrency-interest] Synchronization automatic with Lock in
JVM5.0?


  I am using Sun JVM 5.0.
  The javadoc for Lock interface says
   "All Lock implementations must enforce the same memory synchronization
semantics as provided by the built-in monitor lock:
    a.. A successful lock operation acts like a successful monitorEnter
action
    b.. A successful unlock operation acts like a successful monitorExit
action "
  So does that mean that acquiring a lock is as good as entering a
synchronization block i.e. will all the buffered variables be discarded?
  Also does that mean that releasing a lock is as good as leaving a
synchronization block i.e. will all the buffered changed variables be
flushed to RAM?

  My apologies if this doubt is too basic.
  Thanks.......Vijay.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051013/b30941e5/attachment-0001.htm
From jmanson at cs.purdue.edu  Wed Oct 12 19:23:28 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 12 19:24:00 2005
Subject: [concurrency-interest] Assignment of references atomic?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEOHGFAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEOHGFAA.dholmes@dltech.com.au>
Message-ID: <434D9AF0.70102@cs.purdue.edu>

The word synchronize is slightly overloaded here, and I think it could 
use some more explanation for those unfamiliar with its use.

One meaning involves any mechanism that can be used to communicate 
between threads: locking, volatiles, atomics, final fields, etc.  These 
things all constitute synchronization actions.

If you want to communicate between threads, you need to use 
synchronization actions, whether they be locking, volatiles, or anything 
else.  This is what Brian was describing.

Another meaning is the language construct used for built-in locking, the 
synchronized block.  A synchronized block implies two synchronization 
actions: a lock action and an unlock action.

If you want to ensure mutual exclusion, you need to use locks, and may 
use a synchronized block to do so.

For this example, it is definitely necessary to use some synchronization 
action.  Depending on the desired semantics, it may or may not be 
necessary to use locking (in the form of a synchronized block or a 
JSR-166 lock).

					Jeremy



David Holmes wrote:
>>Chris Purcell writes:
>>Just to clarify here: you can synchronize by using the "volatile"
>>keyword, right? You do not need to use the "synchronized" keyword?
> 
> 
> You can use the volatile keyword to ensure that things are visible and that
> the right ordering is maintained.
> 
> For example, this is perfectly safe:
> 
>   volatile boolean dataReady = false;
>   Object theObject;
> 
>   Thread-1                             Thread-2..n
>   theObject = makeObject();            if (dataReady)
>   dataReady = true;                        use(theObject);
> 
> 
> Here there is a single writer and many potential readers. As long as the
> readers see dataReady is true then they are guaranteed to see everything
> that happened to theObject during makeObject().
> 
> But volatile in itself is not sufficient *if* there is a need for atomicity
> between deciding to update the object and performing the update.
> 
> David Holmes
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From levmatta at uol.com.br  Thu Oct 13 22:48:14 2005
From: levmatta at uol.com.br (=?ISO-8859-1?Q?Lu=EDs_Matta?=)
Date: Thu Oct 13 22:48:36 2005
Subject: [concurrency-interest] Concurrent Programming in Java
Message-ID: <434F1C6E.1010903@uol.com.br>

Hello all (I'm posting public because the answer might be of general 
interest),
    listening this mailing list has being a wonderfull learning 
experience, and there is something to be sad about exposing ideas in the 
form of a dialog, but I would like to know... 
    If Doug Lea, his staff (if he has), or the people at Sun are going 
to publish a book on the subject. More specifically a update to the 
"Concurrent Programming in Java: Design Principles and Patterns", I have 
gone to the site and I have found a online suplement, but I am more 
interested in a complete update of the book directed at the java 5 (and 
above) and plataform.
     Also a better text explananing of the new memory model, and all the 
resoning behind it. Just a comment: the paper has unswered questions for 
picture labels, very anoying (maybe they are obvious or suposed to make 
you think). Or any other book, for that mather, on a related topic.

Thanks all,
Lu?s Matta

From crazybob at crazybob.org  Fri Oct 14 00:33:40 2005
From: crazybob at crazybob.org (Bob Lee)
Date: Fri Oct 14 00:34:04 2005
Subject: [concurrency-interest] Concurrent Programming in Java
In-Reply-To: <434F1C6E.1010903@uol.com.br>
References: <434F1C6E.1010903@uol.com.br>
Message-ID: <a74683f90510132133p50ad7f97o3e85f4aece8d51ea@mail.gmail.com>

http://www.amazon.com/exec/obidos/tg/detail/-/0321349601/qid=1129264349/sr=8-1/ref=pd_bbs_1/002-7330121-3353651?v=glance&s=books&n=507846

On 10/13/05, Lu?s Matta <levmatta@uol.com.br> wrote:
> Hello all (I'm posting public because the answer might be of general
> interest),
>     listening this mailing list has being a wonderfull learning
> experience, and there is something to be sad about exposing ideas in the
> form of a dialog, but I would like to know...
>     If Doug Lea, his staff (if he has), or the people at Sun are going
> to publish a book on the subject. More specifically a update to the
> "Concurrent Programming in Java: Design Principles and Patterns", I have
> gone to the site and I have found a online suplement, but I am more
> interested in a complete update of the book directed at the java 5 (and
> above) and plataform.
>      Also a better text explananing of the new memory model, and all the
> resoning behind it. Just a comment: the paper has unswered questions for
> picture labels, very anoying (maybe they are obvious or suposed to make
> you think). Or any other book, for that mather, on a related topic.
>
> Thanks all,
> Lu?s Matta
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From ryan.lecompte at pangonetworks.com  Fri Oct 14 09:56:55 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Fri Oct 14 09:57:38 2005
Subject: [concurrency-interest] Mixing volatile and synchronized together...
Message-ID: <PANGOSERVERJLQRZ52E00000b2f@pangonetworks.com>

Hello all,

 

I know that there really isn't a need to do this, but I was wondering if the
following code snippet would be "safe" - ie, no deadlocks.

 

class Test {

   private volatile boolean value = false;

 

    synchronized void someOperation() {

        // do something with "value"

     }

}

 

In other words, there is no harm in using a volatile variable in a
synchronized method or block, right?

 

Thanks,

Ryan

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051014/d77a4a80/attachment.htm
From jmanson at cs.purdue.edu  Fri Oct 14 11:32:35 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Fri Oct 14 11:33:19 2005
Subject: [concurrency-interest] Mixing volatile and synchronized
	together...
In-Reply-To: <PANGOSERVERJLQRZ52E00000b2f@pangonetworks.com>
References: <PANGOSERVERJLQRZ52E00000b2f@pangonetworks.com>
Message-ID: <434FCF93.9000907@cs.purdue.edu>

Ryan LeCompte wrote:
> class Test {
> 
>    private volatile boolean value = false;
> 
>     synchronized void someOperation() {
> 
>         // do something with "value"
> 
>      }
> 
> }
> 
> In other words, there is no harm in using a volatile variable in a
> synchronized method or block, right?
> 
>  

Right.

Volatiles can actually be used anywhere fields with no modifiers can be 
used.  It would actually be semantically harmless to mark every field in 
your program volatile, if you wanted (BAD IDEA).  The program's results 
should be the same, for correctly written code.  The reasons it is a bad 
idea are: it would decrease performance, it would damage readability, 
and it probably wouldn't buy you the guarantees that you think it would 
buy you.

Now, as regards your case: it should be said that if you are using 
locking everywhere the volatile is accessed, then the volatile isn't 
going to provide additional assurances.  For this pattern to be useful, 
there would have to be another thread that accesses value without blocking.

					Jeremy
From tim at peierls.net  Fri Oct 14 11:38:27 2005
From: tim at peierls.net (Tim Peierls)
Date: Fri Oct 14 11:40:12 2005
Subject: [concurrency-interest] Mixing volatile and synchronized
	together...
In-Reply-To: <PANGOSERVERJLQRZ52E00000b2f@pangonetworks.com>
References: <PANGOSERVERJLQRZ52E00000b2f@pangonetworks.com>
Message-ID: <434FD0F3.40603@peierls.net>

Ryan LeCompte wrote:
> I know that there really isn?t a need to do this, but I was wondering if 
> In other words, there is no harm in using a volatile variable in a 
> synchronized method or block, right?

Right, no harm. Depending on the situation, one or the other might be superfluous, but there is 
nothing unsafe about accessing a volatile in a synchronized block or method.

--tim



From ryan.lecompte at pangonetworks.com  Fri Oct 14 12:56:06 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Fri Oct 14 12:56:31 2005
Subject: [concurrency-interest] Mixing volatile and synchronized
	together...
In-Reply-To: <434FD0F3.40603@peierls.net>
Message-ID: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>

Great. I just wanted to be clear on the behavior. So it's okay then to use a
volatile variable in a synchronized method/block as well as an
unsynchronized method/block in the same class?

Thanks,
Ryan

-----Original Message-----
From: Tim Peierls [mailto:tim@peierls.net] 
Sent: Friday, October 14, 2005 11:38 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Mixing volatile and synchronized
together...

Ryan LeCompte wrote:
> I know that there really isn't a need to do this, but I was wondering if 
> In other words, there is no harm in using a volatile variable in a 
> synchronized method or block, right?

Right, no harm. Depending on the situation, one or the other might be
superfluous, but there is 
nothing unsafe about accessing a volatile in a synchronized block or method.

--tim

From tim at peierls.net  Fri Oct 14 14:29:27 2005
From: tim at peierls.net (Tim Peierls)
Date: Fri Oct 14 14:30:47 2005
Subject: [concurrency-interest] Mixing volatile and synchronized
	together...
In-Reply-To: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>
References: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>
Message-ID: <434FF907.4070703@peierls.net>

Ryan LeCompte wrote:
> Great. I just wanted to be clear on the behavior. So it's okay then to use a volatile variable
> in a synchronized method/block as well as an unsynchronized method/block in the same class?

Yes. It's worth reading Jeremy Manson's response carefully:

Jeremy Manson wrote:
> Volatiles can actually be used anywhere fields with no modifiers can be used.  It would
> actually be semantically harmless to mark every field in your program volatile, if you wanted
> (BAD IDEA).  The program's results should be the same, for correctly written code.  The reasons
> it is a bad idea are: it would decrease performance, it would damage readability, and it
> probably wouldn't buy you the guarantees that you think it would buy you.
> 
> Now, as regards your case: it should be said that if you are using locking everywhere the
> volatile is accessed, then the volatile isn't going to provide additional assurances.  For this
> pattern to be useful, there would have to be another thread that accesses value without
> blocking.


--tim

From jhu at glog.com  Fri Oct 14 16:30:27 2005
From: jhu at glog.com (Hu, Jinsong)
Date: Fri Oct 14 16:32:06 2005
Subject: [concurrency-interest] About LinkedHashMap
Message-ID: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEB@PA-GLOGEX01.glog.com>

I know this is an off-topic question to post here, but I did struggling
to find the right way to use this class, and I know there are lot of
experts in this mail list. I would appreciate it very much on this if
you could throw some lights. 

Suppose I have a DataBlock which contains a list of DataElement, and I
have a HashMap which basically acts a hash index to quickly identify the
DataElement according to a given key. Now, if I get the element and I
want to find the next element very quickly, but it seems to me that
current Collection API does not provide a quick solution to this, it
appears that I have to link the DataElement by myself to achieve this.
LinkedHashMap seems a promising solution to this problem, but it does
not provide a public method to get the entry, it only guarantees the
order of insertion or access.

Any thought? 

Thanks
Jinsong




From mattocks at mac.com  Fri Oct 14 21:33:54 2005
From: mattocks at mac.com (Craig Mattocks)
Date: Fri Oct 14 21:34:19 2005
Subject: [concurrency-interest] Concurrent Programming in Java
Message-ID: <2606274b4689ce11871b0a328693e6bf@mac.com>

On Thu, 13 Oct 2005 23:33:40 -0500 Bob Lee <crazybob@crazybob.org>  
wrote:

http://www.amazon.com/exec/obidos/tg/detail/-/0321349601/ 
qid=1129264349/sr=8-1/ref=pd_bbs_1/002-7330121-3353651? 
v=glance&s=books&n=507846

Really not shipping until February 10, 2006?

From dawidk at mathcs.emory.edu  Sat Oct 15 01:50:19 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat Oct 15 01:50:40 2005
Subject: [concurrency-interest] Mixing volatile and synchronizedtogether...
In-Reply-To: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>
References: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>
Message-ID: <4350989B.7060207@mathcs.emory.edu>

Ryan LeCompte wrote:

>Great. I just wanted to be clear on the behavior. So it's okay then to use a
>volatile variable in a synchronized method/block as well as an
>unsynchronized method/block in the same class?
>
>Thanks,
>Ryan
>
>-----Original Message-----
>From: Tim Peierls [mailto:tim@peierls.net] 
>Sent: Friday, October 14, 2005 11:38 AM
>To: Ryan LeCompte
>Cc: concurrency-interest@altair.cs.oswego.edu
>Subject: Re: [concurrency-interest] Mixing volatile and synchronized
>together...
>
>Ryan LeCompte wrote:
>  
>
>>I know that there really isn't a need to do this, but I was wondering if 
>>In other words, there is no harm in using a volatile variable in a 
>>synchronized method or block, right?
>>    
>>
Yes, but keep in mind that a volatile (unsynchronized) reader will be 
able to see any value set in the middle of a synchronized block by any 
writer, so it is only OK if the volatile does not participate in a 
multi-field state invariant, and if there are no intermittent illegal 
values set temporarily to the variable inside synchronized blocks. In 
other words, it must be OK for the reader to see any value that can be 
possibly assigned anytime, including in the middle of the synchronized 
block, by any writer.

Good example: emulating atomic variables in Java 1.4: all writer methods 
are synchronized, but get() is a volatile-read.

Regards,
Dawid

From tim at peierls.net  Sat Oct 15 08:10:29 2005
From: tim at peierls.net (Tim Peierls)
Date: Sat Oct 15 08:11:20 2005
Subject: [concurrency-interest] Concurrent Programming in Java
In-Reply-To: <2606274b4689ce11871b0a328693e6bf@mac.com>
References: <2606274b4689ce11871b0a328693e6bf@mac.com>
Message-ID: <4350F1B5.5010409@peierls.net>

Craig Mattocks wrote:
> On Thu, 13 Oct 2005 23:33:40 -0500 Bob Lee <crazybob@crazybob.org>  wrote:
> 
> http://www.amazon.com/exec/obidos/tg/detail/-/0321349601/qid=1129264349/sr=8-1/ref=pd_bbs_1/002-7330121-3353651?v=glance&s=books&n=507846
> 
> Really not shipping until February 10, 2006?


No idea how Amazon arrived at that date. Our target is December availability.

--tim

From forax at univ-mlv.fr  Sat Oct 15 13:08:54 2005
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sat Oct 15 13:09:31 2005
Subject: [concurrency-interest] About LinkedHashMap
In-Reply-To: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEB@PA-GLOGEX01.glog.com>
References: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEB@PA-GLOGEX01.glog.com>
Message-ID: <435137A6.5050308@univ-mlv.fr>

Hu, Jinsong a ?crit :
> I know this is an off-topic question to post here, but I did struggling
> to find the right way to use this class, and I know there are lot of
> experts in this mail list. I would appreciate it very much on this if
> you could throw some lights. 
> 
> Suppose I have a DataBlock which contains a list of DataElement, and I
> have a HashMap which basically acts a hash index to quickly identify the
> DataElement according to a given key. Now, if I get the element and I
> want to find the next element very quickly, but it seems to me that
> current Collection API does not provide a quick solution to this, it
> appears that I have to link the DataElement by myself to achieve this.
> LinkedHashMap seems a promising solution to this problem, but it does
> not provide a public method to get the entry, it only guarantees the
> order of insertion or access.
> 
> Any thought? 

Mustang introduces a new interface, NavigableMap, that permits
to find the previous or the next entry using
lowerEntry(K key) and higherEntry(K key).
(http://download.java.net/jdk6/doc/api/java/util/NavigableMap.html)

But LinkedHashMap doesn't implements NavigableMap :(
I have personnaly warn doug lea about this.
He said that the JSR166 commitee decide to make
NavigableMap a subclass of SortedMap so
because LinkedHashMap doesn't implements SortedMap, it couldn't
be retrofited to implements NavigableMap without breaking
programs already written.

Workaround : use a TreeMap instead of LinkedHashMap
even if it's the method get() have a log(n) complexity.

> 
> Thanks
> Jinsong
> 

R?mi Forax



From Inanc.Gumus at mccann.com.tr  Mon Oct 17 03:37:43 2005
From: Inanc.Gumus at mccann.com.tr (Inanc Gumus)
Date: Mon Oct 17 03:36:57 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 9,
	Issue 13
Message-ID: <A69632DB1AAD174EBA1A4277AF6AF3DE3BA9C7@meist2k3m1.ist_mccann.com.tr>

Dawid, what is 'multi-field state invariant' ?? 


??? 
??nan?? G??m????
McCann Worldgroup Turkey
Yaz??l??m ARGE, Lotus AdServer Yaz??l??m
 
inanc_gumus@mccann.com.tr
0 212 317 56 78 (direk)

-----Original Message-----
From: concurrency-interest-bounces@cs.oswego.edu [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf Of concurrency-interest-request@cs.oswego.edu
Sent: Saturday, October 15, 2005 7:01 PM
To: concurrency-interest@altair.cs.oswego.edu
Subject: Concurrency-interest Digest, Vol 9, Issue 13

Send Concurrency-interest mailing list submissions to
	concurrency-interest@altair.cs.oswego.edu

To subscribe or unsubscribe via the World Wide Web, visit
	http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
or, via email, send a message with subject or body 'help' to
	concurrency-interest-request@altair.cs.oswego.edu

You can reach the person managing the list at
	concurrency-interest-owner@altair.cs.oswego.edu

When replying, please edit your Subject line so it is more specific than "Re: Contents of Concurrency-interest digest..."


Today's Topics:

   1. RE: Mixing volatile and synchronized	together... (Ryan LeCompte)
   2. Re: Mixing volatile and synchronized	together... (Tim Peierls)
   3. About LinkedHashMap (Hu, Jinsong)
   4. Re: Concurrent Programming in Java (Craig Mattocks)
   5. Re: Mixing volatile and synchronizedtogether... (Dawid Kurzyniec)
   6. Re: Concurrent Programming in Java (Tim Peierls)


----------------------------------------------------------------------

Message: 1
Date: Fri, 14 Oct 2005 12:56:06 -0400
From: "Ryan LeCompte" <ryan.lecompte@pangonetworks.com>
Subject: RE: [concurrency-interest] Mixing volatile and synchronized
	together...
To: "'Tim Peierls'" <tim@peierls.net>
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <PANGOSERVER2A1x1Gv500000b8c@pangonetworks.com>
Content-Type: text/plain;	charset="us-ascii"

Great. I just wanted to be clear on the behavior. So it's okay then to use a volatile variable in a synchronized method/block as well as an unsynchronized method/block in the same class?

Thanks,
Ryan

-----Original Message-----
From: Tim Peierls [mailto:tim@peierls.net]
Sent: Friday, October 14, 2005 11:38 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Mixing volatile and synchronized together...

Ryan LeCompte wrote:
> I know that there really isn't a need to do this, but I was wondering 
> if In other words, there is no harm in using a volatile variable in a 
> synchronized method or block, right?

Right, no harm. Depending on the situation, one or the other might be superfluous, but there is nothing unsafe about accessing a volatile in a synchronized block or method.

--tim



------------------------------

Message: 2
Date: Fri, 14 Oct 2005 14:29:27 -0400
From: Tim Peierls <tim@peierls.net>
Subject: Re: [concurrency-interest] Mixing volatile and synchronized
	together...
To: Ryan LeCompte <ryan.lecompte@pangonetworks.com>
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <434FF907.4070703@peierls.net>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Ryan LeCompte wrote:
> Great. I just wanted to be clear on the behavior. So it's okay then to 
> use a volatile variable in a synchronized method/block as well as an unsynchronized method/block in the same class?

Yes. It's worth reading Jeremy Manson's response carefully:

Jeremy Manson wrote:
> Volatiles can actually be used anywhere fields with no modifiers can 
> be used.  It would actually be semantically harmless to mark every 
> field in your program volatile, if you wanted (BAD IDEA).  The 
> program's results should be the same, for correctly written code.  The 
> reasons it is a bad idea are: it would decrease performance, it would damage readability, and it probably wouldn't buy you the guarantees that you think it would buy you.
> 
> Now, as regards your case: it should be said that if you are using 
> locking everywhere the volatile is accessed, then the volatile isn't 
> going to provide additional assurances.  For this pattern to be 
> useful, there would have to be another thread that accesses value without blocking.


--tim



------------------------------

Message: 3
Date: Fri, 14 Oct 2005 16:30:27 -0400
From: "Hu, Jinsong" <jhu@glog.com>
Subject: [concurrency-interest] About LinkedHashMap
To: <concurrency-interest@altair.cs.oswego.edu>
Message-ID:
	<2E6D4A21A5611B4A8F1AA4E22BE04431877CEB@PA-GLOGEX01.glog.com>
Content-Type: text/plain;	charset="us-ascii"

I know this is an off-topic question to post here, but I did struggling to find the right way to use this class, and I know there are lot of experts in this mail list. I would appreciate it very much on this if you could throw some lights. 

Suppose I have a DataBlock which contains a list of DataElement, and I have a HashMap which basically acts a hash index to quickly identify the DataElement according to a given key. Now, if I get the element and I want to find the next element very quickly, but it seems to me that current Collection API does not provide a quick solution to this, it appears that I have to link the DataElement by myself to achieve this.
LinkedHashMap seems a promising solution to this problem, but it does not provide a public method to get the entry, it only guarantees the order of insertion or access.

Any thought? 

Thanks
Jinsong






------------------------------

Message: 4
Date: Fri, 14 Oct 2005 21:33:54 -0400
From: Craig Mattocks <mattocks@mac.com>
Subject: Re: [concurrency-interest] Concurrent Programming in Java
To: concurrency-interest@altair.cs.oswego.edu
Message-ID: <2606274b4689ce11871b0a328693e6bf@mac.com>
Content-Type: text/plain; charset=US-ASCII; delsp=yes; format=flowed

On Thu, 13 Oct 2005 23:33:40 -0500 Bob Lee <crazybob@crazybob.org>
wrote:

http://www.amazon.com/exec/obidos/tg/detail/-/0321349601/
qid=1129264349/sr=8-1/ref=pd_bbs_1/002-7330121-3353651? 
v=glance&s=books&n=507846

Really not shipping until February 10, 2006?



------------------------------

Message: 5
Date: Sat, 15 Oct 2005 01:50:19 -0400
From: Dawid Kurzyniec <dawidk@mathcs.emory.edu>
Subject: Re: [concurrency-interest] Mixing volatile and
	synchronizedtogether...
To: Ryan LeCompte <ryan.lecompte@pangonetworks.com>
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <4350989B.7060207@mathcs.emory.edu>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Ryan LeCompte wrote:

>Great. I just wanted to be clear on the behavior. So it's okay then to 
>use a volatile variable in a synchronized method/block as well as an 
>unsynchronized method/block in the same class?
>
>Thanks,
>Ryan
>
>-----Original Message-----
>From: Tim Peierls [mailto:tim@peierls.net]
>Sent: Friday, October 14, 2005 11:38 AM
>To: Ryan LeCompte
>Cc: concurrency-interest@altair.cs.oswego.edu
>Subject: Re: [concurrency-interest] Mixing volatile and synchronized 
>together...
>
>Ryan LeCompte wrote:
>  
>
>>I know that there really isn't a need to do this, but I was wondering 
>>if In other words, there is no harm in using a volatile variable in a 
>>synchronized method or block, right?
>>    
>>
Yes, but keep in mind that a volatile (unsynchronized) reader will be able to see any value set in the middle of a synchronized block by any writer, so it is only OK if the volatile does not participate in a multi-field state invariant, and if there are no intermittent illegal values set temporarily to the variable inside synchronized blocks. In other words, it must be OK for the reader to see any value that can be possibly assigned anytime, including in the middle of the synchronized block, by any writer.

Good example: emulating atomic variables in Java 1.4: all writer methods are synchronized, but get() is a volatile-read.

Regards,
Dawid



------------------------------

Message: 6
Date: Sat, 15 Oct 2005 08:10:29 -0400
From: Tim Peierls <tim@peierls.net>
Subject: Re: [concurrency-interest] Concurrent Programming in Java
To: Craig Mattocks <mattocks@mac.com>
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <4350F1B5.5010409@peierls.net>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Craig Mattocks wrote:
> On Thu, 13 Oct 2005 23:33:40 -0500 Bob Lee <crazybob@crazybob.org>  wrote:
> 
> http://www.amazon.com/exec/obidos/tg/detail/-/0321349601/qid=112926434
> 9/sr=8-1/ref=pd_bbs_1/002-7330121-3353651?v=glance&s=books&n=507846
> 
> Really not shipping until February 10, 2006?


No idea how Amazon arrived at that date. Our target is December availability.

--tim



------------------------------

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


End of Concurrency-interest Digest, Vol 9, Issue 13
***************************************************

From jhu at glog.com  Mon Oct 17 10:10:36 2005
From: jhu at glog.com (Hu, Jinsong)
Date: Mon Oct 17 10:12:23 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 9,
	Issue 14
Message-ID: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEE@PA-GLOGEX01.glog.com>

Thanks.

I do care about get(), that's why I want to use hash index. What I am trying to do is using hash index and storing data elements in sorted order to achieve range query. 

All the DataElements of my DataBlock are sorted, the first time the DataBlock is loaded from disk, I will build up hash index for them, if a request for range query coming in, I first use hashing to find the correct element, then calling geNext() one by one to get the range result. I think a navigable LinkedHashMap is the answer.


-----Original Message-----
From: concurrency-interest-bounces@cs.oswego.edu [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf Of concurrency-interest-request@cs.oswego.edu
Sent: Sunday, October 16, 2005 12:01 PM
To: concurrency-interest@altair.cs.oswego.edu
Subject: Concurrency-interest Digest, Vol 9, Issue 14

Send Concurrency-interest mailing list submissions to
	concurrency-interest@altair.cs.oswego.edu

To subscribe or unsubscribe via the World Wide Web, visit
	http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
or, via email, send a message with subject or body 'help' to
	concurrency-interest-request@altair.cs.oswego.edu

You can reach the person managing the list at
	concurrency-interest-owner@altair.cs.oswego.edu

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Concurrency-interest digest..."


Today's Topics:

   1. Re: About LinkedHashMap (R?mi Forax)


----------------------------------------------------------------------

Message: 1
Date: Sat, 15 Oct 2005 19:08:54 +0200
From: R?mi Forax <forax@univ-mlv.fr>
Subject: Re: [concurrency-interest] About LinkedHashMap
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <435137A6.5050308@univ-mlv.fr>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hu, Jinsong a ?crit :
> I know this is an off-topic question to post here, but I did struggling
> to find the right way to use this class, and I know there are lot of
> experts in this mail list. I would appreciate it very much on this if
> you could throw some lights. 
> 
> Suppose I have a DataBlock which contains a list of DataElement, and I
> have a HashMap which basically acts a hash index to quickly identify the
> DataElement according to a given key. Now, if I get the element and I
> want to find the next element very quickly, but it seems to me that
> current Collection API does not provide a quick solution to this, it
> appears that I have to link the DataElement by myself to achieve this.
> LinkedHashMap seems a promising solution to this problem, but it does
> not provide a public method to get the entry, it only guarantees the
> order of insertion or access.
> 
> Any thought? 

Mustang introduces a new interface, NavigableMap, that permits
to find the previous or the next entry using
lowerEntry(K key) and higherEntry(K key).
(http://download.java.net/jdk6/doc/api/java/util/NavigableMap.html)

But LinkedHashMap doesn't implements NavigableMap :(
I have personnaly warn doug lea about this.
He said that the JSR166 commitee decide to make
NavigableMap a subclass of SortedMap so
because LinkedHashMap doesn't implements SortedMap, it couldn't
be retrofited to implements NavigableMap without breaking
programs already written.

Workaround : use a TreeMap instead of LinkedHashMap
even if it's the method get() have a log(n) complexity.

> 
> Thanks
> Jinsong
> 

R?mi Forax





------------------------------

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


End of Concurrency-interest Digest, Vol 9, Issue 14
***************************************************

From jhu at glog.com  Mon Oct 17 10:10:36 2005
From: jhu at glog.com (Hu, Jinsong)
Date: Mon Oct 17 10:12:30 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 9,
	Issue 14
Message-ID: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEE@PA-GLOGEX01.glog.com>

Thanks.

I do care about get(), that's why I want to use hash index. What I am trying to do is using hash index and storing data elements in sorted order to achieve range query. 

All the DataElements of my DataBlock are sorted, the first time the DataBlock is loaded from disk, I will build up hash index for them, if a request for range query coming in, I first use hashing to find the correct element, then calling geNext() one by one to get the range result. I think a navigable LinkedHashMap is the answer.


-----Original Message-----
From: concurrency-interest-bounces@cs.oswego.edu [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf Of concurrency-interest-request@cs.oswego.edu
Sent: Sunday, October 16, 2005 12:01 PM
To: concurrency-interest@altair.cs.oswego.edu
Subject: Concurrency-interest Digest, Vol 9, Issue 14

Send Concurrency-interest mailing list submissions to
	concurrency-interest@altair.cs.oswego.edu

To subscribe or unsubscribe via the World Wide Web, visit
	http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
or, via email, send a message with subject or body 'help' to
	concurrency-interest-request@altair.cs.oswego.edu

You can reach the person managing the list at
	concurrency-interest-owner@altair.cs.oswego.edu

When replying, please edit your Subject line so it is more specific
than "Re: Contents of Concurrency-interest digest..."


Today's Topics:

   1. Re: About LinkedHashMap (R?mi Forax)


----------------------------------------------------------------------

Message: 1
Date: Sat, 15 Oct 2005 19:08:54 +0200
From: R?mi Forax <forax@univ-mlv.fr>
Subject: Re: [concurrency-interest] About LinkedHashMap
Cc: concurrency-interest@altair.cs.oswego.edu
Message-ID: <435137A6.5050308@univ-mlv.fr>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hu, Jinsong a ?crit :
> I know this is an off-topic question to post here, but I did struggling
> to find the right way to use this class, and I know there are lot of
> experts in this mail list. I would appreciate it very much on this if
> you could throw some lights. 
> 
> Suppose I have a DataBlock which contains a list of DataElement, and I
> have a HashMap which basically acts a hash index to quickly identify the
> DataElement according to a given key. Now, if I get the element and I
> want to find the next element very quickly, but it seems to me that
> current Collection API does not provide a quick solution to this, it
> appears that I have to link the DataElement by myself to achieve this.
> LinkedHashMap seems a promising solution to this problem, but it does
> not provide a public method to get the entry, it only guarantees the
> order of insertion or access.
> 
> Any thought? 

Mustang introduces a new interface, NavigableMap, that permits
to find the previous or the next entry using
lowerEntry(K key) and higherEntry(K key).
(http://download.java.net/jdk6/doc/api/java/util/NavigableMap.html)

But LinkedHashMap doesn't implements NavigableMap :(
I have personnaly warn doug lea about this.
He said that the JSR166 commitee decide to make
NavigableMap a subclass of SortedMap so
because LinkedHashMap doesn't implements SortedMap, it couldn't
be retrofited to implements NavigableMap without breaking
programs already written.

Workaround : use a TreeMap instead of LinkedHashMap
even if it's the method get() have a log(n) complexity.

> 
> Thanks
> Jinsong
> 

R?mi Forax





------------------------------

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


End of Concurrency-interest Digest, Vol 9, Issue 14
***************************************************

From dawidk at mathcs.emory.edu  Mon Oct 17 10:19:26 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Oct 17 10:19:56 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 
	9,Issue 13
In-Reply-To: <A69632DB1AAD174EBA1A4277AF6AF3DE3BA9C7@meist2k3m1.ist_mccann.c
	om.tr>
References: <A69632DB1AAD174EBA1A4277AF6AF3DE3BA9C7@meist2k3m1.ist_mccann.co
	m.tr>
Message-ID: <4353B2EE.2090407@mathcs.emory.edu>

Inanc Gumus wrote:

>Dawid, what is 'multi-field state invariant' ?? 
>
>  
>
It means an invariant that involves multiple fields. For instance:

class Range {
   private int min, max;
   setMin(int);
   setMax(int);
}

this class has the invariant that min <= max. Methods setMin and setMax 
must be careful not to violate this invariant. Therefore, just making 
min and max volatile is not sufficient; with some unlucky timing, 
setMin(5) and setMax(4) could then both succeed, thus violating the 
invariant. Therefore, locking is needed. (I took the liberty of 
borrowing the example from the incoming JCP book.)

Regards,
Dawid

From Aaron.Harshbarger at crown.com  Mon Oct 17 13:51:48 2005
From: Aaron.Harshbarger at crown.com (Aaron.Harshbarger@crown.com)
Date: Mon Oct 17 13:52:21 2005
Subject: [concurrency-interest] Aaron Harshbarger is out of the office.
Message-ID: <OFEF2D7923.385E6B5E-ON8525709D.0062206F-8525709D.0062206F@crown.com>


I will be out of the office starting  10/17/2005 and will not return until
10/18/2005.

I will respond to your message when I return.

From nathani.vijay at gmail.com  Tue Oct 18 00:20:56 2005
From: nathani.vijay at gmail.com (Vijay)
Date: Tue Oct 18 00:21:33 2005
Subject: [concurrency-interest] Volatile array and Wait
Message-ID: <001001c5d39b$5a17d4b0$d2c94bca@toronto.redknee.com>

Question 1)
I have an array e.g. 
    volatile long[] a1 = new long[100];

Now, is the object a1 volatile or every element of array a1 is volatile or both?
i.e. Is the value we assign to a1 volatile or every long value in the array volatile or both of the previous options are true?

Question 2)
When we come out of wait state, is it the same thing as entering a synchronized block? Will all the variables be taken from RAM again instead of from the register buffer?

Thanks for you help. This is very helpful mailing list.
Vijay.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051018/da25c5be/attachment.htm
From jmanson at cs.purdue.edu  Tue Oct 18 00:30:04 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Oct 18 00:30:41 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <001001c5d39b$5a17d4b0$d2c94bca@toronto.redknee.com>
References: <001001c5d39b$5a17d4b0$d2c94bca@toronto.redknee.com>
Message-ID: <43547A4C.80909@cs.purdue.edu>

Vijay wrote:
> Question 1) I have an array e.g. volatile long[] a1 = new long[100];
> 
> Now, is the object a1 volatile or every element of array a1 is
> volatile or both?

The reference is volatile; the elements of the array are not.  Writes to
a1 will be volatile; writes to elements of a1 (a1[0], a1[1]...) will not.


> Question 2) When we come out of wait state, is it the same thing as
> entering a synchronized block? Will all the variables be taken from
> RAM again instead of from the register buffer?

When you wake from wait(), you must reacquire the lock before 
proceeding; it therefore has the same semantics as a lock acquisition.

It is a little dangerous to think of these semantics in terms of main 
memory and buffers, for reasons that have been discussed in detail on 
this list.

Hope that helps!

					Jeremy
From Inanc.Gumus at mccann.com.tr  Wed Oct 19 07:15:56 2005
From: Inanc.Gumus at mccann.com.tr (Inanc Gumus)
Date: Wed Oct 19 07:15:02 2005
Subject: [concurrency-interest] Volatile array and Wait
Message-ID: <A69632DB1AAD174EBA1A4277AF6AF3DE3F1463@meist2k3m1.ist_mccann.com.tr>

So, how to make inter array elements volatile?

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: Tuesday, October 18, 2005 7:30 AM
To: Vijay
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Volatile array and Wait

Vijay wrote:
> Question 1) I have an array e.g. volatile long[] a1 = new long[100];
> 
> Now, is the object a1 volatile or every element of array a1 is 
> volatile or both?

The reference is volatile; the elements of the array are not.  Writes to
a1 will be volatile; writes to elements of a1 (a1[0], a1[1]...) will
not.


> Question 2) When we come out of wait state, is it the same thing as 
> entering a synchronized block? Will all the variables be taken from 
> RAM again instead of from the register buffer?

When you wake from wait(), you must reacquire the lock before
proceeding; it therefore has the same semantics as a lock acquisition.

It is a little dangerous to think of these semantics in terms of main
memory and buffers, for reasons that have been discussed in detail on
this list.

Hope that helps!

					Jeremy


From jmanson at cs.purdue.edu  Wed Oct 19 10:39:59 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 10:40:50 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <A69632DB1AAD174EBA1A4277AF6AF3DE3F1463@meist2k3m1.ist_mccann.com.tr>
References: <A69632DB1AAD174EBA1A4277AF6AF3DE3F1463@meist2k3m1.ist_mccann.com.tr>
Message-ID: <43565ABF.9080403@cs.purdue.edu>

Inanc Gumus wrote:
> So, how to make inter array elements volatile?
> 

There is no way to do this directly.  The simplest option is probably to 
use AtomicReferenceArray, AtomicIntegerArray or AtomicLongArray.

Another option is to have an extra volatile dummy field associated with 
the array.  After each write to an array element, write to the dummy 
field (it doesn't matter what value is written).  Before each read of an 
array element, read the dummy field (and discard the value).

A third option is to use indirection.  That is to say, you could create 
an array of references to:

      class VolatileRef {
        public volatile Object ref = null;
     }


A fourth option is to use locking instead.

					Jeremy
From dholmes at dltech.com.au  Wed Oct 19 10:44:08 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 10:44:45 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <A69632DB1AAD174EBA1A4277AF6AF3DE3F1463@meist2k3m1.ist_mccann.com.tr>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGNGGAA.dholmes@dltech.com.au>

Inanc  Gumus writes:
>
> So, how to make inter array elements volatile?

There is no way to make plain array elements volatile (or final).

As an alternative, for some cases, you can use the array classes from
java.util.concurrent.atomic, eg. AtomicIntegerArray.

David Holmes

From forax at univ-mlv.fr  Wed Oct 19 10:53:43 2005
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed Oct 19 10:54:13 2005
Subject: [concurrency-interest] ThreadPoolExecutor.shutdown() and permission
Message-ID: <43565DF7.106@univ-mlv.fr>

In ThreadPoolExecutor.shutdown(), if i have correctly understand the
documentation, the code first checks if permission "modifyThread" is granted
and then for each worker threads checks checkAccess.

Why in order to check permission "modifyThread",
you use AccessController.checkPermission() and not
securityManager.checkPermission() ?

This pattern doesn't seems to match with the security
architecture describes here :
http://java.sun.com/j2se/1.5.0/docs/guide/security/spec/security-specTOC.fm.html

R?mi Forax




From dholmes at dltech.com.au  Wed Oct 19 11:35:46 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 11:36:15 2005
Subject: [concurrency-interest] ThreadPoolExecutor.shutdown() and
	permission
In-Reply-To: <43565DF7.106@univ-mlv.fr>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEGPGGAA.dholmes@dltech.com.au>

Remi,

> In ThreadPoolExecutor.shutdown(), if i have correctly understand the
> documentation, the code first checks if permission "modifyThread"
> is granted and then for each worker threads checks checkAccess.
>
> Why in order to check permission "modifyThread",
> you use AccessController.checkPermission() and not
> securityManager.checkPermission() ?

Only AccessController.checkPermission guarantees that you actually check if
you have the permission. This class can't be modified by the application.

The SecurityManager.checkAccess could do anything it wants even ignoring the
installed security policy.

So to perform shutdown() you have to have the global modifyThread
permission, and for each worker thread the SecurityManager's checkAccess
must succeed.

David Holmes

From forax at univ-mlv.fr  Wed Oct 19 12:00:49 2005
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed Oct 19 12:01:16 2005
Subject: [concurrency-interest] ThreadPoolExecutor.shutdown() and
	permission
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEGPGGAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEGPGGAA.dholmes@dltech.com.au>
Message-ID: <43566DB1.50008@univ-mlv.fr>

David Holmes wrote:

>Remi,
>
>  
>
>>In ThreadPoolExecutor.shutdown(), if i have correctly understand the
>>documentation, the code first checks if permission "modifyThread"
>>is granted and then for each worker threads checks checkAccess.
>>
>>Why in order to check permission "modifyThread",
>>you use AccessController.checkPermission() and not
>>securityManager.checkPermission() ?
>>    
>>
>
>Only AccessController.checkPermission guarantees that you actually check if
>you have the permission. This class can't be modified by the application.
>
>The SecurityManager.checkAccess could do anything it wants even ignoring the
>installed security policy.
>  
>
Yes i agree with you,  but if the security manager wants to ignore the 
policy
it's not the responsibility of shutdown() to care about such detail.

By doing this, shutdown() you break the general security architecture of 
Java.
Perhaps for you, this code is more secure but if additionnal security 
checks are
implemented by the security manager, these tests are bypassed.
So this code can be considered as less secured.

>So to perform shutdown() you have to have the global modifyThread
>permission, and for each worker thread the SecurityManager's checkAccess
>must succeed.
>  
>
>David Holmes
>  
>
R?mi Forax


From Darron_Shaffer at stercomm.com  Wed Oct 19 12:07:41 2005
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Wed Oct 19 12:08:36 2005
Subject: [concurrency-interest] Volatile array and Wait
Message-ID: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>

The second option can use the volatile reference to the array as the
"dummy".

So given:

  volatile Object[] b = new Object[50];

reading looks like this:

   Object x = b[25];  // Volatile read must occur first, to find the
array.

writing looks like this:

   b[22] = x; Object dummy = b;  // Must have trailing volatile read.

If you do this, be sure and comment it well enough that a
junior/maintenance programmer doesn't decide that volatile applies to
the entire array.
 

-----Original Message-----
From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf Of Jeremy
Manson
Sent: Wednesday, October 19, 2005 9:40 AM
To: Inanc Gumus
Cc: Vijay; concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Volatile array and Wait

Inanc Gumus wrote:
> So, how to make inter array elements volatile?
> 

There is no way to do this directly.  The simplest option is probably to

use AtomicReferenceArray, AtomicIntegerArray or AtomicLongArray.

Another option is to have an extra volatile dummy field associated with 
the array.  After each write to an array element, write to the dummy 
field (it doesn't matter what value is written).  Before each read of an

array element, read the dummy field (and discard the value).

A third option is to use indirection.  That is to say, you could create 
an array of references to:

      class VolatileRef {
        public volatile Object ref = null;
     }


A fourth option is to use locking instead.

					Jeremy
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From dholmes at dltech.com.au  Wed Oct 19 12:28:18 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 12:28:43 2005
Subject: [concurrency-interest] ThreadPoolExecutor.shutdown() andpermission
In-Reply-To: <43566DB1.50008@univ-mlv.fr>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEHBGGAA.dholmes@dltech.com.au>

> Yes i agree with you,  but if the security manager wants to ignore the
> policy it's not the responsibility of shutdown() to care about such
detail.
>
> By doing this, shutdown() you break the general security architecture of
> Java.

I disagree. The SecurityManager is a remnant of the old security
architecture. The 1.2 architecture says that security is determined by the
installed security policy. If shutdown should be allowed then the correct
permission should be installed. The additional check of the SecurityManager
is to allow for a more restrictive security policy not a less restrictive
one.

That said it does seem that we enforce more security than other core classes
do, even Thread. I thought that Thread used both SecurityManager.checkAccess
and SecurityManager.checkPermission in certain cases, so that the need for
the permission could block access even if checkAccess allowed it. I presumed
that SecurityManager.checkPermission would be final but it is not - which
seems like an oversight to me (otherwise why call checkAccess and
checkPermission?)

The strategy used on shutdown() was approved by the JDK security folk.

Cheers,
David Holmes

From jmanson at cs.purdue.edu  Wed Oct 19 12:32:42 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 12:33:26 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
Message-ID: <4356752A.9090706@cs.purdue.edu>

Shaffer, Darron wrote:
> The second option can use the volatile reference to the array as the
> "dummy".
> 
> So given:
> 
>   volatile Object[] b = new Object[50];
> 
> reading looks like this:
> 
>    Object x = b[25];  // Volatile read must occur first, to find the
> array.
> 
> writing looks like this:
> 
>    b[22] = x; Object dummy = b;  // Must have trailing volatile read.
> 
> If you do this, be sure and comment it well enough that a
> junior/maintenance programmer doesn't decide that volatile applies to
> the entire array.


This won't work.  We should remind ourselves that it is important to be 
very careful when trying to use volatile for patching up our code.  It 
is really difficult to remember all of the rules, and to reason about 
them correctly.

In this case, you have to think carefully about the "acquire" and 
"release" semantics of volatiles.  A read performs an "acquire", and a 
write performs a "release".  When you are writing to memory, you have to 
perform a release.  So, in this case:

>> writing looks like this:
>> 
>>    b[22] = x; Object dummy = b;  // Must have trailing volatile read.

you need a write to a volatile field, not a read of one.  It should look 
like this:

Write:
b[22] = x;
b = b;

Or, slightly better, keep the value of the field called b in a local 
variable:

Write:
int [] arr = b;
arr[22] = x;
b = arr;

IMPORTANT NOTE:

In general, you have to be very careful about using this.  I would 
suspect that in nine cases out of ten, it will not do what you want it 
to do.  For example, if you were trying to use this for a singleton, it 
would NOT be correct:

Bad Writer Thread:
b[22] = new SingletonObject();
b = b;

Bad Reader Thread:
SingletonObject so = b[22];
so.doThings();

You could easily imagine the reader thread being scheduled between the 
two statements of the writer thread.  Guess what?  In that case, you 
don't see the effects of the volatile write.

					Jeremy
From dholmes at dltech.com.au  Wed Oct 19 12:35:44 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 12:36:10 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEHBGGAA.dholmes@dltech.com.au>

> Shaffer, Darron writes:
> The second option can use the volatile reference to the array as the
> "dummy".
>
> So given:
>
>   volatile Object[] b = new Object[50];
>
> reading looks like this:
>
>    Object x = b[25];  // Volatile read must occur first, to find the
> array.
>
> writing looks like this:
>
>    b[22] = x; Object dummy = b;  // Must have trailing volatile read.

No you need a volatile *write* to happen-before the volatile read. So you
would need to do:

     b[22] = x;  // write normal data
     b = b;      // volatile write

but this is not sufficient to guarantee that an element read sees the most
recent value, as the read could occur after the write to the element but
before the write to the array reference. Of course the program can't
distinguish this case from the case where the read of the element happens
before the write of the element.

David Holmes

From jmanson at cs.purdue.edu  Wed Oct 19 12:46:49 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 12:47:22 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <bd5bee60fced5308d9c336aab4861624@gmail.com>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
Message-ID: <43567879.9040503@cs.purdue.edu>

Chris Purcell wrote:

>> Write:
>> b[22] = x;
>> b = b;
> 
> 
> Umm, shouldn't that be
> 
> b = b;
> b[22] = x;
> 
> ?
> 
> After all, there's not use in publishing preceding writes *after* the 
> lock's already released.
> 

Nope.  The rule of thumb is that the release has to happen after you 
perform the writes you want to publish.  The acquire has to happen 
before you read the writes that have been published.

In this case, the write "b = b" is the release, and so must occur after 
the write "b[22] = x", which is the write we wish to publish.

Also, bear in mind that there is no lock in this case.  This is part of 
what makes it extremely difficult to analyze, and why people should 
probably avoid it where possible.

					Jeremy
From chris.purcell.39 at gmail.com  Wed Oct 19 12:48:37 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 12:49:06 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43567879.9040503@cs.purdue.edu>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
Message-ID: <7b56119dd688462b490e3fad0bf82752@gmail.com>

> In this case, the write "b = b" is the release, and so must occur 
> after the write "b[22] = x", which is the write we wish to publish.

Actually, b[22] = x is supposed to have release semantics, so every 
preceding write must be made visible before it commits.

Chris

From chris.purcell.39 at gmail.com  Wed Oct 19 12:51:37 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 12:52:05 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43567879.9040503@cs.purdue.edu>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
Message-ID: <f9ed15a24609b4b1b97e7ae4e8169446@gmail.com>

Actually, that's probably my mistake. Apologies. Excuse me while I go 
absorb the Java Memory Model again...

Chris

From jmanson at cs.purdue.edu  Wed Oct 19 12:53:01 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 12:53:34 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <7b56119dd688462b490e3fad0bf82752@gmail.com>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
	<7b56119dd688462b490e3fad0bf82752@gmail.com>
Message-ID: <435679ED.30601@cs.purdue.edu>

Chris Purcell wrote:
>> In this case, the write "b = b" is the release, and so must occur 
>> after the write "b[22] = x", which is the write we wish to publish.
> 
> 
> Actually, b[22] = x is supposed to have release semantics, so every 
> preceding write must be made visible before it commits.
> 
> Chris


This is exactly my point.  b[22] = x *doesn't* have release semantics. 
It looks like this:

int [] a = b; // volatile read
a[22] = x; // array write

There is no release in there.  That's why you need the write "b = b", 
which looks like this:

int [] a = b; // volatile read
b = a;  // volatile write

It is the volatile write that matters.

Like I said, it is hard to analyze this stuff.

					Jeremy
From chris.purcell.39 at gmail.com  Wed Oct 19 13:34:33 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 13:35:04 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <435679ED.30601@cs.purdue.edu>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
	<7b56119dd688462b490e3fad0bf82752@gmail.com>
	<435679ED.30601@cs.purdue.edu>
Message-ID: <48ee2c0203c746c8c1687293dded12a5@gmail.com>

Okay, I've thought this out a bit. My analysis follows.

1) A volatile write should have the same semantics as a monitor release
2) A volatile read should have the same semantics as a monitor acquire

A monitor release ensures all preceding writes are visible before it 
commits. A monitor acquire prevents subsequent read operations from 
passing across it. These give the appearance of sequential execution of 
critical sections.

We wish to attach release semantics to our write to b[22]. Any 
preceding writes must be visible to any reader who subsequently 
performs an acquire.

If b[22] is *followed* by a volatile write/monitor release, there is a 
period of time in which concurrent updates may see the new value of 
b[22] but not any preceding writes despite performing an acquire in 
between: namely, the time between the write to b[22] committing and the 
release barrier.

Unfortunately, if the write to b[22] is *preceded* by a volatile 
write/monitor release, the write to b[22] can be moved before the 
release barrier (either by compiler or hardware) since it is not itself 
volatile; once again there is a period of time in which concurrent 
updates may see the new value of b[22] then do an acquire, yet still 
not see any preceding writes, namely the time between the write to 
b[22] committing and the release barrier.

However, if one could precede the write to b[22] with a release barrier 
then a subsequent volatile read of a value on which the write operation 
is dependent, then the write can no longer be reordered before the 
barrier. The read of b should be sufficient for this.

Hence the correct code is:

b = b; // Release barrier
b[22] = x; // Subsequent acquire barrier followed by dependent write

as I suggested.

Attaching a monitor acquire to the read of b[22] is more problematic. 
The acquire barrier must occur *after* the read of b[22] to prevent 
subsequent reads from being reordered incorrectly. To do this, one must 
make the acquire barrier *dependent* on the read of b[22], or risk them 
being reordered. In assembler, this would be trivial, as one could fake 
a volatile read that was dependent on the value of b[22]. I'm not clear 
how to do this reliably in Java, as fake dependencies risk being 
optimised away. The following might work, but might not:

x = b[22];
if (x < 1000)
    temp = b;
else
    temp = some_other_volatile;

In conclusion, don't try and emulate volatility.

Cheers,
Chris

From jmanson at cs.purdue.edu  Wed Oct 19 13:39:23 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 13:39:57 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <48ee2c0203c746c8c1687293dded12a5@gmail.com>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
	<7b56119dd688462b490e3fad0bf82752@gmail.com>
	<435679ED.30601@cs.purdue.edu>
	<48ee2c0203c746c8c1687293dded12a5@gmail.com>
Message-ID: <435684CB.1050002@cs.purdue.edu>

Chris Purcell wrote:
> Okay, I've thought this out a bit. My analysis follows.

Hmm...  What we have here is a situation where I pointed out the problem 
you bring up, and then you suggested a fix, and I completely missed that 
you were pointing out a fix.

My mistake.  Sorry.

					Jeremy
From chris.purcell.39 at gmail.com  Wed Oct 19 13:41:43 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 13:42:12 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43567879.9040503@cs.purdue.edu>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
Message-ID: <86e37e0563019a2682e6eeb8a853be49@gmail.com>

> Also, bear in mind that there is no lock in this case.  This is part 
> of what makes it extremely difficult to analyze, and why people should 
> probably avoid it where possible.

Amen to that ;)

Chris

From chris.purcell.39 at gmail.com  Wed Oct 19 13:50:32 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 13:51:01 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <48ee2c0203c746c8c1687293dded12a5@gmail.com>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>
	<4356752A.9090706@cs.purdue.edu>
	<bd5bee60fced5308d9c336aab4861624@gmail.com>
	<43567879.9040503@cs.purdue.edu>
	<7b56119dd688462b490e3fad0bf82752@gmail.com>
	<435679ED.30601@cs.purdue.edu>
	<48ee2c0203c746c8c1687293dded12a5@gmail.com>
Message-ID: <647dfd50680cd65239d5015fc799363a@gmail.com>

> The following might work, but might not:
>
> x = b[22];
> if (x < 1000)
>    temp = b;
> else
>    temp = some_other_volatile;

Hmmm. Looking at this again, it's pretty unlikely that it will *ever* 
work, as it's very easy to reorder reads across ifs. We want some way 
of making the address of the volatile we are reading from unpredictably 
determined by the value we read from b[22]. That means pointer 
arithmetic. Unfortunately, the only way of doing that (that I know of) 
in Java would look like:

     x = b[22]
     temp = a_volatile_array[x % 3]

which of course needs ... an array of volatile elements! Nice catch-22.

Cheers,
Chris

From jmanson at cs.purdue.edu  Wed Oct 19 14:07:58 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 14:08:31 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <435684CB.1050002@cs.purdue.edu>
References: <303629700276DF4D9ED7D011221B8FAA033DA11D@scidubmsg03.sci.local>	<4356752A.9090706@cs.purdue.edu>	<bd5bee60fced5308d9c336aab4861624@gmail.com>	<43567879.9040503@cs.purdue.edu>	<7b56119dd688462b490e3fad0bf82752@gmail.com>	<435679ED.30601@cs.purdue.edu>	<48ee2c0203c746c8c1687293dded12a5@gmail.com>
	<435684CB.1050002@cs.purdue.edu>
Message-ID: <43568B7E.3070409@cs.purdue.edu>

Jeremy Manson wrote:
> Chris Purcell wrote:
> 
>> Okay, I've thought this out a bit. My analysis follows.
> 
> 
> Hmm...  What we have here is a situation where I pointed out the problem 
> you bring up, and then you suggested a fix, and I completely missed that 
> you were pointing out a fix.
> 
> My mistake.  Sorry.
> 

There is a little confusion about this.  I didn't realize that the 
problem that Chris was fixing was this one I brought up:

 > In general, you have to be very careful about using this.  I would
 > suspect that in nine cases out of ten, it will not do what you want
 > it to do.  For example, if you were trying to use this for a
 > singleton, it would NOT be correct:
 >
 > Bad Writer Thread:
 > b[22] = new SingletonObject();
 > b = b;
 >
 > Bad Reader Thread:
 > SingletonObject so = b[22];
 > so.doThings();
 >
 > You could easily imagine the reader thread being scheduled between
 > the two statements of the writer thread.  Guess what?  In that case,
 > you don't see the effects of the volatile write.

Chris's point was that by putting the b = b statement *before* the write 
to b[22], thus:

1: SingletonObject so = new SingletonObject();
2: b = b;
3: b[22] = so;

then if the reader thread sees the SingletonObject reference, it is 
guaranteed to see the fully constructed SingletonObject.

Of course, if we were to use this code, it would mean that no one is 
actually guaranteed to see the write of the SingletonObject reference to 
b[22].  This was my point.  If you want to guarantee *that*, you have to 
do it this way:

1: SingletonObject so = new SingletonObject();
2: b = b;
3: b[22] = so;
4: b = b;

The volatile write on line 2 guarantees that *if* anyone sees the 
SingletonObject reference, they will also see the correctly constructed 
SingletonObject.  The volatile write on line 4 changes that *if* to a 
*when*: it guarantees that other threads will actually see the reference.

Sorry about the crossed signals.  I hope that clears matters up, and is 
a clear illustration to people why this stuff is hard.

					Jeremy
From dholmes at dltech.com.au  Wed Oct 19 14:12:23 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 14:13:09 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <48ee2c0203c746c8c1687293dded12a5@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHGGGAA.dholmes@dltech.com.au>

Chris Purcell writes:
> We wish to attach release semantics to our write to b[22]. Any
> preceding writes must be visible to any reader who subsequently
> performs an acquire.

What we want to do is try to ensure that the write of b[22] is visible to a
subsequent read of b[22] - which would happen if the array element itself
were considered volatile. We're not (or the original context wasn't) using
b[22] as a flag to indicate the "availability" of other data - it is b[22]'s
value that we are interested in.

> If b[22] is *followed* by a volatile write/monitor release, there is a
> period of time in which concurrent updates may see the new value of
> b[22] but not any preceding writes despite performing an acquire in
> between: namely, the time between the write to b[22] committing and the
> release barrier.

Correct. You might see the updated value of b[22] even if you don't see
other writes that occurred prior to writing to b[22].

You can also read b[22] after the last write to it, but before the volatile
write makes it visible. So in short there is no way to guarantee a read of
b[22] will see the latest value because there will always be a window
between the write and the "release" in which a read can sneak in.

You can't use volatiles to guarantee an update won't be seen, only to
guarantee that it will be. So if you want to ensure that a group of updates
are seen either entirely or not at all then you need to use locks to prevent
the values from being inspected.

> Unfortunately, if the write to b[22] is *preceded* by a volatile
> write/monitor release, the write to b[22] can be moved before the
> release barrier (either by compiler or hardware) since it is not itself
> volatile; once again there is a period of time in which concurrent
> updates may see the new value of b[22] then do an acquire, yet still
> not see any preceding writes, namely the time between the write to
> b[22] committing and the release barrier.

Preceding the write to b[22] with a volatile write doesn't achieve anything
with respect to the value of b[22]. You need to think about happens-before
rather than thinking about reordering.

> However, if one could precede the write to b[22] with a release barrier
> then a subsequent volatile read of a value on which the write operation
> is dependent, then the write can no longer be reordered before the
> barrier. The read of b should be sufficient for this.
>
> Hence the correct code is:
>
> b = b; // Release barrier
> b[22] = x; // Subsequent acquire barrier followed by dependent write
>
> as I suggested.

"correct" depends on what you think this is achieving. The read of b will
prevent the reordering of the write to b[22] with the write to b. So any
writes preceding the write to b will become visible. *BUT* the write to
b[22] itself is not guaranteed visible - which is what was wanted: the
effect of having volatile array elements.

Of course another point to take away from this is that an array of volatile
elements isn't that useful anyway.

David Holmes

From dholmes at dltech.com.au  Wed Oct 19 14:15:59 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 14:16:35 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43568B7E.3070409@cs.purdue.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHHGGAA.dholmes@dltech.com.au>

Ah I see where the signals are getting crossed. Thanks for clarifying that
Jeremy.

Sorry for the confusion Chris.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu
> [mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Jeremy
> Manson
> Sent: Thursday, 20 October 2005 4:08 AM
> To: Jeremy Manson
> Cc: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile array and Wait
>
>
> Jeremy Manson wrote:
> > Chris Purcell wrote:
> >
> >> Okay, I've thought this out a bit. My analysis follows.
> >
> >
> > Hmm...  What we have here is a situation where I pointed out
> the problem
> > you bring up, and then you suggested a fix, and I completely
> missed that
> > you were pointing out a fix.
> >
> > My mistake.  Sorry.
> >
>
> There is a little confusion about this.  I didn't realize that the
> problem that Chris was fixing was this one I brought up:
>
>  > In general, you have to be very careful about using this.  I would
>  > suspect that in nine cases out of ten, it will not do what you want
>  > it to do.  For example, if you were trying to use this for a
>  > singleton, it would NOT be correct:
>  >
>  > Bad Writer Thread:
>  > b[22] = new SingletonObject();
>  > b = b;
>  >
>  > Bad Reader Thread:
>  > SingletonObject so = b[22];
>  > so.doThings();
>  >
>  > You could easily imagine the reader thread being scheduled between
>  > the two statements of the writer thread.  Guess what?  In that case,
>  > you don't see the effects of the volatile write.
>
> Chris's point was that by putting the b = b statement *before* the write
> to b[22], thus:
>
> 1: SingletonObject so = new SingletonObject();
> 2: b = b;
> 3: b[22] = so;
>
> then if the reader thread sees the SingletonObject reference, it is
> guaranteed to see the fully constructed SingletonObject.
>
> Of course, if we were to use this code, it would mean that no one is
> actually guaranteed to see the write of the SingletonObject reference to
> b[22].  This was my point.  If you want to guarantee *that*, you have to
> do it this way:
>
> 1: SingletonObject so = new SingletonObject();
> 2: b = b;
> 3: b[22] = so;
> 4: b = b;
>
> The volatile write on line 2 guarantees that *if* anyone sees the
> SingletonObject reference, they will also see the correctly constructed
> SingletonObject.  The volatile write on line 4 changes that *if* to a
> *when*: it guarantees that other threads will actually see the reference.
>
> Sorry about the crossed signals.  I hope that clears matters up, and is
> a clear illustration to people why this stuff is hard.
>
> 					Jeremy
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From chris.purcell.39 at gmail.com  Wed Oct 19 14:37:46 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 19 14:38:10 2005
Subject: [concurrency-interest] Problems understanding "visibility"
Message-ID: <a294f4c0f336cf7889d782dfcdbde747@gmail.com>

I'm having problems understanding this word "visibility" that keeps 
getting used. Is it possible to create a concurrent program that can 
actually tell the difference between acquire/release having only 
ordering constraints, and acquire/release having ordering + visibility 
constraints? Moreover, what concurrent design idioms require visibility 
for correctness?

Cheers,
Chris

From dholmes at dltech.com.au  Wed Oct 19 14:52:59 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 19 14:53:27 2005
Subject: [concurrency-interest] Problems understanding "visibility"
In-Reply-To: <a294f4c0f336cf7889d782dfcdbde747@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHIGGAA.dholmes@dltech.com.au>

Chris,

> I'm having problems understanding this word "visibility" that keeps
> getting used.

Unless a write in one thread happens-before a read in another thread, then
the write need never be seen by that other thread. This is the conceptual
model. For simplicity assume a variable is kept in a thread-local-location
and unless two actions in two different threads are ordered by
happens-before, then writes to the variable only affect the thread's local
version of the variable.

Of course real hardware is unlikely to do this (in fact some cache
architectures pretty much guarantee that all writes go to main memory within
a bounded time). But the compiler could use a register to hold the local
value of a variable.

> Is it possible to create a concurrent program that can
> actually tell the difference between acquire/release having only
> ordering constraints, and acquire/release having ordering + visibility
> constraints?

I don't think it makes sense to talk about ordering without visibility as it
is the order in which writes are seen that is at issue.

> Moreover, what concurrent design idioms require visibility
> for correctness?

Pretty much all of them.

Cheers,
David Holmes

From jmanson at cs.purdue.edu  Wed Oct 19 15:10:21 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 15:10:56 2005
Subject: [concurrency-interest] Problems understanding "visibility"
In-Reply-To: <a294f4c0f336cf7889d782dfcdbde747@gmail.com>
References: <a294f4c0f336cf7889d782dfcdbde747@gmail.com>
Message-ID: <43569A1D.9070700@cs.purdue.edu>

Chris Purcell wrote:
> I'm having problems understanding this word "visibility" that keeps 
> getting used. Is it possible to create a concurrent program that can 
> actually tell the difference between acquire/release having only 
> ordering constraints, and acquire/release having ordering + visibility 
> constraints? Moreover, what concurrent design idioms require visibility 
> for correctness?
> 

When you need visibility, you almost invariably need ordering, so 
release and acquire provide both.

The distinction mostly exists so we can describe separate symptoms of 
broken multithreaded code.  Here's an example of code with a data race 
exhibiting strange behavior with visibility, but without ordering:

Initially, p = q, p.x = 0;

Thread 1:
r1 = p.x;
r2 = q.x;
r3 = p.x;

Thread 2:
p.x = 1;

Let's pretend there was some way of getting visibility without ordering. 
  Somehow, we know that the write by Thread 2 will be seen by Thread 1. 
  But we aren't guaranteeing ordering.  It would be possible for the 
compiler to do the following:

Replacement Thread 1:
r1 = p.x;
r2 = q.x;
r3 = r1;

Let's say the write to p.x was scheduled between the read of p.x and the 
read of q.x by Thread 1.  In this case, you would get the result r1 = r3 
  0, r2 = 1.  In other words, the write was visible, but the results 
were seen out of order.

You could equally construct situations where you get ordering without 
visibility, but I think this illustrates the point.

					Jeremy
From jmanson at cs.purdue.edu  Wed Oct 19 15:33:14 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Wed Oct 19 15:33:49 2005
Subject: [concurrency-interest] Problems understanding "visibility"
In-Reply-To: <43569A1D.9070700@cs.purdue.edu>
References: <a294f4c0f336cf7889d782dfcdbde747@gmail.com>
	<43569A1D.9070700@cs.purdue.edu>
Message-ID: <43569F7A.9060707@cs.purdue.edu>

Jeremy Manson wrote:

> You could equally construct situations where you get ordering without 
> visibility, but I think this illustrates the point.

I'll elaborate.  Here's an example where you can get ordering without 
visibility:

boolean done = false;

Thread 1:
while (!done) {
  // do  stuff
}

Thread 2:
done = true;

Let's say a compiler sees that Thread 1 never changes the value of done, 
and so changes Thread 1 to:

Thread 1:
while (true) {
   // do stuff
}

In the previous example, the order of operations had changed: the third 
write was effectively moved to the beginning of Thread 1.

In this example, the order of operations has not changed; the write by 
Thread 2 is just not visible to Thread 1.

Obviously, ordering and visibility are related, which is why acquires 
and releases provide both.  I've said myself that there is a thin line 
between the two.  The distinction can be useful when discussing these 
issues, though.

					Jeremy
From hans.boehm at hp.com  Wed Oct 19 13:59:09 2005
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu Oct 20 09:42:47 2005
Subject: [concurrency-interest] Volatile array and Wait
Message-ID: <65953E8166311641A685BDF71D8658266C19AD@cacexc12.americas.cpqcorp.net>

I agree with Chris' later posting.  And interestingly, that works only
because Java totally orders synchronization operations (at the cost of
making volatile writes relatively expensive).

Another way to look at this is that for any synchronization free
statements X and Y:

X; dummy = v; Y;

can be transformed to

dummy = v; X; Y;

i.e. the compiler can safely move the volatile load to the beginning of
the synchronization-free block (or equivalently move the preceding
synchronization-free operations past the acquire operation).

In this particular case, this means that that dummy = b can effectively
moved to just after the read of b in the computation of b[22], and is
thus effectively a no-op.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu 
> [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
> Of Shaffer, Darron
> Sent: Wednesday, October 19, 2005 9:08 AM
> To: Jeremy Manson; Inanc Gumus
> Cc: Vijay; concurrency-interest@altair.cs.oswego.edu
> Subject: RE: [concurrency-interest] Volatile array and Wait
> 
> 
> The second option can use the volatile reference to the array 
> as the "dummy".
> 
> So given:
> 
>   volatile Object[] b = new Object[50];
> 
> reading looks like this:
> 
>    Object x = b[25];  // Volatile read must occur first, to 
> find the array.
> 
> writing looks like this:
> 
>    b[22] = x; Object dummy = b;  // Must have trailing volatile read.
> 
> If you do this, be sure and comment it well enough that a 
> junior/maintenance programmer doesn't decide that volatile 
> applies to the entire array.
>  
> 
> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu
> [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
> Of Jeremy Manson
> Sent: Wednesday, October 19, 2005 9:40 AM
> To: Inanc Gumus
> Cc: Vijay; concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile array and Wait
> 
> Inanc Gumus wrote:
> > So, how to make inter array elements volatile?
> > 
> 
> There is no way to do this directly.  The simplest option is 
> probably to
> 
> use AtomicReferenceArray, AtomicIntegerArray or AtomicLongArray.
> 
> Another option is to have an extra volatile dummy field 
> associated with 
> the array.  After each write to an array element, write to the dummy 
> field (it doesn't matter what value is written).  Before each 
> read of an
> 
> array element, read the dummy field (and discard the value).
> 
> A third option is to use indirection.  That is to say, you 
> could create 
> an array of references to:
> 
>       class VolatileRef {
>         public volatile Object ref = null;
>      }
> 
> 
> A fourth option is to use locking instead.
> 
> 					Jeremy
> _______________________________________________
> Concurrency-interest mailing list 
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list 
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From mike.skells at ebizz-consulting.com  Thu Oct 20 10:22:44 2005
From: mike.skells at ebizz-consulting.com (Mike Skells)
Date: Thu Oct 20 10:23:10 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <65953E8166311641A685BDF71D8658266C19AD@cacexc12.americas.cpqcorp.net>
Message-ID: <01d101c5d581$bd122e70$0a01a8c0@MikeLaptop>

On the consideration of a no op ...



  volatile Object[] b = new Object[50];
...
  b = b;      // volatile write



Isnt the compiler/jit liable to remove the b=b statement, through the normal
rules of elimination of redundent code, or is this explicitely barred for
volatile variables

Similarly if a variable is never read, or never written, and is private then
it can be eliminated cant it


Mike



From dholmes at dltech.com.au  Thu Oct 20 10:58:35 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Oct 20 10:59:04 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <01d101c5d581$bd122e70$0a01a8c0@MikeLaptop>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIMGGAA.dholmes@dltech.com.au>

Mike Skells writes:
> On the consideration of a no op ...
>
>   volatile Object[] b = new Object[50];
> ...
>   b = b;      // volatile write
>
> Isnt the compiler/jit liable to remove the b=b statement, through
> the normal rules of elimination of redundent code, or is this
> explicitely barred for volatile variables

It can't remove the memory synchronization actions, even if it could
eliminate the "code".

> Similarly if a variable is never read, or never written, and is
> private then it can be eliminated cant it

Depends how absolute "never" is :) There are various ahead-of-time building
tools that purge unused fields and code from class files using whole-program
analysis - and presume no reflective (or jni) access :). Not sure a jit
would ever get involved with this though.

David Holmes

From tech-lists at mothy.net  Thu Oct 20 12:36:57 2005
From: tech-lists at mothy.net (Tim McAuley)
Date: Thu Oct 20 12:37:26 2005
Subject: [concurrency-interest] Java 1.3 and JSR166
In-Reply-To: <29905.213.94.205.109.1129823993.squirrel@webmail.valina.lunarpages.co
	m>
References: <29905.213.94.205.109.1129823993.squirrel@webmail.valina.lunarpages.com>
Message-ID: <47312.213.94.205.109.1129826217.squirrel@webmail.valina.lunarpages.com>

Hi,

Reading through the archives, I see mention that some people were
interested in getting a Java 1.3 compatible backport of JSR166. I'm just
seeing if anyone ever produced one?

If not, will 1.3.4 be stable to use for Java 1.3.1, until a decision is
made to move to 1.4 or 1.5?

Many thanks,

Tim

From dawidk at mathcs.emory.edu  Thu Oct 20 14:52:48 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu Oct 20 14:53:20 2005
Subject: [concurrency-interest] Java 1.3 and JSR166
In-Reply-To: <47312.213.94.205.109.1129826217.squirrel@webmail.valina.lunarp
	ages.com>
References: <29905.213.94.205.109.1129823993.squirrel@webmail.valina.lunarpa
	ges.com> 
	<47312.213.94.205.109.1129826217.squirrel@webmail.valina.lunarpages.com>
Message-ID: <4357E780.6090705@mathcs.emory.edu>

Tim McAuley wrote:

>Hi,
>
>Reading through the archives, I see mention that some people were
>interested in getting a Java 1.3 compatible backport of JSR166. I'm just
>seeing if anyone ever produced one?
>
>  
>
There is a backport to 1.4:

http://dcl.mathcs.emory.edu/util/backport-util-concurrent/

Sources are available, and it is relatively easy to compile them under 
1.3. Try compiling (you need Apache Ant) - you'll get a bunch of error 
messages, but they stem from only a few places. You need to remove 
"assert" statements, remove exception chaining, and in one place, 
replace Boolean.valueOf() with b ? Boolean.TRUE : Boolean.FALSE.

You can also remove all the test sources (the test/ subdirectory) if 
they make trouble.

Ant produces the JAR file that you can then use in your applications.

Regards,
Dawid Kurzyniec

From crazybob at crazybob.org  Thu Oct 20 14:54:44 2005
From: crazybob at crazybob.org (Bob Lee)
Date: Thu Oct 20 14:55:11 2005
Subject: [concurrency-interest] Unbounded blocking queues - memory
	consumption
In-Reply-To: <4bc396880510201142j7d8857d3g904be4c4e1b782b@mail.google.com>
References: <434678B4.3090703@cs.oswego.edu>
	<4bc396880510201142j7d8857d3g904be4c4e1b782b@mail.google.com>
Message-ID: <a74683f90510201154n581fe4d0j62af222294c4c69f@mail.gmail.com>

Calum,

I might be able to help. I've implemented an object pool for which I
had the same problem, i.e. creating Node objects effectively canceled
out the pool's benefits when the pooled objects were small.

I solved the problem by using ASM to dynamically extend the class of
the object I was pooling at runtime effectively turning it into a
node. For example, given:

class Foo {
}

I dynamically generate this class at runtime:

class PooledFoo extends Foo {
  PooledFoo next;
}

You can get the source here: http://crazybob.org/pool.zip

Bob

---------- Forwarded message ----------
From: Doug Lea <dl@cs.oswego.edu>
To: Calum MacLean <cnmaclean@hotmail.com>
Date: Fri, 07 Oct 2005 09:23:06 -0400
Subject: Re: [concurrency-interest] Unbounded blocking queues - memory
consumption
Calum MacLean wrote:
> Hi
>
> My application is producing a large unbounded quantity of objects which are
> being put into a BlockingQueue for consumption by some other thread.
> I'm currently using LinkedBlockingQueue, as it's unbounded.
> However, I'm seeing that the LinkedBlockingQueue.Node objects take up a fair
> bit of memory - around about 16 bytes each according to my profiler.
> So, while I'm not 100% sure, I'm a bit worried about the memory consumption,
> as it's a large number of objects which are being produced.
>
> Are there any other alternatives for unbounded BlockingQueue implementations
> which maybe take up less memory than LinkedBlockingQueue?
>

I don't believe there is any way to reduce overhead for any
linked structure down and further -- the nodes only have item
and next fields; plus the usual Java per-Object header etc.
If you really need the space and are in full control of the
kinds of elements, you might be able to make a custom version.
If for example, each element is of class Element, you can add
a "next" link to the Element class, used only by the queue, and
then copy-paste-hack LinkedBlockingQueue to directly use it
rather than wrapping each item in a Node. This is not recommended
unless you are desparate though.

-Doug



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From mike.skells at ebizz-consulting.com  Fri Oct 21 08:44:56 2005
From: mike.skells at ebizz-consulting.com (Mike Skells)
Date: Fri Oct 21 08:45:28 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIMGGAA.dholmes@dltech.com.au>
Message-ID: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>

 

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu 
> [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
> Of David Holmes
> Sent: 20 October 2005 15:59
> To: concurrency-interest@altair.cs.oswego.edu
> Subject: RE: [concurrency-interest] Volatile array and Wait
> 
> Mike Skells writes:
> > On the consideration of a no op ...
> >
> >   volatile Object[] b = new Object[50]; ...
> >   b = b;      // volatile write
> >
> > Isnt the compiler/jit liable to remove the b=b statement, 
> through the 
> > normal rules of elimination of redundent code, or is this 
> explicitely 
> > barred for volatile variables
> 
> It can't remove the memory synchronization actions, even if 
> it could eliminate the "code".
> 
> > Similarly if a variable is never read, or never written, and is 
> > private then it can be eliminated cant it
> 
> Depends how absolute "never" is :) There are various 
> ahead-of-time building tools that purge unused fields and 
> code from class files using whole-program analysis - and 
> presume no reflective (or jni) access :). Not sure a jit 
> would ever get involved with this though.

My point for both of these issues is not whether a JIT _does_ make these
tranformations, as much as is there a _specification_ that stops a JIT, java
compiler, or AOT, AOP, or native compiler for that matter, from making that
transformation. If the limitation is just in the current implmentation then
it is unsafe.

I cant remember seeing such a specification anywhere, although there is a
lot of documentation out there  ;-)

> 
> David Holmes
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From jmanson at cs.purdue.edu  Fri Oct 21 10:40:39 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Fri Oct 21 10:41:29 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
Message-ID: <4358FDE7.6040006@cs.purdue.edu>

Mike Skells wrote:

> My point for both of these issues is not whether a JIT _does_ make these
> tranformations, as much as is there a _specification_ that stops a JIT, java
> compiler, or AOT, AOP, or native compiler for that matter, from making that
> transformation. If the limitation is just in the current implmentation then
> it is unsafe.
> 
> I cant remember seeing such a specification anywhere, although there is a
> lot of documentation out there  ;-)

There is such a specification.  The behavior of volatiles is dictated by 
the rules described in the Java memory model, which is Chapter 17 of the 
Java language specification, version 3 (as amended by JSR-133).  Java 
1.5/5.0 is compliant with this specification, although earlier versions 
are not.

The rules state that if one thread writes to a volatile, and another 
thread reads from that volatile, then all of the writes that happen 
before the write by the first thread are ordered before and visible to 
all of the reads that happen after the read by the second thread.

This requires memory synchronization and the cooperation of optimizing 
compilers.  No exception is made for redundant reads or writes.

As you say, if a variable is never read or written, it can be eliminated 
(you can deduce there are no memory effects in this case).  But, as 
David pointed out, you have to be very, very sure that the variable is 
never read or written.

					Jeremy
From tim at peierls.net  Fri Oct 21 10:43:10 2005
From: tim at peierls.net (Tim Peierls)
Date: Fri Oct 21 10:43:45 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
Message-ID: <4358FE7E.1030905@peierls.net>

>>>On the consideration of a no op ...
>>>
>>>  volatile Object[] b = new Object[50]; ...
>>>  b = b;      // volatile write
>>>
>>>Isn't the compiler/jit liable to remove the b=b statement, 

The fact that this thread has gone on for so long about the ins and outs of volatiles might 
encourage folks to think that proper concurrent program design often depends on this kind of 
finicky reasoning. That would be a shame.

Don't use volatile reads and writes to manage visibility of anything but the volatile value 
itself. Yes, the rules permit more than this, but they are there to protect you, not for you to 
write the concurrency equivalent of spaghetti code. (And yes, Doug Lea does it, in AQS/AQLS, but I 
hope fewer than two people reading this believe they are Doug Lea.)

*Do* use Atomic{Integer,Long,Reference}Array if you really need each element of an array to have 
volatile semantics. As a bonus, you'll get per-element atomic updates, something you'll probably 
want eventually even if you think you don't need it right now.

Of course, if there is any interdependence at all between elements of an array, you need explicit 
synchronization, in which case there is no point in using AtomicXxxArray and the entire discussion 
is moot.

--tim

From Pete.Soper at Sun.COM  Fri Oct 21 11:11:19 2005
From: Pete.Soper at Sun.COM (Pete Soper)
Date: Fri Oct 21 11:15:16 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
Message-ID: <43590517.2070607@Sun.COM>

Mike Skells wrote:
>  
> 
> 
>>-----Original Message-----
>>From: concurrency-interest-bounces@cs.oswego.edu 
>>[mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
>>Of David Holmes
>>Sent: 20 October 2005 15:59
>>To: concurrency-interest@altair.cs.oswego.edu
>>Subject: RE: [concurrency-interest] Volatile array and Wait
>>
>>Mike Skells writes:
>>
>>>On the consideration of a no op ...
>>>
>>>  volatile Object[] b = new Object[50]; ...
>>>  b = b;      // volatile write
>>>
>>>Isnt the compiler/jit liable to remove the b=b statement, 
>>
>>through the 
>>
>>>normal rules of elimination of redundent code, or is this 
>>
>>explicitely 
>>
>>>barred for volatile variables
>>
>>It can't remove the memory synchronization actions, even if 
>>it could eliminate the "code".
>>
>>
>>>Similarly if a variable is never read, or never written, and is 
>>>private then it can be eliminated cant it
>>
>>Depends how absolute "never" is :) There are various 
>>ahead-of-time building tools that purge unused fields and 
>>code from class files using whole-program analysis - and 
>>presume no reflective (or jni) access :). Not sure a jit 
>>would ever get involved with this though.
> 
> 
> My point for both of these issues is not whether a JIT _does_ make these
> tranformations, as much as is there a _specification_ that stops a JIT, java
> compiler, or AOT, AOP, or native compiler for that matter, from making that
> transformation. If the limitation is just in the current implmentation then
> it is unsafe.
> 
> I cant remember seeing such a specification anywhere, although there is a
> lot of documentation out there  ;-)

And sometimes hard to find. Some old links for the relevant specs seem
to have been made ineffective, but try this one:

  http://java.sun.com/docs/books/jls/index.html

See sections 8.3.1.4 and 17.4.5, especially the discussion item in the
latter section:

"A write to a volatile field happens happens before every subsequent
read of that field."

This doesn't say "if the read or write doesn't get optimized away." This
text isn't making a suggestion. In general compilers can't eliminate
volatile accesses. Cases like reflective inspection have to be covered,
as David Holmes mentioned, and this simply can't be anticipated (in the
sense of being ruled out) except in very special cases. And as somebody
else mentioned, volatile references impose strict constraints on code
motion (rearrangement of instruction ordering by a compiler). And
David's remark about even "b = b" having to preserve the memory sync
action is an important point: ignore the right hand side when trying to
understand this code vis a vis concurrency.

I'm cautiously optimistic that the Java spec has this area covered now,
thanks to all the JCP and other work that went into its improvement. :-)

By the way, compilers for older languages like C have had to deal with
volatile too and this understanding has fed forward into Java. A
proportion of C programmers have discovered the importance of volatile
declarations when they encounter industrial strength optimizers. An
extreme case of this was Encore Computer's C compiler that could do
automatic parallelization of loops (e.g. making the outermost possible
loop run iterations in parallel with multiple threads). The burden on
the optimizer to not "screw up" instruction reordering like hoisting
invariants, etc, was very high. But if the programmer misunderstood the
need for volatile declarations the program would often fail when run in
parallel (and of course a bug was filed against the "broken compiler").


-Pete

> 
> 
>>David Holmes
>>
>>_______________________________________________
>>Concurrency-interest mailing list
>>Concurrency-interest@altair.cs.oswego.edu
>>http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From Pete.Soper at Sun.COM  Fri Oct 21 11:54:39 2005
From: Pete.Soper at Sun.COM (Pete Soper)
Date: Fri Oct 21 11:58:20 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43590517.2070607@Sun.COM>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
	<43590517.2070607@Sun.COM>
Message-ID: <43590F3F.4000103@Sun.COM>

Pete Soper wrote:

> By the way, compilers for older languages like C have had to deal with
> volatile too and this understanding has fed forward into Java. A
> proportion of C programmers have discovered the importance of volatile
> declarations when they encounter industrial strength optimizers. An
> extreme case of this was Encore Computer's C compiler that could do
> automatic parallelization of loops (e.g. making the outermost possible
> loop run iterations in parallel with multiple threads). The burden on
> the optimizer to not "screw up" instruction reordering like hoisting
> invariants, etc, was very high. But if the programmer misunderstood the
> need for volatile declarations the program would often fail when run in
> parallel (and of course a bug was filed against the "broken compiler").
> 

I went a bit too far with this: requiring a manual volatile decl in this
context would often be an indication that alias or other analysis had
broken down (i.e. it *was* a compiler bug). I should have stopped at
"industrial strength optimizers" as my memory of 15-20 years ago is not
so good.

-Pete
From dawidk at mathcs.emory.edu  Fri Oct 21 14:51:04 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Fri Oct 21 14:51:38 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43590517.2070607@Sun.COM>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop> 
	<43590517.2070607@Sun.COM>
Message-ID: <43593898.9030305@mathcs.emory.edu>

Pete Soper wrote:

>See sections 8.3.1.4 and 17.4.5, especially the discussion item in the
>latter section:
>
>"A write to a volatile field happens happens before every subsequent
>read of that field."
>
>This doesn't say "if the read or write doesn't get optimized away." This
>text isn't making a suggestion. 
>  
>
Optimizations, by definition, are never mentioned in the spec (unless as 
hints). An optimization can do whatever it wants as long as it does not 
violate the spec. So, if the spec says something precise about side 
effects of reads and writes, that side effects must be preserved even in 
the face of optimizations.

>In general compilers can't eliminate
>volatile accesses. Cases like reflective inspection have to be covered,
>as David Holmes mentioned, and this simply can't be anticipated (in the
>sense of being ruled out) except in very special cases. 
>
Luckily, the situation does not look that terribly dire, since much more 
can be asserted about local variables, especially of primitive types 
(e.g. they cannot be seen by other threads, and they cannot be inspected 
reflectively; they cannot even be declared volatile); so aggressive 
optimizations apply to them quite well.

Regards,
Dawid

From Pete.Soper at Sun.COM  Fri Oct 21 15:44:16 2005
From: Pete.Soper at Sun.COM (Pete Soper)
Date: Fri Oct 21 15:48:00 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43593898.9030305@mathcs.emory.edu>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop>
	<43590517.2070607@Sun.COM> <43593898.9030305@mathcs.emory.edu>
Message-ID: <43594510.1080600@Sun.COM>

Dawid Kurzyniec wrote:
> Pete Soper wrote:
> 
> 
>>See sections 8.3.1.4 and 17.4.5, especially the discussion item in the
>>latter section:
>>
>>"A write to a volatile field happens happens before every subsequent
>>read of that field."
>>
>>This doesn't say "if the read or write doesn't get optimized away." This
>>text isn't making a suggestion. 
>> 
>>
> 
> Optimizations, by definition, are never mentioned in the spec (unless as 
> hints). An optimization can do whatever it wants as long as it does not 

I didn't mean to imply it would be proper to mention optimizations in a
spec. That was a flippant remark. I'll try not to make those kinds of
remarks in the future, realizing they don't work well with email!

-Pete
From Aaron.Harshbarger at crown.com  Fri Oct 21 16:03:01 2005
From: Aaron.Harshbarger at crown.com (Aaron.Harshbarger@crown.com)
Date: Fri Oct 21 16:03:34 2005
Subject: [concurrency-interest] Aaron Harshbarger is out of the office.
Message-ID: <OF060DCD59.074894AB-ON852570A1.006E23D1-852570A1.006E23D1@crown.com>


I will be out of the office starting  10/21/2005 and will not return until
10/24/2005.

I will respond to your message when I return.

From dawidk at mathcs.emory.edu  Fri Oct 21 16:15:51 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Fri Oct 21 16:16:23 2005
Subject: [concurrency-interest] Volatile array and Wait
In-Reply-To: <43594510.1080600@Sun.COM>
References: <008701c5d63d$3de27e90$0a01a8c0@MikeLaptop><43590517.2070607@Sun
	.COM> <43593898.9030305@mathcs.emory.edu> <43594510.1080600@Sun.COM>
Message-ID: <43594C77.9000901@mathcs.emory.edu>

Pete Soper wrote:

>Dawid Kurzyniec wrote:
>  
>
>>Pete Soper wrote:
>>
>>
>>    
>>
>>>See sections 8.3.1.4 and 17.4.5, especially the discussion item in the
>>>latter section:
>>>
>>>"A write to a volatile field happens happens before every subsequent
>>>read of that field."
>>>
>>>This doesn't say "if the read or write doesn't get optimized away." This
>>>text isn't making a suggestion. 
>>>
>>>
>>>      
>>>
>>Optimizations, by definition, are never mentioned in the spec (unless as 
>>hints). An optimization can do whatever it wants as long as it does not 
>>    
>>
>
>I didn't mean to imply it would be proper to mention optimizations in a
>spec. That was a flippant remark. I'll try not to make those kinds of
>remarks in the future, realizing they don't work well with email!
>  
>
And I didn't mean to sound patronizing :) My focus was on the second 
part: the freedom of optimizations is constrained by precise 
requirements of the spec; what it says is sacred, what it does _not_ say 
is a room for optimization. I agree, e-mail distorts intentions :)

Regards,
Dawid

From dl at cs.oswego.edu  Sun Oct 23 15:29:40 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun Oct 23 15:31:31 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 9,
	Issue 14
In-Reply-To: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEE@PA-GLOGEX01.glog.com>
References: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEE@PA-GLOGEX01.glog.com>
Message-ID: <435BE4A4.9000303@cs.oswego.edu>

Hu, Jinsong wrote:
>  the first time the
> DataBlock is loaded from disk, I will build up hash index for them,
> if a request for range query coming in, I first use hashing to find
> the correct element, then calling geNext() one by one to get the
> range result. I think a navigable LinkedHashMap is the answer.
> 

See rfe 6266354 http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6266354
which is probably also what you want.

This won't be in Mustang, but you can vote for it for Dolphin :-)

-Doug


From forax at univ-mlv.fr  Mon Oct 24 03:46:06 2005
From: forax at univ-mlv.fr (Remi Forax)
Date: Mon Oct 24 03:46:29 2005
Subject: [concurrency-interest] RE: Concurrency-interest Digest, Vol 9,
	Issue 14
In-Reply-To: <435BE4A4.9000303@cs.oswego.edu>
References: <2E6D4A21A5611B4A8F1AA4E22BE04431877CEE@PA-GLOGEX01.glog.com>
	<435BE4A4.9000303@cs.oswego.edu>
Message-ID: <435C913E.80704@univ-mlv.fr>

Doug Lea wrote:

> Hu, Jinsong wrote:
>
>>  the first time the
>> DataBlock is loaded from disk, I will build up hash index for them,
>> if a request for range query coming in, I first use hashing to find
>> the correct element, then calling geNext() one by one to get the
>> range result. I think a navigable LinkedHashMap is the answer.
>>
>
> See rfe 6266354 
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6266354
> which is probably also what you want.

My to cents, at least that 6182958 and 4848853 are related too.

>
> This won't be in Mustang, but you can vote for it for Dolphin :-)
>
> -Doug
>
R?mi Forax

From ryan.lecompte at pangonetworks.com  Tue Oct 25 09:16:46 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Tue Oct 25 09:17:25 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
Message-ID: <PANGOSERVERBh350mc00000008c@pangonetworks.com>

Hello all,

 

I know that one has two options for "protecting" data that can be
potentially read/written by multiple threads. In order for the main thread
to always see the most recently written value, one must declare the variable
as "volatile" or synchronize all access to it, such as:

 

synchronized void stop() {

   stopped = true;

}

 

synchronized boolean isStopped() {

   return stopped;

}

 

However, if the "synchronized" approach is taken, does it have to be at such
a granular level? Or can it suffice that whenever the variable "stopped" is
used, that it's at least protected by SOME lock? For example, if "stopped"
is only directly referenced in three methods that perform various
operations, can all three methods be declared as "synchronized" and the
above two methods (stop() / isStopped()) simply removed? Or do we always
need to have "synchronized accessors" for the variable in question? Also,
what happens if there are three methods that use the "stopped" variable, but
they are using different locks? For example, let's say method1 uses
"stopped" in a synchronized block on LOCK1, and method2 uses "stopped" in a
synchronized block on LOCK2, and method3 uses "stopped" in a synchronized
block on LOCK3. Will we still have the same effect as simply declaring the
variable as "volatile" here?

 

Thanks,

Ryan

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051025/2378a6f7/attachment.htm
From jmanson at cs.purdue.edu  Tue Oct 25 10:36:58 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Oct 25 10:37:46 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <PANGOSERVERBh350mc00000008c@pangonetworks.com>
References: <PANGOSERVERBh350mc00000008c@pangonetworks.com>
Message-ID: <435E430A.40100@cs.purdue.edu>

Ryan LeCompte wrote:

> However, if the "synchronized" approach is taken, does it have to be
> at such a granular level? Or can it suffice that whenever the
> variable "stopped" is used, that it's at least protected by SOME
> lock? For example, if "stopped" is only directly referenced in three
> methods that perform various operations, can all three methods be
> declared as "synchronized" and the above two methods (stop() /
> isStopped()) simply removed? Or do we always need to have
> "synchronized accessors" for the variable in question?


You can do it this way.  Synchronized accessors are nice because that
way, anyone who uses the variable in future revisions of the code will
be forced to synchronize.  But if it saves a reasonable amount of
locking overhead, there is no reason not to do what you suggest.

Bear in mind the answer to the next question, though - it is crucial
when deciding whether you have done this correctly or not.

> Also, what happens if there are three methods that use the "stopped"
> variable, but they are using different locks? For example, let's say
> method1 uses "stopped" in a synchronized block on LOCK1, and method2
> uses "stopped" in a synchronized block on LOCK2, and method3 uses
> "stopped" in a synchronized block on LOCK3. Will we still have the
> same effect as simply declaring the variable as "volatile" here?

This is no good.  You won't get ordering or visibility guarantees.
Locks are only guaranteed to be ordered with respect to each other if
they are on the same monitor.  There are a whole host of potential
problems.

The first, and most obvious, one is that you won't get mutual exclusion.

The second is that the compiler can remove a lock if it doesn't think
that lock will ever be used to communicate between threads.  The way it
decides this is by determining if there is another thread that accesses
that lock.  So you are introducing the possibility that the lock will be
eliminated.

It can get even more subtle than that.  So it really needs to be the
same lock.

					Jeremy
From ryan.lecompte at pangonetworks.com  Tue Oct 25 10:43:10 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Tue Oct 25 10:43:43 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E430A.40100@cs.purdue.edu>
Message-ID: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>

Thanks for the response, Jeremy. I believe I definitely have a handle on
this now. The idiom that I've described would also be necessary for
non-primitives, right? Also, in practice, how "common" is it that one sees
odd behavior in their programs when they don't properly follow the idiom
that I've described here (synchronization or volatile approach)? I believe
Josh Bloch mentions in his book "Effective Java" that you may only see bad
side effects if you're on a multi processor machine.

Ryan

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: Tuesday, October 25, 2005 10:37 AM
To: Ryan LeCompte; concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Synchronization of data read by multiple
threads

Ryan LeCompte wrote:

> However, if the "synchronized" approach is taken, does it have to be
> at such a granular level? Or can it suffice that whenever the
> variable "stopped" is used, that it's at least protected by SOME
> lock? For example, if "stopped" is only directly referenced in three
> methods that perform various operations, can all three methods be
> declared as "synchronized" and the above two methods (stop() /
> isStopped()) simply removed? Or do we always need to have
> "synchronized accessors" for the variable in question?


You can do it this way.  Synchronized accessors are nice because that
way, anyone who uses the variable in future revisions of the code will
be forced to synchronize.  But if it saves a reasonable amount of
locking overhead, there is no reason not to do what you suggest.

Bear in mind the answer to the next question, though - it is crucial
when deciding whether you have done this correctly or not.

> Also, what happens if there are three methods that use the "stopped"
> variable, but they are using different locks? For example, let's say
> method1 uses "stopped" in a synchronized block on LOCK1, and method2
> uses "stopped" in a synchronized block on LOCK2, and method3 uses
> "stopped" in a synchronized block on LOCK3. Will we still have the
> same effect as simply declaring the variable as "volatile" here?

This is no good.  You won't get ordering or visibility guarantees.
Locks are only guaranteed to be ordered with respect to each other if
they are on the same monitor.  There are a whole host of potential
problems.

The first, and most obvious, one is that you won't get mutual exclusion.

The second is that the compiler can remove a lock if it doesn't think
that lock will ever be used to communicate between threads.  The way it
decides this is by determining if there is another thread that accesses
that lock.  So you are introducing the possibility that the lock will be
eliminated.

It can get even more subtle than that.  So it really needs to be the
same lock.

					Jeremy

From jmanson at cs.purdue.edu  Tue Oct 25 11:17:29 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Oct 25 11:18:09 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
Message-ID: <435E4C89.5090809@cs.purdue.edu>

Ryan LeCompte wrote:
> Thanks for the response, Jeremy. I believe I definitely have a handle on
> this now. The idiom that I've described would also be necessary for
> non-primitives, right? 

If the field being accessed were part of some invariant that required 
mutual exclusion / atomicity guarantees for multiple fields, then yes.

> Also, in practice, how "common" is it that one sees
> odd behavior in their programs when they don't properly follow the idiom
> that I've described here (synchronization or volatile approach)? I believe
> Josh Bloch mentions in his book "Effective Java" that you may only see bad
> side effects if you're on a multi processor machine.

You can see lots of nasty things on a uniprocessor.  The compiler can 
perform transformations that cause results on a uniprocessor that are 
just as surprising as any results on a multiprocessor.  YMMV, of course, 
depending on the program, the compiler, and the architecture.

For example, consider the following:

boolean stopped = false;

Thread 1:
while (!stopped) {
   // do stuff not affecting stopped
}

Thread 2:
stopped = true;

It would be perfectly legal for a compiler to examine the code in Thread 
1 and determine that it does not change the value of stopped.  It could 
then decide that what you have in Thread 1 is an infinite loop, and 
remove the loop guard:

Replacement Thread 1:
if (!stopped) {
   while (true) {
     // do stuff
   }
}

And Thread 1 will never end, regardless of what Thread 2 does.

On the other hand, if you declare stopped to be volatile, you are 
telling the compiler it can be modified by another thread.  It then 
won't perform this transformation.

					Jeremy
From ryan.lecompte at pangonetworks.com  Tue Oct 25 11:25:36 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Tue Oct 25 11:26:07 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E4C89.5090809@cs.purdue.edu>
Message-ID: <PANGOSERVERogkJXrRj00000096@pangonetworks.com>

I'm a bit unclear with your first answer regarding a non-primitive field.
What exactly do you mean by "invariant" here? I was thinking of a
non-primitive field, such as (String foo;) that could be read/modified by
multiple threads. If the program required that whenever the variable "foo"
is used in method1, method2, or method3 that it's most recently written
value is always returned, then method1/method2/method3 would have to be
synchronized (on the same lock of course) OR the 'foo' variable would have
to be declared volatile. 

Ryan

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: Tuesday, October 25, 2005 11:17 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Synchronization of data read by multiple
threads

Ryan LeCompte wrote:
> Thanks for the response, Jeremy. I believe I definitely have a handle on
> this now. The idiom that I've described would also be necessary for
> non-primitives, right? 

If the field being accessed were part of some invariant that required 
mutual exclusion / atomicity guarantees for multiple fields, then yes.

> Also, in practice, how "common" is it that one sees
> odd behavior in their programs when they don't properly follow the idiom
> that I've described here (synchronization or volatile approach)? I believe
> Josh Bloch mentions in his book "Effective Java" that you may only see bad
> side effects if you're on a multi processor machine.

You can see lots of nasty things on a uniprocessor.  The compiler can 
perform transformations that cause results on a uniprocessor that are 
just as surprising as any results on a multiprocessor.  YMMV, of course, 
depending on the program, the compiler, and the architecture.

For example, consider the following:

boolean stopped = false;

Thread 1:
while (!stopped) {
   // do stuff not affecting stopped
}

Thread 2:
stopped = true;

It would be perfectly legal for a compiler to examine the code in Thread 
1 and determine that it does not change the value of stopped.  It could 
then decide that what you have in Thread 1 is an infinite loop, and 
remove the loop guard:

Replacement Thread 1:
if (!stopped) {
   while (true) {
     // do stuff
   }
}

And Thread 1 will never end, regardless of what Thread 2 does.

On the other hand, if you declare stopped to be volatile, you are 
telling the compiler it can be modified by another thread.  It then 
won't perform this transformation.

					Jeremy

From brian at quiotix.com  Tue Oct 25 11:55:16 2005
From: brian at quiotix.com (Brian Goetz)
Date: Tue Oct 25 11:55:52 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
Message-ID: <435E5564.4@quiotix.com>

> I believe
> Josh Bloch mentions in his book "Effective Java" that you may only see bad
> side effects if you're on a multi processor machine.

I don't believe he said that, but if he did, he was wrong.  You can see 
bad side effects as the result of compiler optimizations (instruction 
reordering, hoisting values into registers, etc) as well.

> It can get even more subtle than that.  So it really needs to be the
> same lock.

I can't stress enough the importance of this point.  Even if all 
accesses are "with synchronization", if you are using different locks, 
you might as well be using no synchronization at all.

The Simple Way: For each shared mutable variable in your program, make 
sure that _every_ access (read or write) to that variable is done with 
the _same_ lock held.

The Complicated Way: Learn the Java Memory Model.  Prove that there is a 
happens-before edge between every write of a shared variable and 
subsequent reads of that variable.

There really isn't much of a middle ground.


From ryan.lecompte at pangonetworks.com  Tue Oct 25 11:58:33 2005
From: ryan.lecompte at pangonetworks.com (Ryan LeCompte)
Date: Tue Oct 25 11:59:05 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E5564.4@quiotix.com>
Message-ID: <PANGOSERVERutSgcOrB000000a2@pangonetworks.com>

Page 190 of Effective Java:

"Unless you are running on a multiprocessor, you are unlikely to observe the
problematic behavior in practice, but there are no guarantees."

Thanks for the extra information!

Ryan

-----Original Message-----
From: Brian Goetz [mailto:brian@quiotix.com] 
Sent: Tuesday, October 25, 2005 11:55 AM
To: Ryan LeCompte
Cc: 'Jeremy Manson'; concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Synchronization of data read by multiple
threads

> I believe
> Josh Bloch mentions in his book "Effective Java" that you may only see bad
> side effects if you're on a multi processor machine.

I don't believe he said that, but if he did, he was wrong.  You can see 
bad side effects as the result of compiler optimizations (instruction 
reordering, hoisting values into registers, etc) as well.

> It can get even more subtle than that.  So it really needs to be the
> same lock.

I can't stress enough the importance of this point.  Even if all 
accesses are "with synchronization", if you are using different locks, 
you might as well be using no synchronization at all.

The Simple Way: For each shared mutable variable in your program, make 
sure that _every_ access (read or write) to that variable is done with 
the _same_ lock held.

The Complicated Way: Learn the Java Memory Model.  Prove that there is a 
happens-before edge between every write of a shared variable and 
subsequent reads of that variable.

There really isn't much of a middle ground.


From dawidk at mathcs.emory.edu  Tue Oct 25 12:25:07 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Tue Oct 25 12:25:45 2005
Subject: [concurrency-interest] Synchronization of data read by
	multiplethreads
In-Reply-To: <PANGOSERVERutSgcOrB000000a2@pangonetworks.com>
References: <PANGOSERVERutSgcOrB000000a2@pangonetworks.com>
Message-ID: <435E5C63.1050102@mathcs.emory.edu>

Ryan LeCompte wrote:

>Page 190 of Effective Java:
>
>"Unless you are running on a multiprocessor, you are unlikely to observe the
>problematic behavior in practice, but there are no guarantees."
>
>  
>
"Unlikely" <> "Never" :)

Not using synchronization properly is like driving in the night without 
lights - maybe, if the traffic is low, nothing bad will happen, but why 
on Earth would you do that? :)

Also, if you drive like this often and in high traffic, it's pretty sure 
that eventually a bad thing will happen. Similarly with lack of 
synchronization - it will hit you or somebody sooner or later, and it be 
usually in the worst possible moment. And you really gain nothing by not 
using synchronization properly.

This is not some artificial academic problem; this is a debugging 
nightmare to be taken seriously.

Regards,
Dawid

From jmanson at cs.purdue.edu  Tue Oct 25 12:32:05 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Oct 25 12:32:43 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <PANGOSERVERogkJXrRj00000096@pangonetworks.com>
References: <PANGOSERVERogkJXrRj00000096@pangonetworks.com>
Message-ID: <435E5E05.30305@cs.purdue.edu>

Ryan LeCompte wrote:
> I'm a bit unclear with your first answer regarding a non-primitive field.
> What exactly do you mean by "invariant" here? I was thinking of a
> non-primitive field, such as (String foo;) that could be read/modified by
> multiple threads. If the program required that whenever the variable "foo"
> is used in method1, method2, or method3 that it's most recently written
> value is always returned, then method1/method2/method3 would have to be
> synchronized (on the same lock of course) OR the 'foo' variable would have
> to be declared volatile. 
> 
> Ryan

I think I may have misinterpreted your question.  This is correct.

Having said that, the recent discussion on volatile should be a good 
indication that you shouldn't use it unless you are absolutely sure that 
it will do what you expect it to do.

					Jeremy

From jadams3 at nortel.com  Tue Oct 25 12:38:42 2005
From: jadams3 at nortel.com (John Adams)
Date: Tue Oct 25 12:39:37 2005
Subject: [concurrency-interest] Exception handling & Executors
Message-ID: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.com>

Hello,
 
First off, thanks for maintaining this list, it has proven invaluable to
me as I learn to use the concurrency utilities in Java 5.0.
 
The reason I'm posting is that I've had a lot of problems using the
Executor framework to deal with exceptions from errant tasks.  I've
complained about this to Sun, whose response was essentially that
exceptions belong with the task, so they should be try / caught at that
level.  
 
I don't agree.  I don't, for example, believe that it is the
responsibility of the task to handle OutOfMemoryError's, or in fact most
of the classes that occur within the Error class of throwables.   Many
of those exceptions imply that the tasks should have some knowledge
about how to respond to events are within the domain of the platform the
tasks are executing on.  Now you could reasonably argue --> What is
there to do anyways in that case ?   All I want to do is a
System.exit(), since we have a watchdog process that does "the right
thing" when the VM goes down.  In theory that would have been easy for
me to do with our custom threadpools pre-1.5, however to get it working
with the concurrent utilities has been a painful experience.  
 
I thought it may be helpful to note what we tried in what order, so that
maybe we could get some support beyond the custom Future task support
going in later VM releases.  So, in order ....
 
1. Override the ThreadGroup before creating the Executor.    We did this
initially expecting things to be easy, however we were quickly thwarted
by the DefaultThreadFactory that does this ...
 
       DefaultThreadFactory() {
            SecurityManager s = System.getSecurityManager();
            group = (s != null)? s.getThreadGroup() :
 
Thread.currentThread().getThreadGroup();
         }
 
I can see some security reasons for this, however I was not expecting it
--> I have stages that I wanted to log & name according to threadgroups.
However, that's not insurmountable ... I created my own threadfactory
which just took the currentThread().getThreadGroup() ... no luck.   I
also used the setExceptionHandler() method on Thread & ThreadGroup, but
to no avail.  I left this code in since I wanted this code to be
independent of our security manager choices, and explicitly setting the
exception handler seemed like a reasonable thing to do.  (and maybe
relevant to ppl on this list) 
 
2.  So then we went to look at the Future and realized what was
happening, the future task was catching throwable. (Yes, I know all
about custom futures, they don't help me now)  The only way we could
rationally seem to do this is to override afterExecute() in the executor
and do the following ....
 
try {
                // note I'd rather cast than use temporary objects 
                if ( !((FutureTask) task).isCancelled()){  // I don't
care about CancellationExceptions
                    ((FutureTask) task).get();
                }
             } catch (ExecutionException ee) {
                 // ie if (ee == OutOfMemoryError) {System.exit()}
            }
}
 
3. Ok, so now for every task we're checking if it is cancelled, casting
2x, and getting a result we almost never want, all of which have a
performance impact.   I tried throwing an OutOfMemoryError .... didn't
trip the exit().  More than a little aggravated, I looked at the
exception, and realized that ExecutionException wraps Throwable, which
means I need to inspect the cause.  So I changed that bit of code to 
 
try {
               if ( !((FutureTask) task).isCancelled()){ // I still
don't care about CancellationExceptions
                    ((FutureTask) task).get();
                }
             } catch (ExecutionException ee) {
                 //  ie if (ee.getCause() == OutOfMemoryError)
{System.exit()} 
            }
}
 
4.  The fourth problem I had was a bit subtle, and didn't come up until
later.   We have subsequent tasks that queue up on other executors.
This was resulting in a chain of ExecutionExceptions being set up, which
meant we had to recursively inspect all exceptions in the chain to make
sure there was no out of memory error.   Think SEDA, and you'll be able
to picture why we have multiple stages and chained execution exceptions.
It also makes us more vulnerable to StackOverflow errors.
 
Once you pull that all together, you can catch the OutOfMemoryError
cleanly, I think.  The moral of this story may be to not use the
Executors if you want to be a framework and don't trust your tasks, but
I don't think it has to be this hard.  There are several places in the
Java, and patterns that I can think of that have a setExceptionHandler()
pattern going on, wouldn't it be reasonable to add similar functionality
instead of forcing us to go through these hoops to find an Error ?   
 
Again, all of this can be avoided if you trust your tasks to do a try /
catch (Throwable t) , but as part of a framework I don't exactly have
that luxury.  Thanks again for all the information, I hope this is
useful to others on this list.
 
Regards,
John Adams
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051025/8ee914fb/attachment.htm
From joe.bowbeer at gmail.com  Tue Oct 25 13:24:03 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue Oct 25 13:24:33 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.com>
Message-ID: <31f2a7bd0510251024w7e33049bg675a23d20f5d65a0@mail.gmail.com>

A few ideas:

1. If you execute Runnable instead of FutureTask then any
RuntimeExceptions and Errors thrown can escape the Executor.

2. You can hook the ThreadPoolExecutor.afterExecute method to handle
the RuntimeExceptions thrown by your task.  (Note: the Throwable
signature is a little misleading.)

3. You can use the Thread.set[Default]UncaughtExceptionHandler to
handle uncaught exceptions and errors of all types.  (To associate the
exception handling with your executor, you can provide your own thread
factory.)

4. If you want to use FutureTask then you can hook its done() method
and deal with the exception there.


A comment:

I don't know how you expect to do anything with an
OutOfMemoryException, except perhaps log it.  I don't know of any way
to reliably recover from these.


On 10/25/05, John Adams <jadams3@nortel.com> wrote:
>
> Hello,
>
> First off, thanks for maintaining this list, it has proven invaluable to me
> as I learn to use the concurrency utilities in Java 5.0.
>
> The reason I'm posting is that I've had a lot of problems using the Executor
> framework to deal with exceptions from errant tasks.  I've complained about
> this to Sun, whose response was essentially that exceptions belong with the
> task, so they should be try / caught at that level.
>
> I don't agree.  I don't, for example, believe that it is the responsibility
> of the task to handle OutOfMemoryError's, or in fact most of the classes
> that occur within the Error class of throwables.   Many of those exceptions
> imply that the tasks should have some knowledge about how to respond to
> events are within the domain of the platform the tasks are executing on.
> Now you could reasonably argue --> What is there to do anyways in that case
> ?   All I want to do is a System.exit(), since we have a watchdog process
> that does "the right thing" when the VM goes down.  In theory that would
> have been easy for me to do with our custom threadpools pre-1.5, however to
> get it working with the concurrent utilities has been a painful experience.
>
> I thought it may be helpful to note what we tried in what order, so that
> maybe we could get some support beyond the custom Future task support going
> in later VM releases.  So, in order ....
>
> 1. Override the ThreadGroup before creating the Executor.    We did this
> initially expecting things to be easy, however we were quickly thwarted by
> the DefaultThreadFactory that does this ...
>
>        DefaultThreadFactory() {
>             SecurityManager s = System.getSecurityManager();
>             group = (s != null)? s.getThreadGroup() :
>
> Thread.currentThread().getThreadGroup();
>          }
>
> I can see some security reasons for this, however I was not expecting it -->
> I have stages that I wanted to log & name according to threadgroups.
> However, that's not insurmountable ... I created my own threadfactory which
> just took the currentThread().getThreadGroup() ... no luck.
>   I also used the setExceptionHandler() method on Thread & ThreadGroup, but
> to no avail.  I left this code in since I wanted this code to be independent
> of our security manager choices, and explicitly setting the exception
> handler seemed like a reasonable thing to do.  (and maybe relevant to ppl on
> this list)
>
> 2.  So then we went to look at the Future and realized what was happening,
> the future task was catching throwable. (Yes, I know all about custom
> futures, they don't help me now)  The only way we could rationally seem to
> do this is to override afterExecute() in the executor and do the following
> ....
>
> try {
>                 // note I'd rather cast than use temporary objects
>                 if ( !((FutureTask) task).isCancelled()){  // I don't care
> about CancellationExceptions
>                     ((FutureTask) task).get();
>                 }
>              } catch (ExecutionException ee) {
>                  // ie if (ee == OutOfMemoryError) {System.exit()}
>             }
> }
>
> 3. Ok, so now for every task we're checking if it is cancelled, casting 2x,
> and getting a result we almost never want, all of which have a performance
> impact.   I tried throwing an OutOfMemoryError .... didn't trip the exit().
> More than a little aggravated, I looked at the exception, and realized that
> ExecutionException wraps Throwable, which means I need to inspect the cause.
>  So I changed that bit of code to
>
>
> try {
>                if ( !((FutureTask) task).isCancelled()){ // I still don't
> care about CancellationExceptions
>                     ((FutureTask) task).get();
>                 }
>              } catch (ExecutionException ee) {
>                  //  ie if (ee.getCause() == OutOfMemoryError)
> {System.exit()}
>             }
> }
>
> 4.  The fourth problem I had was a bit subtle, and didn't come up until
> later.   We have subsequent tasks that queue up on other executors.  This
> was resulting in a chain of ExecutionExceptions being set up, which meant we
> had to recursively inspect all exceptions in the chain to make sure there
> was no out of memory error.   Think SEDA, and you'll be able to picture why
> we have multiple stages and chained execution exceptions.  It also makes us
> more vulnerable to StackOverflow errors.
>
> Once you pull that all together, you can catch the OutOfMemoryError cleanly,
> I think.  The moral of this story may be to not use the Executors if you
> want to be a framework and don't trust your tasks, but I don't think it has
> to be this hard.  There are several places in the Java, and patterns that I
> can think of that have a setExceptionHandler() pattern going on, wouldn't it
> be reasonable to add similar functionality instead of forcing us to go
> through these hoops to find an Error ?
>
> Again, all of this can be avoided if you trust your tasks to do a try /
> catch (Throwable t) , but as part of a framework I don't exactly have that
> luxury.  Thanks again for all the information, I hope this is useful to
> others on this list.
>
> Regards,
> John Adams
>

From gregg at cytetech.com  Tue Oct 25 14:31:47 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue Oct 25 14:32:21 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E4C89.5090809@cs.purdue.edu>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
	<435E4C89.5090809@cs.purdue.edu>
Message-ID: <435E7A13.8090008@cytetech.com>



Jeremy Manson wrote:
> It would be perfectly legal for a compiler to examine the code in Thread 
> 1 and determine that it does not change the value of stopped.  It could 
> then decide that what you have in Thread 1 is an infinite loop, and 
> remove the loop guard:
> 
> Replacement Thread 1:
> if (!stopped) {
>   while (true) {
>     // do stuff
>   }
> }
> 
> And Thread 1 will never end, regardless of what Thread 2 does.

I don't believe this is a valid optimization.  If the visibility of stopped is 
such that it can be referenced outside of the context the compiler is 
optimizing, then it can't possibly make this change to the code.  In that code, 
it would be perfectly valid for someone to code

synchronized( something ) {
	stopped = true;
}

and that would make the "stopped" change visible to the Thread 1 context.  This 
is java and late bindings can wreak havoc on such optimizations that are 
possible in other languages.

If Jeremy is suggesting that the compiler has done complete visibility analysis 
and can absolutely determine that the value is not altered, perhaps it might 
make that change.

I'm not convinced that such an optimization strategy would be a safe thing to 
do.  It will create such amazingly ugly bugs.  In code that has failed to 
utilize volatile (it didn't really work before) correctly, there are problems 
that will be very difficult to find with such changes to the code.  Now that 
volatile does actually work, it will be very unhelpful for the language to have 
compilers and JITs being this agressive, out of the box.

It would be very nice to have some tools that would look for variables that are 
read and written in different methods, in unsynchronized (from the method level 
down) blocks.  If they could do more global analysis too, that would be great. 
Then, you could scan your code and look for disparate paths of reading and 
alteration and then go study them to decide whether there were problems there or 
not.

Gregg Wonderly

From joe.bowbeer at gmail.com  Tue Oct 25 15:10:52 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue Oct 25 15:11:20 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E7A13.8090008@cytetech.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
	<435E4C89.5090809@cs.purdue.edu> <435E7A13.8090008@cytetech.com>
Message-ID: <31f2a7bd0510251210v12edacdald136b30e7ddcb38e@mail.gmail.com>

Gregg Wonderly <gregg@cytetech.com> wrote:
> I'm not convinced that such an optimization strategy would be a safe thing to
> do.

It's not a new thing, btw.  Symantec's JIT was doing this in 1997.

Note that the "compiler" in these memory model discussions refers to
the bytecode to native compiler as well as the source to bytecode
compiler.

I believe it is perfectly valid for the 2native compiler to hoist the
"stopped" access out of the loop -- until such time as additional code
starts to execute that might change the value of the flag.

I think hotspot is already equipped to "uncompile" code on the fly,
that is, to undo optimizations when preconditions have changed.

In any event, the JMM now officially sanctions this optimization, so be warned.


On 10/25/05, Gregg Wonderly <gregg@cytetech.com> wrote:
>
>
> Jeremy Manson wrote:
> > It would be perfectly legal for a compiler to examine the code in Thread
> > 1 and determine that it does not change the value of stopped.  It could
> > then decide that what you have in Thread 1 is an infinite loop, and
> > remove the loop guard:
> >
> > Replacement Thread 1:
> > if (!stopped) {
> >   while (true) {
> >     // do stuff
> >   }
> > }
> >
> > And Thread 1 will never end, regardless of what Thread 2 does.
>
> I don't believe this is a valid optimization.  If the visibility of stopped is
> such that it can be referenced outside of the context the compiler is
> optimizing, then it can't possibly make this change to the code.  In that code,
> it would be perfectly valid for someone to code
>
> synchronized( something ) {
>         stopped = true;
> }
>
> and that would make the "stopped" change visible to the Thread 1 context.  This
> is java and late bindings can wreak havoc on such optimizations that are
> possible in other languages.
>
> If Jeremy is suggesting that the compiler has done complete visibility analysis
> and can absolutely determine that the value is not altered, perhaps it might
> make that change.
>
> I'm not convinced that such an optimization strategy would be a safe thing to
> do.  It will create such amazingly ugly bugs.  In code that has failed to
> utilize volatile (it didn't really work before) correctly, there are problems
> that will be very difficult to find with such changes to the code.  Now that
> volatile does actually work, it will be very unhelpful for the language to have
> compilers and JITs being this agressive, out of the box.
>
> It would be very nice to have some tools that would look for variables that are
> read and written in different methods, in unsynchronized (from the method level
> down) blocks.  If they could do more global analysis too, that would be great.
> Then, you could scan your code and look for disparate paths of reading and
> alteration and then go study them to decide whether there were problems there or
> not.
>
> Gregg Wonderly
>

From jmanson at cs.purdue.edu  Tue Oct 25 15:20:59 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Oct 25 15:21:36 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E7A13.8090008@cytetech.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
	<435E4C89.5090809@cs.purdue.edu> <435E7A13.8090008@cytetech.com>
Message-ID: <435E859B.7080601@cs.purdue.edu>

Gregg Wonderly wrote:
> 
> 
> Jeremy Manson wrote:
> 

[reminder: stopped is NOT volatile]

>> It would be perfectly legal for a compiler to examine the code in 
>> Thread 1 and determine that it does not change the value of stopped.  
>> It could then decide that what you have in Thread 1 is an infinite 
>> loop, and remove the loop guard:
>>
>> Replacement Thread 1:
>> if (!stopped) {
>>   while (true) {
>>     // do stuff
>>   }
>> }
>>
>> And Thread 1 will never end, regardless of what Thread 2 does.
> 
> 
> I don't believe this is a valid optimization.  If the visibility of 
> stopped is such that it can be referenced outside of the context the 
> compiler is optimizing, then it can't possibly make this change to the 
> code.  In that code, it would be perfectly valid for someone to code
> 
> synchronized( something ) {
>     stopped = true;
> }
> 
> and that would make the "stopped" change visible to the Thread 1 
> context.  

If you want to use a variable to communicate between threads, you need 
to synchronize both sides.  You could do:

Thread 1:
while (true) {
   synchronized (something) {
      if (stopped) break;
   }
   // stuff
}

Thread 2:
synchronized (something) {
   stopped = true;
}

And that would work fine.  But if Thread 1 does not have the 
synchronization, then it is not incumbent on that thread to be able to 
tell that Thread 2's update has occurred.

If you have to rely on visibility analysis to do compiler optimizations, 
then you aren't going to be able to perform any optimizations at all. 
Or, rather, few enough that you are going to pay a substantial 
performance hit.

 > This is java and late bindings can wreak havoc on such optimizations
 > that are possible in other languages.

Late bindings have nothing to do with this; you can imagine, if you 
like, that there are no method calls inside the loop.

It is true that a compiler is unlikely to perform this transformation if 
there are unknown method calls in the loop.  On the other hand, with 
aggressive inlining, what constitutes an unknown method call may be a 
little unpredictable.

Bear in mind as well that when I say "compiler", I actually mean "JIT 
compiler".

> If Jeremy is suggesting that the compiler has done complete visibility 
> analysis and can absolutely determine that the value is not altered, 
> perhaps it might make that change.

A compiler can make that transformation.  If you want visibility outside 
the thread, then you must use synchronization or make stopped volatile.

> I'm not convinced that such an optimization strategy would be a safe 
> thing to do.  It will create such amazingly ugly bugs.  In code that has 
> failed to utilize volatile (it didn't really work before) correctly, 
> there are problems that will be very difficult to find with such changes 
> to the code.  Now that volatile does actually work, it will be very 
> unhelpful for the language to have compilers and JITs being this 
> agressive, out of the box.

What makes you think that compilers weren't doing it before?  All it is 
is loop invariant code motion.

Now that volatile does work, programmers should be using it.  Or better 
still, if they don't understand the issues well enough to use it 
safely, they should use a JSR-166 ExecutorService for this pattern.

						Jeremy
From eross at m-Qube.com  Tue Oct 25 15:39:13 2005
From: eross at m-Qube.com (Elias Ross)
Date: Tue Oct 25 15:40:10 2005
Subject: [concurrency-interest] Synchronization of data read by
	multiple threads
In-Reply-To: <435E7A13.8090008@cytetech.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>
	<435E4C89.5090809@cs.purdue.edu>  <435E7A13.8090008@cytetech.com>
Message-ID: <1130269153.5623.163.camel@momo.m-qube.com>

On Tue, 2005-10-25 at 13:31 -0500, Gregg Wonderly wrote:

> It would be very nice to have some tools that would look for variables that are 
> read and written in different methods, in unsynchronized (from the method level 
> down) blocks.  If they could do more global analysis too, that would be great. 
> Then, you could scan your code and look for disparate paths of reading and 
> alteration and then go study them to decide whether there were problems there or 
> not.

The "findbugs" analysis tool does an okay job looking for internal class
inconsistencies.

http://findbugs.sourceforge.net/bugDescriptions.html#IS2_INCONSISTENT_SYNC

Is this what you're talking about?

It will catch most unintentional synchronization mistakes.

From jnielsen at sungardsct.com  Tue Oct 25 15:38:10 2005
From: jnielsen at sungardsct.com (Jan Nielsen)
Date: Tue Oct 25 15:45:17 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <31f2a7bd0510251024w7e33049bg675a23d20f5d65a0@mail.gmail.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.com>
	<31f2a7bd0510251024w7e33049bg675a23d20f5d65a0@mail.gmail.com>
Message-ID: <435E89A2.4030300@sungardsct.com>

Joe,

With respect to your comment below about not being able to do anything 
with an out-of-memory condition, could you not reliably release, for 
example, cached data in an OutOfMemoryError handler:

  Map m = new HashMap();
  Object o = new PreAllocatedMemoryInBytes( 16 );
  try
  {
    // blah, blah, blah...allocating all the memory to 'm' values...
  }
  catch( OutOfMemoryError e )
  {
      o = null;
      m.clear();
      // only log after we have enough memory
      e.printStackTrace();
  }

The m.clear() method may require some allocation to work, so it would be 
susceptible to failure without having that much available memory, 
specifically:

    public void clear() {
        modCount++;
        Entry[] tab = table;
        for (int i = 0; i < tab.length; i++)
            tab[i] = null;
        size = 0;
    }

In the single-threaded case, pre-allocating sufficient memory to perform 
this operation should address this, should it not?

Thanks,

-Jan

Joe Bowbeer wrote:

>A few ideas:
>
>1. If you execute Runnable instead of FutureTask then any
>RuntimeExceptions and Errors thrown can escape the Executor.
>
>2. You can hook the ThreadPoolExecutor.afterExecute method to handle
>the RuntimeExceptions thrown by your task.  (Note: the Throwable
>signature is a little misleading.)
>
>3. You can use the Thread.set[Default]UncaughtExceptionHandler to
>handle uncaught exceptions and errors of all types.  (To associate the
>exception handling with your executor, you can provide your own thread
>factory.)
>
>4. If you want to use FutureTask then you can hook its done() method
>and deal with the exception there.
>
>
>A comment:
>
>I don't know how you expect to do anything with an
>OutOfMemoryException, except perhaps log it.  I don't know of any way
>to reliably recover from these.
>
>
>On 10/25/05, John Adams <jadams3@nortel.com> wrote:
>  
>
>>Hello,
>>
>>First off, thanks for maintaining this list, it has proven invaluable to me
>>as I learn to use the concurrency utilities in Java 5.0.
>>
>>The reason I'm posting is that I've had a lot of problems using the Executor
>>framework to deal with exceptions from errant tasks.  I've complained about
>>this to Sun, whose response was essentially that exceptions belong with the
>>task, so they should be try / caught at that level.
>>
>>I don't agree.  I don't, for example, believe that it is the responsibility
>>of the task to handle OutOfMemoryError's, or in fact most of the classes
>>that occur within the Error class of throwables.   Many of those exceptions
>>imply that the tasks should have some knowledge about how to respond to
>>events are within the domain of the platform the tasks are executing on.
>>Now you could reasonably argue --> What is there to do anyways in that case
>>?   All I want to do is a System.exit(), since we have a watchdog process
>>that does "the right thing" when the VM goes down.  In theory that would
>>have been easy for me to do with our custom threadpools pre-1.5, however to
>>get it working with the concurrent utilities has been a painful experience.
>>
>>I thought it may be helpful to note what we tried in what order, so that
>>maybe we could get some support beyond the custom Future task support going
>>in later VM releases.  So, in order ....
>>
>>1. Override the ThreadGroup before creating the Executor.    We did this
>>initially expecting things to be easy, however we were quickly thwarted by
>>the DefaultThreadFactory that does this ...
>>
>>       DefaultThreadFactory() {
>>            SecurityManager s = System.getSecurityManager();
>>            group = (s != null)? s.getThreadGroup() :
>>
>>Thread.currentThread().getThreadGroup();
>>         }
>>
>>I can see some security reasons for this, however I was not expecting it -->
>>I have stages that I wanted to log & name according to threadgroups.
>>However, that's not insurmountable ... I created my own threadfactory which
>>just took the currentThread().getThreadGroup() ... no luck.
>>  I also used the setExceptionHandler() method on Thread & ThreadGroup, but
>>to no avail.  I left this code in since I wanted this code to be independent
>>of our security manager choices, and explicitly setting the exception
>>handler seemed like a reasonable thing to do.  (and maybe relevant to ppl on
>>this list)
>>
>>2.  So then we went to look at the Future and realized what was happening,
>>the future task was catching throwable. (Yes, I know all about custom
>>futures, they don't help me now)  The only way we could rationally seem to
>>do this is to override afterExecute() in the executor and do the following
>>....
>>
>>try {
>>                // note I'd rather cast than use temporary objects
>>                if ( !((FutureTask) task).isCancelled()){  // I don't care
>>about CancellationExceptions
>>                    ((FutureTask) task).get();
>>                }
>>             } catch (ExecutionException ee) {
>>                 // ie if (ee == OutOfMemoryError) {System.exit()}
>>            }
>>}
>>
>>3. Ok, so now for every task we're checking if it is cancelled, casting 2x,
>>and getting a result we almost never want, all of which have a performance
>>impact.   I tried throwing an OutOfMemoryError .... didn't trip the exit().
>>More than a little aggravated, I looked at the exception, and realized that
>>ExecutionException wraps Throwable, which means I need to inspect the cause.
>> So I changed that bit of code to
>>
>>
>>try {
>>               if ( !((FutureTask) task).isCancelled()){ // I still don't
>>care about CancellationExceptions
>>                    ((FutureTask) task).get();
>>                }
>>             } catch (ExecutionException ee) {
>>                 //  ie if (ee.getCause() == OutOfMemoryError)
>>{System.exit()}
>>            }
>>}
>>
>>4.  The fourth problem I had was a bit subtle, and didn't come up until
>>later.   We have subsequent tasks that queue up on other executors.  This
>>was resulting in a chain of ExecutionExceptions being set up, which meant we
>>had to recursively inspect all exceptions in the chain to make sure there
>>was no out of memory error.   Think SEDA, and you'll be able to picture why
>>we have multiple stages and chained execution exceptions.  It also makes us
>>more vulnerable to StackOverflow errors.
>>
>>Once you pull that all together, you can catch the OutOfMemoryError cleanly,
>>I think.  The moral of this story may be to not use the Executors if you
>>want to be a framework and don't trust your tasks, but I don't think it has
>>to be this hard.  There are several places in the Java, and patterns that I
>>can think of that have a setExceptionHandler() pattern going on, wouldn't it
>>be reasonable to add similar functionality instead of forcing us to go
>>through these hoops to find an Error ?
>>
>>Again, all of this can be avoided if you trust your tasks to do a try /
>>catch (Throwable t) , but as part of a framework I don't exactly have that
>>luxury.  Thanks again for all the information, I hope this is useful to
>>others on this list.
>>
>>Regards,
>>John Adams
>>
>>    
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest@altair.cs.oswego.edu
>http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  
>


-- 
Jan Nielsen
System Architect
Luminis Solutions
SunGard SCT

jnielsen@sungardsct.com
http://www.sungardsct.com

+1 801 257 4155 (voice)
+1 801 485 6606 (facsimile)

90 South 400 West
Salt Lake City, UT 84101

From joe.bowbeer at gmail.com  Tue Oct 25 15:55:12 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue Oct 25 15:55:40 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <435E89A2.4030300@sungardsct.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.com>
	<31f2a7bd0510251024w7e33049bg675a23d20f5d65a0@mail.gmail.com>
	<435E89A2.4030300@sungardsct.com>
Message-ID: <31f2a7bd0510251255m4f685f60l173908434dbfab1d@mail.gmail.com>

On 10/25/05, Jan Nielsen <jnielsen@sungardsct.com> wrote:
>
> With respect to your comment below about not being able to do anything
> with an out-of-memory condition, could you not reliably release, for
> example, cached data in an OutOfMemoryError handler:
>

In my experience, recovery from OOME is dicey at best and therefore
the only robust approach is to design and implement the app in such as
way that OOME never happens.

(Note: real-time Java tells a different story.)

I haven't investigated carefully why OOME recovery is dicey, but since
nothing is supposed to catch it I assume that once it is thrown the
system is in an unrecoverable state: no cleanup has been performed
along the way, connections and streams are broken, third-party
libraries are left in some indeterminate state, etc.

If your app could control where the OOME occurs then it would have a
better chance to catch it and recover -- by releasing buffers, for
example.  But I don't have a lot of faith that this can be done
reliably (or efficiently).


On 10/25/05, Jan Nielsen <jnielsen@sungardsct.com> wrote:
> Joe,
>
> With respect to your comment below about not being able to do anything
> with an out-of-memory condition, could you not reliably release, for
> example, cached data in an OutOfMemoryError handler:
>
>   Map m = new HashMap();
>   Object o = new PreAllocatedMemoryInBytes( 16 );
>   try
>   {
>     // blah, blah, blah...allocating all the memory to 'm' values...
>   }
>   catch( OutOfMemoryError e )
>   {
>       o = null;
>       m.clear();
>       // only log after we have enough memory
>       e.printStackTrace();
>   }
>
> The m.clear() method may require some allocation to work, so it would be
> susceptible to failure without having that much available memory,
> specifically:
>
>     public void clear() {
>         modCount++;
>         Entry[] tab = table;
>         for (int i = 0; i < tab.length; i++)
>             tab[i] = null;
>         size = 0;
>     }
>
> In the single-threaded case, pre-allocating sufficient memory to perform
> this operation should address this, should it not?
>
> Thanks,
>
> -Jan
>
> Joe Bowbeer wrote:
> >
> >I don't know how you expect to do anything with an
> >OutOfMemoryException, except perhaps log it.  I don't know of any way
> >to reliably recover from these.
> >

From jadams3 at nortel.com  Tue Oct 25 15:59:24 2005
From: jadams3 at nortel.com (John Adams)
Date: Tue Oct 25 16:00:15 2005
Subject: [concurrency-interest] Exception handling & Executors
Message-ID: <0BDFFF51DC89434FA33F8B37FCE363D504FC0254@zcarhxm2.corp.nortel.com>

Hi Joe,

Thank you for the reply.  Regarding your comment about OutOfMemory, as
was buried in the original post, I want to log the exception and then do
a System.exit().  We have a watchdog process that restarts the server
and starts again.  If we have a gradual leak going on, it's better in
the field for us to raise an alarm and restart the server than it is to
thrash with out of memory.  We're trying to build a reliable system, and
we have to deal with the possibility that this could happen in the
field, despite all of our best leak detection efforts.  

To respond ...

1. We use runnables in our implementation already.  If you pass a
runnable into a task, this code in AbstractExecutorService gets hit ...

 public <T> Future<T> submit(Runnable task, T result) {
        if (task == null) throw new NullPointerException();
        FutureTask<T> ftask = new FutureTask<T>(task, result);
        execute(ftask);
        return ftask;
    }

Note the wrapping of the FutureTask, and the source of my complaint.
The inner task uses Sync, an inner class of FutureTask, which does this
..

 void innerRun() {
            if (!compareAndSetState(0, RUNNING)) 
                return;
            try {
                runner = Thread.currentThread();
                innerSet(callable.call());
            } catch(Throwable ex) {
                innerSetException(ex);
            } 
        }

2. I do hook into the afterExecute() which resolves my problem, albeit
with some ugly code.   I'm really just saying that this solution is
complex and hoping someone in the future will implement a
setDefaultExceptionHandler() equivalent for the Executor framework.  I
also thought others might have this problem, so it would be useful to
provide this code to others with similar requirements.

3. The Thread.setDefaultExceptionHandler doesn't resolve the problem,
the FutureTask/Sync catches throwable. 

4. The done() idea is interesting, although I'm guessing that's part of
how the custom future task will deal with the problem in later releases.
I'm not sure how I can use that to my advantage in 1.5.

Incidentally, someone here did suggest re-implementing the whole
threadpool (ExecutorService) ourselves to "work around" the issue.  I'll
consider that notion if the casts and the isCancelled() call show up in
profiling, but for now I'd like to stick with the standard libraries if
possible. 

Regards,
John.

-----Original Message-----
From: Joe Bowbeer [mailto:joe.bowbeer@gmail.com] 
Sent: Tuesday, October 25, 2005 1:24 PM
To: Adams, John [CAR:3P41:EXCH]
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] Exception handling & Executors


A few ideas:

1. If you execute Runnable instead of FutureTask then any
RuntimeExceptions and Errors thrown can escape the Executor.

2. You can hook the ThreadPoolExecutor.afterExecute method to handle the
RuntimeExceptions thrown by your task.  (Note: the Throwable signature
is a little misleading.)

3. You can use the Thread.set[Default]UncaughtExceptionHandler to handle
uncaught exceptions and errors of all types.  (To associate the
exception handling with your executor, you can provide your own thread
factory.)

4. If you want to use FutureTask then you can hook its done() method and
deal with the exception there.


A comment:

I don't know how you expect to do anything with an OutOfMemoryException,
except perhaps log it.  I don't know of any way to reliably recover from
these.


On 10/25/05, John Adams <jadams3@nortel.com> wrote:
>
> Hello,
>
> First off, thanks for maintaining this list, it has proven invaluable 
> to me as I learn to use the concurrency utilities in Java 5.0.
>
> The reason I'm posting is that I've had a lot of problems using the 
> Executor framework to deal with exceptions from errant tasks.  I've 
> complained about this to Sun, whose response was essentially that 
> exceptions belong with the task, so they should be try / caught at 
> that level.
>
> I don't agree.  I don't, for example, believe that it is the 
> responsibility of the task to handle OutOfMemoryError's, or in fact
most of the classes
> that occur within the Error class of throwables.   Many of those
exceptions
> imply that the tasks should have some knowledge about how to respond 
> to events are within the domain of the platform the tasks are 
> executing on. Now you could reasonably argue --> What is there to do
anyways in that case
> ?   All I want to do is a System.exit(), since we have a watchdog
process
> that does "the right thing" when the VM goes down.  In theory that 
> would have been easy for me to do with our custom threadpools pre-1.5,

> however to get it working with the concurrent utilities has been a 
> painful experience.
>
> I thought it may be helpful to note what we tried in what order, so 
> that maybe we could get some support beyond the custom Future task 
> support going in later VM releases.  So, in order ....
>
> 1. Override the ThreadGroup before creating the Executor.    We did
this
> initially expecting things to be easy, however we were quickly 
> thwarted by the DefaultThreadFactory that does this ...
>
>        DefaultThreadFactory() {
>             SecurityManager s = System.getSecurityManager();
>             group = (s != null)? s.getThreadGroup() :
>
> Thread.currentThread().getThreadGroup();
>          }
>
> I can see some security reasons for this, however I was not expecting 
> it --> I have stages that I wanted to log & name according to 
> threadgroups. However, that's not insurmountable ... I created my own 
> threadfactory which just took the currentThread().getThreadGroup() ...
no luck.
>   I also used the setExceptionHandler() method on Thread & 
> ThreadGroup, but to no avail.  I left this code in since I wanted this

> code to be independent of our security manager choices, and explicitly

> setting the exception handler seemed like a reasonable thing to do.  
> (and maybe relevant to ppl on this list)
>
> 2.  So then we went to look at the Future and realized what was 
> happening, the future task was catching throwable. (Yes, I know all 
> about custom futures, they don't help me now)  The only way we could 
> rationally seem to do this is to override afterExecute() in the 
> executor and do the following ....
>
> try {
>                 // note I'd rather cast than use temporary objects
>                 if ( !((FutureTask) task).isCancelled()){  // I don't 
> care about CancellationExceptions
>                     ((FutureTask) task).get();
>                 }
>              } catch (ExecutionException ee) {
>                  // ie if (ee == OutOfMemoryError) {System.exit()}
>             }
> }
>
> 3. Ok, so now for every task we're checking if it is cancelled, 
> casting 2x, and getting a result we almost never want, all of which
have a performance
> impact.   I tried throwing an OutOfMemoryError .... didn't trip the
exit().
> More than a little aggravated, I looked at the exception, and realized

> that ExecutionException wraps Throwable, which means I need to inspect

> the cause.  So I changed that bit of code to
>
>
> try {
>                if ( !((FutureTask) task).isCancelled()){ // I still 
> don't care about CancellationExceptions
>                     ((FutureTask) task).get();
>                 }
>              } catch (ExecutionException ee) {
>                  //  ie if (ee.getCause() == OutOfMemoryError) 
> {System.exit()}
>             }
> }
>
> 4.  The fourth problem I had was a bit subtle, and didn't come up
until
> later.   We have subsequent tasks that queue up on other executors.
This
> was resulting in a chain of ExecutionExceptions being set up, which 
> meant we had to recursively inspect all exceptions in the chain to
make sure there
> was no out of memory error.   Think SEDA, and you'll be able to
picture why
> we have multiple stages and chained execution exceptions.  It also 
> makes us more vulnerable to StackOverflow errors.
>
> Once you pull that all together, you can catch the OutOfMemoryError 
> cleanly, I think.  The moral of this story may be to not use the 
> Executors if you want to be a framework and don't trust your tasks, 
> but I don't think it has to be this hard.  There are several places in

> the Java, and patterns that I can think of that have a 
> setExceptionHandler() pattern going on, wouldn't it be reasonable to 
> add similar functionality instead of forcing us to go through these 
> hoops to find an Error ?
>
> Again, all of this can be avoided if you trust your tasks to do a try 
> / catch (Throwable t) , but as part of a framework I don't exactly 
> have that luxury.  Thanks again for all the information, I hope this 
> is useful to others on this list.
>
> Regards,
> John Adams
>


From brian at quiotix.com  Tue Oct 25 16:12:55 2005
From: brian at quiotix.com (Brian Goetz)
Date: Tue Oct 25 16:13:32 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E7A13.8090008@cytetech.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>
	<435E7A13.8090008@cytetech.com>
Message-ID: <435E91C7.3080300@quiotix.com>

 > I don't believe this is a valid optimization.  If the visibility of
 > stopped is such that it can be referenced outside of the context the
 > compiler is optimizing, then it can't possibly make this change to the
 > code.  In that code, it would be perfectly valid for someone to code

You would be incorrect, then.

To put it in terms of the new JMM: the compiler can prove that the only 
writes to a non-volatile 'stopped' that can happen-before the next read 
to stopped occur in the body of the loop.  If it can prove that there 
are no such writes in the body of the loop, it becomes loop-invariant 
and can be hoisted.

The fact that another thread may want to write to stopped does not 
matter -- it is only writes that happen-before the next read in this 
thread that the compiler need pay attention to.

All access to shared mutable state must be done with appropriate 
synchronization, otherwise bad things happen.

>> It would be perfectly legal for a compiler to examine the code in 
>> Thread 1 and determine that it does not change the value of stopped.  
>> It could then decide that what you have in Thread 1 is an infinite 
>> loop, and remove the loop guard:
>>
>> Replacement Thread 1:
>> if (!stopped) {
>>   while (true) {
>>     // do stuff
>>   }
>> }
>>
>> And Thread 1 will never end, regardless of what Thread 2 does.
> 
> 

From joe.bowbeer at gmail.com  Tue Oct 25 16:21:40 2005
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue Oct 25 16:22:17 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <0BDFFF51DC89434FA33F8B37FCE363D504FC0254@zcarhxm2.corp.nortel.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FC0254@zcarhxm2.corp.nortel.com>
Message-ID: <31f2a7bd0510251321r7800b570tc01779e6d9f390f4@mail.gmail.com>

I meant:

executor.execute(runnable)

This gives you the most control over the actual task that is
executing.  For example, your task could be a custom FutureTask with a
done() method specialized for handling OOME.

To isolate the impact, you could create a "task factory" that took a
runnable and returned a FutureTask customized for OOME handling.

Or (and this is probably the best solution for you) you could create a
custom ExecutorService that did this for you, by extending
AbstractExecutorService.

This sort of extension is made even easier in 1.6 Mustang with the
addition of the newTaskFor method:

    protected <T> RunnableFuture<T> newTaskFor(Runnable runnable, T value) {
        return new FutureTask<T>(runnable, value);
    }


On 10/25/05, John Adams <jadams3@nortel.com> wrote:
> Hi Joe,
>
> Thank you for the reply.  Regarding your comment about OutOfMemory, as
> was buried in the original post, I want to log the exception and then do
> a System.exit().  We have a watchdog process that restarts the server
> and starts again.  If we have a gradual leak going on, it's better in
> the field for us to raise an alarm and restart the server than it is to
> thrash with out of memory.  We're trying to build a reliable system, and
> we have to deal with the possibility that this could happen in the
> field, despite all of our best leak detection efforts.
>
> To respond ...
>
> 1. We use runnables in our implementation already.  If you pass a
> runnable into a task, this code in AbstractExecutorService gets hit ...
>
>  public <T> Future<T> submit(Runnable task, T result) {
>         if (task == null) throw new NullPointerException();
>         FutureTask<T> ftask = new FutureTask<T>(task, result);
>         execute(ftask);
>         return ftask;
>     }
>
> Note the wrapping of the FutureTask, and the source of my complaint.
> The inner task uses Sync, an inner class of FutureTask, which does this
> ..
>
>  void innerRun() {
>             if (!compareAndSetState(0, RUNNING))
>                 return;
>             try {
>                 runner = Thread.currentThread();
>                 innerSet(callable.call());
>             } catch(Throwable ex) {
>                 innerSetException(ex);
>             }
>         }
>
> 2. I do hook into the afterExecute() which resolves my problem, albeit
> with some ugly code.   I'm really just saying that this solution is
> complex and hoping someone in the future will implement a
> setDefaultExceptionHandler() equivalent for the Executor framework.  I
> also thought others might have this problem, so it would be useful to
> provide this code to others with similar requirements.
>
> 3. The Thread.setDefaultExceptionHandler doesn't resolve the problem,
> the FutureTask/Sync catches throwable.
>
> 4. The done() idea is interesting, although I'm guessing that's part of
> how the custom future task will deal with the problem in later releases.
> I'm not sure how I can use that to my advantage in 1.5.
>
> Incidentally, someone here did suggest re-implementing the whole
> threadpool (ExecutorService) ourselves to "work around" the issue.  I'll
> consider that notion if the casts and the isCancelled() call show up in
> profiling, but for now I'd like to stick with the standard libraries if
> possible.
>
> Regards,
> John.
>
> -----Original Message-----
> From: Joe Bowbeer [mailto:joe.bowbeer@gmail.com]
> Sent: Tuesday, October 25, 2005 1:24 PM
> To: Adams, John [CAR:3P41:EXCH]
> Cc: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] Exception handling & Executors
>
>
> A few ideas:
>
> 1. If you execute Runnable instead of FutureTask then any
> RuntimeExceptions and Errors thrown can escape the Executor.
>
> 2. You can hook the ThreadPoolExecutor.afterExecute method to handle the
> RuntimeExceptions thrown by your task.  (Note: the Throwable signature
> is a little misleading.)
>
> 3. You can use the Thread.set[Default]UncaughtExceptionHandler to handle
> uncaught exceptions and errors of all types.  (To associate the
> exception handling with your executor, you can provide your own thread
> factory.)
>
> 4. If you want to use FutureTask then you can hook its done() method and
> deal with the exception there.
>
>
> A comment:
>
> I don't know how you expect to do anything with an OutOfMemoryException,
> except perhaps log it.  I don't know of any way to reliably recover from
> these.
>
>
> On 10/25/05, John Adams <jadams3@nortel.com> wrote:
> >
> > Hello,
> >
> > First off, thanks for maintaining this list, it has proven invaluable
> > to me as I learn to use the concurrency utilities in Java 5.0.
> >
> > The reason I'm posting is that I've had a lot of problems using the
> > Executor framework to deal with exceptions from errant tasks.  I've
> > complained about this to Sun, whose response was essentially that
> > exceptions belong with the task, so they should be try / caught at
> > that level.
> >
> > I don't agree.  I don't, for example, believe that it is the
> > responsibility of the task to handle OutOfMemoryError's, or in fact
> most of the classes
> > that occur within the Error class of throwables.   Many of those
> exceptions
> > imply that the tasks should have some knowledge about how to respond
> > to events are within the domain of the platform the tasks are
> > executing on. Now you could reasonably argue --> What is there to do
> anyways in that case
> > ?   All I want to do is a System.exit(), since we have a watchdog
> process
> > that does "the right thing" when the VM goes down.  In theory that
> > would have been easy for me to do with our custom threadpools pre-1.5,
>
> > however to get it working with the concurrent utilities has been a
> > painful experience.
> >
> > I thought it may be helpful to note what we tried in what order, so
> > that maybe we could get some support beyond the custom Future task
> > support going in later VM releases.  So, in order ....
> >
> > 1. Override the ThreadGroup before creating the Executor.    We did
> this
> > initially expecting things to be easy, however we were quickly
> > thwarted by the DefaultThreadFactory that does this ...
> >
> >        DefaultThreadFactory() {
> >             SecurityManager s = System.getSecurityManager();
> >             group = (s != null)? s.getThreadGroup() :
> >
> > Thread.currentThread().getThreadGroup();
> >          }
> >
> > I can see some security reasons for this, however I was not expecting
> > it --> I have stages that I wanted to log & name according to
> > threadgroups. However, that's not insurmountable ... I created my own
> > threadfactory which just took the currentThread().getThreadGroup() ...
> no luck.
> >   I also used the setExceptionHandler() method on Thread &
> > ThreadGroup, but to no avail.  I left this code in since I wanted this
>
> > code to be independent of our security manager choices, and explicitly
>
> > setting the exception handler seemed like a reasonable thing to do.
> > (and maybe relevant to ppl on this list)
> >
> > 2.  So then we went to look at the Future and realized what was
> > happening, the future task was catching throwable. (Yes, I know all
> > about custom futures, they don't help me now)  The only way we could
> > rationally seem to do this is to override afterExecute() in the
> > executor and do the following ....
> >
> > try {
> >                 // note I'd rather cast than use temporary objects
> >                 if ( !((FutureTask) task).isCancelled()){  // I don't
> > care about CancellationExceptions
> >                     ((FutureTask) task).get();
> >                 }
> >              } catch (ExecutionException ee) {
> >                  // ie if (ee == OutOfMemoryError) {System.exit()}
> >             }
> > }
> >
> > 3. Ok, so now for every task we're checking if it is cancelled,
> > casting 2x, and getting a result we almost never want, all of which
> have a performance
> > impact.   I tried throwing an OutOfMemoryError .... didn't trip the
> exit().
> > More than a little aggravated, I looked at the exception, and realized
>
> > that ExecutionException wraps Throwable, which means I need to inspect
>
> > the cause.  So I changed that bit of code to
> >
> >
> > try {
> >                if ( !((FutureTask) task).isCancelled()){ // I still
> > don't care about CancellationExceptions
> >                     ((FutureTask) task).get();
> >                 }
> >              } catch (ExecutionException ee) {
> >                  //  ie if (ee.getCause() == OutOfMemoryError)
> > {System.exit()}
> >             }
> > }
> >
> > 4.  The fourth problem I had was a bit subtle, and didn't come up
> until
> > later.   We have subsequent tasks that queue up on other executors.
> This
> > was resulting in a chain of ExecutionExceptions being set up, which
> > meant we had to recursively inspect all exceptions in the chain to
> make sure there
> > was no out of memory error.   Think SEDA, and you'll be able to
> picture why
> > we have multiple stages and chained execution exceptions.  It also
> > makes us more vulnerable to StackOverflow errors.
> >
> > Once you pull that all together, you can catch the OutOfMemoryError
> > cleanly, I think.  The moral of this story may be to not use the
> > Executors if you want to be a framework and don't trust your tasks,
> > but I don't think it has to be this hard.  There are several places in
>
> > the Java, and patterns that I can think of that have a
> > setExceptionHandler() pattern going on, wouldn't it be reasonable to
> > add similar functionality instead of forcing us to go through these
> > hoops to find an Error ?
> >
> > Again, all of this can be avoided if you trust your tasks to do a try
> > / catch (Throwable t) , but as part of a framework I don't exactly
> > have that luxury.  Thanks again for all the information, I hope this
> > is useful to others on this list.
> >
> > Regards,
> > John Adams
> >
>
>

From tim at peierls.net  Tue Oct 25 17:16:25 2005
From: tim at peierls.net (Tim Peierls)
Date: Tue Oct 25 17:17:09 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <31f2a7bd0510251321r7800b570tc01779e6d9f390f4@mail.gmail.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FC0254@zcarhxm2.corp.nortel.com>
	<31f2a7bd0510251321r7800b570tc01779e6d9f390f4@mail.gmail.com>
Message-ID: <435EA0A9.4070101@peierls.net>

Joe Bowbeer wrote:
> I meant:
>
> executor.execute(runnable)
>
> This gives you the most control over the actual task that is
> executing.  For example, your task could be a custom FutureTask with a
> done() method specialized for handling OOME.
>
> To isolate the impact, you could create a "task factory" that took a
> runnable and returned a FutureTask customized for OOME handling.
>
> Or (and this is probably the best solution for you) you could create a
> custom ExecutorService that did this for you, by extending
> AbstractExecutorService.

Specifically, with something like this:

   public OomeHandler {
       void handleOutOfMemoryError(OutOfMemoryError e);
   }

   public class OomeHandlingExecutorService extends AbstractExecutorService {
       private final ExecutorService exec;
       private final OomeHandler handler;
       public OomeHandlingExecutorService(ExecutorService exec, OomeHandler handler) {
           this.exec = exec;
       }
       public void execute(Runnable runnable) {
           exec.execute(runnable);
       }
       public <V> FutureTask<V> submit(Callable<V> callable) {
           if (callable == null) throw new NullPointerException();
           FutureTask<T> ftask = new OomeHandlingFutureTask<V>(callable);
           execute(ftask);
           return ftask;
       }
       public <V> FutureTask<V> submit(Runnable runnable, V result) {
           if (runnable == null) throw new NullPointerException();
           FutureTask<T> ftask = new OomeHandlingFutureTask<V>(runnable);
           execute(ftask);
           return ftask;
       }
       public FutureTask<?> submit(Runnable runnable) {
           if (runnable == null) throw new NullPointerException();
           FutureTask<Object> ftask = new OomeHandlingFutureTask<Object>(runnable, null);
           execute(ftask);
           return ftask;
       }

       // invokeAny/All and lifecycle methods all delegate to exec

       private static class OomeHandlingFutureTask<V> extends FutureTask {
           private final OomeHandler handler;
           OomeHandlingFutureTask(Callable<V> callable, OomeHandler handler) {
               super(callable);
               this.handler = handler;
           }
           OomeHandlingFutureTask(Runnable runnable, V result) {
               super(runnable, result);
               this.handler = handler;
           }
           protected void done() {
               try {
                   get();
               } catch (CancellationException e) {
                   // ignore
               } catch (ExecutionException e) {
                   Throwable t = e.getCause();
                   if (handler != null && t instanceof OutOfMemoryError)
                       handler.handleOutOfMemoryError((OutOfMemoryError) t);
               }
           }
       }
   }

Delegating the invokeAll and invokeAny methods above means that invoked tasks don't
get OOME handling. This can be addressed with copy-and-paste, but as Joe says:

> This sort of extension is made even easier in 1.6 Mustang with the
> addition of the newTaskFor method:
>
>     protected <T> RunnableFuture<T> newTaskFor(Runnable runnable, T value) {
>         return new FutureTask<T>(runnable, value);
>     }

For example, you could subclass TPE so that it returns OomeHandlingFutureTask, which is much more 
compact that the code above.

--tim

From dawidk at mathcs.emory.edu  Wed Oct 26 00:17:18 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed Oct 26 00:17:54 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <435EA0A9.4070101@peierls.net>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FC0254@zcarhxm2.corp.nortel.c
	om><31f2a7bd0510251321r7800b570tc01779e6d9f390f4@mail.gmail.com> 
	<435EA0A9.4070101@peierls.net>
Message-ID: <435F034E.5090806@mathcs.emory.edu>

Tim Peierls wrote:

> Joe Bowbeer wrote:
>
>> I meant:
>>
>> executor.execute(runnable)
>>
>> This gives you the most control over the actual task that is
>> executing.  For example, your task could be a custom FutureTask with a
>> done() method specialized for handling OOME.
>>
>> To isolate the impact, you could create a "task factory" that took a
>> runnable and returned a FutureTask customized for OOME handling.
>>
>> Or (and this is probably the best solution for you) you could create a
>> custom ExecutorService that did this for you, by extending
>> AbstractExecutorService.
>
>
> Specifically, with something like this:
>
...

>       public void execute(Runnable runnable) {
>           exec.execute(runnable);
>       }

...

This seems to require "And", not "Or": in the above, you still cannot 
pass a FutureTask to execute(), or it will swallow the throwable. 
Instead, you need to use OomeHandlingFutureTask, or just a plain 
runnables that do not catch Throwable.

Regards,
Dawid


From dawidk at mathcs.emory.edu  Wed Oct 26 00:24:55 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed Oct 26 00:25:32 2005
Subject: [concurrency-interest] Exception handling & Executors
In-Reply-To: <31f2a7bd0510251255m4f685f60l173908434dbfab1d@mail.gmail.com>
References: <0BDFFF51DC89434FA33F8B37FCE363D504FBFF03@zcarhxm2.corp.nortel.c
	om><31f2a7bd0510251024w7e33049bg675a23d20f5d65a0@mail.gmail.com><435E89A2.
	4	030300@sungardsct.com> 
	<31f2a7bd0510251255m4f685f60l173908434dbfab1d@mail.gmail.com>
Message-ID: <435F0517.4080109@mathcs.emory.edu>

Joe Bowbeer wrote:

>On 10/25/05, Jan Nielsen <jnielsen@sungardsct.com> wrote:
>  
>
>>With respect to your comment below about not being able to do anything
>>with an out-of-memory condition, could you not reliably release, for
>>example, cached data in an OutOfMemoryError handler:
>>
>>    
>>
>In my experience, recovery from OOME is dicey at best and therefore
>the only robust approach is to design and implement the app in such as
>way that OOME never happens.
>
>(Note: real-time Java tells a different story.)
>
>I haven't investigated carefully why OOME recovery is dicey, but since
>nothing is supposed to catch it I assume that once it is thrown the
>system is in an unrecoverable state: no cleanup has been performed
>along the way, connections and streams are broken, third-party
>libraries are left in some indeterminate state, etc.
>
>  
>
I think that you have just said precisely why it is dicey (at least it 
convinced me): OOME can be thrown by any memory allocation attempt, 
including that performed by the library code that you don't control. If 
the allocation was attempted inside a synchronized block, the error 
causes the abrupt completion, and may leave shared data structures in an 
inconsistent state, since no library is actually written to guard 
against OOME. It is a bit similar to dangers of Thread.stop(), although 
just a little bit less bad, since OOME is not asynchronous.

>If your app could control where the OOME occurs then it would have a
>better chance to catch it and recover -- by releasing buffers, for
>example.  But I don't have a lot of faith that this can be done
>reliably (or efficiently).
>
>  
>
If something can be safely released to recover from OOME, it is better 
to make that something soft -referenceable, in which case it will be 
automatically released, *preventing* OOME, when you're running low on 
memory. Soft-referenced objects are released only if the system cannot 
otherwise satisfy an allocation request.

The problem with OOME is that you never know which thread will 
experience it first if you are running low on memory.

Regards,
Dawid


From hanmlw at gmail.com  Wed Oct 26 05:54:11 2005
From: hanmlw at gmail.com (ray)
Date: Wed Oct 26 05:54:30 2005
Subject: [concurrency-interest] LinkedBlockingQueue and iterator question
Message-ID: <435F5243.5030102@gmail.com>

When a thread is using an iterator which point to an object in a 
LinkedBlockingQueue, can other threads remove this object from the 
queue? I think not, because if so, the iterator may point to a object 
which is not in the queue. Right? Thanks
From dl at cs.oswego.edu  Wed Oct 26 07:54:35 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Oct 26 07:56:27 2005
Subject: [concurrency-interest] LinkedBlockingQueue and iterator question
In-Reply-To: <435F5243.5030102@gmail.com>
References: <435F5243.5030102@gmail.com>
Message-ID: <435F6E7B.1040004@cs.oswego.edu>

ray wrote:
> When a thread is using an iterator which point to an object in a 
> LinkedBlockingQueue, can other threads remove this object from the 
> queue? 

Yes, they can. This is a property of "weakly consistent iterators"
See
http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/package-summary.html
which says in part:

    Most concurrent Collection implementations (including most Queues)
    also differ from the usual java.util conventions in that their
    Iterators provide weakly consistent rather than fast-fail traversal.
    A   weakly consistent iterator is thread-safe, but does not
    necessarily freeze the collection while iterating, so it may (or may
    not) reflect any updates since the iterator was created.

This means that an iterator need not reflect any concurrent updates
performed since the iterator was constructed. For example, one
legal implementation, that isn't actually used in LinkedBlockingQueue,
is to perform toArray() on the queue upon iterator construction, and
just iterate over that array. (The actual implementation is one
in which you may see some additions and removals since iterator
construction.)

This is the best policy we know.


-Doug
From gergg at cox.net  Wed Oct 26 10:56:22 2005
From: gergg at cox.net (Gregg Wonderly)
Date: Wed Oct 26 10:57:05 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435E91C7.3080300@quiotix.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>	<435E7A13.8090008@cytetech.com>
	<435E91C7.3080300@quiotix.com>
Message-ID: <435F9916.4030109@cox.net>

Brian Goetz wrote:
>  > I don't believe this is a valid optimization.  If the visibility of
>  > stopped is such that it can be referenced outside of the context the
>  > compiler is optimizing, then it can't possibly make this change to the
>  > code.  In that code, it would be perfectly valid for someone to code
> 
> You would be incorrect, then.
> 
> To put it in terms of the new JMM: the compiler can prove that the only 
> writes to a non-volatile 'stopped' that can happen-before the next read 
> to stopped occur in the body of the loop.  If it can prove that there 
> are no such writes in the body of the loop, it becomes loop-invariant 
> and can be hoisted.

I won't dispute that it can do this.  My argument is that hoisting the loop 
control as a loop invariant, is almost never what a user wants, unless they are 
specifying a constant for control.  Typically there is a mistake in the 
software, as we are discussing here, which needs to be fixed.  People make 
mistakes.  They are imperfect.  It's fun and challenging to do the work to make 
your optimizer this smart.  I just think JIT design should take a safe approach 
to considering the intent of a user, or allow the user to turn it off (which we 
can still in Sun's JVM).

If I decide that I want my software loop to be controlled by a lazy read on an 
unsynchornized/non-volatile value so that eventually, it changes its behavior 
based on that value, but I don't care when, there is no means to do that given 
what the JMM says.

I have to undergo the potential system impact of using volatile/synchronized 
access.  For highspeed, active application logic, where large areas of memory 
are touched, a user today, casually familar with typical caching will guess that 
they'll eventually see changes.  The JMM says that this can't be guarenteed.

I am not sure that we are actually adding value to the Java proposition by 
creating the expectation from the JITs perspective that all out optimizations 
are possible now that volatile has a meaning.  I understand, potentially, how 
important that can be to performance.  I'm just frightened by some of the 
thoughts that go through my head based on some of the Java software that I've 
seen over the years.

I'm lamenting more about the state of things, then trying to argue that my 
thinking should somehow be right.

Thanks for putting down words and examples for others to read here to help them 
understand all the ins and outs that they need to consider.

Gregg Wonderly
From dl at cs.oswego.edu  Wed Oct 26 11:11:23 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Oct 26 11:13:14 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435F9916.4030109@cox.net>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>	<435E7A13.8090008@cytetech.com>	<435E91C7.3080300@quiotix.com>
	<435F9916.4030109@cox.net>
Message-ID: <435F9C9B.8030703@cs.oswego.edu>

Gregg Wonderly wrote:
> 
> If I decide that I want my software loop to be controlled by a lazy read 
> on an unsynchornized/non-volatile value so that eventually, it changes 
> its behavior based on that value, but I don't care when, there is no 
> means to do that given what the JMM says.

Most of the posts so far focussed on correctness, which is of
course primary. Here are a couple of notes on performance side:

You should find it comforting that on nearly all processors,
a volatile read costs no more than any other kind of read,
except that compiler/VM/processors cannot retain/cache values in
registers etc. Since you seem to want exactly this effect in
these cases, volatile is just want you want conceptually as well.

Volatile writes on the other hand cost almost as much in terms of
underlying barriers as synchronized blocks. Similarly for compareAndSet
on atomics. Although both are still are normally cheaper than locks 
because they never block on locks when contended. Except that if
you have a lot of memory contention retrying compareAndSets
and the like, you are usually better off using locks or related
synchronizations to cause some of those threads to block rather than
fight for cache lines etc.

So, basically, the story on the performance side meshes pretty
well with that on the correctness side. So don't worry, just be
happy and obey the rules :-)

-Doug

From chris.purcell.39 at gmail.com  Wed Oct 26 11:43:29 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 26 11:43:59 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435F9C9B.8030703@cs.oswego.edu>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>	<435E7A13.8090008@cytetech.com>	<435E91C7.3080300@quiotix.com>
	<435F9916.4030109@cox.net> <435F9C9B.8030703@cs.oswego.edu>
Message-ID: <50c4bf1ebf675f746c526d3f61e7b1e5@gmail.com>

> You should find it comforting that on nearly all processors, a 
> volatile read costs no more than any other kind of read

I was under the impression that more recent Intels added a read-read 
barrier to the instruction set for this purpose: only older chips don't 
reorder reads.

On the other hand, x86 guarantees write ordering, making write-write 
barriers (and hence volatile writes) cheap-as-free.

Am I missing something?

Cheers,
Chris

From dl at cs.oswego.edu  Wed Oct 26 12:14:37 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Oct 26 12:16:31 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <50c4bf1ebf675f746c526d3f61e7b1e5@gmail.com>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>	<435E7A13.8090008@cytetech.com>	<435E91C7.3080300@quiotix.com>
	<435F9916.4030109@cox.net> <435F9C9B.8030703@cs.oswego.edu>
	<50c4bf1ebf675f746c526d3f61e7b1e5@gmail.com>
Message-ID: <435FAB6D.1030800@cs.oswego.edu>

Chris Purcell wrote:
>>
> 
> Am I missing something? 


I believe that the most common technique for supporting for volatiles
etc in JVMs is basically the one laid out in the "recipes" section of:
   http://gee.cs.oswego.edu/dl/jmm/cookbook.html

-Doug

From chris.purcell.39 at gmail.com  Wed Oct 26 12:25:05 2005
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Wed Oct 26 12:25:32 2005
Subject: [concurrency-interest] Synchronization of data read by multiple
	threads
In-Reply-To: <435FAB6D.1030800@cs.oswego.edu>
References: <PANGOSERVERWwr2UAaT00000092@pangonetworks.com>	<435E4C89.5090809@cs.purdue.edu>	<435E7A13.8090008@cytetech.com>	<435E91C7.3080300@quiotix.com>
	<435F9916.4030109@cox.net> <435F9C9B.8030703@cs.oswego.edu>
	<50c4bf1ebf675f746c526d3f61e7b1e5@gmail.com>
	<435FAB6D.1030800@cs.oswego.edu>
Message-ID: <64b707a40cc02df6e6760e12deb8df15@gmail.com>

> I believe that the most common technique for supporting for volatiles
> etc in JVMs is basically the one laid out in the "recipes" section of:
>   http://gee.cs.oswego.edu/dl/jmm/cookbook.html

Ah! More restrictions on a volatile write than I'd thought.

That processor barrier table will come in handy for me. Many thanks.

Chris

From hans.boehm at hp.com  Wed Oct 26 13:37:38 2005
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed Oct 26 13:51:01 2005
Subject: [concurrency-interest] Synchronization of data read by
	multiplethreads
Message-ID: <65953E8166311641A685BDF71D8658266C19DB@cacexc12.americas.cpqcorp.net>

I think there is another related issue here, which came up in the JSR133
discussions.

Assume you have two threads (stopped initially true):

Thread 1:

while (stopped) {}

Thread 2:

...
stopped = false;

Even if the compiler doesn't "optimize" thread 1 into an infinite loop,
there is no guarantee that a uniprocessor scheduler would ever schedule
thread 2 after thread 1 gets a chance to run.  Indeed, a number of older
JVMs wouldn't ever run thread 2 again, and I think some current more
specialized JVMs still won't.  These two scenarios are not quite
indistinguishable in general, but they're close.  Thus, compiler
optimizations aside, code like this is a bit brittle, even if stopped is
declared volatile.

If you wanted to prevent the compiler optimization, things like this
make that tricky to specify (or to take advantage of).

Hans

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu 
> [mailto:concurrency-interest-bounces@cs.oswego.edu] On Behalf 
> Of Gregg Wonderly
> Sent: Wednesday, October 26, 2005 7:56 AM
> To: concurrency-interest@altair.cs.oswego.edu
> Subject: Re: [concurrency-interest] Synchronization of data 
> read by multiplethreads
> 
> 
> Brian Goetz wrote:
> >  > I don't believe this is a valid optimization.  If the 
> visibility of  
> > > stopped is such that it can be referenced outside of the 
> context the  
> > > compiler is optimizing, then it can't possibly make this 
> change to 
> > the  > code.  In that code, it would be perfectly valid for 
> someone to 
> > code
> > 
> > You would be incorrect, then.
> > 
> > To put it in terms of the new JMM: the compiler can prove that the 
> > only
> > writes to a non-volatile 'stopped' that can happen-before 
> the next read 
> > to stopped occur in the body of the loop.  If it can prove 
> that there 
> > are no such writes in the body of the loop, it becomes 
> loop-invariant 
> > and can be hoisted.
> 
> I won't dispute that it can do this.  My argument is that 
> hoisting the loop 
> control as a loop invariant, is almost never what a user 
> wants, unless they are 
> specifying a constant for control.  Typically there is a 
> mistake in the 
> software, as we are discussing here, which needs to be fixed. 
>  People make 
> mistakes.  They are imperfect.  It's fun and challenging to 
> do the work to make 
> your optimizer this smart.  I just think JIT design should 
> take a safe approach 
> to considering the intent of a user, or allow the user to 
> turn it off (which we 
> can still in Sun's JVM).
> 
> If I decide that I want my software loop to be controlled by 
> a lazy read on an 
> unsynchornized/non-volatile value so that eventually, it 
> changes its behavior 
> based on that value, but I don't care when, there is no means 
> to do that given 
> what the JMM says.
> 
> I have to undergo the potential system impact of using 
> volatile/synchronized 
> access.  For highspeed, active application logic, where large 
> areas of memory 
> are touched, a user today, casually familar with typical 
> caching will guess that 
> they'll eventually see changes.  The JMM says that this can't 
> be guarenteed.
> 
> I am not sure that we are actually adding value to the Java 
> proposition by 
> creating the expectation from the JITs perspective that all 
> out optimizations 
> are possible now that volatile has a meaning.  I understand, 
> potentially, how 
> important that can be to performance.  I'm just frightened by 
> some of the 
> thoughts that go through my head based on some of the Java 
> software that I've 
> seen over the years.
> 
> I'm lamenting more about the state of things, then trying to 
> argue that my 
> thinking should somehow be right.
> 
> Thanks for putting down words and examples for others to read 
> here to help them 
> understand all the ins and outs that they need to consider.
> 
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list 
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From hanmlw at gmail.com  Wed Oct 26 21:29:15 2005
From: hanmlw at gmail.com (ray)
Date: Wed Oct 26 21:29:27 2005
Subject: [concurrency-interest] LinkedBlockingQueue and iterator question
In-Reply-To: <435F6E7B.1040004@cs.oswego.edu>
References: <435F5243.5030102@gmail.com> <435F6E7B.1040004@cs.oswego.edu>
Message-ID: <43602D6B.1000608@gmail.com>

Thanks for your reply.

If I only want to iterate the objets IN the queue(other threads may 
remove objects from the queue using the method of 
LinkedBlockingQueue.take at the same time), should I lock the queue? 
That seems too expensive. Or I can use toArray and iterate on that 
array, but also seems not better.  Other ideas?

Doug Lea wrote:

> ray wrote:
>
>> When a thread is using an iterator which point to an object in a 
>> LinkedBlockingQueue, can other threads remove this object from the 
>> queue? 
>
>
> Yes, they can. This is a property of "weakly consistent iterators"
> See
> http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/package-summary.html 
>
> which says in part:
>
>    Most concurrent Collection implementations (including most Queues)
>    also differ from the usual java.util conventions in that their
>    Iterators provide weakly consistent rather than fast-fail traversal.
>    A   weakly consistent iterator is thread-safe, but does not
>    necessarily freeze the collection while iterating, so it may (or may
>    not) reflect any updates since the iterator was created.
>
> This means that an iterator need not reflect any concurrent updates
> performed since the iterator was constructed. For example, one
> legal implementation, that isn't actually used in LinkedBlockingQueue,
> is to perform toArray() on the queue upon iterator construction, and
> just iterate over that array. (The actual implementation is one
> in which you may see some additions and removals since iterator
> construction.)
>
> This is the best policy we know.
>
>
> -Doug
>
From dholmes at dltech.com.au  Wed Oct 26 21:52:31 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Oct 26 21:53:01 2005
Subject: [concurrency-interest] LinkedBlockingQueue and iterator question
In-Reply-To: <43602D6B.1000608@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOECEGHAA.dholmes@dltech.com.au>

Ray,

> If I only want to iterate the objets IN the queue(other threads may
> remove objects from the queue using the method of
> LinkedBlockingQueue.take at the same time), should I lock the queue?

You can't "lock" the queue. There is no single lock that protects access to
the queue - this is a concurrent data structure that supports concurrent put
and take operations.

I'm not clear what your requirements are. You say you want to iterate
objects IN the queue, but at the same time you are allowing the queue to be
modified concurrently - these conflict. You cannot guarantee that an item
returned by the iterator (or through an entry in toArray()) is still in
queue at the time you access it - you only know that it was in the queue.

If you need to exclude access to the queue while iterating you need a
higher-level protocol to provide that in your application. The simplest way
to get such a protocol is to not use a concurrent data structure like
LinkedBlockingQueue but, for example, a synchronizedList wrapping a
LinkedList.

What you need to do really depends on your exact requirements.

Cheers,
David Holmes

From hanmlw at gmail.com  Wed Oct 26 22:24:23 2005
From: hanmlw at gmail.com (ray)
Date: Wed Oct 26 22:24:34 2005
Subject: [concurrency-interest] LinkedBlockingQueue and iterator question
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOECEGHAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCOECEGHAA.dholmes@dltech.com.au>
Message-ID: <43603A57.5030606@gmail.com>




>I'm not clear what your requirements are. You say you want to iterate
>objects IN the queue, but at the same time you are allowing the queue to be
>modified concurrently - these conflict. You cannot guarantee that an item
>returned by the iterator (or through an entry in toArray()) is still in
>queue at the time you access it - you only know that it was in the queue.
>  
>
This is exactly my requirements. Now I see it's impossible using 
LinkedBlockingQueue. Thanks!

>If you need to exclude access to the queue while iterating you need a
>higher-level protocol to provide that in your application. The simplest way
>to get such a protocol is to not use a concurrent data structure like
>LinkedBlockingQueue but, for example, a synchronizedList wrapping a
>LinkedList.
>  
>
One reason I use LinkedBlockingQueue is that it implements 
BlockingQueue. Maybe I should write my data structure which extends 
LinkedList and implements BlockingQueue.

>
>  
>
From Ryan.LeCompte at pangonetworks.com  Sat Oct 29 09:55:47 2005
From: Ryan.LeCompte at pangonetworks.com (Ryan LeCompte)
Date: Sat Oct 29 09:56:32 2005
Subject: [concurrency-interest] RE: Synchronization of data read by multiple
	threads
Message-ID: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetworks.com>


Hello all,

 

As a follow-on to my original question
 I have another scenario where I?m not entirely sure if synchronization/volatile is needed. Let?s say we have the following:

 

Class Test implements Runnable {

   Private boolean instantiated = false;

    

   Public Test() {

      Instantiated = true;

   }

 

   private boolean isInstantiated() {

       Return instantiated;

    }

 

     Public void run() {

         If (isInstantiated()) {

             // do something

         }

     }

}

 

And then somewhere in the main thread we do the following:

 

Test t = new Test();

Thread th = new Thread(t);

t.start();

 

Is it possible that the new thread that is running, which it invokes isInstantiated() will see the value ?false? instead of ?true? ? Does it have to be declared volatile? Do all variables of the class have to be declared ?volatile? in order for the newly instantiated thread to see their most recently written values? Note that the only time the class is used by different threads would be in the main thread when it is instantiated (new Test()), and then after that the only thread ever reading that particular value would be the newly instantiated Thread.

 

Ryan

 


--------------------------------------------------------------------------------

From: Ryan LeCompte [mailto:ryan.lecompte@pangonetworks.com] 
Sent: Tuesday, October 25, 2005 9:17 AM
To: 'concurrency-interest@altair.cs.oswego.edu'
Subject: Synchronization of data read by multiple threads

 

Hello all,

 

I know that one has two options for ?protecting? data that can be potentially read/written by multiple threads. In order for the main thread to always see the most recently written value, one must declare the variable as ?volatile? or synchronize all access to it, such as:

 

synchronized void stop() {

   stopped = true;

}

 

synchronized boolean isStopped() {

   return stopped;

}

 

However, if the ?synchronized? approach is taken, does it have to be at such a granular level? Or can it suffice that whenever the variable ?stopped? is used, that it?s at least protected by SOME lock? For example, if ?stopped? is only directly referenced in three methods that perform various operations, can all three methods be declared as ?synchronized? and the above two methods (stop() / isStopped()) simply removed? Or do we always need to have ?synchronized accessors? for the variable in question? Also, what happens if there are three methods that use the ?stopped? variable, but they are using different locks? For example, let?s say method1 uses ?stopped? in a synchronized block on LOCK1, and method2 uses ?stopped? in a synchronized block on LOCK2, and method3 uses ?stopped? in a synchronized block on LOCK3. Will we still have the same effect as simply declaring the variable as ?volatile? here?

 

Thanks,

Ryan

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051029/4e82b288/attachment.htm
From jmanson at cs.purdue.edu  Sat Oct 29 10:40:31 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Sat Oct 29 10:41:28 2005
Subject: [concurrency-interest] RE: Synchronization of data read by
	multiple threads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetworks.com>
Message-ID: <436389DF.5090509@cs.purdue.edu>

Ryan LeCompte wrote:
> Is it possible that the new thread that is running, which it invokes
> isInstantiated() will see the value ?false? instead of ?true? ? Does
> it have to be declared volatile? Do all variables of the class have
> to be declared ?volatile? in order for the newly instantiated thread
> to see their most recently written values? Note that the only time
> the class is used by different threads would be in the main thread
> when it is instantiated (new Test()), and then after that the only
> thread ever reading that particular value would be the newly
> instantiated Thread.
> 

You don't need any additional synchronization in this case.  You can 
conceptualize the call to Thread.start() as being a volatile write, and 
the actual beginning of the thread as a volatile read.  So everything 
that happens before the call to Thread.start() (including object 
initialization) is ordered before and visible to everything that happens 
after the start of the thread.

					Jeremy

From larryr at saturn.sdsu.edu  Sat Oct 29 13:00:23 2005
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Sat Oct 29 13:00:55 2005
Subject: [concurrency-interest] RE: Synchronization of data read by multiple
	threads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetworks.com>
Message-ID: <20051029170023.18775.qmail@home19.riedel.org>


> Class Test implements Runnable {
>    Private boolean instantiated = false;
>    Public Test() { Instantiated = true; }
>    private boolean isInstantiated() { Return instantiated; }
>    Public void run() { 
>          If (isInstantiated()) 
>             // do something 
> 
> And then somewhere in the main thread we do the following:
> 
> Test t = new Test();
> Thread th = new Thread(t);
> t.start();

I would like to think there will only ever be one thread
aware of the existence of an object before it has
been completely constructed (assuming, of course, the
constructor does not give away references to itself),
and consequently no other thread will see any state the
object was in before its construction was complete.  In
other words all these things about "happens before" only
apply to changes which take place after another thread
has been given a reference to the object, and that can
only happen after its construction is complete.

But is this correct?


Larry

From dawidk at mathcs.emory.edu  Sat Oct 29 13:19:04 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat Oct 29 13:19:47 2005
Subject: [concurrency-interest] RE: Synchronization of data read by 
	multiplethreads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetwor
	ks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3C9@pangoserver.pangonetwork
	s.com>
Message-ID: <4363AF08.7070008@mathcs.emory.edu>

Ryan LeCompte wrote:

>
> Hello all,
>
>
>
> As a follow-on to my original question? I have another scenario where 
> I?m not entirely sure if synchronization/volatile is needed. Let?s say 
> we have the following:
>
>
>
> Class Test implements Runnable {
>
> Private boolean instantiated = false;
>
>
>
> Public Test() {
>
> Instantiated = true;
>
> }
>
>
>
> private boolean isInstantiated() {
>
> Return instantiated;
>
> }
>
>
>
> Public void run() {
>
> If (isInstantiated()) {
>
> // do something
>
> }
>
> }
>
> }
>
Just a question: why do you need the "instantiated" field in the first 
place? Do you ever set it to false? If not, it can be made final, and 
consequently, removed completely. If yes, you probably set it from 
another thread, so you may need synchronization after all.

Regards,
Dawid



From David.Biesack at sas.com  Sat Oct 29 13:47:58 2005
From: David.Biesack at sas.com (David J. Biesack)
Date: Sat Oct 29 13:48:32 2005
Subject: [concurrency-interest] spurious wakeups semantics
Message-ID: <200510291747.j9THlwQ23546@mozart.unx.sas.com>


Can someone clarify the semantics of spurious wakeup as noted in java.lang.Object wait() in Java 5?

What happens to the monitor that the thread is waiting on? If another thread T2 has the lock
and a waiting thread T1 wakes up "suriously", what happens to thread T2? Are both threads T1 and T2 now
running, and what risks are there of race conditions? I understand that wait must be guarded
by a loop, but is it not possible that the wait condition is no longer safe from concurrent update?
Do variables used for wait conditions have to be volatile, for example?

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513

From josh at bloch.us  Sat Oct 29 13:56:12 2005
From: josh at bloch.us (Joshua Bloch)
Date: Sat Oct 29 13:56:39 2005
Subject: [concurrency-interest] spurious wakeups semantics
In-Reply-To: <200510291747.j9THlwQ23546@mozart.unx.sas.com>
References: <200510291747.j9THlwQ23546@mozart.unx.sas.com>
Message-ID: <b097ac510510291056w64046a41g98a92284b0f0b887@mail.gmail.com>

David,

Any wakeup, spurious or otherwise, is followed by acquisition of the
intrinsic lock on the object being waited on.  There is no race
condition, and no need for a volatile variable.

        Regards,

        Josh

On 10/29/05, David J. Biesack <David.Biesack@sas.com> wrote:
>
> Can someone clarify the semantics of spurious wakeup as noted in java.lang.Object wait() in Java 5?
>
> What happens to the monitor that the thread is waiting on? If another thread T2 has the lock
> and a waiting thread T1 wakes up "suriously", what happens to thread T2? Are both threads T1 and T2 now
> running, and what risks are there of race conditions? I understand that wait must be guarded
> by a loop, but is it not possible that the wait condition is no longer safe from concurrent update?
> Do variables used for wait conditions have to be volatile, for example?
>
> --
> David J. Biesack     SAS Institute Inc.
> (919) 531-7771       SAS Campus Drive
> http://www.sas.com   Cary, NC 27513
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From Ryan.LeCompte at pangonetworks.com  Sun Oct 30 11:49:05 2005
From: Ryan.LeCompte at pangonetworks.com (Ryan LeCompte)
Date: Sun Oct 30 11:49:40 2005
Subject: [concurrency-interest] RE: Synchronization of data read by
	multiple threads
Message-ID: <2F224C8C9A914A408E318BD1A1507DCB02B3CA@pangoserver.pangonetworks.com>


Okay, that sounds fine. However, what about if the main thread (the one that instantiated Test) continues to read values that were set in the constructor? For example, if the main thread does: t.isInstantiated() later on, will it continue to return the value that was set in the constructor when Test was instantiated? Keep in mind that in this particular scenario the values that the main thread will read never change after being initially set in the constructor of Test. There is no need for making the 'instantiated' variable volatile in this case, right? 

Thanks,
Ryan

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: Saturday, October 29, 2005 10:41 AM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] RE: Synchronization of data read by multiple threads

Ryan LeCompte wrote:
> Is it possible that the new thread that is running, which it invokes
> isInstantiated() will see the value ?false? instead of ?true? ? Does
> it have to be declared volatile? Do all variables of the class have
> to be declared ?volatile? in order for the newly instantiated thread
> to see their most recently written values? Note that the only time
> the class is used by different threads would be in the main thread
> when it is instantiated (new Test()), and then after that the only
> thread ever reading that particular value would be the newly
> instantiated Thread.
> 

You don't need any additional synchronization in this case.  You can 
conceptualize the call to Thread.start() as being a volatile write, and 
the actual beginning of the thread as a volatile read.  So everything 
that happens before the call to Thread.start() (including object 
initialization) is ordered before and visible to everything that happens 
after the start of the thread.

					Jeremy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051030/933436a4/attachment.htm
From jmanson at cs.purdue.edu  Sun Oct 30 12:18:56 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Sun Oct 30 12:19:37 2005
Subject: [concurrency-interest] RE: Synchronization of data read
	by	multiple threads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3CA@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3CA@pangoserver.pangonetworks.com>
Message-ID: <43650080.7080205@cs.purdue.edu>

Ryan LeCompte wrote:
> Okay, that sounds fine. However, what about if the main thread (the
> one that instantiated Test) continues to read values that were set in
> the constructor? For example, if the main thread does:
> t.isInstantiated() later on, will it continue to return the value
> that was set in the constructor when Test was instantiated? Keep in
> mind that in this particular scenario the values that the main thread
> will read never change after being initially set in the constructor
> of Test. There is no need for making the 'instantiated' variable
> volatile in this case, right?
> 

Are you asking, "If one thread sets a field, and then later reads that 
field, do I have to do additional synchronization?"  The answer to that 
is no.  You don't need synchronization if you are only dealing with one 
thread.

By the way, if you are setting a field once, in the constructor, and 
then never changing it, then it is a good idea to make the field final. 
  All reads of final fields return the correctly constructed value, 
without synchronization (unless a reference to the constructed object 
escapes the constructor, or you are using some weird custom 
deserialization protocols).

					Jeremy
From dawidk at mathcs.emory.edu  Sun Oct 30 13:33:47 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sun Oct 30 13:34:50 2005
Subject: [concurrency-interest] RE: Synchronization of data readby	multiple
	threads
In-Reply-To: <43650080.7080205@cs.purdue.edu>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3CA@pangoserver.pangonetwork	s.com>
	<43650080.7080205@cs.purdue.edu>
Message-ID: <4365120B.2010602@mathcs.emory.edu>

Jeremy Manson wrote:

> Ryan LeCompte wrote:
>
>> Okay, that sounds fine. However, what about if the main thread (the
>> one that instantiated Test) continues to read values that were set in
>> the constructor? For example, if the main thread does:
>> t.isInstantiated() later on, will it continue to return the value
>> that was set in the constructor when Test was instantiated? Keep in
>> mind that in this particular scenario the values that the main thread
>> will read never change after being initially set in the constructor
>> of Test. There is no need for making the 'instantiated' variable
>> volatile in this case, right?
>>
>
> Are you asking, "If one thread sets a field, and then later reads that 
> field, do I have to do additional synchronization?"  The answer to 
> that is no.  You don't need synchronization if you are only dealing 
> with one thread.
>
> By the way, if you are setting a field once, in the constructor, and 
> then never changing it, then it is a good idea to make the field 
> final.  All reads of final fields return the correctly constructed 
> value, without synchronization (unless a reference to the constructed 
> object escapes the constructor, or you are using some weird custom 
> deserialization protocols).
>
And again, if this field is final, it means it is not needed here at 
all. Its value is always true, so isInstantiated() always returns true, 
so the test in the run() method will always yield true. If you managed 
to invoked an instance method on an object, it means that this object 
has been instantiated.

The existence of a final field like this does only have sense if you set 
it to different values in the constructor, e.g. depending on constructor 
parameters. But if you always set it to true, you can as well get rid of it.

Regards,
Dawid

From Ryan.LeCompte at pangonetworks.com  Sun Oct 30 16:13:06 2005
From: Ryan.LeCompte at pangonetworks.com (Ryan LeCompte)
Date: Sun Oct 30 16:13:39 2005
Subject: [concurrency-interest] RE: Synchronization of data read
	by	multiple threads
Message-ID: <2F224C8C9A914A408E318BD1A1507DCB02B3CB@pangoserver.pangonetworks.com>

Okay, this sounds reasonable. So, if I make the variables "final" and they are assigned their values in the constructor, then there is no need to make them volatile if they are accessed by multiple threads, correct? Also, what happens in the case of a boolean variable that is declared as a blank final? If it gets a value assigned to it in the constructor, then it keeps that value. Otherwise it just defaults to "false" as the final value, correct?

Thanks,
Ryan

-----Original Message-----
From: Jeremy Manson [mailto:jmanson@cs.purdue.edu] 
Sent: Sunday, October 30, 2005 12:19 PM
To: Ryan LeCompte
Cc: concurrency-interest@altair.cs.oswego.edu
Subject: Re: [concurrency-interest] RE: Synchronization of data read by multiple threads

Ryan LeCompte wrote:
> Okay, that sounds fine. However, what about if the main thread (the
> one that instantiated Test) continues to read values that were set in
> the constructor? For example, if the main thread does:
> t.isInstantiated() later on, will it continue to return the value
> that was set in the constructor when Test was instantiated? Keep in
> mind that in this particular scenario the values that the main thread
> will read never change after being initially set in the constructor
> of Test. There is no need for making the 'instantiated' variable
> volatile in this case, right?
> 

Are you asking, "If one thread sets a field, and then later reads that 
field, do I have to do additional synchronization?"  The answer to that 
is no.  You don't need synchronization if you are only dealing with one 
thread.

By the way, if you are setting a field once, in the constructor, and 
then never changing it, then it is a good idea to make the field final. 
  All reads of final fields return the correctly constructed value, 
without synchronization (unless a reference to the constructed object 
escapes the constructor, or you are using some weird custom 
deserialization protocols).

					Jeremy

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051030/bd53145d/attachment.htm
From jmanson at cs.purdue.edu  Sun Oct 30 16:46:01 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Sun Oct 30 16:47:19 2005
Subject: [concurrency-interest] RE: Synchronization of data
	read	by	multiple threads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3CB@pangoserver.pangonetworks.com>
References: <2F224C8C9A914A408E318BD1A1507DCB02B3CB@pangoserver.pangonetworks.com>
Message-ID: <43653F19.8030504@cs.purdue.edu>

Ryan LeCompte wrote:
> Okay, this sounds reasonable. So, if I make the variables "final" and
> they are assigned their values in the constructor, then there is no
> need to make them volatile if they are accessed by multiple threads,
> correct? 

True, but be careful when doing this:

class Foo {
   static Foo global;
   final boolean f;
   Foo() {
     global = this;  // dangerous!
     f = true;
   }
}

In that case, you are "publishing" a reference to the constructed object 
inside the constructor.  Another thread can access the constructed 
object by reading global before the assignment to f happens.  Also, in 
this case:

   Foo() {
     f = true;
     global = this;  // also dangerous!
   }

The compiler can reorder the writes, and you end up with the same problem.

The way to avoid this is not to publish the constructed object inside 
the constructor.  There are guarantees in that case.

 > Also, what happens in the case of a boolean variable that is
> declared as a blank final? If it gets a value assigned to it in the
> constructor, then it keeps that value. Otherwise it just defaults to
> "false" as the final value, correct?

javac won't compile it if you don't definitely assign to it.

					Jeremy
From dholmes at dltech.com.au  Sun Oct 30 18:25:09 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Sun Oct 30 18:25:49 2005
Subject: [concurrency-interest] RE: Synchronization of data
	readby	multiple threads
In-Reply-To: <2F224C8C9A914A408E318BD1A1507DCB02B3CB@pangoserver.pangonetworks.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEGMGHAA.dholmes@dltech.com.au>

RE: [concurrency-interest] RE: Synchronization of data read by multiple
threadsRyan,

You seem to be missing some language fundamentals here. A final variable is
one that can never have its value change - it is immutable. And as Jeremy
informed you a final variable must always be definitely assigned otherwise
your code won't compile.

volatile variables are by definition mutable variables that change their
value. It is a compile time error to declare a variable both final and
volatile.

You need to use volatile variables if the variable will be accessed by
multiple threads, at least one of which will update the value of the
variable, and there is no other synchronization controlling access to the
variable.

David Holmes

-----Original Message-----
From: concurrency-interest-bounces@cs.oswego.edu
[mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Ryan
LeCompte
Sent: Monday, 31 October 2005 7:13 AM
To: concurrency-interest@altair.cs.oswego.edu
Subject: RE: [concurrency-interest] RE: Synchronization of data readby
multiple threads


  Okay, this sounds reasonable. So, if I make the variables "final" and they
are assigned their values in the constructor, then there is no need to make
them volatile if they are accessed by multiple threads, correct? Also, what
happens in the case of a boolean variable that is declared as a blank final?
If it gets a value assigned to it in the constructor, then it keeps that
value. Otherwise it just defaults to "false" as the final value, correct?

  Thanks,
  Ryan

  -----Original Message-----
  From: Jeremy Manson [mailto:jmanson@cs.purdue.edu]
  Sent: Sunday, October 30, 2005 12:19 PM
  To: Ryan LeCompte
  Cc: concurrency-interest@altair.cs.oswego.edu
  Subject: Re: [concurrency-interest] RE: Synchronization of data read by
multiple threads

  Ryan LeCompte wrote:
  > Okay, that sounds fine. However, what about if the main thread (the
  > one that instantiated Test) continues to read values that were set in
  > the constructor? For example, if the main thread does:
  > t.isInstantiated() later on, will it continue to return the value
  > that was set in the constructor when Test was instantiated? Keep in
  > mind that in this particular scenario the values that the main thread
  > will read never change after being initially set in the constructor
  > of Test. There is no need for making the 'instantiated' variable
  > volatile in this case, right?
  >

  Are you asking, "If one thread sets a field, and then later reads that
  field, do I have to do additional synchronization?"  The answer to that
  is no.  You don't need synchronization if you are only dealing with one
  thread.

  By the way, if you are setting a field once, in the constructor, and
  then never changing it, then it is a good idea to make the field final.
    All reads of final fields return the correctly constructed value,
  without synchronization (unless a reference to the constructed object
  escapes the constructor, or you are using some weird custom
  deserialization protocols).

                                          Jeremy


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20051031/689987af/attachment.htm
From dholmes at dltech.com.au  Sun Oct 30 18:28:28 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Sun Oct 30 18:29:05 2005
Subject: [concurrency-interest] RE: Synchronization of data read by
	multiplethreads
In-Reply-To: <20051029170023.18775.qmail@home19.riedel.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEGMGHAA.dholmes@dltech.com.au>

Larry,

> I would like to think there will only ever be one thread
> aware of the existence of an object before it has
> been completely constructed (assuming, of course, the
> constructor does not give away references to itself),
> and consequently no other thread will see any state the
> object was in before its construction was complete.  In
> other words all these things about "happens before" only
> apply to changes which take place after another thread
> has been given a reference to the object, and that can
> only happen after its construction is complete.
>
> But is this correct?

No. If the thread doing the construction publishes the reference to the
newly constructed object in an unsafe manner (ie without synchronization)
then other threads accessing the object need not see the constructed state
of its fields (unless they are final or volatile).

David Holmes

From larryr at saturn.sdsu.edu  Sun Oct 30 19:54:11 2005
From: larryr at saturn.sdsu.edu (Larry Riedel)
Date: Sun Oct 30 19:54:43 2005
Subject: [concurrency-interest] RE: Synchronization of data read by
	multiplethreads
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEGMGHAA.dholmes@dltech.com.au>
Message-ID: <20051031005411.13369.qmail@home19.riedel.org>


> > [...] there will only ever be one thread aware of the existence of
> > an object before it has been completely constructed (assuming, of
> > course, the constructor does not give away references to itself),
> > and consequently no other thread will see any state the object was
> > in before its construction was complete. [...]
> 
> If the thread doing the construction publishes the reference to the
> newly constructed object in an unsafe manner (ie without synchronization)
> then other threads accessing the object need not see the constructed state
> of its fields (unless they are final or volatile).

Would it be easy to provide an example which demonstrates
this (another thread seeing the inchoate state)?


Larry

From dholmes at dltech.com.au  Sun Oct 30 20:10:08 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Sun Oct 30 20:10:42 2005
Subject: [concurrency-interest] RE: Synchronization of data read
	bymultiplethreads
In-Reply-To: <20051031005411.13369.qmail@home19.riedel.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEHAGHAA.dholmes@dltech.com.au>

Larry writes:
> > If the thread doing the construction publishes the reference to the
> > newly constructed object in an unsafe manner (ie without
synchronization)
> > then other threads accessing the object need not see the constructed
> > state of its fields (unless they are final or volatile).
>
> Would it be easy to provide an example which demonstrates
> this (another thread seeing the inchoate state)?

Sure.

 class X {
     public static theX;

     // fields with non-default initialization (or done through constructor
     Object a = new Object();
     int b = 54;
     long c = 0x1234567812345678L;
 }

      Thread-A                                 Thread-B
     X.theX = new X();
                                       print(X.theX.b);

Thread-B could print either 0 or 54.

When ever one thread creates an object and then 'publishes' that reference
for other threads to see, and the publishing does not involve any
synchronization, then there is no happens-before relationship between the
construction of the object and the use of it in a different thread.

The above example can be fixed by making theX volatile, or by providing a
synchronized accessor and settor for theX.

Cheers,
David Holmes

From jbaxter at panscient.com  Mon Oct 31 01:36:22 2005
From: jbaxter at panscient.com (Jonathan Baxter)
Date: Mon Oct 31 03:30:06 2005
Subject: [concurrency-interest] Atomic double arrays
Message-ID: <200510311706.22668.jbaxter@panscient.com>

Is there a reason util.concurrent has no AtomicDoubleArray class?

There is this throw-away line at the end of the package docs: "You can also 
hold floats using Float.floatToIntBits and Float.intBitstoFloat conversions, 
and doubles using Double.doubleToLongBits and Double.longBitsToDouble 
conversions."

But that doesn't seem to help if you want to use the atomic add methods, eg 
AtomicLongArray.getAndAdd(int i, long delta), because for doubles a and b, it 
is generally not true that Double.doubleToLongBits(a) + 
Double.doubleToLongBits(b) = Double.doubleToLongBits(a + b).

Is there another way to do atomic double array ops that doesn't involve using 
synchronize?

Thanks,

Jonathan 
 
-- 
Jonathan Baxter
Panscient Technologies
http://www.panscient.com

From thanot at infovista.com  Mon Oct 31 03:57:28 2005
From: thanot at infovista.com (Thierry Hanot)
Date: Mon Oct 31 03:58:12 2005
Subject: [concurrency-interest] Lock memory/resource Cost 
Message-ID: <F520B214418AD4119F7000508BD90CC207F84083@ivhqsr02>



Hello 

I 'm just wondering what is the memory/system resource cost of
ReentrantLocks or read write lock.

I'm trying to use the concurrent lock when it's possible but there is one
case where I 'm not sure.

I have millions of object in memory and I need to synchronize access to each
one. I cannot use a global lock because the treatment is done by many
threads. So the simple way is to use classic java synchronization.
.....
synchronized(theObject){
	// treatment on theObject 
}
.....


But I want to use 
.....
theObject.getLock().lock();
try{
  // treatment on theObject
}finally{
	theObject.getLock().unlock();
}



-Because it has better performances.
-Because it's more flexible 
	- If I have something to do on many objects
		If I use classic java synchronization 

		List<MyObject> toDo = ...;
		For(MyObject theObject:toDo){
			synchronized(theObject){
				// treatment on theObject 
			}
		}

	

	
		If I use concurrent objects I could do something like
		
		Int cursor_ = 0;
		While(todo.size()>0){
			MyObject theObject = toDo.get(cursor_);
			If(theObject.getLock().tryLock()){
				Try{
					toDo.remove(cursor_);
					// treatment on theObject
				}finally{
					theObject.getLock().unlock();
				}	
			}
			if(toDo.size>0)
			cursor_ = (cursor_+1)%todo.size(); 
		}

But you need to have one lock per objects ( and I  really have millions ...)

So my question is:
 If I put a Lock by object.
	How much system resources will be used (is system resource are
acquired only when something has been done on the lock or when constructing
the object)

	What is the lighter lock implementation in memory?



Thank in advance for any recommendations.

T.Hanot
From dholmes at dltech.com.au  Mon Oct 31 04:44:41 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Oct 31 04:45:33 2005
Subject: [concurrency-interest] Atomic double arrays
In-Reply-To: <200510311706.22668.jbaxter@panscient.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHFGHAA.dholmes@dltech.com.au>

Jonathon,

> Is there a reason util.concurrent has no AtomicDoubleArray class?

Yes because there are no generally available atomic instructions for
operating on floating-point values. So basically your only option is
synchronization using locks.

> There is this throw-away line at the end of the package docs:
> "You can also hold floats using Float.floatToIntBits and
> Float.intBitstoFloat conversions, and doubles using
> Double.doubleToLongBits and Double.longBitsToDouble conversions."
>
> But that doesn't seem to help if you want to use the atomic add
> methods

Right, these techniques allow you to hold float bits and do atomic get/set
and CAS but not general arithmetic - because again there are no atomic
instructions for doing that.

> Is there another way to do atomic double array ops that doesn't
> involve using synchronize?

Nope - locking is really your only option.

Cheers,
David Holmes

From dholmes at dltech.com.au  Mon Oct 31 05:12:49 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Oct 31 05:13:30 2005
Subject: [concurrency-interest] Lock memory/resource Cost 
In-Reply-To: <F520B214418AD4119F7000508BD90CC207F84083@ivhqsr02>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHHGHAA.dholmes@dltech.com.au>

Thierry,

> I 'm just wondering what is the memory/system resource cost of
> ReentrantLocks or read write lock.

Each explicit lock is at least two distinct objects: the lock itself and the
AbstractQueuedSynchronizer instance it uses. Then there's around another
16-20 bytes of state. So compared to a monitor lock that doesn't come out of
the heap, an explicit lock uses a lot of heap memory.

> I have millions of object in memory and I need to synchronize
> access to each one.

That seems impractical, even if you had the memory resources. This level of
locking sounds too fine-grained, and the degree of sharing seems excessive.

But between one global lock and one lock per object there are many other
ways of partitioning things. If you can somehow group your objects in some
way then you could use a lock per group.

But rather than figuring out how to synchronize these millions of objects,
I'd spend some time trying to figure out if you can avoid needing to
synchronize in the first place.

Can you elaborate more on the system you are working on?

> 	- If I have something to do on many objects
> 		If I use classic java synchronization
>
> 		List<MyObject> toDo = ...;
> 		For(MyObject theObject:toDo){
> 			synchronized(theObject){
> 				// treatment on theObject
> 			}
> 		}

Make sure your iterators have consistent ordering, otherwise you could
easily deadlock.

> 		If I use concurrent objects I could do something like
>
> 		Int cursor_ = 0;
> 		While(todo.size()>0){
> 			MyObject theObject = toDo.get(cursor_);
> 			If(theObject.getLock().tryLock()){
> 				Try{
> 					toDo.remove(cursor_);
> 					// treatment on theObject
> 				}finally{
> 					theObject.getLock().unlock();
> 				}
> 			}
> 			if(toDo.size>0)
> 			cursor_ = (cursor_+1)%todo.size();
> 		}

If you can ask each object for its lock (which need not be a distinct lock
per object) then I don't see why the above form is needed instead of:

 	     for(MyObject theObject:toDo){
               Lock l = theObject.getLock();
               l.lock();
               try {
                 ...
               } finally {
                 lock.unlock();
               }
             }
??

> 	How much system resources will be used (is system resource are
> acquired only when something has been done on the lock or when
> constructing the object)

The main resource used will be memory, both in constructing the locks and
anytime a thread needs to block on the lock. Actually blocking of a thread
may need additional low-level resources but these are likely to be needed by
a thread during its lifetime anyway.

Cheers,
David Holmes

From thanot at infovista.com  Mon Oct 31 06:52:02 2005
From: thanot at infovista.com (Thierry Hanot)
Date: Mon Oct 31 06:52:36 2005
Subject: [concurrency-interest] Lock memory/resource Cost 
Message-ID: <F520B214418AD4119F7000508BD90CC207F84089@ivhqsr02>



T.Hanot 


-----Original Message-----
From: David Holmes [mailto:dholmes@dltech.com.au] 
Sent: lundi 31 octobre 2005 11:13
To: Thierry Hanot; concurrency-interest@altair.cs.oswego.edu
Subject: RE: [concurrency-interest] Lock memory/resource Cost 

Thierry,

> I 'm just wondering what is the memory/system resource cost of
> ReentrantLocks or read write lock.

Each explicit lock is at least two distinct objects: the lock itself and the
AbstractQueuedSynchronizer instance it uses. Then there's around another
16-20 bytes of state. So compared to a monitor lock that doesn't come out of
the heap, an explicit lock uses a lot of heap memory.



It's exactly what I was afraid of and why I'm currently using monitor
instead of locks.


> I have millions of object in memory and I need to synchronize
> access to each one.

That seems impractical, even if you had the memory resources. This level of
locking sounds too fine-grained, and the degree of sharing seems excessive.

But between one global lock and one lock per object there are many other
ways of partitioning things. If you can somehow group your objects in some
way then you could use a lock per group.



But rather than figuring out how to synchronize these millions of objects,
I'd spend some time trying to figure out if you can avoid needing to
synchronize in the first place.

Can you elaborate more on the system you are working on?



We will investigate the partitioning possibility but I'm afraid that will
cause more problem than it solve. The input is delivered by some external
process which does not respect any locality rules. So the treatment could
not be clustered effectively.  (This is why we have choose to lock per
object)


It seems also difficult to remove some part of the lock. What we really want
is to isolate the treatment of one object for avoiding access to its data
and update in the same time (which is also not possible with monitor
read/write lock ...).  One other constraint is that on treatment on one
object could require data from other objects. For avoiding any dead lock we
are keeping object in an acyclic graph. 





> 	- If I have something to do on many objects
> 		If I use classic java synchronization
>
> 		List<MyObject> toDo = ...;
> 		For(MyObject theObject:toDo){
> 			synchronized(theObject){
> 				// treatment on theObject
> 			}
> 		}

Make sure your iterators have consistent ordering, otherwise you could
easily deadlock.

  I agree :) , but this is just an example , not the real code ;)

> 		If I use concurrent objects I could do something like
>
> 		Int cursor_ = 0;
> 		While(todo.size()>0){
> 			MyObject theObject = toDo.get(cursor_);
> 			If(theObject.getLock().tryLock()){
> 				Try{
> 					toDo.remove(cursor_);
> 					// treatment on theObject
> 				}finally{
> 					theObject.getLock().unlock();
> 				}
> 			}
> 			if(toDo.size>0)
> 			cursor_ = (cursor_+1)%todo.size();
> 		}

If you can ask each object for its lock (which need not be a distinct lock
per object) then I don't see why the above form is needed instead of:

 	     for(MyObject theObject:toDo){
               Lock l = theObject.getLock();
               l.lock();
               try {
                 ...
               } finally {
                 lock.unlock();
               }
             }
??


	This a small optimization: avoid to wait for an object if you can
the treatment on the other in the same time. (Which is a lot more
complicated to do with monitors .. )


> 	How much system resources will be used (is system resource are
> acquired only when something has been done on the lock or when
> constructing the object)

The main resource used will be memory, both in constructing the locks and
anytime a thread needs to block on the lock. Actually blocking of a thread
may need additional low-level resources but these are likely to be needed by
a thread during its lifetime anyway.

Cheers,
David Holmes


Thank you for your quick answer.

From dl at cs.oswego.edu  Mon Oct 31 08:05:18 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Oct 31 08:06:29 2005
Subject: [concurrency-interest] Lock memory/resource Cost
In-Reply-To: <F520B214418AD4119F7000508BD90CC207F84083@ivhqsr02>
References: <F520B214418AD4119F7000508BD90CC207F84083@ivhqsr02>
Message-ID: <4366168E.2020901@cs.oswego.edu>

Thierry Hanot wrote:

>Hello 
>
>I 'm just wondering what is the memory/system resource cost of
>ReentrantLocks or read write lock.
>  
>
An extension to David's answer: 

Builtin locks take less space (basically just a bit) until they are ever 
used.
When they are contended, the space is probably about the same (for 
builtin locks,
space for queuing etc is allocated internally by the JVM).  When they are
simply locked without contention, builtin locks may use space that may 
or may
not be allocated on heap, but is in any case slightly less than 
ReentrantLock.

You can save a little space by subclassing ReentrantLock instead of
declaring one as a field. This is only reasonable if the class isn't public.

All in all though, if you really must create millions of these, and don't
absolutely require extended functionality (tryLock, etc) of ReentrantLock,
you are probably better off using builtin locking.

-Doug

From David.Biesack at sas.com  Mon Oct 31 08:19:36 2005
From: David.Biesack at sas.com (David J. Biesack)
Date: Mon Oct 31 08:20:11 2005
Subject: [concurrency-interest] spurious wakeups semantics
In-Reply-To: <b097ac510510291056w64046a41g98a92284b0f0b887@mail.gmail.com>
	(message from Joshua Bloch on Sat, 29 Oct 2005 10:56:12 -0700)
References: <200510291747.j9THlwQ23546@mozart.unx.sas.com>
	<b097ac510510291056w64046a41g98a92284b0f0b887@mail.gmail.com>
Message-ID: <200510311319.j9VDJaQ28626@mozart.unx.sas.com>

> Date: Sat, 29 Oct 2005 10:56:12 -0700
> From: Joshua Bloch <josh@bloch.us>
> Cc: concurrency-interest@altair.cs.oswego.edu
> 
> David,
> 
> Any wakeup, spurious or otherwise, is followed by acquisition of the
> intrinsic lock on the object being waited on.  There is no race
> condition, and no need for a volatile variable.
> 
>         Regards,
> 
>         Josh

what happens to thread T2 which was running and holding the lock when T1 gets the spurious wakeup?
At that time, it is holding the lock, so if T1 acquires it, T2 must give it up.. thus T2 must enter
a wait state, correct?

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513

From matthias.ernst at coremedia.com  Mon Oct 31 08:41:45 2005
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Mon Oct 31 08:42:54 2005
Subject: AW: [concurrency-interest] spurious wakeups semantics
Message-ID: <F34C8A704C489B46B9E9FBDBD1B91D5F01926AB4@MARS.coremedia.com>


> what happens to thread T2 which was running and holding the 
> lock when T1 gets the spurious wakeup?
> At that time, it is holding the lock, so if T1 acquires it, 
> T2 must give it up.. thus T2 must enter a wait state, correct?

No, it's the other way around: T2 continues to hold the lock, so T1 
cannot acquire it _yet_. It has to wait until T2 releases the lock.
It's a spontaneous transition from "waiting on xxx" to
"trying to enter xxx".

Matthias

From dl at cs.oswego.edu  Mon Oct 31 09:06:29 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Oct 31 09:07:40 2005
Subject: [concurrency-interest] Atomic double arrays
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEHFGHAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEHFGHAA.dholmes@dltech.com.au>
Message-ID: <436624E5.5010207@cs.oswego.edu>


>>There is this throw-away line at the end of the package docs:
>>"You can also hold floats using Float.floatToIntBits and
>>Float.intBitstoFloat conversions, and doubles using
>>Double.doubleToLongBits and Double.longBitsToDouble conversions."
>>
>>But that doesn't seem to help if you want to use the atomic add
>>methods
>>    
>>
>
>Right, these techniques allow you to hold float bits and do atomic get/set
>and CAS but not general arithmetic - because again there are no atomic
>instructions for doing that.
>
>  
>
Although, if you absolutely cannot use a lock, you might (but probably 
shouldn't)
use something like:

void addToAsDouble(AtomicLong a, double x) {
  for(;;) {
    long b = a.get();
    long u = Double.doubleBitsToLong(x + Double.longBitsToDouble(b));
    if (a.compareAndSet(b, u)) break;
  }
}

This can fail to work in the desired way if you ever hit values that are 
== as doubles but have
different bit patterns as longs (like positive and negative zero).   
Conversely if the
value is ever NaN, the bits will be equal, but a double == comparison 
would return false,
which would make you expect that CAS would fail.
If you can live with these problems, you mught consider it. Although
the conversion functions are not always cheap, and so this is  not likely
to be faster than locking. But  it might be applicable if you must use 
CAS to
avoid deadlock scenarios. (In  which case, you might also need to put in 
backoffs
to avoid livelock as well.)

In other words, the short answer is as David said: Use a lock.

-Doug

From dholmes at dltech.com.au  Mon Oct 31 16:49:05 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Oct 31 16:49:54 2005
Subject: [concurrency-interest] Lock memory/resource Cost 
In-Reply-To: <F520B214418AD4119F7000508BD90CC207F84089@ivhqsr02>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHPGHAA.dholmes@dltech.com.au>

Thierry,

> We will investigate the partitioning possibility but I'm afraid that will
> cause more problem than it solve. The input is delivered by some external
> process which does not respect any locality rules. So the treatment could
> not be clustered effectively.  (This is why we have choose to lock per
> object)
>
>
> It seems also difficult to remove some part of the lock. What we
> really want
> is to isolate the treatment of one object for avoiding access to its data
> and update in the same time (which is also not possible with monitor
> read/write lock ...).  One other constraint is that on treatment on one
> object could require data from other objects. For avoiding any
> dead lock we are keeping object in an acyclic graph.

If you really can't change the structure then as Doug said built-in locks
seems your best choice. I forgot to make the very important point that Doug
made: which is that built-in locks take no extra space until contended,
while explicit locks consume their full space immediately.

Without knowing more about the system and its architecture there no specific
advice that can be offered. The generic advice is: try to avoid sharing the
objects; use ownership hand-off protocols to restrict access etc.

Good luck.

David Holmes

From dholmes at dltech.com.au  Mon Oct 31 16:58:44 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Oct 31 16:59:21 2005
Subject: [concurrency-interest] spurious wakeups semantics
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5F01926AB4@MARS.coremedia.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIAGHAA.dholmes@dltech.com.au>

Ernst, Matthias writes:
>
> No, it's the other way around: T2 continues to hold the lock, so T1
> cannot acquire it _yet_. It has to wait until T2 releases the lock.
> It's a spontaneous transition from "waiting on xxx" to
> "trying to enter xxx".

Exactly.

I see that the documentation for Object.wait() is a bit unclear on this. It
treats all the normal causes for wait returning and states:

"The thread T is then removed from the wait set for this object and
re-enabled for thread scheduling. It then competes in the usual manner with
other threads for the right to synchronize on the object"

but the mention of spurious wakeup follows that and doesn't make it clear
exactly what happens. Spurious wakeup should just be another bullet in the
list along with notify, notifyall, interruption and timeout.

David Holmes


