From Ryan.LeCompte at pango.com  Thu Feb  1 10:54:35 2007
From: Ryan.LeCompte at pango.com (Ryan LeCompte)
Date: Thu, 1 Feb 2007 10:54:35 -0500
Subject: [concurrency-interest] Executors, Work Manager, JEE
Message-ID: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>

Hello,
 
I'd like to know how "safe" it is to use the new Java concurrency utilities within a JEE container, also taking into account the Work Manager API designed/implemented by IBM/BEA. Traditionally it has been discouraged to spawn your own threads within the JEE container, because the container can't manage those threads, etc. Does the same of thinking hold true if I utilize the new Java concurrency utilities for thread pools, etc? What are my options if my JEE container doesn't support the Work Manager specification (which, if I recall, hasn't even been finally approved as a standard part of JEE).
 
Thanks,
Ryan
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070201/46661f7c/attachment.html 

From alarmnummer at gmail.com  Thu Feb  1 11:34:28 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 1 Feb 2007 17:34:28 +0100
Subject: [concurrency-interest] Executors, Work Manager, JEE
In-Reply-To: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
Message-ID: <1466c1d60702010834y145ddf3fn20737e025eacb32c@mail.gmail.com>

Hi Ryan.

it depends on what you want to do and which technologies you need to
use. I use the JSR166 in a lot of serverside projects and in
combination with Spring it works like a dream.

On 2/1/07, Ryan LeCompte <Ryan.LeCompte at pango.com> wrote:
>
>
> Hello,
>
> I'd like to know how "safe" it is to use the new Java concurrency utilities
> within a JEE container, also taking into account the Work Manager API
> designed/implemented by IBM/BEA. Traditionally it has been discouraged to
> spawn your own threads within the JEE container, because the container can't
> manage those threads, etc. Does the same of thinking hold true if I utilize
> the new Java concurrency utilities for thread pools, etc? What are my
> options if my JEE container doesn't support the Work Manager specification
> (which, if I recall, hasn't even been finally approved as a standard part of
> JEE).
>
> Thanks,
> Ryan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From Ryan.LeCompte at pango.com  Thu Feb  1 11:33:18 2007
From: Ryan.LeCompte at pango.com (Ryan LeCompte)
Date: Thu, 1 Feb 2007 11:33:18 -0500
Subject: [concurrency-interest] Executors, Work Manager, JEE
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
	<1466c1d60702010834y145ddf3fn20737e025eacb32c@mail.gmail.com>
Message-ID: <5F9C31E563BD404DB284240CAE7ACD5205347B@pangomail2k3.pangonetworks.com>

Peter,
 
Thanks for responding. I basically need a safe way to asynchronously schedule work to be executed, as well as schedule work to be executed periodically (think Quartz or java.util.Timer). I was thinking I could use the new Executor/task framework to achieve this, but I am not sure if this is "safe" within JEE containers, although the new concurrency utilities avoid the direct use of java.lang.Thread directly. Any insight would be very helpful.
 
Thanks,
Ryan

________________________________

From: Peter Veentjer [mailto:alarmnummer at gmail.com]
Sent: Thu 2/1/2007 11:34 AM
To: Ryan LeCompte
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Executors, Work Manager, JEE



Hi Ryan.

it depends on what you want to do and which technologies you need to
use. I use the JSR166 in a lot of serverside projects and in
combination with Spring it works like a dream.

On 2/1/07, Ryan LeCompte <Ryan.LeCompte at pango.com> wrote:
>
>
> Hello,
>
> I'd like to know how "safe" it is to use the new Java concurrency utilities
> within a JEE container, also taking into account the Work Manager API
> designed/implemented by IBM/BEA. Traditionally it has been discouraged to
> spawn your own threads within the JEE container, because the container can't
> manage those threads, etc. Does the same of thinking hold true if I utilize
> the new Java concurrency utilities for thread pools, etc? What are my
> options if my JEE container doesn't support the Work Manager specification
> (which, if I recall, hasn't even been finally approved as a standard part of
> JEE).
>
> Thanks,
> Ryan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070201/c58ecba0/attachment.html 

From brian at quiotix.com  Thu Feb  1 12:02:22 2007
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 01 Feb 2007 12:02:22 -0500
Subject: [concurrency-interest] Executors, Work Manager, JEE
In-Reply-To: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
Message-ID: <45C21D1E.4040003@quiotix.com>

The JEE philosophy is that it has to be in control of all resource 
management, in order to make good global resource allocation decisions.

In the real world, however, sometimes this is too theoretical a goal. 
Different containers have different degrees of tolerance for breaking 
these rules.  Some (e.g., JBoss) let "anything go"; others (e.g., 
WebSphere) may install security managers that won't even let you create 
a thread.

Assuming that your security manager and class loader don't fight with 
you, this is one of those purity-vs-practicality decisions you need to 
make for yourself.  Bear in mind that going down this route makes your 
app no longer JEE compliant, and therefore not portable across 
containers (do you care?), as well as potentially compromises overall 
performance (maybe, maybe not) because of resource management issues.

I've written applications that have both EE and SE components; I use an 
EE container to manage the EE components, and run the SE components 
in-process.  JBoss makes this easy, as you just write an MBean to invoke 
your service start method, which can do anything it wants.  In those 
cases, I found the flexibility outweighed the loss of conformance; in 
other cases, it gets complicated and you may be better off segregating 
code employing the SE approach from your EE container.  Your mileage may 
vary.

EE is a tool, not a religion.  Use it appropriately, and it gives you 
leverage.  Use it inappropriately, and you risk hurting yourself.

Ryan LeCompte wrote:
> Hello,
>  
> I'd like to know how "safe" it is to use the new Java concurrency 
> utilities within a JEE container, also taking into account the Work 
> Manager API designed/implemented by IBM/BEA. Traditionally it has been 
> discouraged to spawn your own threads within the JEE container, because 
> the container can't manage those threads, etc. Does the same of thinking 
> hold true if I utilize the new Java concurrency utilities for thread 
> pools, etc? What are my options if my JEE container doesn't support the 
> Work Manager specification (which, if I recall, hasn't even been finally 
> approved as a standard part of JEE).
>  
> Thanks,
> Ryan
>  
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From gregg at cytetech.com  Thu Feb  1 13:42:10 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 01 Feb 2007 12:42:10 -0600
Subject: [concurrency-interest] Executors, Work Manager, JEE
In-Reply-To: <5F9C31E563BD404DB284240CAE7ACD5205347B@pangomail2k3.pangonetworks.com>
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
	<1466c1d60702010834y145ddf3fn20737e025eacb32c@mail.gmail.com>
	<5F9C31E563BD404DB284240CAE7ACD5205347B@pangomail2k3.pangonetworks.com>
Message-ID: <45C23482.2020404@cytetech.com>



Ryan LeCompte wrote:
> Thanks for responding. I basically need a safe way to asynchronously 
> schedule work to be executed, as well as schedule work to be executed 
> periodically (think Quartz or java.util.Timer). I was thinking I could 
> use the new Executor/task framework to achieve this, but I am not sure 
> if this is "safe" within JEE containers, although the new concurrency 
> utilities avoid the direct use of java.lang.Thread directly. Any insight 
> would be very helpful.

The JEE model is already using "pools" to dispatch inbound work requests (web 
pages or method calls etc).  I don't think that the design paradigm was intended 
to include so many of the other things that one might find in code that is more 
like an application or container itself.  As others have said, you might need to 
consider your design basis to see if you really should put that other stuff into 
another place such as a simple JSE application or into a larger service platform 
such as Jini or something else which can provide your the power that you need 
with the portability that you might desire.

Gregg Wonderly

From Ryan.LeCompte at pango.com  Thu Feb  1 14:29:39 2007
From: Ryan.LeCompte at pango.com (Ryan LeCompte)
Date: Thu, 1 Feb 2007 14:29:39 -0500
Subject: [concurrency-interest] Executors, Work Manager, JEE
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com><1466c1d60702010834y145ddf3fn20737e025eacb32c@mail.gmail.com><5F9C31E563BD404DB284240CAE7ACD5205347B@pangomail2k3.pangonetworks.com>
	<45C23482.2020404@cytetech.com>
Message-ID: <5F9C31E563BD404DB284240CAE7ACD5205347E@pangomail2k3.pangonetworks.com>

Thank you all for your suggestions! I'll be investigating this further.
 
Ryan

________________________________

From: concurrency-interest-bounces at cs.oswego.edu on behalf of Gregg Wonderly
Sent: Thu 2/1/2007 1:42 PM
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Executors, Work Manager, JEE





Ryan LeCompte wrote:
> Thanks for responding. I basically need a safe way to asynchronously
> schedule work to be executed, as well as schedule work to be executed
> periodically (think Quartz or java.util.Timer). I was thinking I could
> use the new Executor/task framework to achieve this, but I am not sure
> if this is "safe" within JEE containers, although the new concurrency
> utilities avoid the direct use of java.lang.Thread directly. Any insight
> would be very helpful.

The JEE model is already using "pools" to dispatch inbound work requests (web
pages or method calls etc).  I don't think that the design paradigm was intended
to include so many of the other things that one might find in code that is more
like an application or container itself.  As others have said, you might need to
consider your design basis to see if you really should put that other stuff into
another place such as a simple JSE application or into a larger service platform
such as Jini or something else which can provide your the power that you need
with the portability that you might desire.

Gregg Wonderly
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070201/f4981967/attachment.html 

From joe.bowbeer at gmail.com  Thu Feb  1 19:04:59 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 1 Feb 2007 16:04:59 -0800
Subject: [concurrency-interest] Executors, Work Manager, JEE
In-Reply-To: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
Message-ID: <31f2a7bd0702011604jdf9d0b4i2d20e0a986002cf@mail.gmail.com>

On 2/1/07, Ryan LeCompte <Ryan.LeCompte at pango.com> wrote:
>
> I'd like to know how "safe" it is to use the new Java concurrency utilities
> within a JEE container, also taking into account the Work Manager API
> designed/implemented by IBM/BEA. Traditionally it has been discouraged to
> spawn your own threads within the JEE container, because the container can't
> manage those threads, etc. Does the same of thinking hold true if I utilize
> the new Java concurrency utilities for thread pools, etc? What are my
> options if my JEE container doesn't support the Work Manager specification
> (which, if I recall, hasn't even been finally approved as a standard part of
> JEE).
>

There's some mention of custom thread pools in this article:

http://www-128.ibm.com/developerworks/websphere/techjournal/0606_johnson/0606_johnson.html

The article above was written by one of the Java EE concurrency guys:

http://gee.cs.oswego.edu/dl/concurrencyee-interest/

From dhanji at gmail.com  Thu Feb  1 21:11:01 2007
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Fri, 2 Feb 2007 12:11:01 +1000
Subject: [concurrency-interest] Executors, Work Manager, JEE
In-Reply-To: <5F9C31E563BD404DB284240CAE7ACD5205347E@pangomail2k3.pangonetworks.com>
References: <5F9C31E563BD404DB284240CAE7ACD5205347A@pangomail2k3.pangonetworks.com>
	<1466c1d60702010834y145ddf3fn20737e025eacb32c@mail.gmail.com>
	<5F9C31E563BD404DB284240CAE7ACD5205347B@pangomail2k3.pangonetworks.com>
	<45C23482.2020404@cytetech.com>
	<5F9C31E563BD404DB284240CAE7ACD5205347E@pangomail2k3.pangonetworks.com>
Message-ID: <aa067ea10702011811v3df3d434g897c3911b2e91d17@mail.gmail.com>

If you have access to JEE 1.4+ container(s) you can use the Timer EJB
service to schedule and execute tasks asynchronously.

http://java.sun.com/j2ee/1.4/docs/tutorial/doc/Session5.html

Honestly I cant think of a reason you would need to spawn threads from
an appserver. JEE 1.4 and above give you most of the functionality you
require out of the box. And whatever obscure need you may have outside
of that (maybe some warped and twisted reading/writing of multiple
disk files?) can easily be added by wrapping an whatever SE service in
a resource adapter.

On 2/2/07, Ryan LeCompte <Ryan.LeCompte at pango.com> wrote:
>
>
>
> Thank you all for your suggestions! I'll be investigating this further.
>
> Ryan
>
>  ________________________________
>  From: concurrency-interest-bounces at cs.oswego.edu on behalf
> of Gregg Wonderly
> Sent: Thu 2/1/2007 1:42 PM
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Executors, Work Manager, JEE
>
>
>
>
>
>
>
> Ryan LeCompte wrote:
> > Thanks for responding. I basically need a safe way to asynchronously
> > schedule work to be executed, as well as schedule work to be executed
> > periodically (think Quartz or java.util.Timer). I was thinking I could
> > use the new Executor/task framework to achieve this, but I am not sure
> > if this is "safe" within JEE containers, although the new concurrency
> > utilities avoid the direct use of java.lang.Thread directly. Any insight
> > would be very helpful.
>
> The JEE model is already using "pools" to dispatch inbound work requests
> (web
> pages or method calls etc).  I don't think that the design paradigm was
> intended
> to include so many of the other things that one might find in code that is
> more
> like an application or container itself.  As others have said, you might
> need to
> consider your design basis to see if you really should put that other stuff
> into
> another place such as a simple JSE application or into a larger service
> platform
> such as Jini or something else which can provide your the power that you
> need
> with the portability that you might desire.
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From hanson.char at gmail.com  Sat Feb  3 21:12:23 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 3 Feb 2007 18:12:23 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
Message-ID: <ca53c8f80702031812q6feffe25gd7bdca38a3fee827@mail.gmail.com>

Hi Tim,

>Josh told me he'd be willing to distribute this more widely if there was
sufficient interest.

I am very interested in the retry policy code.

>I've placed an incomplete draft of this code in a public area of the Java
Concurrency in Practice source repository:

Any chance I can find the missing pieces ?

Thanks in advance.

Hanson

On 1/20/07, Tim Peierls <tim at peierls.net> wrote:
>
> Last spring Josh Bloch proposed an interface and some standard
> implementations to capture something like what Bela described.
>
> public interface RetryPolicy {
>     boolean isFailureRecoverable(Exception e);
>     long nextDelay(long startTime, int retries);
> }
>
> The implementations included exponential backoff, truncated exponential
> backoff, and fixed delay.
>
> There was also a factory method for an ExecutorService wrapper to wrap
> Runnables and Callables with the machinery needed to implement a given
> RetryPolicy.
>
> The idea is that a task signals that it may be retried by throwing an
> exception e for which RetryPolicy.isFailureRecoverable(e) is true, and the
> nextDelay method decides, based on the start time of the initial attempt and
> the number of retries that have occurred, how long to wait before trying
> again. A negative return from nextDelay means not to try again, in which
> case the most recent failure exception is rethrown.
>
> For example, a policy of incessant retrying would be expressed by:
>
> class IncessantRetrying implements RetryPolicy {
>     public boolean isFailureRecoverable(Exception e) { return true; }
>     long nextDelay(long startTime, int retries) { return 0; }
> }
>
> Josh told me he'd be willing to distribute this more widely if there was
> sufficient interest.
>
> I've placed an incomplete draft of this code in a public area of the Java
> Concurrency in Practice source repository:
>
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
>
> It's incomplete because it doesn't define AbstractRetryPolicy.
>
> What Bela describes is slightly different: each task can be submitted with
> its own "RetryPolicy". That could be achieved within Josh's framework, for
> example, by extending ScheduledThreadPoolExecutor and overriding
> decorateTask to do the requisite wrapping for tasks that implement
> RetryPolicy.
>
> --tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070203/121d93c1/attachment.html 

From hanson.char at gmail.com  Sat Feb  3 22:28:52 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 3 Feb 2007 19:28:52 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702031812q6feffe25gd7bdca38a3fee827@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702031812q6feffe25gd7bdca38a3fee827@mail.gmail.com>
Message-ID: <ca53c8f80702031928p4f690fcbkef0fe020fabcc4c5@mail.gmail.com>

Interesting that the code compiles by javac 1.5.0_10, but not by javac
1.6.0with the following compilation error, after I tried to "fill in
the blank"
by creating an AbstractRetryPolicy.java (see below.)

RetryPolicies.java:181: <anonymous RetryPolicies$5> is not abstract and does
not override abstract method <T>invokeAny(java.util.Collection<? extends
java.util.concurrent.Callable<T>>,long,java.util.concurrent.TimeUnit) in
java.util.concurrent.ExecutorService
    return new ExecutorService() {
                                 ^
1 error

Any idea if this should or should not compile ?  If it should not, how it
can be fixed ?  If it should, is that a compiler bug in jdk1.6.0 ?

Hanson

public abstract class AbstractRetryPolicy implements RetryPolicy
{
    public Class<? extends Exception>[] recoverableExceptions;

    public AbstractRetryPolicy(Class<? extends Exception>[]
recoverableExceptions)
    {
        this.recoverableExceptions = recoverableExceptions;
    }

    public abstract long nextDelay(long startTime, int retries);

    public boolean isFailureRecoverable(Exception e)
    {
        for (Class<? extends Exception> c : recoverableExceptions)
            if (c.isAssignableFrom(e.getClass()))
                return true;
        return false;
    }
}
On 2/3/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi Tim,
>
> >Josh told me he'd be willing to distribute this more widely if there was
> sufficient interest.
>
> I am very interested in the retry policy code.
>
> >I've placed an incomplete draft of this code in a public area of the Java
> Concurrency in Practice source repository:
>
> Any chance I can find the missing pieces ?
>
> Thanks in advance.
>
> Hanson
>
> On 1/20/07, Tim Peierls <tim at peierls.net> wrote:
> >
> > Last spring Josh Bloch proposed an interface and some standard
> > implementations to capture something like what Bela described.
> >
> > public interface RetryPolicy {
> >     boolean isFailureRecoverable(Exception e);
> >     long nextDelay(long startTime, int retries);
> > }
> >
> > The implementations included exponential backoff, truncated exponential
> > backoff, and fixed delay.
> >
> > There was also a factory method for an ExecutorService wrapper to wrap
> > Runnables and Callables with the machinery needed to implement a given
> > RetryPolicy.
> >
> > The idea is that a task signals that it may be retried by throwing an
> > exception e for which RetryPolicy.isFailureRecoverable(e) is true, and
> > the nextDelay method decides, based on the start time of the initial attempt
> > and the number of retries that have occurred, how long to wait before trying
> > again. A negative return from nextDelay means not to try again, in which
> > case the most recent failure exception is rethrown.
> >
> > For example, a policy of incessant retrying would be expressed by:
> >
> > class IncessantRetrying implements RetryPolicy {
> >     public boolean isFailureRecoverable(Exception e) { return true; }
> >     long nextDelay(long startTime, int retries) { return 0; }
> > }
> >
> > Josh told me he'd be willing to distribute this more widely if there was
> > sufficient interest.
> >
> > I've placed an incomplete draft of this code in a public area of the Java
> > Concurrency in Practice source repository:
> >
> > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> >
> > It's incomplete because it doesn't define AbstractRetryPolicy.
> >
> > What Bela describes is slightly different: each task can be submitted
> > with its own "RetryPolicy". That could be achieved within Josh's framework,
> > for example, by extending ScheduledThreadPoolExecutor and overriding
> > decorateTask to do the requisite wrapping for tasks that implement
> > RetryPolicy.
> >
> > --tim
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070203/9b67d788/attachment.html 

From hanson.char at gmail.com  Sun Feb  4 20:58:14 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 4 Feb 2007 17:58:14 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702031928p4f690fcbkef0fe020fabcc4c5@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702031812q6feffe25gd7bdca38a3fee827@mail.gmail.com>
	<ca53c8f80702031928p4f690fcbkef0fe020fabcc4c5@mail.gmail.com>
Message-ID: <ca53c8f80702041758p659ac5beua475fd9c4b76f59e@mail.gmail.com>

Once I fixed the code to compile under jdk1.6, it would no longer compile
under 1.5, and vice-versa.  Good enough for me for now.

Hanson

On 2/3/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Interesting that the code compiles by javac 1.5.0_10, but not by javac
> 1.6.0 with the following compilation error, after I tried to "fill in the
> blank" by creating an AbstractRetryPolicy.java (see below.)
>
> RetryPolicies.java:181: <anonymous RetryPolicies$5> is not abstract and
> does not override abstract method <T>invokeAny(java.util.Collection<?
> extends
> java.util.concurrent.Callable<T>>,long,java.util.concurrent.TimeUnit ) in
> java.util.concurrent.ExecutorService
>     return new ExecutorService() {
>                                  ^
> 1 error
>
> Any idea if this should or should not compile ?  If it should not, how it
> can be fixed ?  If it should, is that a compiler bug in jdk1.6.0 ?
>
> Hanson
>
> public abstract class AbstractRetryPolicy implements RetryPolicy
> {
>     public Class<? extends Exception>[] recoverableExceptions;
>
>     public AbstractRetryPolicy(Class<? extends Exception>[]
> recoverableExceptions)
>     {
>         this.recoverableExceptions = recoverableExceptions;
>     }
>
>     public abstract long nextDelay(long startTime, int retries);
>
>     public boolean isFailureRecoverable(Exception e)
>     {
>         for (Class<? extends Exception> c : recoverableExceptions)
>             if (c.isAssignableFrom(e.getClass()))
>                 return true;
>         return false;
>     }
> }
> On 2/3/07, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Hi Tim,
> >
> > >Josh told me he'd be willing to distribute this more widely if there
> > was sufficient interest.
> >
> > I am very interested in the retry policy code.
> >
> > >I've placed an incomplete draft of this code in a public area of the Java
> > Concurrency in Practice source repository:
> >
> > Any chance I can find the missing pieces ?
> >
> > Thanks in advance.
> >
> > Hanson
> >
> > On 1/20/07, Tim Peierls <tim at peierls.net> wrote:
> > >
> > > Last spring Josh Bloch proposed an interface and some standard
> > > implementations to capture something like what Bela described.
> > >
> > > public interface RetryPolicy {
> > >     boolean isFailureRecoverable(Exception e);
> > >     long nextDelay(long startTime, int retries);
> > > }
> > >
> > > The implementations included exponential backoff, truncated
> > > exponential backoff, and fixed delay.
> > >
> > > There was also a factory method for an ExecutorService wrapper to wrap
> > > Runnables and Callables with the machinery needed to implement a given
> > > RetryPolicy.
> > >
> > > The idea is that a task signals that it may be retried by throwing an
> > > exception e for which RetryPolicy.isFailureRecoverable(e) is true, and
> > > the nextDelay method decides, based on the start time of the initial attempt
> > > and the number of retries that have occurred, how long to wait before trying
> > > again. A negative return from nextDelay means not to try again, in which
> > > case the most recent failure exception is rethrown.
> > >
> > > For example, a policy of incessant retrying would be expressed by:
> > >
> > > class IncessantRetrying implements RetryPolicy {
> > >     public boolean isFailureRecoverable(Exception e) { return true; }
> > >     long nextDelay(long startTime, int retries) { return 0; }
> > > }
> > >
> > > Josh told me he'd be willing to distribute this more widely if there
> > > was sufficient interest.
> > >
> > > I've placed an incomplete draft of this code in a public area of the Java
> > > Concurrency in Practice source repository:
> > >
> > > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> > >
> > > It's incomplete because it doesn't define AbstractRetryPolicy.
> > >
> > > What Bela describes is slightly different: each task can be submitted
> > > with its own "RetryPolicy". That could be achieved within Josh's framework,
> > > for example, by extending ScheduledThreadPoolExecutor and overriding
> > > decorateTask to do the requisite wrapping for tasks that implement
> > > RetryPolicy.
> > >
> > > --tim
> > >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070204/b2285e51/attachment.html 

From alarmnummer at gmail.com  Mon Feb  5 09:40:17 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Mon, 5 Feb 2007 15:40:17 +0100
Subject: [concurrency-interest] ReadWriteLocks and Conditions
Message-ID: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>

I'm working on a structure called the LendeableReference. This
structure makes it possible to lend a reference to multiple threads
(values are taken from the LendableReference) and after a thread is
done with the reference, it takes it back so that a new reference can
be set. If no reference is available, taking threads block until a
reference is available.

I'm using a ReadWriteLock: when a value is taken the readlock is
locked, when it is takenback, the readlock is released. And when a new
value is set, a writelock needs to be obtained first (all readlocks
need to be returned first), the value is set, and the writelock is
released, and readers are now able to obtain readlocks again.

So this is how it should work. The problem is the waitingpart of the
takes. If there are two threads that want to take a value, one of the
takers doesn't return immediately. I understand why it happens (one of
the takers has the readlock at the end, so the second taker can't
obtain the writelock). But what is the correct way to implement the
waiting logic? I have looked at the documentation of the ReadWriteLock
and the ReentrantReadWriteLock, but I haven't figured out a way to
solve this problem.

This is the code of the take.

 public E take() throws InterruptedException {
        readLock.lockInterruptibly();
        if(ref != null){
            return ref;
        }

        readLock.unlock();

        writeLock.lock();

        try{
            while (ref == null)
                refAvailableCondition.await();
            readLock.lockInterruptibly();
            return ref;
        }finally{
            writeLock.unlock();
        }
    }

And the refAvailableCondition is a Condition made by the writelock.

From gergg at cox.net  Mon Feb  5 10:19:49 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 05 Feb 2007 09:19:49 -0600
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
Message-ID: <45C74B15.3050300@cox.net>

Peter Veentjer wrote:
> I'm working on a structure called the LendeableReference. This
> structure makes it possible to lend a reference to multiple threads
> (values are taken from the LendableReference) and after a thread is
> done with the reference, it takes it back so that a new reference can
> be set. If no reference is available, taking threads block until a
> reference is available.

It seems to me that Future would be a better match for this.  Is there something 
about Future that is not a good match?

Gregg Wonderly

From yechielf at gigaspaces.com  Mon Feb  5 11:30:06 2007
From: yechielf at gigaspaces.com (Yechiel)
Date: Mon, 5 Feb 2007 18:30:06 +0200
Subject: [concurrency-interest] array of volatiles ?
Message-ID: <008e01c74942$e5d48d90$820aa8c0@pcyechieldesk>

How can I declare an array of volatiles ?
i.e   volatile int[] myArray declares the ref to the whole array as
volatile. How can I make each int volatile  ?

Regrds,
Yechiel
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070205/1707acfd/attachment.html 

From tim at peierls.net  Mon Feb  5 11:26:52 2007
From: tim at peierls.net (Tim Peierls)
Date: Mon, 5 Feb 2007 11:26:52 -0500
Subject: [concurrency-interest] array of volatiles ?
In-Reply-To: <008e01c74942$e5d48d90$820aa8c0@pcyechieldesk>
References: <008e01c74942$e5d48d90$820aa8c0@pcyechieldesk>
Message-ID: <63b4e4050702050826j5bad60a6g831ec59e48a67c07@mail.gmail.com>

You can't, but consider using
java.util.conccurent.atomic.AtomicIntegerArrayinstead. See Java
Concurrency in Practice section 15.3, page 325.

--tim

On 2/5/07, Yechiel <yechielf at gigaspaces.com> wrote:
>
>  How can I declare an array of volatiles ?
> i.e   volatile int[] myArray declares the ref to the whole array as
> volatile. How can I make each int volatile  ?
>
> Regrds,
> Yechiel
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070205/aa2174c4/attachment.html 

From mailinglist.taras.tielkes at gmail.com  Mon Feb  5 12:07:27 2007
From: mailinglist.taras.tielkes at gmail.com (Taras Tielkes)
Date: Mon, 05 Feb 2007 18:07:27 +0100
Subject: [concurrency-interest] array of volatiles ?
In-Reply-To: <008e01c74942$e5d48d90$820aa8c0@pcyechieldesk>
References: <008e01c74942$e5d48d90$820aa8c0@pcyechieldesk>
Message-ID: <45C7644F.20607@gmail.com>

Yechiel wrote:
> How can I declare an array of volatiles ?
> i.e   volatile int[] myArray declares the ref to the whole array as
> volatile. How can I make each int volatile  ?
You can't. You could use the *Array classes from 
java.util.concurrent.atomic to get something similar.

-tt

From alarmnummer at gmail.com  Tue Feb  6 03:55:36 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 6 Feb 2007 09:55:36 +0100
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <45C74B15.3050300@cox.net>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
Message-ID: <1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>

I don't see how a Future would fit in, maybe you could elaborate on this?

I have decided to drop the version with the ReadWriteLock completely.
One of the problems was with the condition, and another problem is
that a reference can't be taken back by a different thread than took
it (a lock has to be released by the same thread as locked it).

In the bus to my work I created a different implementation:

public class NewStrictLendeableReference<E> implements LendableReference<E> {

    public static Lock newDefaultMainLock(){
        return new ReentrantLock();
    }

    private final Lock mainLock;
    private final Condition refAvailableCondition;
    private final Condition noTakersCondition;
    private volatile long lendCount = 0;
    private volatile E ref;

    public NewStrictLendeableReference(){
        this(newDefaultMainLock(),null);
    }

    public NewStrictLendeableReference(E ref){
        this(newDefaultMainLock(),ref);
    }

    public NewStrictLendeableReference(boolean fair, E ref){
        this(new ReentrantLock(fair),ref);
    }

    public NewStrictLendeableReference(Lock mainLock, E ref) {
        if (mainLock == null) throw new NullPointerException();
        this.ref = ref;
        this.mainLock = mainLock;
        refAvailableCondition = mainLock.newCondition();
        noTakersCondition = mainLock.newCondition();
    }

    public Lock getMainLock() {
        return mainLock;
    }

    public Condition getRefAvailableCondition() {
        return refAvailableCondition;
    }

    public Condition getNoTakersCondition() {
        return noTakersCondition;
    }

    public long getLendCount() {
        return lendCount;
    }

    public E take() throws InterruptedException {
        mainLock.lockInterruptibly();

        try {
            while (ref == null)
                refAvailableCondition.await();

            lendCount++;
            return ref;
        } finally {
            mainLock.unlock();
        }
    }

    public E tryTake(long timeout, TimeUnit unit) throws
InterruptedException, TimeoutException {
        if (unit == null) throw new NullPointerException();

        long timeoutNs = toUsableNanos(timeout,unit);
        mainLock.lockInterruptibly();
        try{
            while(ref == null)
                timeoutNs = awaitAndThrow(refAvailableCondition,timeoutNs);

            lendCount++;
            return ref;
        }finally{
            mainLock.unlock();
        }
    }

    public void takeBack(E ref) {
        if (ref == null) throw new NullPointerException();

        mainLock.lock();
        try {
            if (!ref.equals(this.ref))
                throw new IncorrectReferenceTakenBackException();

            if (lendCount == 0)
                throw new RuntimeException();

            lendCount--;
            if (lendCount == 0)
                noTakersCondition.signalAll();
        } finally {
            mainLock.unlock();
        }
    }

    public E put(E newRef) throws InterruptedException {
        mainLock.lockInterruptibly();
        try {
            while (lendCount > 0)
                noTakersCondition.await();

            return updateReference(newRef);
        } finally {
            mainLock.unlock();
        }
    }

    private E updateReference(E newRef) {
        E oldRef = ref;
        this.ref = newRef;
        if (ref != null)
            refAvailableCondition.signalAll();
        return oldRef;
    }

    public E tryPut(E newRef, long timeout, TimeUnit unit) throws
InterruptedException, TimeoutException {
        if (unit == null) throw new NullPointerException();

        long timeoutNs = toUsableNanos(timeout,unit);

        mainLock.lockInterruptibly();
        try {
            while (lendCount > 0)
                timeoutNs = awaitAndThrow(noTakersCondition,timeoutNs);

            return updateReference(newRef);
        } finally {
            mainLock.unlock();
        }
    }

    public E peek() {
        return ref;
    }
}


On 2/5/07, Gregg Wonderly <gergg at cox.net> wrote:
> Peter Veentjer wrote:
> > I'm working on a structure called the LendeableReference. This
> > structure makes it possible to lend a reference to multiple threads
> > (values are taken from the LendableReference) and after a thread is
> > done with the reference, it takes it back so that a new reference can
> > be set. If no reference is available, taking threads block until a
> > reference is available.
>
> It seems to me that Future would be a better match for this.  Is there something
> about Future that is not a good match?
>
> Gregg Wonderly
>

From tim at peierls.net  Tue Feb  6 08:39:12 2007
From: tim at peierls.net (Tim Peierls)
Date: Tue, 6 Feb 2007 08:39:12 -0500
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
Message-ID: <63b4e4050702060539h68c27030h26a3aa7bd59cb150@mail.gmail.com>

On 2/6/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
>
> public class NewStrictLendeableReference<E> implements
> LendableReference<E> {
>

Peter,

It's hard for me to reason backwards from an implementation. Maybe you could
give us the LendableReference interface with a brief example or sketch of
how it might be used?

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070206/b1aff0e6/attachment.html 

From alarmnummer at gmail.com  Tue Feb  6 09:15:11 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 6 Feb 2007 15:15:11 +0100
Subject: [concurrency-interest] Fwd:  ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702060614g12d3990bg92c3cc2d8c8721fd@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<63b4e4050702060539h68c27030h26a3aa7bd59cb150@mail.gmail.com>
	<1466c1d60702060614g12d3990bg92c3cc2d8c8721fd@mail.gmail.com>
Message-ID: <1466c1d60702060615u48a9fa06w54630b38b6ab5c49@mail.gmail.com>

---------- Forwarded message ----------
From: Peter Veentjer <alarmnummer at gmail.com>
Date: Feb 6, 2007 3:14 PM
Subject: Re: [concurrency-interest] ReadWriteLocks and Conditions
To: Tim Peierls <tim at peierls.net>


Hi Tim,

Example usage:

while(true){
        Runnable task = lendableRef.take();
        try{
                task.execute();
        }finally{
                lendableRef.takeBack(task);
        }
}


By placing a Runnable in the lendableRef, the task is being executed.
By removing the Runnable from the lendableRef (so setting a null
value) the take blocks.

And this is the interface:

package org.jph.concurrent.lendable;

import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

/**
 * The LendableReference is a synchronization stone that makes it easy to deal
 * with a value that can be lend by one of more threads.
 * <p/>
 * Todo remark about comparison between LendableReference and
AwaitableReference.
 * <p/>
 * Todo: remark about save hand of
 * <p/>
 * Idiom:
 * <code>
 * Integer someref = lendableRef.tryTake();
 * try{
 *      ..do something
 * }finally{
 *      someRef.takeBack(someref);
 * }
 * </code>
 * <p/>
 *
 * The reference check for takeback is done based on equals method.
 * So you can send back a different object, as long as the equals
 * says they are equal.
 *
 * A LendableReference should allow that a lend reference is
 * takenback by a different thread than took the value.
 *
 * @author Peter Veentjer.
 */
public interface LendableReference<E> {

    /**
     * Takes the current reference. If no reference is available, this
     * call blocks until:
     * <ol>
     * <li>a non <tt>null</tt> comes available</li>
     * <li>the calling thread is interrupted</li>
     * </ol>
     *
     * @return the found value. This value will never be <tt>null</tt>.
     * @throws InterruptedException if interrupted while waiting.
     */
    E take() throws InterruptedException;

    /**
     * Takes the current reference. If no reference is available, this
     * call blocks until:
     * <ol>
     * <li>a non <tt>null</tt> comes available</li>
     * <li>the calling thread is interrupted</li>
     * <li>a timeout occurs</li>
     * </ol>
     * <p/>
     * If the timeout is smaller than zero, a TimeoutException is
     * thrown.
     * todo: nul timeouts.
     *
     * @param timeout how long to wait before giving up in units of
<tt>unit</tt>.
     * @param unit    a <tt>TimeUnit</tt> determining how to interpret
the <tt>timeout</tt> parameter.
     * @return the found value. If a timeout occurs, <tt>null</tt> is returned.
     * @throws NullPointerException if unit is <tt>null</tt>.
     * @throws InterruptedException if interrupted while waiting.
     * @throws TimeoutException     if a timeout occurrs.
     */
    E tryTake(long timeout, TimeUnit unit) throws
InterruptedException, TimeoutException;


    /**
     * Returns a lend reference back to this LendableReference. If a incorrect
     * reference is returned, a IncorrectReferenceTakenBackException is thrown.
     * The check on correctness is done based on the equal method. So you are
     * allowed to send back a different object, as long as it is equal.
     *
     * <p/>
     * A different thread is allowed to tryTake the reference back.
     * <p/>
     *
     * todo:
     * what happens if a bogus takeback is done? So a value is taken back
     * although nothing is lend.
     *
     * @param ref the reference taken back.
     * @throws NullPointerException if ref is <tt>null</tt>. Because a
<tt>null</tt> will
     *                              never be lend, it can't be returned.
     * @throws InvalidTakeBackException
     *                              if a incorrect reference is returned. It
     *                              depends on the implementation if this
     *                              exception is thrown. todo
     */
    void takeBack(E ref);

    /**
     * Puts a new reference into this LendableReference. This call blocks until:
     * <ol>
     * <li>the lendable reference allows the new value to be placed.
todo: explain situations</li>
     * <li>it is interrupted</li>
     * </ol>
     * <p/>
     * todo:
     * it depends on te implementation if blocking needs to tryTake place.
     * <p/>
     * todo:
     * explain why the tryExecute method now has an interrupt while
the AwaitableValue doesn't need it.
     *
     * @param newRef the new reference. The newRef is allowed to be
<tt>null</tt>.
     * @return the old reference, could be <tt>null</tt>.
     * @throws InterruptedException if interrupted while waiting.
     */
    E put(E newRef) throws InterruptedException;

    /**
     * Puts a new reference into this LendableReference. This call blocks until:
     * <ol>
     * <li></li>
     * <li>a timeout occurs</li>
     * <li>the calling thread is interrupted</li>
     * </ol>
     * <p/>
     * If the timeout is smaller than zero, a TimeoutException is
     * thrown.
     * todo: nul timeouts.
     *
     * @param newRef  the new reference. The newRef is allowed to be
<tt>null</tt>.
     * @param timeout how long to wait before giving up in units of
<tt>unit</tt>.
     * @param unit    a <tt>TimeUnit</tt> determining how to interpret
the <tt>timeout</tt> parameter.
     * @return the old reference, could be <tt>null</tt>.
     * @throws InterruptedException if interrupted while waiting.
     * @throws TimeoutException     if a timeout occurs.
     * @throws NullPointerException if unit is <tt>null</tt>.
     */
    E tryPut(E newRef, long timeout, TimeUnit unit) throws
InterruptedException, TimeoutException;

    /**
     * Returns the current reference. If no reference is available,
<tt>null</tt>
     * is returned.
     *
     * @return the current reference, if no reference is available,
<tt>null</tt> is
     *         returned.
     */
    E peek();
}



On 2/6/07, Tim Peierls <tim at peierls.net> wrote:
> On 2/6/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
> > public class NewStrictLendeableReference<E> implements
> LendableReference<E> {
> >
>
> Peter,
>
> It's hard for me to reason backwards from an implementation. Maybe you could
> give us the LendableReference interface with a brief example or sketch of
> how it might be used?
>
> --tim
>

From tim at peierls.net  Tue Feb  6 17:42:59 2007
From: tim at peierls.net (Tim Peierls)
Date: Tue, 6 Feb 2007 17:42:59 -0500
Subject: [concurrency-interest] Fwd: ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702060615u48a9fa06w54630b38b6ab5c49@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<63b4e4050702060539h68c27030h26a3aa7bd59cb150@mail.gmail.com>
	<1466c1d60702060614g12d3990bg92c3cc2d8c8721fd@mail.gmail.com>
	<1466c1d60702060615u48a9fa06w54630b38b6ab5c49@mail.gmail.com>
Message-ID: <63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>

LendableReference is essentially a resource pool of size 1. Other folks have
done a lot of work on the general case -- why not take advantage of that?

Or if you really want to roll your own, consider using Semaphores, as
described in section 5.5.3 of Java Concurrency in Practice.

--tim


On 2/6/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
>
> Example usage:
>
> while(true){
>         Runnable task = lendableRef.take();
>         try{
>                 task.execute();
>         }finally{
>                 lendableRef.takeBack(task);
>         }
> }
>
> By placing a Runnable in the lendableRef, the task is being executed.
> By removing the Runnable from the lendableRef (so setting a null
> value) the take blocks.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070206/6f768dfa/attachment.html 

From gregg at cytetech.com  Tue Feb  6 09:47:19 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 06 Feb 2007 08:47:19 -0600
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
Message-ID: <45C894F7.8060306@cytetech.com>



Peter Veentjer wrote:
> I don't see how a Future would fit in, maybe you could elaborate on this?

>>>I'm working on a structure called the LendeableReference. This
>>>structure makes it possible to lend a reference to multiple threads
>>>(values are taken from the LendableReference) and after a thread is
>>>done with the reference, it takes it back so that a new reference can
>>>be set. If no reference is available, taking threads block until a
>>>reference is available.

I don't know that I understand the constraints that you want to maintain, but 
based on your comments, it seems to me that the lended reference should access a 
Future.  The user of that object would 'get' the value, and thus block when 
there is no reference available (yet).  The algorithm that would apply in that 
case, is that they 'readers' would always ask a factory for the appropriate 
Future and thus use a relevant new Future when needed.

Here's something that you can pass around, and the users can "get" the value at 
anytime.  You can expand this to do more things about deferring object creation 
beyond the simple setValue() implementation, but this is what I was thinking about.

Maybe you could elaborate on the specifics of what else you need if this is not 
appropriate.

public class LendableReference<T> implements Runnable {
	volatile FutureTask<T> fut;
	volatile T val;
	public LendableReference( T value ) {
		setValue( value );
	}

	public LendableReference( Callable<T> call ) {
		setValue(call);
	}

	public void setValue( T value ) {
		val = value;
		fut = new FutureTask<T>( this, value );
	}

	public void setValue( Callable<T> call ) {
		fut = new FutureTask<T>( call );
	}

	public T get() {
		return val = fut.get();
	}

	/**
	 *  Do nothing to create value.  If you need to do something, override
	 *  run to do the work.
	 */
	public void run() {}
}


From s_ikhlef at hotmail.com  Tue Feb  6 23:50:16 2007
From: s_ikhlef at hotmail.com (sidali ikhlef)
Date: Wed, 07 Feb 2007 04:50:16 +0000
Subject: [concurrency-interest] synchronize design
In-Reply-To: <63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
Message-ID: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>


Hi,

I have multiple threads accessing a shared class called Resources, witch is 
composed of many other objects such as Current Activity, current environment 
..... I want to synchronize the access to this resource but not using simple 
mechanisms like synchronize or synchronize(this). Any help or referencing 
will be appreciated.

Precision: writing to the resource must have higher priority than reading 
and the chronology is very important to keep data coherent.

Regards.

_________________________________________________________________
MSN Messenger: appels gratuits de PC ? PC ! 
http://www.msn.fr/newhotmail/Default.asp?Ath=f


From rbalamohan at sonoasystems.com  Wed Feb  7 06:49:35 2007
From: rbalamohan at sonoasystems.com (Rajesh Balamohan)
Date: Wed, 07 Feb 2007 11:49:35 +0000
Subject: [concurrency-interest] synchronize design
In-Reply-To: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
References: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
Message-ID: <45C9BCCF.2090004@sonoasystems.com>

Can you try checking ReadWrite Locks in JDK 1.5 for this?.. Aquire the 
write lock only when you want to change it.

~Rajesh.B

sidali ikhlef wrote:
> Hi,
>
> I have multiple threads accessing a shared class called Resources, witch is 
> composed of many other objects such as Current Activity, current environment 
> ..... I want to synchronize the access to this resource but not using simple 
> mechanisms like synchronize or synchronize(this). Any help or referencing 
> will be appreciated.
>
> Precision: writing to the resource must have higher priority than reading 
> and the chronology is very important to keep data coherent.
>
> Regards.
>
> _________________________________________________________________
> MSN Messenger: appels gratuits de PC ? PC ! 
> http://www.msn.fr/newhotmail/Default.asp?Ath=f
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>   



From dcholmes at optusnet.com.au  Wed Feb  7 02:34:46 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 7 Feb 2007 17:34:46 +1000
Subject: [concurrency-interest] Queue.add() vs Executor.exec()
In-Reply-To: <45B290F5.6080301@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECMHFAA.dcholmes@optusnet.com.au>

Sorry for the late reply but I was on vacation and am still catching up.

To be able to use the queue directly and have it "just work" either the
queue needs to know about the pool, or else the pool must always have
threads ready to read from the queue. Having the queue know about the pool
means the queue is no longer just a BlockingQueue. Have the pool always
ready to read from the queue means you don't have control of the number of
threads that you generally would want (you could add heuristics to examine
the length of the queue after take() returns and use that to add more
threads etc - but that's messy).

Just my 2c.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Sunday, 21 January 2007 8:00 AM
> To: Martin Buchholz
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Queue.add() vs Executor.exec()
>
>
>
>
> Martin Buchholz wrote:
> >From: Gregg Wonderly <gregg at cytetech.com>
> >>It seems that if the core thread pool size is non-zero that the
> associated
> >>number of threads would be running and prepared to take
> whatever was put onto
> >>the queue.
> >
> > The number of threads might be zero even if core pool size is non-zero.
> > Did you call prestartAllCoreThreads() ?
>
> No, I didn't do that.  I generally seem to code the use of these things
> correctly, and am able to type "executor.exec" correctly.  It's
> just those times
> when I don't which have caused me to ask this question.  The fact
> that Queue and
> ThreadPoolExecutor work together causes be to think about this a
> little harder.
>   It seems counter intuitive that you can't talk to the Queue
> directly.  As an
> example, imagine an API which passes around a Queue for code to
> add elements
> too.  You don't want to expose the TPE because that allows users
> to stop it,
> peek into it and otherwise futz with things that you might not
> intend them to
> have access to.  The Queue, while allowing certain types of
> modifications, is a
> different beast.  I guess, in the end, creating a wrapper object
> to hide them
> both and expose the minimal API would work fine.
>
> I'm just trying to get some thoughts from others on whether this
> issue seems
> important.  The more I think about it, the more I think that
> these two objects
> don't work together quite as well as they could/should.
>
> It seems that prestartAllCoreThreads() should be the default with
> a constructor
> that could say otherwise.
>
> > In general, adding directly to the queue is asking for trouble.
> > I dream of writing my own thread pool where the queue would not be
> > user-accessible.
>
> Adding to the queue, somehow floats into by typing patterns from
> using queues
> and other collections in other places so frequently.  So,
> occasionally, I seem
> to be good at typing queue.add instead of exec.execute, and
> that's the issue
> that I'm trying to bring to light.  The software fails to work,
> and it takes
> considerable investigation in some cases to see that the wrong
> object is being
> used in that way.
>
> > A number of issues with TPE/STPE are being worked on for jdk7,
> > including some that leave the pool with less than the
> > desired number of threads.
>
> It will be great to get these issues addressed.  I'm just
> wondering if there
> shouldn't be some investigation and thought into a mechanism
> which could keep
> one from manipulating (in terms of mutations) the queue object
> once the TPE is
> using it.
>
> In particular, I'm thinking about something like a
> CollectionModificationListener that might be a more general
> concept usable in
> other cases that could be used by the TPE to know that the user is adding
> objects directly, and either throw an exception, or allow it to
> happen taking
> the appropriate actions.
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Wed Feb  7 03:45:06 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 7 Feb 2007 09:45:06 +0100
Subject: [concurrency-interest] Fwd: ReadWriteLocks and Conditions
In-Reply-To: <63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<63b4e4050702060539h68c27030h26a3aa7bd59cb150@mail.gmail.com>
	<1466c1d60702060614g12d3990bg92c3cc2d8c8721fd@mail.gmail.com>
	<1466c1d60702060615u48a9fa06w54630b38b6ab5c49@mail.gmail.com>
	<63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
Message-ID: <1466c1d60702070045q6ab59751xf6cec35d5e32876a@mail.gmail.com>

Hi Tim,

I haven't found another synchronization stone that fitted my needs.
Can you point me to the work others have done? Something I want to
prevent is 'modifying' other synchronization structures so they behave
the way I want. Concurrency control is complex, and that is why I
prefer using synchronization structures that make clear what they are
doing.

I'm going to look at simplifying the design by using a semaphore.
Thank you for the suggestion.



On 2/6/07, Tim Peierls <tim at peierls.net> wrote:
> LendableReference is essentially a resource pool of size 1. Other folks have
> done a lot of work on the general case -- why not take advantage of that?
>
>  Or if you really want to roll your own, consider using Semaphores, as
> described in section 5.5.3 of Java Concurrency in Practice.
>
>  --tim
>
>
> On 2/6/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
>
> > Example usage:
> >
> > while(true){
> >         Runnable task = lendableRef.take();
> >         try{
> >                 task.execute();
> >         }finally{
> >                 lendableRef.takeBack(task);
> >         }
> > }
> >
> > By placing a Runnable in the lendableRef, the task is being executed.
> > By removing the Runnable from the lendableRef (so setting a null
> > value) the take blocks.
> >
>

From alarmnummer at gmail.com  Wed Feb  7 03:55:54 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 7 Feb 2007 09:55:54 +0100
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <45C894F7.8060306@cytetech.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<45C894F7.8060306@cytetech.com>
Message-ID: <1466c1d60702070055o2f332b18j4a5ff1ba2bc0adc@mail.gmail.com>

Hi Gregg,

I see what you mean. You are using a future as a latch (a point
threads can fall through when a condition has been met). That would
also be a good alternative.

ps:
I'm trying to give each synchronization stone a specific features. So
integrating the runnable with the lendablereference would not be my
first solution. The LendableReference is (like the name says) a
reference that can be lend to multiple threads (if there is one
available) and if no reference is available, they block untill one is
available. It could be used to pass runnable instances through, but it
also could be used for other types of references.



On 2/6/07, Gregg Wonderly <gregg at cytetech.com> wrote:
>
>
> Peter Veentjer wrote:
> > I don't see how a Future would fit in, maybe you could elaborate on this?
>
> >>>I'm working on a structure called the LendeableReference. This
> >>>structure makes it possible to lend a reference to multiple threads
> >>>(values are taken from the LendableReference) and after a thread is
> >>>done with the reference, it takes it back so that a new reference can
> >>>be set. If no reference is available, taking threads block until a
> >>>reference is available.
>
> I don't know that I understand the constraints that you want to maintain, but
> based on your comments, it seems to me that the lended reference should access a
> Future.  The user of that object would 'get' the value, and thus block when
> there is no reference available (yet).  The algorithm that would apply in that
> case, is that they 'readers' would always ask a factory for the appropriate
> Future and thus use a relevant new Future when needed.
>
> Here's something that you can pass around, and the users can "get" the value at
> anytime.  You can expand this to do more things about deferring object creation
> beyond the simple setValue() implementation, but this is what I was thinking about.
>
> Maybe you could elaborate on the specifics of what else you need if this is not
> appropriate.
>
> public class LendableReference<T> implements Runnable {
>         volatile FutureTask<T> fut;
>         volatile T val;
>         public LendableReference( T value ) {
>                 setValue( value );
>         }
>
>         public LendableReference( Callable<T> call ) {
>                 setValue(call);
>         }
>
>         public void setValue( T value ) {
>                 val = value;
>                 fut = new FutureTask<T>( this, value );
>         }
>
>         public void setValue( Callable<T> call ) {
>                 fut = new FutureTask<T>( call );
>         }
>
>         public T get() {
>                 return val = fut.get();
>         }
>
>         /**
>          *  Do nothing to create value.  If you need to do something, override
>          *  run to do the work.
>          */
>         public void run() {}
> }
>
>

From alarmnummer at gmail.com  Wed Feb  7 04:17:14 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 7 Feb 2007 10:17:14 +0100
Subject: [concurrency-interest] synchronize design
In-Reply-To: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
References: <63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
	<BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
Message-ID: <1466c1d60702070117k25614f2dh510e7473e987e998@mail.gmail.com>

Another solution would be the following:

Create an Executor with a single thread and let this thread
communicate with the structure (if the structure is accessed by a
single thread, you don't need concurrency control in the structure
itself).

If a client want to read something, it creates a runnable, places it
on the executor and by using a Future the thread can wait for
completion and the result. The same goes for the writes.

And to give writes a higher priority than the reads, you could use a
BlockingQueue as workQueue that is able to deal with priorities. Items
with a higher priority fall through faster than items with a lower
priortity (give the writes a higher priority than the reads).

I guess there are lots of other solutions, but I don't have enough
information to give a good answer.





On 2/7/07, Rajesh Balamohan <rbalamohan at sonoasystems.com> wrote:
> Can you try checking ReadWrite Locks in JDK 1.5 for this?.. Aquire the
> write lock only when you want to change it.
>
> ~Rajesh.B
>
> sidali ikhlef wrote:
> > Hi,
> >
> > I have multiple threads accessing a shared class called Resources, witch is
> > composed of many other objects such as Current Activity, current environment
> > ..... I want to synchronize the access to this resource but not using simple
> > mechanisms like synchronize or synchronize(this). Any help or referencing
> > will be appreciated.
> >
> > Precision: writing to the resource must have higher priority than reading
> > and the chronology is very important to keep data coherent.
> >
> > Regards.
> >
> > _________________________________________________________________
> > MSN Messenger: appels gratuits de PC ? PC !
> > http://www.msn.fr/newhotmail/Default.asp?Ath=f
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>


From tim at peierls.net  Wed Feb  7 11:15:30 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 7 Feb 2007 11:15:30 -0500
Subject: [concurrency-interest] Fwd: ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702070045q6ab59751xf6cec35d5e32876a@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<63b4e4050702060539h68c27030h26a3aa7bd59cb150@mail.gmail.com>
	<1466c1d60702060614g12d3990bg92c3cc2d8c8721fd@mail.gmail.com>
	<1466c1d60702060615u48a9fa06w54630b38b6ab5c49@mail.gmail.com>
	<63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
	<1466c1d60702070045q6ab59751xf6cec35d5e32876a@mail.gmail.com>
Message-ID: <63b4e4050702070815y31403858g2e3002b387e3b630@mail.gmail.com>

I'm saying the signature of your LendableReference looks like a resource
pool. The restriction to pool size of 1 doesn't affect the interface, so why
limit it unnecessarily at that level?

    interface ResourcePool<T> {
        void release(T resource);
        T acquire() throws InterruptedException;
        T tryAcquire();
        T tryAcquire(long timeout, TimeUnit unit) throws
InterruptedException;
    }

    abstract class AbstractResourcePool<T> implements ResourcePool<T> {
        private final Semaphore semaphore;
        protected AbstractResourcePool(int poolSize) {
            this.semaphore = new Semaphore(poolSize);
        }
        protected abstract T allocateResource();
        protected abstract void deallocateResource(T resource);
        ...
        public void release(T resource) {
            deallocateResource(resource);
            semaphore.release();
        }
        public T acquire() throws InterruptedException {
            semaphore.acquire();
            return allocateResource();
        }
        public T tryAcquire() {
             return semaphore.tryAcquire() ? allocateResource() : null;
        }
        public T tryAcquire(long timeout, TimeUnit unit) throws
InterruptedException {
            return semaphore.tryAcquire(timeout, unit) ? allocateResource()
: null;
        }
     }

--tim

On 2/7/07, Peter Veentjer <alarmnummer at gmail.com > wrote:
>
> Hi Tim,
>
> I haven't found another synchronization stone that fitted my needs.
> Can you point me to the work others have done? Something I want to
> prevent is 'modifying' other synchronization structures so they behave
> the way I want. Concurrency control is complex, and that is why I
> prefer using synchronization structures that make clear what they are
> doing.
>
> I'm going to look at simplifying the design by using a semaphore.
> Thank you for the suggestion.
>
>
>
> On 2/6/07, Tim Peierls < tim at peierls.net> wrote:
> > LendableReference is essentially a resource pool of size 1. Other folks
> have
> > done a lot of work on the general case -- why not take advantage of
> that?
> >
> >  Or if you really want to roll your own, consider using Semaphores, as
> > described in section 5.5.3 of Java Concurrency in Practice.
> >
> >  --tim
> >
> >
> > On 2/6/07, Peter Veentjer < alarmnummer at gmail.com> wrote:
> >
> > > Example usage:
> > >
> > > while(true){
> > >         Runnable task = lendableRef.take();
> > >         try{
> > >                 task.execute();
> > >         }finally{
> > >                 lendableRef.takeBack(task);
> > >         }
> > > }
> > >
> > > By placing a Runnable in the lendableRef, the task is being executed.
> > > By removing the Runnable from the lendableRef (so setting a null
> > > value) the take blocks.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/e54095c9/attachment.html 

From s_ikhlef at hotmail.com  Wed Feb  7 11:45:04 2007
From: s_ikhlef at hotmail.com (sidali ikhlef)
Date: Wed, 07 Feb 2007 16:45:04 +0000
Subject: [concurrency-interest] synchronize design
In-Reply-To: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
Message-ID: <BAY122-F265896D36B32169FCE658DE69E0@phx.gbl>


Hi,

First thank you for all your suggestions. I will explore in details all 
propositions. Before that, i want to give more details about the 
synchronization problem:

I have four agents or simply threads, in the contex of intelligent tutoring 
systems, that i launch first. When i create the agents, i give them the 
shared resource throw constructor called Session. Session includes all 
learner session informations. Each agent can reads or writes on Session.

public class Session {

    private CurrentActivity currentActivity = null;
    private Environment environment = null;
    private GraphBuilder gb = null;
}

CurrentActivity can be extyended by many types 
CurrentProblemSolvingActivity, CurrentDemonstrationActivity ... etc.

1-Where exactly can i perform high leve synchronization mecanisms. On 
Session class, on ConcurrentActivity class, on each extended classes or on 
thread bodies?

2- Is it possible to synchronize using getters like:
    public Activity getCurrentActivity(){
    return  currentActivity;
    }
and how?

Regards.

_________________________________________________________________
MSN Messenger : discutez en direct avec vos amis ! 
http://www.msn.fr/msger/default.asp


From hanson.char at gmail.com  Wed Feb  7 13:06:24 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 7 Feb 2007 10:06:24 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
Message-ID: <ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>

Hi,

Can I presume the jcip/retry/*.java source code is in the public domain ?
If not, please kindly advise the licensing used.

Thanks,
Hanson

Josh told me he'd be willing to distribute this more widely if there was
> sufficient interest.
>
> I've placed an incomplete draft of this code in a public area of the Java
> Concurrency in Practice source repository:
>
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
>
> It's incomplete because it doesn't define AbstractRetryPolicy.
>
> What Bela describes is slightly different: each task can be submitted with
> its own "RetryPolicy". That could be achieved within Josh's framework, for
> example, by extending ScheduledThreadPoolExecutor and overriding
> decorateTask to do the requisite wrapping for tasks that implement
> RetryPolicy.
>
> --tim
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/7a2afa9d/attachment.html 

From joe.bowbeer at gmail.com  Wed Feb  7 13:47:51 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Feb 2007 10:47:51 -0800
Subject: [concurrency-interest] synchronize design
In-Reply-To: <BAY122-F265896D36B32169FCE658DE69E0@phx.gbl>
References: <BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
	<BAY122-F265896D36B32169FCE658DE69E0@phx.gbl>
Message-ID: <31f2a7bd0702071047u249eb8ddm6bd6aa2553d8f4e8@mail.gmail.com>

The information is still too sketchy to provide much concrete advise.
But I can think of three things:

1. If possible, make the fields of Session 'final'.

2. See JCiP for a thorough discussion of protecting mutable state and
GuardedBy documentation.

3. For two ReadWriteLock examples, see the javadoc for

 java.util.concurrent.locks.ReentrantReadWriteLock

You could for example add the R-W lock to Session and require all
clients to use it:

 public final ReadWriteLock rwl = new ReentrantReadWriteLock();

(Or you could build it into every Session method, or you could
implement Session as an active object, as Peter suggests, so that all
requests are processed by a single thread dedicated to Session...)


On 2/7/07, sidali ikhlef <s_ikhlef at hotmail.com> wrote:
>
> First thank you for all your suggestions. I will explore in details all
> propositions. Before that, i want to give more details about the
> synchronization problem:
>
> I have four agents or simply threads, in the contex of intelligent tutoring
> systems, that i launch first. When i create the agents, i give them the
> shared resource throw constructor called Session. Session includes all
> learner session informations. Each agent can reads or writes on Session.
>
> public class Session {
>
>     private CurrentActivity currentActivity = null;
>     private Environment environment = null;
>     private GraphBuilder gb = null;
> }
>
> CurrentActivity can be extyended by many types
> CurrentProblemSolvingActivity, CurrentDemonstrationActivity ... etc.
>
> 1-Where exactly can i perform high leve synchronization mecanisms. On
> Session class, on ConcurrentActivity class, on each extended classes or on
> thread bodies?
>
> 2- Is it possible to synchronize using getters like:
>     public Activity getCurrentActivity(){
>     return  currentActivity;
>     }
> and how?
>
> Regards.
>

From tim at peierls.net  Wed Feb  7 13:51:47 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 7 Feb 2007 13:51:47 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
Message-ID: <63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>

I don't know. Best thing would be for Josh to decide how he wants it
distributed.

--tim

On 2/7/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi,
>
> Can I presume the jcip/retry/*.java source code is in the public domain ?
> If not, please kindly advise the licensing used.
>
> Thanks,
> Hanson
>
> Josh told me he'd be willing to distribute this more widely if there was
> > sufficient interest.
> >
> > I've placed an incomplete draft of this code in a public area of the Java
> > Concurrency in Practice source repository:
> >
> > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> >
> > It's incomplete because it doesn't define AbstractRetryPolicy.
> >
> > What Bela describes is slightly different: each task can be submitted
> > with its own "RetryPolicy". That could be achieved within Josh's framework,
> > for example, by extending ScheduledThreadPoolExecutor and overriding
> > decorateTask to do the requisite wrapping for tasks that implement
> > RetryPolicy.
> >
> > --tim
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/84b11368/attachment.html 

From josh at bloch.us  Wed Feb  7 14:00:52 2007
From: josh at bloch.us (Joshua Bloch)
Date: Wed, 7 Feb 2007 11:00:52 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
Message-ID: <b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>

Sure; I dont' think public domain will cause any problems.  Google and I
hereby release it into the public domain. (It may end up in the JDK some
day, but it may not.)

                 Josh

On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
>
> I don't know. Best thing would be for Josh to decide how he wants it
> distributed.
>
> --tim
>
> On 2/7/07, Hanson Char < hanson.char at gmail.com> wrote:
> >
> > Hi,
> >
> > Can I presume the jcip/retry/*.java source code is in the public domain
> > ?  If not, please kindly advise the licensing used.
> >
> > Thanks,
> > Hanson
> >
> > Josh told me he'd be willing to distribute this more widely if there was
> > > sufficient interest.
> > >
> > > I've placed an incomplete draft of this code in a public area of the Java
> > > Concurrency in Practice source repository:
> > >
> > > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> > >
> > > It's incomplete because it doesn't define AbstractRetryPolicy.
> > >
> > > What Bela describes is slightly different: each task can be submitted
> > > with its own "RetryPolicy". That could be achieved within Josh's framework,
> > > for example, by extending ScheduledThreadPoolExecutor and overriding
> > > decorateTask to do the requisite wrapping for tasks that implement
> > > RetryPolicy.
> > >
> > > --tim
> > >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/49e081d0/attachment.html 

From tim at peierls.net  Wed Feb  7 14:04:13 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 7 Feb 2007 14:04:13 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
Message-ID: <63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>

Thanks! Could you post AbstractRetryPolicy? I can't find it.

--tim

On 2/7/07, Joshua Bloch <josh at bloch.us> wrote:
>
> Sure; I dont' think public domain will cause any problems.  Google and I
> hereby release it into the public domain. (It may end up in the JDK some
> day, but it may not.)
>
>                  Josh
>
> On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
> >
> > I don't know. Best thing would be for Josh to decide how he wants it
> > distributed.
> >
> > --tim
> >
> > On 2/7/07, Hanson Char < hanson.char at gmail.com> wrote:
> > >
> > > Hi,
> > >
> > > Can I presume the jcip/retry/*.java source code is in the public
> > > domain ?  If not, please kindly advise the licensing used.
> > >
> > > Thanks,
> > > Hanson
> > >
> > > Josh told me he'd be willing to distribute this more widely if there
> > > > was sufficient interest.
> > > >
> > > > I've placed an incomplete draft of this code in a public area of the
> > > > Java Concurrency in Practice source repository:
> > > >
> > > > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> > > >
> > > > It's incomplete because it doesn't define AbstractRetryPolicy.
> > > >
> > > > What Bela describes is slightly different: each task can be
> > > > submitted with its own "RetryPolicy". That could be achieved within Josh's
> > > > framework, for example, by extending ScheduledThreadPoolExecutor and
> > > > overriding decorateTask to do the requisite wrapping for tasks that
> > > > implement RetryPolicy.
> > > >
> > > > --tim
> > > >
> > >
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/ba768ecd/attachment.html 

From gregg at cytetech.com  Wed Feb  7 11:04:34 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 07 Feb 2007 10:04:34 -0600
Subject: [concurrency-interest] Queue.add() vs Executor.exec()
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECMHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCIECMHFAA.dcholmes@optusnet.com.au>
Message-ID: <45C9F892.7010103@cytetech.com>



David Holmes wrote:
> Sorry for the late reply but I was on vacation and am still catching up.
> 
> To be able to use the queue directly and have it "just work" either the
> queue needs to know about the pool, or else the pool must always have
> threads ready to read from the queue. Having the queue know about the pool
> means the queue is no longer just a BlockingQueue. Have the pool always
> ready to read from the queue means you don't have control of the number of
> threads that you generally would want (you could add heuristics to examine
> the length of the queue after take() returns and use that to add more
> threads etc - but that's messy).

Hi David, thanks for the followup.  In one of my queuing based thread pools, I 
use a multiplication factor to control the number of threads that are active. 
The enqueuing method basically does.

	if( curThreadCount < maxThreadCnt &&
			queueSize > curThreadCount * multFactor ) {
		startNewThread();
	}

this makes it possible for a fairly simple management of the total number of 
threads running without an unnecessary ramp up of active threads.  The 
multFactor value is something that the application can tune based on its needs. 
  The main user of this thread pool is a database application which processes 
about 200 transactions per second into a handful of databases.  The db admin 
like the fact that they can tune the number of threads so that it is minimal 
based on the fairly predictable bandwidth through the system.  But, when they 
reboot the database server and things are backed up, it will ramp up to get the 
queue cleared out more quickly to make sure the database is in sync ASAP.

Gregg Wonderly

Gregg Wonderly


From joe.bowbeer at gmail.com  Wed Feb  7 14:21:03 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Feb 2007 11:21:03 -0800
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <1466c1d60702070055o2f332b18j4a5ff1ba2bc0adc@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<45C894F7.8060306@cytetech.com>
	<1466c1d60702070055o2f332b18j4a5ff1ba2bc0adc@mail.gmail.com>
Message-ID: <31f2a7bd0702071121w9dcb7b1u4ddccec4b48b268e@mail.gmail.com>

Peter,

I think your implementation looks reasonable and nothing pops out as
incorrect (but this is no seal of approval of course).

It's implementing some interesting behavior (waiting for a value to
appear) that's sort of Future-like (or queue like).  But then you
allow users to change the value...  And then you allow unlimited
checkouts...  The checkouts, once the ref exists, might use an
unbounded semaphore.  Delegating more of the implementation to objects
such as these (future/semaphore) may be an improvement -- but your
implementation is already fairly concise.


By the way, have you looked at JavaSpaces?  What you are describing
sounds like a JavaSpaces kind of thing.  Its transactions add a nice
bit of robustness to the checkouts.

http://www.dancres.org/cottage/javaspaces.html

(Though adding JavaSpaces to your project may add a whole 'nother
level of pain -- or does Blitz make it all better now??)


On 2/7/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
> Hi Gregg,
>
> I see what you mean. You are using a future as a latch (a point
> threads can fall through when a condition has been met). That would
> also be a good alternative.
>
> ps:
> I'm trying to give each synchronization stone a specific features. So
> integrating the runnable with the lendablereference would not be my
> first solution. The LendableReference is (like the name says) a
> reference that can be lend to multiple threads (if there is one
> available) and if no reference is available, they block untill one is
> available. It could be used to pass runnable instances through, but it
> also could be used for other types of references.
>
>
>
> On 2/6/07, Gregg Wonderly <gregg at cytetech.com> wrote:
> >
> >
> > Peter Veentjer wrote:
> > > I don't see how a Future would fit in, maybe you could elaborate on this?
> >
> > >>>I'm working on a structure called the LendeableReference. This
> > >>>structure makes it possible to lend a reference to multiple threads
> > >>>(values are taken from the LendableReference) and after a thread is
> > >>>done with the reference, it takes it back so that a new reference can
> > >>>be set. If no reference is available, taking threads block until a
> > >>>reference is available.
> >
> > I don't know that I understand the constraints that you want to maintain, but
> > based on your comments, it seems to me that the lended reference should access a
> > Future.  The user of that object would 'get' the value, and thus block when
> > there is no reference available (yet).  The algorithm that would apply in that
> > case, is that they 'readers' would always ask a factory for the appropriate
> > Future and thus use a relevant new Future when needed.
> >
> > Here's something that you can pass around, and the users can "get" the value at
> > anytime.  You can expand this to do more things about deferring object creation
> > beyond the simple setValue() implementation, but this is what I was thinking about.
> >
> > Maybe you could elaborate on the specifics of what else you need if this is not
> > appropriate.
> >
> > public class LendableReference<T> implements Runnable {
> >         volatile FutureTask<T> fut;
> >         volatile T val;
> >         public LendableReference( T value ) {
> >                 setValue( value );
> >         }
> >
> >         public LendableReference( Callable<T> call ) {
> >                 setValue(call);
> >         }
> >
> >         public void setValue( T value ) {
> >                 val = value;
> >                 fut = new FutureTask<T>( this, value );
> >         }
> >
> >         public void setValue( Callable<T> call ) {
> >                 fut = new FutureTask<T>( call );
> >         }
> >
> >         public T get() {
> >                 return val = fut.get();
> >         }
> >
> >         /**
> >          *  Do nothing to create value.  If you need to do something, override
> >          *  run to do the work.
> >          */
> >         public void run() {}
> > }
> >
> >
>

From hanson.char at gmail.com  Wed Feb  7 18:03:38 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 7 Feb 2007 15:03:38 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
Message-ID: <ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>

Excellent!  The existing RetryPolicies compiles only under Java 5 (not Java
6).  I've fixed up a version that preserves all the generics but compiles
only under Java 6.  Can send you a copy if interested.

Hanson

On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
>
> Thanks! Could you post AbstractRetryPolicy? I can't find it.
>
> --tim
>
> On 2/7/07, Joshua Bloch <josh at bloch.us > wrote:
> >
> > Sure; I dont' think public domain will cause any problems.  Google and I
> > hereby release it into the public domain. (It may end up in the JDK some
> > day, but it may not.)
> >
> >                  Josh
> >
> > On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
> > >
> > > I don't know. Best thing would be for Josh to decide how he wants it
> > > distributed.
> > >
> > > --tim
> > >
> > > On 2/7/07, Hanson Char < hanson.char at gmail.com> wrote:
> > > >
> > > > Hi,
> > > >
> > > > Can I presume the jcip/retry/*.java source code is in the public
> > > > domain ?  If not, please kindly advise the licensing used.
> > > >
> > > > Thanks,
> > > > Hanson
> > > >
> > > > Josh told me he'd be willing to distribute this more widely if there
> > > > > was sufficient interest.
> > > > >
> > > > > I've placed an incomplete draft of this code in a public area of
> > > > > the Java Concurrency in Practice source repository:
> > > > >
> > > > > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> > > > >
> > > > >
> > > > > It's incomplete because it doesn't define AbstractRetryPolicy.
> > > > >
> > > > > What Bela describes is slightly different: each task can be
> > > > > submitted with its own "RetryPolicy". That could be achieved within Josh's
> > > > > framework, for example, by extending ScheduledThreadPoolExecutor and
> > > > > overriding decorateTask to do the requisite wrapping for tasks that
> > > > > implement RetryPolicy.
> > > > >
> > > > > --tim
> > > > >
> > > >
> > > >
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070207/5bd31b4d/attachment.html 

From joe.bowbeer at gmail.com  Wed Feb  7 23:00:13 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Feb 2007 20:00:13 -0800
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <31f2a7bd0702071121w9dcb7b1u4ddccec4b48b268e@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>
	<45C74B15.3050300@cox.net>
	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>
	<45C894F7.8060306@cytetech.com>
	<1466c1d60702070055o2f332b18j4a5ff1ba2bc0adc@mail.gmail.com>
	<31f2a7bd0702071121w9dcb7b1u4ddccec4b48b268e@mail.gmail.com>
Message-ID: <31f2a7bd0702072000u5152ff1di1544e685dcc37fa9@mail.gmail.com>

Correction.

> [JavaSpaces'] transactions add a nice bit of robustness to checkouts.
>

I meant "lease" instead of "transaction".  Leases prevent one laggard
client from messing up the pool for everyone else.


On 2/7/07, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> Peter,
>
> I think your implementation looks reasonable and nothing pops out as
> incorrect (but this is no seal of approval of course).
>
> It's implementing some interesting behavior (waiting for a value to
> appear) that's sort of Future-like (or queue like).  But then you
> allow users to change the value...  And then you allow unlimited
> checkouts...  The checkouts, once the ref exists, might use an
> unbounded semaphore.  Delegating more of the implementation to objects
> such as these (future/semaphore) may be an improvement -- but your
> implementation is already fairly concise.
>
>
> By the way, have you looked at JavaSpaces?  What you are describing
> sounds like a JavaSpaces kind of thing.  Its transactions add a nice
> bit of robustness to the checkouts.
>
> http://www.dancres.org/cottage/javaspaces.html
>
> (Though adding JavaSpaces to your project may add a whole 'nother
> level of pain -- or does Blitz make it all better now??)
>
>
> On 2/7/07, Peter Veentjer <alarmnummer at gmail.com> wrote:
> > Hi Gregg,
> >
> > I see what you mean. You are using a future as a latch (a point
> > threads can fall through when a condition has been met). That would
> > also be a good alternative.
> >
> > ps:
> > I'm trying to give each synchronization stone a specific features. So
> > integrating the runnable with the lendablereference would not be my
> > first solution. The LendableReference is (like the name says) a
> > reference that can be lend to multiple threads (if there is one
> > available) and if no reference is available, they block untill one is
> > available. It could be used to pass runnable instances through, but it
> > also could be used for other types of references.
> >
> >
> >
> > On 2/6/07, Gregg Wonderly <gregg at cytetech.com> wrote:
> > >
> > >
> > > Peter Veentjer wrote:
> > > > I don't see how a Future would fit in, maybe you could elaborate on this?
> > >
> > > >>>I'm working on a structure called the LendeableReference. This
> > > >>>structure makes it possible to lend a reference to multiple threads
> > > >>>(values are taken from the LendableReference) and after a thread is
> > > >>>done with the reference, it takes it back so that a new reference can
> > > >>>be set. If no reference is available, taking threads block until a
> > > >>>reference is available.
> > >
> > > I don't know that I understand the constraints that you want to maintain, but
> > > based on your comments, it seems to me that the lended reference should access a
> > > Future.  The user of that object would 'get' the value, and thus block when
> > > there is no reference available (yet).  The algorithm that would apply in that
> > > case, is that they 'readers' would always ask a factory for the appropriate
> > > Future and thus use a relevant new Future when needed.
> > >
> > > Here's something that you can pass around, and the users can "get" the value at
> > > anytime.  You can expand this to do more things about deferring object creation
> > > beyond the simple setValue() implementation, but this is what I was thinking about.
> > >
> > > Maybe you could elaborate on the specifics of what else you need if this is not
> > > appropriate.
> > >
> > > public class LendableReference<T> implements Runnable {
> > >         volatile FutureTask<T> fut;
> > >         volatile T val;
> > >         public LendableReference( T value ) {
> > >                 setValue( value );
> > >         }
> > >
> > >         public LendableReference( Callable<T> call ) {
> > >                 setValue(call);
> > >         }
> > >
> > >         public void setValue( T value ) {
> > >                 val = value;
> > >                 fut = new FutureTask<T>( this, value );
> > >         }
> > >
> > >         public void setValue( Callable<T> call ) {
> > >                 fut = new FutureTask<T>( call );
> > >         }
> > >
> > >         public T get() {
> > >                 return val = fut.get();
> > >         }
> > >
> > >         /**
> > >          *  Do nothing to create value.  If you need to do something, override
> > >          *  run to do the work.
> > >          */
> > >         public void run() {}
> > > }
> > >
> > >
> >
>

From gergg at cox.net  Thu Feb  8 01:04:47 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Thu, 08 Feb 2007 00:04:47 -0600
Subject: [concurrency-interest] ReadWriteLocks and Conditions
In-Reply-To: <31f2a7bd0702072000u5152ff1di1544e685dcc37fa9@mail.gmail.com>
References: <1466c1d60702050640t6a968ca1g45440afa24ff8999@mail.gmail.com>	<45C74B15.3050300@cox.net>	<1466c1d60702060055qcd37b6ey8d15bc5f6ccd4806@mail.gmail.com>	<45C894F7.8060306@cytetech.com>	<1466c1d60702070055o2f332b18j4a5ff1ba2bc0adc@mail.gmail.com>	<31f2a7bd0702071121w9dcb7b1u4ddccec4b48b268e@mail.gmail.com>
	<31f2a7bd0702072000u5152ff1di1544e685dcc37fa9@mail.gmail.com>
Message-ID: <45CABD7F.7070608@cox.net>

Joe Bowbeer wrote:
> Correction.
>
>>[JavaSpaces'] transactions add a nice bit of robustness to checkouts.
> 
> I meant "lease" instead of "transaction".  Leases prevent one laggard
> client from messing up the pool for everyone else.

Jini's distributed 2 phase commit lock implementation is very powerful for doing 
distributed locking.  JGroups can be good for certain kinds of locks.  Dan has 
an example distributed lock application which is good for latches.  I use it for 
creating database domain indexes in an application which I use jini transactions 
to keep multiple instances of postgres synchronized.

Gregg Wonderly

From alarmnummer at gmail.com  Thu Feb  8 05:02:53 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 8 Feb 2007 11:02:53 +0100
Subject: [concurrency-interest] synchronize design
In-Reply-To: <45CB2C32.9000809@sonoasystems.com>
References: <63b4e4050702061442x1382e5c6jc328e8cb9d539e6@mail.gmail.com>
	<BAY122-F296330EFEBA266DA13C9EEE69E0@phx.gbl>
	<1466c1d60702070117k25614f2dh510e7473e987e998@mail.gmail.com>
	<45CB2C32.9000809@sonoasystems.com>
Message-ID: <1466c1d60702080202m1c17d368n6a5e0cdef97aab89@mail.gmail.com>

Hi Rajesh

You are creating a lot of objects, but modern virtual machines have a
very good performance (a lot of recycling). So unless you have very
high performance requirements, I don't think it is an issue. You could
create a benchmark and check if the performance is acceptable.

And it is true that you need locking (the workqueue does it for you).
But locking is not as expensive as it was once. The locks that cause
problems are locks that are kept for a long time (you get lock
contention). And in both cases (with a read/writelock and with an
executor) you could get lock contention (especially with futures). But
if you don't have to do a lot of stuff in that strucure, and you don't
need to access slow io devices, I don't think it would be a problem
(although I don't know enough of your requirements).

The technique I describe, is also explained in "Java Concurrency in
Practice" and I have used it in a few projects, and it works like a
dream. Concurrency control related complexity is reduced quite a lot
(unlike with rwlocks) and performance has always been good enough.

On 2/8/07, Rajesh Balamohan <rbalamohan at sonoasystems.com> wrote:
>
>  Peter - It was a different perspective on approaching synch issues.
>
>  I was just wondering about the performance in this case. If we follow this
> approach compared to ReadWriteLocks, won't we be creating lots of objects
> and queueing and dequeiing (which internally might be using locks?).
>
>  Wont it add it more cost to do the synch operation?
>
>  ~Rajesh.B
>
>
>  Peter Veentjer wrote:
>  Another solution would be the following:
>
> Create an Executor with a single thread and let this thread
> communicate with the structure (if the structure is accessed by a
> single thread, you don't need concurrency control in the structure
> itself).
>
> If a client want to read something, it creates a runnable, places it
> on the executor and by using a Future the thread can wait for
> completion and the result. The same goes for the writes.
>
> And to give writes a higher priority than the reads, you could use a
> BlockingQueue as workQueue that is able to deal with priorities. Items
> with a higher priority fall through faster than items with a lower
> priortity (give the writes a higher priority than the reads).
>
> I guess there are lots of other solutions, but I don't have enough
> information to give a good answer.
>
>
>
>
>
> On 2/7/07, Rajesh Balamohan <rbalamohan at sonoasystems.com> wrote:
>
>
>  Can you try checking ReadWrite Locks in JDK 1.5 for this?.. Aquire the
> write lock only when you want to change it.
>
> ~Rajesh.B
>
> sidali ikhlef wrote:
>
>
>  Hi,
>
> I have multiple threads accessing a shared class called Resources, witch is
> composed of many other objects such as Current Activity, current environment
> ..... I want to synchronize the access to this resource but not using simple
> mechanisms like synchronize or synchronize(this). Any help or referencing
> will be appreciated.
>
> Precision: writing to the resource must have higher priority than reading
> and the chronology is very important to keep data coherent.
>
> Regards.
>
> _________________________________________________________________
> MSN Messenger: appels gratuits de PC ? PC !
> http://www.msn.fr/newhotmail/Default.asp?Ath=f
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>  _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>


From s_ikhlef at hotmail.com  Thu Feb  8 11:17:44 2007
From: s_ikhlef at hotmail.com (sidali ikhlef)
Date: Thu, 08 Feb 2007 16:17:44 +0000
Subject: [concurrency-interest] synchronize design
In-Reply-To: <1466c1d60702080202m1c17d368n6a5e0cdef97aab89@mail.gmail.com>
Message-ID: <BAY122-F17D6EF33392F06AF24D86DE69D0@phx.gbl>



Thanks so much for all your help. I will explore these excellent 
recommendations and solutions and call you back if i have any questions.

Regards

_________________________________________________________________
MSN Messenger : appels gratuits de PC ? PC ! 
http://www.msn.fr/newhotmail/Default.asp?Ath=f


From sergemasse1 at yahoo.com  Thu Feb  8 20:13:11 2007
From: sergemasse1 at yahoo.com (serge masse)
Date: Thu, 8 Feb 2007 17:13:11 -0800 (PST)
Subject: [concurrency-interest] Are functional languages so much better at
	concurrency?
Message-ID: <429754.25189.qm@web51401.mail.yahoo.com>

Joel, from joelonsoftware, claimed twice that functional languages are excellent for developing concurrent apps. 

He wrote twice that *purely functional programs have no side effects and are thus trivially parallelizable.*

He wrote this here, http://www.joelonsoftware.com/items/2006/08/01.html (a great intro to MapReduce, btw), and here: http://www.joelonsoftware.com/articles/ThePerilsofJavaSchools.html

My experience with functional programs goes a long way back, circa 1980s, and I do not seem to recall any significant advantages for concurrency over current mainstream languages, for example, Java. 

Do you agree or disagree with Joel on this one?

I'm searching for solutions to the problem of the huge difficulty in developing bug free concurrent apps and your opinion would be important to prioritize my search.

thanks
serge








-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070208/2e3cecf5/attachment.html 

From mwh at cs.umd.edu  Thu Feb  8 20:25:43 2007
From: mwh at cs.umd.edu (Michael Hicks)
Date: Thu, 8 Feb 2007 20:25:43 -0500
Subject: [concurrency-interest] Are functional languages so much better
	at concurrency?
In-Reply-To: <429754.25189.qm@web51401.mail.yahoo.com>
References: <429754.25189.qm@web51401.mail.yahoo.com>
Message-ID: <DF803E2B-0180-43CC-870A-80845A8BC865@cs.umd.edu>

Check out a recent article in Queue by Sutter and Larus, which  
addresses the larger issue you are worried about (correctness for  
concurrent apps), and briefly addresses functional programming in the  
context of other solutions:

http://research.microsoft.com/~larus/Papers/queue01.pdf
(the published version is at http://doi.acm.org/10.1145/1095408.1095421)

You might also find STM Haskell to be of interest (a version of the  
Haskell functional programming language outfitted with software  
transactional memory, used to support atomicity): http:// 
research.microsoft.com/~simonpj/papers/stm/stm.pdf

-Mike

On Feb 8, 2007, at 8:13 PM, serge masse wrote:

> Joel, from joelonsoftware, claimed twice that functional languages  
> are excellent for developing concurrent apps.
>
> He wrote twice that *purely functional programs have no side  
> effects and are thus trivially parallelizable.*
>
> He wrote this here, http://www.joelonsoftware.com/items/ 
> 2006/08/01.html (a great intro to MapReduce, btw), and here: http:// 
> www.joelonsoftware.com/articles/ThePerilsofJavaSchools.html
>
> My experience with functional programs goes a long way back, circa  
> 1980s, and I do not seem to recall any significant advantages for  
> concurrency over current mainstream languages, for example, Java.
>
> Do you agree or disagree with Joel on this one?
>
> I'm searching for solutions to the problem of the huge difficulty  
> in developing bug free concurrent apps and your opinion would be  
> important to prioritize my search.
>
> thanks
> serge
>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From brian at quiotix.com  Thu Feb  8 21:14:16 2007
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 08 Feb 2007 21:14:16 -0500
Subject: [concurrency-interest] Are functional languages so much better
 at	concurrency?
In-Reply-To: <429754.25189.qm@web51401.mail.yahoo.com>
References: <429754.25189.qm@web51401.mail.yahoo.com>
Message-ID: <45CBD8F8.3090104@quiotix.com>

> Joel, from joelonsoftware, claimed twice that functional languages are 
> excellent for developing concurrent apps.

The side-effect-free property is indeed nice and makes a lot of 
concurrency problems just go away.  Programming in this style (heavy use 
of immutable objects, sharing as little state as possible) in stateful 
languages like Java also makes concurrency easier.

So, the question is, are functional languages better or worse for 
ordinary programming?  My innate confidence in the efficient market 
hypothesis tells me there's a reason that languages like C and C++ and 
Java and Perl and Python have been so successful; they are well-suited 
to the problems most programmers need to solve and map well to how most 
programmers think.  Functional languages, despite their obvious 
mathematical advantages, don't seem to have caught on in mainstream 
programming.

From jseigh_cp00 at xemaps.com  Fri Feb  9 06:26:05 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Fri, 09 Feb 2007 06:26:05 -0500
Subject: [concurrency-interest] Are functional languages so much better
 at concurrency?
In-Reply-To: <DF803E2B-0180-43CC-870A-80845A8BC865@cs.umd.edu>
References: <429754.25189.qm@web51401.mail.yahoo.com>
	<DF803E2B-0180-43CC-870A-80845A8BC865@cs.umd.edu>
Message-ID: <45CC5A4D.2040401@xemaps.com>

Michael Hicks wrote:

>Check out a recent article in Queue by Sutter and Larus, which  
>addresses the larger issue you are worried about (correctness for  
>concurrent apps), and briefly addresses functional programming in the  
>context of other solutions:
>
>http://research.microsoft.com/~larus/Papers/queue01.pdf
>(the published version is at http://doi.acm.org/10.1145/1095408.1095421)
>
>You might also find STM Haskell to be of interest (a version of the  
>Haskell functional programming language outfitted with software  
>transactional memory, used to support atomicity): http:// 
>research.microsoft.com/~simonpj/papers/stm/stm.pdf
>
>  
>

There's a Java STM implementation on Herlihy's home page here
Maurice Herlihy's Home Page <http://www.cs.brown.edu/people/mph/home.html>
that you can try out.  He has it set up so you can try out other STM
implementations against it.  I haven't gotten to the point on my STM
implementation to see if the api's are compatible enough for that.

STM works well as long as your program is obstruction-free,  a term
Herlihy coined.

--
Joe Seigh

From osvaldo at visionnaire.com.br  Fri Feb  9 07:26:17 2007
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Fri, 09 Feb 2007 09:26:17 -0300
Subject: [concurrency-interest] Are functional languages so much better
 at	concurrency?
In-Reply-To: <45CBD8F8.3090104@quiotix.com>
References: <429754.25189.qm@web51401.mail.yahoo.com>
	<45CBD8F8.3090104@quiotix.com>
Message-ID: <45CC6869.9020206@visionnaire.com.br>

Brian Goetz escreveu:
>> Joel, from joelonsoftware, claimed twice that functional languages are 
>> excellent for developing concurrent apps.
>>     
> The side-effect-free property is indeed nice and makes a lot of 
> concurrency problems just go away.  Programming in this style (heavy use 
> of immutable objects, sharing as little state as possible) in stateful 
> languages like Java also makes concurrency easier.
>
> So, the question is, are functional languages better or worse for 
> ordinary programming?  My innate confidence in the efficient market 
> hypothesis tells me there's a reason that languages like C and C++ and 
> Java and Perl and Python have been so successful; they are well-suited 
> to the problems most programmers need to solve and map well to how most 
> programmers think.  Functional languages, despite their obvious 
> mathematical advantages, don't seem to have caught on in mainstream 
> programming.
> _______________________________________________
>   
Yes, I think the trick (like in many other cases) is combining the 
advantages of both models. Haskell is already a great innovation with 
its Modans system, which accomodates things that actually require 
side-effects like I/O in a ghetto that doesn't spoil the rest of the 
language. But they still try to be purely functional for all normal 
computation needs.

In Java, I try to keep code close to functional when possible. See 
http://www.netbeans.org/download/magazine/01/nb01_writing_quality.pdf, 
page 40, "/The final modifier and refactoring: towards functional-style 
Java/". Even inside a single method I try to avoid gratuitous side 
effects - a mutable for loop counter is fine, but updating a local 
variable in a  on-loop code path just doesn't make sense for me, just 
create a new variable and your code is even more readable (see article). 
Updating attributes of "worker" objects - things used temporarily (and 
locally) to perform some task that demand more state than it's 
comfortable to pass around by parameters through several methods - is 
okay too, just don't share the objects. But for shared data that gets 
held by static variables and accessible (even potentially) for multiple 
threads, I try to use only immutable objects. This requires some extra 
work and patience - like when a coworker complains "where are the 
f****** setters in your POJOs, I wanna update'em in the GUI!". But they 
payoff is great, I can build complex, highly scalable server apps where 
you can hardly find a synchronized keyword or a concurrency bug. 
Scalability is great and easy, double the CPUs and the throughput 
doubles, of course if there are no other issues like concurrent, 
pessimist database updates.

Interestingly, I think that the Java EE programming model helps here. 
Your are admonished to not use threads and synchronization, and while 
you can still do that with care, the whole framework is geared towards 
share-nothing code and transactions. Now, transactions are great for 
expensive JTA resources like database and JMS connections, but we still 
don't have STM with or without JTA support, so the EE model doesn't work 
for lean-and-mean shared stuff, such as configuration data, singleton 
instances, caches of fixed domain tables (those critical enough so that 
the overhead of an ORM tool is too big, even with L2 caching), "worker" 
objects that are more expensive to build than to execute, etc. Well, 
just make all that stuff immutable and your're ready to go: scalable, 
efficient AND Java EE-compatible, no locking required.

Great example if Java API design: java.util.regex. My app has a little 
expression language used in decision trees, so I can just bring the 
whole trees to memory and precompile the expressions; any regular 
expressions are Pattern.compile()'d, everything is put in a shared cache 
(beautiful name for static variables), and because all objects are 
immutable including the compiled Patterns, I can use them from multiple 
transactions/threads without synchronization. It's easy, fast, and 
bulletproof.

Bad example of API design: java.text. I cannot use the same trick for 
SimpleDateFormat and the likes, because these objects are neither 
immutable, nor thread-safe. I'm forced to create new formatter objects 
every time, and recreating these objects is expensive (I measured it) 
even though they seem to make some effort to reuse the precompiled state 
from an internal cache (if your're making all that effort, why not just 
working harder on the precompiled state so it can be immutable?). Add 
this to the parse() methods that only accept a String (Pattern at least, 
accepts CharSequence; ideally, both should accept readers/writers or 
streams) and these classes are totally useless for performance-critical 
code... So I end up writing my own date and number parsing/formatting 
methods for often-used formats.

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070209/0a15f878/attachment.html 

From Ryan.LeCompte at pango.com  Fri Feb  9 11:07:21 2007
From: Ryan.LeCompte at pango.com (Ryan LeCompte)
Date: Fri, 9 Feb 2007 11:07:21 -0500
Subject: [concurrency-interest] Synchronization/locking in a clustered JEE
	environment
Message-ID: <5F9C31E563BD404DB284240CAE7ACD52053492@pangomail2k3.pangonetworks.com>

Hello,
 
In simple Java applications that reside in a single JVM, it is safe to do things like the following when there are multiple threads of execution:
 
public synchronized void go() { ...}
 
or
 
synchronized (LOCK) {
   ...
}
 
However, consider the case where there are multiple instances of stateless session beans in a clustered JEE environment. You may have different instances of the same stateless session bean running in multiple JVMs. How would one ensure that a "global lock" is acquired if I have a method on the stateless session bean that looks like the following:
 
public void doSomething() {
   // Perform some logic that does not need to be globally protected....
 
   synchronized (LOCK) {
      // Perform some logic that DOES need to be globally protected....
   }
 
  // Perform other logic that does not need to be globally protected....
}
 
I know that I could use container-managed transactions to provide locking semantics at the method level, but what is the suggested approach to get finder-grained distributed locking as demonstrated in the above example? I'd love to be able to continue using the new 1.5 concurrency utilities for read/write locks, etc. Is there some general pattern that I'm not aware of?
 
Thanks,
Ryan
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070209/4f244b98/attachment-0001.html 

From teck at terracottatech.com  Fri Feb  9 12:08:49 2007
From: teck at terracottatech.com (Tim Eck)
Date: Fri, 09 Feb 2007 09:08:49 -0800
Subject: [concurrency-interest] Synchronization/locking in a clustered,
	JEE environment
In-Reply-To: <mailman.1230.1171037427.4262.concurrency-interest@altair.cs.oswego.edu>
References: <mailman.1230.1171037427.4262.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <45CCAAA1.2060403@terracottatech.com>

Ryan LeCompte wrote:

 > ...How would one ensure that a "global lock" is acquired if I have a
 > method on the stateless session bean

You might want to check out Terracotta (www.terracotta.org) for
something like this. In terracotta, an object's monitor (ie. java
synchronization) can be used in a distributed fashion. For instance,
you can take a standard instance of j.u.c.l.ReentrantLock, declare it to
be distributed, and then use it use across multiple VMs. Of course,
you can also regular synchronization and wait/notify with Terracotta
too (and many other things). Feel free to contact me directly or start
up a thread on of our mailing lists for more info

-tim
Disclaimer: I am employee of Terracotta.

From belaban at yahoo.com  Fri Feb  9 13:07:03 2007
From: belaban at yahoo.com (Bela Ban)
Date: Fri, 09 Feb 2007 19:07:03 +0100
Subject: [concurrency-interest] Synchronization/locking in a clustered,
 JEE environment
In-Reply-To: <45CCAAA1.2060403@terracottatech.com>
References: <mailman.1230.1171037427.4262.concurrency-interest@altair.cs.oswego.edu>
	<45CCAAA1.2060403@terracottatech.com>
Message-ID: <45CCB847.5050907@yahoo.com>

Question: in the code below, will we acquire 1000 cluster-wide locks ?

Lock lock=new ReentrantLock(); // make it distributed

for(int i=0; i < 1000; i++) {
  lock.lock(); // acquires a distributed lock across the cluster
  try {
    // do something
  }
  finally {
    lock.unlock();
  }
}

Tim Eck wrote:
> Ryan LeCompte wrote:
>
>  > ...How would one ensure that a "global lock" is acquired if I have a
>  > method on the stateless session bean
>
> You might want to check out Terracotta (www.terracotta.org) for
> something like this. In terracotta, an object's monitor (ie. java
> synchronization) can be used in a distributed fashion. For instance,
> you can take a standard instance of j.u.c.l.ReentrantLock, declare it to
> be distributed, and then use it use across multiple VMs. Of course,
> you can also regular synchronization and wait/notify with Terracotta
> too (and many other things). Feel free to contact me directly or start
> up a thread on of our mailing lists for more info
>
> -tim
> Disclaimer: I am employee of Terracotta.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>   

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat


From teck at terracottatech.com  Fri Feb  9 13:47:52 2007
From: teck at terracottatech.com (Tim Eck)
Date: Fri, 09 Feb 2007 10:47:52 -0800
Subject: [concurrency-interest] Synchronization/locking in a clustered,
	JEE environment
In-Reply-To: <45CCB847.5050907@yahoo.com>
References: <mailman.1230.1171037427.4262.concurrency-interest@altair.cs.oswego.edu>
	<45CCAAA1.2060403@terracottatech.com> <45CCB847.5050907@yahoo.com>
Message-ID: <45CCC1D8.40303@terracottatech.com>

Bela Ban wrote:
> Question: in the code below, will we acquire 1000 cluster-wide locks ?
> 
> Lock lock=new ReentrantLock(); // make it distributed
> 
> for(int i=0; i < 1000; i++) {
>  lock.lock(); // acquires a distributed lock across the cluster
>  try {
>    // do something
>  }
>  finally {
>    lock.unlock();
>  }
> }

As a courtesy of to the members of concurrency-interest, a terracotta specific discussion is perhaps better suited for a 
terracotta specific mailing list or forum.

With regards to your question, the answer is that it depends. In the absence of contention for the lock from another 
node, there will only be one cluster level (ie. remote) lock acquisition necessary. If there is contention from another 
thread in the same node, then lock will behave the same as if it were not distributed at all.

-tim


From matthias.ernst at coremedia.com  Fri Feb  9 14:15:04 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Fri, 9 Feb 2007 20:15:04 +0100
Subject: [concurrency-interest] Feedback requested: virtual threading on top
	of a thread-pool
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D576643@hermes.coremedia.com>

Hi,

I've implemented an Executor implementation that provides for
"threading" on top of another Executor. Its application fits well with
Peter's description in a recent thread:

Peter Veentjer:
> Create an Executor with a single thread and let this thread
> communicate with the structure (if the structure is accessed by a
> single thread, you don't need concurrency control in the structure
> itself).

If I have many "structures" I don't want to have one single-threaded
executor per structure but use a pool. The pool should however garantuee
that jobs _for a particular structure_ are never executed concurrently.
Thus my design: a ThreadedExecutor has a linked queue of jobs and can
tell whether a command is currently executing on its behalf in the
delegate thread pool.

Enqueuing a job into the ThreadedExecutor, if idle, enqueues a
corresponding worker into the delegate pool. This worker will execute
jobs on behalf of that queue until empty and atomically fall back to
idle.

Thus you can combine single-threaded access to data structures and still
leverage thread-pooling.

I would welcome feedback on any blatant problems in the code or whether
this functionality could have been implemented more easily with existing
concurrency classes.


Thanks
Matthias


-- 
matthias.ernst at coremedia.com
software architect
+49.40.32 55 87.503


From joe.bowbeer at gmail.com  Fri Feb  9 14:53:12 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 9 Feb 2007 11:53:12 -0800
Subject: [concurrency-interest] Feedback requested: virtual threading on
	top of a thread-pool
In-Reply-To: <AE2A8E488D9B26438919DF3C9C95528D576643@hermes.coremedia.com>
References: <AE2A8E488D9B26438919DF3C9C95528D576643@hermes.coremedia.com>
Message-ID: <31f2a7bd0702091153u2add48f1v637155244fbaabc@mail.gmail.com>

Matthias wrote:
>
> I would welcome feedback on any blatant problems in the code or whether
> this functionality could have been implemented more easily with existing
> concurrency classes.
>

Is your ThreadedExecutor similar to the SerialExector example in the
Executor javadoc?

http://java.sun.com/javase/6/docs/api/java/util/concurrent/Executor.html


On 2/9/07, Ernst, Matthias <matthias.ernst at coremedia.com> wrote:
>
> I've implemented an Executor implementation that provides for
> "threading" on top of another Executor. Its application fits well with
> Peter's description in a recent thread:
>
> Peter Veentjer:
> > Create an Executor with a single thread and let this thread
> > communicate with the structure (if the structure is accessed by a
> > single thread, you don't need concurrency control in the structure
> > itself).
>
> If I have many "structures" I don't want to have one single-threaded
> executor per structure but use a pool. The pool should however garantuee
> that jobs _for a particular structure_ are never executed concurrently.
> Thus my design: a ThreadedExecutor has a linked queue of jobs and can
> tell whether a command is currently executing on its behalf in the
> delegate thread pool.
>
> Enqueuing a job into the ThreadedExecutor, if idle, enqueues a
> corresponding worker into the delegate pool. This worker will execute
> jobs on behalf of that queue until empty and atomically fall back to
> idle.
>
> Thus you can combine single-threaded access to data structures and still
> leverage thread-pooling.
>
> I would welcome feedback on any blatant problems in the code or whether
> this functionality could have been implemented more easily with existing
> concurrency classes.
>

From matthias.ernst at coremedia.com  Sat Feb 10 03:51:07 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Sat, 10 Feb 2007 09:51:07 +0100
Subject: [concurrency-interest] Feedback requested: virtual threading
	ontop of a thread-pool
References: <AE2A8E488D9B26438919DF3C9C95528D576643@hermes.coremedia.com>
	<31f2a7bd0702091153u2add48f1v637155244fbaabc@mail.gmail.com>
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D168E6B@hermes.coremedia.com>

Jip, that's similar. I may have been a little obsessed with making it non-blocking - this version looks much simpler.

Thanks
Matthias

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu on behalf of Joe Bowbeer
Sent: Fri 2/9/2007 20:53
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Feedback requested: virtual threading ontop of a thread-pool
 
Matthias wrote:
>
> I would welcome feedback on any blatant problems in the code or whether
> this functionality could have been implemented more easily with existing
> concurrency classes.
>

Is your ThreadedExecutor similar to the SerialExector example in the
Executor javadoc?

http://java.sun.com/javase/6/docs/api/java/util/concurrent/Executor.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070210/cd8aee81/attachment.html 

From hlship at gmail.com  Sat Feb 10 18:23:03 2007
From: hlship at gmail.com (Howard Lewis Ship)
Date: Sat, 10 Feb 2007 15:23:03 -0800
Subject: [concurrency-interest] Is java.lang.String threadsafe?
Message-ID: <ecd0e3310702101523r2bcc84b4o4ecd8d364054c82b@mail.gmail.com>

I was just putting together a utility class (a non-case-sensitive
String Map implementation) and I happened to look at some code in
java.lang.String:

  public int hashCode() {
	int h = hash;
	if (h == 0) {
	    int off = offset;
	    char val[] = value;
	    int len = count;

            for (int i = 0; i < len; i++) {
                h = 31*h + val[off++];
            }
            hash = h;
        }
        return h;
    }

All the hallmarks of non-threadsafe code.  I think this is only
threadsafe because:

a) In any race condition, it will always compute the same value
(everything else is final)
b) The field type is int, not long (atomic write for int, non-atomic for long)

I have a feeling I'm right here, if only because otherwise nothing in
the JDK that uses a String is likely threadsafe.

-- 
Howard M. Lewis Ship
TWD Consulting, Inc.
Independent J2EE / Open-Source Java Consultant
Creator and PMC Chair, Apache Tapestry
Creator, Apache HiveMind

Professional Tapestry training, mentoring, support
and project work.  http://howardlewisship.com

From dawidk at mathcs.emory.edu  Sat Feb 10 18:37:28 2007
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat, 10 Feb 2007 18:37:28 -0500
Subject: [concurrency-interest] Is java.lang.String threadsafe?
In-Reply-To: <ecd0e3310702101523r2bcc84b4o4ecd8d364054c82b@mail.gmail.com>
References: <ecd0e3310702101523r2bcc84b4o4ecd8d364054c82b@mail.gmail.com>
Message-ID: <45CE5738.3090804@mathcs.emory.edu>

Howard Lewis Ship wrote:
> I was just putting together a utility class (a non-case-sensitive
> String Map implementation) and I happened to look at some code in
> java.lang.String:
>
>   public int hashCode() {
> 	int h = hash;
> 	if (h == 0) {
> 	    int off = offset;
> 	    char val[] = value;
> 	    int len = count;
>
>             for (int i = 0; i < len; i++) {
>                 h = 31*h + val[off++];
>             }
>             hash = h;
>         }
>         return h;
>     }
>
> All the hallmarks of non-threadsafe code.  I think this is only
> threadsafe because:
>
> a) In any race condition, it will always compute the same value
> (everything else is final)
> b) The field type is int, not long (atomic write for int, non-atomic for long)
>
>   

Correct. String class purposely allows race conditions, for sake of 
improved performance (through avoiding synchronization), which is OK 
here since races are benign due to (a) and (b) above. The only 
"negative" side effect is that several threads may re-do the work by 
independently computing the hash code, but it actually costs much less 
than blocking and waiting would in this case.

Regards,
Dawid



From brian at quiotix.com  Sat Feb 10 20:01:40 2007
From: brian at quiotix.com (Brian Goetz)
Date: Sat, 10 Feb 2007 20:01:40 -0500
Subject: [concurrency-interest] Is java.lang.String threadsafe?
In-Reply-To: <ecd0e3310702101523r2bcc84b4o4ecd8d364054c82b@mail.gmail.com>
References: <ecd0e3310702101523r2bcc84b4o4ecd8d364054c82b@mail.gmail.com>
Message-ID: <45CE6AF4.3060801@quiotix.com>

This is a data race (writes and reads to a field not ordered by 
synchronization), but a benign one, because the hash field can take on 
only one nondefault value, and if multiple threads write to hash, they 
will always write the same value.  So all threads will either use the 
value computed by another thread, or will compute that same value for 
themselves.  See the footnote on p47 of JCiP.

Howard Lewis Ship wrote:
> I was just putting together a utility class (a non-case-sensitive
> String Map implementation) and I happened to look at some code in
> java.lang.String:
> 
>   public int hashCode() {
> 	int h = hash;
> 	if (h == 0) {
> 	    int off = offset;
> 	    char val[] = value;
> 	    int len = count;
> 
>             for (int i = 0; i < len; i++) {
>                 h = 31*h + val[off++];
>             }
>             hash = h;
>         }
>         return h;
>     }
> 
> All the hallmarks of non-threadsafe code.  I think this is only
> threadsafe because:
> 
> a) In any race condition, it will always compute the same value
> (everything else is final)
> b) The field type is int, not long (atomic write for int, non-atomic for long)
> 
> I have a feeling I'm right here, if only because otherwise nothing in
> the JDK that uses a String is likely threadsafe.
> 

From hanson.char at gmail.com  Mon Feb 12 17:15:09 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 12 Feb 2007 14:15:09 -0800
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
Message-ID: <ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>

Also, it would be really nice if the javadoc of the retry related classes
can be published somewhere at jcip similar to the one for the annotations:

  http://jcip.net/annotations/doc/

Hanson

On 2/7/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Excellent!  The existing RetryPolicies compiles only under Java 5 (not
> Java 6).  I've fixed up a version that preserves all the generics but
> compiles only under Java 6.  Can send you a copy if interested.
>
> Hanson
>
> On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
> >
> > Thanks! Could you post AbstractRetryPolicy? I can't find it.
> >
> > --tim
> >
> > On 2/7/07, Joshua Bloch <josh at bloch.us > wrote:
> > >
> > > Sure; I dont' think public domain will cause any problems.  Google and
> > > I hereby release it into the public domain. (It may end up in the JDK some
> > > day, but it may not.)
> > >
> > >                  Josh
> > >
> > > On 2/7/07, Tim Peierls <tim at peierls.net> wrote:
> > > >
> > > > I don't know. Best thing would be for Josh to decide how he wants it
> > > > distributed.
> > > >
> > > > --tim
> > > >
> > > > On 2/7/07, Hanson Char < hanson.char at gmail.com> wrote:
> > > > >
> > > > > Hi,
> > > > >
> > > > > Can I presume the jcip/retry/*.java source code is in the public
> > > > > domain ?  If not, please kindly advise the licensing used.
> > > > >
> > > > > Thanks,
> > > > > Hanson
> > > > >
> > > > > Josh told me he'd be willing to distribute this more widely if
> > > > > > there was sufficient interest.
> > > > > >
> > > > > > I've placed an incomplete draft of this code in a public area of
> > > > > > the Java Concurrency in Practice source repository:
> > > > > >
> > > > > > https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> > > > > >
> > > > > >
> > > > > > It's incomplete because it doesn't define AbstractRetryPolicy.
> > > > > >
> > > > > > What Bela describes is slightly different: each task can be
> > > > > > submitted with its own "RetryPolicy". That could be achieved within Josh's
> > > > > > framework, for example, by extending ScheduledThreadPoolExecutor and
> > > > > > overriding decorateTask to do the requisite wrapping for tasks that
> > > > > > implement RetryPolicy.
> > > > > >
> > > > > > --tim
> > > > > >
> > > > >
> > > > >
> > > >
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070212/fa7be378/attachment.html 

From tim at peierls.net  Tue Feb 13 14:30:28 2007
From: tim at peierls.net (Tim Peierls)
Date: Tue, 13 Feb 2007 14:30:28 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<45AF9E34.3070707@yahoo.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>
Message-ID: <63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>

I can do this

   1. when I have time and
   2. if Brian thinks it's reasonable and
   3. if no one thinks of a better place and
   4. if someone can provide an AbstractRetryPolicy that compiles under
   both Java 5 and 6 (which might involve modifications to RetryPolicy and/or
   RetryPolicies).

--tim

On 2/12/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Also, it would be really nice if the javadoc of the retry related classes
> can be published somewhere at jcip similar to the one for the annotations:
>
>   http://jcip.net/annotations/doc/
>
> Hanson
>
> On 2/7/07, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Excellent!  The existing RetryPolicies compiles only under Java 5 (not
> > Java 6).  I've fixed up a version that preserves all the generics but
> > compiles only under Java 6.  Can send you a copy if interested.
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070213/6687ffdc/attachment.html 

From hanson.char at gmail.com  Tue Feb 13 15:43:35 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 13 Feb 2007 12:43:35 -0800
Subject: [concurrency-interest] Fwd:  ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>
	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>
	<63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>
	<ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>
Message-ID: <ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>

AbstractRetryPolicy has no problem compiling in Java 5 and 6, it's the
RetryPolicies that is having problem.  See below my reversed engineered
version of AbstractRetryPolicy.

I also put it under package net.jcip.retry , instead of the default
namespace.

H

package net.jcip.retry;

/**
 * @author Josh Bloch
 * @author Hanson Char - fill in the missing pieces
 */
// https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
public abstract class AbstractRetryPolicy implements RetryPolicy
{
    private final Class<? extends Exception>[] recoverableExceptions;

    public AbstractRetryPolicy(Class<? extends Exception>[]
recoverableExceptions)
    {
        this.recoverableExceptions = recoverableExceptions;
    }

    public abstract long nextDelay(long startTime, int retries);

    public boolean isFailureRecoverable(Exception e)
    {
        for (Class<? extends Exception> c : recoverableExceptions)
            if (c.isAssignableFrom(e.getClass()))
                return true;
        return false;
    }
}


On 2/13/07, Tim Peierls <tim at peierls.net> wrote:
>
> I can do this
>
>    1. when I have time and
>    2. if Brian thinks it's reasonable and
>    3. if no one thinks of a better place and
>    4. if someone can provide an AbstractRetryPolicy that compiles under
>    both Java 5 and 6 (which might involve modifications to RetryPolicy and/or
>    RetryPolicies).
>
> --tim
>
> On 2/12/07, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Also, it would be really nice if the javadoc of the retry related
> > classes can be published somewhere at jcip similar to the one for the
> > annotations:
> >
> >   http://jcip.net/annotations/doc/
> >
> > Hanson
> >
> > On 2/7/07, Hanson Char < hanson.char at gmail.com> wrote:
> > >
> > > Excellent!  The existing RetryPolicies compiles only under Java 5 (not
> > > Java 6).  I've fixed up a version that preserves all the generics but
> > > compiles only under Java 6.  Can send you a copy if interested.
> > >
> >
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070213/115f9c2f/attachment.html 

From forax at univ-mlv.fr  Tue Feb 13 16:46:23 2007
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Tue, 13 Feb 2007 22:46:23 +0100
Subject: [concurrency-interest] Fwd:  ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>	<63b4e4050701200839s33d5eb9al5f41342041bb49d6@mail.gmail.com>	<ca53c8f80702071006h7efa3711pf97368ca40f84975@mail.gmail.com>	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>	<63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>	<ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>
	<ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>
Message-ID: <45D231AF.8010409@univ-mlv.fr>

Hanson Char a ?crit :
> AbstractRetryPolicy has no problem compiling in Java 5 and 6, it's the 
> RetryPolicies that is having problem.  See below my reversed engineered 
> version of AbstractRetryPolicy.
> 
> I also put it under package net.jcip.retry , instead of the default 
> namespace.
> 
> H
> 
> package net.jcip.retry;
> 
> /**
>  * @author Josh Bloch
>  * @author Hanson Char - fill in the missing pieces
>  */
> // https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
> public abstract class AbstractRetryPolicy implements RetryPolicy
> {
>     private final Class<? extends Exception>[] recoverableExceptions;
>    
>     public AbstractRetryPolicy(Class<? extends Exception>[] 
> recoverableExceptions)
>     {
>         this.recoverableExceptions = recoverableExceptions;
>     }
>    
>     public abstract long nextDelay(long startTime, int retries);
>    
>     public boolean isFailureRecoverable(Exception e)
>     {
>         for (Class<? extends Exception> c : recoverableExceptions)
>             if (c.isAssignableFrom (e.getClass()))
>                 return true;
>         return false;
>     }
> }
> 

I think your code is not useable,
even if the way you use your array of recoverable exceptions is safe,
there is no way to create such array without a warning.
And there is no typesafe array in Java:
(see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4888066)

Arrays of parametrized type are rarely safe,
in my opinion, the only way to safely create an array of parametrized 
wilcard is to create a non generics subclass.

enum Foo {
   bar;
}
...
Enum<? extends Foo>[] foos = Foo.values();

Here foos is created without an unsafe warning.
But in your case java.lang.Class is final.

How do you expect to use your constructor ?

cheers,
R?mi Forax





From pfeiffer at tzi.de  Tue Feb 13 17:21:40 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Tue, 13 Feb 2007 23:21:40 +0100
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally fails
	with a broken barrier!?
Message-ID: <002f01c74fbd$558d5f10$4201a8c0@olli>

Hi,

I'm wondering why the given JUnit test (shown below) occasionally fails with
a broken barrier on multi processor systems using Java 5. The repetitive
test fails 27 times of 10.000 runs on my dual-core system. The test should
check wheter the acquired maximum number of simultaneous pool threads are
usable and that the pool doesn't fail even when the internal task queue is
full (caller runs policy).

Greetings
 Oliver

=====================

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.SynchronousQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import junit.extensions.RepeatedTest;
import junit.framework.Test;
import junit.framework.TestCase;

public class ThreadPoolExecutorTest extends TestCase {
  
  private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
    new ThreadPoolExecutor(
      0, 16, 10, TimeUnit.SECONDS,
      new SynchronousQueue<Runnable>(),
      new ThreadPoolExecutor.CallerRunsPolicy()
    );

  public ThreadPoolExecutorTest(String name) {
    super(name);
  }
  
  public static Test suite() {
    return new RepeatedTest(
      new ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
    );
  }

  public void testThreadPoolExecutor() throws InterruptedException {
    final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
    final int loops = threads * 16;
    final CountDownLatch latch = new CountDownLatch(loops);
    final AtomicInteger counter = new AtomicInteger();
    final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
    for (int i = 0; i < loops; i++) {
      THREAD_POOL_EXECUTOR.submit(new Runnable() {
        public void run() {
          if (counter.incrementAndGet() <= barrier.getParties()) {
            try {
              barrier.await(1, TimeUnit.SECONDS);
            } catch (Exception ign) {
              // can be ignored (broken barrier is tested below)
            }
          }
          latch.countDown();
        }
      });
    }
    assertTrue(latch.await(2, TimeUnit.SECONDS));
    assertFalse(barrier.isBroken());
    assertEquals(0, barrier.getNumberWaiting());
    assertEquals(loops, counter.get());
  }

}


From dcholmes at optusnet.com.au  Tue Feb 13 21:21:27 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 14 Feb 2007 12:21:27 +1000
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
	failswith a broken barrier!?
In-Reply-To: <002f01c74fbd$558d5f10$4201a8c0@olli>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEFMHFAA.dcholmes@optusnet.com.au>

Oliver,

You have a race condition testing the count against the barrier parties.
Between the change of the count and the test of getParties() other threads
could have hit the barrier. As a result the current thread doesn't wait on
the barrier, and as a result of that any threads already at the barrier will
eventually timeout, hence the broken barrier.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oliver
> Pfeiffer
> Sent: Wednesday, 14 February 2007 8:22 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
> failswith a broken barrier!?
>
>
> Hi,
>
> I'm wondering why the given JUnit test (shown below) occasionally
> fails with
> a broken barrier on multi processor systems using Java 5. The repetitive
> test fails 27 times of 10.000 runs on my dual-core system. The test should
> check wheter the acquired maximum number of simultaneous pool threads are
> usable and that the pool doesn't fail even when the internal task queue is
> full (caller runs policy).
>
> Greetings
>  Oliver
>
> =====================
>
> import java.util.concurrent.CountDownLatch;
> import java.util.concurrent.CyclicBarrier;
> import java.util.concurrent.SynchronousQueue;
> import java.util.concurrent.ThreadPoolExecutor;
> import java.util.concurrent.TimeUnit;
> import java.util.concurrent.atomic.AtomicInteger;
>
> import junit.extensions.RepeatedTest;
> import junit.framework.Test;
> import junit.framework.TestCase;
>
> public class ThreadPoolExecutorTest extends TestCase {
>
>   private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
>     new ThreadPoolExecutor(
>       0, 16, 10, TimeUnit.SECONDS,
>       new SynchronousQueue<Runnable>(),
>       new ThreadPoolExecutor.CallerRunsPolicy()
>     );
>
>   public ThreadPoolExecutorTest(String name) {
>     super(name);
>   }
>
>   public static Test suite() {
>     return new RepeatedTest(
>       new ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
>     );
>   }
>
>   public void testThreadPoolExecutor() throws InterruptedException {
>     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
>     final int loops = threads * 16;
>     final CountDownLatch latch = new CountDownLatch(loops);
>     final AtomicInteger counter = new AtomicInteger();
>     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
>     for (int i = 0; i < loops; i++) {
>       THREAD_POOL_EXECUTOR.submit(new Runnable() {
>         public void run() {
>           if (counter.incrementAndGet() <= barrier.getParties()) {
>             try {
>               barrier.await(1, TimeUnit.SECONDS);
>             } catch (Exception ign) {
>               // can be ignored (broken barrier is tested below)
>             }
>           }
>           latch.countDown();
>         }
>       });
>     }
>     assertTrue(latch.await(2, TimeUnit.SECONDS));
>     assertFalse(barrier.isBroken());
>     assertEquals(0, barrier.getNumberWaiting());
>     assertEquals(loops, counter.get());
>   }
>
> }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From joe.bowbeer at gmail.com  Tue Feb 13 22:05:37 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 13 Feb 2007 19:05:37 -0800
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
	failswith a broken barrier!?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEFMHFAA.dcholmes@optusnet.com.au>
References: <002f01c74fbd$558d5f10$4201a8c0@olli>
	<NFBBKALFDCPFIDBNKAPCGEFMHFAA.dcholmes@optusnet.com.au>
Message-ID: <31f2a7bd0702131905t5d415626p92c812af4dc4e41e@mail.gmail.com>

but getParties returns parties, which is final...

On 2/13/07, David Holmes <dcholmes at optusnet.com.au> wrote:
>
> Oliver,
>
> You have a race condition testing the count against the barrier parties.
> Between the change of the count and the test of getParties() other threads
> could have hit the barrier. As a result the current thread doesn't wait on
> the barrier, and as a result of that any threads already at the barrier
> will
> eventually timeout, hence the broken barrier.
>
> David Holmes
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oliver
> > Pfeiffer
> > Sent: Wednesday, 14 February 2007 8:22 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
> > failswith a broken barrier!?
> >
> >
> > Hi,
> >
> > I'm wondering why the given JUnit test (shown below) occasionally
> > fails with
> > a broken barrier on multi processor systems using Java 5. The repetitive
> > test fails 27 times of 10.000 runs on my dual-core system. The test
> should
> > check wheter the acquired maximum number of simultaneous pool threads
> are
> > usable and that the pool doesn't fail even when the internal task queue
> is
> > full (caller runs policy).
> >
> > Greetings
> >  Oliver
> >
> > =====================
> >
> > import java.util.concurrent.CountDownLatch;
> > import java.util.concurrent.CyclicBarrier;
> > import java.util.concurrent.SynchronousQueue;
> > import java.util.concurrent.ThreadPoolExecutor;
> > import java.util.concurrent.TimeUnit;
> > import java.util.concurrent.atomic.AtomicInteger;
> >
> > import junit.extensions.RepeatedTest;
> > import junit.framework.Test;
> > import junit.framework.TestCase;
> >
> > public class ThreadPoolExecutorTest extends TestCase {
> >
> >   private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
> >     new ThreadPoolExecutor(
> >       0, 16, 10, TimeUnit.SECONDS,
> >       new SynchronousQueue<Runnable>(),
> >       new ThreadPoolExecutor.CallerRunsPolicy()
> >     );
> >
> >   public ThreadPoolExecutorTest(String name) {
> >     super(name);
> >   }
> >
> >   public static Test suite() {
> >     return new RepeatedTest(
> >       new ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
> >     );
> >   }
> >
> >   public void testThreadPoolExecutor() throws InterruptedException {
> >     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
> >     final int loops = threads * 16;
> >     final CountDownLatch latch = new CountDownLatch(loops);
> >     final AtomicInteger counter = new AtomicInteger();
> >     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
> >     for (int i = 0; i < loops; i++) {
> >       THREAD_POOL_EXECUTOR.submit(new Runnable() {
> >         public void run() {
> >           if (counter.incrementAndGet() <= barrier.getParties()) {
> >             try {
> >               barrier.await(1, TimeUnit.SECONDS);
> >             } catch (Exception ign) {
> >               // can be ignored (broken barrier is tested below)
> >             }
> >           }
> >           latch.countDown();
> >         }
> >       });
> >     }
> >     assertTrue(latch.await(2, TimeUnit.SECONDS));
> >     assertFalse(barrier.isBroken());
> >     assertEquals(0, barrier.getNumberWaiting());
> >     assertEquals(loops, counter.get());
> >   }
> >
> > }
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070213/c72f9a08/attachment-0001.html 

From dcholmes at optusnet.com.au  Tue Feb 13 22:23:28 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 14 Feb 2007 13:23:28 +1000
Subject: [concurrency-interest] ThreadPoolExecutorTest
	occasionallyfailswith a broken barrier!?
In-Reply-To: <31f2a7bd0702131905t5d415626p92c812af4dc4e41e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEFNHFAA.dcholmes@optusnet.com.au>

Oops - my mistake. Thanks Joe.

But now the logic of comparing the count to getParties doesn't make sense to
me.

The barrier will break if a thread times out, so that is what should be
checked for - the reason for the broken barrier. The next step is to
determine why it timed out.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe Bowbeer
  Sent: Wednesday, 14 February 2007 1:06 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ThreadPoolExecutorTest
occasionallyfailswith a broken barrier!?


  but getParties returns parties, which is final...


  On 2/13/07, David Holmes <dcholmes at optusnet.com.au> wrote:
    Oliver,

    You have a race condition testing the count against the barrier parties.
    Between the change of the count and the test of getParties() other
threads
    could have hit the barrier. As a result the current thread doesn't wait
on
    the barrier, and as a result of that any threads already at the barrier
will
    eventually timeout, hence the broken barrier.

    David Holmes

    > -----Original Message-----
    > From: concurrency-interest-bounces at cs.oswego.edu
    > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oliver
    > Pfeiffer
    > Sent: Wednesday, 14 February 2007 8:22 AM
    > To: concurrency-interest at cs.oswego.edu
    > Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
    > failswith a broken barrier!?
    >
    >
    > Hi,
    >
    > I'm wondering why the given JUnit test (shown below) occasionally
    > fails with
    > a broken barrier on multi processor systems using Java 5. The
repetitive
    > test fails 27 times of 10.000 runs on my dual-core system. The test
should
    > check wheter the acquired maximum number of simultaneous pool threads
are
    > usable and that the pool doesn't fail even when the internal task
queue is
    > full (caller runs policy).
    >
    > Greetings
    >  Oliver
    >
    > =====================
    >
    > import java.util.concurrent.CountDownLatch;
    > import java.util.concurrent.CyclicBarrier ;
    > import java.util.concurrent.SynchronousQueue;
    > import java.util.concurrent.ThreadPoolExecutor;
    > import java.util.concurrent.TimeUnit;
    > import java.util.concurrent.atomic.AtomicInteger;
    >
    > import junit.extensions.RepeatedTest;
    > import junit.framework.Test;
    > import junit.framework.TestCase;
    >
    > public class ThreadPoolExecutorTest extends TestCase {
    >
    >   private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
    >     new ThreadPoolExecutor(
    >       0, 16, 10, TimeUnit.SECONDS,
    >       new SynchronousQueue<Runnable>(),
    >       new ThreadPoolExecutor.CallerRunsPolicy()
    >     );
    >
    >   public ThreadPoolExecutorTest(String name) {
    >     super(name);
    >   }
    >
    >   public static Test suite() {
    >     return new RepeatedTest(
    >       new ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
    >     );
    >   }
    >
    >   public void testThreadPoolExecutor() throws InterruptedException {
    >     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
    >     final int loops = threads * 16;
    >     final CountDownLatch latch = new CountDownLatch(loops);
    >     final AtomicInteger counter = new AtomicInteger();
    >     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
    >     for (int i = 0; i < loops; i++) {
    >       THREAD_POOL_EXECUTOR.submit(new Runnable() {
    >         public void run() {
    >           if (counter.incrementAndGet() <= barrier.getParties()) {
    >             try {
    >               barrier.await(1, TimeUnit.SECONDS);
    >             } catch (Exception ign) {
    >               // can be ignored (broken barrier is tested below)
    >             }
    >           }
    >           latch.countDown();
    >         }
    >       });
    >     }
    >     assertTrue( latch.await(2, TimeUnit.SECONDS));
    >     assertFalse(barrier.isBroken());
    >     assertEquals(0, barrier.getNumberWaiting());
    >     assertEquals(loops, counter.get());
    >   }
    >
    > }
    >


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070214/ea6ea4e4/attachment.html 

From joe.bowbeer at gmail.com  Tue Feb 13 22:38:26 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 13 Feb 2007 19:38:26 -0800
Subject: [concurrency-interest] ThreadPoolExecutorTest
	occasionallyfailswith a broken barrier!?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEFNHFAA.dcholmes@optusnet.com.au>
References: <31f2a7bd0702131905t5d415626p92c812af4dc4e41e@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEFNHFAA.dcholmes@optusnet.com.au>
Message-ID: <31f2a7bd0702131938s4a37c135m3034de4af53d5721@mail.gmail.com>

I got this far with my analysis:

It looks like all 16 pool threads will wait on the barrier, the main thread
(caller) will wait on the 17th submission, and then none of the remaining
executions will wait on the barrier.

There are several assertions that can fail.  Which one or ones are failing?


Btw, why not just execute these runnables?  I hate for good futures (created
by submit) to be wasted.. and they can swallow errors.


On 2/13/07, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Oops - my mistake. Thanks Joe.
>
> But now the logic of comparing the count to getParties doesn't make sense
> to me.
>
> The barrier will break if a thread times out, so that is what should be
> checked for - the reason for the broken barrier. The next step is to
> determine why it timed out.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Joe Bowbeer
> *Sent:* Wednesday, 14 February 2007 1:06 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] ThreadPoolExecutorTest
> occasionallyfailswith a broken barrier!?
>
> but getParties returns parties, which is final...
>
> On 2/13/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> >
> > Oliver,
> >
> > You have a race condition testing the count against the barrier parties.
> >
> > Between the change of the count and the test of getParties() other
> > threads
> > could have hit the barrier. As a result the current thread doesn't wait
> > on
> > the barrier, and as a result of that any threads already at the barrier
> > will
> > eventually timeout, hence the broken barrier.
> >
> > David Holmes
> >
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oliver
> > > Pfeiffer
> > > Sent: Wednesday, 14 February 2007 8:22 AM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
> > > failswith a broken barrier!?
> > >
> > >
> > > Hi,
> > >
> > > I'm wondering why the given JUnit test (shown below) occasionally
> > > fails with
> > > a broken barrier on multi processor systems using Java 5. The
> > repetitive
> > > test fails 27 times of 10.000 runs on my dual-core system. The test
> > should
> > > check wheter the acquired maximum number of simultaneous pool threads
> > are
> > > usable and that the pool doesn't fail even when the internal task
> > queue is
> > > full (caller runs policy).
> > >
> > > Greetings
> > >  Oliver
> > >
> > > =====================
> > >
> > > import java.util.concurrent.CountDownLatch;
> > > import java.util.concurrent.CyclicBarrier ;
> > > import java.util.concurrent.SynchronousQueue;
> > > import java.util.concurrent.ThreadPoolExecutor;
> > > import java.util.concurrent.TimeUnit;
> > > import java.util.concurrent.atomic.AtomicInteger;
> > >
> > > import junit.extensions.RepeatedTest;
> > > import junit.framework.Test;
> > > import junit.framework.TestCase;
> > >
> > > public class ThreadPoolExecutorTest extends TestCase {
> > >
> > >   private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
> > >     new ThreadPoolExecutor(
> > >       0, 16, 10, TimeUnit.SECONDS,
> > >       new SynchronousQueue<Runnable>(),
> > >       new ThreadPoolExecutor.CallerRunsPolicy()
> > >     );
> > >
> > >   public ThreadPoolExecutorTest(String name) {
> > >     super(name);
> > >   }
> > >
> > >   public static Test suite() {
> > >     return new RepeatedTest(
> > >       new ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
> > >     );
> > >   }
> > >
> > >   public void testThreadPoolExecutor() throws InterruptedException {
> > >     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
> > >     final int loops = threads * 16;
> > >     final CountDownLatch latch = new CountDownLatch(loops);
> > >     final AtomicInteger counter = new AtomicInteger();
> > >     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
> > >     for (int i = 0; i < loops; i++) {
> > >       THREAD_POOL_EXECUTOR.submit(new Runnable() {
> > >         public void run() {
> > >           if (counter.incrementAndGet() <= barrier.getParties()) {
> > >             try {
> > >               barrier.await(1, TimeUnit.SECONDS);
> > >             } catch (Exception ign) {
> > >               // can be ignored (broken barrier is tested below)
> > >             }
> > >           }
> > >           latch.countDown();
> > >         }
> > >       });
> > >     }
> > >     assertTrue( latch.await(2, TimeUnit.SECONDS));
> > >     assertFalse(barrier.isBroken());
> > >     assertEquals(0, barrier.getNumberWaiting());
> > >     assertEquals(loops, counter.get());
> > >   }
> > >
> > > }
> > >
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070213/19511bb1/attachment-0001.html 

From hanson.char at gmail.com  Tue Feb 13 23:16:19 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 13 Feb 2007 20:16:19 -0800
Subject: [concurrency-interest] Fwd: ScheduledThreadPoolExecutor woes
In-Reply-To: <45D231AF.8010409@univ-mlv.fr>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>
	<63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>
	<ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>
	<ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>
	<45D231AF.8010409@univ-mlv.fr>
Message-ID: <ca53c8f80702132016ic61a577le17517162368ed5f@mail.gmail.com>

>I think your code is not useable,

Not sure what you mean.  It's being used everywhere in RetryPolicies and
there is no warnings.

See:


https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/RetryPolicies.java

>How do you expect to use your constructor ?

See the RetryPolicies.java for usages.

A slight improvement on the AbstractRetryPolicy is to let the constructor
accept varargs.  See below.

Hanson

package net.jcip.retry;

/**
 * @author Josh Bloch
 * @author Hanson Char - fill in the missing pieces
 */
// https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/
public abstract class AbstractRetryPolicy implements RetryPolicy
{
    private final Class<? extends Exception>[] recoverableExceptions;

    public AbstractRetryPolicy(Class<? extends Exception>...
recoverableExceptions)
    {
        this.recoverableExceptions = recoverableExceptions;
    }

    public abstract long nextDelay(long startTime, int retries);

    public boolean isFailureRecoverable(Exception e)
    {
        for (Class<? extends Exception> c : recoverableExceptions)
            if (c.isAssignableFrom(e.getClass()))
                return true;
        return false;
    }
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070213/4b38ee52/attachment.html 

From forax at univ-mlv.fr  Wed Feb 14 02:50:48 2007
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Wed, 14 Feb 2007 08:50:48 +0100
Subject: [concurrency-interest] Fwd: ScheduledThreadPoolExecutor woes
In-Reply-To: <ca53c8f80702132016ic61a577le17517162368ed5f@mail.gmail.com>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>	
	<63b4e4050702071051h795b9cf9x143846d9514bd0e2@mail.gmail.com>	
	<b097ac510702071100x26bd208ax50a185a9d6b80200@mail.gmail.com>	
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>	
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>	
	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>	
	<63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>	
	<ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>	
	<ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>	
	<45D231AF.8010409@univ-mlv.fr>
	<ca53c8f80702132016ic61a577le17517162368ed5f@mail.gmail.com>
Message-ID: <45D2BF58.9050904@univ-mlv.fr>

Hanson Char a ?crit :
>  >I think your code is not useable,
> 
> Not sure what you mean.  It's being used everywhere in RetryPolicies and 
> there is no warnings.

try to call exponentialBackoff, truncatedExponentialBackoff
or fixedDelay and you will see.

By example, for this code:
static void test(Class<? extends Exception>... cs) {

}

public static void main(String[] args) {
   test(RuntimeException.class); // is unsafe here
}

javac generates that output:
Foo.java:13: warning: [unchecked] unchecked generic array creation of 
type java.lang.Class<? extends java.lang.Exception>[] for varargs parameter
       test(RuntimeException.class);
           ^
1 warning


> 
> See:
> 
>     
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/RetryPolicies.java
> 
>  >How do you expect to use your constructor ?
> 
> See the RetryPolicies.java for usages.
> 
> A slight improvement on the AbstractRetryPolicy is to let the 
> constructor accept varargs.  See below.
> 
> Hanson

cheers,
R?mi


From pfeiffer at tzi.de  Wed Feb 14 03:06:22 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Wed, 14 Feb 2007 09:06:22 +0100
Subject: [concurrency-interest]
	ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
In-Reply-To: <9489155.1171424554413.JavaMail.pfeiffer@mailhost>
Message-ID: <000001c7500f$0497feb0$38def395@orca>

The counter is compared to getParties to ensure that only the first 16
(pooled) + 1 (main) threads touch the barrier since the behaviour of
CyclicBarrier for additional threads arriving after the barrier was opened
(not broken) isn't obvious in the API doc. Actually the test will deadlock
if the counter isn't checked at all.

Oliver

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of David Holmes
> Sent: Wednesday, February 14, 2007 4:23 AM
> To: Joe Bowbeer; concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] 
> ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
> 
> 
> Oops - my mistake. Thanks Joe.
> ?
> But?now the logic of comparing the count to getParties 
> doesn't make sense to me.
> ?
> The barrier will break if a thread times out, so that is what 
> should be checked for - the reason for the broken barrier. 
> The next step is to determine why it timed out.
> ?
> David
> 
> > -----Original Message-----
> > _From:_ concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]_On 
> Behalf Of _Joe 
> > Bowbeer _Sent:_ Wednesday, 14 February 2007 1:06 PM
> > _To:_ concurrency-interest at cs.oswego.edu
> > _Subject:_ Re: [concurrency-interest] ThreadPoolExecutorTest
> > occasionallyfailswith a broken barrier!?
> > 
> > but getParties returns parties, which is final...
> > 
> > On 2/13/07, _David Holmes_ <<dcholmes at optusnet.com.au>> wrote:
> > 
> > > Oliver,
> > > 
> > > You have a race condition testing the count against the barrier 
> > > parties. Between the change of the count and the test of 
> > > getParties() other threads
> > > could have hit the barrier. As a result the current thread doesn't
> > > wait on
> > > the barrier, and as a result of that any threads already at the
> > > barrier will
> > > eventually timeout, hence the broken barrier.
> > > 
> > > David Holmes
> > > 
> > > > -----Original Message-----
> > > > From: <concurrency-interest-bounces at cs.oswego.edu>
> > > > [mailto:<concurrency-interest-bounces at cs.oswego.edu>]On 
> Behalf Of
> > > Oliver
> > > > Pfeiffer
> > > > Sent: Wednesday, 14 February 2007 8:22 AM
> > > > To: <concurrency-interest at cs.oswego.edu>
> > > > Subject: [concurrency-interest] ThreadPoolExecutorTest
> > > occasionally
> > > > failswith a broken barrier!?
> > > >
> > > >
> > > > Hi,
> > > >
> > > > I'm wondering why the given JUnit test (shown below) 
> occasionally 
> > > > fails with a broken barrier on multi processor systems 
> using Java 
> > > > 5. The
> > > repetitive
> > > > test fails 27 times of 10.000 runs on my dual-core system. The
> > > test should
> > > > check wheter the acquired maximum number of simultaneous pool
> > > threads are
> > > > usable and that the pool doesn't fail even when the 
> internal task
> > > queue is
> > > > full (caller runs policy).
> > > >
> > > > Greetings
> > > >??Oliver
> > > >
> > > > =====================
> > > >
> > > > import java.util.concurrent.CountDownLatch;
> > > > import java.util.concurrent.CyclicBarrier ;
> > > > import java.util.concurrent.SynchronousQueue;
> > > > import java.util.concurrent.ThreadPoolExecutor;
> > > > import java.util.concurrent.TimeUnit;
> > > > import java.util.concurrent.atomic.AtomicInteger;
> > > >
> > > > import junit.extensions.RepeatedTest;
> > > > import junit.framework.Test;
> > > > import junit.framework.TestCase;
> > > >
> > > > public class ThreadPoolExecutorTest extends TestCase {
> > > >
> > > >?? private static final ThreadPoolExecutor THREAD_POOL_EXECUTOR =
> > > >???? new ThreadPoolExecutor(
> > > >?????? 0, 16, 10, TimeUnit.SECONDS,
> > > >?????? new SynchronousQueue<Runnable>(),
> > > >?????? new ThreadPoolExecutor.CallerRunsPolicy()
> > > >???? );
> > > >
> > > >?? public ThreadPoolExecutorTest(String name) {
> > > >???? super(name);
> > > >?? }
> > > >
> > > >?? public static Test suite() {
> > > >???? return new RepeatedTest(
> > > >?????? new 
> ThreadPoolExecutorTest("testThreadPoolExecutor"), 10000
> > > >???? );
> > > >?? }
> > > >
> > > >?? public void testThreadPoolExecutor() throws 
> InterruptedException
> > > {
> > > >???? final int threads = 
> THREAD_POOL_EXECUTOR.getMaximumPoolSize();
> > > >???? final int loops = threads * 16;
> > > >???? final CountDownLatch latch = new CountDownLatch(loops);
> > > >???? final AtomicInteger counter = new AtomicInteger();
> > > >???? final CyclicBarrier barrier = new 
> CyclicBarrier(threads + 1);
> > > >???? for (int i = 0; i < loops; i++) {
> > > >?????? THREAD_POOL_EXECUTOR.submit(new Runnable() {
> > > >???????? public void run() {
> > > >?????????? if (counter.incrementAndGet() <= 
> barrier.getParties()) {
> > > >???????????? try {
> > > >?????????????? barrier.await(1, TimeUnit.SECONDS);
> > > >???????????? } catch (Exception ign) {
> > > >?????????????? // can be ignored (broken barrier is tested below)
> > > >???????????? }
> > > >?????????? }
> > > >?????????? latch.countDown();
> > > >???????? }
> > > >?????? });
> > > >???? }
> > > >???? assertTrue( latch.await(2, TimeUnit.SECONDS));
> > > >???? assertFalse(barrier.isBroken());
> > > >???? assertEquals(0, barrier.getNumberWaiting());
> > > >???? assertEquals(loops, counter.get());
> > > >?? }
> > > >
> > > > }
> > > >
> > 
> > 
> 



From pfeiffer at tzi.de  Wed Feb 14 03:14:44 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Wed, 14 Feb 2007 09:14:44 +0100
Subject: [concurrency-interest]
	ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
In-Reply-To: <2654289.1171425281053.JavaMail.pfeiffer@mailhost>
Message-ID: <000101c75010$2ff54350$38def395@orca>

The barrier should check that a total number of 17 threads (16 + 1) are able
to touch it in parallel. Thus all further threads arriving later do not need
to pass the barrier.

The occasionally thrown AssertionError comes from the broken barrier as
checked by assertFalse(barrier.isBroken()); at the end of the test method.

Future#get() isn't called after submission since an exception should never
be thrown in the runnable as long as latch#countDown() is harmless. :)

Oliver

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Joe Bowbeer
> Sent: Wednesday, February 14, 2007 4:38 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] 
> ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
> 
> 
> I got this far with my analysis:
> 
> It looks like all 16 pool threads will wait on the barrier, 
> the main thread (caller) will wait on the 17th submission, 
> and then none of the remaining executions will wait on the barrier.
> 
> There are several assertions that can fail.? Which one or 
> ones are failing??
> 
> Btw, why not just execute these runnables?? I hate for good 
> futures (created by submit) to be wasted.. and they can 
> swallow errors.
> 
> 
> On 2/13/07, _David Holmes_ <<dcholmes at optusnet.com.au>> wrote:
> 
> > Oops - my mistake. Thanks Joe.
> > ?
> > But?now the logic of comparing the count to getParties doesn't make 
> > sense to me.
> > ?
> > The barrier will break if a thread times out, so that is 
> what should 
> > be checked for - the reason for the broken barrier. The 
> next step is 
> > to determine why it timed out.
> > ?
> > David
> > 
> > > -----Original Message-----
> > > _From: concurrency-interest-bounces at cs.oswego.edu>
> > > [mailto:<concurrency-interest-bounces at cs.oswego.edu>]_On 
> Behalf Of 
> > > _Joe Bowbeer _Sent:_ Wednesday, 14 February 2007 1:06 PM
> > > _To: concurrency-interest at cs.oswego.edu>
> > > 
> > > _Subject:_ Re: [concurrency-interest] ThreadPoolExecutorTest 
> > > occasionallyfailswith a broken barrier!?
> > > 
> > > but getParties returns parties, which is final...
> > > 
> > > On 2/13/07, _David Holmes_ <<dcholmes at optusnet.com.au>> wrote:
> > > 
> > > > Oliver,
> > > > 
> > > > You have a race condition testing the count against the barrier 
> > > > parties. Between the change of the count and the test of 
> > > > getParties() other threads
> > > > could have hit the barrier. As a result the current 
> thread doesn't
> > > > wait on
> > > > the barrier, and as a result of that any threads already at the
> > > > barrier will
> > > > eventually timeout, hence the broken barrier.
> > > > 
> > > > David Holmes
> > > > 
> > > > > -----Original Message-----
> > > > > From: <concurrency-interest-bounces at cs.oswego.edu>
> > > > > [mailto:<concurrency-interest-bounces at cs.oswego.edu>]On Behalf
> > > > Of Oliver
> > > > > Pfeiffer
> > > > > Sent: Wednesday, 14 February 2007 8:22 AM
> > > > > To: <concurrency-interest at cs.oswego.edu>
> > > > > Subject: [concurrency-interest] ThreadPoolExecutorTest
> > > > occasionally
> > > > > failswith a broken barrier!?
> > > > >
> > > > >
> > > > > Hi,
> > > > >
> > > > > I'm wondering why the given JUnit test (shown below)
> > > > occasionally
> > > > > fails with
> > > > > a broken barrier on multi processor systems using Java 5. The
> > > > repetitive
> > > > > test fails 27 times of 10.000 runs on my dual-core system. The
> > > > test should
> > > > > check wheter the acquired maximum number of simultaneous pool
> > > > threads are
> > > > > usable and that the pool doesn't fail even when the internal
> > > > task queue is
> > > > > full (caller runs policy).
> > > > >
> > > > > Greetings
> > > > >??Oliver
> > > > >
> > > > > =====================
> > > > >
> > > > > import java.util.concurrent.CountDownLatch;
> > > > > import java.util.concurrent.CyclicBarrier ;
> > > > > import java.util.concurrent.SynchronousQueue;
> > > > > import java.util.concurrent.ThreadPoolExecutor;
> > > > > import java.util.concurrent.TimeUnit;
> > > > > import java.util.concurrent.atomic.AtomicInteger;
> > > > >
> > > > > import junit.extensions.RepeatedTest;
> > > > > import junit.framework.Test;
> > > > > import junit.framework.TestCase;
> > > > >
> > > > > public class ThreadPoolExecutorTest extends TestCase {
> > > > >
> > > > >?? private static final ThreadPoolExecutor 
> THREAD_POOL_EXECUTOR =
> > > > >???? new ThreadPoolExecutor(
> > > > >?????? 0, 16, 10, TimeUnit.SECONDS,
> > > > >?????? new SynchronousQueue<Runnable>(),
> > > > >?????? new ThreadPoolExecutor.CallerRunsPolicy()
> > > > >???? );
> > > > >
> > > > >?? public ThreadPoolExecutorTest(String name) {
> > > > >???? super(name);
> > > > >?? }
> > > > >
> > > > >?? public static Test suite() {
> > > > >???? return new RepeatedTest(
> > > > >?????? new ThreadPoolExecutorTest("testThreadPoolExecutor"),
> > > > 10000
> > > > >???? );
> > > > >?? }
> > > > >
> > > > >?? public void testThreadPoolExecutor() throws
> > > > InterruptedException {
> > > > >???? final int threads =
> > > > THREAD_POOL_EXECUTOR.getMaximumPoolSize();
> > > > >???? final int loops = threads * 16;
> > > > >???? final CountDownLatch latch = new CountDownLatch(loops);
> > > > >???? final AtomicInteger counter = new AtomicInteger();
> > > > >???? final CyclicBarrier barrier = new CyclicBarrier(threads +
> > > > 1);
> > > > >???? for (int i = 0; i < loops; i++) {
> > > > >?????? THREAD_POOL_EXECUTOR.submit(new Runnable() {
> > > > >???????? public void run() {
> > > > >?????????? if (counter.incrementAndGet() <= 
> barrier.getParties())
> > > > {
> > > > >???????????? try {
> > > > >?????????????? barrier.await(1, TimeUnit.SECONDS);
> > > > >???????????? } catch (Exception ign) {
> > > > >?????????????? // can be ignored (broken barrier is 
> tested below)
> > > > >???????????? }
> > > > >?????????? }
> > > > >?????????? latch.countDown();
> > > > >???????? }
> > > > >?????? });
> > > > >???? }
> > > > >???? assertTrue( latch.await(2, TimeUnit.SECONDS));
> > > > >???? assertFalse(barrier.isBroken());
> > > > >???? assertEquals(0, barrier.getNumberWaiting());
> > > > >???? assertEquals(loops, counter.get());
> > > > >?? }
> > > > >
> > > > > }
> > > > >
> > > 
> > > 
> > 
> 
> 



From joe.bowbeer at gmail.com  Wed Feb 14 03:45:02 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 14 Feb 2007 00:45:02 -0800
Subject: [concurrency-interest]
	ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
In-Reply-To: <000101c75010$2ff54350$38def395@orca>
References: <2654289.1171425281053.JavaMail.pfeiffer@mailhost>
	<000101c75010$2ff54350$38def395@orca>
Message-ID: <31f2a7bd0702140045i5e2413e7hfc9eb3fd2849811d@mail.gmail.com>

Maybe, every so often, it takes more than 1 second to execute the first 17
tasks?

What happens if you change the barrier timeout to 2 seconds?  Do the
failures vanish?


On 2/14/07, Oliver Pfeiffer <pfeiffer at tzi.de> wrote:
>
> The barrier should check that a total number of 17 threads (16 + 1) are
> able
> to touch it in parallel. Thus all further threads arriving later do not
> need
> to pass the barrier.
>
> The occasionally thrown AssertionError comes from the broken barrier as
> checked by assertFalse(barrier.isBroken()); at the end of the test method.
>
> Future#get() isn't called after submission since an exception should never
> be thrown in the runnable as long as latch#countDown() is harmless. :)



But Errors can still escape.  And RuntimeExceptions and Errors can
potentially escape from execution of countDown.  Why not execute(Runnable)?

--Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070214/53c893bd/attachment.html 

From pfeiffer at tzi.de  Wed Feb 14 04:11:05 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Wed, 14 Feb 2007 10:11:05 +0100
Subject: [concurrency-interest]
	ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
In-Reply-To: <27653945.1171443813393.JavaMail.pfeiffer@mailhost>
Message-ID: <000601c75018$0f43fea0$38def395@orca>

An increased timeout of 10 seconds doesn't have any effect (same ratio of
failed tests). :(

Oliver


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Joe Bowbeer
> Sent: Wednesday, February 14, 2007 9:45 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: RE: 
> [concurrency-interest]ThreadPoolExecutorTestoccasionallyfailsw
> ith a broken barrier!?
> 
> 
> Maybe, every so often, it takes more than 1 second to execute 
> the first 17 tasks?
> 
> What happens if you change the barrier timeout to 2 seconds?? 
> Do the failures vanish?
> 
> 
> On 2/14/07, _Oliver Pfeiffer_ <<pfeiffer at tzi.de>> wrote:
> 
> > The barrier should check that a total number of 17 threads (16 + 1) 
> > are able to touch it in parallel. Thus all further threads arriving 
> > later do not need
> > to pass the barrier.
> > 
> > The occasionally thrown AssertionError comes from the 
> broken barrier 
> > as checked by assertFalse(barrier.isBroken()); at the end 
> of the test
> > method.
> > 
> > Future#get() isn't called after submission since an 
> exception should 
> > never be thrown in the runnable as long as latch#countDown() is 
> > harmless. :)
> 
> 
> 
> But Errors can still escape.? And RuntimeExceptions and 
> Errors can potentially escape from execution of countDown.? 
> Why not execute(Runnable)?
> 
> --Joe?
> 



From hanson.char at gmail.com  Wed Feb 14 05:15:28 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 14 Feb 2007 02:15:28 -0800
Subject: [concurrency-interest] Fwd: ScheduledThreadPoolExecutor woes
In-Reply-To: <45D2BF58.9050904@univ-mlv.fr>
References: <AE2A8E488D9B26438919DF3C9C95528D4170D1@hermes.coremedia.com>
	<63b4e4050702071104t4faa18adla9eccbbd0649d30a@mail.gmail.com>
	<ca53c8f80702071503t43bfa6aam5b1a06a858e61064@mail.gmail.com>
	<ca53c8f80702121415h19563c0lfc67f75475c4b949@mail.gmail.com>
	<63b4e4050702131130j66717835yb4d7de27038f32e@mail.gmail.com>
	<ca53c8f80702131242t48a5ccfes4bd0024fa8477732@mail.gmail.com>
	<ca53c8f80702131243v75e9a204yf382a7a46cad56cf@mail.gmail.com>
	<45D231AF.8010409@univ-mlv.fr>
	<ca53c8f80702132016ic61a577le17517162368ed5f@mail.gmail.com>
	<45D2BF58.9050904@univ-mlv.fr>
Message-ID: <ca53c8f80702140215j3119d3dcw1b4a9da042e50571@mail.gmail.com>

I see, but then this warning is related the RetryPolicies.java, not the
missing AbstractRetryPolicy.java that I added in.  I guess one way to get
rid of the warning is to use the signature Class<?> instead of Class<?
extends Exception>, and then throw an IllegalArgumentException if the actual
arguments turn out to be not subclasses of Exception.

Joshua, the original author of RetryPolicies.java, is probably the better
person to comment on this.

Hanson

On 2/13/07, R?mi Forax <forax at univ-mlv.fr> wrote:
>
> Hanson Char a ?crit :
> >  >I think your code is not useable,
> >
> > Not sure what you mean.  It's being used everywhere in RetryPolicies and
> > there is no warnings.
>
> try to call exponentialBackoff, truncatedExponentialBackoff
> or fixedDelay and you will see.
>
> By example, for this code:
> static void test(Class<? extends Exception>... cs) {
>
> }
>
> public static void main(String[] args) {
>    test(RuntimeException.class); // is unsafe here
> }
>
> javac generates that output:
> Foo.java:13: warning: [unchecked] unchecked generic array creation of
> type java.lang.Class<? extends java.lang.Exception>[] for varargs
> parameter
>        test(RuntimeException.class);
>            ^
> 1 warning
>
>
> >
> > See:
> >
> >
> >
> https://dev.priorartisans.com/repos/jcip/trunk/src/main/jcip/retry/RetryPolicies.java
> >
> >  >How do you expect to use your constructor ?
> >
> > See the RetryPolicies.java for usages.
> >
> > A slight improvement on the AbstractRetryPolicy is to let the
> > constructor accept varargs.  See below.
> >
> > Hanson
>
> cheers,
> R?mi
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070214/8d34cbcc/attachment.html 

From pfeiffer at tzi.de  Wed Feb 14 06:33:28 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Wed, 14 Feb 2007 12:33:28 +0100
Subject: [concurrency-interest]
	ThreadPoolExecutorTestoccasionallyfailswith a broken barrier!?
In-Reply-To: <27653945.1171443813393.JavaMail.pfeiffer@mailhost>
Message-ID: <000001c7502b$f3563f00$38def395@orca>

You're right there is no need to pass the task by submit() instead execute()
should be used to avoid the future encapsulation. Anyway this doesn't make
any difference -> the test still fails with the same ratio.

Oliver


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Joe Bowbeer
> Sent: Wednesday, February 14, 2007 9:45 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: RE: 
> [concurrency-interest]ThreadPoolExecutorTestoccasionallyfailsw
> ith a broken barrier!?
> 
> 
> Maybe, every so often, it takes more than 1 second to execute 
> the first 17 tasks?
> 
> What happens if you change the barrier timeout to 2 seconds?? 
> Do the failures vanish?
> 
> 
> On 2/14/07, _Oliver Pfeiffer_ <<pfeiffer at tzi.de>> wrote:
> 
> > The barrier should check that a total number of 17 threads (16 + 1) 
> > are able to touch it in parallel. Thus all further threads arriving 
> > later do not need
> > to pass the barrier.
> > 
> > The occasionally thrown AssertionError comes from the 
> broken barrier 
> > as checked by assertFalse(barrier.isBroken()); at the end 
> of the test
> > method.
> > 
> > Future#get() isn't called after submission since an 
> exception should 
> > never be thrown in the runnable as long as latch#countDown() is 
> > harmless. :)
> 
> 
> 
> But Errors can still escape.? And RuntimeExceptions and 
> Errors can potentially escape from execution of countDown.? 
> Why not execute(Runnable)?
> 
> --Joe?
> 



From dl at cs.oswego.edu  Wed Feb 14 09:03:55 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Feb 2007 09:03:55 -0500
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
 fails with a broken barrier!?
In-Reply-To: <002f01c74fbd$558d5f10$4201a8c0@olli>
References: <002f01c74fbd$558d5f10$4201a8c0@olli>
Message-ID: <45D316CB.8080705@cs.oswego.edu>

Oliver Pfeiffer wrote:
> Hi,
> 
> I'm wondering why the given JUnit test (shown below) occasionally fails with
> a broken barrier on multi processor systems using Java 5. The repetitive
> test fails 27 times of 10.000 runs on my dual-core system. The test should
> check wheter the acquired maximum number of simultaneous pool threads are
> usable and that the pool doesn't fail even when the internal task queue is
> full (caller runs policy).
> 

>   public void testThreadPoolExecutor() throws InterruptedException {
>     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
>     final int loops = threads * 16;
>     final CountDownLatch latch = new CountDownLatch(loops);
>     final AtomicInteger counter = new AtomicInteger();
>     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
>     for (int i = 0; i < loops; i++) {
>       THREAD_POOL_EXECUTOR.submit(new Runnable() {
>         public void run() {
>           if (counter.incrementAndGet() <= barrier.getParties()) {

Presumably your intent is to have each thread hit one barrier.
But sometimes, one thread will hit two or more, and others will
hit zero. In those cases some will be waiting for a thread that
never arrives, timing out on barrier.

-Doug

From pfeiffer at tzi.de  Wed Feb 14 11:46:15 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Wed, 14 Feb 2007 17:46:15 +0100
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
	fails with a broken barrier!?
In-Reply-To: <2771331.1171461855736.JavaMail.pfeiffer@mailhost>
Message-ID: <000501c75057$a4724ec0$4201a8c0@olli>

> >   public void testThreadPoolExecutor() throws InterruptedException {
> >     final int threads = THREAD_POOL_EXECUTOR.getMaximumPoolSize();
> >     final int loops = threads * 16;
> >     final CountDownLatch latch = new CountDownLatch(loops);
> >     final AtomicInteger counter = new AtomicInteger();
> >     final CyclicBarrier barrier = new CyclicBarrier(threads + 1);
> >     for (int i = 0; i < loops; i++) {
> >       THREAD_POOL_EXECUTOR.submit(new Runnable() {
> >         public void run() {
> >           if (counter.incrementAndGet() <= barrier.getParties()) {
> 
> Presumably your intent is to have each thread hit one barrier.
> But sometimes, one thread will hit two or more, and others will
> hit zero. In those cases some will be waiting for a thread that
> never arrives, timing out on barrier.

No, the pool has a maximum of 16 threads and the test-loop repeats 256 times
(16*16) but the barrier is set to 17 (16+1). Thus only the first 16 pool
threads and the main thread itself (callers run policy) hit the barrier.
After the main thread trips the barrier they should continue all at once to
finish the remaining loops (without further barrier checking due to the
counter check).

The test should perform as follows:

T.01-T.16 arrive barrier and wait
T.main arrives barrier and trips it
barrier trips -> T.01-T.16 and T.main continue
T.01-T.16 only count the latch down without hitting the barrier (counter
check)

Thus the major question is still: Why does it occasionally fail? :)

Oliver


From peter.jones at sun.com  Wed Feb 14 13:44:02 2007
From: peter.jones at sun.com (Peter Jones)
Date: Wed, 14 Feb 2007 13:44:02 -0500
Subject: [concurrency-interest] ThreadPoolExecutorTest
	occasionally	fails with a broken barrier!?
In-Reply-To: <000501c75057$a4724ec0$4201a8c0@olli>
References: <2771331.1171461855736.JavaMail.pfeiffer@mailhost>
	<000501c75057$a4724ec0$4201a8c0@olli>
Message-ID: <20070214184401.GA14288@east>

On Wed, Feb 14, 2007 at 05:46:15PM +0100, Oliver Pfeiffer wrote:

> the pool has a maximum of 16 threads and the test-loop repeats 256 times
> (16*16) but the barrier is set to 17 (16+1). Thus only the first 16 pool
> threads and the main thread itself (callers run policy) hit the barrier.
> After the main thread trips the barrier they should continue all at once to
> finish the remaining loops (without further barrier checking due to the
> counter check).
> 
> The test should perform as follows:
> 
> T.01-T.16 arrive barrier and wait
> T.main arrives barrier and trips it
> barrier trips -> T.01-T.16 and T.main continue
> T.01-T.16 only count the latch down without hitting the barrier (counter
> check)
> 
> Thus the major question is still: Why does it occasionally fail? :)

You're reusing the same thread pool for each repeat of the test,
right?  (It appears to be statically initialized once for the test
class.)  I suspect that occasionally one (or more) of the pooled tasks
from a previous test run have not fully completed as far as the thread
pool is concerned, so during the next test run, the main loop gets
blocked on a iteration earlier than the 17th, and then a barrier await
times out because the main loop cannot submit the 17th task, which
breaks the barrier.

-- Peter

From peter.kovacs.1.0rc at gmail.com  Wed Feb 14 15:08:33 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Wed, 14 Feb 2007 21:08:33 +0100
Subject: [concurrency-interest] Questions about testing (CRCG, RNG)
Message-ID: <b6e8f2e80702141208j77d147l2ca5e7bc305e251e@mail.gmail.com>

Hi,

Following the suggestions in JCIP, I would like to use an ordered CRC
generator to verify input and output data consistency. Can any one
suggest a free Java implementation suitable for use with a large
number of input of numeric type. Would the CRC32 class be appropriate
for this purpose?

Also, I would like to simulate client code doing some unspecified
useful function by calling sleep for a random number of milliseconds.
(The sleep will hopefully have the beneficial side effect of creating
diverse thread interleaving.) I see that the java.util.Random class
has a nextGaussian method, but I thought I may eventually need
something different than standard deviation 1 and may be not even
Gaussian but something skewed toward the left. Am I right to be
concerned about the actual distribution or should Random.nextGaussion
be appropriate for this purpose?

Thanks
Peter

From dcholmes at optusnet.com.au  Wed Feb 14 17:43:24 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 15 Feb 2007 08:43:24 +1000
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionally
	fails with a broken barrier!?
In-Reply-To: <20070214184401.GA14288@east>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEGDHFAA.dcholmes@optusnet.com.au>

I think Peter has hit the nail on the head. If this test is repeated then
the state of the executor threads is unknown when the next round of
submissions occur. It could be that none of the pool threads have gotten
back to polling the queue, in which case the main thread will execute the
barrier await() and timeout.

Oliver: to debug this sort of thing you needed to see why the barrier was
breaking. We suspect timeouts but you need to confirm this. Then you need to
see what iteration (both loop-level and repeated-test level) is failing.
That should show that it doesn't fail on the first test.

As to what happens after a barrier is "opened":

"The barrier is called cyclic because it can be re-used after the waiting
threads are released. "

threads just starting waiting again until the required number of parties
arrive.

You say without the count you "deadlocked" - was that with a timeout
supplied to await()? Without the count you would run into the same problem
within the loop as you now do across tests. After the barrier opens it takes
time for the pool threads to complete their tasks and go back to wait for
the next task. If the main thread completes first then it will execute one
of the next tasks itself and so block in await() until the timeout expires.

BTW: what were you actually trying to test?

Cheers,
David Holmes


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Jones
> Sent: Thursday, 15 February 2007 4:44 AM
> To: Oliver Pfeiffer
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ThreadPoolExecutorTestoccasionally
> fails with a broken barrier!?
>
>
> On Wed, Feb 14, 2007 at 05:46:15PM +0100, Oliver Pfeiffer wrote:
>
> > the pool has a maximum of 16 threads and the test-loop repeats 256 times
> > (16*16) but the barrier is set to 17 (16+1). Thus only the first 16 pool
> > threads and the main thread itself (callers run policy) hit the barrier.
> > After the main thread trips the barrier they should continue
> all at once to
> > finish the remaining loops (without further barrier checking due to the
> > counter check).
> >
> > The test should perform as follows:
> >
> > T.01-T.16 arrive barrier and wait
> > T.main arrives barrier and trips it
> > barrier trips -> T.01-T.16 and T.main continue
> > T.01-T.16 only count the latch down without hitting the barrier (counter
> > check)
> >
> > Thus the major question is still: Why does it occasionally fail? :)
>
> You're reusing the same thread pool for each repeat of the test,
> right?  (It appears to be statically initialized once for the test
> class.)  I suspect that occasionally one (or more) of the pooled tasks
> from a previous test run have not fully completed as far as the thread
> pool is concerned, so during the next test run, the main loop gets
> blocked on a iteration earlier than the 17th, and then a barrier await
> times out because the main loop cannot submit the 17th task, which
> breaks the barrier.
>
> -- Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From nyin at packetmotion.com  Wed Feb 14 18:39:10 2007
From: nyin at packetmotion.com (Neal Yin)
Date: Wed, 14 Feb 2007 15:39:10 -0800
Subject: [concurrency-interest] Anywhere I can get orginal JSR166 spec
	document? <EOM>
Message-ID: <E8AFFEFDBE97C94E9297963F0527A07B0115608E@pmi00exf00.us.packetmotion.com>

Thanks,

 

-Neal

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070214/e0f3d4ab/attachment.html 

From joe.bowbeer at gmail.com  Wed Feb 14 19:06:29 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 14 Feb 2007 16:06:29 -0800
Subject: [concurrency-interest] Anywhere I can get original JSR166 spec
	document? <EOM>
Message-ID: <31f2a7bd0702141606k18e4ec32r47665d0ffa3e7d09@mail.gmail.com>

The JSR-166 spec is in the source, or vice versa:

See http://jcp.org/en/jsr/detail?id=166

The first public review draft is available from:

http://jcp.org/aboutJava/communityprocess/review/jsr166/index.html

Also see http://gee.cs.oswego.edu/dl/concurrency-interest/index.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070214/892600a0/attachment.html 

From pfeiffer at tzi.de  Thu Feb 15 04:32:30 2007
From: pfeiffer at tzi.de (Oliver Pfeiffer)
Date: Thu, 15 Feb 2007 10:32:30 +0100
Subject: [concurrency-interest] ThreadPoolExecutorTest occasionallyfails
	with a broken barrier!?
In-Reply-To: <4219289.1171478663002.JavaMail.pfeiffer@mailhost>
Message-ID: <000301c750e4$3686fbd0$4201a8c0@olli>

> You're reusing the same thread pool for each repeat of the test,
> right?  (It appears to be statically initialized once for the test
> class.)  I suspect that occasionally one (or more) of the pooled tasks
> from a previous test run have not fully completed as far as the thread
> pool is concerned, so during the next test run, the main loop gets
> blocked on a iteration earlier than the 17th, and then a barrier await
> times out because the main loop cannot submit the 17th task, which
> breaks the barrier.

Even if I didn't test it, I'm absolutely sure that this is the problem!
Thanks for exposing!

Oliver


From peter.kovacs.1.0rc at gmail.com  Thu Feb 15 16:13:09 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Thu, 15 Feb 2007 22:13:09 +0100
Subject: [concurrency-interest] Target CPU utilization
Message-ID: <b6e8f2e80702151313m5416bb00oe24032684303410d@mail.gmail.com>

Hi,

The function determining the optimal pool size includes the target CPU
utilization as a factor. What are the typical reasons why one would
want its value to be less than 1?

Thanks
P.

From dcholmes at optusnet.com.au  Thu Feb 15 16:43:04 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 16 Feb 2007 07:43:04 +1000
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702151313m5416bb00oe24032684303410d@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>

Two possibilities I can think of straight away:

- You might not be the only application trying to run on the system.
- You might want to allow room for handling transient overload.

Others?

David Holmes


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Friday, 16 February 2007 7:13 AM
> To: concurrency-interest
> Subject: [concurrency-interest] Target CPU utilization
> 
> 
> Hi,
> 
> The function determining the optimal pool size includes the target CPU
> utilization as a factor. What are the typical reasons why one would
> want its value to be less than 1?
> 
> Thanks
> P.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at quiotix.com  Thu Feb 15 17:30:08 2007
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 15 Feb 2007 17:30:08 -0500
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
Message-ID: <45D4DEF0.3010903@quiotix.com>

Another is that you might have multiple thread pools within an 
application, and want to allocate (coarsely) CPU cycles to different 
activities.

David Holmes wrote:
> Two possibilities I can think of straight away:
> 
> - You might not be the only application trying to run on the system.
> - You might want to allow room for handling transient overload.
> 
> Others?
> 
> David Holmes
> 
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
>> Kovacs
>> Sent: Friday, 16 February 2007 7:13 AM
>> To: concurrency-interest
>> Subject: [concurrency-interest] Target CPU utilization
>>
>>
>> Hi,
>>
>> The function determining the optimal pool size includes the target CPU
>> utilization as a factor. What are the typical reasons why one would
>> want its value to be less than 1?
>>
>> Thanks
>> P.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From peter.kovacs.1.0rc at gmail.com  Thu Feb 15 17:47:41 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Thu, 15 Feb 2007 23:47:41 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
References: <b6e8f2e80702151313m5416bb00oe24032684303410d@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
Message-ID: <b6e8f2e80702151447i519f60b0rb4dc727d04959334@mail.gmail.com>

On 2/15/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> Two possibilities I can think of straight away:
>
> - You might not be the only application trying to run on the system.

I thought of that...but most modern operating systems will do a good
job scheduling the running applications fairly. (Even with a single
threaded application, I will not include sleeps in order to yield CPU
time to other apps in the system.)

> - You might want to allow room for handling transient overload.

I am not sure I understand this one. Please, could you elaborate?
(Assume my target is full CPU utilization. Assume I also allow for a
copious wait time ratio. I will create a superfluous number of
threads... Hmm... I'd instinctively think that just the opposite is
true: the less the CPU utilization target, the less I am able to
handle transient overload. Isn't it?)

Thanks
Peter

>
> Others?
>
> David Holmes
>
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> > Kovacs
> > Sent: Friday, 16 February 2007 7:13 AM
> > To: concurrency-interest
> > Subject: [concurrency-interest] Target CPU utilization
> >
> >
> > Hi,
> >
> > The function determining the optimal pool size includes the target CPU
> > utilization as a factor. What are the typical reasons why one would
> > want its value to be less than 1?
> >
> > Thanks
> > P.
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From peter.kovacs.1.0rc at gmail.com  Thu Feb 15 18:14:16 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 00:14:16 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <45D4DEF0.3010903@quiotix.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
Message-ID: <b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>

I've been thinking a lot about this. The major challenge about this
aspect in my case that I am facing the task of making a library
multi-threaded. The library will then be sold and built into custom
applications.

How do I best integrate a library into an application context which is
unknown beforehand (in terms of thread pools)? Do I have to share
pools with the enclosing applications?

For example assume our library is used in a GUI application which
itself is built on the NetBeans platform. NetBeans has its own kind of
thread pools: http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html.
Does my library have to provide hooks where the application can have
my libraries use the thread management of NetBeans?

Or is the best approach "laisser fair, laisser aller"? And leave it to
"the invisible hand" of the OS scheduler to arbitrate between the
threads of coexisting thread pools?

Thanks
Peter

On 2/15/07, Brian Goetz <brian at quiotix.com> wrote:
> Another is that you might have multiple thread pools within an
> application, and want to allocate (coarsely) CPU cycles to different
> activities.
>
> David Holmes wrote:
> > Two possibilities I can think of straight away:
> >
> > - You might not be the only application trying to run on the system.
> > - You might want to allow room for handling transient overload.
> >
> > Others?
> >
> > David Holmes
> >
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> >> Kovacs
> >> Sent: Friday, 16 February 2007 7:13 AM
> >> To: concurrency-interest
> >> Subject: [concurrency-interest] Target CPU utilization
> >>
> >>
> >> Hi,
> >>
> >> The function determining the optimal pool size includes the target CPU
> >> utilization as a factor. What are the typical reasons why one would
> >> want its value to be less than 1?
> >>
> >> Thanks
> >> P.
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at altair.cs.oswego.edu
> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dcholmes at optusnet.com.au  Thu Feb 15 18:20:38 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 16 Feb 2007 09:20:38 +1000
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702151447i519f60b0rb4dc727d04959334@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEGHHFAA.dcholmes@optusnet.com.au>

Peter,

> > - You might not be the only application trying to run on the system.
>
> I thought of that...but most modern operating systems will do a good
> job scheduling the running applications fairly. (Even with a single
> threaded application, I will not include sleeps in order to yield CPU
> time to other apps in the system.)

Yes but the response time will suffer. Two systems "tuned" for 100%
utilization won't meet the throughput/response times they were tuned for if
they only get 50% of the expected CPU.

> > - You might want to allow room for handling transient overload.
>
> I am not sure I understand this one. Please, could you elaborate?
> (Assume my target is full CPU utilization. Assume I also allow for a
> copious wait time ratio. I will create a superfluous number of
> threads... Hmm... I'd instinctively think that just the opposite is
> true: the less the CPU utilization target, the less I am able to
> handle transient overload. Isn't it?)

I'm thinking about this from a queuing theory perspective. You have an
expected load on your application due to the expected number of tasks and
the work those tasks perform. Assuming you can quantify both then you can
work out expected response times, average queue length etc for a given
number of worker threads and CPUs. If more work comes in than expected - a
transient overload - then the queue length will grow and the response time
will increase. If this queue growth triggers growth of the pool (ie blocking
queue becomes full so pool creates threads from core size up to maximum
size) then without spare CPU cycles you won't help with response times (just
queue length). But if you've allowed for some CPU cycles then they are
available to assist with dealing with the transient overload.

Or to put it more simply. Your normal processing mode allows for idle time
on some CPU's and meets throughput goal X. Then under overload you can
utilize the idle time to try and meet a revised throughput goal Y < X.
Without the spare capacity under overload your throughput might drop to Z <<
X.

If you are providing pools as part of a library you need to provide the
necessary tuning knobs as part of the API.

Cheers,
David


From brian at quiotix.com  Thu Feb 15 18:21:23 2007
From: brian at quiotix.com (Brian Goetz)
Date: Thu, 15 Feb 2007 18:21:23 -0500
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>	
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
Message-ID: <45D4EAF3.9020202@quiotix.com>

All this said, usually U=1.  And, realistically, setting three pool 
sizes does not have to be that exact, as long as you avoid the extremes 
of "way too small" and "way too big".

That said, if you have a library which will be managing threads, you 
need to present it to the application as a service, with lifecycle 
methods like shutdown().  Otherwise, your user's applications will not 
be able to shut down.  And, if you have a lifecycle interface, that 
gives you room to let the user supply (optionally) an Executor to be 
used to process tasks.

Peter Kovacs wrote:
> I've been thinking a lot about this. The major challenge about this
> aspect in my case that I am facing the task of making a library
> multi-threaded. The library will then be sold and built into custom
> applications.
> 
> How do I best integrate a library into an application context which is
> unknown beforehand (in terms of thread pools)? Do I have to share
> pools with the enclosing applications?
> 
> For example assume our library is used in a GUI application which
> itself is built on the NetBeans platform. NetBeans has its own kind of
> thread pools: 
> http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html. 
> 
> Does my library have to provide hooks where the application can have
> my libraries use the thread management of NetBeans?
> 
> Or is the best approach "laisser fair, laisser aller"? And leave it to
> "the invisible hand" of the OS scheduler to arbitrate between the
> threads of coexisting thread pools?
> 
> Thanks
> Peter
> 
> On 2/15/07, Brian Goetz <brian at quiotix.com> wrote:
>> Another is that you might have multiple thread pools within an
>> application, and want to allocate (coarsely) CPU cycles to different
>> activities.
>>
>> David Holmes wrote:
>> > Two possibilities I can think of straight away:
>> >
>> > - You might not be the only application trying to run on the system.
>> > - You might want to allow room for handling transient overload.
>> >
>> > Others?
>> >
>> > David Holmes
>> >
>> >
>> >> -----Original Message-----
>> >> From: concurrency-interest-bounces at cs.oswego.edu
>> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
>> >> Kovacs
>> >> Sent: Friday, 16 February 2007 7:13 AM
>> >> To: concurrency-interest
>> >> Subject: [concurrency-interest] Target CPU utilization
>> >>
>> >>
>> >> Hi,
>> >>
>> >> The function determining the optimal pool size includes the target CPU
>> >> utilization as a factor. What are the typical reasons why one would
>> >> want its value to be less than 1?
>> >>
>> >> Thanks
>> >> P.
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at altair.cs.oswego.edu
>> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at altair.cs.oswego.edu
>> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

From joe.bowbeer at gmail.com  Thu Feb 15 19:27:46 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 15 Feb 2007 16:27:46 -0800
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
Message-ID: <31f2a7bd0702151627p1a776578sdc3c2135b8f9939c@mail.gmail.com>

The specifics depend on how your service is expected to be used.  For
example, can it be invoked as-needed by a SwingWorker-style task?  Or
does it churn away in the background?

Staying out of way of the GUI is especially important in the latter
case.  Lowering the priority of your worker threads may be enough to
keep the GUI from dragging or sputtering.

If your service runs on-demand (and the user is essentially waiting
for you to finish), then providing prompt cancellation and periodic
progress reports is usually sufficient.

It looks like NetBeans and Eclipse have modes where a background task
can be foregrounded, and vice versa.

For the best integration (both in terms of UI and managing load), yes,
it can be important to leverage the platform's existing executors.

On 2/15/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>
> How do I best integrate a library into an application context which is
> unknown beforehand (in terms of thread pools)? Do I have to share
> pools with the enclosing applications?
>
> For example assume our library is used in a GUI application which
> itself is built on the NetBeans platform. NetBeans has its own kind of
> thread pools: http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html.
> Does my library have to provide hooks where the application can have
> my libraries use the thread management of NetBeans?
>
> Or is the best approach "laisser fair, laisser aller"? And leave it to
> "the invisible hand" of the OS scheduler to arbitrate between the
> threads of coexisting thread pools?
>

From matthias.ernst at coremedia.com  Fri Feb 16 03:04:40 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Fri, 16 Feb 2007 09:04:40 +0100
Subject: [concurrency-interest] Target CPU utilization
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au><45D4DEF0.3010903@quiotix.com><b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<31f2a7bd0702151627p1a776578sdc3c2135b8f9939c@mail.gmail.com>
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>

On a related note: how do you size your pools? Do you really go ahead and size them for each machine they are deployed on? Do you use Runtime.getAvailableProcessors() * ASSUMED_BLOCKING_RATIO?

I'm asking because the .NET runtime provides a vm wide thread pool that reacts to the load conditions and adds more threads if the system is underutilized - albeit slowly (every 500 seconds). I've been wondering whether that is a good idea or an MSFT looks-easy-but-ignores-the-realities feature.

Matthias

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu on behalf of Joe Bowbeer
Sent: Fri 2/16/2007 01:27
To: concurrency-interest
Subject: Re: [concurrency-interest] Target CPU utilization
 
The specifics depend on how your service is expected to be used.  For
example, can it be invoked as-needed by a SwingWorker-style task?  Or
does it churn away in the background?

Staying out of way of the GUI is especially important in the latter
case.  Lowering the priority of your worker threads may be enough to
keep the GUI from dragging or sputtering.

If your service runs on-demand (and the user is essentially waiting
for you to finish), then providing prompt cancellation and periodic
progress reports is usually sufficient.

It looks like NetBeans and Eclipse have modes where a background task
can be foregrounded, and vice versa.

For the best integration (both in terms of UI and managing load), yes,
it can be important to leverage the platform's existing executors.

On 2/15/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>
> How do I best integrate a library into an application context which is
> unknown beforehand (in terms of thread pools)? Do I have to share
> pools with the enclosing applications?
>
> For example assume our library is used in a GUI application which
> itself is built on the NetBeans platform. NetBeans has its own kind of
> thread pools: http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html.
> Does my library have to provide hooks where the application can have
> my libraries use the thread management of NetBeans?
>
> Or is the best approach "laisser fair, laisser aller"? And leave it to
> "the invisible hand" of the OS scheduler to arbitrate between the
> threads of coexisting thread pools?
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070216/99feb890/attachment-0001.html 

From matthias.ernst at coremedia.com  Fri Feb 16 03:08:16 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Fri, 16 Feb 2007 09:08:16 +0100
Subject: [concurrency-interest] Target CPU utilization
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au><45D4DEF0.3010903@quiotix.com><b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<31f2a7bd0702151627p1a776578sdc3c2135b8f9939c@mail.gmail.com>
	<AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D168E71@hermes.coremedia.com>

> [the clr thread pool reacts to load conditions] every 500 seconds.

Oops. That would be milliseconds.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070216/4ac5f802/attachment.html 

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 05:40:24 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 11:40:24 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEGHHFAA.dcholmes@optusnet.com.au>
References: <b6e8f2e80702151447i519f60b0rb4dc727d04959334@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEGHHFAA.dcholmes@optusnet.com.au>
Message-ID: <b6e8f2e80702160240i49a529b1mca7125a5c5519ce4@mail.gmail.com>

Thank you for your advice, David! Honestly, I haven't thought about
this problem from the response time perspective. One of the reasons
may be that my performance requirements are mainly centered around
throughput. But there will certainly be applications where response
time will be important.

This leads to another interesting part of my task. The library itself
I am working with is layered. More elementary operations (like
aromatic ring discovery) are used in throughput-minded batch-style
jobs (like generating fingerprints during database import of compounds
-- fingerprints include the number of aromatic rings). But the more
elementary operations are also used in interactive GUI applications
such is in "aromatizing" a displayed chemical structure -- where
response time has a clear priority over throughput.

In addition to the response time vs. throughput dilemma, the question
that really intrigues me about this layered aspect is the following.
Batch-style jobs are coarsely grained operations and are relatively
easy to make parallel. More elementary operations are finely grained
and typically more difficult to make parallel. (The more elementary
operations are more difficult to make parallel partly due to
algorithmic constraints resulting from the nature of the problem they
solve, partly because of their different scale of performance: their
execution times are much more closely comparable to those of the
elementary platform operations, hence they are more sensitive to the
extra overhead associated with adding multithreaded capability.) The
obvious approach to getting ready for "Amdahl's era of computing" is
to start with making parallel the batch category of operations. But
should I not make my multithreading infrasturcture already prepared
for 800 systems. And if I should, what should the underlying thread
management model look like? Should I use for a highly layered library,
the traditional "flat" model of side-by-side thread-pools with
(internal) client code picking the thread-pool best fitting for its
work category? Shouldn't there be a "vertical" coordination between
layers in terms thread management/multithreading for performance (or
other?) reasons?

In fact, in my particular case, some of the "more elementary
operations" (such as a complex chemical reaction) can quickly scale up
through a combination of increased "problem size" and increased
problem complexity into a work unit which already warrants the
introduction of multi-threaded capability into the more elementary
operation (which is not so elementary any more) -- even on two- or
four-way systems.

I would like to give you a specific example of the layered structure I
have to deal with, but I'd rather not for fear of breaking some
confidentiality rule. The point is that I have at least 7 or 8 layers
with components in each that call components in the layer below and
(potentially) run multiple instances of the called components in
separate (pooled) threads. I feel this structure calls for some
vertical "coordination". Or am I overcomplicating things?

And how do I deal with the other aspect your reply made me aware of:
downstream layers should be sometimes optimized for response time
(when called from a certain context of a GUI application) and
sometimes for throughput (when called in a batch job)?

Thanks,
Peter

On 2/16/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> Peter,
>
> > > - You might not be the only application trying to run on the system.
> >
> > I thought of that...but most modern operating systems will do a good
> > job scheduling the running applications fairly. (Even with a single
> > threaded application, I will not include sleeps in order to yield CPU
> > time to other apps in the system.)
>
> Yes but the response time will suffer. Two systems "tuned" for 100%
> utilization won't meet the throughput/response times they were tuned for if
> they only get 50% of the expected CPU.
>
> > > - You might want to allow room for handling transient overload.
> >
> > I am not sure I understand this one. Please, could you elaborate?
> > (Assume my target is full CPU utilization. Assume I also allow for a
> > copious wait time ratio. I will create a superfluous number of
> > threads... Hmm... I'd instinctively think that just the opposite is
> > true: the less the CPU utilization target, the less I am able to
> > handle transient overload. Isn't it?)
>
> I'm thinking about this from a queuing theory perspective. You have an
> expected load on your application due to the expected number of tasks and
> the work those tasks perform. Assuming you can quantify both then you can
> work out expected response times, average queue length etc for a given
> number of worker threads and CPUs. If more work comes in than expected - a
> transient overload - then the queue length will grow and the response time
> will increase. If this queue growth triggers growth of the pool (ie blocking
> queue becomes full so pool creates threads from core size up to maximum
> size) then without spare CPU cycles you won't help with response times (just
> queue length). But if you've allowed for some CPU cycles then they are
> available to assist with dealing with the transient overload.
>
> Or to put it more simply. Your normal processing mode allows for idle time
> on some CPU's and meets throughput goal X. Then under overload you can
> utilize the idle time to try and meet a revised throughput goal Y < X.
> Without the spare capacity under overload your throughput might drop to Z <<
> X.
>
> If you are providing pools as part of a library you need to provide the
> necessary tuning knobs as part of the API.
>
> Cheers,
> David
>
>

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 09:15:23 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 15:15:23 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<31f2a7bd0702151627p1a776578sdc3c2135b8f9939c@mail.gmail.com>
	<AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>
Message-ID: <b6e8f2e80702160615r4d65ac63td1aef6ee21aed584@mail.gmail.com>

Matthias,

On 2/16/07, Ernst, Matthias <matthias.ernst at coremedia.com> wrote:
>
>
>
> On a related note: how do you size your pools? Do you really go ahead and
> size them for each machine they are deployed on? Do you use
> Runtime.getAvailableProcessors() * ASSUMED_BLOCKING_RATIO?

I am still in the planning phase. My immediate plan is to use three
thread pools for three classes of jobs based on the order of the
expected ASSUMED_BLOCKING_RATIO. And yes, right now I expect to use
the formula you mentioned.

I am still pondering how GUI background jobs will fit into this
picture... Joe suggested lowering the priority of GUI background
jobs... I am not sure how to provide the "tuning knob" for this
behaviour to the enclosing application... Especially not sure in the
"pervasive multithreading" scenario where all kinds of operations will
execute multi-threaded and one instance of an "elementary kind" of
operation is eligible for execution in a background job while another
instance of the same kind is executing in user-interaction at the same
time. In this scenario, there appears to be no straightforward way of
telling apart a background work unit from an interactive work unit
based on their kind(s)...One transparent way of doing this (which
comes right now to my mind) is to check the call stack and if we find
the GUI event handler among the callers, we are part of an
interaction...No, it is not a good idea: interactive operations might
start their non-background work in a thread-pool and on the other
hand, basically every payload operation is started from the GUI event
handler in a typical GUI application. It seems that there is no way
around it, the enclosing application must explicitly turn on some kind
of "background" flag for a top level operation and this "background"
flag being then propagated across thread-pools involved in performing
the work units composing this top level operation.

>
>  I'm asking because the .NET runtime provides a vm wide thread pool that
> reacts to the load conditions and adds more threads if the system is
> underutilized - albeit slowly (every 500 seconds). I've been wondering
> whether that is a good idea or an MSFT
> looks-easy-but-ignores-the-realities feature.

>From a program designer's perspective, this "service" provides the
ability to share a common thread pool without having to create the
machinery (interfaces, custom adapters) required for passing around
the reference to it between components which are agnostic of each
other. On the other hand, having just one such thread pool smells
inflexibility (and appears to seriously limit its usefulness).

I am even less sure about the implications from a system engineer's
perspective. Still, it is interesting to know that you have something
like this in .NET.

More knowledgeable people on this list will surely give their
insightful opinion about this.

Peter

>
>  Matthias
>
>
>  -----Original Message-----
>  From: concurrency-interest-bounces at cs.oswego.edu on behalf
> of Joe Bowbeer
>  Sent: Fri 2/16/2007 01:27
>  To: concurrency-interest
>  Subject: Re: [concurrency-interest] Target CPU utilization
>
>  The specifics depend on how your service is expected to be used.  For
>  example, can it be invoked as-needed by a SwingWorker-style task?  Or
>  does it churn away in the background?
>
>  Staying out of way of the GUI is especially important in the latter
>  case.  Lowering the priority of your worker threads may be enough to
>  keep the GUI from dragging or sputtering.
>
>  If your service runs on-demand (and the user is essentially waiting
>  for you to finish), then providing prompt cancellation and periodic
>  progress reports is usually sufficient.
>
>  It looks like NetBeans and Eclipse have modes where a background task
>  can be foregrounded, and vice versa.
>
>  For the best integration (both in terms of UI and managing load), yes,
>  it can be important to leverage the platform's existing executors.
>
>  On 2/15/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>  >
>  > How do I best integrate a library into an application context which is
>  > unknown beforehand (in terms of thread pools)? Do I have to share
>  > pools with the enclosing applications?
>  >
>  > For example assume our library is used in a GUI application which
>  > itself is built on the NetBeans platform. NetBeans has its own kind of
>  > thread pools:
> http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html.
>  > Does my library have to provide hooks where the application can have
>  > my libraries use the thread management of NetBeans?
>  >
>  > Or is the best approach "laisser fair, laisser aller"? And leave it to
>  > "the invisible hand" of the OS scheduler to arbitrate between the
>  > threads of coexisting thread pools?
>  >
>  _______________________________________________
>  Concurrency-interest mailing list
>  Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 09:26:40 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 15:26:40 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<31f2a7bd0702151627p1a776578sdc3c2135b8f9939c@mail.gmail.com>
	<AE2A8E488D9B26438919DF3C9C95528D168E70@hermes.coremedia.com>
Message-ID: <b6e8f2e80702160626p17dc1f25rb4b3cd4e8c2c3745@mail.gmail.com>

In my previous mail, I meant:
'this "background" flag being then propagated across threads involved'
instead of
'this "background" flag being then propagated across thread-pools involved'

P.

On 2/16/07, Ernst, Matthias <matthias.ernst at coremedia.com> wrote:
>
>
>
> On a related note: how do you size your pools? Do you really go ahead and
> size them for each machine they are deployed on? Do you use
> Runtime.getAvailableProcessors() * ASSUMED_BLOCKING_RATIO?
>
>  I'm asking because the .NET runtime provides a vm wide thread pool that
> reacts to the load conditions and adds more threads if the system is
> underutilized - albeit slowly (every 500 seconds). I've been wondering
> whether that is a good idea or an MSFT
> looks-easy-but-ignores-the-realities feature.
>
>  Matthias
>
>
>  -----Original Message-----
>  From: concurrency-interest-bounces at cs.oswego.edu on behalf
> of Joe Bowbeer
>  Sent: Fri 2/16/2007 01:27
>  To: concurrency-interest
>  Subject: Re: [concurrency-interest] Target CPU utilization
>
>  The specifics depend on how your service is expected to be used.  For
>  example, can it be invoked as-needed by a SwingWorker-style task?  Or
>  does it churn away in the background?
>
>  Staying out of way of the GUI is especially important in the latter
>  case.  Lowering the priority of your worker threads may be enough to
>  keep the GUI from dragging or sputtering.
>
>  If your service runs on-demand (and the user is essentially waiting
>  for you to finish), then providing prompt cancellation and periodic
>  progress reports is usually sufficient.
>
>  It looks like NetBeans and Eclipse have modes where a background task
>  can be foregrounded, and vice versa.
>
>  For the best integration (both in terms of UI and managing load), yes,
>  it can be important to leverage the platform's existing executors.
>
>  On 2/15/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>  >
>  > How do I best integrate a library into an application context which is
>  > unknown beforehand (in terms of thread pools)? Do I have to share
>  > pools with the enclosing applications?
>  >
>  > For example assume our library is used in a GUI application which
>  > itself is built on the NetBeans platform. NetBeans has its own kind of
>  > thread pools:
>
http://www.netbeans.org/download/dev/javadoc/org-openide-util/org/openide/util/RequestProcessor.Task.html
.
>  > Does my library have to provide hooks where the application can have
>  > my libraries use the thread management of NetBeans?
>  >
>  > Or is the best approach "laisser fair, laisser aller"? And leave it to
>  > "the invisible hand" of the OS scheduler to arbitrate between the
>  > threads of coexisting thread pools?
>  >
>  _______________________________________________
>  Concurrency-interest mailing list
>  Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070216/5aa55360/attachment.html 

From gergg at cox.net  Fri Feb 16 10:33:46 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 16 Feb 2007 09:33:46 -0600
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <45D4EAF3.9020202@quiotix.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>		<45D4DEF0.3010903@quiotix.com>	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com>
Message-ID: <45D5CEDA.1080104@cox.net>

Brian Goetz wrote:
> All this said, usually U=1.  And, realistically, setting three pool 
> sizes does not have to be that exact, as long as you avoid the extremes 
> of "way too small" and "way too big".
> 
> That said, if you have a library which will be managing threads, you 
> need to present it to the application as a service, with lifecycle 
> methods like shutdown().  Otherwise, your user's applications will not 
> be able to shut down.  And, if you have a lifecycle interface, that 
> gives you room to let the user supply (optionally) an Executor to be 
> used to process tasks.

I like having the ability to plug all aspects of thread pooling.  Having a 
default implementation that works fine for J2SE would be nice.  Then, you could 
provide for plugging a thread factory and as Brian says, even an executor so 
that while your library partitions the work, the application could still decide 
on the relative priority and queuing necessary to use it effectively.

Gregg Wonderly

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 12:28:57 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 18:28:57 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <45D5CEDA.1080104@cox.net>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
Message-ID: <b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>

Thank you, Greg, for your input!

What if an Executor is not sufficient and I need at least an
ExecutorService? The edu.emory.mathcs.backport.java.util.concurrent
does not seem to provide an obvious way to wrapping up an Executor
instance into an ExecutorService.

Similiarily to the way the static methods of the Executors class wrap
up ThreadFactory instances  into Executor instances
(http://dcl.mathcs.emory.edu/util/backport-util-concurrent/doc/api/edu/emory/mathcs/backport/java/util/concurrent/Executors.html),
I would expect an ExecutorServices to be there for wrapping up
Executor instances into ExecutorService instances. Does this make
sense or is there a good reason for the lack of this kind of
functionality?

Thanks
Peter

On 2/16/07,  Gregg Wonderly <gergg at cox.net> wrote:
>  Brian Goetz wrote:
> > All this said, usually U=1.  And, realistically, setting three pool
> > sizes does not have to be that exact, as long as you avoid the extremes
> > of "way too small" and "way too big".
> >
> > That said, if you have a library which will be managing threads, you
> > need to present it to the application as a service, with lifecycle
> > methods like shutdown().  Otherwise, your user's applications will not
> > be able to shut down.  And, if you have a lifecycle interface, that
> > gives you room to let the user supply (optionally) an Executor to be
> > used to process tasks.
>
> I like having the ability to plug all aspects of thread pooling.  Having a
> default implementation that works fine for J2SE would be nice.  Then, you could
> provide for plugging a thread factory and as Brian says, even an executor so
> that while your library partitions the work, the application could still decide
> on the relative priority and queuing necessary to use it effectively.
>
> Gregg Wonderly
>

From gergg at cox.net  Fri Feb 16 13:04:59 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 16 Feb 2007 12:04:59 -0600
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>	
	<45D4DEF0.3010903@quiotix.com>	
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>	
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
	<b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
Message-ID: <45D5F24B.6080206@cox.net>

Peter Kovacs wrote:
> What if an Executor is not sufficient and I need at least an
> ExecutorService? The edu.emory.mathcs.backport.java.util.concurrent
> does not seem to provide an obvious way to wrapping up an Executor
> instance into an ExecutorService.

I think you just need to decide at what level you want to limit the user's 
choice of JVM.  If you want to just go with JDK 1.5 and later, you could just 
keep your internal operations limited to the use of an ExecutorService 
reference, and then allow that type of object to be plugged into your library.

The ThreadPoolExecutor (as an ExecutorService) provides the control that most 
would need via ThreadFactory (where do threads come from) and BlockingQueue (how 
are they prioritized/related).

If you provide this level of interface, I think that there would be few issues 
that other applications could not deal with using as much control as needed.

Gregg Wonderly

From tim at peierls.net  Fri Feb 16 13:21:21 2007
From: tim at peierls.net (Tim Peierls)
Date: Fri, 16 Feb 2007 13:21:21 -0500
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
	<b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
Message-ID: <63b4e4050702161021i67bd8467oa6c3c76766e3152d@mail.gmail.com>

On 2/16/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>
> What if an Executor is not sufficient and I need at least an
> ExecutorService? The edu.emory.mathcs.backport.java.util.concurrent
> does not seem to provide an obvious way to wrapping up an Executor
> instance into an ExecutorService.
>

You can use AbstractExecutorService to create a wrapper around other
Executors and ExecutorServices.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070216/5dddf552/attachment.html 

From joe.bowbeer at gmail.com  Fri Feb 16 13:51:35 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 16 Feb 2007 10:51:35 -0800
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
	<b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
Message-ID: <31f2a7bd0702161051m5f192482j2092cb1c4fe71256@mail.gmail.com>

On 2/16/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
>
> What if an Executor is not sufficient and I need at least an
> ExecutorService? The edu.emory.mathcs.backport.java.util.concurrent
> does not seem to provide an obvious way to wrapping up an Executor
> instance into an ExecutorService.
>
> I would expect an ExecutorServices to be there for wrapping up
> Executor instances into ExecutorService instances. Does this make
> sense or is there a good reason for the lack of this kind of
> functionality?
>

AbstractExecutorService can wrap an Executor.  It implements
everything but the execute method.  (This needs to be spelled out in
the javadoc.)

http://java.sun.com/javase/6/docs/api/java/util/concurrent/AbstractExecutorService.html

Concerning which type is best for your library to require, Executor is
the easiest to provide (it is the lowest common denominator).  In
addition, ExecutorService exposes shutdown methods that your clients
may not wish to provide to everyone.

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 13:51:58 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 19:51:58 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <63b4e4050702161021i67bd8467oa6c3c76766e3152d@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
	<b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
	<63b4e4050702161021i67bd8467oa6c3c76766e3152d@mail.gmail.com>
Message-ID: <b6e8f2e80702161051l6509bbaala228264a8e0a4eb8@mail.gmail.com>

On 2/16/07, Tim Peierls <tim at peierls.net> wrote:
> On 2/16/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
> > What if an Executor is not sufficient and I need at least an
> > ExecutorService? The
> edu.emory.mathcs.backport.java.util.concurrent
> > does not seem to provide an obvious way to wrapping up an Executor
> > instance into an ExecutorService.
> >
>
> You can use AbstractExecutorService to create a wrapper around other
> Executors and ExecutorServices.

I thought of this...the obvious way would be to subclass
AbstractExecutorService and override the Executor.execute method by
delegating to the wrapped custom Executor instance. But for this to
work, all relevant methods in AbstractExecutorService (the submit and
invokeXXX methods in particular) should ultimately call
AbstractExecutorService's own published execute method, which is not
necessarily the case. It is possible that the submit and invokeXXX
methods of AbstractExecutorService directly call the execute method of
a contained (default) Executor instance -- and I found nothing in the
API doc which would encourage me to think the contrary.

Thanks
Peter
>
> --tim
>

From peter.kovacs.1.0rc at gmail.com  Fri Feb 16 13:58:00 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 16 Feb 2007 19:58:00 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <31f2a7bd0702161051m5f192482j2092cb1c4fe71256@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEGGHFAA.dcholmes@optusnet.com.au>
	<45D4DEF0.3010903@quiotix.com>
	<b6e8f2e80702151514id7256dbq55f8301f2dd1f5e3@mail.gmail.com>
	<45D4EAF3.9020202@quiotix.com> <45D5CEDA.1080104@cox.net>
	<b6e8f2e80702160928t5b0712c9wf57493a82fafea3a@mail.gmail.com>
	<31f2a7bd0702161051m5f192482j2092cb1c4fe71256@mail.gmail.com>
Message-ID: <b6e8f2e80702161058k10151eafhe3dd5767f6676987@mail.gmail.com>

On 2/16/07, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> On 2/16/07, Peter Kovacs <peter.kovacs.1.0rc at gmail.com> wrote:
> >
> > What if an Executor is not sufficient and I need at least an
> > ExecutorService? The edu.emory.mathcs.backport.java.util.concurrent
> > does not seem to provide an obvious way to wrapping up an Executor
> > instance into an ExecutorService.
> >
> > I would expect an ExecutorServices to be there for wrapping up
> > Executor instances into ExecutorService instances. Does this make
> > sense or is there a good reason for the lack of this kind of
> > functionality?
> >
>
> AbstractExecutorService can wrap an Executor.  It implements
> everything but the execute method.  (This needs to be spelled out in
> the javadoc.)

Thank you Joe! This clarifies the confusion in my previous mail.

Peter

>
> http://java.sun.com/javase/6/docs/api/java/util/concurrent/AbstractExecutorService.html
>
> Concerning which type is best for your library to require, Executor is
> the easiest to provide (it is the lowest common denominator).  In
> addition, ExecutorService exposes shutdown methods that your clients
> may not wish to provide to everyone.
>

From peter.kovacs.1.0rc at gmail.com  Sun Feb 18 08:08:21 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Sun, 18 Feb 2007 14:08:21 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
Message-ID: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>

Hi,

Are there any frameworks available that provide implementations for
the Executor or ExecutorService interface with extensions for
distributed work? (I have "light weight" frameworks in mind -- more
along the lines of tightly coupled clusters than WAN-based, "open"
grids.)

Are there any hooks in the core util.concurrent implementations in
support of such extensions? (For example the ThreadPoolExecutor could
provide a plug-in interface for alternative "overflow" strategies,
such as: offer additional incoming tasks for remote execution after
all core threads are started and fully utilized before putting them on
the internal queue. This idea may not be very good, but is something
to start with.) Are there any plans to provide such hooks?

In general, I am interested in any technology/frameworks for
distributed computing which can be easily integrated with Java. Can
you suggest any such frameworks (possible free :-) ). Can you point me
to literature that can help me implement such a framework. (I remember
that "Concurrent Programming in Java" [which I have lent to one of my
colleagues] contains a lot of references to literature on distributed
computing -- actually too many to find the relevant ones among them;
and most of them apparently having a more theoretic approach than
desirable for my purpose. JCIP doesn't seem to include any relevant
reference.)

Thanks
Peter

From gregg at cytetech.com  Sun Feb 18 10:14:51 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sun, 18 Feb 2007 09:14:51 -0600
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
Message-ID: <45D86D6B.3010509@cytetech.com>



Peter Kovacs wrote:
> In general, I am interested in any technology/frameworks for
> distributed computing which can be easily integrated with Java. Can
> you suggest any such frameworks (possible free :-) ).

One of the free java based distributed computing systems which I think matches 
this kind of need directly is Jini and Javaspaces.  Mapping a BlockingQueue onto 
a Javaspace would be straight forward.  Mapping an overflow situation onto a 
Javaspace write would be quite natural.

This would allow a client computer to compute many simple things locally, but 
then when the compute load got high, it could pump work out on the "grid" to get 
some help.

Gregg Wonderly

From peter.kovacs.1.0rc at gmail.com  Sun Feb 18 11:30:49 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Sun, 18 Feb 2007 17:30:49 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <45D86D6B.3010509@cytetech.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
	<45D86D6B.3010509@cytetech.com>
Message-ID: <b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>

Thank you Greg! I will check it out.
Peter

On 2/18/07, Gregg Wonderly <gregg at cytetech.com> wrote:
>
>
> Peter Kovacs wrote:
> > In general, I am interested in any technology/frameworks for
> > distributed computing which can be easily integrated with Java. Can
> > you suggest any such frameworks (possible free :-) ).
>
> One of the free java based distributed computing systems which I think matches
> this kind of need directly is Jini and Javaspaces.  Mapping a BlockingQueue onto
> a Javaspace would be straight forward.  Mapping an overflow situation onto a
> Javaspace write would be quite natural.
>
> This would allow a client computer to compute many simple things locally, but
> then when the compute load got high, it could pump work out on the "grid" to get
> some help.
>
> Gregg Wonderly
>

From dawidk at mathcs.emory.edu  Sun Feb 18 11:49:09 2007
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sun, 18 Feb 2007 11:49:09 -0500
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>	<45D86D6B.3010509@cytetech.com>
	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>
Message-ID: <45D88385.6000506@mathcs.emory.edu>

Also, try this:

http://dcl.mathcs.emory.edu/h2o/

It won't give you exactly what you need (just like Jini), but it will 
greatly simplify implementing what you need. It has asynchronous RMI and 
distributed events, which may be useful for implementing the distributed 
executor framework.

Disclaimer: I am biased, as the author of that project.

Peter Kovacs wrote:
> Thank you Greg! I will check it out.
> Peter
>
> On 2/18/07, Gregg Wonderly <gregg at cytetech.com> wrote:
>   
>> Peter Kovacs wrote:
>>     
>>> In general, I am interested in any technology/frameworks for
>>> distributed computing which can be easily integrated with Java. Can
>>> you suggest any such frameworks (possible free :-) ).
>>>       
>> One of the free java based distributed computing systems which I think matches
>> this kind of need directly is Jini and Javaspaces.  Mapping a BlockingQueue onto
>> a Javaspace would be straight forward.  Mapping an overflow situation onto a
>> Javaspace write would be quite natural.
>>
>> This would allow a client computer to compute many simple things locally, but
>> then when the compute load got high, it could pump work out on the "grid" to get
>> some help.
>>
>> Gregg Wonderly
>>
>>     
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   



From peter.kovacs.1.0rc at gmail.com  Sun Feb 18 12:10:33 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Sun, 18 Feb 2007 18:10:33 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <45D88385.6000506@mathcs.emory.edu>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
	<45D86D6B.3010509@cytetech.com>
	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>
	<45D88385.6000506@mathcs.emory.edu>
Message-ID: <b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>

Thank you Dawid for the link. I am relieved that I have a choice.
(After reading the first half [ :-) ] of the Sun introduction to
JavaSpaces, I've started being worried that JavaSpaces might carry a
substantial overhead associated with goals which I am not too much
interested in (persistency, transaction?) -- at least not initially. I
know I get easily worried. :-) )

Thanks
Peter

On 2/18/07, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
> Also, try this:
>
> http://dcl.mathcs.emory.edu/h2o/
>
> It won't give you exactly what you need (just like Jini), but it will
> greatly simplify implementing what you need. It has asynchronous RMI and
> distributed events, which may be useful for implementing the distributed
> executor framework.
>
> Disclaimer: I am biased, as the author of that project.
>
> Peter Kovacs wrote:
> > Thank you Greg! I will check it out.
> > Peter
> >
> > On 2/18/07, Gregg Wonderly <gregg at cytetech.com> wrote:
> >
> >> Peter Kovacs wrote:
> >>
> >>> In general, I am interested in any technology/frameworks for
> >>> distributed computing which can be easily integrated with Java. Can
> >>> you suggest any such frameworks (possible free :-) ).
> >>>
> >> One of the free java based distributed computing systems which I think matches
> >> this kind of need directly is Jini and Javaspaces.  Mapping a BlockingQueue onto
> >> a Javaspace would be straight forward.  Mapping an overflow situation onto a
> >> Javaspace write would be quite natural.
> >>
> >> This would allow a client computer to compute many simple things locally, but
> >> then when the compute load got high, it could pump work out on the "grid" to get
> >> some help.
> >>
> >> Gregg Wonderly
> >>
> >>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
>

From belaban at yahoo.com  Sun Feb 18 13:41:57 2007
From: belaban at yahoo.com (Bela Ban)
Date: Sun, 18 Feb 2007 19:41:57 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>	<45D86D6B.3010509@cytetech.com>	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>	<45D88385.6000506@mathcs.emory.edu>
	<b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
Message-ID: <45D89DF5.3000402@yahoo.com>

Take a look at JGroups, it allows you to build something like JavaSpaces 
in no time.

Peter Kovacs wrote:
> Thank you Dawid for the link. I am relieved that I have a choice.
> (After reading the first half [ :-) ] of the Sun introduction to
> JavaSpaces, I've started being worried that JavaSpaces might carry a
> substantial overhead associated with goals which I am not too much
> interested in (persistency, transaction?) -- at least not initially. I
> know I get easily worried. :-) )
>
> Thanks
> Peter
>
> On 2/18/07, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
>   
>> Also, try this:
>>
>> http://dcl.mathcs.emory.edu/h2o/
>>
>> It won't give you exactly what you need (just like Jini), but it will
>> greatly simplify implementing what you need. It has asynchronous RMI and
>> distributed events, which may be useful for implementing the distributed
>> executor framework.
>>
>> Disclaimer: I am biased, as the author of that project.
>>
>> Peter Kovacs wrote:
>>     
>>> Thank you Greg! I will check it out.
>>> Peter
>>>
>>> On 2/18/07, Gregg Wonderly <gregg at cytetech.com> wrote:
>>>
>>>       
>>>> Peter Kovacs wrote:
>>>>
>>>>         
>>>>> In general, I am interested in any technology/frameworks for
>>>>> distributed computing which can be easily integrated with Java. Can
>>>>> you suggest any such frameworks (possible free :-) ).
>>>>>
>>>>>           
>>>> One of the free java based distributed computing systems which I think matches
>>>> this kind of need directly is Jini and Javaspaces.  Mapping a BlockingQueue onto
>>>> a Javaspace would be straight forward.  Mapping an overflow situation onto a
>>>> Javaspace write would be quite natural.
>>>>
>>>> This would allow a client computer to compute many simple things locally, but
>>>> then when the compute load got high, it could pump work out on the "grid" to get
>>>> some help.
>>>>
>>>> Gregg Wonderly
>>>>
>>>>
>>>>         
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>       
>>
>>     
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>   

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat


From mailinglist.taras.tielkes at gmail.com  Sun Feb 18 14:19:48 2007
From: mailinglist.taras.tielkes at gmail.com (Taras Tielkes)
Date: Sun, 18 Feb 2007 20:19:48 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <45D89DF5.3000402@yahoo.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>	<45D86D6B.3010509@cytetech.com>	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>	<45D88385.6000506@mathcs.emory.edu>	<b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
	<45D89DF5.3000402@yahoo.com>
Message-ID: <45D8A6D4.8060607@gmail.com>

Bela Ban wrote:
> Take a look at JGroups, it allows you to build something like JavaSpaces 
> in no time.

This is a bit off topic, but what protocol and what parts of the 
infrastructure of JGroups would you suggest for a distributed work 
queue? I'd need to handle slow/failing nodes, so I would expect some 
form of timed task leasing...

(Actually, I implemented such a thing recently, but chose to simply use 
multicast. The code did return Futures to clients however, which 
implemented distributed cancellation)


-tt

From orion at terracottatech.com  Sun Feb 18 17:26:18 2007
From: orion at terracottatech.com (Orion Letizi)
Date: Sun, 18 Feb 2007 14:26:18 -0800
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
	<45D86D6B.3010509@cytetech.com>
	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>
	<45D88385.6000506@mathcs.emory.edu>
	<b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
Message-ID: <BB86B315-DEB0-4B4B-B436-871D760CAE94@terracottatech.com>

You could also check out Terracotta:

	http://www.terracotta.org/

It does transparent application clustering.

Disclaimer:  I work for Terracotta.

--Orion

On Feb 18, 2007, at 9:10 AM, Peter Kovacs wrote:

> Thank you Dawid for the link. I am relieved that I have a choice.
> (After reading the first half [ :-) ] of the Sun introduction to
> JavaSpaces, I've started being worried that JavaSpaces might carry a
> substantial overhead associated with goals which I am not too much
> interested in (persistency, transaction?) -- at least not initially. I
> know I get easily worried. :-) )
>
> Thanks
> Peter
>
> On 2/18/07, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
>> Also, try this:
>>
>> http://dcl.mathcs.emory.edu/h2o/
>>
>> It won't give you exactly what you need (just like Jini), but it will
>> greatly simplify implementing what you need. It has asynchronous  
>> RMI and
>> distributed events, which may be useful for implementing the  
>> distributed
>> executor framework.
>>
>> Disclaimer: I am biased, as the author of that project.
>>
>> Peter Kovacs wrote:
>>> Thank you Greg! I will check it out.
>>> Peter
>>>
>>> On 2/18/07, Gregg Wonderly <gregg at cytetech.com> wrote:
>>>
>>>> Peter Kovacs wrote:
>>>>
>>>>> In general, I am interested in any technology/frameworks for
>>>>> distributed computing which can be easily integrated with Java.  
>>>>> Can
>>>>> you suggest any such frameworks (possible free :-) ).
>>>>>
>>>> One of the free java based distributed computing systems which I  
>>>> think matches
>>>> this kind of need directly is Jini and Javaspaces.  Mapping a  
>>>> BlockingQueue onto
>>>> a Javaspace would be straight forward.  Mapping an overflow  
>>>> situation onto a
>>>> Javaspace write would be quite natural.
>>>>
>>>> This would allow a client computer to compute many simple things  
>>>> locally, but
>>>> then when the compute load got high, it could pump work out on  
>>>> the "grid" to get
>>>> some help.
>>>>
>>>> Gregg Wonderly
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From jseigh_cp00 at xemaps.com  Sun Feb 18 18:38:09 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Sun, 18 Feb 2007 18:38:09 -0500
Subject: [concurrency-interest] Lock-free signaling
Message-ID: <45D8E361.4070902@xemaps.com>

Is there a way to do signaling without requiring a monitor lock?  Something
like events or eventcounts.   I'm doing an STM implementation and was
thinking of adding a listener pattern to it but there's no point in 
making the
implementation lock-free if I'm going to need locking to implement 
signaling.

--
Joe Seigh

From gregg at cytetech.com  Sun Feb 18 19:40:20 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sun, 18 Feb 2007 18:40:20 -0600
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>
	<45D86D6B.3010509@cytetech.com>
	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>
	<45D88385.6000506@mathcs.emory.edu>
	<b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>
Message-ID: <45D8F1F4.4040908@cytetech.com>



Peter Kovacs wrote:
> Thank you Dawid for the link. I am relieved that I have a choice.
> (After reading the first half [ :-) ] of the Sun introduction to
> JavaSpaces, I've started being worried that JavaSpaces might carry a
> substantial overhead associated with goals which I am not too much
> interested in (persistency, transaction?) -- at least not initially. I
> know I get easily worried. :-) )

In a transient environment, the persistence and transaction capabilties of 
javaspaces are not required.  They are tools which you can choose to use, or not 
use.

Gregg Wonderly

From dcholmes at optusnet.com.au  Sun Feb 18 20:01:12 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 19 Feb 2007 11:01:12 +1000
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <45D8E361.4070902@xemaps.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEHBHFAA.dcholmes@optusnet.com.au>

Joe Seigh writes:
> Is there a way to do signaling without requiring a monitor lock?
> Something like events or eventcounts.   I'm doing an STM implementation
and was
> thinking of adding a listener pattern to it but there's no point in
> making the implementation lock-free if I'm going to need locking to
implement
> signaling.

I don't quite follow the usage context. If the threads you need to signal
are blocked by calling park(), then a lock-free signal is an unpark(). But
the hard part is ensuring no lost signals.

Or do you mean at a lower level? park()/unpark() may ultimately involved OS
locks and condvars.

David Holmes


From jseigh_cp00 at xemaps.com  Sun Feb 18 21:21:55 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Sun, 18 Feb 2007 21:21:55 -0500
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEHBHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEHBHFAA.dcholmes@optusnet.com.au>
Message-ID: <45D909C3.4060800@xemaps.com>

David Holmes wrote:

>* Replies will be sent through Spamex to dholmes at ieee.org
>* For additional info click -> http://www.spamex.com/i/?v=13730463
>
>Joe Seigh writes:
>  
>
>>Is there a way to do signaling without requiring a monitor lock?
>>Something like events or eventcounts.   I'm doing an STM implementation
>>    
>>
>and was
>  
>
>>thinking of adding a listener pattern to it but there's no point in
>>making the implementation lock-free if I'm going to need locking to
>>    
>>
>implement
>  
>
>>signaling.
>>    
>>
>
>I don't quite follow the usage context. If the threads you need to signal
>are blocked by calling park(), then a lock-free signal is an unpark(). But
>the hard part is ensuring no lost signals.
>  
>

It looks like they call native methods, at least for Sun's JVM, so that 
might work.

>Or do you mean at a lower level? park()/unpark() may ultimately involved OS
>locks and condvars.
>  
>
There's Linux futexes, windows events, and probably some undocumented 
Solaris interfaces
but yeah, a lock and condvar implementation could be problematic.

--
Joe Seigh


From dcholmes at optusnet.com.au  Sun Feb 18 21:38:05 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 19 Feb 2007 12:38:05 +1000
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <45D909C3.4060800@xemaps.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEHCHFAA.dcholmes@optusnet.com.au>

Joe,

> There's Linux futexes, windows events, and probably some undocumented
> Solaris interfaces
> but yeah, a lock and condvar implementation could be problematic.

Current hotspot implementation is mutex+condvar on Solaris and Linux, and an
Event on Windows.

David Holmes


From belaban at yahoo.com  Mon Feb 19 01:49:24 2007
From: belaban at yahoo.com (Bela Ban)
Date: Mon, 19 Feb 2007 07:49:24 +0100
Subject: [concurrency-interest] Extension(s) for distributed computing
In-Reply-To: <45D8A6D4.8060607@gmail.com>
References: <b6e8f2e80702180508y51d6a3c7kbf096c53a01f39df@mail.gmail.com>	<45D86D6B.3010509@cytetech.com>	<b6e8f2e80702180830s66aa6c04g95247bf78a87b746@mail.gmail.com>	<45D88385.6000506@mathcs.emory.edu>	<b6e8f2e80702180910o7a82cd78p854627e5fc3d1e6d@mail.gmail.com>	<45D89DF5.3000402@yahoo.com>
	<45D8A6D4.8060607@gmail.com>
Message-ID: <45D94874.9010501@yahoo.com>

Sounds like you want a JMS-like queue, where producers place work items 
and consumers consume them, but rather than 1 global queue you want 
multiple queues, and work items distributed/replicated between the queues ?
This is already done in JBossMessaging, with JGroups as the reliable 
transport.

I suggest to continue this discussion on the JGroups dev or user mailing 
list...


Taras Tielkes wrote:
> This is a bit off topic, but what protocol and what parts of the
> infrastructure of JGroups would you suggest for a distributed work
> queue? I'd need to handle slow/failing nodes, so I would expect some
> form of timed task leasing...
>
> (Actually, I implemented such a thing recently, but chose to simply use
> multicast. The code did return Futures to clients however, which
> implemented distributed cancellation)

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat

From jseigh_cp00 at xemaps.com  Mon Feb 19 06:19:19 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Mon, 19 Feb 2007 06:19:19 -0500
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEHCHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEHCHFAA.dcholmes@optusnet.com.au>
Message-ID: <45D987B7.20402@xemaps.com>

David Holmes wrote:

>  
>
>>There's Linux futexes, windows events, and probably some undocumented
>>Solaris interfaces
>>but yeah, a lock and condvar implementation could be problematic.
>>    
>>
>
>Current hotspot implementation is mutex+condvar on Solaris and Linux, and an
>Event on Windows.
>  
>
I once did a lock-free condvar implementation on Linux using the futex 
syscall.  The NPTL version uses
an internal mutex.  I measured a 10% scalability performance 
difference.  That's at least 10%
because Linux preempts signalers and that causes major degradation in 
scalability so the
actual amount of degradation might be much greater.

Hopefully the atomics use native compare and swap where possible. 


--
Joe Seigh

From dcholmes at optusnet.com.au  Mon Feb 19 18:31:47 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 20 Feb 2007 09:31:47 +1000
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <b6e8f2e80702160240i49a529b1mca7125a5c5519ce4@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHHHFAA.dcholmes@optusnet.com.au>

Peter,

Sorry for the delay in responding. The issues you are wrestling with go
beyond my experience in putting any of this into practice - sorry.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Friday, 16 February 2007 8:40 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] Target CPU utilization
>
>
> Thank you for your advice, David! Honestly, I haven't thought about
> this problem from the response time perspective. One of the reasons
> may be that my performance requirements are mainly centered around
> throughput. But there will certainly be applications where response
> time will be important.
>
> This leads to another interesting part of my task. The library itself
> I am working with is layered. More elementary operations (like
> aromatic ring discovery) are used in throughput-minded batch-style
> jobs (like generating fingerprints during database import of compounds
> -- fingerprints include the number of aromatic rings). But the more
> elementary operations are also used in interactive GUI applications
> such is in "aromatizing" a displayed chemical structure -- where
> response time has a clear priority over throughput.
>
> In addition to the response time vs. throughput dilemma, the question
> that really intrigues me about this layered aspect is the following.
> Batch-style jobs are coarsely grained operations and are relatively
> easy to make parallel. More elementary operations are finely grained
> and typically more difficult to make parallel. (The more elementary
> operations are more difficult to make parallel partly due to
> algorithmic constraints resulting from the nature of the problem they
> solve, partly because of their different scale of performance: their
> execution times are much more closely comparable to those of the
> elementary platform operations, hence they are more sensitive to the
> extra overhead associated with adding multithreaded capability.) The
> obvious approach to getting ready for "Amdahl's era of computing" is
> to start with making parallel the batch category of operations. But
> should I not make my multithreading infrasturcture already prepared
> for 800 systems. And if I should, what should the underlying thread
> management model look like? Should I use for a highly layered library,
> the traditional "flat" model of side-by-side thread-pools with
> (internal) client code picking the thread-pool best fitting for its
> work category? Shouldn't there be a "vertical" coordination between
> layers in terms thread management/multithreading for performance (or
> other?) reasons?
>
> In fact, in my particular case, some of the "more elementary
> operations" (such as a complex chemical reaction) can quickly scale up
> through a combination of increased "problem size" and increased
> problem complexity into a work unit which already warrants the
> introduction of multi-threaded capability into the more elementary
> operation (which is not so elementary any more) -- even on two- or
> four-way systems.
>
> I would like to give you a specific example of the layered structure I
> have to deal with, but I'd rather not for fear of breaking some
> confidentiality rule. The point is that I have at least 7 or 8 layers
> with components in each that call components in the layer below and
> (potentially) run multiple instances of the called components in
> separate (pooled) threads. I feel this structure calls for some
> vertical "coordination". Or am I overcomplicating things?
>
> And how do I deal with the other aspect your reply made me aware of:
> downstream layers should be sometimes optimized for response time
> (when called from a certain context of a GUI application) and
> sometimes for throughput (when called in a batch job)?
>
> Thanks,
> Peter
>
> On 2/16/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> > Peter,
> >
> > > > - You might not be the only application trying to run on the system.
> > >
> > > I thought of that...but most modern operating systems will do a good
> > > job scheduling the running applications fairly. (Even with a single
> > > threaded application, I will not include sleeps in order to yield CPU
> > > time to other apps in the system.)
> >
> > Yes but the response time will suffer. Two systems "tuned" for 100%
> > utilization won't meet the throughput/response times they were
> tuned for if
> > they only get 50% of the expected CPU.
> >
> > > > - You might want to allow room for handling transient overload.
> > >
> > > I am not sure I understand this one. Please, could you elaborate?
> > > (Assume my target is full CPU utilization. Assume I also allow for a
> > > copious wait time ratio. I will create a superfluous number of
> > > threads... Hmm... I'd instinctively think that just the opposite is
> > > true: the less the CPU utilization target, the less I am able to
> > > handle transient overload. Isn't it?)
> >
> > I'm thinking about this from a queuing theory perspective. You have an
> > expected load on your application due to the expected number of
> tasks and
> > the work those tasks perform. Assuming you can quantify both
> then you can
> > work out expected response times, average queue length etc for a given
> > number of worker threads and CPUs. If more work comes in than
> expected - a
> > transient overload - then the queue length will grow and the
> response time
> > will increase. If this queue growth triggers growth of the pool
> (ie blocking
> > queue becomes full so pool creates threads from core size up to maximum
> > size) then without spare CPU cycles you won't help with
> response times (just
> > queue length). But if you've allowed for some CPU cycles then they are
> > available to assist with dealing with the transient overload.
> >
> > Or to put it more simply. Your normal processing mode allows
> for idle time
> > on some CPU's and meets throughput goal X. Then under overload you can
> > utilize the idle time to try and meet a revised throughput goal Y < X.
> > Without the spare capacity under overload your throughput might
> drop to Z <<
> > X.
> >
> > If you are providing pools as part of a library you need to provide the
> > necessary tuning knobs as part of the API.
> >
> > Cheers,
> > David
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.kovacs.1.0rc at gmail.com  Tue Feb 20 11:08:51 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Tue, 20 Feb 2007 17:08:51 +0100
Subject: [concurrency-interest] Target CPU utilization
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEHHHFAA.dcholmes@optusnet.com.au>
References: <b6e8f2e80702160240i49a529b1mca7125a5c5519ce4@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEHHHFAA.dcholmes@optusnet.com.au>
Message-ID: <b6e8f2e80702200808xc1df683r3658106da30e29c2@mail.gmail.com>

Thank you, David! I appreciate your feedback anyway. (I am aware that
the problem domain I've been trying to explore may be purely
imaginary. :-) )

Thanks
Peter

On 2/20/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> Peter,
>
> Sorry for the delay in responding. The issues you are wrestling with go
> beyond my experience in putting any of this into practice - sorry.
>
> David
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> > Kovacs
> > Sent: Friday, 16 February 2007 8:40 PM
> > To: dholmes at ieee.org
> > Cc: concurrency-interest
> > Subject: Re: [concurrency-interest] Target CPU utilization
> >
> >
> > Thank you for your advice, David! Honestly, I haven't thought about
> > this problem from the response time perspective. One of the reasons
> > may be that my performance requirements are mainly centered around
> > throughput. But there will certainly be applications where response
> > time will be important.
> >
> > This leads to another interesting part of my task. The library itself
> > I am working with is layered. More elementary operations (like
> > aromatic ring discovery) are used in throughput-minded batch-style
> > jobs (like generating fingerprints during database import of compounds
> > -- fingerprints include the number of aromatic rings). But the more
> > elementary operations are also used in interactive GUI applications
> > such is in "aromatizing" a displayed chemical structure -- where
> > response time has a clear priority over throughput.
> >
> > In addition to the response time vs. throughput dilemma, the question
> > that really intrigues me about this layered aspect is the following.
> > Batch-style jobs are coarsely grained operations and are relatively
> > easy to make parallel. More elementary operations are finely grained
> > and typically more difficult to make parallel. (The more elementary
> > operations are more difficult to make parallel partly due to
> > algorithmic constraints resulting from the nature of the problem they
> > solve, partly because of their different scale of performance: their
> > execution times are much more closely comparable to those of the
> > elementary platform operations, hence they are more sensitive to the
> > extra overhead associated with adding multithreaded capability.) The
> > obvious approach to getting ready for "Amdahl's era of computing" is
> > to start with making parallel the batch category of operations. But
> > should I not make my multithreading infrasturcture already prepared
> > for 800 systems. And if I should, what should the underlying thread
> > management model look like? Should I use for a highly layered library,
> > the traditional "flat" model of side-by-side thread-pools with
> > (internal) client code picking the thread-pool best fitting for its
> > work category? Shouldn't there be a "vertical" coordination between
> > layers in terms thread management/multithreading for performance (or
> > other?) reasons?
> >
> > In fact, in my particular case, some of the "more elementary
> > operations" (such as a complex chemical reaction) can quickly scale up
> > through a combination of increased "problem size" and increased
> > problem complexity into a work unit which already warrants the
> > introduction of multi-threaded capability into the more elementary
> > operation (which is not so elementary any more) -- even on two- or
> > four-way systems.
> >
> > I would like to give you a specific example of the layered structure I
> > have to deal with, but I'd rather not for fear of breaking some
> > confidentiality rule. The point is that I have at least 7 or 8 layers
> > with components in each that call components in the layer below and
> > (potentially) run multiple instances of the called components in
> > separate (pooled) threads. I feel this structure calls for some
> > vertical "coordination". Or am I overcomplicating things?
> >
> > And how do I deal with the other aspect your reply made me aware of:
> > downstream layers should be sometimes optimized for response time
> > (when called from a certain context of a GUI application) and
> > sometimes for throughput (when called in a batch job)?
> >
> > Thanks,
> > Peter
> >
> > On 2/16/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> > > Peter,
> > >
> > > > > - You might not be the only application trying to run on the system.
> > > >
> > > > I thought of that...but most modern operating systems will do a good
> > > > job scheduling the running applications fairly. (Even with a single
> > > > threaded application, I will not include sleeps in order to yield CPU
> > > > time to other apps in the system.)
> > >
> > > Yes but the response time will suffer. Two systems "tuned" for 100%
> > > utilization won't meet the throughput/response times they were
> > tuned for if
> > > they only get 50% of the expected CPU.
> > >
> > > > > - You might want to allow room for handling transient overload.
> > > >
> > > > I am not sure I understand this one. Please, could you elaborate?
> > > > (Assume my target is full CPU utilization. Assume I also allow for a
> > > > copious wait time ratio. I will create a superfluous number of
> > > > threads... Hmm... I'd instinctively think that just the opposite is
> > > > true: the less the CPU utilization target, the less I am able to
> > > > handle transient overload. Isn't it?)
> > >
> > > I'm thinking about this from a queuing theory perspective. You have an
> > > expected load on your application due to the expected number of
> > tasks and
> > > the work those tasks perform. Assuming you can quantify both
> > then you can
> > > work out expected response times, average queue length etc for a given
> > > number of worker threads and CPUs. If more work comes in than
> > expected - a
> > > transient overload - then the queue length will grow and the
> > response time
> > > will increase. If this queue growth triggers growth of the pool
> > (ie blocking
> > > queue becomes full so pool creates threads from core size up to maximum
> > > size) then without spare CPU cycles you won't help with
> > response times (just
> > > queue length). But if you've allowed for some CPU cycles then they are
> > > available to assist with dealing with the transient overload.
> > >
> > > Or to put it more simply. Your normal processing mode allows
> > for idle time
> > > on some CPU's and meets throughput goal X. Then under overload you can
> > > utilize the idle time to try and meet a revised throughput goal Y < X.
> > > Without the spare capacity under overload your throughput might
> > drop to Z <<
> > > X.
> > >
> > > If you are providing pools as part of a library you need to provide the
> > > necessary tuning knobs as part of the API.
> > >
> > > Cheers,
> > > David
> > >
> > >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From David.Biesack at sas.com  Wed Feb 21 08:07:37 2007
From: David.Biesack at sas.com (David J. Biesack)
Date: Wed, 21 Feb 2007 08:07:37 -0500 (EST)
Subject: [concurrency-interest] A Fast Wait-Free HashTable
Message-ID: <200702211307.l1LD7bEb001400@cs.oswego.edu>


Does anyone know more about Cliff Click's wait-free hashtable?

http://cs.stanford.edu/calendar/abstract.php?eventId=2270

"A Faster Wait-Free Hash Table"

"I present a wait-free (lock-free) concurrent Hash Table implementation with better single-thread performance than most Hash Tables, and better (usual much better) multi-thread performance than all other implementations I tried. I demonstrate scaling up to 768 CPUs even with high mutation rates. I show correctness by looking at the problem in a very different light than the usual "happens-before" / memory-order / fencing style of thinking -- indeed the algorithm requires no memory-ordering, no memory fencing, no D-CAS and no CAS-spin-loops."

Sounds interesting. Has anyone seen or reviewed the design or implementation, or comparisons to ConcurrentHashMap?

See also Tim O'Reilly's blog:

 http://radar.oreilly.com/archives/2007/02/a_fast_waitfree_1.html

-- 
David J. Biesack     SAS Institute Inc.
(919) 531-7771       SAS Campus Drive
http://www.sas.com   Cary, NC 27513



From dl at cs.oswego.edu  Wed Feb 21 08:18:15 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 21 Feb 2007 08:18:15 -0500
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <200702211307.l1LD7bEb001400@cs.oswego.edu>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>
Message-ID: <45DC4697.9090300@cs.oswego.edu>

David J. Biesack wrote:
> Does anyone know more about Cliff Click's wait-free hashtable?
> 
> http://cs.stanford.edu/calendar/abstract.php?eventId=2270
> 
> "A Faster Wait-Free Hash Table"

Yes. Cliff and I and other JSR166 folks have been discussing the steps
needed to adapt for j.u.c., which entails some extra support in
atomics package, which we are in turn right now mapping out.
Expect something (at the very least a version specialized as
ConcurrentIdentityHashMap) to show up sometime in CVS of
preliminary versions of Java-7 stuff, which we hope to set up
fairly soon.

-Doug

From alarmnummer at gmail.com  Wed Feb 21 09:08:55 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 21 Feb 2007 15:08:55 +0100
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <45DC4697.9090300@cs.oswego.edu>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>
	<45DC4697.9090300@cs.oswego.edu>
Message-ID: <1466c1d60702210608p98cc6ddp9162b25764fd368b@mail.gmail.com>

Does anyone know if this structure supports 'safe handoff'?

On 2/21/07, Doug Lea <dl at cs.oswego.edu> wrote:
> David J. Biesack wrote:
> > Does anyone know more about Cliff Click's wait-free hashtable?
> >
> > http://cs.stanford.edu/calendar/abstract.php?eventId=2270
> >
> > "A Faster Wait-Free Hash Table"
>
> Yes. Cliff and I and other JSR166 folks have been discussing the steps
> needed to adapt for j.u.c., which entails some extra support in
> atomics package, which we are in turn right now mapping out.
> Expect something (at the very least a version specialized as
> ConcurrentIdentityHashMap) to show up sometime in CVS of
> preliminary versions of Java-7 stuff, which we hope to set up
> fairly soon.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dl at cs.oswego.edu  Wed Feb 21 09:20:16 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 21 Feb 2007 09:20:16 -0500
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <1466c1d60702210608p98cc6ddp9162b25764fd368b@mail.gmail.com>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>	
	<45DC4697.9090300@cs.oswego.edu>
	<1466c1d60702210608p98cc6ddp9162b25764fd368b@mail.gmail.com>
Message-ID: <45DC5520.4050104@cs.oswego.edu>

Peter Veentjer wrote:
> Does anyone know if this structure supports 'safe handoff'?
> 

The j.u.c versions most certainly will. And I believe the
one the slides report performance results on also does.

(The "extra support in atomics package" I mentioned
is mainly to make generally available operations
that allow you to more carefully control these kinds of
happens-before relations that external users need without
adding needless overhead to internal aspects that don't.
Cliff needed to use underlying VM primitives to get
this effect. We'll be adding some classes/operations
that expose these to non-privileged programs, so anyone
can use them.)

-Doug


> On 2/21/07, Doug Lea <dl at cs.oswego.edu> wrote:
>> David J. Biesack wrote:
>> > Does anyone know more about Cliff Click's wait-free hashtable?
>> >
>> > http://cs.stanford.edu/calendar/abstract.php?eventId=2270
>> >

From alarmnummer at gmail.com  Wed Feb 21 10:28:08 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 21 Feb 2007 16:28:08 +0100
Subject: [concurrency-interest] dealing with people that question visibility
	problems
Message-ID: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>

I have a question about dealing with other developers that question
the usefulness of preventing visibility problems.

At the moment I'm almost finished with writing a blogpost about
potential visibility problems in Spring. But one of the questions I'm
going to receive (and already have received) is : why should we go for
all the extra trouble? It works at the moment, we haven't seen any
visibility problems yet, so why should we fix what isn't broken.

There are a lot of theoretical reasons I can think of why writing
threadsafe code is a good thing. But if it doesn't go wrong in
practice, it is hard to convince other developers.

My guess is that with the introduction of multi core systems (with an
increasing number of cpu's), memory coherence is going to be weaker,
and the chance of visibility problems is going to increase. But if you
don't get errors today, why should someone bother?

Does someone have experience with dealing with this situation?

From Darron_Shaffer at stercomm.com  Wed Feb 21 11:37:26 2007
From: Darron_Shaffer at stercomm.com (Shaffer, Darron)
Date: Wed, 21 Feb 2007 11:37:26 -0500
Subject: [concurrency-interest] dealing with people that question
	visibilityproblems
In-Reply-To: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
Message-ID: <303629700276DF4D9ED7D011221B8FAA0949994B@scidubmsg03.sci.local>

You can always use FUD in a good cause.

Explain that these problems will likely pop up at a customer site, in
production, at 2 AM on a national holiday during critical end-of-quarter
processing.  And take a minimum of 5 days to figure out. 

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Peter
Veentjer
Sent: Wednesday, February 21, 2007 9:28 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] dealing with people that question
visibilityproblems

I have a question about dealing with other developers that question
the usefulness of preventing visibility problems.

At the moment I'm almost finished with writing a blogpost about
potential visibility problems in Spring. But one of the questions I'm
going to receive (and already have received) is : why should we go for
all the extra trouble? It works at the moment, we haven't seen any
visibility problems yet, so why should we fix what isn't broken.

There are a lot of theoretical reasons I can think of why writing
threadsafe code is a good thing. But if it doesn't go wrong in
practice, it is hard to convince other developers.

My guess is that with the introduction of multi core systems (with an
increasing number of cpu's), memory coherence is going to be weaker,
and the chance of visibility problems is going to increase. But if you
don't get errors today, why should someone bother?

Does someone have experience with dealing with this situation?
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From gergg at cox.net  Wed Feb 21 11:47:59 2007
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 21 Feb 2007 10:47:59 -0600
Subject: [concurrency-interest] dealing with people that question
 visibility problems
In-Reply-To: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
Message-ID: <45DC77BF.6090704@cox.net>

Peter Veentjer wrote:
> Does someone have experience with dealing with this situation?

On a single core systems, and prior to JDK 1.5 (and some 1.4) simple (incorrect) 
code such as

public class MyThread implements Runnable {
     private boolean done;
     public void shutdown() {
	done = true;
     }

     public void run() {
         while( !done ) {
             ...
         }
     }
}

will work just fine.  Now days, you will have visibility problems with changes 
in the value of "done".  I recently had a conversation with a research professor 
who said that his current mantra is that all class fields must either be "final" 
or "volatile".  If a field doesn't have one of those qualifiers on it, it will 
be problematic and the overhead of volatile is far less of a concern than 
incorrect code.

There are probably a million or more ways to demonstrate visibility problems for 
the inexperienced to learn from.  Maybe someone can create a nice set of 
graphical demos that show the problem literally.

Gregg Wonderly

From alarmnummer at gmail.com  Wed Feb 21 11:50:02 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 21 Feb 2007 17:50:02 +0100
Subject: [concurrency-interest] dealing with people that question
	visibility problems
In-Reply-To: <45DC77BF.6090704@cox.net>
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
	<45DC77BF.6090704@cox.net>
Message-ID: <1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>

Hi Greg,

you are using the same example as I have in my post (not completely
finished.. 99%):

?++ Spring and visibility problems

<h3>Abstract</h3>
Spring is a great framework, but I expect that some Spring based
applications are  subject to visibility problems (a type of
concurrency problem). This blogentry describes the cause and effects
of this problem, and also how it can be solved.

<h3>Visibility problems</h3>
Most programmers don't have much experience with multi threading and
have a very simplistic view on reality: if a thread makes a change to
a variable, this change directly is visible to all other threads that
access the same variable. This view is called <a
href="http://en.wikipedia.org/wiki/Sequential_consistency">sequential
consistency</a>, but the problem is that no virtual machine
automatically provides this view.

The reason for this apparently faulty behavior is performance. Most
variables are not shared between threads, so no special instructions
(memory barriers) have to be added to make the program sequentially
consistent. If these instructions were automatically added to all
variable accesses, it would greatly reduce performance because a lot
of optimizations would be excluded from being used. To name a few:
<ol>
    <li>
    	usage of super fast local-memory (like caches and cpu-registers).
It also would (greatly) reduce the performance benefits of multi-core
systems because access to main memory would be the bottleneck.
    </li>
    <li>	
    	reordering of instructions to increase the chance of a cache hit.
    </li>
</ol>

The consequence of a visibility problem, is that a value written to a
variable by one thread, doesn't have to be visible in other threads
(maybe never). This could lead to an easy to detect
NullPointerException, but it can also lead to much harder to detect
problems like shown in the following example:

<pre>
public class SomeRunnable implements Runnable{
    private boolean stop = false;

    public void stop(){
	stop = true;
    }

    public void run(){
    	while(!stop){
		System.out.println("hello");
	}
    }
}
</pre>

There are a few reasons why this runnable could fail to stop:
<ol>
	<li>
		stop is not safely published: the thread that executed the run
method, doesn't need to see a change in the stop value. A possible
scenario would be: thread1 (the thread that executed run) is running
on cpu1 (with cache1). Thread2 (the thread that calls stop) is running
on cpu2 (with cache2). When thread1 runs, it needs the value of stop,
sees the initial false value and places it in his cache. When thread2
executes stop, stop is set to true. But it might happen that this
value remains in cache2 for an undetermined time and isn't flushed to
main memory, so thread1 never sees the new value when it happens to
read from main memory. Even if the value is flushed to main memory,
thread1 could still be reading the stale value from cache1.
	</li>
	<li>
		because the access to the stop variable isn't safely published, and
the value doesn't change in the loop, the 'compiler' (JIT, cpu etc)
could decide to replace the variable read by the constant 'true' in
that loop.
	</li>
</ol>
Visibility problems can lead to hard to detect errors and
unpredictable behavior, and you definitely want to keep them out of
your system. And betting on the <a
href="http://en.wikipedia.org/wiki/Memory_coherence">Memory
Coherence</a> of a computer, is asking for trouble (the application
also wouldn't platform independant).

<h3>Spring applications and visibility problems</h3>
I expect that a lot of Spring applications are subject to visibility
problems. If you look at singleton beans for example (dao's,
controllers, managers etc), these beans often are shared by many
threads, to name a few:
<ul>
    <li>threads from the servlet container</li>
    <li>threads from remoting middleware</li>
    <li>threads from jmx</li>
    <li>threads from triggers or other internal threads</li>
</ul>

Take a look at the following example:
<pre>
public class EmployeeManagerImpl implements EmployeeManager{

    private EmployeeDao employeeDao;

    public setEmployeeDao(EmployeeDao employeeDao){
        this.employeeDao = employeeDao;
    }

    public void fire(long employeeId){
        Employee employee = employeeDao.load(employeeId);
        employee.fire();
    }
}
</pre>

And the bean configuration that belongs to it:
<pre>
    &lt;bean id=&quot;employeeManager&quot;
class=&quot;EmployeeManagerImpl&quot;&gt;
        &lt;property name=&quot;employeeDao&quot;
ref=&quot;employeeDao&quot;/&gt;
    &lt;/bean&gt;
</pre>

The visibility problem here, is that employeeDao, set by the thread
that constructed the employeeManager, isn't safely published and
doesn't have to be visible in other threads. If one of those threads
calls the fire method on the EmployeeManagerImpl, it could lead to a
NullPointerException because that thread still sees a null value for
the employeeDao.

The problem can be solved in a few ways:
<ol>
    <li>
        make employeeDao volatile. Although it sounds quite strange,
because the employeeDao is not going to change after it has been set,
volatile variables are always safely published. Personally I don't
like this solution much because it is misleading (the value is never
going to change after the object has been constructed) and it also
reduces performance (the variable always has to be read from main
memory instead of local memory).
    </li>
    <li>
        make employeeDao final, because final variables also are
safely published. The problem with final variables is that they only
can be set by constructor based Dependency Injection (DI) and Spring
promotes setter based DI. Personally I prefer constructor based DI,
because it is a lot easier to guarantee class invariants. Although
those setters only are visible in an implementing class, and not in
the interface, I still don't feel happy about because it also makes
classes harder to understand. But constructor based DI isn't perfect
either; I guess most of us have struggled with large constructors.
    </li>
</ol>

<h3>Problem not that bad?</h3>
Before going into detail why the problems maybe are not that bad, I
need to explain the new Memory Model found in Java 5 and higher (<a
href="http://jcp.org/aboutJava/communityprocess/review/jsr133/index.html">JSR-133</a>)
(pointer to footnote). The model describes under what kinds of
conditions a variable will be visible in other threads.
<p/>
The model is expressed in terms of actions:
<ol>
	<li>
		read/writes to normal/volatile/final variables
	</li>
	<li>
		lock acquire/release
	</li>
	<li>
		thread start/join
	</li>
</ol>
<p/>
The model also contains a set of happens-before rules between actions.
If a happens-before rule applies between action1 and action2, then all
changes made by action1 are visible in action2. Examples of
happens-before rules are:
<ol>
	<li>
		Program Order rule: Each action in a thread happens-before every
action in that thread that comes later in the program order. It is
important to realize that normal variable reads/writes, can be
reordered as long as they keep the 'within-thread as-if-serial
semantics semantics': the reordering should not be visible inside the
thread.
	</li>
	<li>
		Volatile variable rule. A write to a volatile variable
happens-before every subsequent read of that same variable (that is
why making the employeeDao volatile works).
	</li>
	<li>
		Monitor lock rule. An unlock on a monitor lock happens-before every
subsequent lock on that same monitor lock. (The same goes for the the
new <a href="http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/locks/Lock.html">Lock</a>
found in Java 5.
	</li>
	<li>
		Transitivity rule. If action1 happens-before action2, and action2
happens-before action3, then action1 happens-before action3.
	</li>
</ol>

<h3>Safe handoff</h3>
These elementary happens-before rules can be used to check if there is
a happens-before relation between two actions. Take a look at the
following example:
<pre>
int a=0;
volatile boolean b=0;

void initialize(){
	a=console.readInteger();		
	b=console.readBoolean();
}

void print(){
	print(b);
	print(a);
}
</pre>

A possible ordering of actions could be:
<table border="1">
	<tr><td>action</td><td>thread 1</td><td>thread 2</td></tr>	
	<tr><td>action1</td><td>normalwrite(a)</td><td></td></tr>
	<tr><td>action2</td><td>volatilewrite(b)</td><td></td></tr>
	<tr><td>action3</td><td></td><td>volatileread(b)</td></tr>
	<tr><td>action4</td><td></td><td>normalread(a)</td></tr>
</table>
Because action1 happens-before action2 (program order) and because
action2 happens-before action3 (monitor lock rule) and because action3
happens before action4 (program order), action1 happens before action4
(remember that the happens before rules are transitive). This means
that the change in action1, is visible in action4. This technique is
called 'safe handoff' (aka 'piggybacking on synchronization').
Safe handoff uses the safe publication of a variable X (in this case
b), to safely publish all unsafely-published-variables that are
changed before X (in this case a).

<h4>Safe handoff in Spring</h4>
The Spring applicationcontext also provides safe handoff (in Java 5
and higher): after a bean is created, it is placed in a synchronized
map. When it is needed, it is retrieved from that synchronized map.
But instead of using the 'volatile variable' rule, it uses the
'monitor lock' rule.

The following table shows a very simplified ordering of actions
(variable 'X' is mapentry.ref):
<table border="1">
	<tr><td>action</td><td>thread 1</td><td>thread 2</td></tr>	
	<tr><td></td><td><i>construction of the employeeManager</i></td><td></td></tr>	
	<tr><td>action1</td><td>normalwrite(employeeDao)</td><td></td></tr>	
	<tr><td></td><td><i>employeeManager is placed in the
applicationcontext</i></td><td></td></tr>
	<tr><td>action2</td><td>lock(applicationcontext)</td><td></td></tr>
	<tr><td>action3</td><td>normalwrite(mapentry.ref)</td><td></td></tr>
	<tr><td>action4</td><td>unlock(applicationcontext)</td><td></td></tr>
	<tr><td></td><td></td><td><i>employeeManager is read from the
applicationcontext</i></td></tr>
	<tr><td>action5</td><td></td><td>lock(applicationcontext)</td></tr>
	<tr><td>action6</td><td></td><td>normalread(mapentry.ref)</td></tr>
	<tr><td>action7</td><td></td><td>unlock(applicationcontext)</td></tr>
	<tr><td></td><td></td><td><i>employeeManager fire method is
called</i></td></tr>
	<tr><td>action8</td><td></td><td>normalread(employeeDao)</td></tr>
</table>
This means that there is a happens before relation between action1 and
action8, so the value set in the employeeDao (action1), is visible
when it is read by another thread (action8). That is why standard
Spring singletons don't have visibility problems.

<h3>Conclusion</h3>
I'm glad that standard singleton beans aren't subject to visibility
problems (under Java 5 and higher), but I don't think it is the
correct way to deal with concurrency control in Spring:
<ol>
	<li>
		objects can't be safely used in a different environment. So your
objects are tied to the Spring framework to make them work correct.
	</li>
	<li>
		the save handoff behavior is not guaranteed by Spring, and it only
works under Java 5 and higher. The behavior is undefined in older
JVM's.
	</li>
</ol>
It also gives a dangerous feeling of security. If objects are used in
a multi threaded environment, you <b>have</b> to make your objects
thread safe. You can't assume everything will work out, because
eventually it will come back to you. Especially with the increase in
number of local memories (introduced by multi core machines) and the
now clearly defined Memory Model, the chance of problems (caused by
unsafe code) will increase. Hoping that nothing falls over, while the
specs are clear (although not for the faint hearted), is in my opinion
not a professional way to develop software.

<p/>
Footnote:
If you want a more in depth explanation of the new Java Memory Model,
I suggest <a href="http://www.amazon.com/Java-Concurrency-Practice-Brian-Goetz/dp/0321349601">'Java
Concurrency in Practice'</a> from Brian Goets, Tim Peierls, Joshua
Bloch, Joseph Bowbeer, David Holmes and Doug Lea, or check out the
following website <a
href="http://www.cs.umd.edu/~pugh/java/memoryModel">The Java Memory
Model</a>
<p/>



On 2/21/07, Gregg Wonderly <gergg at cox.net> wrote:
> Peter Veentjer wrote:
> > Does someone have experience with dealing with this situation?
>
> On a single core systems, and prior to JDK 1.5 (and some 1.4) simple (incorrect)
> code such as
>
> public class MyThread implements Runnable {
>      private boolean done;
>      public void shutdown() {
>         done = true;
>      }
>
>      public void run() {
>          while( !done ) {
>              ...
>          }
>      }
> }
>
> will work just fine.  Now days, you will have visibility problems with changes
> in the value of "done".  I recently had a conversation with a research professor
> who said that his current mantra is that all class fields must either be "final"
> or "volatile".  If a field doesn't have one of those qualifiers on it, it will
> be problematic and the overhead of volatile is far less of a concern than
> incorrect code.
>
> There are probably a million or more ways to demonstrate visibility problems for
> the inexperienced to learn from.  Maybe someone can create a nice set of
> graphical demos that show the problem literally.
>
> Gregg Wonderly
>


From matthias.ernst at coremedia.com  Wed Feb 21 12:41:17 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Wed, 21 Feb 2007 18:41:17 +0100
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com><45DC77BF.6090704@cox.net>
	<1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D61B756@hermes.coremedia.com>

> The visibility problem here, is that employeeDao, set by the thread
> that constructed the employeeManager, isn't safely published and
> doesn't have to be visible in other threads. 

Peter,

access to singleton beans is correctly synchronized by the spring
application context - construction and publication happen under the lock
of the app context. After that the fields are effectively final.

Additionally, by default, the instantiation of the singletons already
happens during the construction of the spring application context which
in turn is loaded at web application initialization. Only after that do
other threads get to access the application context. I do expect that
the servlet container safely publishes the web application from the
initializer thread to request handler threads. 

I've had a long discussion about that here:
http://weblogs.java.net/blog/tomwhite/archive/2006/09/are_your_beans_1.h
tml


> The problem can be solved in a few ways:
> make employeeDao volatile. 

Even if it were a problem, I do not particularly like that
recommendation. I'd not try to solve the problem at this level. This is
too low level, there are too many object associations to protect, you'll
lose oversight and if you add volatile everywhere, you're back to
sequential consistency. I would rather make sure that the whole object
graph is effectively immutable and safely published at the root. This
makes the happens-before-relationship dead simple: object graph creation
happens-entirely-before access. Selectively apply "visibility
engineering" only where that is impossible.

I've once tried to express that sentiment here:
http://mernst.org/blog/rss.xml#volatile-is-the-new-synchronized

Matthias

-- 
matthias.ernst at coremedia.com
software architect
+49.40.32 55 87.503


From matthias.ernst at coremedia.com  Wed Feb 21 12:55:21 2007
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Wed, 21 Feb 2007 18:55:21 +0100
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com><45DC77BF.6090704@cox.net>
	<1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
Message-ID: <AE2A8E488D9B26438919DF3C9C95528D61B75B@hermes.coremedia.com>

> The Spring applicationcontext also provides safe handoff (in Java 5
> and higher): after a bean is created, it is placed in a synchronized
> map. When it is needed, it is retrieved from that synchronized map.

I'm sorry, I didn't read till the end. At any rate: Spring's use of
synchronized makes it also safe in 1.4.

> If objects are used in a multi threaded environment, you <b>have</b>
> to make your objects thread safe.

I do not agree here. I have to document how to use them in a thread-safe
manner. With setter based injection that means to me: "Do not call
setters after construction/publication except if explicitly allowed." 

Matthias

-- 
matthias.ernst at coremedia.com
software architect
+49.40.32 55 87.503


From gregg at cytetech.com  Wed Feb 21 13:44:04 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 21 Feb 2007 12:44:04 -0600
Subject: [concurrency-interest] dealing with people that
 questionvisibility problems
In-Reply-To: <AE2A8E488D9B26438919DF3C9C95528D61B75B@hermes.coremedia.com>
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com><45DC77BF.6090704@cox.net>
	<1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
	<AE2A8E488D9B26438919DF3C9C95528D61B75B@hermes.coremedia.com>
Message-ID: <45DC92F4.6070809@cytetech.com>



Ernst, Matthias wrote:
>>If objects are used in a multi threaded environment, you <b>have</b>
>>to make your objects thread safe.
> 
> I do not agree here. I have to document how to use them in a thread-safe
> manner. With setter based injection that means to me: "Do not call
> setters after construction/publication except if explicitly allowed." 

This is the "knowledge" that is problematic.  Demanding a particular usage 
sequence that is not controlled by the system is where we have created the 
biggest problems for concurrent programming.

This is an important issue that the current hardware implementations and 
associated software interfaces have not provided suffient tools for the 
developer to manage the issues effectively.  There is no way to demonstrate 
correct software.  There are only tools that find some of the potential 
incorrect software.

Gregg Wonderly

From dcholmes at optusnet.com.au  Wed Feb 21 18:50:48 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 Feb 2007 09:50:48 +1000
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>

Peter,

> public class SomeRunnable implements Runnable{
>     private boolean stop = false;
> 
>     public void stop(){
> 	stop = true;
>     }
> 
>     public void run(){
>     	while(!stop){
> 		System.out.println("hello");
> 	}
>     }
> }
> There are a few reasons why this runnable could fail to stop:
> ... A possible
> scenario would be: thread1 (the thread that executed run) is running
> on cpu1 (with cache1). Thread2 (the thread that calls stop) is running
> on cpu2 (with cache2). When thread1 runs, it needs the value of stop,
> sees the initial false value and places it in his cache. When thread2
> executes stop, stop is set to true. But it might happen that this
> value remains in cache2 for an undetermined time and isn't flushed to
> main memory, so thread1 never sees the new value when it happens to
> read from main memory. Even if the value is flushed to main memory,
> thread1 could still be reading the stale value from cache1.
> 	</li>

As I've written before using caches as a motivation is a bad idea. While this is theoretically possible given an abstract caching mechanism, in practice cache coherency on modern SMPs won't allow the above to happen. It might have been possible on Alpha's. When people who understand hardware caching realize the example can not happen they extrapolate that to imply visibility is not an issue they have to worry about.

> 	<li>
> 		because the access to the stop variable isn't 
> safely published, and
> the value doesn't change in the loop, the 'compiler' (JIT, cpu etc)
> could decide to replace the variable read by the constant 'true' in
> that loop.
> 	</li>

Yes. This can and will happen. Hoptspot server compiler will effectively rewrite the loop as:
   if (!stop)
      while(true) ...


As for the general argument "if it ain't broke don't fit it" - well it *IS* broke. Just because the bug hasn't manifested itself yet doesn't mean it never will.

Howver, going to the extreme of making every field either final or volatile is pointless. If a field has visibility issues because it isn't volatile, there's a good chance that it also has atomicity issues that volatile can't fix.

Cheers,
David Holmes



From dcholmes at optusnet.com.au  Wed Feb 21 19:19:35 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 Feb 2007 10:19:35 +1000
Subject: [concurrency-interest] dealing with people
	thatquestionvisibility problems
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEIGHFAA.dcholmes@optusnet.com.au>

I wrote:
> Howver, going to the extreme of making every field either final
> or volatile is pointless. If a field has visibility issues
> because it isn't volatile, there's a good chance that it also has
> atomicity issues that volatile can't fix.

And I'll add that this just demonstrates that the programmer doesn't
understand the code they are writing, or how/when variables get shared. That
to me is the biggest problem with concurrency - people don't understand the
sharing that exists in their program.

Rule #1 for sharing mutable objects: Don't do it unless you have to.

What concurrent programming languages lack is a clear model for sharing,
where sharing is explicit. Some have tried variations of it (eg the
Eiffel/SCOOP 'seperate' notion) but it does get complex - requires escape
analysis etc. Ownership types cover some of the same space.

David Holmes


From jseigh_cp00 at xemaps.com  Wed Feb 21 20:56:44 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Wed, 21 Feb 2007 20:56:44 -0500
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <200702211307.l1LD7bEb001400@cs.oswego.edu>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>
Message-ID: <45DCF85C.5080808@xemaps.com>

David J. Biesack wrote:

>
>Does anyone know more about Cliff Click's wait-free hashtable?
>
>http://cs.stanford.edu/calendar/abstract.php?eventId=2270
>
>"A Faster Wait-Free Hash Table"
>
>"I present a wait-free (lock-free) concurrent Hash Table implementation with better single-thread performance than most Hash Tables, and better (usual much better) multi-thread performance than all other implementations I tried. I demonstrate scaling up to 768 CPUs even with high mutation rates. I show correctness by looking at the problem in a very different light than the usual "happens-before" / memory-order / fencing style of thinking -- indeed the algorithm requires no memory-ordering, no memory fencing, no D-CAS and no CAS-spin-loops."
>
>Sounds interesting. Has anyone seen or reviewed the design or implementation, or comparisons to ConcurrentHashMap?
>

No.  I guess we'll have to wait for the wait-free hash table. :)

I sort of have a blocking linked list which runs about 3X faster than 
LinkedBlockingQueue
under high contention on a single processor system.  I am waiting for a 
768 processor
system to test how well it will really scale.

--
Joe Seigh

From gregg at cytetech.com  Wed Feb 21 21:01:40 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 21 Feb 2007 20:01:40 -0600
Subject: [concurrency-interest] dealing with people that
 questionvisibility problems
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>
Message-ID: <45DCF984.5070008@cytetech.com>



David Holmes wrote:
> Howver, going to the extreme of making every field either final or
> volatile is pointless. If a field has visibility issues because it
> isn't volatile, there's a good chance that it also has atomicity 
> issues that volatile can't fix.

The issue that I see is that it is more possible to study code for a race 
condition when you see the effects.  These are more readily understood.  It is 
difficult to understand visibility related bugs and this particular optimization 
because it just doesn't seem right that a variable could possibly not have the 
value assigned.

So, declaring a variable volatile can create a more tractible behavior than will 
be present when the compiler is optimizing the code to not behave as it is written.

Gregg Wonderly

From dcholmes at optusnet.com.au  Wed Feb 21 21:27:44 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 Feb 2007 12:27:44 +1000
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <45DCF984.5070008@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>

Gregg Wonderly wrote:
> The issue that I see is that it is more possible to study code for a race
> condition when you see the effects.  These are more readily
> understood.  It is difficult to understand visibility related bugs

It is difficult to uncover the fact that you have a visiblity related bug.
But studying the code if you find a shared mutable variable that is not
volatile and not accessed under synchronization, then you potentially have a
visibility related bug.

Making all variables volatile fixes only pure-visibility bugs and in the
process will totally kill performance by disallowing a slew of
optimizations. My contention is that it is more likely that there is an
atomicity bug as well as a visibility bug, and that volatile won't help fix
it. In fact volatile could further obscure the bug by making it even less
likely to appear in practice, such as during testing. Effective testing of
concurrent programs benefits from the least allowed order, as this makes it
easier to expose races.

Cheers,
David


From tim at peierls.net  Wed Feb 21 22:56:23 2007
From: tim at peierls.net (Tim Peierls)
Date: Wed, 21 Feb 2007 22:56:23 -0500
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
Message-ID: <63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>

On 2/21/07, David Holmes <dcholmes at optusnet.com.au> wrote:
>
> Making all variables volatile fixes only pure-visibility bugs and in the
> process will totally kill performance by disallowing a slew of
> optimizations.


This is less likely to be a concern in setter-injected fields of beans
managed by a container framework.


My contention is that it is more likely that there is an atomicity bug as
> well as a visibility bug, and that volatile won't help fix it.


Again, not as likely in setter-injected fields of container-managed beans,
where the wiring together of multiple fields is usually independent.

I think the hard-core concurrency view in this case does a disservice to
application programmers, who will be unduly swayed by phrases like "will
totally kill performance by disallowing a slew of optimizations". They will
rush to remove the volatile keyword and they will hear the Spring expert
say, "Trust ApplicationContext -- it does the right thing," and then they
won't bother thinking about the possibility that their bean might someday be
deployed in a framework that doesn't provide the requisite guarantees.

I'm not saying that it is right to blindly make every field volatile and
claim correctness. I'm saying if you have a setter-injected field of a
container-managed bean, it's unlikely to be significant performance problem
to make that field volatile (or to synchronize access to that field) -- but
the cost of failing to guard it in some way is potentially huge.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070221/49e005e1/attachment-0001.html 

From gregg at cytetech.com  Wed Feb 21 23:39:21 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 21 Feb 2007 22:39:21 -0600
Subject: [concurrency-interest] dealing with people that
 questionvisibility problems
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
Message-ID: <45DD1E79.5010905@cytetech.com>



David Holmes wrote:
> Gregg Wonderly wrote:
> 
>>The issue that I see is that it is more possible to study code for a race
>>condition when you see the effects.  These are more readily
>>understood.  It is difficult to understand visibility related bugs
> 
> It is difficult to uncover the fact that you have a visiblity related bug.
> But studying the code if you find a shared mutable variable that is not
> volatile and not accessed under synchronization, then you potentially have a
> visibility related bug.

I am not trying to dispute the facts David.  It is true that observation of "the 
code" can reveal to you that visibility is in question.  The observed bug will 
be lack of loop termination, in this case.  I contend that this is a red-herring 
which will potentially focus debugging outside of this code and to the callers 
of the setter and the associated flow control into the setter.

> Making all variables volatile fixes only pure-visibility bugs and in the
> process will totally kill performance by disallowing a slew of
> optimizations. 

This particular pattern is not a CPU bound pattern.  But, there are clearly 
others that will potentially suffer from recurring volatile references.

 > My contention is that it is more likely that there is an
> atomicity bug as well as a visibility bug, and that volatile won't help fix
>  it.

I've been greeted with numerous bugs to solve related to visibilty.  When I got 
a multi-core laptop to do my development on, old dependable software suddenly 
collapsed in a pile of bugs.  I don't recall ever having any atomicity bugs. 
I've always used Hashtable and Vector for containers.  The atomicity thing I've 
always had a grip on.

The visibility thing is something that I hadn't had any bad experiences with to 
help me understand what I should have been doing with synchronizing access to 
manage visibility (prior to volatile working).

 > In fact volatile could further obscure the bug by making it even less
> likely to appear in practice, such as during testing. Effective testing of
> concurrent programs benefits from the least allowed order, as this makes it
> easier to expose races.

I agree with this.  The more optimizations the compiler can make, the more 
ordering/visibility related issues you'll actually see.  This example is 
something that is a very common pattern due to "Thread.stop()" not being 
supported.  I think it's an important pattern to document as a "visibilty is 
important" example.

Gregg Wonderly

From dcholmes at optusnet.com.au  Wed Feb 21 23:46:36 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 Feb 2007 14:46:36 +1000
Subject: [concurrency-interest] dealing with people
	thatquestionvisibility problems
In-Reply-To: <63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEIJHFAA.dcholmes@optusnet.com.au>

Tim,

My comments were in response to the comment re the research professor making
all fields final/volatile - and that was not in the context of a framework
like Spring. Or if it was then it wasn't clear that was the context.

The fact this might be necessary with some frameworks and not others simply
reflects the sorry state of some frameworks with regard to their
thread-safety (for want of a better term) properties.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Tim Peierls
  Sent: Thursday, 22 February 2007 1:56 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] dealing with people
thatquestionvisibility problems


  On 2/21/07, David Holmes <dcholmes at optusnet.com.au> wrote:
    Making all variables volatile fixes only pure-visibility bugs and in the
process will totally kill performance by disallowing a slew of
optimizations.

  This is less likely to be a concern in setter-injected fields of beans
managed by a container framework.




    My contention is that it is more likely that there is an atomicity bug
as well as a visibility bug, and that volatile won't help fix it.

  Again, not as likely in setter-injected fields of container-managed beans,
where the wiring together of multiple fields is usually independent.

  I think the hard-core concurrency view in this case does a disservice to
application programmers, who will be unduly swayed by phrases like "will
totally kill performance by disallowing a slew of optimizations". They will
rush to remove the volatile keyword and they will hear the Spring expert
say, "Trust ApplicationContext -- it does the right thing," and then they
won't bother thinking about the possibility that their bean might someday be
deployed in a framework that doesn't provide the requisite guarantees.

  I'm not saying that it is right to blindly make every field volatile and
claim correctness. I'm saying if you have a setter-injected field of a
container-managed bean, it's unlikely to be significant performance problem
to make that field volatile (or to synchronize access to that field) -- but
the cost of failing to guard it in some way is potentially huge.


  --tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070222/eeec7e6d/attachment.html 

From dcholmes at optusnet.com.au  Thu Feb 22 00:08:19 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 Feb 2007 15:08:19 +1000
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <45DD1E79.5010905@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEIJHFAA.dcholmes@optusnet.com.au>

Gregg,

> I've been greeted with numerous bugs to solve related to
> visibilty.  ... I don't recall ever having any
> atomicity bugs.

You're right. Considering legacy code, the biggest visibility mistake people
make is having a non-synchronized accessor but all the mutators are
correctly synchronized. Hence atomicity is preserved but suddenly you get
stale reads that lead to unexpected behaviour.

David


From joe.bowbeer at gmail.com  Thu Feb 22 00:52:03 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 21 Feb 2007 21:52:03 -0800
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
	<63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
Message-ID: <31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>

A question and a comment.

Q: Can we agree on whether "setter-injected fields of
container-managed beans" (in Spring?) do or do not need additional
synchronization?  I remember analyzing this before and delving through
the Spring sources and I thought I'd decided that no additional
synchronization was needed.  I'd like us to agree on this before
deciding on a remedy.

C: As for volatile, if I were a professor I would be very tempted to
subtract points for their use.  (Remember the days when goto existed
but no one was allowed to use it?)  Even though there are a few very
good uses for volatiles (and gotos as well), they can always be
avoided and I think concurrent programming in Java is best learned
without them, as least initially.

While David's concerns are about the performance of extraneous
volatiles, my concerns are almost entirely about readability and
maintainability.  Undocumented volatiles are a big stumbling block
when I'm trying to understand someone else's code.  That one keyword
can mean all sorts of things.  Extraneous volatiles are very hard to
remove with confidence, and make the code unnecessarily hard to
maintain.

In my view, adorning code with volatile just-in-case there might be a
visibility problem is worse than doing nothing.

--Joe

On 2/21/07, Tim Peierls <tim at peierls.net> wrote:
> On 2/21/07, David Holmes <dcholmes at optusnet.com.au> wrote:
> > Making all variables volatile fixes only pure-visibility bugs and in the
> process will totally kill performance by disallowing a slew of
> optimizations.
>
> This is less likely to be a concern in setter-injected fields of beans
> managed by a container framework.
>
>
> > My contention is that it is more likely that there is an atomicity bug as
> well as a visibility bug, and that volatile won't help fix it.
>
> Again, not as likely in setter-injected fields of container-managed beans,
> where the wiring together of multiple fields is usually independent.
>
> I think the hard-core concurrency view in this case does a disservice to
> application programmers, who will be unduly swayed by phrases like "will
> totally kill performance by disallowing a slew of optimizations". They will
> rush to remove the volatile keyword and they will hear the Spring expert
> say, "Trust ApplicationContext -- it does the right thing," and then they
> won't bother thinking about the possibility that their bean might someday be
> deployed in a framework that doesn't provide the requisite guarantees.
>
> I'm not saying that it is right to blindly make every field volatile and
> claim correctness. I'm saying if you have a setter-injected field of a
> container-managed bean, it's unlikely to be significant performance problem
> to make that field volatile (or to synchronize access to that field) -- but
> the cost of failing to guard it in some way is potentially huge.
>
> --tim
>

From kimo at webnetic.net  Thu Feb 22 01:38:59 2007
From: kimo at webnetic.net (Kimo Crossman)
Date: Wed, 21 Feb 2007 22:38:59 -0800
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <45DCF85C.5080808@xemaps.com>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>
	<45DCF85C.5080808@xemaps.com>
Message-ID: <058f01c7564c$242e5d30$28681948@DesktopSystem>

 
Go here for slides of the talk.

http://www.stanford.edu/class/ee380/Abstracts/070221.html

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joseph Seigh
Sent: 2007 February 21 17:57
To: concurrency-interest
Subject: Re: [concurrency-interest] A Fast Wait-Free HashTable

David J. Biesack wrote:

>
>Does anyone know more about Cliff Click's wait-free hashtable?
>
>http://cs.stanford.edu/calendar/abstract.php?eventId=2270
>
>"A Faster Wait-Free Hash Table"
>
>"I present a wait-free (lock-free) concurrent Hash Table implementation with better single-thread performance than most Hash Tables, and better (usual much better) multi-thread performance than all other implementations I tried. I demonstrate scaling up to 768 CPUs even with high mutation rates. I show correctness by looking at the problem in a very different light than the usual "happens-before" / memory-order / fencing style of thinking -- indeed the algorithm requires no memory-ordering, no memory fencing, no D-CAS and no CAS-spin-loops."
>
>Sounds interesting. Has anyone seen or reviewed the design or implementation, or comparisons to ConcurrentHashMap?
>

No.  I guess we'll have to wait for the wait-free hash table. :)

I sort of have a blocking linked list which runs about 3X faster than LinkedBlockingQueue under high contention on a single processor system.  I am waiting for a
768 processor
system to test how well it will really scale.

--
Joe Seigh
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From alarmnummer at gmail.com  Thu Feb 22 03:30:25 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 22 Feb 2007 09:30:25 +0100
Subject: [concurrency-interest] Fwd: dealing with people that
	questionvisibility problems
In-Reply-To: <1466c1d60702220030n7f2dcc9dr72422ee6b97fe2d7@mail.gmail.com>
References: <1466c1d60702210728i60d1613aj8484017c99784f10@mail.gmail.com>
	<45DC77BF.6090704@cox.net>
	<1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
	<AE2A8E488D9B26438919DF3C9C95528D61B75B@hermes.coremedia.com>
	<1466c1d60702220030n7f2dcc9dr72422ee6b97fe2d7@mail.gmail.com>
Message-ID: <1466c1d60702220030r1d4859c4q9fc38fa8ecd97a72@mail.gmail.com>

Hi Matthias,

thank you for your reply.

>>  At any rate: Spring's use of synchronized makes it also safe in 1.4.

Safe handoff is a feature that was added to Java 5. The behavior was
undefined in Java 1.4. Most cpu's have a strong memory coherence, so
changes will be visible, but no guarantees are made.

>
> > If objects are used in a multi threaded environment, you <b>have</b>
> > to make your objects thread safe.
>
> I do not agree here. I have to document how to use them in a thread-safe
> manner. With setter based injection that means to me: "Do not call
> setters after construction/publication except if explicitly allowed."

If you document that your object depends on being used in an
environment that provides safe handoff, you are doing concurrency
control :) You are controlling how your object should be used in a
multi-threaded environment.

> Matthias
>
> --
> matthias.ernst at coremedia.com
> software architect
> +49.40.32 55 87.503
>

From alarmnummer at gmail.com  Thu Feb 22 03:37:50 2007
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 22 Feb 2007 09:37:50 +0100
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>
References: <1466c1d60702210850m34b58777n49a6af0b934caf82@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEIGHFAA.dcholmes@optusnet.com.au>
Message-ID: <1466c1d60702220037j1fd4eaesaaf86a9121baad05@mail.gmail.com>

Hi David,

>>  As I've written before using caches as a motivation
>> is a bad idea. While this is >>theoretically possible
>> given an abstract caching mechanism, in practice
>> cache coherency on modern SMPs won't allow the
>> above to happen.

True. But I needed an example. If you know a better
one, please tell me.

>> Howver, going to the extreme of making every
>> field either final or volatile is pointless. If a field
>> has visibility issues because it isn't volatile,
>>  there's a good chance that it also has atomicity
>>  issues that volatile can't fix.

I agree. In most cases object are not shared
between threads, or are 'really' immutable.
But for those object that are being shared
and are mutable, I suggest doing one of
2 things:
1) make the fields volatile
2) depend on save handoff

In a lot of producer/consumer problems
I rely on number 2 because only a single
thread 'owns' the object at any given moment.

Thank you for your feedback.

From gregg at cytetech.com  Thu Feb 22 09:16:21 2007
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 22 Feb 2007 08:16:21 -0600
Subject: [concurrency-interest] dealing with people that
 questionvisibility problems
In-Reply-To: <31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
	<63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
	<31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
Message-ID: <45DDA5B5.9080505@cytetech.com>



Joe Bowbeer wrote:
> A question and a comment.
> 
> Q: Can we agree on whether "setter-injected fields of
> container-managed beans" (in Spring?) do or do not need additional
> synchronization?  I remember analyzing this before and delving through
> the Spring sources and I thought I'd decided that no additional
> synchronization was needed.  I'd like us to agree on this before
> deciding on a remedy.

I think that it's important to realize how much legacy code was written prior to 
the existance of such frameworks.  There will also be new code written without 
such frameworks.  It's that programming in the RAW that is creating the greatest 
potential impact.

> C: As for volatile, if I were a professor I would be very tempted to
> subtract points for their use.  (Remember the days when goto existed
> but no one was allowed to use it?)  Even though there are a few very
> good uses for volatiles (and gotos as well), they can always be
> avoided and I think concurrent programming in Java is best learned
> without them, as least initially.

The professor is involved in producing research conducting software systems.  He 
is trying to maintain some amount of coding standard that creates more 
predictable results I suppose.  It's just interesting to me that this is the 
perspective that is comming out of peoples experiences.  I'm not at all 
condoning the rampant use of volatile.  I'm just trying to stress that not using 
volatile creates much less desireable results in the presence of visibility 
related software misdesign.

> While David's concerns are about the performance of extraneous
> volatiles, my concerns are almost entirely about readability and
> maintainability.  Undocumented volatiles are a big stumbling block
> when I'm trying to understand someone else's code.  That one keyword
> can mean all sorts of things.  Extraneous volatiles are very hard to
> remove with confidence, and make the code unnecessarily hard to
> maintain.

I think this is a valid point.  But in the presense of visiblity related bugs, 
it's an argument of "does it work now" vs "will we have problems maintaining 
it".  Making it work first, is usually the driving force.  Only a limited number 
of people (from my experience) actually have a desire for perfection over function.

> In my view, adorning code with volatile just-in-case there might be a
> visibility problem is worse than doing nothing.

Well, the issue is that as a casual step in debugging a software misbehavior, 
adding volatile will almost always change the behavior of a visibility related 
problem.  If there is no atomicity issue, it will "fix" the problem.

Developers usually prefer "adding code" to fix a problem to "changing code" to 
fix a problem because it has, in general, always been less risky to just tack 
something on rather than trying to understand the code and refactor it.

Gregg Wonderly

From tim at peierls.net  Thu Feb 22 09:32:07 2007
From: tim at peierls.net (Tim Peierls)
Date: Thu, 22 Feb 2007 09:32:07 -0500
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
	<63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
	<31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
Message-ID: <63b4e4050702220632h17c3ef67ye2986be63705b594@mail.gmail.com>

On 2/22/07, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>
> Q: Can we agree on whether "setter-injected fields of
> container-managed beans" (in Spring?) do or do not need additional
> synchronization?  I remember analyzing this before and delving through
> the Spring sources and I thought I'd decided that no additional
> synchronization was needed.  I'd like us to agree on this before
> deciding on a remedy.


I think it is extremely likely that the Spring ApplicationContext as it
stands does provide the requisite synchronization for singleton beans. There
are several problems:

1. There is no centralized documentation in the Spring source of the
synchronization policy used to obtain the visibility guarantees, so future
well-meaning developers would have no warning against, say, changing the
bean registration machinery to use a lock-free map.

2. There are other types of scope for beans than singleton, and I haven't
been able to convince myself that ApplicationContext provides visibility
guarantees for beans in all of these other scopes. Should I have to mark my
beans with "only deploy as singleton"?

3. There are other implementations of the BeanFactory interface that aren't
ApplicationContexts. Should I have to mark my beans with "only deploy in
ApplicationContext"?

4. There are other container frameworks than Spring. Should I have to mark
my beans with "works in Spring, might not work in other frameworks"?

I've making these arguments for months, and by now it probably sounds as
though I have something against Spring. For the record: I love Spring, I use
Spring, and I wholeheartedly recommend it.  I have not been pleased,
however, with the reaction of some Spring developers to my concern, which
has been to deny that there might be problem and to posture ("tens of
thousands of developers unhesitatingly trust Spring, so why can't you?").



C: As for volatile, if I were a professor I would be very tempted to
> subtract points for their use.  (Remember the days when goto existed
> but no one was allowed to use it?)  Even though there are a few very
> good uses for volatiles (and gotos as well), they can always be
> avoided and I think concurrent programming in Java is best learned
> without them, as least initially.


I would agree completely if it weren't for the way volatile is such a good
match for the setter-injection problem (under the assumption that there is a
problem). Setter-injected fields are usually independent of each other, so
atomicity is less likely to be a concern.

However, Joe reminded me privately that there has been some discussion of
using annotations to mark "wannabe final" fields that can't be initialized
in the constructor but are effectively final once the bean is published in a
particular way, e.g., via a Spring ApplicationContext. I'm hoping those who
took part in that discussion will chime in here.

Using an annotation would make it clear that the setter should not be called
after publication, and it would allow an annotation processor to insert
appropriate synchronization if necessary. (If you think that's an abuse of
annotation processors, then just use the annotation as a documentation aid
and insert the synchronization manually.)


While David's concerns are about the performance of extraneous
> volatiles, my concerns are almost entirely about readability and
> maintainability.  Undocumented volatiles are a big stumbling block
> when I'm trying to understand someone else's code.  That one keyword
> can mean all sorts of things.  Extraneous volatiles are very hard to
> remove with confidence, and make the code unnecessarily hard to
> maintain.


I'm not saying don't document it. But if you declare globally that volatile
-- unless marked otherwise -- means "setter-injected read-only" (in the
absence of a standard annotation for this), that should make the code easy
to understand.

I'm not trying to encourage the use of volatile in any other context.


In my view, adorning code with volatile just-in-case there might be a
> visibility problem is worse than doing nothing.
>

I disagree. Make your code correct and then worry about performance.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070222/27136ff9/attachment.html 

From hanson.char at gmail.com  Thu Feb 22 13:12:49 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 22 Feb 2007 10:12:49 -0800
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <63b4e4050702220632h17c3ef67ye2986be63705b594@mail.gmail.com>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
	<63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
	<31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
	<63b4e4050702220632h17c3ef67ye2986be63705b594@mail.gmail.com>
Message-ID: <ca53c8f80702221012q1de69689w961577b6ddcf8f16@mail.gmail.com>

>
> I disagree. Make your code correct and then worry about performance.
>

If that's the case wouldn't it be nice if volatile was the default behavior,
and (an imaginary) "nonvolatile" be the keyword that needs to be explicitely
specified ?

:)
Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070222/b0cd6d06/attachment.html 

From tim at peierls.net  Thu Feb 22 14:29:04 2007
From: tim at peierls.net (Tim Peierls)
Date: Thu, 22 Feb 2007 14:29:04 -0500
Subject: [concurrency-interest] dealing with people that
	questionvisibility problems
In-Reply-To: <ca53c8f80702221012q1de69689w961577b6ddcf8f16@mail.gmail.com>
References: <45DCF984.5070008@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCAEIIHFAA.dcholmes@optusnet.com.au>
	<63b4e4050702211956n2614330j814102f299f50354@mail.gmail.com>
	<31f2a7bd0702212152g6299d3d3q63604646946490d9@mail.gmail.com>
	<63b4e4050702220632h17c3ef67ye2986be63705b594@mail.gmail.com>
	<ca53c8f80702221012q1de69689w961577b6ddcf8f16@mail.gmail.com>
Message-ID: <63b4e4050702221129s8d24da0r8e88516de128fe65@mail.gmail.com>

On 2/22/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> I disagree. Make your code correct and then worry about performance.
> >
>
> If that's the case wouldn't it be nice if volatile was the default
> behavior, and (an imaginary) "nonvolatile" be the keyword that needs to be
> explicitely specified ?
>
> :)
>

Given the smiley, you're probably not asking this seriously, but my serious
answer is no, it would not be nice. Class invariants in general involve
multiple fields, so defaulting to volatile would be insufficient in the
general case.

My (grudging) use of volatile is for independent setter-injected fields. I
would prefer to have a standard annotation to mark "effectively final"
("read-only-after-publication"?) fields, but until there is consensus, I'm
using plain volatile (plus a global note saying what unadorned volatile
field with setter means in my beans).

(For those of you who play bridge, using volatile in this way is like using
an artificial conventional bid that coincidentally happens to be a natural
bid.)

Not to use any kind of synchronization/volatile at all when there is even
the slightest possibility of a visibility failure seems like a terrible idea
to me. I'd be interested in hearing why so many people seem think it is
justified in the case of setter-injection, given the arguments I listed in
an earlier message.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070222/c9b6280f/attachment.html 

From joe.bowbeer at gmail.com  Thu Feb 22 17:13:45 2007
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 22 Feb 2007 14:13:45 -0800
Subject: [concurrency-interest] dealing with people that question
	visibility problems
Message-ID: <31f2a7bd0702221413v5a88c639ve6f650946c3f7727@mail.gmail.com>

Observing all the effort that goes into implementing and documenting
thread-safe single-assign fields in Java, I definitely 'get' the
motivation behind the explicit single-assign variables of declarative
concurrency. As for whether to add 'volatile' or not, I like to
aggressively refactor and eliminate unneeded code. Uncertainty leads
to rot. If I'm certain a volatile isn't needed, I remove it, and then
my motto is "Show me the bug". (Bugs are a great learning opportunity;
if something's going to fail then it should fail fast and often.) If
I'm not certain a volatile is needed then resolving that uncertainty
is my primary concern.

On 2/22/07, Tim Peierls <tim at peierls.net> wrote:
> On 2/22/07, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > I disagree. Make your code correct and then worry about performance.
> > >
> >
> > If that's the case wouldn't it be nice if volatile was the default
> > behavior, and (an imaginary) "nonvolatile" be the keyword that needs to be
> > explicitely specified ?
> >
> > :)
> >
>
> Given the smiley, you're probably not asking this seriously, but my serious
> answer is no, it would not be nice. Class invariants in general involve
> multiple fields, so defaulting to volatile would be insufficient in the
> general case.
>
> My (grudging) use of volatile is for independent setter-injected fields. I
> would prefer to have a standard annotation to mark "effectively final"
> ("read-only-after-publication"?) fields, but until there is consensus, I'm
> using plain volatile (plus a global note saying what unadorned volatile
> field with setter means in my beans).
>
> (For those of you who play bridge, using volatile in this way is like using
> an artificial conventional bid that coincidentally happens to be a natural
> bid.)
>
> Not to use any kind of synchronization/volatile at all when there is even
> the slightest possibility of a visibility failure seems like a terrible idea
> to me. I'd be interested in hearing why so many people seem think it is
> justified in the case of setter-injection, given the arguments I listed in
> an earlier message.
>
> --tim
>

From dcholmes at optusnet.com.au  Thu Feb 22 18:12:03 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 23 Feb 2007 09:12:03 +1000
Subject: [concurrency-interest] dealing with people
	thatquestionvisibility problems
In-Reply-To: <BDA38860DCFD334EAEA905E44EE8E7EF7BAB5C@G3W0067.americas.hpqcorp.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEINHFAA.dcholmes@optusnet.com.au>

Hans Boehm wrote:
> You get atomicity only if the fields never acquire any intermediate
> values, and particularly never if the fields have type long or double.

That is true. But in the current context that expectation had to already
exist ie they used an unsynchronized accessor because the value would always
be valid and so didn't need to be "synchronized". They just neglected the
visibility aspect.

Cheers,
David


From peter.kovacs.1.0rc at gmail.com  Fri Feb 23 17:33:58 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Fri, 23 Feb 2007 23:33:58 +0100
Subject: [concurrency-interest] Splitting long running jobs into short ones
	(Task-type affinity in thread pools?)
Message-ID: <b6e8f2e80702231433l69e03c0n5af436a5c8a1a08c@mail.gmail.com>

Hi,

I have a potentially long running iterative operation which itself
takes its input by iterating on a producer (the producer and the
operation in question [the consumer] implement some kind of iterator
interface). Each iteration on the producer provides an input which is
independent of the inputs provided in previous and subsequent
iterations -- so processing of inputs is also independent.

My initial approach have been to split the long running consumer
operation into short tasks where inputs from each iteration are
processed in a Callable executed by a thread taken from a thread pool.
What makes this difficult to implement is that processing of the
independent inputs requires an object which is very expensive to
construct and this object has to be dedicated to the short-running
Callable for the duration of Callable.call. (An obvious source of lock
contention.)

The appeal of splitting the long running consumer operation into
short-running tasks is that execution can then be tuned through tuning
the thread pool(s) to which the short-running tasks are assigned. The
alternative approach of starting a number of long running threads with
their dedicated "heavy-to-construct" objects requires the introduction
of another mechanism for determining the number of threads to start.

Is there a way to deal efficiently with heavy-to-construct object in
the split-into-short-running task scenario? (pooling? see PS)

What would be a good mechanism for long running operations
(essentially a way to tune the number of threads dedicated to the
operations)?

Thanks
Peter

PS:
My initial approach to pooling the heavy object have been to use a
ConcurrentHashMap as the pool from which the short-running tasks
retrieve the heavy-to-construct object using Thread.currentThread() as
the key (the object is created, if not yet mapped to the thread).

While this approach is not too bad with small fixed thread pools, it
doesn't scale up very well. With a potentially large number of other
indepent operations/tasks in the same thread management system and
different task types randomly assigned to threads, there will be an
increasing tendency toward constructing much more of the "heavy
objects" required for this particular operation than will be actually
used (one "heavy object" for each thread in the thread pool).
Depending on the actual length of the "long-runnning" operation, this
could be counter-productive. (This is a *potentially* long-running
operation. It can also be, say, 1000 iterations -- which normally take
2-4 seconds to process, if just one heavy object needs to be created.
If there happens to be 1000 threads in the pool, there is the
possibility that such an object should be constructed for each
iteration [short-running task]. Which is bad.)

A way out of the "heavy-object-spreading" problem would be, if thread
pools had a "task-type affinity" feature: if a task of a given type is
requesting a thread from the pool, one is preferably given which has
recently been used by a task of the same type.

Another way out would be to replace the current thread's reference as
the key for ConcurrentHashMap. I can think, e.g., of the following
scheme:
(1) generate with a cheap RNG (xorShift?) a number between 0 and 16
(2) use this number to retrieve a LinkedList from the ConcurrentHashMap
(3) use the LinkedList as a pool.

But this starts to look awsome, isn't it?

From chris.purcell.39 at gmail.com  Sat Feb 24 06:49:10 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Sat, 24 Feb 2007 11:49:10 +0000
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <058f01c7564c$242e5d30$28681948@DesktopSystem>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu>
	<45DCF85C.5080808@xemaps.com>
	<058f01c7564c$242e5d30$28681948@DesktopSystem>
Message-ID: <4856e16d0702240349k7ae7510rf463f708fc279dd4@mail.gmail.com>

I have a few questions about that algorithm, perhaps someone on the
list can clear them up?

(1) How is the algorithm wait-free? It seems to me that if thread A
starts trying to insert a key that hashes to, say, 0; and if another,
much faster, thread B inserts an infinite sequence of distinct keys
that all hash to 0; then it's perfectly possible for thread A to never
succeed, being always preempted in its next possible move by thread B.

(2) Is this algorithm significantly different from "A Scalable
Non-Blocking Concurrent Hash Table Implementation with Incremental
Rehashing" by Martin and Davis?
[http://vision.bc.edu/~dmartin/papers/nonblocksync.ps] Sure, the key
and the data use two pointers rather than one; Java's garbage
collection allows the version field to be dropped; and the state field
is encoded by storing nulls; but is there actually a major difference?

(3) How can those results be reproduced? What did the test harness
look like? I would expect a synthetic workload, choosing keys
uniformly at random from a limited set -- not  a strenuous test of the
resizing. There's also no indication of how much (non-reclaimable)
memory the hash table took up. I would guess this scaled linearly with
the key-space, not the population, or the table would need to be
constantly replicated to remove tombstones. How well does the
algorithm perform when it replicates? Is it feasible to keep the
memory footprint on the order of population?

Cheers,
Chris Purcell

On 2/22/07, Kimo Crossman <kimo at webnetic.net> wrote:
>
> Go here for slides of the talk.
>
> http://www.stanford.edu/class/ee380/Abstracts/070221.html
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joseph Seigh
> Sent: 2007 February 21 17:57
> To: concurrency-interest
> Subject: Re: [concurrency-interest] A Fast Wait-Free HashTable
>
> David J. Biesack wrote:
>
> >
> >Does anyone know more about Cliff Click's wait-free hashtable?
> >
> >http://cs.stanford.edu/calendar/abstract.php?eventId=2270
> >
> >"A Faster Wait-Free Hash Table"
> >
> >"I present a wait-free (lock-free) concurrent Hash Table implementation with better single-thread performance than most Hash Tables, and better (usual much better) multi-thread performance than all other implementations I tried. I demonstrate scaling up to 768 CPUs even with high mutation rates. I show correctness by looking at the problem in a very different light than the usual "happens-before" / memory-order / fencing style of thinking -- indeed the algorithm requires no memory-ordering, no memory fencing, no D-CAS and no CAS-spin-loops."
> >
> >Sounds interesting. Has anyone seen or reviewed the design or implementation, or comparisons to ConcurrentHashMap?
> >
>
> No.  I guess we'll have to wait for the wait-free hash table. :)
>
> I sort of have a blocking linked list which runs about 3X faster than LinkedBlockingQueue under high contention on a single processor system.  I am waiting for a
> 768 processor
> system to test how well it will really scale.
>
> --
> Joe Seigh
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From kimo at webnetic.net  Sat Feb 24 18:19:14 2007
From: kimo at webnetic.net (Kimo Crossman)
Date: Sat, 24 Feb 2007 15:19:14 -0800
Subject: [concurrency-interest] A Fast Wait-Free HashTable
In-Reply-To: <4856e16d0702240349k7ae7510rf463f708fc279dd4@mail.gmail.com>
References: <200702211307.l1LD7bEb001400@cs.oswego.edu><45DCF85C.5080808@xemaps.com><058f01c7564c$242e5d30$28681948@DesktopSystem>
	<4856e16d0702240349k7ae7510rf463f708fc279dd4@mail.gmail.com>
Message-ID: <00fe01c7586a$350cfdd0$28681948@DesktopSystem>

Wondering,  Did you watch the video here?

http://www.stanford.edu/class/ee380/

Well actually here:

http://stanford-online.stanford.edu/courses/ee380/070221-ee380-300.asx


-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Chris Purcell
Sent: 2007 February 24 03:49
To: concurrency-interest
Subject: Re: [concurrency-interest] A Fast Wait-Free HashTable

I have a few questions about that algorithm, perhaps someone on the list can clear them up?

(1) How is the algorithm wait-free? It seems to me that if thread A starts trying to insert a key that hashes to, say, 0; and if another, much faster, thread B inserts an infinite sequence of distinct keys that all hash to 0; then it's perfectly possible for thread A to never succeed, being always preempted in its next possible move by thread B.

(2) Is this algorithm significantly different from "A Scalable Non-Blocking Concurrent Hash Table Implementation with Incremental Rehashing" by Martin and Davis?
[http://vision.bc.edu/~dmartin/papers/nonblocksync.ps] Sure, the key and the data use two pointers rather than one; Java's garbage collection allows the version field to be dropped; and the state field is encoded by storing nulls; but is there actually a major difference?

(3) How can those results be reproduced? What did the test harness look like? I would expect a synthetic workload, choosing keys uniformly at random from a limited set -- not  a strenuous test of the resizing. There's also no indication of how much (non-reclaimable) memory the hash table took up. I would guess this scaled linearly with the key-space, not the population, or the table would need to be constantly replicated to remove tombstones. How well does the algorithm perform when it replicates? Is it feasible to keep the memory footprint on the order of population?

Cheers,
Chris Purcell

On 2/22/07, Kimo Crossman <kimo at webnetic.net> wrote:
>
> Go here for slides of the talk.
>
> http://www.stanford.edu/class/ee380/Abstracts/070221.html
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of 
> Joseph Seigh
> Sent: 2007 February 21 17:57
> To: concurrency-interest
> Subject: Re: [concurrency-interest] A Fast Wait-Free HashTable
>
> David J. Biesack wrote:
>
> >
> >Does anyone know more about Cliff Click's wait-free hashtable?
> >
> >http://cs.stanford.edu/calendar/abstract.php?eventId=2270
> >
> >"A Faster Wait-Free Hash Table"
> >
> >"I present a wait-free (lock-free) concurrent Hash Table implementation with better single-thread performance than most Hash Tables, and better (usual much better) multi-thread performance than all other implementations I tried. I demonstrate scaling up to 768 CPUs even with high mutation rates. I show correctness by looking at the problem in a very different light than the usual "happens-before" / memory-order / fencing style of thinking -- indeed the algorithm requires no memory-ordering, no memory fencing, no D-CAS and no CAS-spin-loops."
> >
> >Sounds interesting. Has anyone seen or reviewed the design or implementation, or comparisons to ConcurrentHashMap?
> >
>
> No.  I guess we'll have to wait for the wait-free hash table. :)
>
> I sort of have a blocking linked list which runs about 3X faster than 
> LinkedBlockingQueue under high contention on a single processor 
> system.  I am waiting for a
> 768 processor
> system to test how well it will really scale.
>
> --
> Joe Seigh
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From cliffc at acm.org  Sat Feb 24 17:13:49 2007
From: cliffc at acm.org (Cliff Click)
Date: Sat, 24 Feb 2007 14:13:49 -0800
Subject: [concurrency-interest] [Fwd: Re:  A Fast Wait-Free HashTable]
Message-ID: <45E0B89D.7050302@acm.org>

An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070224/bed887d1/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cliffc.vcf
Type: text/x-vcard
Size: 136 bytes
Desc: not available
Url : /pipermail/attachments/20070224/bed887d1/attachment.vcf 

From chris.purcell.39 at gmail.com  Sat Feb 24 20:22:31 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Sun, 25 Feb 2007 01:22:31 +0000
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <45E0B89D.7050302@acm.org>
References: <45E0B89D.7050302@acm.org>
Message-ID: <4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>

>>  (1) How is the algorithm wait-free? It seems to me that if thread A starts
>> trying to insert a key that hashes to, say, 0; and if another, much faster,
>> thread B inserts an infinite sequence of distinct keys that all hash to 0;
>> then it's perfectly possible for thread A to never succeed, being always
>> preempted in its next possible move by thread B.
>
>  When A fails because of some other write (that's the
> 'no-endless-spurious-failure-CAS' requirement), then A
> treats it as-if A succeeded but was immediately overwritten by B, and A can
> claim (invisible) success.

But in each case, the key B inserted only hashes to the same constant;
it is not the same key. (Indeed, since this is an open-addressed hash
table, one can easily construct an example where none of the keys even
hash to the same constant.) Hence none of B's operations can be
considered to have overwritten A's, and A must continue indefinitely.

>  Doug Adds:
>
>> It is obstruction-free, not necessarily wait-free
> I'm not the pro here, so What's the difference?

Wait-free means there exists an a priori bound on the number of steps
a thread must take to complete its *own* operation. Lock-free means
there exists an a priori bound on the number of steps a thread must
take before being assured that there has been *global* progress.
Obstruction-free means there exists an a priori bound on the number of
steps a thread executing in *isolation* must take before completing
its own operation. Wait-free is strictly stronger than lock-free, and
lock-free is strictly stronger than obstruction-free.

I believe your algorithm is lock-free, but wait-free seems like an
impossible goal for a fast hash table. Hence my interest in assessing
the claim!

>> (2) Is this algorithm significantly different...
>
>  Let's see:  no CAS-size > ptr-size requirement (hence no >64b CAS
> requirement on 64b pointers), no version...

Sorry, I can see that "significantly" there may have been
unintentionally offensive. So the contributions (setting aside the
wait-free issue) are:

 * Given a garbage collector, the version counter field is
unnecessary. This also appears to be true in the unmodified Martin and
Davis algorithm; I can't see how it was used. An important
optimization, as it means 32-bit CAS is sufficient (or 64-bit CAS on a
64-bit machine).

 * Key and value can safely be linked to separately, doubling the size
of the hash table but potentially reducing memory churn. I'm not sure
how this works out overall.

> readers do not need to realize a table-copy is in progress unless...

I think this is true of the Martin and Davis algorithm, too.

>  Rehash can be triggered via a number of ways, but if you use "table
> fullness" you need to maintain a table-full value - which is hard to do if
> 768 cpus are hammering away at the table size!  I am triggering off some
> thread-local state at the moment (too many reprobes on this attempt) but the
> heuristics are still in flux.

Yes, that's a tricky one. I'm looking forward to seeing how you solve it.

> 2. Given that all hash tables share a lot of  commonalities, how "different"
> is "significantly different" and who cares. Do you have a PDF version of
> this 10-yr-old paper?  I could only find the above PS version.

Again, apologies for offense, I was unsure if my initial appraisal of
the similarities was correct. Probably digging myself into a hole here
:)

>>  What did the test harness look like?
>  I copied Doug's - so "synthetic workload, choosing keys uniformly at random
> from a limited set" is exactly it.  In my case I purposely choose a small
> set of keys (1000) to force high levels of write conflicts during updates.

I've used Doug's workload in my experiments, too.

What random number generator do you use?

> There are indeed some tradeoffs in resizing and tombstone scans, that could
> stand better empirical investigation, but it's a work in progress so give
> poor Cliff a break!

No attack intended! Just trying to figure out what situations the
numbers cover, and where to be cautious.

As a suggestion for future investigation, perhaps a good worst-case
test would be to have each thread maintain a set of "owned" keys, K_o.
At each mutate step, the thread picks a key kd from K_o, and does a
delete operation on that key in the table; it then picks a new key ki
uniformly at random from the (very large) keyspace, and inserts that
into the table. (At the end of the step, K_o := K_o - {kd} + {ki}.)
That ensures a bounded population together with a high creation rate
of tombstones, allowing the footprint/time tradeoff to be explored. I
wonder if a high rate of table replications will cause
disproportionately high synchronization overheads, especially on a
768-way?

Cheers,
Chris Purcell

From dl at cs.oswego.edu  Sun Feb 25 07:08:51 2007
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Feb 2007 07:08:51 -0500
Subject: [concurrency-interest] [Fwd: Re: [Fwd: Re: A Fast Wait-Free
	HashTable]]
Message-ID: <45E17C53.3020601@cs.oswego.edu>

Resending this reply from Cliff that had gotten trapped as list spam...
-------------- next part --------------
An embedded message was scrubbed...
From: Cliff Click <cliffc at acm.org>
Subject: Re: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
Date: Sat, 24 Feb 2007 22:11:12 -0800
Size: 9516
Url: /pipermail/attachments/20070225/d02813ec/attachment-0001.nws 

From chris.purcell.39 at gmail.com  Sun Feb 25 08:52:25 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Sun, 25 Feb 2007 13:52:25 +0000
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <45E12880.3060009@acm.org>
References: <45E0B89D.7050302@acm.org>
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>
	<45E12880.3060009@acm.org>
Message-ID: <4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>

> Hence I'm assuming the hash function has, on average, some kind of
> reasonable distribution and hence B cannot always hash to the same
> value.  How's that for a 'proof'.   ;-)

Unfortunately insufficient for wait-freedom, which demands a
non-probabilistic bound. Lock-freedom is fine, though.

> 'A' would need to
> unconditionally set some flag that B promises to check periodically; and
> on seeing it allow A to succeed (or do A's operation on A's behalf).
> But this requires some kind of O(threads) check by B (but B can check it
> 1/#threads often, so constant more work for B whilst A is slowed by
> #threads).  Maybe there's more efficient wait-free versions of "thread X
> wants to be noticed by thread Y" Out There.

There's been some good stuff coming out of the obstruction-freedom
research that would help you here. If you add a "panic" flag, A can
ensure it's raised with a single write; other threads can check the
per-thread assistance requests only when the panic flag is set. Not
sure of the best way of lowering it again.

I wouldn't worry too much about getting a wait-free progress
guarantee, though. It's a *lot* of extra work, and adds great
potential for bugs.

> I think key-delete and value/mod remain wait-free.

Key delete needs to retry CAS operations if the table is resized;
thus, it is only lock-free. Table lookup can be wait-free if you leave
values in discarded tables when you copy, assuming you have some kind
of guarantee that there will only be two copies of the table live at
any point.

> And everybody needs the OS to cooperate.

The progress guarantees all talk about a thread taking a number of
steps; if the OS halts the thread, it takes no more steps, so the
progress guarantees trivially hold.

> Given a
> hash with random distribution I can show the steps are bounded by the
> statistically (un)likelyhood of collision.

As I said, unfortunately that's not enough for wait-freedom. Perhaps
"lock-freedom with probabilistic fairness" would be a good term to
introduce?

>  I've already picked a table such that nearly every operation
> is a max-cost cache-miss (small table, very high mod rate on the Values,
> so endless cache-misses on both lookup and mod) - and this still scales
> well.

Interesting. When I benchmarked things, they tended to run out of
bandwidth very rapidly if every processor was missing in cache ? and
this was only on a 96-way. Must be a decent memory subsystem you were
running on!

Cheers,
Chris Purcell


From kimo at webnetic.net  Sun Feb 25 20:25:53 2007
From: kimo at webnetic.net (Kimo Crossman)
Date: Sun, 25 Feb 2007 17:25:53 -0800
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <45D8E361.4070902@xemaps.com>
References: <45D8E361.4070902@xemaps.com>
Message-ID: <00c301c75945$0fa251e0$28681948@DesktopSystem>

By the way Joe have you seen this?

Software Transactional Memory should Not be Lock Free
Robert Ennals
Intel Research Cambridge Technical Report: IRC-TR-06-052, 2006
This paper was submitted to SCOOL 2005, but deemed to be too contraversial and so was made the subject of a panel instead. 
http://berkeley.intel-research.net/rennals/pubs/052RobEnnals.pdf


-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Joseph Seigh
Sent: 2007 February 18 15:38
To: concurrency-interest
Subject: [concurrency-interest] Lock-free signaling

Is there a way to do signaling without requiring a monitor lock?  Something
like events or eventcounts.   I'm doing an STM implementation and was
thinking of adding a listener pattern to it but there's no point in making the implementation lock-free if I'm going to need locking to implement signaling.

--
Joe Seigh
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From cliffc at azulsystems.com  Mon Feb 26 11:00:31 2007
From: cliffc at azulsystems.com (Cliff Click)
Date: Mon, 26 Feb 2007 08:00:31 -0800
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
References: <45E0B89D.7050302@acm.org>	
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>	
	<45E12880.3060009@acm.org>
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
Message-ID: <45E3041F.7000004@azulsystems.com>

An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/ff1fb33f/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cliffc.vcf
Type: text/x-vcard
Size: 311 bytes
Desc: not available
Url : /pipermail/attachments/20070226/ff1fb33f/attachment.vcf 

From chris.purcell.39 at gmail.com  Mon Feb 26 11:56:23 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Mon, 26 Feb 2007 16:56:23 +0000
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <45E30EF2.5030604@azulsystems.com>
References: <45E0B89D.7050302@acm.org>
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>
	<45E12880.3060009@acm.org>
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
	<45E30EF2.5030604@azulsystems.com>
Message-ID: <4856e16d0702260856l6a0024b1l5c880df746b2594f@mail.gmail.com>

Why not just call the table lock-free?

Chris

From chris.purcell.39 at gmail.com  Mon Feb 26 13:03:45 2007
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Mon, 26 Feb 2007 18:03:45 +0000
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <45E315EB.40907@azulsystems.com>
References: <45E0B89D.7050302@acm.org>
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>
	<45E12880.3060009@acm.org>
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
	<45E30EF2.5030604@azulsystems.com>
	<4856e16d0702260856l6a0024b1l5c880df746b2594f@mail.gmail.com>
	<45E315EB.40907@azulsystems.com>
Message-ID: <4856e16d0702261003y51fbd17cu6a547dff03c40533@mail.gmail.com>

>  I clearly can't call it Wait-Free but the table has some properties that
> are interestingly stronger than Lock-Free even if it's not quite Wait-Free:
> other threads can be halted indefinitely (crashed) without damaging the
> table, and on 'reasonable' machines everybody is guaranteed forward progress
> (and it is really efficient).  So I'm bummed that it's "only" lock-free,
> that's all.

Lock-free *does* imply that crashing threads don't damage the table
(or forward-progress of the system when other threads take steps would
be impossible). Further, on reasonable machines, a lock-free system
with decent properties like population-oblivousness, read-parallelism
and disjoint-access parallelism, which your system has, pretty much
guarantees forward progress, since actual obstruction is pretty rare.
So don't be so upset :)

Cheers,
Chris

From hanson.char at gmail.com  Mon Feb 26 14:11:30 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 26 Feb 2007 11:11:30 -0800
Subject: [concurrency-interest] Jcip annotation license
Message-ID: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>

Hi,

Can anyone points me to a link (at http://jcip.net/)  to the license text
file for the JCIP related code ?  (eg @ThreadSafe, etc.)

Thanks,
Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/a7a52365/attachment.html 

From tim at peierls.net  Mon Feb 26 14:26:22 2007
From: tim at peierls.net (Tim Peierls)
Date: Mon, 26 Feb 2007 14:26:22 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
Message-ID: <63b4e4050702261126k63d82fe9k6670dd4e71f5d2d1@mail.gmail.com>

There is no separate license file. Each source file has this comment at the
top:

/*
 * Copyright (c) 2005 Brian Goetz and Tim Peierls
 * Released under the Creative Commons Attribution License
 *   (http://creativecommons.org/licenses/by/2.5)
 * Official home: http://www.jcip.net
 *
 * Any republication or derived work distributed in source code form
 * must include this copyright and license notice.
 */

Maybe we should change the copyright year.

--tim


On 2/26/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi,
>
> Can anyone points me to a link (at http://jcip.net/)  to the license text
> file for the JCIP related code ?  (eg @ThreadSafe, etc.)
>
> Thanks,
> Hanson
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/4c310e95/attachment.html 

From holger at wizards.de  Mon Feb 26 14:44:37 2007
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Mon, 26 Feb 2007 20:44:37 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
Message-ID: <45E338A5.70408@wizards.de>

Hanson Char wrote:
> Can anyone points me to a link (at http://jcip.net/)  to the license
> text file for the JCIP related code ?  (eg @ThreadSafe, etc.)

It's in the source and the javadocs at the end:
http://www.jcip.net/annotations/doc/index.html

-h

From brian at quiotix.com  Mon Feb 26 15:12:24 2007
From: brian at quiotix.com (Brian Goetz)
Date: Mon, 26 Feb 2007 15:12:24 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
Message-ID: <45E33F28.3080409@quiotix.com>

The annotation sources are under Creative Commons: 
http://creativecommons.org/licenses/by/2.5

Not sure of the license status of the sample code; Tim?


Hanson Char wrote:
> Hi,
> 
> Can anyone points me to a link (at http://jcip.net/)  to the license 
> text file for the JCIP related code ?  (eg @ThreadSafe, etc.)
> 
> Thanks,
> Hanson
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From tim at peierls.net  Mon Feb 26 15:33:34 2007
From: tim at peierls.net (Tim Peierls)
Date: Mon, 26 Feb 2007 15:33:34 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E33F28.3080409@quiotix.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com>
Message-ID: <63b4e4050702261233m29248332h9de918a7986bc87c@mail.gmail.com>

On 2/26/07, Brian Goetz <brian at quiotix.com> wrote:
>
> The annotation sources are under Creative Commons:
> http://creativecommons.org/licenses/by/2.5
>
> Not sure of the license status of the sample code; Tim?
>

There's nothing in them about copyright or license. IANAL, but wouldn't they
be seen as parts of the book?

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/e0cd42e5/attachment.html 

From brian at quiotix.com  Mon Feb 26 15:35:52 2007
From: brian at quiotix.com (Brian Goetz)
Date: Mon, 26 Feb 2007 15:35:52 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <63b4e4050702261233m29248332h9de918a7986bc87c@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>	
	<45E33F28.3080409@quiotix.com>
	<63b4e4050702261233m29248332h9de918a7986bc87c@mail.gmail.com>
Message-ID: <45E344A8.6000601@quiotix.com>

> There's nothing in them about copyright or license. IANAL, but wouldn't 
> they be seen as parts of the book?

That was my thought as well; they would be copyright by A-W, unless any 
of them are prior inventions.  Most books release the source code under 
some license; I'll check with Greg on that.

From hanson.char at gmail.com  Mon Feb 26 15:53:44 2007
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 26 Feb 2007 12:53:44 -0800
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E33F28.3080409@quiotix.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com>
Message-ID: <ca53c8f80702261253y4f458a31u6afd77d0c09c25aa@mail.gmail.com>

Hi Brian,

Sorry about this but due to some bureaucratic nonsense I won't be allowed to
use the jcip jars unless I can find a link to the license file on the jcip
site :(

Can you help ?  Much appreciated.

Hanson

On 2/26/07, Brian Goetz <brian at quiotix.com> wrote:
>
> The annotation sources are under Creative Commons:
> http://creativecommons.org/licenses/by/2.5
>
> Not sure of the license status of the sample code; Tim?
>
>
> Hanson Char wrote:
> > Hi,
> >
> > Can anyone points me to a link (at http://jcip.net/)  to the license
> > text file for the JCIP related code ?  (eg @ThreadSafe, etc.)
> >
> > Thanks,
> > Hanson
> >
> >
> > ------------------------------------------------------------------------
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/f1a8bae9/attachment.html 

From tim at peierls.net  Mon Feb 26 15:57:22 2007
From: tim at peierls.net (Tim Peierls)
Date: Mon, 26 Feb 2007 15:57:22 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <ca53c8f80702261253y4f458a31u6afd77d0c09c25aa@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com>
	<ca53c8f80702261253y4f458a31u6afd77d0c09c25aa@mail.gmail.com>
Message-ID: <63b4e4050702261257t783c2bf4gef72c4c3c76b47b2@mail.gmail.com>

So use
http://jcip.net/annotations/doc/net/jcip/annotations/package-summary.html

--tim

On 2/26/07, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi Brian,
>
> Sorry about this but due to some bureaucratic nonsense I won't be allowed
> to use the jcip jars unless I can find a link to the license file on the
> jcip site :(
>
> Can you help ?  Much appreciated.
>
> Hanson
>
> On 2/26/07, Brian Goetz <brian at quiotix.com> wrote:
> >
> > The annotation sources are under Creative Commons:
> > http://creativecommons.org/licenses/by/2.5
> >
> > Not sure of the license status of the sample code; Tim?
> >
> >
> > Hanson Char wrote:
> > > Hi,
> > >
> > > Can anyone points me to a link (at http://jcip.net/)  to the license
> > > text file for the JCIP related code ?  (eg @ThreadSafe, etc.)
> > >
> > > Thanks,
> > > Hanson
> > >
> > >
> > >
> > ------------------------------------------------------------------------
> > >
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at altair.cs.oswego.edu
> > > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/49b1e1b0/attachment-0001.html 

From jseigh_cp00 at xemaps.com  Mon Feb 26 23:00:17 2007
From: jseigh_cp00 at xemaps.com (Joseph Seigh)
Date: Mon, 26 Feb 2007 23:00:17 -0500
Subject: [concurrency-interest] Lock-free signaling
In-Reply-To: <00c301c75945$0fa251e0$28681948@DesktopSystem>
References: <45D8E361.4070902@xemaps.com>
	<00c301c75945$0fa251e0$28681948@DesktopSystem>
Message-ID: <45E3ACD1.7050509@xemaps.com>

Kimo Crossman wrote:

>
>By the way Joe have you seen this?
>
>Software Transactional Memory should Not be Lock Free
>Robert Ennals
>Intel Research Cambridge Technical Report: IRC-TR-06-052, 2006
>This paper was submitted to SCOOL 2005, but deemed to be too contraversial and so was made the subject of a panel instead. 
>http://berkeley.intel-research.net/rennals/pubs/052RobEnnals.pdf
>  
>
I was only able to look at it briefly.  I'm not sure what made it too 
controversial.  If I
understand it the readers are subject to obstruction.  It's a standard 
retry readers
if the data has changed during the read.   Also I'm not sure about the 
claim about
versioned data having more significant effect on cache than non 
versioned data.
Java's object model has so much indirection already, I don't think you'd 
notice
the extra overhead.  Plus there's hardware strategies to deal with 
that.   E.g.
Niagara's hardware threading model, and not having cache coherency more
strict than it has to be.

On the afore mentioned STM implementation, I should have a prototype 
ready soon.

--
Joe Seigh

From belaban at yahoo.com  Tue Feb 27 03:29:50 2007
From: belaban at yahoo.com (Bela Ban)
Date: Tue, 27 Feb 2007 09:29:50 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E33F28.3080409@quiotix.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com>
Message-ID: <45E3EBFE.4040200@yahoo.com>

I re-created @Immutable and @GuardedBy in JGroups. Do you guys think 
this is a derivative work ? If so, what happens if I rename @GuardedBy 
to @ProtectedBy ?

I'm investigating which license I need to ship with the JGroups distro...

Brian Goetz wrote:
> The annotation sources are under Creative Commons:
> http://creativecommons.org/licenses/by/2.5
>
> Not sure of the license status of the sample code; Tim?

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat

From cliffc at azulsystems.com  Mon Feb 26 11:46:42 2007
From: cliffc at azulsystems.com (Cliff Click)
Date: Mon, 26 Feb 2007 08:46:42 -0800
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
References: <45E0B89D.7050302@acm.org>	
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>	
	<45E12880.3060009@acm.org>
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>
Message-ID: <45E30EF2.5030604@azulsystems.com>

An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/3d36fd3a/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cliffc.vcf
Type: text/x-vcard
Size: 311 bytes
Desc: not available
Url : /pipermail/attachments/20070226/3d36fd3a/attachment.vcf 

From cliffc at azulsystems.com  Mon Feb 26 12:16:27 2007
From: cliffc at azulsystems.com (Cliff Click)
Date: Mon, 26 Feb 2007 09:16:27 -0800
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <4856e16d0702260856l6a0024b1l5c880df746b2594f@mail.gmail.com>
References: <45E0B89D.7050302@acm.org>	
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>	
	<45E12880.3060009@acm.org>	
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>	
	<45E30EF2.5030604@azulsystems.com>
	<4856e16d0702260856l6a0024b1l5c880df746b2594f@mail.gmail.com>
Message-ID: <45E315EB.40907@azulsystems.com>

An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070226/3f8fde16/attachment.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: cliffc.vcf
Type: text/x-vcard
Size: 311 bytes
Desc: not available
Url : /pipermail/attachments/20070226/3f8fde16/attachment.vcf 

From cliffc at azulsystems.com  Mon Feb 26 14:19:14 2007
From: cliffc at azulsystems.com (Cliff Click)
Date: Mon, 26 Feb 2007 11:19:14 -0800
Subject: [concurrency-interest] [Fwd: Re: A Fast Wait-Free HashTable]
In-Reply-To: <4856e16d0702261003y51fbd17cu6a547dff03c40533@mail.gmail.com>
References: <45E0B89D.7050302@acm.org>	
	<4856e16d0702241722r6c39503w6ae35ed39220c30@mail.gmail.com>	
	<45E12880.3060009@acm.org>	
	<4856e16d0702250552w671db7eau54c88cec8d2ce7a2@mail.gmail.com>	
	<45E30EF2.5030604@azulsystems.com>	
	<4856e16d0702260856l6a0024b1l5c880df746b2594f@mail.gmail.com>	
	<45E315EB.40907@azulsystems.com>
	<4856e16d0702261003y51fbd17cu6a547dff03c40533@mail.gmail.com>
Message-ID: <45E332B2.1070106@azulsystems.com>

Chris Purcell wrote:
>>  I clearly can't call it Wait-Free but the table has some properties 
>> that
>> are interestingly stronger than Lock-Free even if it's not quite 
>> Wait-Free:
>> other threads can be halted indefinitely (crashed) without damaging the
>> table, and on 'reasonable' machines everybody is guaranteed forward 
>> progress
>> (and it is really efficient).  So I'm bummed that it's "only" lock-free,
>> that's all.
>
> Lock-free *does* imply that crashing threads don't damage the table
> (or forward-progress of the system when other threads take steps would
> be impossible). Further, on reasonable machines, a lock-free system
> with decent properties like population-oblivousness, read-parallelism
> and disjoint-access parallelism, which your system has, pretty much
> guarantees forward progress, since actual obstruction is pretty rare.
> So don't be so upset :)
>
> Cheers,
> Chris

Ok.  Thanks all, this has been very informative to me.

Cliff

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cliffc.vcf
Type: text/x-vcard
Size: 266 bytes
Desc: not available
Url : /pipermail/attachments/20070226/f9e1b433/attachment.vcf 

From tim at peierls.net  Tue Feb 27 08:52:41 2007
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Feb 2007 08:52:41 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E3EBFE.4040200@yahoo.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com> <45E3EBFE.4040200@yahoo.com>
Message-ID: <63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>

I'm sure there's no problem under the Creative Commons license as long as
you include the copyright and license notice and make it clear that you have
modified it. Check the CC link to be sure.

But although I don't see any legal obstacles here, consider that the JCiP
uses @GuardedBy, so @ProtectedBy risks confusing those of your users who
also read the book -- which we hope will be an increasingly significant
portion! :-)

@GuardedBy might not have been the best choice, but it's a widely published
choice. And "protected" already has another meaning in Java.

--tim

On 2/27/07, Bela Ban <belaban at yahoo.com> wrote:
>
> I re-created @Immutable and @GuardedBy in JGroups. Do you guys think
> this is a derivative work ? If so, what happens if I rename @GuardedBy
> to @ProtectedBy ?
>
> I'm investigating which license I need to ship with the JGroups distro...
>
> Brian Goetz wrote:
> > The annotation sources are under Creative Commons:
> > http://creativecommons.org/licenses/by/2.5
> >
> > Not sure of the license status of the sample code; Tim?
>
> --
> Bela Ban
> Lead JGroups / JBoss Clustering team
> JBoss - a division of Red Hat
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070227/23e22d77/attachment.html 

From tim at peierls.net  Tue Feb 27 08:53:31 2007
From: tim at peierls.net (Tim Peierls)
Date: Tue, 27 Feb 2007 08:53:31 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com> <45E3EBFE.4040200@yahoo.com>
	<63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
Message-ID: <63b4e4050702270553k11b1c021h22c0c54c6e0faed3@mail.gmail.com>

On 2/27/07, Tim Peierls <tim at peierls.net> wrote:
>
> I'm sure there's no problem under the Creative Commons license as long as
> you include the copyright and license notice and make it clear that you have
> modified it.


By "it" I mean the source, not the notice.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20070227/32b0ee0e/attachment.html 

From brian at quiotix.com  Tue Feb 27 09:47:16 2007
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 27 Feb 2007 09:47:16 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E3EBFE.4040200@yahoo.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com> <45E3EBFE.4040200@yahoo.com>
Message-ID: <45E44474.3010406@quiotix.com>

Hmm, let me call my lawyer!

Seriously, there's some advantage to using the net.jcip annotations -- 
for example, FindBugs has some support for them.

If the chosen license is a problem, we can relicense under another 
license.  Is BSD compatible with GPL?

Bela Ban wrote:
> I re-created @Immutable and @GuardedBy in JGroups. Do you guys think 
> this is a derivative work ? If so, what happens if I rename @GuardedBy 
> to @ProtectedBy ?
> 
> I'm investigating which license I need to ship with the JGroups distro...
> 
> Brian Goetz wrote:
>> The annotation sources are under Creative Commons:
>> http://creativecommons.org/licenses/by/2.5
>>
>> Not sure of the license status of the sample code; Tim?
> 

From belaban at yahoo.com  Tue Feb 27 09:53:54 2007
From: belaban at yahoo.com (Bela Ban)
Date: Tue, 27 Feb 2007 15:53:54 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>	
	<45E33F28.3080409@quiotix.com> <45E3EBFE.4040200@yahoo.com>
	<63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
Message-ID: <45E44602.2030403@yahoo.com>


Tim Peierls wrote:
> I'm sure there's no problem under the Creative Commons license as long 
> as you include the copyright and license notice and make it clear that 
> you have modified it. Check the CC link to be sure.

OK, done

> But although I don't see any legal obstacles here, consider that the 
> JCiP uses @GuardedBy, so @ProtectedBy risks confusing those of your 
> users who also read the book -- which we hope will be an increasingly 
> significant portion! :-) 
>
> @GuardedBy might not have been the best choice, but it's a widely 
> published choice. And "protected" already has another meaning in Java.

OK, I'll use @GuardedBy for now. Any plans to come up with a 
standardized set of annotations pertaining to concurrent programming ?

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat

From belaban at yahoo.com  Tue Feb 27 10:00:06 2007
From: belaban at yahoo.com (Bela Ban)
Date: Tue, 27 Feb 2007 16:00:06 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E44474.3010406@quiotix.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>
	<45E33F28.3080409@quiotix.com> <45E3EBFE.4040200@yahoo.com>
	<45E44474.3010406@quiotix.com>
Message-ID: <45E44776.4050606@yahoo.com>


Brian Goetz wrote:
> Hmm, let me call my lawyer!
:-)

> Seriously, there's some advantage to using the net.jcip annotations -- 
> for example, FindBugs has some support for them.

OK

> If the chosen license is a problem, we can relicense under another 
> license.  Is BSD compatible with GPL?

There are no problems with the Creative Commons license - it is 
completely compatible with the LGPL license (under which JGroups is 
licensed). No need to switch to BSD.
Thanks,

-- 
Bela Ban
Lead JGroups / JBoss Clustering team
JBoss - a division of Red Hat

From holger at wizards.de  Tue Feb 27 10:33:06 2007
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 27 Feb 2007 16:33:06 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E44602.2030403@yahoo.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>		<45E33F28.3080409@quiotix.com>
	<45E3EBFE.4040200@yahoo.com>	<63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
	<45E44602.2030403@yahoo.com>
Message-ID: <45E44F32.2030808@wizards.de>

Bela Ban wrote:
> OK, I'll use @GuardedBy for now. Any plans to come up with a 
> standardized set of annotations pertaining to concurrent programming ?

Not only for concurrent programming:
http://www.jcp.org/en/jsr/detail?id=305

-h


From brian at quiotix.com  Tue Feb 27 10:47:44 2007
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 27 Feb 2007 10:47:44 -0500
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E44602.2030403@yahoo.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>		<45E33F28.3080409@quiotix.com>
	<45E3EBFE.4040200@yahoo.com>	<63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
	<45E44602.2030403@yahoo.com>
Message-ID: <45E452A0.2010904@quiotix.com>

>> @GuardedBy might not have been the best choice, but it's a widely 
>> published choice. And "protected" already has another meaning in Java.
> 
> OK, I'll use @GuardedBy for now. Any plans to come up with a 
> standardized set of annotations pertaining to concurrent programming ?

JSR 305?

From mailinglist.taras.tielkes at gmail.com  Tue Feb 27 11:05:23 2007
From: mailinglist.taras.tielkes at gmail.com (Taras Tielkes)
Date: Tue, 27 Feb 2007 17:05:23 +0100
Subject: [concurrency-interest] Jcip annotation license
In-Reply-To: <45E44602.2030403@yahoo.com>
References: <ca53c8f80702261111h1bdd00dah21a1158aeb4a78e4@mail.gmail.com>		<45E33F28.3080409@quiotix.com>
	<45E3EBFE.4040200@yahoo.com>	<63b4e4050702270552x647360ffnc161be79dd3d4c31@mail.gmail.com>
	<45E44602.2030403@yahoo.com>
Message-ID: <45E456C3.7010809@gmail.com>

Bela Ban wrote:
>> @GuardedBy might not have been the best choice, but it's a widely 
>> published choice. And "protected" already has another meaning in Java.
> 
> OK, I'll use @GuardedBy for now. Any plans to come up with a 
> standardized set of annotations pertaining to concurrent programming ?
> 

I think this is planned as part of JSR 305.

Speaking of that, is there any progress information available?

-tt


From peter.kovacs.1.0rc at gmail.com  Wed Feb 28 12:37:48 2007
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Wed, 28 Feb 2007 18:37:48 +0100
Subject: [concurrency-interest] Behaviour of Future.get(0)
Message-ID: <b6e8f2e80702280937m1e960ab6n32937168a3bd764e@mail.gmail.com>

Hi,

What is the reason for Future.get(0) not behaving the same way as
Object.wait(0) (ie. immediately returning as opposed to never timing
out)?

Thanks
Peter

From dcholmes at optusnet.com.au  Wed Feb 28 19:47:57 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 1 Mar 2007 10:47:57 +1000
Subject: [concurrency-interest] Behaviour of Future.get(0)
In-Reply-To: <b6e8f2e80702280937m1e960ab6n32937168a3bd764e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEKCHFAA.dcholmes@optusnet.com.au>

Hi Peter,

wait(0) meaning "wait forever" is an abberation. All the j.u.c timed methods
treat a timeout of zero as "return immediately".

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Thursday, 1 March 2007 3:38 AM
> To: concurrency-interest
> Subject: [concurrency-interest] Behaviour of Future.get(0)
>
>
> Hi,
>
> What is the reason for Future.get(0) not behaving the same way as
> Object.wait(0) (ie. immediately returning as opposed to never timing
> out)?
>
> Thanks
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Wed Feb 28 19:49:34 2007
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 1 Mar 2007 10:49:34 +1000
Subject: [concurrency-interest] Behaviour of Future.get(0)
In-Reply-To: <b6e8f2e80702280937m1e960ab6n32937168a3bd764e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEKCHFAA.dcholmes@optusnet.com.au>

BTW the package docs mention this under "Timing":

"In all cases that time-outs are used, the time-out specifies the minimum
time that the method should wait before indicating that it timed-out."

Hence a minimum waiting time of zero means don't wait.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Kovacs
> Sent: Thursday, 1 March 2007 3:38 AM
> To: concurrency-interest
> Subject: [concurrency-interest] Behaviour of Future.get(0)
>
>
> Hi,
>
> What is the reason for Future.get(0) not behaving the same way as
> Object.wait(0) (ie. immediately returning as opposed to never timing
> out)?
>
> Thanks
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


