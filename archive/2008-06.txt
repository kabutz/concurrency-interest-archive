From dcholmes at optusnet.com.au  Sun Jun  1 00:44:40 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sun, 1 Jun 2008 14:44:40 +1000
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <48414FA4.9030905@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEOMHLAA.dcholmes@optusnet.com.au>

Gregg,

The compiler might remove the "no-op assignment" but it isn't allowed to
remove the associated memory model actions. So even if the store is elided
the "memory barriers" that might be needed would still have to occur.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Saturday, 31 May 2008 11:16 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu; Tim Peierls; Guy Korland
> Subject: Re: [concurrency-interest] Volatile and primitive arrays
>
>
> David Holmes wrote:
> > Yes he's sure :) The only "happens-before" property that
> volatile has is
> > as follows: a volatile write happens-before a subsequent volatile read
> > of the value written.
> >
> > If you only have reads you don't have happens-before.
>
> The interesting point, of course, is that if the array value
> reference is not
> written, then the array dereferences for read, don't come before
> any write of
> the same dereferenced array value.
>
> Those trying to be clever with volatiles and arrays (not me, but
> I can just see
> this question comming) might think that the next question of
> interest would be,
> if I code the following:
>
> 	volatile int []arr;
>
> 	public void setValue( int idx, int val ) {
> 		arr[idx] = val;
> 		arr = arr;
> 	}
>
> 	public int getValue( int idx ) {
> 		arr = arr;
> 		return arr[ idx ];
> 	}
>
> Is there now a meaningful happens before for the elements of the
> array?  I'd
> guess that the compiler might remove the no-op assignment.
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Sun Jun  1 00:50:33 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sun, 1 Jun 2008 14:50:33 +1000
Subject: [concurrency-interest] Throttling the submission
	withThreadPoolExecutor
In-Reply-To: <3DCE3824-6DF8-43BB-9EB3-6CE7FC3F904C@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEOMHLAA.dcholmes@optusnet.com.au>

Robert,

How do you establish the order of submission? The queue will have some
order, but its a race to put things into the queue. And unless you have a
single threaded executor the order of submissions in the queue need not be
the order of execution. So I'd need to understand the ordering requirement
more fully here.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Robert
> Nicholson
> Sent: Sunday, 1 June 2008 10:51 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Throttling the submission
> withThreadPoolExecutor
>
>
> I would like to ask how you ensure that order of submissions is
> maintained yet still be able to throttle those submissions when using
> a bounded queue? It seems that CallerRunsPolicy allows the Task to be
> run earlier than those currently in the working queue and that in our
> particular case is unacceptable. So what is the typical approach to
> guaranteeing order at the same time still being able to throttle the
> rate of submissions. Presumably this relys on an approach whereby you
> resubmit those resubmissions upon rejection after some interval to
> increase the likehood that the resubmission itself isn't rejected?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From moran at gigaspaces.com  Mon Jun  2 06:21:53 2008
From: moran at gigaspaces.com (Moran Avigdor)
Date: Mon, 2 Jun 2008 13:21:53 +0300
Subject: [concurrency-interest] ThreadPoolExecutor goes below corePoolSize
Message-ID: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>

Hi,

I searched a bit on bugs opened related to corePoolSize, but did not
find a related post 

regarding corePoolThreads being removed from the cache, although they
shouldn't.

 

My test case below simulates a ThreadPool, which is constructed with a
min corePoolSize and a max pool size.

The idle wait time is set to 1 ms. I block my threads with a
CountDownLatch to ensure that threads scale from min to max (and are not
reused).

By releasing the latch I free up all the threads to complete their work,
and check that we did not go below core pool size.

 

This test sometimes fails on the assertion that we are below
corePoolSize:

junit.framework.AssertionFailedError: Expected minimum pool size.
expected:<2> but was:<1>

 

Is anyone familiar with such a scenario?

 

I am using JDK 1.5.0_12.

Regards,

Moran Avigdor

 

 

 

    public void testScaleDown() throws Exception {

        final int min = 2;

        final int max = 4;

        final CountDownLatch active = new CountDownLatch(max +1);

        

        ThreadPoolExecutor pool =
(ThreadPoolExecutor)Executors.newCachedThreadPool();

        pool.setCorePoolSize(min);

        pool.setMaximumPoolSize(max);

        pool.setKeepAliveTime(1, TimeUnit.MILLISECONDS);

 

        for (int i=0; i<max; ++i) {

            pool.execute( new Runnable() {

                public void run()

                {

                    try {

                        active.countDown();

                        active.await();

                    }catch (InterruptedException e) {

                        e.printStackTrace(); //ignore

                    }

                } 

            });

            //wait time to execute task by pool

            Thread.sleep(100);

        }

        

        Assert.assertEquals("Expected all threads to block. ", 1,
active.getCount());

        Assert.assertEquals("Expected active. ", max,
pool.getActiveCount());

        Assert.assertEquals("Expected max pool size. ", max,
pool.getPoolSize());

 

        //release the latch

        active.countDown();

        active.await();

        

        //wait around for one second for state to settle

        Thread.sleep(1000);

        Assert.assertEquals("Expected completion of all tasks. ", max,
pool.getCompletedTaskCount());

        Assert.assertEquals("Expected completion of all threads. ", 0,
pool.getActiveCount());

        Assert.assertEquals("Expected minimum pool size. ", min,
pool.getPoolSize());

    }

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080602/d2cdc249/attachment-0001.html 

From holger at wizards.de  Mon Jun  2 09:11:54 2008
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Mon, 02 Jun 2008 15:11:54 +0200
Subject: [concurrency-interest] ThreadPoolExecutor goes below
	corePoolSize
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
References: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
Message-ID: <4843F19A.6070701@wizards.de>

Moran,

Moran Avigdor wrote:
> I searched a bit on bugs opened related to corePoolSize, but did not 
> find a related post regarding corePoolThreads being removed from the
> cache, although they shouldn't.

Probably http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6458662

This can be seen in a debugger - threads expire regardless of corePoolSize 
and allowCoreThreadTimeout being false. In JDK6 this happens every time.

Interestingly, the backport library gets it right and keeps min 
(corePoolSize) threads running; it reliably lets your test pass.

Holger

From martinrb at google.com  Mon Jun  2 16:13:31 2008
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 2 Jun 2008 13:13:31 -0700
Subject: [concurrency-interest] ThreadPoolExecutor goes below
	corePoolSize
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
References: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
Message-ID: <1ccfd1c10806021313i22965fe3wf440e729d0134b1a@mail.gmail.com>

Lots of ThreadPoolExecutor bugs got fixed in very recent JDKs
(including OpenJDK 6 and 7).
The changes were too extensive to backport to JDK 5.

http://bugs.sun.com/bugdatabase/search.do?process=1&category=java&bugStatus=10%2C11&subcategory=classes_util_concurrent&type=bug&keyword=ThreadPoolExecutor+%22Release+Fixed%2C+7%22

Martin

On Mon, Jun 2, 2008 at 3:21 AM, Moran Avigdor <moran at gigaspaces.com> wrote:
> Hi,
>
> I searched a bit on bugs opened related to corePoolSize, but did not find a
> related post
>
> regarding corePoolThreads being removed from the cache, although they
> shouldn't.

From robert.nicholson at gmail.com  Mon Jun  2 22:23:05 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Mon, 2 Jun 2008 21:23:05 -0500
Subject: [concurrency-interest] ThreadPoolExecutor goes below
	corePoolSize
In-Reply-To: <1ccfd1c10806021313i22965fe3wf440e729d0134b1a@mail.gmail.com>
References: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
	<1ccfd1c10806021313i22965fe3wf440e729d0134b1a@mail.gmail.com>
Message-ID: <8E88A854-FCA4-4386-B900-130DC0EDB22E@gmail.com>

If you restrict your usage of ThreadPoolExecutor to an unbounded  
queue. It's generally more reliable. That said you need to consider  
the memory footprint of running an application with an bounded queue.

It's already proven in 1.5 that with a bounded queue if you submit N  
tasks it will execute only < N tasks.

Consider the following code that is a modified version from one of the  
bug submissions

import java.util.concurrent.*;
import java.util.concurrent.atomic.*;

public class RecycledTPETask {
      public static void main(String[] args) throws Exception {
          final int nTasks = 1000;
          final AtomicInteger nRun = new AtomicInteger(0);
          final AtomicInteger nRejections = new AtomicInteger(0);
          final Runnable recycledTask = new Runnable() {
                  public void run() {
                      nRun.getAndIncrement();
                  } };
          final ThreadPoolExecutor p =
              new ThreadPoolExecutor(1, 30, 60, TimeUnit.SECONDS,
                                     new ArrayBlockingQueue(30));
          try {
              for (int i = 0; i < nTasks; ++i) {
                  for (;;) {
                      try {
                          p.execute(recycledTask);
                          break;
                      }
                      catch (RejectedExecutionException ignore) {
                          nRejections.getAndIncrement();
                      }
                  }
              }
              Thread.sleep(10000); // enough time to run all tasks

              System.out.println(nRejections.intValue() + "  
rejections");

              if (nRun.get() < nTasks)
                  throw new Error("Started " + nTasks +
                                  " Ran " + nRun.get());
          } catch(Exception ex) {
              ex.printStackTrace();
          } finally {
              p.shutdown();
          }
      }
}

init:
deps-jar:
compile-single:
run-single:
3013 rejections
Exception in thread "main" java.lang.Error: Started 1000 Ran 971
         at RecycledTPETask.main(RecycledTPETask.java:33)
Java Result: 1
BUILD SUCCESSFUL (total time: 10 seconds)

import java.util.concurrent.*;
import java.util.concurrent.atomic.*;

public class RecycledTPETask {
      public static void main(String[] args) throws Exception {
          final int nTasks = 1000;
          final AtomicInteger nRun = new AtomicInteger(0);
          final AtomicInteger nRejections = new AtomicInteger(0);
          final Runnable recycledTask = new Runnable() {
                  public void run() {
                      nRun.getAndIncrement();
                  } };
          final ThreadPoolExecutor p =
              new ThreadPoolExecutor(1, 30, 60, TimeUnit.SECONDS,
                                     new LinkedBlockingQueue());
          try {
              for (int i = 0; i < nTasks; ++i) {
                  for (;;) {
                      try {
                          p.execute(recycledTask);
                          break;
                      }
                      catch (RejectedExecutionException ignore) {
                          nRejections.getAndIncrement();
                      }
                  }
              }
              Thread.sleep(10000); // enough time to run all tasks

              System.out.println(nRejections.intValue() + "  
rejections");

              if (nRun.get() < nTasks)
                  throw new Error("Started " + nTasks +
                                  " Ran " + nRun.get());
          } catch(Exception ex) {
              ex.printStackTrace();
          } finally {
              p.shutdown();
          }
      }
}

init:
deps-jar:
compile-single:
run-single:
0 rejections
BUILD SUCCESSFUL (total time: 11 seconds)

On Jun 2, 2008, at 3:13 PM, Martin Buchholz wrote:

> Lots of ThreadPoolExecutor bugs got fixed in very recent JDKs
> (including OpenJDK 6 and 7).
> The changes were too extensive to backport to JDK 5.
>
> http://bugs.sun.com/bugdatabase/search.do?process=1&category=java&bugStatus=10%2C11&subcategory=classes_util_concurrent&type=bug&keyword=ThreadPoolExecutor+%22Release+Fixed%2C+7%22
>
> Martin
>
> On Mon, Jun 2, 2008 at 3:21 AM, Moran Avigdor <moran at gigaspaces.com>  
> wrote:
>> Hi,
>>
>> I searched a bit on bugs opened related to corePoolSize, but did  
>> not find a
>> related post
>>
>> regarding corePoolThreads being removed from the cache, although they
>> shouldn't.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From robert.nicholson at gmail.com  Mon Jun  2 22:29:15 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Mon, 2 Jun 2008 21:29:15 -0500
Subject: [concurrency-interest] What's more reliable in 1.5 JRE with
	ThreadPoolExecutor?
Message-ID: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>

I have to deploy my application on a 1.5 JRE and it looks like a lot  
of reliability/consistency goes out the window with ThreadPoolExecutor  
if you deploy an application that uses a bounded queue.

So I'm currently thinking of using an unbounded queue with a semaphore  
to block the submitting thread. So my submitting thread simply passes  
a reference along to my tasks and at the completion of each tasks via  
finally I release the semaphore and the submitting thread acquires a  
permit prior to submission to the ExecutorService.

Will the above work reliably on a 1.5 JRE? Is there a better way that  
takes the blocking burden off the submitter but still doesn't blow out  
the heap should the rate of submission spike?

From alarmnummer at gmail.com  Tue Jun  3 03:46:44 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 3 Jun 2008 09:46:44 +0200
Subject: [concurrency-interest] What's more reliable in 1.5 JRE with
	ThreadPoolExecutor?
In-Reply-To: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>
References: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>
Message-ID: <1466c1d60806030046u21a043fds4b09ddf53f0fd7c5@mail.gmail.com>

Hi Robert,

you can create a custom RejectedExecutionHandler that is called when
the task can't be placed on the queue with an offer. As soon as the
handler is called, you do a workqueue.put.

Check the ScheduledThreadPoolExecutor for an example.

ps:
I also think it is a shame that blocking behavior is not part of the
Executor. That is why I wrote a BlockingExecutor that throws an
InterruptedException, and also allows to pass a timeout.

On Tue, Jun 3, 2008 at 4:29 AM, Robert Nicholson
<robert.nicholson at gmail.com> wrote:
> I have to deploy my application on a 1.5 JRE and it looks like a lot
> of reliability/consistency goes out the window with ThreadPoolExecutor
> if you deploy an application that uses a bounded queue.
>
> So I'm currently thinking of using an unbounded queue with a semaphore
> to block the submitting thread. So my submitting thread simply passes
> a reference along to my tasks and at the completion of each tasks via
> finally I release the semaphore and the submitting thread acquires a
> permit prior to submission to the ExecutorService.
>
> Will the above work reliably on a 1.5 JRE? Is there a better way that
> takes the blocking burden off the submitter but still doesn't blow out
> the heap should the rate of submission spike?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From joe.bowbeer at gmail.com  Tue Jun  3 04:11:50 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 3 Jun 2008 01:11:50 -0700
Subject: [concurrency-interest] What's more reliable in 1.5 JRE with
	ThreadPoolExecutor?
In-Reply-To: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>
References: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>
Message-ID: <31f2a7bd0806030111w5d535ba4yb773d6467bf6cfe1@mail.gmail.com>

On Mon, Jun 2, 2008 at 7:29 PM, Robert Nicholson <robert.nicholson at gmail.com>
wrote:

> I have to deploy my application on a 1.5 JRE and it looks like a lot
> of reliability/consistency goes out the window with ThreadPoolExecutor
> if you deploy an application that uses a bounded queue.
>
> So I'm currently thinking of using an unbounded queue with a semaphore
> to block the submitting thread. So my submitting thread simply passes
> a reference along to my tasks and at the completion of each tasks via
> finally I release the semaphore and the submitting thread acquires a
> permit prior to submission to the ExecutorService.
>
> Will the above work reliably on a 1.5 JRE? Is there a better way that
> takes the blocking burden off the submitter but still doesn't blow out
> the heap should the rate of submission spike?
>


Your executor + semaphore idea seems reasonable to me.  I tried my hand at
an implementation, below.

The semaphore is assumed to hold queueSize permits and to be fair, while the
executor is assumed to have a bounded number of threads and an unbounded
internal queue size (e.g., Executors.newFixedThreadPool).

The "scheduled" logic ensures the semaphore is released even if execute
fails.

Note that the semaphore is released prior to running. That way, only queued
tasks hold a permit.

class BlockingExecutor implements Executor {

    final Semaphore semaphore;
    final Executor executor;

    BlockingExecutor(Semaphore semaphore, Executor executor) {
        this.semaphore = semaphore;
        this.executor = executor;
    }

    public void execute(final Runnable r) {
        semaphore.acquireUninterruptibly();
        boolean scheduled = false;
        try {
            executor.execute(new Runnable() {
                public void run() {
                    semaphore.release();
                    r.run();
                }
            });
            scheduled = true;
        } finally {
            if (!scheduled)
               semaphore.release();
        }
     }
}

--
Joe Bowbeer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080603/244c7f9e/attachment.html 

From moran at gigaspaces.com  Tue Jun  3 05:33:36 2008
From: moran at gigaspaces.com (Moran Avigdor)
Date: Tue, 3 Jun 2008 12:33:36 +0300
Subject: [concurrency-interest] ThreadPoolExecutor goes below
	corePoolSize
In-Reply-To: <3623E06481E65B45866CB3AF32C4FA87013126E0@hercules.gspaces.com>
Message-ID: <3623E06481E65B45866CB3AF32C4FA8701312989@hercules.gspaces.com>

 

I remember there was a problem in 6.0, but I wasn't aware of problems in
1.5 in this regard.

Thank you for the replies - this obviously answers my question.

 

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6458662

 

 

________________________________

From: Moran Avigdor 
Sent: Monday, June 02, 2008 1:22 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] ThreadPoolExecutor goes below
corePoolSize

 

Hi,

I searched a bit on bugs opened related to corePoolSize, but did not
find a related post 

regarding corePoolThreads being removed from the cache, although they
shouldn't.

 

My test case below simulates a ThreadPool, which is constructed with a
min corePoolSize and a max pool size.

The idle wait time is set to 1 ms. I block my threads with a
CountDownLatch to ensure that threads scale from min to max (and are not
reused).

By releasing the latch I free up all the threads to complete their work,
and check that we did not go below core pool size.

 

This test sometimes fails on the assertion that we are below
corePoolSize:

junit.framework.AssertionFailedError: Expected minimum pool size.
expected:<2> but was:<1>

 

Is anyone familiar with such a scenario?

 

I am using JDK 1.5.0_12.

Regards,

Moran Avigdor

 

 

 

    public void testScaleDown() throws Exception {

        final int min = 2;

        final int max = 4;

        final CountDownLatch active = new CountDownLatch(max +1);

        

        ThreadPoolExecutor pool =
(ThreadPoolExecutor)Executors.newCachedThreadPool();

        pool.setCorePoolSize(min);

        pool.setMaximumPoolSize(max);

        pool.setKeepAliveTime(1, TimeUnit.MILLISECONDS);

 

        for (int i=0; i<max; ++i) {

            pool.execute( new Runnable() {

                public void run()

                {

                    try {

                        active.countDown();

                        active.await();

                    }catch (InterruptedException e) {

                        e.printStackTrace(); //ignore

                    }

                } 

            });

            //wait time to execute task by pool

            Thread.sleep(100);

        }

        

        Assert.assertEquals("Expected all threads to block. ", 1,
active.getCount());

        Assert.assertEquals("Expected active. ", max,
pool.getActiveCount());

        Assert.assertEquals("Expected max pool size. ", max,
pool.getPoolSize());

 

        //release the latch

        active.countDown();

        active.await();

        

        //wait around for one second for state to settle

        Thread.sleep(1000);

        Assert.assertEquals("Expected completion of all tasks. ", max,
pool.getCompletedTaskCount());

        Assert.assertEquals("Expected completion of all threads. ", 0,
pool.getActiveCount());

        Assert.assertEquals("Expected minimum pool size. ", min,
pool.getPoolSize());

    }

 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080603/12508da4/attachment-0001.html 

From robert.nicholson at gmail.com  Tue Jun  3 09:03:33 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Tue, 3 Jun 2008 08:03:33 -0500
Subject: [concurrency-interest] What's more reliable in 1.5 JRE with
	ThreadPoolExecutor?
In-Reply-To: <1466c1d60806030046u21a043fds4b09ddf53f0fd7c5@mail.gmail.com>
References: <BD268283-B74B-4F8A-976B-2B73FACEDA47@gmail.com>
	<1466c1d60806030046u21a043fds4b09ddf53f0fd7c5@mail.gmail.com>
Message-ID: <B6E04FCE-C474-44E8-850C-2B543051E0D8@gmail.com>

That's true but what I was saying though was that under 1.5. The  
Service doesn't execute all submitted tasks whenever you are using a  
bounded queue.

On Jun 3, 2008, at 2:46 AM, Peter Veentjer wrote:

> Hi Robert,
>
> you can create a custom RejectedExecutionHandler that is called when
> the task can't be placed on the queue with an offer. As soon as the
> handler is called, you do a workqueue.put.
>
> Check the ScheduledThreadPoolExecutor for an example.
>
> ps:
> I also think it is a shame that blocking behavior is not part of the
> Executor. That is why I wrote a BlockingExecutor that throws an
> InterruptedException, and also allows to pass a timeout.
>
> On Tue, Jun 3, 2008 at 4:29 AM, Robert Nicholson
> <robert.nicholson at gmail.com> wrote:
>> I have to deploy my application on a 1.5 JRE and it looks like a lot
>> of reliability/consistency goes out the window with  
>> ThreadPoolExecutor
>> if you deploy an application that uses a bounded queue.
>>
>> So I'm currently thinking of using an unbounded queue with a  
>> semaphore
>> to block the submitting thread. So my submitting thread simply passes
>> a reference along to my tasks and at the completion of each tasks via
>> finally I release the semaphore and the submitting thread acquires a
>> permit prior to submission to the ExecutorService.
>>
>> Will the above work reliably on a 1.5 JRE? Is there a better way that
>> takes the blocking burden off the submitter but still doesn't blow  
>> out
>> the heap should the rate of submission spike?
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


From alarmnummer at gmail.com  Tue Jun  3 09:12:44 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 3 Jun 2008 15:12:44 +0200
Subject: [concurrency-interest] A threadsafe list with a high number of
	writes and a low number of reads.
Message-ID: <1466c1d60806030612j7fcf08f1p9c6dd283c1f8ec94@mail.gmail.com>

The CopyOnWriteArrayList is a threadsafe list that is useful if the
number of reads is much larger than the number of writes. But what if
you have the requirement for a List where the number of writes is a
lot larger than the number of reads?

And if there are no List alternatives, are there other collection
classes that are better suited for a high percentage of writes?

From forax at univ-mlv.fr  Tue Jun  3 11:08:32 2008
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Tue, 03 Jun 2008 17:08:32 +0200
Subject: [concurrency-interest] A threadsafe list with a high number of
 writes and a low number of reads.
In-Reply-To: <1466c1d60806030612j7fcf08f1p9c6dd283c1f8ec94@mail.gmail.com>
References: <1466c1d60806030612j7fcf08f1p9c6dd283c1f8ec94@mail.gmail.com>
Message-ID: <48455E70.6010406@univ-mlv.fr>

Peter Veentjer a ?crit :
> The CopyOnWriteArrayList is a threadsafe list that is useful if the
> number of reads is much larger than the number of writes. But what if
> you have the requirement for a List where the number of writes is a
> lot larger than the number of reads?
>   
Making LinkedList a java.util.List was an error, by example
LinkedList list = ...
for(int i=0;i<list.size();i++)
  System.out.println(list.get(i));
is quadratic.

Java 1.5 introduces a new interface java.util.Queue,
a queue is a list without access using index.

One threadsafe queue is:
ConcurrentLinkedQueue ?

> And if there are no List alternatives, are there other collection
> classes that are better suited for a high percentage of writes?
>   
regards,
R?mi

From jdmarshall at gmail.com  Tue Jun  3 15:03:09 2008
From: jdmarshall at gmail.com (jason marshall)
Date: Tue, 3 Jun 2008 12:03:09 -0700
Subject: [concurrency-interest] A threadsafe list with a high number of
	writes and a low number of reads.
In-Reply-To: <48455E70.6010406@univ-mlv.fr>
References: <1466c1d60806030612j7fcf08f1p9c6dd283c1f8ec94@mail.gmail.com>
	<48455E70.6010406@univ-mlv.fr>
Message-ID: <3cf41bb90806031203x4cc22763w9c485cb84b8f838d@mail.gmail.com>

To take Remi's line of thinking and extend it, what features of List
are you relying on?  Read/write patterns aren't the only criteria
here.

Do you care about: Absolute position?  Relative position?  Multiple
inserts of the same values?  Are you adding items only, or deleting as
well?  Randomly, or head delete/tail insert?  Do you scan the list for
collisions/existence tests, or just walk it to perform actions?


-Jason


On Tue, Jun 3, 2008 at 8:08 AM, R?mi Forax <forax at univ-mlv.fr> wrote:
> Peter Veentjer a ?crit :
>> The CopyOnWriteArrayList is a threadsafe list that is useful if the
>> number of reads is much larger than the number of writes. But what if
>> you have the requirement for a List where the number of writes is a
>> lot larger than the number of reads?
>>
> Making LinkedList a java.util.List was an error, by example
> LinkedList list = ...
> for(int i=0;i<list.size();i++)
>  System.out.println(list.get(i));
> is quadratic.
>
> Java 1.5 introduces a new interface java.util.Queue,
> a queue is a list without access using index.
>
> One threadsafe queue is:
> ConcurrentLinkedQueue ?
>
>> And if there are no List alternatives, are there other collection
>> classes that are better suited for a high percentage of writes?
>>
> regards,
> R?mi
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
- Jason


From mmcgrady at topiatechnology.com  Tue Jun  3 21:13:39 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Tue, 3 Jun 2008 18:13:39 -0700
Subject: [concurrency-interest] Multiple Targets Queue Policy?
Message-ID: <F2880CA4-AF19-4DF3-ACDF-9FD4ACE88EB9@topiatechnology.com>

Is there a Java code construct (queue?) available that has the  
following policy: a queue that allows a specified number of multiple  
threaded users to use an item before that item is discarded?  This is  
to avoid the obvious sorts of out-of-memory problems that can arise  
when multiple users have different processing speeds.

Thanks for any assistance.

Mike

Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365





From dcholmes at optusnet.com.au  Wed Jun  4 00:32:51 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 4 Jun 2008 14:32:51 +1000
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <F2880CA4-AF19-4DF3-ACDF-9FD4ACE88EB9@topiatechnology.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEABHMAA.dcholmes@optusnet.com.au>

Hi Michael,

It is unclear to me what you are looking for. With any of the Java
collection classes you, the programmer, are in control of the items in the
collection: if you add it then it's there; while it's there you can access
it; if you remove it then it's no longer in the collection.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Michael
> McGrady
> Sent: Wednesday, 4 June 2008 11:14 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Multiple Targets Queue Policy?
>
>
> Is there a Java code construct (queue?) available that has the
> following policy: a queue that allows a specified number of multiple
> threaded users to use an item before that item is discarded?  This is
> to avoid the obvious sorts of out-of-memory problems that can arise
> when multiple users have different processing speeds.
>
> Thanks for any assistance.
>
> Mike
>
> Michael McGrady
> Senior Engineer
> mmcgrady at topiatechnology.com
> 1 (253) 720-3365
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From jed at atlassian.com  Wed Jun  4 02:17:52 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Wed, 04 Jun 2008 16:17:52 +1000
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <F2880CA4-AF19-4DF3-ACDF-9FD4ACE88EB9@topiatechnology.com>
References: <F2880CA4-AF19-4DF3-ACDF-9FD4ACE88EB9@topiatechnology.com>
Message-ID: <48463390.1040509@atlassian.com>

I recently wrote something that sounds similar to your needs (it is for 
multiplexing Input/OutputStream pairs) where a single producer thread 
would have multiple consumer threads. It is implemented with a blocking 
queue for each consumer thread and calls on the producer thread just add 
a element to each consumer's queue.

The code is here for SVN:
https://svn.atlassian.com/svn/public/atlassian/atlassian-stream-multiplexer/trunk 


or for browse via FishEye
http://svn.atlassian.com/fisheye/browse/public/atlassian/atlassian-stream-multiplexer/trunk/ 


where most of the interesting concurrency stuff is here
http://svn.atlassian.com/fisheye/browse/public/atlassian/atlassian-stream-multiplexer/trunk/src/main/java/com/atlassian/multiplexer/concurrent 


please note this project is still very young and doesn't have a release 
yet. Some or all of it will probably go into various Apache Commons 
projects at some point.

cheers,
jed.

Michael McGrady wrote:
> Is there a Java code construct (queue?) available that has the  
> following policy: a queue that allows a specified number of multiple  
> threaded users to use an item before that item is discarded?  This is  
> to avoid the obvious sorts of out-of-memory problems that can arise  
> when multiple users have different processing speeds.
>   


From mmcgrady at topiatechnology.com  Wed Jun  4 06:21:15 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Wed, 4 Jun 2008 03:21:15 -0700
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEABHMAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCCEABHMAA.dcholmes@optusnet.com.au>
Message-ID: <8655FA7D-00D5-4ECE-A65A-4119683A2EEE@topiatechnology.com>

I can see that my question makes it look like I am interested in a  
collection class.  My mistake.  I was not.  Thanks to those who  
assumed otherwise.  I appreciate the responses.  I think the  
decoration of a concurrent queue sounds like the ticket1

MG


Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




On Jun 3, 2008, at 9:32 PM, David Holmes wrote:

> Hi Michael,
>
> It is unclear to me what you are looking for. With any of the Java
> collection classes you, the programmer, are in control of the items  
> in the
> collection: if you add it then it's there; while it's there you can  
> access
> it; if you remove it then it's no longer in the collection.
>
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
>> Michael
>> McGrady
>> Sent: Wednesday, 4 June 2008 11:14 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Multiple Targets Queue Policy?
>>
>>
>> Is there a Java code construct (queue?) available that has the
>> following policy: a queue that allows a specified number of multiple
>> threaded users to use an item before that item is discarded?  This is
>> to avoid the obvious sorts of out-of-memory problems that can arise
>> when multiple users have different processing speeds.
>>
>> Thanks for any assistance.
>>
>> Mike
>>
>> Michael McGrady
>> Senior Engineer
>> mmcgrady at topiatechnology.com
>> 1 (253) 720-3365
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dcholmes at optusnet.com.au  Wed Jun  4 07:55:04 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 4 Jun 2008 21:55:04 +1000
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <8655FA7D-00D5-4ECE-A65A-4119683A2EEE@topiatechnology.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEACHMAA.dcholmes@optusnet.com.au>

In what way is a "concurrent queue" not a collection class ?

I'm afraid I'm none the wiser as to what you are actually looking for.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Michael
> McGrady
> Sent: Wednesday, 4 June 2008 8:21 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Multiple Targets Queue Policy?
> 
> 
> I can see that my question makes it look like I am interested in a  
> collection class.  My mistake.  I was not.  Thanks to those who  
> assumed otherwise.  I appreciate the responses.  I think the  
> decoration of a concurrent queue sounds like the ticket1
> 
> MG
> 
> 
> Michael McGrady
> Senior Engineer
> mmcgrady at topiatechnology.com
> 1 (253) 720-3365
> 
> 
> 
> 
> On Jun 3, 2008, at 9:32 PM, David Holmes wrote:
> 
> > Hi Michael,
> >
> > It is unclear to me what you are looking for. With any of the Java
> > collection classes you, the programmer, are in control of the items  
> > in the
> > collection: if you add it then it's there; while it's there you can  
> > access
> > it; if you remove it then it's no longer in the collection.
> >
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
> >> Michael
> >> McGrady
> >> Sent: Wednesday, 4 June 2008 11:14 AM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] Multiple Targets Queue Policy?
> >>
> >>
> >> Is there a Java code construct (queue?) available that has the
> >> following policy: a queue that allows a specified number of multiple
> >> threaded users to use an item before that item is discarded?  This is
> >> to avoid the obvious sorts of out-of-memory problems that can arise
> >> when multiple users have different processing speeds.
> >>
> >> Thanks for any assistance.
> >>
> >> Mike
> >>
> >> Michael McGrady
> >> Senior Engineer
> >> mmcgrady at topiatechnology.com
> >> 1 (253) 720-3365
> >>
> >>
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at altair.cs.oswego.edu
> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From mmcgrady at topiatechnology.com  Wed Jun  4 10:54:10 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Wed, 4 Jun 2008 07:54:10 -0700
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEACHMAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEACHMAA.dcholmes@optusnet.com.au>
Message-ID: <DDB17C01-78E8-4632-BB85-1F01CE9D0CFC@topiatechnology.com>

I will try again, David.  Thanks for your call for clarity.

The present code passes off each incoming data instance to multiple  
instances of a PooledExecutor wrapper that uses a LinkedQueue.  The  
problem is that the processing times of the different wrappers is  
different, leading to data piling up in one of the wrappers and  
throwing an out of memory exception.  What is needed is some way to  
throttle the data flow so that the slowest consumer queue does not  
blow up.

MG


Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




On Jun 4, 2008, at 4:55 AM, David Holmes wrote:

> In what way is a "concurrent queue" not a collection class ?
>
> I'm afraid I'm none the wiser as to what you are actually looking for.
>
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of  
>> Michael
>> McGrady
>> Sent: Wednesday, 4 June 2008 8:21 PM
>> To: dholmes at ieee.org
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Multiple Targets Queue Policy?
>>
>>
>> I can see that my question makes it look like I am interested in a
>> collection class.  My mistake.  I was not.  Thanks to those who
>> assumed otherwise.  I appreciate the responses.  I think the
>> decoration of a concurrent queue sounds like the ticket1
>>
>> MG
>>
>>
>> Michael McGrady
>> Senior Engineer
>> mmcgrady at topiatechnology.com
>> 1 (253) 720-3365
>>
>>
>>
>>
>> On Jun 3, 2008, at 9:32 PM, David Holmes wrote:
>>
>>> Hi Michael,
>>>
>>> It is unclear to me what you are looking for. With any of the Java
>>> collection classes you, the programmer, are in control of the items
>>> in the
>>> collection: if you add it then it's there; while it's there you can
>>> access
>>> it; if you remove it then it's no longer in the collection.
>>>
>>> David Holmes
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>> Michael
>>>> McGrady
>>>> Sent: Wednesday, 4 June 2008 11:14 AM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] Multiple Targets Queue Policy?
>>>>
>>>>
>>>> Is there a Java code construct (queue?) available that has the
>>>> following policy: a queue that allows a specified number of  
>>>> multiple
>>>> threaded users to use an item before that item is discarded?   
>>>> This is
>>>> to avoid the obvious sorts of out-of-memory problems that can arise
>>>> when multiple users have different processing speeds.
>>>>
>>>> Thanks for any assistance.
>>>>
>>>> Mike
>>>>
>>>> Michael McGrady
>>>> Senior Engineer
>>>> mmcgrady at topiatechnology.com
>>>> 1 (253) 720-3365
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at altair.cs.oswego.edu
>>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From alexdmiller at yahoo.com  Wed Jun  4 12:46:41 2008
From: alexdmiller at yahoo.com (Alex Miller)
Date: Wed, 4 Jun 2008 09:46:41 -0700 (PDT)
Subject: [concurrency-interest] Multiple Targets Queue Policy?
Message-ID: <233009.23908.qm@web32103.mail.mud.yahoo.com>

Micheal McGrady said: 

> I will try again, David.  Thanks for your call for clarity.
> 
> The present code passes off each incoming data instance to multiple  
> instances of a PooledExecutor wrapper that uses a LinkedQueue.  The  
> problem is that the processing times of the different wrappers is  
> different, leading to data piling up in one of the wrappers and  
> throwing an out of memory exception.  What is needed is some way to  
> throttle the data flow so that the slowest consumer queue does not  
> blow up.
> 
> MG

That sounds shocking familiar to the work-stealing stuff that the upcoming fork-join framework does (except much more primitive :).  Maybe you could make use of it to recast your code in terms of fork-join?  Probably would be a lot simpler.  To point at a relatively arbitrary starting point:  http://www.infoq.com/news/2008/03/fork_join

Alex Miller
Terracotta

From mmcgrady at topiatechnology.com  Wed Jun  4 13:14:48 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Wed, 4 Jun 2008 10:14:48 -0700
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <233009.23908.qm@web32103.mail.mud.yahoo.com>
References: <233009.23908.qm@web32103.mail.mud.yahoo.com>
Message-ID: <BA01ACA2-DC19-4146-9D35-FE2A6ED298C3@topiatechnology.com>

Thanks, will check this out.

Mike

Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




On Jun 4, 2008, at 9:46 AM, Alex Miller wrote:

> Micheal McGrady said:
>
>> I will try again, David.  Thanks for your call for clarity.
>>
>> The present code passes off each incoming data instance to multiple
>> instances of a PooledExecutor wrapper that uses a LinkedQueue.  The
>> problem is that the processing times of the different wrappers is
>> different, leading to data piling up in one of the wrappers and
>> throwing an out of memory exception.  What is needed is some way to
>> throttle the data flow so that the slowest consumer queue does not
>> blow up.
>>
>> MG
>
> That sounds shocking familiar to the work-stealing stuff that the  
> upcoming fork-join framework does (except much more primitive :).   
> Maybe you could make use of it to recast your code in terms of fork- 
> join?  Probably would be a lot simpler.  To point at a relatively  
> arbitrary starting point:  http://www.infoq.com/news/2008/03/fork_join
>
> Alex Miller
> Terracotta


From mmcgrady at topiatechnology.com  Wed Jun  4 16:32:33 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Wed, 4 Jun 2008 13:32:33 -0700
Subject: [concurrency-interest] Fwd: Multiple Targets Queue Policy?
References: <229992.72645.qm@web32108.mail.mud.yahoo.com>
Message-ID: <4C197816-5A21-4BC8-8321-08B937FC1E0F@topiatechnology.com>

Thanks, Alex.  I think that a module handling these types of problems  
along the lines you suggest would be useful.  Where do you think in  
the open source world they would best be fit?


Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




Begin forwarded message:

> From: Alex Miller <alexdmiller at yahoo.com>
> Date: June 4, 2008 12:10:47 PM PDT
> To: Michael McGrady <mmcgrady at topiatechnology.com>
> Subject: Re: Multiple Targets Queue Policy?
>
> Another solution would be to enqueue the work multiple times and  
> encode the processing to be done with each item.  So, if each Foo  
> needed to be diced, sliced, and smashed, you could enqueue Foo-dice,  
> Foo-slice, and Foo-smash, then let one pool of generic workers  
> handle pull the work and the processing to do on the work off the  
> queue.
>
> Sort of a classic problem with queue/worker based systems.  If you  
> create multiple worker pools you tend to get idle workers in one or  
> more pools.  This is good if you can actively tune or resize the  
> pools (but usually you can't).  The alternative is to make one big  
> pool and let any worker do any kind of processing.  That has its own  
> downsides of course, although they're usually more on the  
> programming model side.
>
> Alex
>
>
> ----- Original Message ----
> From: Michael McGrady <mmcgrady at topiatechnology.com>
> To: Alex Miller <alexdmiller at yahoo.com>
> Sent: Wednesday, June 4, 2008 1:55:47 PM
> Subject: Re: Multiple Targets Queue Policy?
>
> Very interesting.  Two Things:
>
> FIRST
>
> I need more problem clarification, I can see.  This case involves
> workers doing different things to the same data and processing at
> differ speeds.  So, the same input must be sent to all consumers.
>
> BUT, SECOND
>
> However, this does give me another idea.  I could imagine having,
> e.g., three consumers and having them each able to do the others work
> when waiting for another to finish.  Where the work could be
> apportioned like that, it would be a great solution.  Maybe data could
> be fed until some queue was a set number of data units behind and then
> the other two workers could switch jobs to get it caught up.
>
> Michael McGrady
> Senior Engineer
> mmcgrady at topiatechnology.com
> 1 (253) 720-3365
>
>
>
>
> On Jun 4, 2008, at 10:27 AM, Alex Miller wrote:
>
>> Brian Goetz went through more of the details at his JavaOne
>> presentation - basically you get N queues and each consumer pulls
>> from their own queue.  BUT, if a consumer runs out of work, he can
>> steal it from the back of another worker's queue.  He does a lot of
>> slick stuff to minimize contention on the queues so all gets and
>> puts are almost always uncontended.  Fork-join is a really nice API
>> over the top of all that.
>>
>> I wrote up my notes from Brian's talk at
>> http://tech.puredanger.com/2008/05/06/javaone-brian-goetz-on-concurrency-in-java-7/
>> if you're interested.
>>
>> Alex
>>
>>
>> ----- Original Message ----
>> From: Michael McGrady <mmcgrady at topiatechnology.com>
>> To: Alex Miller <alexdmiller at yahoo.com>
>> Cc: concurrency-interest at cs.oswego.edu
>> Sent: Wednesday, June 4, 2008 12:14:48 PM
>> Subject: Re: Multiple Targets Queue Policy?
>>
>> Thanks, will check this out.
>>
>> Mike
>>
>> Michael McGrady
>> Senior Engineer
>> mmcgrady at topiatechnology.com
>> 1 (253) 720-3365
>>
>>
>>
>>
>> On Jun 4, 2008, at 9:46 AM, Alex Miller wrote:
>>
>>> Micheal McGrady said:
>>>
>>>> I will try again, David.  Thanks for your call for clarity.
>>>>
>>>> The present code passes off each incoming data instance to multiple
>>>> instances of a PooledExecutor wrapper that uses a LinkedQueue.  The
>>>> problem is that the processing times of the different wrappers is
>>>> different, leading to data piling up in one of the wrappers and
>>>> throwing an out of memory exception.  What is needed is some way to
>>>> throttle the data flow so that the slowest consumer queue does not
>>>> blow up.
>>>>
>>>> MG
>>>
>>> That sounds shocking familiar to the work-stealing stuff that the
>>> upcoming fork-join framework does (except much more primitive :).
>>> Maybe you could make use of it to recast your code in terms of fork-
>>> join?  Probably would be a lot simpler.  To point at a relatively
>>> arbitrary starting point:  http://www.infoq.com/news/2008/03/
>>> fork_join
>>>
>>> Alex Miller
>>> Terracotta

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080604/6cd42bce/attachment.html 

From alexdmiller at yahoo.com  Wed Jun  4 16:43:20 2008
From: alexdmiller at yahoo.com (Alex Miller)
Date: Wed, 4 Jun 2008 13:43:20 -0700 (PDT)
Subject: [concurrency-interest] Fwd: Multiple Targets Queue Policy?
Message-ID: <153584.41619.qm@web32102.mail.mud.yahoo.com>

I don't think you need anything beyond what already exists in the  Java 5 Executor - just need to be slightly more clever about how you create your "task".  


----- Original Message ----
From: Michael McGrady <mmcgrady at topiatechnology.com>
To: Alex Miller <alexdmiller at yahoo.com>; concurrency-interest at cs.oswego.edu
Sent: Wednesday, June 4, 2008 3:32:33 PM
Subject: Fwd: Multiple Targets Queue Policy?

Thanks, Alex.  I think that a module handling these types of problems along the lines you suggest would be useful.  Where do you think in the open source world they would best be fit?



Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365





Begin forwarded message:

From: Alex Miller <alexdmiller at yahoo.com>
Date: June 4, 2008 12:10:47 PM PDT
To: Michael McGrady <mmcgrady at topiatechnology.com>
Subject: Re: Multiple Targets Queue Policy?

Another solution would be to enqueue the work multiple times and encode the processing to be done with each item.  So, if each Foo needed to be diced, sliced, and smashed, you could enqueue Foo-dice, Foo-slice, and Foo-smash, then let one pool of generic workers handle pull the work and the processing to do on the work off the queue.

Sort of a classic problem with queue/worker based systems.  If you create multiple worker pools you tend to get idle workers in one or more pools.  This is good if you can actively tune or resize the pools (but usually you can't).  The alternative is to make one big pool and let any worker do any kind of processing.  That has its own downsides of course, although they're usually more on the programming model side.

Alex


----- Original Message ----
From: Michael McGrady <mmcgrady at topiatechnology.com>
To: Alex Miller <alexdmiller at yahoo.com>
Sent: Wednesday, June 4, 2008 1:55:47 PM
Subject: Re: Multiple Targets Queue Policy?

Very interesting.  Two Things:

FIRST

I need more problem clarification, I can see.  This case involves  
workers doing different things to the same data and processing at  
differ speeds.  So, the same input must be sent to all consumers.

BUT, SECOND

However, this does give me another idea.  I could imagine having,  
e.g., three consumers and having them each able to do the others work  
when waiting for another to finish.  Where the work could be  
apportioned like that, it would be a great solution.  Maybe data could  
be fed until some queue was a set number of data units behind and then  
the other two workers could switch jobs to get it caught up.

Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




On Jun 4, 2008, at 10:27 AM, Alex Miller wrote:


Brian Goetz went through more of the details at his JavaOne  

presentation - basically you get N queues and each consumer pulls  

from their own queue.  BUT, if a consumer runs out of work, he can  

steal it from the back of another worker's queue.  He does a lot of  

slick stuff to minimize contention on the queues so all gets and  

puts are almost always uncontended.  Fork-join is a really nice API  

over the top of all that.


I wrote up my notes from Brian's talk at

http://tech.puredanger.com/2008/05/06/javaone-brian-goetz-on-concurrency-in-java-7/

if you're interested.


Alex



----- Original Message ----

From: Michael McGrady <mmcgrady at topiatechnology.com>

To: Alex Miller <alexdmiller at yahoo.com>

Cc: concurrency-interest at cs.oswego.edu

Sent: Wednesday, June 4, 2008 12:14:48 PM

Subject: Re: Multiple Targets Queue Policy?


Thanks, will check this out.


Mike


Michael McGrady

Senior Engineer

mmcgrady at topiatechnology.com

1 (253) 720-3365





On Jun 4, 2008, at 9:46 AM, Alex Miller wrote:


Micheal McGrady said:


I will try again, David.  Thanks for your call for clarity.


The present code passes off each incoming data instance to multiple

instances of a PooledExecutor wrapper that uses a LinkedQueue.  The

problem is that the processing times of the different wrappers is

different, leading to data piling up in one of the wrappers and

throwing an out of memory exception.  What is needed is some way to

throttle the data flow so that the slowest consumer queue does not

blow up.


MG


That sounds shocking familiar to the work-stealing stuff that the

upcoming fork-join framework does (except much more primitive :).

Maybe you could make use of it to recast your code in terms of fork-

join?  Probably would be a lot simpler.  To point at a relatively

arbitrary starting point:  http://www.infoq.com/news/2008/03/ 

fork_join


Alex Miller

Terracotta
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080604/8f040111/attachment-0001.html 

From tim at peierls.net  Wed Jun  4 17:53:09 2008
From: tim at peierls.net (Tim Peierls)
Date: Wed, 4 Jun 2008 17:53:09 -0400
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <233009.23908.qm@web32103.mail.mud.yahoo.com>
References: <233009.23908.qm@web32103.mail.mud.yahoo.com>
Message-ID: <63b4e4050806041453p13eadc4al4fc1e2f31e47b625@mail.gmail.com>

Be careful. The fork-join framework is only appropriate for use with pure
computational tasks. If tasks block for I/O, it won't work nicely.

--tim

On Wed, Jun 4, 2008 at 12:46 PM, Alex Miller <alexdmiller at yahoo.com> wrote:

> Micheal McGrady said:
>
> > I will try again, David.  Thanks for your call for clarity.
> >
> > The present code passes off each incoming data instance to multiple
> > instances of a PooledExecutor wrapper that uses a LinkedQueue.  The
> > problem is that the processing times of the different wrappers is
> > different, leading to data piling up in one of the wrappers and
> > throwing an out of memory exception.  What is needed is some way to
> > throttle the data flow so that the slowest consumer queue does not
> > blow up.
> >
> > MG
>
> That sounds shocking familiar to the work-stealing stuff that the upcoming
> fork-join framework does (except much more primitive :).  Maybe you could
> make use of it to recast your code in terms of fork-join?  Probably would be
> a lot simpler.  To point at a relatively arbitrary starting point:
> http://www.infoq.com/news/2008/03/fork_join
>
> Alex Miller
> Terracotta
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080604/fbf114a9/attachment.html 

From mmcgrady at topiatechnology.com  Wed Jun  4 18:13:35 2008
From: mmcgrady at topiatechnology.com (Michael McGrady)
Date: Wed, 4 Jun 2008 15:13:35 -0700
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <153584.41619.qm@web32102.mail.mud.yahoo.com>
References: <153584.41619.qm@web32102.mail.mud.yahoo.com>
Message-ID: <A9DA5AA7-A793-4297-A3B4-16F95DB8F1A6@topiatechnology.com>

Slightly more clever in the environment I am working is a bit more  
than usual.  We are creating tasks dynamically.  Nonetheless, I still  
agree with you that this would be a good place to expect a solution  
along the lines discussed.

Michael McGrady
Senior Engineer
mmcgrady at topiatechnology.com
1 (253) 720-3365




On Jun 4, 2008, at 1:43 PM, Alex Miller wrote:

> I don't think you need anything beyond what already exists in the   
> Java 5 Executor - just need to be slightly more clever about how you  
> create your "task".
>
> ----- Original Message ----
> From: Michael McGrady <mmcgrady at topiatechnology.com>
> To: Alex Miller <alexdmiller at yahoo.com>; concurrency-interest at cs.oswego.edu
> Sent: Wednesday, June 4, 2008 3:32:33 PM
> Subject: Fwd: Multiple Targets Queue Policy?
>
> Thanks, Alex.  I think that a module handling these types of  
> problems along the lines you suggest would be useful.  Where do you  
> think in the open source world they would best be fit?
>
>
> Michael McGrady
> Senior Engineer
> mmcgrady at topiatechnology.com
> 1 (253) 720-3365
>
>
>
>
> Begin forwarded message:
>
>> From: Alex Miller <alexdmiller at yahoo.com>
>> Date: June 4, 2008 12:10:47 PM PDT
>> To: Michael McGrady <mmcgrady at topiatechnology.com>
>> Subject: Re: Multiple Targets Queue Policy?
>>
>> Another solution would be to enqueue the work multiple times and  
>> encode the processing to be done with each item.  So, if each Foo  
>> needed to be diced, sliced, and smashed, you could enqueue Foo- 
>> dice, Foo-slice, and Foo-smash, then let one pool of generic  
>> workers handle pull the work and the processing to do on the work  
>> off the queue.
>>
>> Sort of a classic problem with queue/worker based systems.  If you  
>> create multiple worker pools you tend to get idle workers in one or  
>> more pools.  This is good if you can actively tune or resize the  
>> pools (but usually you can't).  The alternative is to make one big  
>> pool and let any worker do any kind of processing.  That has its  
>> own downsides of course, although they're usually more on the  
>> programming model side.
>>
>> Alex
>>
>>
>> ----- Original Message ----
>> From: Michael McGrady <mmcgrady at topiatechnology.com>
>> To: Alex Miller <alexdmiller at yahoo.com>
>> Sent: Wednesday, June 4, 2008 1:55:47 PM
>> Subject: Re: Multiple Targets Queue Policy?
>>
>> Very interesting.  Two Things:
>>
>> FIRST
>>
>> I need more problem clarification, I can see.  This case involves
>> workers doing different things to the same data and processing at
>> differ speeds.  So, the same input must be sent to all consumers.
>>
>> BUT, SECOND
>>
>> However, this does give me another idea.  I could imagine having,
>> e.g., three consumers and having them each able to do the others work
>> when waiting for another to finish.  Where the work could be
>> apportioned like that, it would be a great solution.  Maybe data  
>> could
>> be fed until some queue was a set number of data units behind and  
>> then
>> the other two workers could switch jobs to get it caught up.
>>
>> Michael McGrady
>> Senior Engineer
>> mmcgrady at topiatechnology.com
>> 1 (253) 720-3365
>>
>>
>>
>>
>> On Jun 4, 2008, at 10:27 AM, Alex Miller wrote:
>>
>>> Brian Goetz went through more of the details at his JavaOne
>>> presentation - basically you get N queues and each consumer pulls
>>> from their own queue.  BUT, if a consumer runs out of work, he can
>>> steal it from the back of another worker's queue.  He does a lot of
>>> slick stuff to minimize contention on the queues so all gets and
>>> puts are almost always uncontended.  Fork-join is a really nice API
>>> over the top of all that.
>>>
>>> I wrote up my notes from Brian's talk at
>>> http://tech.puredanger.com/2008/05/06/javaone-brian-goetz-on-concurrency-in-java-7/
>>> if you're interested.
>>>
>>> Alex
>>>
>>>
>>> ----- Original Message ----
>>> From: Michael McGrady <mmcgrady at topiatechnology.com>
>>> To: Alex Miller <alexdmiller at yahoo.com>
>>> Cc: concurrency-interest at cs.oswego.edu
>>> Sent: Wednesday, June 4, 2008 12:14:48 PM
>>> Subject: Re: Multiple Targets Queue Policy?
>>>
>>> Thanks, will check this out.
>>>
>>> Mike
>>>
>>> Michael McGrady
>>> Senior Engineer
>>> mmcgrady at topiatechnology.com
>>> 1 (253) 720-3365
>>>
>>>
>>>
>>>
>>> On Jun 4, 2008, at 9:46 AM, Alex Miller wrote:
>>>
>>>> Micheal McGrady said:
>>>>
>>>>> I will try again, David.  Thanks for your call for clarity.
>>>>>
>>>>> The present code passes off each incoming data instance to  
>>>>> multiple
>>>>> instances of a PooledExecutor wrapper that uses a LinkedQueue.   
>>>>> The
>>>>> problem is that the processing times of the different wrappers is
>>>>> different, leading to data piling up in one of the wrappers and
>>>>> throwing an out of memory exception.  What is needed is some way  
>>>>> to
>>>>> throttle the data flow so that the slowest consumer queue does not
>>>>> blow up.
>>>>>
>>>>> MG
>>>>
>>>> That sounds shocking familiar to the work-stealing stuff that the
>>>> upcoming fork-join framework does (except much more primitive :).
>>>> Maybe you could make use of it to recast your code in terms of  
>>>> fork-
>>>> join?  Probably would be a lot simpler.  To point at a relatively
>>>> arbitrary starting point:  http://www.infoq.com/news/2008/03/
>>>> fork_join
>>>>
>>>> Alex Miller
>>>> Terracotta
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080604/bf94f19c/attachment-0001.html 

From robert.nicholson at gmail.com  Wed Jun  4 21:11:56 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Wed, 4 Jun 2008 20:11:56 -0500
Subject: [concurrency-interest] Dispatching work that's executed in some
	predefined order
Message-ID: <9E576909-89C8-40CA-976B-C492AEE66BED@gmail.com>

Can somebody critique these two approaches?

You have a runnable with a work queue and it's work queue is processed  
in it's run method. How there is no interation.  Just poll for the  
first element and if there's no element run returns.

This runnable is then a task that's put in the work queue of an  
executor service such that the executor service manages the threads  
which are assigned to each task. The run method simply calls poll on  
the runnables work queue and takes the first item it can find in the  
work queue and executes that. So other items in the work queue will be  
processed the next time the executor service gives a thread to this  
runnable. Secondly the number of these tasks is unbounded ie. if one  
doesn't exist already (by some differentiator) it's created on the fly  
otherwise an existing one is used. I think the idea is that if the  
runnable isn't processing tasks it can still queue future submissions  
which are then processed next time run is called on the runnable from  
the executor service.

Contrast this approach with another that simply has N tasks running  
which poll a blocking queue for each task and a simple dispatcher  
based on mod some value is used to submit a job to each of these tasks.

In the first approach the runnable is short live. When a task's work  
queue is empty all references to the task are removed such that the  
task is elligble for collection.

In the second approach it never goes away and is predictably always  
available but limited to some bounds.

It seems to me that there is less overhead and management required if  
the runnable that processes the work queue is longer living and simply  
doesn't have to "come and go" so to speak and little is gained by  
having them being short lived.

Again the executor service's thread pool is used to assign threads  
that will service each of these runnables that each process their own  
work queues.







From alarmnummer at gmail.com  Thu Jun  5 05:53:47 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 5 Jun 2008 11:53:47 +0200
Subject: [concurrency-interest] Looking for information about performance
	impact of memory barriers on caches.
Message-ID: <1466c1d60806050253t6107057dn40c152c4b48b848f@mail.gmail.com>

Just out of professional curiosity,

are there any performance benchmarks on the influence of memory
barriers (caused by volatiles/synchronized blocks) on caches?

I know this is very application and architecture specific, but at the
moment it is completely unclear to me.

From khalil.bouhamza at gmail.com  Thu Jun  5 08:44:25 2008
From: khalil.bouhamza at gmail.com (Bouhamza Khalil)
Date: Thu, 5 Jun 2008 14:44:25 +0200
Subject: [concurrency-interest] Looking for information about
	performance impact of memory barriers on caches.
In-Reply-To: <1466c1d60806050253t6107057dn40c152c4b48b848f@mail.gmail.com>
References: <1466c1d60806050253t6107057dn40c152c4b48b848f@mail.gmail.com>
Message-ID: <19507f870806050544i4a7702f2r11933b3a018130ed@mail.gmail.com>

Paul Tyma published some numbers
http://mailinator.blogspot.com/2008/03/how-fast-is-java-volatile-or-atomic-or.html.
At the time I ran the benchmark on my laptop a dual core intel on
windows
using various 32bit JDKs, results were consitent. I would have expected
uncontended synchronized block cost to be a bit lower, it was roughly 15
times slower than strait static access. ReentrantLock performed roughly 10%
better  than a synchronized block.

On Thu, Jun 5, 2008 at 11:53 AM, Peter Veentjer <alarmnummer at gmail.com>
wrote:

> Just out of professional curiosity,
>
> are there any performance benchmarks on the influence of memory
> barriers (caused by volatiles/synchronized blocks) on caches?
>
> I know this is very application and architecture specific, but at the
> moment it is completely unclear to me.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080605/bc3a6aa1/attachment.html 

From joe.bowbeer at gmail.com  Thu Jun  5 13:38:07 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 5 Jun 2008 10:38:07 -0700
Subject: [concurrency-interest] Dispatching work that's executed in some
	predefined order
In-Reply-To: <9E576909-89C8-40CA-976B-C492AEE66BED@gmail.com>
References: <9E576909-89C8-40CA-976B-C492AEE66BED@gmail.com>
Message-ID: <31f2a7bd0806051038s3949a570k808d99f735e1db2f@mail.gmail.com>

On Wed, Jun 4, 2008 at 6:11 PM, Robert Nicholson wrote:
> Can somebody critique these two approaches?
>
> You have a runnable with a work queue and it's work queue is processed
> in it's run method. However there is no iteration.  Just poll for the
> first element and if there's no element run returns.
>
> This runnable is then a task that's put in the work queue of an
> executor service such that the executor service manages the threads
> which are assigned to each task. The run method simply calls poll on
> the runnables work queue and takes the first item it can find in the
> work queue and executes that. So other items in the work queue will be
> processed the next time the executor service gives a thread to this
> runnable. Secondly the number of these tasks is unbounded ie. if one
> doesn't exist already (by some differentiator) it's created on the fly
> otherwise an existing one is used. I think the idea is that if the
> runnable isn't processing tasks it can still queue future submissions
> which are then processed next time run is called on the runnable from
> the executor service.
>
> Contrast this approach with another that simply has N tasks running
> which poll a blocking queue for each task and a simple dispatcher
> based on mod some value is used to submit a job to each of these tasks.
>
> In the first approach the runnable is short live. When a task's work
> queue is empty all references to the task are removed such that the
> task is elligble for collection.
>
> In the second approach it never goes away and is predictably always
> available but limited to some bounds.
>
> It seems to me that there is less overhead and management required if
> the runnable that processes the work queue is longer living and simply
> doesn't have to "come and go" so to speak and little is gained by
> having them being short lived.
>
> Again the executor service's thread pool is used to assign threads
> that will service each of these runnables that each process their own
> work queues.
>

The second option sounds like a fixedThreadPool or a set of
singleThreadExecutors, where each worker thread is executing a single
runnable instance that operates on a private queue.

I don't understand the description of the first option.  In
particular, I don't understand how new short-lived tasks are created.

A code sketch of the first option could make this clearer.

--
Joe Bowbeer

From mbien at fh-landshut.de  Thu Jun  5 15:56:44 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Thu, 05 Jun 2008 21:56:44 +0200
Subject: [concurrency-interest] Project FishFarm;
	making a ForkJoinPool distributeable
Message-ID: <484844FC.5040305@fh-landshut.de>

Hello,

I am Michael Bien the project owner of https://fishfarm.dev.java.net/ a 
project to distribute tasks written in the fork-join framework over the 
network. The project is based on Shoal for the p2p communication and 
jsr166y for local distribution. We maintain a temporary fork of jsr166y 
because we made minor modifications to the ForkJoinPool to make it 
distributable over grids.

Our main goal is to distribute as simple but also as efficient as 
possible. We introduced the DistributedForkJoinPool which extends the 
ForkJoinPool. It is under the hood a member of a shoal peer group and 
allows the tasks to be stolen from other group members (work stealing 
concurrency pattern).  The distributed version behaves identical to the 
original fj-pool it will also work while being offline. That enables a 
dynamic cluster - you can add and remove nodes any time you wish. If you 
run your grid in one network you need even no additional configurations.

As I already said, I made minor modifications to the framework. I would 
like to discuss the changes here since if we can't contribute them back 
to the jsr our project will stay a proof of concept because fragmenting 
the framework makes in my opinion no sens and was never intended.

changes:
-made ForkJoinTask Serializeable
-added popQuedTask() to ForkJoinPool which returns a not-yet executed 
task and removes it from the pool (i am pretty sure this will not work 
under all conditions)
-added getTask() to Submission

all changes are tagged with '//NEW' + comment in the sources

Any chance to get this functionality integrated?

(I planned to attach a diff patch but I haven't found a tool which is 
able to diff svn vrs. cvs - I guess it was a mistake to commit the 
jsr166y fork into our own repository..)

FishFarm is not yet ready for production but we recently run a 24h test 
with 40k submitted tasks on 5 nodes successfully but there is still some 
work left (e.g cancelling tasks is not yet implemented).
Feel free to check out from svn and try it. There is a ready to run 
test.DistributedFibunacciTask and the worker nodes are startable via 
webstart: https://fishfarm.dev.java.net/demo (because of system tray 
bugs we currently require the early access java re update 10 version for 
the webstart app but this will change).

feedback is very appreciated ;)

best regards,

Michael


From ben_manes at yahoo.com  Thu Jun  5 17:54:20 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 5 Jun 2008 14:54:20 -0700 (PDT)
Subject: [concurrency-interest] Project FishFarm;
	making a ForkJoinPool distributeable
Message-ID: <138429.26330.qm@web38807.mail.mud.yahoo.com>

How far along is this project?

I wrote a distributed master/slave framework and (for fun) have a map-reduce abstraction prototyped.  I was hoping to add fork-join, similar to your message, when we moved to Java-6 (and could use the jsr lib).  So far my framework has been in production for 2 years, gone through performance reviews, etc.  I've always wanted to open source it, but never got the powers that be to approve.

If your interested, we can discuss such things off-line.


----- Original Message ----
From: Michael Bien <mbien at fh-landshut.de>
To: concurrency-interest at cs.oswego.edu
Cc: Ludwig Griebl <griebl at fh-landshut.de>; Adam Bien <abien at adam-bien.com>
Sent: Thursday, June 5, 2008 12:56:44 PM
Subject: [concurrency-interest] Project FishFarm; making a ForkJoinPool distributeable

Hello,

I am Michael Bien the project owner of https://fishfarm.dev.java.net/ a 
project to distribute tasks written in the fork-join framework over the 
network. The project is based on Shoal for the p2p communication and 
jsr166y for local distribution. We maintain a temporary fork of jsr166y 
because we made minor modifications to the ForkJoinPool to make it 
distributable over grids.

Our main goal is to distribute as simple but also as efficient as 
possible. We introduced the DistributedForkJoinPool which extends the 
ForkJoinPool. It is under the hood a member of a shoal peer group and 
allows the tasks to be stolen from other group members (work stealing 
concurrency pattern).  The distributed version behaves identical to the 
original fj-pool it will also work while being offline. That enables a 
dynamic cluster - you can add and remove nodes any time you wish. If you 
run your grid in one network you need even no additional configurations.

As I already said, I made minor modifications to the framework. I would 
like to discuss the changes here since if we can't contribute them back 
to the jsr our project will stay a proof of concept because fragmenting 
the framework makes in my opinion no sens and was never intended.

changes:
-made ForkJoinTask Serializeable
-added popQuedTask() to ForkJoinPool which returns a not-yet executed 
task and removes it from the pool (i am pretty sure this will not work 
under all conditions)
-added getTask() to Submission

all changes are tagged with '//NEW' + comment in the sources

Any chance to get this functionality integrated?

(I planned to attach a diff patch but I haven't found a tool which is 
able to diff svn vrs. cvs - I guess it was a mistake to commit the 
jsr166y fork into our own repository..)

FishFarm is not yet ready for production but we recently run a 24h test 
with 40k submitted tasks on 5 nodes successfully but there is still some 
work left (e.g cancelling tasks is not yet implemented).
Feel free to check out from svn and try it. There is a ready to run 
test.DistributedFibunacciTask and the worker nodes are startable via 
webstart: https://fishfarm.dev.java.net/demo (because of system tray 
bugs we currently require the early access java re update 10 version for 
the webstart app but this will change).

feedback is very appreciated ;)

best regards,

Michael

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080605/2a87e1d9/attachment.html 

From robertlazarski at gmail.com  Thu Jun  5 18:02:42 2008
From: robertlazarski at gmail.com (robert lazarski)
Date: Thu, 5 Jun 2008 19:02:42 -0300
Subject: [concurrency-interest] ScheduledFuture and Binary latch
Message-ID: <f87675ee0806051502y4ca2ff1ag2857b10583b787b0@mail.gmail.com>

Hi all, allow this code to explain:

public class ParseTask implements Runnable {

   private static final Logger _log = Logger.getLogger();
   private static final ThreadFactory factory = new
ONExceptionThreadFactory(new ONExceptionHandler());
   private static final ExecutorService executorService =
Executors.newSingleThreadScheduledExecutor(factory);
   private static final BinaryLatch binaryLatch = new BinaryLatch();

       public void run() {
           if (binaryLatch.isRunning()) {
               // skip task if its already running
               return;
           }
           Future future = executorService.submit(new ParseJob());
           try {
               // wait and timeout or return result
               future.get(300, TimeUnit.SECONDS);
           } catch (InterruptedException ex) {
              _log.error(ex.getMessage(), ex);
              // Re-assert the thread's interrupted status
              Thread.currentThread().interrupt();
              // We don't need the result, so cancel the task too
              future.cancel(true);
           } catch (Exception ex) {
              _log.error(ex.getMessage(), ex);
              throw ThreadUtils.launderThrowable(ex.getCause());
          }
       }
}

This ParseTask is started via a Servlet init() as:

ScheduledExecutorService ses = Executors.newSingleThreadScheduledExecutor();
ScheduledFuture <?> scheduledParseJob = ses.scheduleWithFixedDelay(new
ParseTask(),
                 0L, 1, TimeUnit.MINUTES);

Problem: I want to write a latch, perhaps a semaphore, that skips this
task if its already running. So I want to acquire some type of lock
when I start this task, skip the task if its already been started, and
release the lock when this single task is completed. CountDownLatch
doesn't seem right for a repeatable task. I've written a few versions
of BinaryLatch but nothing seems right to me. Any ideas?

Robert

From mbien at fh-landshut.de  Thu Jun  5 18:40:27 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Fri, 06 Jun 2008 00:40:27 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <138429.26330.qm@web38807.mail.mud.yahoo.com>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>
Message-ID: <48486B5B.7060802@fh-landshut.de>

Ben Manes wrote:
> How far along is this project?
>
> I wrote a distributed master/slave framework and (for fun) have a 
> map-reduce abstraction prototyped.  I was hoping to add fork-join, 
> similar to your message, when we moved to Java-6 (and could use the 
> jsr lib).  So far my framework has been in production for 2 years, 
> gone through performance reviews, etc.  I've always wanted to open 
> source it, but never got the powers that be to approve.
>
> If your interested, we can discuss such things off-line.
For sure we can discuss this off-line if you like. Feel free to contact 
me directly.
But keep in mind all the motivation to build FishFarm was actually doing 
distribution without introducing a new Framework ;) The idea is to make 
grid computing optional in case you are using the fj-framework anyway.

FishFarm is around 6 month old and is a freetime project, it was never 
in production since I would never introduce a project which is based on 
a fork of a jsr ;-)

-michael
>
> ----- Original Message ----
> From: Michael Bien <mbien at fh-landshut.de>
> To: concurrency-interest at cs.oswego.edu
> Sent: Thursday, June 5, 2008 12:56:44 PM
> Subject: [concurrency-interest] Project FishFarm; making a 
> ForkJoinPool distributeable
>
> Hello,
>
> I am Michael Bien the project owner of https://fishfarm.dev.java.net/ a
> project to distribute tasks written in the fork-join framework over the
> network. The project is based on Shoal for the p2p communication and
> jsr166y for local distribution. We maintain a temporary fork of jsr166y
> because we made minor modifications to the ForkJoinPool to make it
> distributable over grids.
>
> Our main goal is to distribute as simple but also as efficient as
> possible. We introduced the DistributedForkJoinPool which extends the
> ForkJoinPool. It is under the hood a member of a shoal peer group and
> allows the tasks to be stolen from other group members (work stealing
> concurrency pattern).  The distributed version behaves identical to the
> original fj-pool it will also work while being offline. That enables a
> dynamic cluster - you can add and remove nodes any time you wish. If you
> run your grid in one network you need even no additional configurations.
>
> As I already said, I made minor modifications to the framework. I would
> like to discuss the changes here since if we can't contribute them back
> to the jsr our project will stay a proof of concept because fragmenting
> the framework makes in my opinion no sens and was never intended.
>
> changes:
> -made ForkJoinTask Serializeable
> -added popQuedTask() to ForkJoinPool which returns a not-yet executed
> task and removes it from the pool (i am pretty sure this will not work
> under all conditions)
> -added getTask() to Submission
>
> all changes are tagged with '//NEW' + comment in the sources
>
> Any chance to get this functionality integrated?
>
> (I planned to attach a diff patch but I haven't found a tool which is
> able to diff svn vrs. cvs - I guess it was a mistake to commit the
> jsr166y fork into our own repository..)
>
> FishFarm is not yet ready for production but we recently run a 24h test
> with 40k submitted tasks on 5 nodes successfully but there is still some
> work left (e.g cancelling tasks is not yet implemented).
> Feel free to check out from svn and try it. There is a ready to run
> test.DistributedFibunacciTask and the worker nodes are startable via
> webstart: https://fishfarm.dev.java.net/demo (because of system tray
> bugs we currently require the early access java re update 10 version for
> the webstart app but this will change).
>
> feedback is very appreciated ;)
>
> best regards,
>
> Michael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu 
> <mailto:Concurrency-interest at altair.cs.oswego.edu>
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080606/d2304991/attachment-0001.html 

From joe.bowbeer at gmail.com  Thu Jun  5 18:51:44 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 5 Jun 2008 15:51:44 -0700
Subject: [concurrency-interest] ScheduledFuture and Binary latch
In-Reply-To: <f87675ee0806051502y4ca2ff1ag2857b10583b787b0@mail.gmail.com>
References: <f87675ee0806051502y4ca2ff1ag2857b10583b787b0@mail.gmail.com>
Message-ID: <31f2a7bd0806051551t716b26e3yf7ebcde4881c90f@mail.gmail.com>

On Thu, Jun 5, 2008 at 3:02 PM, robert lazarski wrote:
>
> Problem: I want to write a latch, perhaps a semaphore, that skips this
> task if its already running. So I want to acquire some type of lock
> when I start this task, skip the task if its already been started, and
> release the lock when this single task is completed. CountDownLatch
> doesn't seem right for a repeatable task. I've written a few versions
> of BinaryLatch but nothing seems right to me. Any ideas?
>

final AtomicBoolean isRunning = new AtomicBoolean()

if (isRunning.compareAndSet(false, true))
    try {
        // run task
    } finally {
        isRunning.set(false);
    }

From jnewsham at referentia.com  Thu Jun  5 18:57:33 2008
From: jnewsham at referentia.com (Jim Newsham)
Date: Thu, 5 Jun 2008 12:57:33 -1000
Subject: [concurrency-interest] ScheduledFuture and Binary latch
In-Reply-To: <f87675ee0806051502y4ca2ff1ag2857b10583b787b0@mail.gmail.com>
References: <f87675ee0806051502y4ca2ff1ag2857b10583b787b0@mail.gmail.com>
Message-ID: <04a701c8c75f$8de8bd80$8700a8c0@referentia.com>


I usually use an AtomicBoolean for this type of thing.

private AtomicBoolean running = new AtomicBoolean();

...


public void run() {
  if (running.compareAndSet(false, true)) {
    try {
      // ...do work...
    }
    finally {
      running.set(false);
    }
  }
}

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of robert lazarski
> Sent: Thursday, June 05, 2008 12:03 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] ScheduledFuture and Binary latch
> 
> Hi all, allow this code to explain:
> 
> public class ParseTask implements Runnable {
> 
>    private static final Logger _log = Logger.getLogger();
>    private static final ThreadFactory factory = new
> ONExceptionThreadFactory(new ONExceptionHandler());
>    private static final ExecutorService executorService =
> Executors.newSingleThreadScheduledExecutor(factory);
>    private static final BinaryLatch binaryLatch = new BinaryLatch();
> 
>        public void run() {
>            if (binaryLatch.isRunning()) {
>                // skip task if its already running
>                return;
>            }
>            Future future = executorService.submit(new ParseJob());
>            try {
>                // wait and timeout or return result
>                future.get(300, TimeUnit.SECONDS);
>            } catch (InterruptedException ex) {
>               _log.error(ex.getMessage(), ex);
>               // Re-assert the thread's interrupted status
>               Thread.currentThread().interrupt();
>               // We don't need the result, so cancel the task too
>               future.cancel(true);
>            } catch (Exception ex) {
>               _log.error(ex.getMessage(), ex);
>               throw ThreadUtils.launderThrowable(ex.getCause());
>           }
>        }
> }
> 
> This ParseTask is started via a Servlet init() as:
> 
> ScheduledExecutorService ses =
> Executors.newSingleThreadScheduledExecutor();
> ScheduledFuture <?> scheduledParseJob = ses.scheduleWithFixedDelay(new
> ParseTask(),
>                  0L, 1, TimeUnit.MINUTES);
> 
> Problem: I want to write a latch, perhaps a semaphore, that skips this
> task if its already running. So I want to acquire some type of lock
> when I start this task, skip the task if its already been started, and
> release the lock when this single task is completed. CountDownLatch
> doesn't seem right for a repeatable task. I've written a few versions
> of BinaryLatch but nothing seems right to me. Any ideas?
> 
> Robert
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




From ben_manes at yahoo.com  Thu Jun  5 19:14:44 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 5 Jun 2008 16:14:44 -0700 (PDT)
Subject: [concurrency-interest] Project FishFarm;
	making a ForkJoinPool distributeable
Message-ID: <203655.71203.qm@web38805.mail.mud.yahoo.com>

Well, my framework doesn't dictate the topology.  Its pretty pluggable - you can specify the dispatcher (we use JMS, or threads for tests), message transcoder (base64, zip), etc.  Basically it provides a distributed ThreadPoolExecutor with retrial, error handling, cancellation, etc and of course avoid all the lock contention issues.  Its pretty hard to convince adoption of a topology change in a stable production environment, making a dependency on Shoal a blocker for many people.  Otherwise, your project looks pretty interesting so far.


----- Original Message ----
From: Michael Bien <mbien at fh-landshut.de>
To: concurrency-interest at cs.oswego.edu; ben_manes at yahoo.com
Sent: Thursday, June 5, 2008 3:40:27 PM
Subject: Re: [concurrency-interest] Project FishFarm; making a ForkJoinPool distributeable

Ben Manes wrote: 
How
far along is this project?

I wrote a distributed master/slave framework and (for fun) have a
map-reduce abstraction prototyped.  I was hoping to add fork-join,
similar to your message, when we moved to Java-6 (and could use the jsr
lib).  So far my framework has been in production for 2 years, gone
through performance reviews, etc.  I've always wanted to open source
it, but never got the powers that be to approve.

If your interested, we can discuss such things off-line.

For sure we can discuss this off-line if you like. Feel free to contact
me directly.
But keep in mind all the motivation to build FishFarm was actually
doing distribution without introducing a new Framework ;) The idea is
to make grid computing optional in case you are using the fj-framework
anyway.

FishFarm is around 6 month old and is a freetime project, it was never
in production since I would never introduce a project which is based on
a fork of a jsr ;-)

-michael



-----
Original Message ----
From: Michael Bien <mbien at fh-landshut.de>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, June 5, 2008 12:56:44 PM
Subject: [concurrency-interest] Project FishFarm; making a ForkJoinPool
distributeable

Hello,

I am Michael Bien the project owner of https://fishfarm.dev.java.net/ a 
project to distribute tasks written in the fork-join framework over the 
network. The project is based on Shoal for the p2p communication and 
jsr166y for local distribution. We maintain a temporary fork of jsr166y 
because we made minor modifications to the ForkJoinPool to make it 
distributable over grids.

Our main goal is to distribute as simple but also as efficient as 
possible. We introduced the DistributedForkJoinPool which extends the 
ForkJoinPool. It is under the hood a member of a shoal peer group and 
allows the tasks to be stolen from other group members (work stealing 
concurrency pattern).  The distributed version behaves identical to the 
original fj-pool it will also work while being offline. That enables a 
dynamic cluster - you can add and remove nodes any time you wish. If
you 
run your grid in one network you need even no additional configurations.

As I already said, I made minor modifications to the framework. I would 
like to discuss the changes here since if we can't contribute them back 
to the jsr our project will stay a proof of concept because fragmenting 
the framework makes in my opinion no sens and was never intended.

changes:
-made ForkJoinTask Serializeable
-added popQuedTask() to ForkJoinPool which returns a not-yet executed 
task and removes it from the pool (i am pretty sure this will not work 
under all conditions)
-added getTask() to Submission

all changes are tagged with '//NEW' + comment in the sources

Any chance to get this functionality integrated?

(I planned to attach a diff patch but I haven't found a tool which is 
able to diff svn vrs. cvs - I guess it was a mistake to commit the 
jsr166y fork into our own repository..)

FishFarm is not yet ready for production but we recently run a 24h test 
with 40k submitted tasks on 5 nodes successfully but there is still
some 
work left (e.g cancelling tasks is not yet implemented).
Feel free to check out from svn and try it. There is a ready to run 
test.DistributedFibunacciTask and the worker nodes are startable via 
webstart: https://fishfarm.dev.java.net/demo (because of system tray 
bugs we currently require the early access java re update 10 version
for 
the webstart app but this will change).

feedback is very appreciated ;)

best regards,

Michael

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080605/7079677e/attachment.html 

From ben_manes at yahoo.com  Thu Jun  5 19:16:38 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 5 Jun 2008 16:16:38 -0700 (PDT)
Subject: [concurrency-interest] ScheduledFuture and Binary latch
Message-ID: <518527.43068.qm@web38802.mail.mud.yahoo.com>

I believe that you could use a flag (volatile boolean; AtomicBoolean) or a try-lock approach.


----- Original Message ----
From: robert lazarski <robertlazarski at gmail.com>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, June 5, 2008 3:02:42 PM
Subject: [concurrency-interest] ScheduledFuture and Binary latch

Hi all, allow this code to explain:

public class ParseTask implements Runnable {

   private static final Logger _log = Logger.getLogger();
   private static final ThreadFactory factory = new
ONExceptionThreadFactory(new ONExceptionHandler());
   private static final ExecutorService executorService =
Executors.newSingleThreadScheduledExecutor(factory);
   private static final BinaryLatch binaryLatch = new BinaryLatch();

       public void run() {
           if (binaryLatch.isRunning()) {
               // skip task if its already running
               return;
           }
           Future future = executorService.submit(new ParseJob());
           try {
               // wait and timeout or return result
               future.get(300, TimeUnit.SECONDS);
           } catch (InterruptedException ex) {
              _log.error(ex.getMessage(), ex);
              // Re-assert the thread's interrupted status
              Thread.currentThread().interrupt();
              // We don't need the result, so cancel the task too
              future.cancel(true);
           } catch (Exception ex) {
              _log.error(ex.getMessage(), ex);
              throw ThreadUtils.launderThrowable(ex.getCause());
          }
       }
}

This ParseTask is started via a Servlet init() as:

ScheduledExecutorService ses = Executors.newSingleThreadScheduledExecutor();
ScheduledFuture <?> scheduledParseJob = ses.scheduleWithFixedDelay(new
ParseTask(),
                 0L, 1, TimeUnit.MINUTES);

Problem: I want to write a latch, perhaps a semaphore, that skips this
task if its already running. So I want to acquire some type of lock
when I start this task, skip the task if its already been started, and
release the lock when this single task is completed. CountDownLatch
doesn't seem right for a repeatable task. I've written a few versions
of BinaryLatch but nothing seems right to me. Any ideas?

Robert
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080605/4e7d4e7d/attachment.html 

From conivek at gmail.com  Fri Jun  6 10:24:05 2008
From: conivek at gmail.com (Kevin Condon)
Date: Fri, 6 Jun 2008 10:24:05 -0400
Subject: [concurrency-interest] ScheduledFuture and Binary latch
Message-ID: <2e780ac60806060724t52315586q20b7217c1e2e58f7@mail.gmail.com>

Hi,

In addition to the recommendations for using AtomicBoolean, I also notice
that the use of scheduleWithFixedDelay() should in itself prevent
overlapping executions of the task without any checks in the task itselt.
According to the javadoc:  "Creates and executes a periodic action that
becomes enabled first after the given initial delay, and subsequently with
the given delay between the termination of one execution and the
commencement of the next."  The following test code confirms:

public class Tst {
  public static void main(String[] args) {
    ScheduledExecutorService exec =
      Executors.newSingleThreadScheduledExecutor();
    exec.scheduleWithFixedDelay(
      new LongRun(), 0L, 1000L, TimeUnit.MILLISECONDS);
  }

  private static class LongRun implements Runnable {
    private final AtomicInteger nextId = new AtomicInteger(0);

    public void run() {
      int id = nextId.getAndIncrement();
      System.out.println(new Date() + " " + id + " starting");
      try {
        Thread.sleep(10000L);
      } catch (InterruptedException ex) {
        // fall through and exit
      } finally {
        System.out.println(new Date() + " " + id + " done");
      }
    }
  }
}

Output indicates the task runs for 10 sec, then after a 1 sec delay it runs
again:
Fri Jun 06 10:19:24 EDT 2008 0 starting
Fri Jun 06 10:19:34 EDT 2008 0 done
Fri Jun 06 10:19:35 EDT 2008 1 starting
Fri Jun 06 10:19:45 EDT 2008 1 done
Fri Jun 06 10:19:46 EDT 2008 2 starting
Fri Jun 06 10:19:56 EDT 2008 2 done

Regards,
Kevin

From: "robert lazarski" <robertlazarski at gmail.com>
> To: concurrency-interest at cs.oswego.edu
> Date: Thu, 5 Jun 2008 19:02:42 -0300
> Subject: [concurrency-interest] ScheduledFuture and Binary latch
> Hi all, allow this code to explain:
>
> public class ParseTask implements Runnable {
>
>   private static final Logger _log = Logger.getLogger();
>   private static final ThreadFactory factory = new
> ONExceptionThreadFactory(new ONExceptionHandler());
>   private static final ExecutorService executorService =
> Executors.newSingleThreadScheduledExecutor(factory);
>   private static final BinaryLatch binaryLatch = new BinaryLatch();
>
>       public void run() {
>           if (binaryLatch.isRunning()) {
>               // skip task if its already running
>               return;
>           }
>           Future future = executorService.submit(new ParseJob());
>           try {
>               // wait and timeout or return result
>               future.get(300, TimeUnit.SECONDS);
>           } catch (InterruptedException ex) {
>              _log.error(ex.getMessage(), ex);
>              // Re-assert the thread's interrupted status
>              Thread.currentThread().interrupt();
>              // We don't need the result, so cancel the task too
>              future.cancel(true);
>           } catch (Exception ex) {
>              _log.error(ex.getMessage(), ex);
>              throw ThreadUtils.launderThrowable(ex.getCause());
>          }
>       }
> }
>
> This ParseTask is started via a Servlet init() as:
>
> ScheduledExecutorService ses =
> Executors.newSingleThreadScheduledExecutor();
> ScheduledFuture <?> scheduledParseJob = ses.scheduleWithFixedDelay(new
> ParseTask(),
>                 0L, 1, TimeUnit.MINUTES);
>
> Problem: I want to write a latch, perhaps a semaphore, that skips this
> task if its already running. So I want to acquire some type of lock
> when I start this task, skip the task if its already been started, and
> release the lock when this single task is completed. CountDownLatch
> doesn't seem right for a repeatable task. I've written a few versions
> of BinaryLatch but nothing seems right to me. Any ideas?
>
> Robert
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080606/c490e3a4/attachment-0001.html 

From conivek at gmail.com  Fri Jun  6 13:06:27 2008
From: conivek at gmail.com (Kevin Condon)
Date: Fri, 6 Jun 2008 13:06:27 -0400
Subject: [concurrency-interest] ScheduledFuture and Binary latch
Message-ID: <2e780ac60806061006uf8adabbm75487f5f0384b17e@mail.gmail.com>

Doh.  Now I see why you need the active task check.  Somehow missed the
submit in the ParseTask.run() ...

Regards,
Kevin


> From: "Kevin Condon" <conivek at gmail.com>
> To: concurrency-interest at cs.oswego.edu
> Date: Fri, 6 Jun 2008 10:24:05 -0400
> Subject: Re: [concurrency-interest] ScheduledFuture and Binary latch
> Hi,
>
> In addition to the recommendations for using AtomicBoolean, I also notice
> that the use of scheduleWithFixedDelay() should in itself prevent
> overlapping executions of the task without any checks in the task itselt.
> According to the javadoc:  "Creates and executes a periodic action that
> becomes enabled first after the given initial delay, and subsequently with
> the given delay between the termination of one execution and the
> commencement of the next."  The following test code confirms:
>
> public class Tst {
>   public static void main(String[] args) {
>     ScheduledExecutorService exec =
>       Executors.newSingleThreadScheduledExecutor();
>     exec.scheduleWithFixedDelay(
>       new LongRun(), 0L, 1000L, TimeUnit.MILLISECONDS);
>   }
>
>   private static class LongRun implements Runnable {
>     private final AtomicInteger nextId = new AtomicInteger(0);
>
>     public void run() {
>       int id = nextId.getAndIncrement();
>       System.out.println(new Date() + " " + id + " starting");
>       try {
>         Thread.sleep(10000L);
>       } catch (InterruptedException ex) {
>         // fall through and exit
>       } finally {
>         System.out.println(new Date() + " " + id + " done");
>       }
>     }
>   }
> }
>
> Output indicates the task runs for 10 sec, then after a 1 sec delay it runs
> again:
> Fri Jun 06 10:19:24 EDT 2008 0 starting
> Fri Jun 06 10:19:34 EDT 2008 0 done
> Fri Jun 06 10:19:35 EDT 2008 1 starting
> Fri Jun 06 10:19:45 EDT 2008 1 done
> Fri Jun 06 10:19:46 EDT 2008 2 starting
> Fri Jun 06 10:19:56 EDT 2008 2 done
>
> Regards,
> Kevin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080606/42197ad0/attachment.html 

From gregg at cytetech.com  Fri Jun  6 19:41:11 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 06 Jun 2008 18:41:11 -0500
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48486B5B.7060802@fh-landshut.de>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>
	<48486B5B.7060802@fh-landshut.de>
Message-ID: <4849CB17.40100@cytetech.com>

Michael Bien wrote:
> Ben Manes wrote:
>> How far along is this project?
>>
>> I wrote a distributed master/slave framework and (for fun) have a 
>> map-reduce abstraction prototyped.  I was hoping to add fork-join, 
>> similar to your message, when we moved to Java-6 (and could use the 
>> jsr lib).  So far my framework has been in production for 2 years, 
>> gone through performance reviews, etc.  I've always wanted to open 
>> source it, but never got the powers that be to approve.
>>
>> If your interested, we can discuss such things off-line.
> For sure we can discuss this off-line if you like. Feel free to contact 
> me directly.
> But keep in mind all the motivation to build FishFarm was actually doing 
> distribution without introducing a new Framework ;) The idea is to make 
> grid computing optional in case you are using the fj-framework anyway.
> 
> FishFarm is around 6 month old and is a freetime project, it was never 
> in production since I would never introduce a project which is based on 
> a fork of a jsr ;-)

I hope that everyone working in this direction with Java, remembers to pay 
attention to all the work that has gone into Jini to make this possible with 
security and all the other tools such as leases and transactions, already 
implemented and working to help track distributed resources effectively, 
including the security features of Java, extended in the JERI stack to help 
eliminate the impact of DOS attacks.

The current version of Jini is now the River Podling in the Apache Incubator.

Gregg Wonderly

From robert.nicholson at gmail.com  Fri Jun  6 20:52:18 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Fri, 6 Jun 2008 19:52:18 -0500
Subject: [concurrency-interest] Multiple Targets Queue Policy?
In-Reply-To: <DDB17C01-78E8-4632-BB85-1F01CE9D0CFC@topiatechnology.com>
References: <NFBBKALFDCPFIDBNKAPCKEACHMAA.dcholmes@optusnet.com.au>
	<DDB17C01-78E8-4632-BB85-1F01CE9D0CFC@topiatechnology.com>
Message-ID: <BC0A6B8E-D53F-4BA2-AA8D-07FAEC146AA1@gmail.com>

So do you have an unbounded queue?

If that's the case you might want to make it a finite bounds and add  
code to handle rejections

On Jun 4, 2008, at 9:54 AM, Michael McGrady wrote:

> I will try again, David.  Thanks for your call for clarity.
>
> The present code passes off each incoming data instance to multiple
> instances of a PooledExecutor wrapper that uses a LinkedQueue.  The
> problem is that the processing times of the different wrappers is
> different, leading to data piling up in one of the wrappers and
> throwing an out of memory exception.  What is needed is some way to
> throttle the data flow so that the slowest consumer queue does not
> blow up.
>
> MG
>
>
> Michael McGrady
> Senior Engineer
> mmcgrady at topiatechnology.com
> 1 (253) 720-3365
>
>
>
>
> On Jun 4, 2008, at 4:55 AM, David Holmes wrote:
>
>> In what way is a "concurrent queue" not a collection class ?
>>
>> I'm afraid I'm none the wiser as to what you are actually looking  
>> for.
>>
>> David Holmes
>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>> Michael
>>> McGrady
>>> Sent: Wednesday, 4 June 2008 8:21 PM
>>> To: dholmes at ieee.org
>>> Cc: concurrency-interest at cs.oswego.edu
>>> Subject: Re: [concurrency-interest] Multiple Targets Queue Policy?
>>>
>>>
>>> I can see that my question makes it look like I am interested in a
>>> collection class.  My mistake.  I was not.  Thanks to those who
>>> assumed otherwise.  I appreciate the responses.  I think the
>>> decoration of a concurrent queue sounds like the ticket1
>>>
>>> MG
>>>
>>>
>>> Michael McGrady
>>> Senior Engineer
>>> mmcgrady at topiatechnology.com
>>> 1 (253) 720-3365
>>>
>>>
>>>
>>>
>>> On Jun 3, 2008, at 9:32 PM, David Holmes wrote:
>>>
>>>> Hi Michael,
>>>>
>>>> It is unclear to me what you are looking for. With any of the Java
>>>> collection classes you, the programmer, are in control of the items
>>>> in the
>>>> collection: if you add it then it's there; while it's there you can
>>>> access
>>>> it; if you remove it then it's no longer in the collection.
>>>>
>>>> David Holmes
>>>>
>>>>> -----Original Message-----
>>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>> Michael
>>>>> McGrady
>>>>> Sent: Wednesday, 4 June 2008 11:14 AM
>>>>> To: concurrency-interest at cs.oswego.edu
>>>>> Subject: [concurrency-interest] Multiple Targets Queue Policy?
>>>>>
>>>>>
>>>>> Is there a Java code construct (queue?) available that has the
>>>>> following policy: a queue that allows a specified number of
>>>>> multiple
>>>>> threaded users to use an item before that item is discarded?
>>>>> This is
>>>>> to avoid the obvious sorts of out-of-memory problems that can  
>>>>> arise
>>>>> when multiple users have different processing speeds.
>>>>>
>>>>> Thanks for any assistance.
>>>>>
>>>>> Mike
>>>>>
>>>>> Michael McGrady
>>>>> Senior Engineer
>>>>> mmcgrady at topiatechnology.com
>>>>> 1 (253) 720-3365
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at altair.cs.oswego.edu
>>>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From mbien at fh-landshut.de  Sat Jun  7 09:01:19 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Sat, 07 Jun 2008 15:01:19 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <4849CB17.40100@cytetech.com>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>
	<48486B5B.7060802@fh-landshut.de> <4849CB17.40100@cytetech.com>
Message-ID: <484A869F.7090508@fh-landshut.de>

Gregg Wonderly wrote:
> Michael Bien wrote:
>> Ben Manes wrote:
>>> How far along is this project?
>>>
>>> I wrote a distributed master/slave framework and (for fun) have a 
>>> map-reduce abstraction prototyped.  I was hoping to add fork-join, 
>>> similar to your message, when we moved to Java-6 (and could use the 
>>> jsr lib).  So far my framework has been in production for 2 years, 
>>> gone through performance reviews, etc.  I've always wanted to open 
>>> source it, but never got the powers that be to approve.
>>>
>>> If your interested, we can discuss such things off-line.
>> For sure we can discuss this off-line if you like. Feel free to 
>> contact me directly.
>> But keep in mind all the motivation to build FishFarm was actually 
>> doing distribution without introducing a new Framework ;) The idea is 
>> to make grid computing optional in case you are using the 
>> fj-framework anyway.
>>
>> FishFarm is around 6 month old and is a freetime project, it was 
>> never in production since I would never introduce a project which is 
>> based on a fork of a jsr ;-)
>
> I hope that everyone working in this direction with Java, remembers to 
> pay attention to all the work that has gone into Jini to make this 
> possible with security and all the other tools such as leases and 
> transactions, already implemented and working to help track 
> distributed resources effectively, including the security features of 
> Java, extended in the JERI stack to help eliminate the impact of DOS 
> attacks.
>
> The current version of Jini is now the River Podling in the Apache 
> Incubator.
>
> Gregg Wonderly
>
Hello Gregg,

There are many distribution systems around and each of them has its use 
case. Java Spaces (and its commercial forks), Grid Gain, Terracotta, 
Apache River... and even more specialized and therefore not mainstream 
projects like Helios for Sunflow ray tracing tasks. Almost each of them 
introduced its own framework or lets say it in other words - IS a 
framework. (Terracotta is the exception which is a layer on top of the JVM)
 
FishFarm is no full featured distribution system, it concentrates 
currently only on the distribution of ForkJoinPools and has no framework 
at all (jsr166y is its framework). Again, it simple works if you 
exchange "new ForkJoinPool()" with "new DistributedForkJoinPool()" and 
put the jars in the classpath. It has currently no security at all since 
we haven't got the requirement yet to install it outside closed networks 
which are pretty common for clusters. We even decided to try it without 
transactions (there is no JEE in it just simple messaging based on Shoal 
which is based on JXTA) because the master simple does not care if 
worker succeed with the computation or got its requested work or not. 
The master simple puts all of its "stolen" work into a backup and copies 
it back into the pool when the pool is empty - it still behaves like a 
ForkJoinPool.

But my initial question was pretty general even FishFarm independent and 
could be used in any framework you like:
Is there interest in making ForkJoinTasks Serializeable and providing a 
mechanism for querying not yet executed tasks from a ForkJoinPool?

best regards,

Michael




From niko at alum.mit.edu  Sat Jun  7 09:13:39 2008
From: niko at alum.mit.edu (Niko Matsakis)
Date: Sat, 7 Jun 2008 15:13:39 +0200
Subject: [concurrency-interest] Searching for programs using
	java.util.concurrent
Message-ID: <A17FB314-688B-4E58-B94E-1C2120D002C5@alum.mit.edu>

Hello,

I am a PhD student researching new ways to verify thread safety.  I am  
looking for programs which use the java.util.concurrent classes to  
implement Fork-Join parallel processing.  Ideally, such examples would  
be realistic, but simple and self-contained.  I would like to use them  
to help validate my designs, etc.  I am also interested in other  
parallel Java programs that seem suitable, even if they don't use  
JSR-166 or follow a fork-join model.  Perhaps someone on this list has  
suggestions?  Thanks in advance for any assistance.

In case you are interested, the basic thrust of my research involves  
grouping together objects into disjoint sets called partitions.  A  
flow-sensitive analysis then checks that no two threads simultaneously  
use objects in the same partition in incompatible ways.  I would like  
to find sample programs both to validate my ideas in a practical  
setting and to help drive the implementation.


regards,
Niko Matsakis

From robert.nicholson at gmail.com  Sat Jun  7 12:22:16 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Sat, 7 Jun 2008 11:22:16 -0500
Subject: [concurrency-interest] When is logging a significant bottleneck?
Message-ID: <5fe4687a0806070922g7bfe70d9lb3bcd415d71e11df@mail.gmail.com>

When using ExecutorServices and dispatching work to a number of different
threads how often is logging to the same file appender from each "worker"
thread a significant bottleneck?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080607/e3ee52d5/attachment.html 

From edharned2002 at yahoo.com  Sat Jun  7 15:28:09 2008
From: edharned2002 at yahoo.com (Edward Harned)
Date: Sat, 7 Jun 2008 12:28:09 -0700 (PDT)
Subject: [concurrency-interest] Searching for programs using
	java.util.concurrent
In-Reply-To: <A17FB314-688B-4E58-B94E-1C2120D002C5@alum.mit.edu>
Message-ID: <822243.37347.qm@web35507.mail.mud.yahoo.com>


Take a look at open source Tymeac starting here:
http://coopsoft.com/JavaProduct.html

Tymeac uses the concurrency classes. It runs both as an internal server as well as with RMI.

In Tymeac you predefine the Queues with number of threads, etc. Tymeac separates (forks) the request into its parts, places the request into Queues for parallel processing and concatenates the results (joins) for the client.

It is well documented with plenty of samples.

Ed Harned


--- On Sat, 6/7/08, Niko Matsakis <niko at alum.mit.edu> wrote:

> From: Niko Matsakis <niko at alum.mit.edu>
> Subject: [concurrency-interest] Searching for programs using java.util.concurrent
> To: concurrency-interest at cs.oswego.edu
> Date: Saturday, June 7, 2008, 9:13 AM
> Hello,
> 
> I am a PhD student researching new ways to verify thread
> safety.  I am  
> looking for programs which use the java.util.concurrent
> classes to  
> implement Fork-Join parallel processing.  Ideally, such
> examples would  
> be realistic, but simple and self-contained.  I would like
> to use them  
> to help validate my designs, etc.  I am also interested in
> other  
> parallel Java programs that seem suitable, even if they
> don't use  
> JSR-166 or follow a fork-join model.  Perhaps someone on
> this list has  
> suggestions?  Thanks in advance for any assistance.
> 
> In case you are interested, the basic thrust of my research
> involves  
> grouping together objects into disjoint sets called
> partitions.  A  
> flow-sensitive analysis then checks that no two threads
> simultaneously  
> use objects in the same partition in incompatible ways.  I
> would like  
> to find sample programs both to validate my ideas in a
> practical  
> setting and to help drive the implementation.
> 
> 
> regards,
> Niko Matsakis
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


      

From martinrb at google.com  Sun Jun  8 17:30:48 2008
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 8 Jun 2008 14:30:48 -0700
Subject: [concurrency-interest] NavigableMap implementation bugs
Message-ID: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>

Doug and Josh and Chris,

(Chris, please file a bug)

TreeMap.navigableKeySet().subSet(x,y).remove(o)
fails because TreeMap.KeySet.subset calls the
TreeSet(NavigableMap<E,Object>)
constructor, and that requires an argument map
not containing arbitrary value objects for correctness.
This correctness is probably only
noticed in the return value from remove being incorrect.

In the case of ConcurrentSkipListMap, the correctness
issue is even more serious, since remove(existing element)
fails to remove it.

It's not obvious how to best fix it.  The TreeSet(NavigableMap)
and ConcurrentSkipListMap(NavigableMap) constructors
simply cannot be used to get views of non-empty maps.
It's clear that the usual straightforward but tedious solution
will work, but is there an elegant solution?

(Curiously, this is one of the rare times where
generics compiler warnings caught a genuine bug!
It's a mistake in this case to force a NavigableMap<K,V>
into a NavigableMap<K,Object>)

Most of the hits below are bugs:

 $ find -name '*Map.java' | xargs grep -E 'new (TreeSet|ConcurrentSkipListSet)'
./TreeMap.java:            return new TreeSet<E>(m.subMap(fromElement,
fromInclusive,
./TreeMap.java:            return new TreeSet<E>(m.headMap(toElement,
inclusive));
./TreeMap.java:            return new
TreeSet<E>(m.tailMap(fromElement, inclusive));
./TreeMap.java:            return new TreeSet(m.descendingMap());
./concurrent/ConcurrentSkipListMap.java:            return new
ConcurrentSkipListSet<E>
./concurrent/ConcurrentSkipListMap.java:            return new
ConcurrentSkipListSet<E>(m.headMap(toElement, inclusive));
./concurrent/ConcurrentSkipListMap.java:            return new
ConcurrentSkipListSet<E>(m.tailMap(fromElement, inclusive));
./concurrent/ConcurrentSkipListMap.java:            return new
ConcurrentSkipListSet(m.descendingMap());

Test case follows:

/*
 * Copyright 2008 Sun Microsystems, Inc.  All Rights Reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 *
 * This code is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 only, as
 * published by the Free Software Foundation.
 *
 * This code is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 * version 2 for more details (a copy is included in the LICENSE file that
 * accompanied this code).
 *
 * You should have received a copy of the GNU General Public License version
 * 2 along with this work; if not, write to the Free Software Foundation,
 * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 *
 * Please contact Sun Microsystems, Inc., 4150 Network Circle, Santa Clara,
 * CA 95054 USA or visit www.sun.com if you need additional information or
 * have any questions.
 */

/*
 * @test
 * @summary navigableMap.keyset().subSet().remove(x)
 * @author  Martin Buchholz
 */

import java.util.Collection;
import java.util.NavigableMap;
import java.util.TreeMap;
import java.util.concurrent.ConcurrentSkipListMap;

public class Bug12 {
    void test(String[] args) throws Throwable {
        testNavigableMap(new TreeMap<Integer,Integer>());
        testNavigableMap(new ConcurrentSkipListMap<Integer,Integer>());
    }

    void checkEmpty(Collection<?> c) {
        check(c.isEmpty());
        check(c.size() == 0);
        check(! c.iterator().hasNext());
    }

    void checkEmpty(NavigableMap<?,?> m) {
        check(m.isEmpty());
        check(m.size() == 0);
        checkEmpty(m.keySet());
        checkEmpty(m.values());
        checkEmpty(m.entrySet());
    }

    void testNavigableMap(NavigableMap<Integer,Integer> m) {
        System.out.printf("Testing %s%n", m.getClass());
        checkEmpty(m);
        equal(m.put(1,2),null);
        equal(m.put(1,3),2);
        equal(m.remove(1),3);
        checkEmpty(m);
        equal(m.put(1,2),null);
        check(m.navigableKeySet().remove(1));
        checkEmpty(m);
        equal(m.put(1,2),null);
        check(m.navigableKeySet().headSet(3).remove(1));
        checkEmpty(m);
        equal(m.put(1,2),null);
        check(m.navigableKeySet().tailSet(-3).remove(1));
        checkEmpty(m);
        equal(m.put(1,2),null);
        check(m.navigableKeySet().subSet(-3,3).remove(1));
        checkEmpty(m);
        equal(m.put(1,2),null);
        check(m.isEmpty());
    }

    //--------------------- Infrastructure ---------------------------
    volatile int passed = 0, failed = 0;
    void pass() {passed++;}
    void fail() {failed++; Thread.dumpStack();}
    void fail(String msg) {System.err.println(msg); fail();}
    void unexpected(Throwable t) {failed++; t.printStackTrace();}
    void check(boolean cond) {if (cond) pass(); else fail();}
    void equal(Object x, Object y) {
	if (x == null ? y == null : x.equals(y)) pass();
	else fail(x + " not equal to " + y);}
    public static void main(String[] args) throws Throwable {
        Class<?> k = new Object(){}.getClass().getEnclosingClass();
        try {k.getMethod("instanceMain",String[].class)
                .invoke( k.newInstance(), (Object) args);}
        catch (Throwable e) {throw e.getCause();}}
    public void instanceMain(String[] args) throws Throwable {
	try {test(args);} catch (Throwable t) {unexpected(t);}
	System.out.printf("%nPassed = %d, failed = %d%n%n", passed, failed);
	if (failed > 0) throw new AssertionError("Some tests failed");}
}

From fw at deneb.enyo.de  Mon Jun  9 08:03:14 2008
From: fw at deneb.enyo.de (Florian Weimer)
Date: Mon, 09 Jun 2008 14:03:14 +0200
Subject: [concurrency-interest] Out-of-order execution, in-order retirement
Message-ID: <87fxrmu871.fsf@mid.deneb.enyo.de>

Is there an existing utility class that I could use to implement this?

The canonical example is a block-based compressor: individual blocks can
be processed in parallel, but they must be written in the original
sequence to the output stream.

Would the JDK 7 fork/join framework help here?

From gergg at cox.net  Mon Jun  9 12:03:44 2008
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 09 Jun 2008 11:03:44 -0500
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <484A869F.7090508@fh-landshut.de>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>	<48486B5B.7060802@fh-landshut.de>
	<4849CB17.40100@cytetech.com> <484A869F.7090508@fh-landshut.de>
Message-ID: <484D5460.7000608@cox.net>

Michael Bien wrote:
> Gregg Wonderly wrote:
>> Michael Bien wrote:
>>> Ben Manes wrote:
>>>> How far along is this project?
>>>>
>>>> I wrote a distributed master/slave framework and (for fun) have a 
>>>> map-reduce abstraction prototyped.  I was hoping to add fork-join, 
>>>> similar to your message, when we moved to Java-6 (and could use the 
>>>> jsr lib).  So far my framework has been in production for 2 years, 
>>>> gone through performance reviews, etc.  I've always wanted to open 
>>>> source it, but never got the powers that be to approve.
>>>>
>>>> If your interested, we can discuss such things off-line.
>>> For sure we can discuss this off-line if you like. Feel free to 
>>> contact me directly.
>>> But keep in mind all the motivation to build FishFarm was actually 
>>> doing distribution without introducing a new Framework ;) The idea is 
>>> to make grid computing optional in case you are using the 
>>> fj-framework anyway.
>>>
>>> FishFarm is around 6 month old and is a freetime project, it was 
>>> never in production since I would never introduce a project which is 
>>> based on a fork of a jsr ;-)
>> I hope that everyone working in this direction with Java, remembers to 
>> pay attention to all the work that has gone into Jini to make this 
>> possible with security and all the other tools such as leases and 
>> transactions, already implemented and working to help track 
>> distributed resources effectively, including the security features of 
>> Java, extended in the JERI stack to help eliminate the impact of DOS 
>> attacks.
>>
>> The current version of Jini is now the River Podling in the Apache 
>> Incubator.
>>
>> Gregg Wonderly
>>
> Hello Gregg,
> 
> There are many distribution systems around and each of them has its use 
> case. Java Spaces (and its commercial forks), Grid Gain, Terracotta, 
> Apache River... and even more specialized and therefore not mainstream 
> projects like Helios for Sunflow ray tracing tasks. Almost each of them 
> introduced its own framework or lets say it in other words - IS a 
> framework. (Terracotta is the exception which is a layer on top of the JVM)
>  
> FishFarm is no full featured distribution system, it concentrates 
> currently only on the distribution of ForkJoinPools and has no framework 
> at all (jsr166y is its framework). Again, it simple works if you 
> exchange "new ForkJoinPool()" with "new DistributedForkJoinPool()" and 
> put the jars in the classpath.

One the more interesting issues is that Remote computational tasks have a unique 
remote communications requirement that has to be accounted for in some way. 
Partial failure and security usually become issues faster than most think, and 
as you start to deal with those issues, you end up reinventing the wheel because 
of all the work already done to deal with those issues.

In the end, it comes down to APIs and some simple algorithmic choices.  All of 
the issues that tool sets such as Jini deal with are real and important to 
manage at some level.  The JERI stack, finally provides a unique opportunity to 
optimize and tune the RPC stack, dynamically, at deployment time, instead of in 
your code.  The Jini Configuration interface and the ConfigurationFile 
implementation allow you to change the Exporter instance you deploy with for 
example, so that you can put TCP vs SSL, No-Auth vs X.500 vs Kerbos 
authentication and security constraints all into a configuration step instead of 
a programming/reprogramming task.

Many people still look at Jini as "tied" to "RMI".  It is tied to the RMI 
programming model, and through the use of JERI, you can separate most language 
issues for remote interaction (HTTP endpoint with an XML invocation layer 
factory for example).

 > It has currently no security at all since
> we haven't got the requirement yet to install it outside closed networks 
> which are pretty common for clusters. We even decided to try it without 
> transactions (there is no JEE in it just simple messaging based on Shoal 
> which is based on JXTA) because the master simple does not care if 
> worker succeed with the computation or got its requested work or not. 
> The master simple puts all of its "stolen" work into a backup and copies 
> it back into the pool when the pool is empty - it still behaves like a 
> ForkJoinPool.

Simple is easy to start with.  There was a study done on creating a JXTA 
endpoint for the JERI stack.  I've not used it.  The transactional services in 
Jini are based on a simple 2PC model that is implemented by the Mahalo service 
in the River podling (and the previously released Jini Technology Starter Kit v2.1).

The ability to get dependable and predictable results and see progress, seems 
like an important thing to me.  So I'm not sure that I understand how this 
framework can support that.

The Javaspaces master/worker pattern is pretty well proven as a distributed 
system for massive scaling.  Creating a worker that uses the ForkJoin stuff to 
further divide work locally would be an added mechanism for further exploiting 
the power available.

> But my initial question was pretty general even FishFarm independent and 
> could be used in any framework you like:
> Is there interest in making ForkJoinTasks Serializeable and providing a 
> mechanism for querying not yet executed tasks from a ForkJoinPool?

I think these two things have some merit.  Serializable implies a number of 
issues about the underlying references as well though.

Gregg Wonderly

From dl at cs.oswego.edu  Mon Jun  9 15:02:48 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 09 Jun 2008 15:02:48 -0400
Subject: [concurrency-interest] NavigableMap implementation bugs
In-Reply-To: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
Message-ID: <484D7E58.8020500@cs.oswego.edu>

Martin Buchholz wrote:

> TreeMap.navigableKeySet().subSet(x,y).remove(o)
> fails because TreeMap.KeySet.subset calls the
> TreeSet(NavigableMap<E,Object>)
> constructor

Thanks for finding this!

> In the case of ConcurrentSkipListMap, the correctness
> issue is even more serious, since remove(existing element)
> fails to remove it.
> 
> It's not obvious how to best fix it.  The TreeSet(NavigableMap)
> and ConcurrentSkipListMap(NavigableMap) constructors
> simply cannot be used to get views of non-empty maps.
> It's clear that the usual straightforward but tedious solution
> will work, but is there an elegant solution?

I don't think so. Relaying the subset methods to the *Set classes
was just an implementation expediency under the mis-thought that they
would take care of recursive subsets etc rather than needing
special implementations for KeySets. But the byproduct for remove()
is clearly wrong so they do need separate implementations.

-Doug




From gregg at cytetech.com  Mon Jun  9 15:21:55 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 09 Jun 2008 14:21:55 -0500
Subject: [concurrency-interest] Searching for programs using
	java.util.concurrent
In-Reply-To: <A17FB314-688B-4E58-B94E-1C2120D002C5@alum.mit.edu>
References: <A17FB314-688B-4E58-B94E-1C2120D002C5@alum.mit.edu>
Message-ID: <484D82D3.5060007@cytetech.com>

This article is quite telling of the importance of things like the fork/join 
stuff for making effective use of losely coupled computing engines.  The fact 
that there are 3 different processor types on board makes it clear that certain 
types of calculations can and should be optimized by implementation details of 
"platforms" instead of by programmers directly.

How cool it would be to have one of these are your finger tips with fork/join as 
the programming control implementation!

<http://www.nytimes.com/2008/06/09/technology/09petaflops.html?_r=1&oref=slogin>

Gregg Wonderly

From osvaldo at visionnaire.com.br  Mon Jun  9 17:09:52 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Mon, 09 Jun 2008 18:09:52 -0300
Subject: [concurrency-interest] NavigableMap implementation bugs
In-Reply-To: <484D7E58.8020500@cs.oswego.edu>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu>
Message-ID: <484D9C20.3070607@visionnaire.com.br>

Doug Lea wrote:
> I don't think so. Relaying the subset methods to the *Set classes
> was just an implementation expediency under the mis-thought that they
> would take care of recursive subsets etc rather than needing
> special implementations for KeySets. But the byproduct for remove()
> is clearly wrong so they do need separate implementations.
>    
This remembers me from an old pet peeve, the fact that java.util.HashSet 
is implemented as a wrapper over java.util.HashMap. This sucks 
performance-wise, due to the additional indirections everywhere, and 
even more important, because every entry single object is larger than 
necessary - it contains a key and a value fields, but a single field 
would be necessary for HashSet. If you have a Set with millions of 
entries, the overhead of one useless pointer per entry is significant. 
Plus, the wrapping implementation uses a dummy, fixed Object instance as 
the value for all entries - and this may create another subtle 
performance problem: large numbers of "cross-space" references in the 
heap. I know that in most GCs only old-to-young refs are evil, but it 
seems that in some GCs any cross-space reference is bad (requires remset 
tracking), like Detlef et al's new "Garbage-First" collector (if I 
understood it).

Back in J2SE1.2 time, the runtime savings from the wrapping 
implementation could be significant... but, today? The HashMap* classes 
inside the latest rt.jar are 18,5Kb total (uncompressed), versus 3Kb 
from HashSet. That's a 12Kb of space saving, plus some small saving in 
JIT time too. I'd say that this is an irrelevant advantage, even in 
JavaME. The performance bonus of a tuned HashSet, however modest, would 
certainly be much more significant. I know it's ugly from a maintenance 
POV to have two large classes that will be 90% identical, but to 
paraphrase the movie Some Like It Hot, nothing is perfect...

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223


From josh at bloch.us  Mon Jun  9 17:43:07 2008
From: josh at bloch.us (Joshua Bloch)
Date: Mon, 9 Jun 2008 14:43:07 -0700
Subject: [concurrency-interest] NavigableMap implementation bugs
In-Reply-To: <484D9C20.3070607@visionnaire.com.br>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
Message-ID: <b097ac510806091443m27ca7e2ftc4e44c6e88b6de3d@mail.gmail.com>

Osvaldo,

Yes, I must say that I'm surprised that my original implementation of
HashSet still stands.  It would be interesting to see what sort of
performance improvement (time and space) we could get with a freestanding
HashSet implementation.

     Josh

On Mon, Jun 9, 2008 at 2:09 PM, Osvaldo Pinali Doederlein <
osvaldo at visionnaire.com.br> wrote:

> Doug Lea wrote:
> > I don't think so. Relaying the subset methods to the *Set classes
> > was just an implementation expediency under the mis-thought that they
> > would take care of recursive subsets etc rather than needing
> > special implementations for KeySets. But the byproduct for remove()
> > is clearly wrong so they do need separate implementations.
> >
> This remembers me from an old pet peeve, the fact that java.util.HashSet
> is implemented as a wrapper over java.util.HashMap. This sucks
> performance-wise, due to the additional indirections everywhere, and
> even more important, because every entry single object is larger than
> necessary - it contains a key and a value fields, but a single field
> would be necessary for HashSet. If you have a Set with millions of
> entries, the overhead of one useless pointer per entry is significant.
> Plus, the wrapping implementation uses a dummy, fixed Object instance as
> the value for all entries - and this may create another subtle
> performance problem: large numbers of "cross-space" references in the
> heap. I know that in most GCs only old-to-young refs are evil, but it
> seems that in some GCs any cross-space reference is bad (requires remset
> tracking), like Detlef et al's new "Garbage-First" collector (if I
> understood it).
>
> Back in J2SE1.2 time, the runtime savings from the wrapping
> implementation could be significant... but, today? The HashMap* classes
> inside the latest rt.jar are 18,5Kb total (uncompressed), versus 3Kb
> from HashSet. That's a 12Kb of space saving, plus some small saving in
> JIT time too. I'd say that this is an irrelevant advantage, even in
> JavaME. The performance bonus of a tuned HashSet, however modest, would
> certainly be much more significant. I know it's ugly from a maintenance
> POV to have two large classes that will be 90% identical, but to
> paraphrase the movie Some Like It Hot, nothing is perfect...
>
> A+
> Osvaldo
>
> --
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080609/ca58ae9a/attachment.html 

From martinrb at google.com  Mon Jun  9 18:01:05 2008
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 9 Jun 2008 15:01:05 -0700
Subject: [concurrency-interest] NavigableMap implementation bugs
In-Reply-To: <484D9C20.3070607@visionnaire.com.br>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
Message-ID: <1ccfd1c10806091501k2048a202ha37c2f3c5255cf22@mail.gmail.com>

Osvaldo, I fundamentally agree with you that
optimizing HashSet in the manner you describe
is worthwhile, but because the implementation
needs to "parallel" HashMap, it would be good
to do this after the current hacking activity on
HashMap quiesces.

Unfortunately, this kind of programming
is pure tedium - who will volunteer?

Martin

On Mon, Jun 9, 2008 at 2:09 PM, Osvaldo Pinali Doederlein
<osvaldo at visionnaire.com.br> wrote:
> Doug Lea wrote:
>>
>> I don't think so. Relaying the subset methods to the *Set classes
>> was just an implementation expediency under the mis-thought that they
>> would take care of recursive subsets etc rather than needing
>> special implementations for KeySets. But the byproduct for remove()
>> is clearly wrong so they do need separate implementations.
>>
>
> This remembers me from an old pet peeve, the fact that java.util.HashSet is
> implemented as a wrapper over java.util.HashMap. This sucks
> performance-wise, due to the additional indirections everywhere, and even
> more important, because every entry single object is larger than necessary -
> it contains a key and a value fields, but a single field would be necessary
> for HashSet. If you have a Set with millions of entries, the overhead of one
> useless pointer per entry is significant. Plus, the wrapping implementation
> uses a dummy, fixed Object instance as the value for all entries - and this
> may create another subtle performance problem: large numbers of
> "cross-space" references in the heap. I know that in most GCs only
> old-to-young refs are evil, but it seems that in some GCs any cross-space
> reference is bad (requires remset tracking), like Detlef et al's new
> "Garbage-First" collector (if I understood it).
>
> Back in J2SE1.2 time, the runtime savings from the wrapping implementation
> could be significant... but, today? The HashMap* classes inside the latest
> rt.jar are 18,5Kb total (uncompressed), versus 3Kb from HashSet. That's a
> 12Kb of space saving, plus some small saving in JIT time too. I'd say that
> this is an irrelevant advantage, even in JavaME. The performance bonus of a
> tuned HashSet, however modest, would certainly be much more significant. I
> know it's ugly from a maintenance POV to have two large classes that will be
> 90% identical, but to paraphrase the movie Some Like It Hot, nothing is
> perfect...
>
> A+
> Osvaldo
>
> --
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>
>


From dcholmes at optusnet.com.au  Mon Jun  9 18:34:15 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 10 Jun 2008 08:34:15 +1000
Subject: [concurrency-interest] Out-of-order execution,
	in-order retirement
In-Reply-To: <87fxrmu871.fsf@mid.deneb.enyo.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEBEHMAA.dcholmes@optusnet.com.au>

Florian,

Can you not simply store the result into a queue that is sorted on the
appropriate criteria? You could do an invokeAll to submit the set of tasks
and wait until they have all completed, check the results and then pass the
ordered queue to the end-consumer.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Florian
> Weimer
> Sent: Monday, 9 June 2008 10:03 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Out-of-order execution, in-order
> retirement
>
>
> Is there an existing utility class that I could use to implement this?
>
> The canonical example is a block-based compressor: individual blocks can
> be processed in parallel, but they must be written in the original
> sequence to the output stream.
>
> Would the JDK 7 fork/join framework help here?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From josh at bloch.us  Mon Jun  9 19:20:53 2008
From: josh at bloch.us (Joshua Bloch)
Date: Mon, 9 Jun 2008 16:20:53 -0700
Subject: [concurrency-interest] NavigableMap implementation bugs
In-Reply-To: <1ccfd1c10806091501k2048a202ha37c2f3c5255cf22@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<1ccfd1c10806091501k2048a202ha37c2f3c5255cf22@mail.gmail.com>
Message-ID: <b097ac510806091620t576a915dg38403d4d95739b96@mail.gmail.com>

Oooh, ooh, pick me!

       Josh

P.S. It isn't as tedious as you think.  Doing the obvious transformation on
HashMap gives you a benchmark.  Then you try to beat it.



On Mon, Jun 9, 2008 at 3:01 PM, Martin Buchholz <martinrb at google.com> wrote:

> Osvaldo, I fundamentally agree with you that
> optimizing HashSet in the manner you describe
> is worthwhile, but because the implementation
> needs to "parallel" HashMap, it would be good
> to do this after the current hacking activity on
> HashMap quiesces.
>
> Unfortunately, this kind of programming
> is pure tedium - who will volunteer?
>
> Martin
>
> On Mon, Jun 9, 2008 at 2:09 PM, Osvaldo Pinali Doederlein
> <osvaldo at visionnaire.com.br> wrote:
>  > Doug Lea wrote:
> >>
> >> I don't think so. Relaying the subset methods to the *Set classes
> >> was just an implementation expediency under the mis-thought that they
> >> would take care of recursive subsets etc rather than needing
> >> special implementations for KeySets. But the byproduct for remove()
> >> is clearly wrong so they do need separate implementations.
> >>
> >
> > This remembers me from an old pet peeve, the fact that java.util.HashSet
> is
> > implemented as a wrapper over java.util.HashMap. This sucks
> > performance-wise, due to the additional indirections everywhere, and even
> > more important, because every entry single object is larger than
> necessary -
> > it contains a key and a value fields, but a single field would be
> necessary
> > for HashSet. If you have a Set with millions of entries, the overhead of
> one
> > useless pointer per entry is significant. Plus, the wrapping
> implementation
> > uses a dummy, fixed Object instance as the value for all entries - and
> this
> > may create another subtle performance problem: large numbers of
> > "cross-space" references in the heap. I know that in most GCs only
> > old-to-young refs are evil, but it seems that in some GCs any cross-space
> > reference is bad (requires remset tracking), like Detlef et al's new
> > "Garbage-First" collector (if I understood it).
> >
> > Back in J2SE1.2 time, the runtime savings from the wrapping
> implementation
> > could be significant... but, today? The HashMap* classes inside the
> latest
> > rt.jar are 18,5Kb total (uncompressed), versus 3Kb from HashSet. That's a
> > 12Kb of space saving, plus some small saving in JIT time too. I'd say
> that
> > this is an irrelevant advantage, even in JavaME. The performance bonus of
> a
> > tuned HashSet, however modest, would certainly be much more significant.
> I
> > know it's ugly from a maintenance POV to have two large classes that will
> be
> > 90% identical, but to paraphrase the movie Some Like It Hot, nothing is
> > perfect...
> >
> > A+
> > Osvaldo
> >
> > --
> > -----------------------------------------------------------------------
> > Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> > osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> > Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080609/83c4f281/attachment.html 

From fw at deneb.enyo.de  Tue Jun 10 00:21:08 2008
From: fw at deneb.enyo.de (Florian Weimer)
Date: Tue, 10 Jun 2008 06:21:08 +0200
Subject: [concurrency-interest] Out-of-order execution,
	in-order retirement
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEBEHMAA.dcholmes@optusnet.com.au> (David
	Holmes's message of "Tue, 10 Jun 2008 08:34:15 +1000")
References: <NFBBKALFDCPFIDBNKAPCGEBEHMAA.dcholmes@optusnet.com.au>
Message-ID: <87tzg1539n.fsf@mid.deneb.enyo.de>

* David Holmes:

> Can you not simply store the result into a queue that is sorted on the
> appropriate criteria? You could do an invokeAll to submit the set of tasks
> and wait until they have all completed, check the results and then pass the
> ordered queue to the end-consumer.

The data doesn't necessarily fit into memory, so I need some sort of
streamed processing.

From dcholmes at optusnet.com.au  Tue Jun 10 00:28:07 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 10 Jun 2008 14:28:07 +1000
Subject: [concurrency-interest] Out-of-order execution,
	in-order retirement
In-Reply-To: <87tzg1539n.fsf@mid.deneb.enyo.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEBIHMAA.dcholmes@optusnet.com.au>

Florian Weimer writes:
> * David Holmes:
>
> > Can you not simply store the result into a queue that is sorted on the
> > appropriate criteria? You could do an invokeAll to submit the
> > set of tasks and wait until they have all completed, check the results
> > and then pass the ordered queue to the end-consumer.
>
> The data doesn't necessarily fit into memory, so I need some sort of
> streamed processing.

Ok. So assuming you have enough memory to buffer the maximum number of
out-of-order results, store the results in an ordered queue as suggested and
have the consumer either poll for the "next" item, or else have the
production tasks notify it by tracking which is the next expected piece.

Cheers,
David Holmes


From joe.bowbeer at gmail.com  Tue Jun 10 03:15:41 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 10 Jun 2008 00:15:41 -0700
Subject: [concurrency-interest] Out-of-order execution,
	in-order retirement
In-Reply-To: <87fxrmu871.fsf@mid.deneb.enyo.de>
References: <87fxrmu871.fsf@mid.deneb.enyo.de>
Message-ID: <31f2a7bd0806100015l997e050xc2782da3c755eb91@mail.gmail.com>

On Mon, Jun 9, 2008 at 5:03 AM, Florian Weimer wrote:
> Is there an existing utility class that I could use to implement this?
>
> The canonical example is a block-based compressor: individual blocks can
> be processed in parallel, but they must be written in the original
> sequence to the output stream.
>

Given an iterator to produce the callables that will execute
concurrently, an executor service to execute them, and the maximum
number of tasks that will fit in memory:

  Iterator<Callable<V>> iter;
  ExecutorService exec;
  int numTasks;

The following method submits the callables to the executor and
processes their results in order, without accumulating too many tasks:

    void process() throws InterruptedException, ExecutionException {
        List<Future<V>> list = new ArrayList<Future<V>>();
        while (iter.hasNext() || !list.isEmpty()) {
            while (iter.hasNext() && list.size() < numTasks)
                list.add(exec.submit(iter.next()));
            retire(list.remove(0).get());
        }
    }

The "retire" method consumes the values in order:

    void retire(V result) { ... }

Executors.newCachedThreadPool() might be a good choice for the
executor.  Or, if numTasks is larger than the desired thread count,
then a fixed thread pool would be better.

--
Joe Bowbeer

From mthornton at optrak.co.uk  Tue Jun 10 03:48:24 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 10 Jun 2008 08:48:24 +0100
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484D9C20.3070607@visionnaire.com.br>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>	<484D7E58.8020500@cs.oswego.edu>
	<484D9C20.3070607@visionnaire.com.br>
Message-ID: <484E31C8.3050009@optrak.co.uk>

Osvaldo Pinali Doederlein wrote:
> This remembers me from an old pet peeve, the fact that java.util.HashSet 
> is implemented as a wrapper over java.util.HashMap. This sucks 
> performance-wise, due to the additional indirections everywhere, and 
> even more important, because every entry single object is larger than 
> necessary - it contains a key and a value fields, but a single field 
> would be necessary for HashSet. If you have a Set with millions of 
> entries, the overhead of one useless pointer per entry is significant. 
>   

When I have millions of entries I usually use an implementation with 
open addressing and double hashing. This eliminates all the Entry 
objects and thus saves a LOT more space. It is tempting to implement 
Map's in the same way, except for the need to present Map.Entry objects 
in the interface. I haven't compared performance for a while, but when I 
did there wasn't much difference in time (those indirections aren't 
particularly expensive). Space use is reduced and for large maps this 
has an indirect impact on speed.

Mark Thornton


From mbien at fh-landshut.de  Tue Jun 10 07:41:52 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Tue, 10 Jun 2008 13:41:52 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <484D5460.7000608@cox.net>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>
	<48486B5B.7060802@fh-landshut.de> <4849CB17.40100@cytetech.com>
	<484A869F.7090508@fh-landshut.de> <484D5460.7000608@cox.net>
Message-ID: <484E6880.90508@fh-landshut.de>

Gregg Wonderly wrote:
> Michael Bien wrote:
>> Gregg Wonderly wrote:
>>> Michael Bien wrote:
>>>> Ben Manes wrote:
>>>>> How far along is this project?
>>>>>
>>>>> I wrote a distributed master/slave framework and (for fun) have a 
>>>>> map-reduce abstraction prototyped.  I was hoping to add fork-join, 
>>>>> similar to your message, when we moved to Java-6 (and could use 
>>>>> the jsr lib).  So far my framework has been in production for 2 
>>>>> years, gone through performance reviews, etc.  I've always wanted 
>>>>> to open source it, but never got the powers that be to approve.
>>>>>
>>>>> If your interested, we can discuss such things off-line.
>>>> For sure we can discuss this off-line if you like. Feel free to 
>>>> contact me directly.
>>>> But keep in mind all the motivation to build FishFarm was actually 
>>>> doing distribution without introducing a new Framework ;) The idea 
>>>> is to make grid computing optional in case you are using the 
>>>> fj-framework anyway.
>>>>
>>>> FishFarm is around 6 month old and is a freetime project, it was 
>>>> never in production since I would never introduce a project which 
>>>> is based on a fork of a jsr ;-)
>>> I hope that everyone working in this direction with Java, remembers 
>>> to pay attention to all the work that has gone into Jini to make 
>>> this possible with security and all the other tools such as leases 
>>> and transactions, already implemented and working to help track 
>>> distributed resources effectively, including the security features 
>>> of Java, extended in the JERI stack to help eliminate the impact of 
>>> DOS attacks.
>>>
>>> The current version of Jini is now the River Podling in the Apache 
>>> Incubator.
>>>
>>> Gregg Wonderly
>>>
>> Hello Gregg,
>>
>> There are many distribution systems around and each of them has its 
>> use case. Java Spaces (and its commercial forks), Grid Gain, 
>> Terracotta, Apache River... and even more specialized and therefore 
>> not mainstream projects like Helios for Sunflow ray tracing tasks. 
>> Almost each of them introduced its own framework or lets say it in 
>> other words - IS a framework. (Terracotta is the exception which is a 
>> layer on top of the JVM)
>>  
>> FishFarm is no full featured distribution system, it concentrates 
>> currently only on the distribution of ForkJoinPools and has no 
>> framework at all (jsr166y is its framework). Again, it simple works 
>> if you exchange "new ForkJoinPool()" with "new 
>> DistributedForkJoinPool()" and put the jars in the classpath.
>
> One the more interesting issues is that Remote computational tasks 
> have a unique remote communications requirement that has to be 
> accounted for in some way. Partial failure and security usually become 
> issues faster than most think, and as you start to deal with those 
> issues, you end up reinventing the wheel because of all the work 
> already done to deal with those issues.
>
> In the end, it comes down to APIs and some simple algorithmic 
> choices.  All of the issues that tool sets such as Jini deal with are 
> real and important to manage at some level.  The JERI stack, finally 
> provides a unique opportunity to optimize and tune the RPC stack, 
> dynamically, at deployment time, instead of in your code.  The Jini 
> Configuration interface and the ConfigurationFile implementation allow 
> you to change the Exporter instance you deploy with for example, so 
> that you can put TCP vs SSL, No-Auth vs X.500 vs Kerbos authentication 
> and security constraints all into a configuration step instead of a 
> programming/reprogramming task.
>
> Many people still look at Jini as "tied" to "RMI".  It is tied to the 
> RMI programming model, and through the use of JERI, you can separate 
> most language issues for remote interaction (HTTP endpoint with an XML 
> invocation layer factory for example).
>
> > It has currently no security at all since
>> we haven't got the requirement yet to install it outside closed 
>> networks which are pretty common for clusters. We even decided to try 
>> it without transactions (there is no JEE in it just simple messaging 
>> based on Shoal which is based on JXTA) because the master simple does 
>> not care if worker succeed with the computation or got its requested 
>> work or not. The master simple puts all of its "stolen" work into a 
>> backup and copies it back into the pool when the pool is empty - it 
>> still behaves like a ForkJoinPool.
>
> Simple is easy to start with.  There was a study done on creating a 
> JXTA endpoint for the JERI stack.  I've not used it.  The 
> transactional services in Jini are based on a simple 2PC model that is 
> implemented by the Mahalo service in the River podling (and the 
> previously released Jini Technology Starter Kit v2.1).
>
> The ability to get dependable and predictable results and see 
> progress, seems like an important thing to me.  So I'm not sure that I 
> understand how this framework can support that.
>
> The Javaspaces master/worker pattern is pretty well proven as a 
> distributed system for massive scaling.  Creating a worker that uses 
> the ForkJoin stuff to further divide work locally would be an added 
> mechanism for further exploiting the power available.


Hello Gregg,

I agree with you with almost everything you said, the point is: FishFarm 
is no framework and does not try to reinvent the wheel ;-) It really 
just distributes the ForkJoinPool behind the scenes - nothing more. It 
does not have to order task execution because ForkJoinPools also have no 
order, same for seeing progress...etc

It would be insane to try to reinvent javaspaces & co and claim to be 
better within 6 month of a freetime project. Thats not possible and I 
will never try that (despite the fact that students have a lot of time 
;-) ).

If you are in a green field and design your app to be distributed you 
will design it with distribution framework in mind. But maybe you 
already have code which uses ForkJoinPools and was never designed to be 
distributed you can distribute it immediately if you want with FishFarm 
without any configuration - thats the usecase.

>
>> But my initial question was pretty general even FishFarm independent 
>> and could be used in any framework you like:
>> Is there interest in making ForkJoinTasks Serializeable and providing 
>> a mechanism for querying not yet executed tasks from a ForkJoinPool?
>
> I think these two things have some merit.  Serializable implies a 
> number of issues about the underlying references as well though.
ForkJoinTask:

    ForkJoinTask<V> implements Serializable

is the only diff regarding serialization. You don't want to serialize 
the whole pool including state. ForkJoinTasks are plain objects if you 
want something special, provide write and readObject(..) as always.

regarding the not yet executed tasks (ForkJoinPool):

    public <T> ForkJoinTask<T> popQueuedTask() {

        Submission<?> submission = submissionQueue.poll();

        if(submission == null)

            return null;                

        return (ForkJoinTask<T>)submission.getTask();

    }


Submission:

    public ForkJoinTask<T> getTask() {

        return task;

    }


(is there a issue tracker for jsr166y where i can submit it?)

>
> Gregg Wonderly
>
>
thank you for your feedback,

-michael


From martinrb at google.com  Tue Jun 10 12:39:07 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 10 Jun 2008 09:39:07 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484E31C8.3050009@optrak.co.uk>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
Message-ID: <1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>

I believe it is imposssible to use double hashing
in a general-purpose Map implementation,
because we are basically stuck with the one hash
function provided by hashCode().  Right or wrong?

Martin

On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton > When I have millions
of entries I usually use an implementation with
> open addressing and double hashing. This eliminates all the Entry
> objects and thus saves a LOT more space. It is tempting to implement
> Map's in the same way, except for the need to present Map.Entry objects
> in the interface.

From takeshi10 at gmail.com  Tue Jun 10 13:28:42 2008
From: takeshi10 at gmail.com (Marcelo Fukushima)
Date: Tue, 10 Jun 2008 14:28:42 -0300
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
Message-ID: <7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>

you can apply some function over the result of the hashCode, if the
hashCode's of colliding entries are not identical

now regarding the Map.Entry, theres a library (i forgot witch) that
builds the Set<Entry> lazily when its required, hence extra memory is
required only while the set is being used.

On 6/10/08, Martin Buchholz <martinrb at google.com> wrote:
> I believe it is imposssible to use double hashing
>  in a general-purpose Map implementation,
>  because we are basically stuck with the one hash
>  function provided by hashCode().  Right or wrong?
>
>  Martin
>
>  On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton > When I have millions
>
> of entries I usually use an implementation with
>  > open addressing and double hashing. This eliminates all the Entry
>  > objects and thus saves a LOT more space. It is tempting to implement
>  > Map's in the same way, except for the need to present Map.Entry objects
>  > in the interface.
>
> _______________________________________________
>  Concurrency-interest mailing list
>  Concurrency-interest at altair.cs.oswego.edu
>  http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
[]'s
Marcelo Takeshi Fukushima

From dave.l.griffith at gmail.com  Tue Jun 10 14:01:26 2008
From: dave.l.griffith at gmail.com (Dave Griffith)
Date: Tue, 10 Jun 2008 14:01:26 -0400
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
Message-ID: <2ec602740806101101r37449585mceb98c4a119c509a@mail.gmail.com>

The GNU Trove library is probably what you're thinking of.  I've used it
successfully in several performance-critical circumstances.

http://trove4j.sourceforge.net/



On Tue, Jun 10, 2008 at 1:28 PM, Marcelo Fukushima <takeshi10 at gmail.com>
wrote:

> you can apply some function over the result of the hashCode, if the
> hashCode's of colliding entries are not identical
>
> now regarding the Map.Entry, theres a library (i forgot witch) that
> builds the Set<Entry> lazily when its required, hence extra memory is
> required only while the set is being used.
>
> On 6/10/08, Martin Buchholz <martinrb at google.com> wrote:
> > I believe it is imposssible to use double hashing
> >  in a general-purpose Map implementation,
> >  because we are basically stuck with the one hash
> >  function provided by hashCode().  Right or wrong?
> >
> >  Martin
> >
> >  On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton > When I have millions
> >
> > of entries I usually use an implementation with
> >  > open addressing and double hashing. This eliminates all the Entry
> >  > objects and thus saves a LOT more space. It is tempting to implement
> >  > Map's in the same way, except for the need to present Map.Entry
> objects
> >  > in the interface.
> >
> > _______________________________________________
> >  Concurrency-interest mailing list
> >  Concurrency-interest at altair.cs.oswego.edu
> >  http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
> --
> []'s
> Marcelo Takeshi Fukushima
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/5af56fe4/attachment.html 

From martinrb at google.com  Tue Jun 10 14:22:43 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 10 Jun 2008 11:22:43 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
Message-ID: <1ccfd1c10806101122j71ed364al747aaa10850b9bff@mail.gmail.com>

On Tue, Jun 10, 2008 at 10:28 AM, Marcelo Fukushima <takeshi10 at gmail.com> wrote:
> you can apply some function over the result of the hashCode, if the
> hashCode's of colliding entries are not identical

Good point.

The two hash functions won't be completely independent,
but this may be good enough in practice.  We know the
low order bits are colliding, so perhaps just shift them
away to the right, and use the remaining ones?

Martin

From mthornton at optrak.co.uk  Tue Jun 10 14:47:18 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 10 Jun 2008 19:47:18 +0100
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>	
	<484D7E58.8020500@cs.oswego.edu>
	<484D9C20.3070607@visionnaire.com.br>	
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
Message-ID: <484ECC36.7060408@optrak.co.uk>

Martin Buchholz wrote:
> I believe it is imposssible to use double hashing
> in a general-purpose Map implementation,
> because we are basically stuck with the one hash
> function provided by hashCode().  Right or wrong?
>
> Martin
>   
If the hash table has more than 65536 entries then the two hash values 
won't be completely independent, but in practice it doesn't seem to matter.

Mark Thornton


From osvaldo at visionnaire.com.br  Tue Jun 10 15:31:52 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Tue, 10 Jun 2008 16:31:52 -0300
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>	
	<484D7E58.8020500@cs.oswego.edu>
	<484D9C20.3070607@visionnaire.com.br>	
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
Message-ID: <484ED6A8.6020404@visionnaire.com.br>


> On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton>  When I have millions
> of entries I usually use an implementation with
>    
>> open addressing and double hashing. This eliminates all the Entry
>> objects and thus saves a LOT more space. It is tempting to implement
>> Map's in the same way, except for the need to present Map.Entry objects
>> in the interface.
>>      
You don't always get to choose your collections. For example, when using 
most ORM tools, the relationships of persistent entities are often 
mapped to collections of List, Set or Map types (typically, wrappers 
over java.util's collections with extra stuff like lazy loading). There 
are many other examples out there, collections are pretty popular 
objects and there are tons of libraries that manipulate collections, 
often creating them without any mechanism to override the concrete types 
they use.

Not to mention that the Java runtime itself (i.e. the JCL) makes heavy 
use of basic types like collections. I'd bet some serious money that if 
I could make a significant, no-compromise optimization in a class like 
java.util.HashMap, this would provide important performance benefits in 
many places (reflection metadata, String internalization, etc.). HashSet 
is a more niche class, but it still seems to be used very frequently in 
the JCL: a quick grep for HashSet inside the JDK's src.zip reveals 
significant usage in serialization, swing, security, CORBA, 
reflection/classloading, JMX, and in java.util itself (EnumSet, 
ThreadPoolExecutor, ResourceBundle and several other classes). So, it's 
very likely that the suggested free-standing optimization of HashSet 
would benefit the JRE performance (not to mention JavaEE and other 
libs/APIs/frameworks), even before considering any application code.

Now, while we are playing with HashSet optimization, another idea is: we 
could have a method like get(), that looks for an object and if it's 
found, returns it. The Set interface only offers contains(), which 
returns a boolean - definitely a cleaner interface for the most common 
usage of Set, but then you can't use a Set for the VERY important 
use-case of canonicalization. Canonicalization would only require a Set, 
but when the canonic object is found, if want to return it, so the 
caller can use it in lieu of the original object. We are forced to use 
an explicit Map collection just because it offers the get() lookup 
method. A Set.get() method would be easily defined: public T get (T 
obj), checks if the set contains an object that's equal to obj, and 
returns that object (or null if not found). Now, we obviously cannot add 
new methods to the Set interface, but we could do that in concrete 
implementations like HashSet. Implementation of such method would be 
trivial (the existing contains() would me morphed into that new method - 
requiring only to change a return statement and the signature - and the 
new contains() would return "get(obj) != null").

P.S.: Sun doesn't need a HashSet that allows efficient canonicalization 
because they cheat - String.intern() is a native method, probably 
implemented with a more efficient collection in the native layer. :)

A+
Osvaldo


-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/5f4ceae8/attachment.html 

From forax at univ-mlv.fr  Tue Jun 10 15:34:13 2008
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Tue, 10 Jun 2008 21:34:13 +0200
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>	<484D7E58.8020500@cs.oswego.edu>
	<484D9C20.3070607@visionnaire.com.br>	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
Message-ID: <484ED735.7060400@univ-mlv.fr>

Martin Buchholz a ?crit :
> I believe it is imposssible to use double hashing
> in a general-purpose Map implementation,
> because we are basically stuck with the one hash
> function provided by hashCode().  Right or wrong?
>
> Martin
>   
Martin, take a look to implementation. of IdentityHashMap
It already uses a double hashing.

And like trove, Map.Entry(ies) are created when needed (at least in 1.6),
see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6312706
> On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton > When I have millions
> of entries I usually use an implementation with
>   
>> open addressing and double hashing. This eliminates all the Entry
>> objects and thus saves a LOT more space. It is tempting to implement
>> Map's in the same way, except for the need to present Map.Entry objects
>> in the interface.
>>     
R?mi

From mthornton at optrak.co.uk  Tue Jun 10 15:48:02 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 10 Jun 2008 20:48:02 +0100
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484ED735.7060400@univ-mlv.fr>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>	<484D7E58.8020500@cs.oswego.edu>
	<484D9C20.3070607@visionnaire.com.br>	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED735.7060400@univ-mlv.fr>
Message-ID: <484EDA72.9070700@optrak.co.uk>

R?mi Forax wrote:
> Martin Buchholz a ?crit :
>> I believe it is imposssible to use double hashing
>> in a general-purpose Map implementation,
>> because we are basically stuck with the one hash
>> function provided by hashCode().  Right or wrong?
>>
>> Martin
>>   
> Martin, take a look to implementation. of IdentityHashMap
> It already uses a double hashing.
>
> And like trove, Map.Entry(ies) are created when needed (at least in 1.6),
> see http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6312706

Much as I like optimisation, reusing the Map.Entry object was always a 
step too far for me.

Mark Thornton


From studdugie at gmail.com  Tue Jun 10 16:19:27 2008
From: studdugie at gmail.com (studdugie)
Date: Tue, 10 Jun 2008 16:19:27 -0400
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <2ec602740806101101r37449585mceb98c4a119c509a@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
	<2ec602740806101101r37449585mceb98c4a119c509a@mail.gmail.com>
Message-ID: <5a59ce530806101319n54b7260fofa86b6fe716248e0@mail.gmail.com>

There is also fastutils: http://fastutil.dsi.unimi.it/

I've used them both.

On Tue, Jun 10, 2008 at 2:01 PM, Dave Griffith
<dave.l.griffith at gmail.com> wrote:
> The GNU Trove library is probably what you're thinking of.  I've used it
> successfully in several performance-critical circumstances.
>
> http://trove4j.sourceforge.net/
>
>
>
> On Tue, Jun 10, 2008 at 1:28 PM, Marcelo Fukushima <takeshi10 at gmail.com>
> wrote:
>>
>> you can apply some function over the result of the hashCode, if the
>> hashCode's of colliding entries are not identical
>>
>> now regarding the Map.Entry, theres a library (i forgot witch) that
>> builds the Set<Entry> lazily when its required, hence extra memory is
>> required only while the set is being used.
>>
>> On 6/10/08, Martin Buchholz <martinrb at google.com> wrote:
>> > I believe it is imposssible to use double hashing
>> >  in a general-purpose Map implementation,
>> >  because we are basically stuck with the one hash
>> >  function provided by hashCode().  Right or wrong?
>> >
>> >  Martin
>> >
>> >  On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton > When I have millions
>> >
>> > of entries I usually use an implementation with
>> >  > open addressing and double hashing. This eliminates all the Entry
>> >  > objects and thus saves a LOT more space. It is tempting to implement
>> >  > Map's in the same way, except for the need to present Map.Entry
>> > objects
>> >  > in the interface.
>> >
>> > _______________________________________________
>> >  Concurrency-interest mailing list
>> >  Concurrency-interest at altair.cs.oswego.edu
>> >  http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>>
>> --
>> []'s
>> Marcelo Takeshi Fukushima
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From kevinb at google.com  Tue Jun 10 17:19:28 2008
From: kevinb at google.com (kevin bourrillion)
Date: Tue, 10 Jun 2008 14:19:28 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484E31C8.3050009@optrak.co.uk>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
Message-ID: <108fcdeb0806101419x2e96b144s4376dcde161b6a60@mail.gmail.com>

On Tue, Jun 10, 2008 at 12:48 AM, Mark Thornton <mthornton at optrak.co.uk>
wrote:

Osvaldo Pinali Doederlein wrote:
> > This remembers me from an old pet peeve, the fact that java.util.HashSet
> > is implemented as a wrapper over java.util.HashMap. This sucks
> > performance-wise, due to the additional indirections everywhere, and
> > even more important, because every entry single object is larger than
> > necessary - it contains a key and a value fields, but a single field
> > would be necessary for HashSet. If you have a Set with millions of
> > entries, the overhead of one useless pointer per entry is significant.


If you have a hash set that doesn't need to mutate, you may appreciate our
ImmutableSet collection:
http://google-collections.googlecode.com/svn/trunk/src/com/google/common/collect/ImmutableSet.java

Its memory usage is (size + tablesize) as opposed to (6 * size + tablesize)
for HashSet and (8 * size + tablesize) for LinkedHashSet, whose behavior it
replicates.


-- 
Kevin Bourrillion @ Google
internal: go/javalibraries
google-collections.googlecode.com
google-guice.googlecode.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/59a77930/attachment.html 

From martinrb at google.com  Tue Jun 10 17:25:32 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 10 Jun 2008 14:25:32 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484ED735.7060400@univ-mlv.fr>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED735.7060400@univ-mlv.fr>
Message-ID: <1ccfd1c10806101425n6fb88187w6c34f51e7f6a1442@mail.gmail.com>

On Tue, Jun 10, 2008 at 12:34 PM, R?mi Forax <forax at univ-mlv.fr> wrote:
> Martin Buchholz a ?crit :
>>
>> I believe it is imposssible to use double hashing
>> in a general-purpose Map implementation,
>> because we are basically stuck with the one hash
>> function provided by hashCode().  Right or wrong?
>>
>> Martin
>>
>
> Martin, take a look to implementation. of IdentityHashMap
> It already uses a double hashing.

I don't consider IdentityHashMap a general-purpose Map implementation.
It doesn't satisfy the contract for Map, and doesn't call Object.hashCode.

Martin


From kevinb at google.com  Tue Jun 10 17:30:04 2008
From: kevinb at google.com (kevin bourrillion)
Date: Tue, 10 Jun 2008 14:30:04 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484ED6A8.6020404@visionnaire.com.br>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED6A8.6020404@visionnaire.com.br>
Message-ID: <108fcdeb0806101430k2fb665dbkd598daefd6f71b68@mail.gmail.com>

On Tue, Jun 10, 2008 at 12:31 PM, Osvaldo Pinali Doederlein <
osvaldo at visionnaire.com.br> wrote:

Now, while we are playing with HashSet optimization, another idea is: we
> could have a method like get(), that looks for an object and if it's found,
> returns it.
>

It doesn't seem too useful to me.  Our InterningPool implementations have no
particular desire to be Sets or implement Set; it's really something else
altogether.


-- 
Kevin Bourrillion @ Google
internal: go/javalibraries
google-collections.googlecode.com
google-guice.googlecode.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/9bf871ef/attachment.html 

From martinrb at google.com  Tue Jun 10 17:38:39 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 10 Jun 2008 14:38:39 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484EDA72.9070700@optrak.co.uk>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED735.7060400@univ-mlv.fr> <484EDA72.9070700@optrak.co.uk>
Message-ID: <1ccfd1c10806101438n46a7f65ej9fae51369b25d231@mail.gmail.com>

On Tue, Jun 10, 2008 at 12:48 PM, Mark Thornton <mthornton at optrak.co.uk> wrote:
> Much as I like optimisation, reusing the Map.Entry object was always a step
> too far for me.

I agree.  I believe only IdentityHashMap is evil in this way today.

If someone is willing to do some benchmarking to prove that
the loss of performance is not too great, we might be able to get
this fixed in OpenJDK7.  There is the hope that in cases where
the Map.Entry immediately becomes garbage, escape analysis
in hotspot can elide all actual allocation of Map.Entry objects.
But escape analysis is a new feature in hotspot, so we are
tempted to wait for it to become more mature.

Martin

From joe.bowbeer at gmail.com  Tue Jun 10 17:56:45 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 10 Jun 2008 14:56:45 -0700
Subject: [concurrency-interest] Out-of-order execution,
	in-order retirement
In-Reply-To: <31f2a7bd0806100015l997e050xc2782da3c755eb91@mail.gmail.com>
References: <87fxrmu871.fsf@mid.deneb.enyo.de>
	<31f2a7bd0806100015l997e050xc2782da3c755eb91@mail.gmail.com>
Message-ID: <31f2a7bd0806101456i5093615ag4b79230c180c7b6e@mail.gmail.com>

On Tue, Jun 10, 2008 at 12:15 AM, Joe Bowbeer wrote:
> On Mon, Jun 9, 2008 at 5:03 AM, Florian Weimer wrote:
>> Is there an existing utility class that I could use to implement this?
>>
>> The canonical example is a block-based compressor: individual blocks can
>> be processed in parallel, but they must be written in the original
>> sequence to the output stream.
>>
>
> Given an iterator to produce the callables that will execute
> concurrently, an executor service to execute them, and the maximum
> number of tasks that will fit in memory:
>
>  Iterator<Callable<V>> iter;
>  ExecutorService exec;
>  int numTasks;
>
> The following method submits the callables to the executor and
> processes their results in order, without accumulating too many tasks:
>
>    void process() throws InterruptedException, ExecutionException {
>        List<Future<V>> list = new ArrayList<Future<V>>();
>        while (iter.hasNext() || !list.isEmpty()) {
>            while (iter.hasNext() && list.size() < numTasks)
>                list.add(exec.submit(iter.next()));
>            retire(list.remove(0).get());
>        }
>    }
>
> The "retire" method consumes the values in order:
>
>    void retire(V result) { ... }
>
> Executors.newCachedThreadPool() might be a good choice for the
> executor.  Or, if numTasks is larger than the desired thread count,
> then a fixed thread pool would be better.
>

Florian,

I'm not sure if your question concerned forkjoin exclusively, or if
you already have the information you need. If not, here's a possible
enhancement to the approach I suggested.

If the task limit applies to pending tasks and not to completed tasks,
then a CompletionService can be used to feed the executor more
efficiently.

In a block-based compressor, for example, the pending tasks that hold
uncompressed blocks are likely to require more memory than the
completed tasks that hold compressed data.

The class below uses a completion service to track completed tasks so
that it can feed new pending tasks to the executor, as needed.

The implementation below interleaves submission and retirement.  A
more efficient implementation would use a BlockingQueue and dedicate
one thread to queuing submitted tasks, and another thread to retiring
queued tasks.

abstract class Processor<V> {

    private final Iterator<Callable<V>> iter;
    private final CompletionService<V> ecs;
    private final int numTasks;

    Processor(Iterator<Callable<V>> iter, Executor exec, int numTasks) {
        this.iter = iter;
        this.ecs = new ExecutorCompletionService<V>(exec);
        this.numTasks = numTasks;
    }

    void process() throws InterruptedException, ExecutionException {
        Queue<Future<V>> queue = new ArrayDeque<Future<V>>();
        int count = 0;
        while (iter.hasNext() || !queue.isEmpty()) {
            while (ecs.poll() != null) {
                count--;
            }
            for (; iter.hasNext() && count < numTasks; count++)
                queue.add(ecs.submit(iter.next()));
            retire(queue.remove().get());
        }
    }

    protected abstract void retire(V result);
}

--
Joe Bowbeer

From kevinb at google.com  Tue Jun 10 17:58:24 2008
From: kevinb at google.com (kevin bourrillion)
Date: Tue, 10 Jun 2008 14:58:24 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806101122j71ed364al747aaa10850b9bff@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<7288749d0806101028k715f57abu76186c5a312f6d7d@mail.gmail.com>
	<1ccfd1c10806101122j71ed364al747aaa10850b9bff@mail.gmail.com>
Message-ID: <108fcdeb0806101458h35bfc56co5b3009ab80026ccb@mail.gmail.com>

One thing to consider is that if

(a) you have high-quality hashCode() functions, and
(b) you have fewer than 2^15 elements,

then you could trivially carve two independent functions out of one hash
code.  Likewise, three if you have fewer than 2^10, etc.

Problem is, (a) is almost never the case, is it? :)



On Tue, Jun 10, 2008 at 11:22 AM, Martin Buchholz <martinrb at google.com>
wrote:

> On Tue, Jun 10, 2008 at 10:28 AM, Marcelo Fukushima <takeshi10 at gmail.com>
> wrote:
> > you can apply some function over the result of the hashCode, if the
> > hashCode's of colliding entries are not identical
>
> Good point.
>
> The two hash functions won't be completely independent,
> but this may be good enough in practice.  We know the
> low order bits are colliding, so perhaps just shift them
> away to the right, and use the remaining ones?
>
> Martin
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Kevin Bourrillion @ Google
internal: go/javalibraries
google-collections.googlecode.com
google-guice.googlecode.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/caf7da51/attachment-0001.html 

From kevinb at google.com  Tue Jun 10 18:12:09 2008
From: kevinb at google.com (kevin bourrillion)
Date: Tue, 10 Jun 2008 15:12:09 -0700
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <1ccfd1c10806101438n46a7f65ej9fae51369b25d231@mail.gmail.com>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED735.7060400@univ-mlv.fr> <484EDA72.9070700@optrak.co.uk>
	<1ccfd1c10806101438n46a7f65ej9fae51369b25d231@mail.gmail.com>
Message-ID: <108fcdeb0806101512r386f0b72t6dab589bef7455b7@mail.gmail.com>

On Tue, Jun 10, 2008 at 2:38 PM, Martin Buchholz <martinrb at google.com>
wrote:

On Tue, Jun 10, 2008 at 12:48 PM, Mark Thornton <mthornton at optrak.co.uk>
> wrote:
> > Much as I like optimisation, reusing the Map.Entry object was always a
> step
> > too far for me.
>
> I agree.  I believe only IdentityHashMap is evil in this way today.
>

Yep, I verified this fact a few weeks ago.  And I agree that it's evil; it
causes some extremely bizarre situations.



-- 
Kevin Bourrillion @ Google
internal: go/javalibraries
google-collections.googlecode.com
google-guice.googlecode.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080610/9be7b3b1/attachment.html 

From markus.kohler at gmail.com  Wed Jun 11 03:20:46 2008
From: markus.kohler at gmail.com (Markus Kohler)
Date: Wed, 11 Jun 2008 09:20:46 +0200
Subject: [concurrency-interest] HashSet performance
In-Reply-To: <484ED6A8.6020404@visionnaire.com.br>
References: <1ccfd1c10806081430u361ccc58y9d423aca49c09057@mail.gmail.com>
	<484D7E58.8020500@cs.oswego.edu> <484D9C20.3070607@visionnaire.com.br>
	<484E31C8.3050009@optrak.co.uk>
	<1ccfd1c10806100939y136ae87dwddb60c642aabb1d6@mail.gmail.com>
	<484ED6A8.6020404@visionnaire.com.br>
Message-ID: <771905290806110020y7ec13f0bn915cf0f773542d07@mail.gmail.com>

Hi Osvaldo,


On Tue, Jun 10, 2008 at 9:31 PM, Osvaldo Pinali Doederlein <
osvaldo at visionnaire.com.br> wrote:

>
> [snip]
>
> Now, while we are playing with HashSet optimization, another idea is: we
> could have a method like get(), that looks for an object and if it's found,
> returns it. The Set interface only offers contains(), which returns a
> boolean - definitely a cleaner interface for the most common usage of Set,
> but then you can't use a Set for the VERY important use-case of
> canonicalization. Canonicalization would only require a Set, but when the
> canonic object is found, if want to return it, so the caller can use it in
> lieu of the original object. We are forced to use an explicit Map collection
> just because it offers the get() lookup method. A Set.get() method would be
> easily defined: public T get (T obj), checks if the set contains an object
> that's equal to obj, and returns that object (or null if not found). Now, we
> obviously cannot add new methods to the Set interface, but we could do that
> in concrete implementations like HashSet. Implementation of such method
> would be trivial (the existing contains() would me morphed into that new
> method - requiring only to change a return statement and the signature - and
> the new contains() would return "get(obj) != null").
>

This sound like very good idea to me.

>
> P.S.: Sun doesn't need a HashSet that allows efficient canonicalization
> because they cheat - String.intern() is a native method, probably
> implemented with a more efficient collection in the native layer. :)
>

It is implemented in the native layer, but it is not necessarily faster than
HashMap or ConcurrentHashMap.
Actually it used to be much slower in JDK 1.4.2 because the JVM used a
relatively small hashtable and if you interned enough Strings it would
degenerate because it uses linked lists to resolve collisions.
"We" fixed this in the SAP JVM and I think newer Java version got better as
well.

Still, ConcurrentHashMap can be faster in case you have parallel access by
many threads.

The main advantage of String.intern() is that it acts like a WeakHashmap,
but without the high memory overhead.

In the SAP JVM we therefore implemented an optimized String.intern(), which
is fast enough for us for now, but it would be really nice to have an
optimized ConcurrentWeakSet with support for the API you described above.

Regards,
Markus


> A+
> Osvaldo
>
>
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/Aosvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080611/fd28ffa3/attachment.html 

From brian at briangoetz.com  Wed Jun 11 23:40:00 2008
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 11 Jun 2008 23:40:00 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
Message-ID: <48509A90.6030707@briangoetz.com>

I noticed the same thing.  The code is wrong if you interpret it as Java code. 
   You could argue that it is pseudo-code, but its really too close to Java to 
make that argument effectively.  I've already got it scribbled down for my 
next batch of errata that I'll send to Maurice.

Tim Halloran wrote:
> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" 
> I ran into this code which is a 2-thread solution for a Lock.  (I don't 
> think you need the book to understand my question.)
> 
> public class Peterson implements Lock {
>     // thread-local index, 0 or 1
>     private volatile boolean[] flag = new boolean[2];
>     private volatile int victim;
> 
>     public void lock() {
>         int i = ThreadID.get();
>         int j = 1 - i;
>         flag[i] = true; // I'm interested
>         victim = i; // You go first
>         while (flag[j] && victim == i) {
>             // wait
>         }
>     }
> 
>     public void unlock() {
>         int i = ThreadID.get();
>         flag[i] = false;
>     }
> }
> 
> The issue is the declaration of the boolean array "flag" (not the lock 
> algorithm).  The authors note (on page 25) that the fields need to be 
> volatile in Java to work, however, I don't think "flag" is working the 
> way they expect.  Am I wrong?
> 
> First, I think "flag" should be declared "final" not "volatile" as the 
> field should never be mutated.
> 
>     private final boolean[] flag = new boolean[2];
> 
> Here, my confusion starts as I think this can't be fixed in Java.  I 
> believe there is no way to indicate that the primitive elements of an 
> array are volatile (e.g., give them the memory model semantics the 
> authors desire).
> 
> I was hoping someone here would know for sure.  I was going to let the 
> authors know--but wanted to check my facts :-)
> 
> Best Regards,
> Tim Halloran
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at briangoetz.com  Wed Jun 11 23:42:11 2008
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 11 Jun 2008 23:42:11 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>
References: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>
Message-ID: <48509B13.5080508@briangoetz.com>

Nope, that's incorrect.  Since no one _ever_ writes to the volatile array 
reference (only to its elements), it can't possibly participate in a 
happens-before ordering.

Guy Korland wrote:
> Hi Tim
>  
> If you really think about it, since java 5, it's not an error.
> Since all the author was trying to do is to promise that thread will 
> read a fresh value.
>  
> When a thread call flag[j] it really access a volatile reference "flag" 
> which flashes the all memory since java 5 and then read the place in the 
> array which in fact was already refreshed.
>  
> Guy
>  
>  
>  >Thanks Tim, I'll send the authors an errata...they can decide what to do.
> 
>  >On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net 
> <mailto:tim at peierls.net>> wrote:
> 
>  > I noticed that, too, but I figured the authors were skating (sloppily)
>  > around the unfortunate fact that you can't have arrays of volatiles. They
>  > should certainly have footnoted it as pseudo-code. They should have just
>  > used two volatiles, flag1 and flag2.
>  >
>  > --tim
>  >
>  > On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com 
> <mailto:hallorant at gmail.com>> wrote:
>  >
>  >> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor 
> Programming" I
>  >> ran into this code which is a 2-thread solution for a Lock.  (I 
> don't think
>  >> you need the book to understand my question.)
>  >>
>  >> public class Peterson implements Lock {
>  >>     // thread-local index, 0 or 1
>  >>     private volatile boolean[] flag = new boolean[2];
>  >>     private volatile int victim;
>  >>
>  >>     public void lock() {
>  >>         int i = ThreadID.get();
>  >>         int j = 1 - i;
>  >>         flag[i] = true; // I'm interested
>  >>         victim = i; // You go first
>  >>         while (flag[j] && victim == i) {
>  >>             // wait
>  >>         }
>  >>     }
>  >>
>  >>     public void unlock() {
>  >>         int i = ThreadID.get();
>  >>         flag[i] = false;
>  >>     }
>  >> }
>  >>
>  >> The issue is the declaration of the boolean array "flag" (not the lock
>  >> algorithm).  The authors note (on page 25) that the fields need to be
>  >> volatile in Java to work, however, I don't think "flag" is working 
> the way
>  >> they expect.  Am I wrong?
>  >>
>  >> First, I think "flag" should be declared "final" not "volatile" as the
>  >> field should never be mutated.
>  >>
>  >>     private final boolean[] flag = new boolean[2];
>  >>
>  >> Here, my confusion starts as I think this can't be fixed in Java.  I
>  >> believe there is no way to indicate that the primitive elements of 
> an array
>  >> are volatile (e.g., give them the memory model semantics the authors
>  >> desire).
>  >>
>  >> I was hoping someone here would know for sure.  I was going to let the
>  >> authors know--but wanted to check my facts :-)
>  >>
>  >> Best Regards,
>  >> Tim Halloran
>  >>
>  >> _______________________________________________
>  >> Concurrency-interest mailing list
>  >> Concurrency-interest at altair.cs.oswego.edu 
> <mailto:Concurrency-interest at altair.cs.oswego.edu>
>  >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>  >>
>  >>
>  >
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at briangoetz.com  Wed Jun 11 23:43:26 2008
From: brian at briangoetz.com (Brian Goetz)
Date: Wed, 11 Jun 2008 23:43:26 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <48414FA4.9030905@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCCEOKHLAA.dcholmes@optusnet.com.au>
	<48414FA4.9030905@cytetech.com>
Message-ID: <48509B5E.1010903@briangoetz.com>

 > 		arr = arr;

This be what we call Pirate Coding, arr.


From gregg at cytetech.com  Thu Jun 12 11:42:11 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 12 Jun 2008 10:42:11 -0500
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <48509B5E.1010903@briangoetz.com>
References: <NFBBKALFDCPFIDBNKAPCCEOKHLAA.dcholmes@optusnet.com.au>
	<48414FA4.9030905@cytetech.com> <48509B5E.1010903@briangoetz.com>
Message-ID: <485143D3.5080009@cytetech.com>

Brian Goetz wrote:
>  > 		arr = arr;
> 
> This be what we call Pirate Coding, arr.

But the question is, does this cause the "happens before" to be true?  If it 
does, some people will start to ask why

	arr[i] = 1;

can't implicitly create the same behavior as

	arr[i] = 1;
	arr = arr;

I suspect...  Volatile arrays having this odd characteristic of not really being 
volatile, all the way through, seems a bit "clunky" to some people that I 
discuss this issue with.

Gregg Wonderly

From jed at atlassian.com  Mon Jun 16 00:08:09 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 16 Jun 2008 14:08:09 +1000
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <48509A90.6030707@briangoetz.com>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
	<48509A90.6030707@briangoetz.com>
Message-ID: <4855E729.80708@atlassian.com>

After finally getting my copy and reading on the weekend, it seems that 
the whole book uses a kind of pseudo-code written in Java syntax that 
run on an idealised VM where the code is sequentially consistent (at 
least, no read-write re-orderings) and with immediate visibility 
effects. This is a useful device as far as it goes (in explaining their 
concepts) but can be quite frustrating for the casual observer (no, it 
must be final!). The fact that they also mix java.util.concurrent.* 
classes into the mix further muddies the waters. The fact that those 
specific examples misuse volatile must be an error as it doesn't appear 
to be repeated later (I haven't been through it with a fine toothed comb).

That said, I'm enjoying the book a lot.

Brian Goetz wrote:
> I noticed the same thing.  The code is wrong if you interpret it as Java code. 
>    You could argue that it is pseudo-code, but its really too close to Java to 
> make that argument effectively.  I've already got it scribbled down for my 
> next batch of errata that I'll send to Maurice.
>
> Tim Halloran wrote:
>   
>> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" 
>> I ran into this code which is a 2-thread solution for a Lock.  (I don't 
>> think you need the book to understand my question.)
>>
>> public class Peterson implements Lock {
>>     // thread-local index, 0 or 1
>>     private volatile boolean[] flag = new boolean[2];
>>     private volatile int victim;
>>
>>     public void lock() {
>>         int i = ThreadID.get();
>>         int j = 1 - i;
>>         flag[i] = true; // I'm interested
>>         victim = i; // You go first
>>         while (flag[j] && victim == i) {
>>             // wait
>>         }
>>     }
>>
>>     public void unlock() {
>>         int i = ThreadID.get();
>>         flag[i] = false;
>>     }
>> }
>>
>> The issue is the declaration of the boolean array "flag" (not the lock 
>> algorithm).  The authors note (on page 25) that the fields need to be 
>> volatile in Java to work, however, I don't think "flag" is working the 
>> way they expect.  Am I wrong?
>>
>> First, I think "flag" should be declared "final" not "volatile" as the 
>> field should never be mutated.
>>
>>     private final boolean[] flag = new boolean[2];
>>
>> Here, my confusion starts as I think this can't be fixed in Java.  I 
>> believe there is no way to indicate that the primitive elements of an 
>> array are volatile (e.g., give them the memory model semantics the 
>> authors desire).
>>
>> I was hoping someone here would know for sure.  I was going to let the 
>> authors know--but wanted to check my facts :-)
>>
>> Best Regards,
>> Tim Halloran
>>
>>     


From ben_manes at yahoo.com  Mon Jun 16 03:31:48 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Mon, 16 Jun 2008 00:31:48 -0700 (PDT)
Subject: [concurrency-interest] Volatile and primitive arrays
Message-ID: <771488.65303.qm@web38806.mail.mud.yahoo.com>

That was my impression too, and I'm over half-way through, but it gets better when you get passed the registers/locks chapters.  There are enough minor programming errors throughout the text, such as constructors not properly setting fields (e.g. Y(x) {x = x}) that its pretty evident that the code was not tested.  It would have been nice if the authors had written the code, with tests, and made it available.  But alas its pseudo-java, with an absolutely excellent presentation.

So far I've really enjoyed the chapter exercises.  I may ask the Dante's Inferno question (ch10, #123) during interviews for performance engineers, since its easy enough to write a naive implementation and provides a lot of good material for discussions.


----- Original Message ----
From: Jed Wesley-Smith <jed at atlassian.com>
To: Brian Goetz <brian at briangoetz.com>
Cc: concurrency-interest at cs.oswego.edu
Sent: Sunday, June 15, 2008 9:08:09 PM
Subject: Re: [concurrency-interest] Volatile and primitive arrays

After finally getting my copy and reading on the weekend, it seems that 
the whole book uses a kind of pseudo-code written in Java syntax that 
run on an idealised VM where the code is sequentially consistent (at 
least, no read-write re-orderings) and with immediate visibility 
effects. This is a useful device as far as it goes (in explaining their 
concepts) but can be quite frustrating for the casual observer (no, it 
must be final!). The fact that they also mix java.util.concurrent.* 
classes into the mix further muddies the waters. The fact that those 
specific examples misuse volatile must be an error as it doesn't appear 
to be repeated later (I haven't been through it with a fine toothed comb).

That said, I'm enjoying the book a lot.

Brian Goetz wrote:
> I noticed the same thing.  The code is wrong if you interpret it as Java code. 
>    You could argue that it is pseudo-code, but its really too close to Java to 
> make that argument effectively.  I've already got it scribbled down for my 
> next batch of errata that I'll send to Maurice.
>
> Tim Halloran wrote:
>  
>> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" 
>> I ran into this code which is a 2-thread solution for a Lock.  (I don't 
>> think you need the book to understand my question.)
>>
>> public class Peterson implements Lock {
>>     // thread-local index, 0 or 1
>>     private volatile boolean[] flag = new boolean[2];
>>     private volatile int victim;
>>
>>     public void lock() {
>>         int i = ThreadID.get();
>>         int j = 1 - i;
>>         flag[i] = true; // I'm interested
>>         victim = i; // You go first
>>         while (flag[j] && victim == i) {
>>             // wait
>>         }
>>     }
>>
>>     public void unlock() {
>>         int i = ThreadID.get();
>>         flag[i] = false;
>>     }
>> }
>>
>> The issue is the declaration of the boolean array "flag" (not the lock 
>> algorithm).  The authors note (on page 25) that the fields need to be 
>> volatile in Java to work, however, I don't think "flag" is working the 
>> way they expect.  Am I wrong?
>>
>> First, I think "flag" should be declared "final" not "volatile" as the 
>> field should never be mutated.
>>
>>     private final boolean[] flag = new boolean[2];
>>
>> Here, my confusion starts as I think this can't be fixed in Java.  I 
>> believe there is no way to indicate that the primitive elements of an 
>> array are volatile (e.g., give them the memory model semantics the 
>> authors desire).
>>
>> I was hoping someone here would know for sure.  I was going to let the 
>> authors know--but wanted to check my facts :-)
>>
>> Best Regards,
>> Tim Halloran
>>
>>    

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080616/76e7308c/attachment.html 

From mbien at fh-landshut.de  Fri Jun 20 06:04:20 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Fri, 20 Jun 2008 12:04:20 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <484E6880.90508@fh-landshut.de>
References: <138429.26330.qm@web38807.mail.mud.yahoo.com>	<48486B5B.7060802@fh-landshut.de>
	<4849CB17.40100@cytetech.com>	<484A869F.7090508@fh-landshut.de>
	<484D5460.7000608@cox.net> <484E6880.90508@fh-landshut.de>
Message-ID: <485B80A4.8040906@fh-landshut.de>

Michael Bien wrote:
> ForkJoinTask:
>
>     ForkJoinTask<V> implements Serializable
>
> is the only diff regarding serialization. You don't want to serialize 
> the whole pool including state. ForkJoinTasks are plain objects if you 
> want something special, provide write and readObject(..) as always.
>
> regarding the not yet executed tasks (ForkJoinPool):
>
>     public <T> ForkJoinTask<T> popQueuedTask() {
>
>         Submission<?> submission = submissionQueue.poll();
>
>         if(submission == null)
>
>             return null;                
>
>         return (ForkJoinTask<T>)submission.getTask();
>
>     }
>
>
> Submission:
>
>     public ForkJoinTask<T> getTask() {
>
>         return task;
>
>     }
>
>
> (is there a issue tracker for jsr166y where i can submit it?)
>
>   
> thank you for your feedback,
>
> -michael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

I haven't got an answer yet.

Is there no interest in providing this kind of features in jsr166y?

yes/no/perhaps?

best regards,

michael


https://fishfarm.dev.java.net/
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080620/143f3182/attachment.html 

From juliusdavies at gmail.com  Tue Jun 24 20:12:30 2008
From: juliusdavies at gmail.com (Julius Davies)
Date: Tue, 24 Jun 2008 17:12:30 -0700
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
Message-ID: <598ad5b50806241712pc2ed76fi17f9c407cbe2b54a@mail.gmail.com>

[Sorry if this gets double-posted....!]

Hi,

I opened an enhancement in Sun's bug database to ask for
System.currentTimeNanos():

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908

Normally I wouldn't harrass a random mailing list with my private
ravings, but the "Evaluation" section in the bug points back to this
list!  After reading the thread in question
(http://www.nabble.com/High-resolution-timers-and-JSR-310-td17302683.html),
I did not think the currentTimeNanos() idea was actually addressed.
The idea is *not* to compose System.currentTimeMillis() and
System.nanoTime().  Composing those two methods just doesn't work, and
never will.

Instead the idea is to extend System.currentTimeMillis() a little - to
nanosecond precision (when possible) - by returning a 2-element long
array, and calling it System.getCurrentTimeNanos().  It appears that
all the major OS's *except* Windows support this, and it can be
reasonably faked out on Windows, even, in a single JNI call.

In other words, the idea is to stop truncating what the OS is already
giving java when we call System.currentMillis().

I invite people to download the zip file and take a look:

http://juliusdavies.ca/nanotime/nanotime.zip

It builds on unix/mac-os-x with no problem.  On windows in compiles
nicely if you have cygwin, gcc, and mingw.


Here's a cut & paste from the Sun ticket (with minor edits on my part
for clarity, as well as mention of the 2038 problem that might lurk
behind gettimeofday()):

----------------------------------------------
JUSTIFICATION :
Solaris and Linux already support nanosecond precision thanks to
clock_gettime(CLOCK_REALTIME).  System.currentTimeMillis() already
uses that, but it would be nice to not truncate the nanoseconds.  The
situation on Mac OSX is a little different, since only gettimeofday()
is available, which gives microseconds (possible 32bit issues - e.g.
infamous 2038?).  But it's still an improvement.

Of course on Windows all timing information between the 10ms
granularity is quasi-fictional, but for figuring out order-of-events
in a log, a "high-resolution clock simulator" on windows would still
be useful.

EXPECTED VERSUS ACTUAL BEHAVIOR :
EXPECTED -
System.currentTimeNanos() would return a two-element long array
containing {seconds, nanos} since epoch.  The time returned would be
as accurate as the operating system could provide (e.g. nanos would be
micros * 1000 on Mac OSX).

On Windows the time returned would be an approximation, since Windows
cannot accurately report time changes faster than 10ms apart.

---------- BEGIN SOURCE ----------
Here is a prototype java library which illustrates the idea:

http://juliusdavies.ca/nanotime/

Download "nanotime.jar" and type:

java -jar nanotime.jar

It will print out 1 timing using System.currentTimeMillis(), and 10
timings using something like currentTimeNanos():

2008-05-14/17:08:15.940000000/PDT JavaTime
2008-05-14/17:08:15.940787900/PDT NativeTime
2008-05-14/17:08:15.943128363/PDT NativeTime
2008-05-14/17:08:15.943134021/PDT NativeTime
2008-05-14/17:08:15.943138875/PDT NativeTime
2008-05-14/17:08:15.943149498/PDT NativeTime
2008-05-14/17:08:15.943154580/PDT NativeTime
2008-05-14/17:08:15.943158845/PDT NativeTime
2008-05-14/17:08:15.943163060/PDT NativeTime
2008-05-14/17:08:15.943167074/PDT NativeTime
2008-05-14/17:08:15.943171148/PDT NativeTime

At this time the jar file can be run on Java 1.3, 1.4, 5, 6 on the
following platforms:

linux-amd64
linux-ppc (IBM's JDK)
linux-x86
mac-x86
solaris-sparc
win32


You need write-permission to ${user.home} to run this jar file.  It
automatically extracts a native library to ${user.home}/.libjnt/ for
the current platform.


Here is the JNI C code that asks the Operating System for the current time:

http://juliusdavies.ca/nanotime/src/native/mac_nano.c
http://juliusdavies.ca/nanotime/src/native/unix_nano.c
http://juliusdavies.ca/nanotime/src/native/win_nano.c


--
yours,

Julius Davies
250-592-2284 (Home)
250-893-4579 (Mobile)
http://juliusdavies.ca/

From dcholmes at optusnet.com.au  Tue Jun 24 23:25:52 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 25 Jun 2008 13:25:52 +1000
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
In-Reply-To: <598ad5b50806241712pc2ed76fi17f9c407cbe2b54a@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEFNHMAA.dcholmes@optusnet.com.au>

Hello Julius,

So what you are basically asking for is to have a method that exposes
clock_gettime(CLOCK_REALTIME) at whatever resolution the underlying OS
provides - with an understanding that Windows probably won't be able to do
better than the 10ms default time-of-day update. This method would represent
time-since-the-epoch in nanoseconds, and would also track changes in
wall-clock time (ntp updates, DST changes etc).

Is that the semantics you are looking for?

I ran some tests and modern Linux systems (2.6.21+) and Solaris seem to have
an update rate of a few microseconds for CLOCK_REALTIME - which is good. I
don't know how well it tracks tod changes. Older linux systems, eg 2.4
series seem to be stuck with the 10ms update rate. I don't know if the good
resolution on Linux is dependent on the fairly recent hrtimers work done in
the kernel - I don't have a huge range of systems to test on.

Regarding the return value, I don't see why we need anything other than a
long. A Java long can hold sufficient nanoseconds to represent 292+ years,
so there is no concern about overflow.

I still wonder exactly what people would use this time information for - it
would be necessary, but not sufficient to allow comparison of timestamps
across systems. I also wonder how well behaved clock_gettime is on MP
systems.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Julius
> Davies
> Sent: Wednesday, 25 June 2008 10:13 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Propose: System.currentTimeNanos()
>
>
> [Sorry if this gets double-posted....!]
>
> Hi,
>
> I opened an enhancement in Sun's bug database to ask for
> System.currentTimeNanos():
>
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908
>
> Normally I wouldn't harrass a random mailing list with my private
> ravings, but the "Evaluation" section in the bug points back to this
> list!  After reading the thread in question
> (http://www.nabble.com/High-resolution-timers-and-JSR-310-td173026
> 83.html),
> I did not think the currentTimeNanos() idea was actually addressed.
> The idea is *not* to compose System.currentTimeMillis() and
> System.nanoTime().  Composing those two methods just doesn't work, and
> never will.
>
> Instead the idea is to extend System.currentTimeMillis() a little - to
> nanosecond precision (when possible) - by returning a 2-element long
> array, and calling it System.getCurrentTimeNanos().  It appears that
> all the major OS's *except* Windows support this, and it can be
> reasonably faked out on Windows, even, in a single JNI call.
>
> In other words, the idea is to stop truncating what the OS is already
> giving java when we call System.currentMillis().
>
> I invite people to download the zip file and take a look:
>
> http://juliusdavies.ca/nanotime/nanotime.zip
>
> It builds on unix/mac-os-x with no problem.  On windows in compiles
> nicely if you have cygwin, gcc, and mingw.
>
>
> Here's a cut & paste from the Sun ticket (with minor edits on my part
> for clarity, as well as mention of the 2038 problem that might lurk
> behind gettimeofday()):
>
> ----------------------------------------------
> JUSTIFICATION :
> Solaris and Linux already support nanosecond precision thanks to
> clock_gettime(CLOCK_REALTIME).  System.currentTimeMillis() already
> uses that, but it would be nice to not truncate the nanoseconds.  The
> situation on Mac OSX is a little different, since only gettimeofday()
> is available, which gives microseconds (possible 32bit issues - e.g.
> infamous 2038?).  But it's still an improvement.
>
> Of course on Windows all timing information between the 10ms
> granularity is quasi-fictional, but for figuring out order-of-events
> in a log, a "high-resolution clock simulator" on windows would still
> be useful.
>
> EXPECTED VERSUS ACTUAL BEHAVIOR :
> EXPECTED -
> System.currentTimeNanos() would return a two-element long array
> containing {seconds, nanos} since epoch.  The time returned would be
> as accurate as the operating system could provide (e.g. nanos would be
> micros * 1000 on Mac OSX).
>
> On Windows the time returned would be an approximation, since Windows
> cannot accurately report time changes faster than 10ms apart.
>
> ---------- BEGIN SOURCE ----------
> Here is a prototype java library which illustrates the idea:
>
> http://juliusdavies.ca/nanotime/
>
> Download "nanotime.jar" and type:
>
> java -jar nanotime.jar
>
> It will print out 1 timing using System.currentTimeMillis(), and 10
> timings using something like currentTimeNanos():
>
> 2008-05-14/17:08:15.940000000/PDT JavaTime
> 2008-05-14/17:08:15.940787900/PDT NativeTime
> 2008-05-14/17:08:15.943128363/PDT NativeTime
> 2008-05-14/17:08:15.943134021/PDT NativeTime
> 2008-05-14/17:08:15.943138875/PDT NativeTime
> 2008-05-14/17:08:15.943149498/PDT NativeTime
> 2008-05-14/17:08:15.943154580/PDT NativeTime
> 2008-05-14/17:08:15.943158845/PDT NativeTime
> 2008-05-14/17:08:15.943163060/PDT NativeTime
> 2008-05-14/17:08:15.943167074/PDT NativeTime
> 2008-05-14/17:08:15.943171148/PDT NativeTime
>
> At this time the jar file can be run on Java 1.3, 1.4, 5, 6 on the
> following platforms:
>
> linux-amd64
> linux-ppc (IBM's JDK)
> linux-x86
> mac-x86
> solaris-sparc
> win32
>
>
> You need write-permission to ${user.home} to run this jar file.  It
> automatically extracts a native library to ${user.home}/.libjnt/ for
> the current platform.
>
>
> Here is the JNI C code that asks the Operating System for the
> current time:
>
> http://juliusdavies.ca/nanotime/src/native/mac_nano.c
> http://juliusdavies.ca/nanotime/src/native/unix_nano.c
> http://juliusdavies.ca/nanotime/src/native/win_nano.c
>
>
> --
> yours,
>
> Julius Davies
> 250-592-2284 (Home)
> 250-893-4579 (Mobile)
> http://juliusdavies.ca/
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From juliusdavies at gmail.com  Wed Jun 25 02:30:36 2008
From: juliusdavies at gmail.com (Julius Davies)
Date: Tue, 24 Jun 2008 23:30:36 -0700
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEFNHMAA.dcholmes@optusnet.com.au>
References: <598ad5b50806241712pc2ed76fi17f9c407cbe2b54a@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCKEFNHMAA.dcholmes@optusnet.com.au>
Message-ID: <598ad5b50806242330y540b1589na98d6c66d170e606@mail.gmail.com>

Hi, David,

Thanks so much for looking at this.  You've got it!  Expose
clock_gettime(CLOCK_REALTIME)!  (Or, in the worst case,
gettimeofday()).  I would really like that.

>
> So what you are basically asking for is to have a method that exposes
> clock_gettime(CLOCK_REALTIME) at whatever resolution the underlying OS
> provides - with an understanding that Windows probably won't be able to do
> better than the 10ms default time-of-day update. This method would represent
> time-since-the-epoch in nanoseconds, and would also track changes in
> wall-clock time (ntp updates, DST changes etc).

Yes!  (Do you think the windows "hires simulator" is worth bothering
with?  It does have some weird behaviour when NTP is slewing, but how
often does that happen?

Here's an example on Windows with clock slew (scroll down a little):
https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1274

Here's the "hires simulator" code:
http://juliusdavies.ca/nanotime/src/native/win_nano.c

>
> I ran some tests and modern Linux systems (2.6.21+) and Solaris seem to have
> an update rate of a few microseconds for CLOCK_REALTIME - which is good. I
> don't know how well it tracks tod changes. Older linux systems, eg 2.4
> series seem to be stuck with the 10ms update rate. I don't know if the good
> resolution on Linux is dependent on the fairly recent hrtimers work done in
> the kernel - I don't have a huge range of systems to test on.
>

I'm getting the same results on amd64 linux and sparc solaris:  about
1 micro.  I'm curious if multi-processor / multi-core would allow
those 1 micro reads to be staggered such that throughput was more like
1 read every 500 nanos.  For example:

CPU1  2500 nanos  CPU2 3000 nanos
CPU1  3500 nanos  CPU2 4000 nanos
CPU1  4500 nanos  CPU2 5000 nanos

What I'm really getting at is this:  is it really an update rate of 1
micro, or is the update rate really good, and the problem is that the
code itself takes 1 micro to execute?  If that's the case, maybe the
update rate is actually in the nano range, and even on a 64 core
system, the timestamps will tend to never be unique!?  (e.g. within
1000/64 nanos of each other)  That would be awesome!

I imagine clock_gettime(CLOCK_REALTIME) tracks TOD changes quite well
- doesn't System.currentTimeMillis() already use it?


> Regarding the return value, I don't see why we need anything other than a
> long. A Java long can hold sufficient nanoseconds to represent 292+ years,
> so there is no concern about overflow.
>

I think a 2-element long array is the way to go.  JSR-310 is already
taking that approach (well... long + int).  Clock_gettime() also does
the same thing (the timespec struct is also essentially long + int,
a.k.a. long long + long).  I guess we save 12 bytes by going with just
one long?  Is it worth it?

> I still wonder exactly what people would use this time information for - it
> would be necessary, but not sufficient to allow comparison of timestamps
> across systems. I also wonder how well behaved clock_gettime is on MP
> systems.
>

Here's my usecase:

http://juliusdavies.ca/logging.html#timestamps

In addition to that, nanosecond precision makes it much more possible
for log timestamps to become part of a primary key in a natural way
that preserves chronology.

It's not really about "do we need nanoseconds."  It's more about "how
can we track order of events on a heavily loaded multi-core system?"
I think it's obvious that millis is not good enough.  Micros is
probably good enough.  Might as well overshoot with nanos, to be safe.

As for distributed systems... well, you can't even assume clocks are
set to the same century.  It's just not applicable to this discussion.
 It's just order-of-events on a single system.... err... well, a
single-clock, whatever that might be.


> Cheers,
> David Holmes
>

Again, thanks for reading!  If you think it's a good idea, would you
mind commenting over at the bug?

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908


yours,

Julius

ps.  when it comes to java timestamps, all roads lead to
http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks .
Thanks for your original blog post.  It was very helpful.



>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Julius
>> Davies
>> Sent: Wednesday, 25 June 2008 10:13 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Propose: System.currentTimeNanos()
>>
>>
>> [Sorry if this gets double-posted....!]
>>
>> Hi,
>>
>> I opened an enhancement in Sun's bug database to ask for
>> System.currentTimeNanos():
>>
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908
>>
>> Normally I wouldn't harrass a random mailing list with my private
>> ravings, but the "Evaluation" section in the bug points back to this
>> list!  After reading the thread in question
>> (http://www.nabble.com/High-resolution-timers-and-JSR-310-td173026
>> 83.html),
>> I did not think the currentTimeNanos() idea was actually addressed.
>> The idea is *not* to compose System.currentTimeMillis() and
>> System.nanoTime().  Composing those two methods just doesn't work, and
>> never will.
>>
>> Instead the idea is to extend System.currentTimeMillis() a little - to
>> nanosecond precision (when possible) - by returning a 2-element long
>> array, and calling it System.getCurrentTimeNanos().  It appears that
>> all the major OS's *except* Windows support this, and it can be
>> reasonably faked out on Windows, even, in a single JNI call.
>>
>> In other words, the idea is to stop truncating what the OS is already
>> giving java when we call System.currentMillis().
>>
>> I invite people to download the zip file and take a look:
>>
>> http://juliusdavies.ca/nanotime/nanotime.zip
>>
>> It builds on unix/mac-os-x with no problem.  On windows in compiles
>> nicely if you have cygwin, gcc, and mingw.
>>
>>
>> Here's a cut & paste from the Sun ticket (with minor edits on my part
>> for clarity, as well as mention of the 2038 problem that might lurk
>> behind gettimeofday()):
>>
>> ----------------------------------------------
>> JUSTIFICATION :
>> Solaris and Linux already support nanosecond precision thanks to
>> clock_gettime(CLOCK_REALTIME).  System.currentTimeMillis() already
>> uses that, but it would be nice to not truncate the nanoseconds.  The
>> situation on Mac OSX is a little different, since only gettimeofday()
>> is available, which gives microseconds (possible 32bit issues - e.g.
>> infamous 2038?).  But it's still an improvement.
>>
>> Of course on Windows all timing information between the 10ms
>> granularity is quasi-fictional, but for figuring out order-of-events
>> in a log, a "high-resolution clock simulator" on windows would still
>> be useful.
>>
>> EXPECTED VERSUS ACTUAL BEHAVIOR :
>> EXPECTED -
>> System.currentTimeNanos() would return a two-element long array
>> containing {seconds, nanos} since epoch.  The time returned would be
>> as accurate as the operating system could provide (e.g. nanos would be
>> micros * 1000 on Mac OSX).
>>
>> On Windows the time returned would be an approximation, since Windows
>> cannot accurately report time changes faster than 10ms apart.
>>
>> ---------- BEGIN SOURCE ----------
>> Here is a prototype java library which illustrates the idea:
>>
>> http://juliusdavies.ca/nanotime/
>>
>> Download "nanotime.jar" and type:
>>
>> java -jar nanotime.jar
>>
>> It will print out 1 timing using System.currentTimeMillis(), and 10
>> timings using something like currentTimeNanos():
>>
>> 2008-05-14/17:08:15.940000000/PDT JavaTime
>> 2008-05-14/17:08:15.940787900/PDT NativeTime
>> 2008-05-14/17:08:15.943128363/PDT NativeTime
>> 2008-05-14/17:08:15.943134021/PDT NativeTime
>> 2008-05-14/17:08:15.943138875/PDT NativeTime
>> 2008-05-14/17:08:15.943149498/PDT NativeTime
>> 2008-05-14/17:08:15.943154580/PDT NativeTime
>> 2008-05-14/17:08:15.943158845/PDT NativeTime
>> 2008-05-14/17:08:15.943163060/PDT NativeTime
>> 2008-05-14/17:08:15.943167074/PDT NativeTime
>> 2008-05-14/17:08:15.943171148/PDT NativeTime
>>
>> At this time the jar file can be run on Java 1.3, 1.4, 5, 6 on the
>> following platforms:
>>
>> linux-amd64
>> linux-ppc (IBM's JDK)
>> linux-x86
>> mac-x86
>> solaris-sparc
>> win32
>>
>>
>> You need write-permission to ${user.home} to run this jar file.  It
>> automatically extracts a native library to ${user.home}/.libjnt/ for
>> the current platform.
>>
>>
>> Here is the JNI C code that asks the Operating System for the
>> current time:
>>
>> http://juliusdavies.ca/nanotime/src/native/mac_nano.c
>> http://juliusdavies.ca/nanotime/src/native/unix_nano.c
>> http://juliusdavies.ca/nanotime/src/native/win_nano.c
>>
>>
>> --
>> yours,
>>
>> Julius Davies
>> 250-592-2284 (Home)
>> 250-893-4579 (Mobile)
>> http://juliusdavies.ca/
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>



-- 
yours,

Julius Davies
250-592-2284 (Home)
250-893-4579 (Mobile)
http://juliusdavies.ca/

From dcholmes at optusnet.com.au  Wed Jun 25 03:22:45 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 25 Jun 2008 17:22:45 +1000
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
In-Reply-To: <598ad5b50806242330y540b1589na98d6c66d170e606@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEFPHMAA.dcholmes@optusnet.com.au>

Hi Julius,

> Thanks so much for looking at this.  You've got it!  Expose
> clock_gettime(CLOCK_REALTIME)!  (Or, in the worst case,
> gettimeofday()).  I would really like that.

Doesn't seem unreasonable. :)

> Yes!  (Do you think the windows "hires simulator" is worth bothering
> with?  It does have some weird behaviour when NTP is slewing, but how
> often does that happen?
>
> Here's an example on Windows with clock slew (scroll down a little):
> https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1274
>
> Here's the "hires simulator" code:
> http://juliusdavies.ca/nanotime/src/native/win_nano.c

The cost/benefit ratio takes a dive if you have to play these sorts of
tricks. And any attempt to convert from "cycles" to "time" is fraught with
peril unless you're certain you have a well behaved Windows that uses a
fixed-frequency time source.

Note of course that you could become an OpenJDK contributor and offer up
what you've already done. In these OpenSource days it's sometimes hard to
see what the path is for getting small API updates into the system - the
code ends up being the easy part.

> What I'm really getting at is this:  is it really an update rate of 1
> micro, or is the update rate really good, and the problem is that the
> code itself takes 1 micro to execute?  If that's the case, maybe the
> update rate is actually in the nano range, and even on a 64 core
> system, the timestamps will tend to never be unique!?  (e.g. within
> 1000/64 nanos of each other)  That would be awesome!

:) Undoubtedly the call itself takes some time. You'd have to delve into the
details of the OS and the hardware time source to determine what it's actual
update rate is. But you'll never be guaranteed unique values.

> I imagine clock_gettime(CLOCK_REALTIME) tracks TOD changes quite well
> - doesn't System.currentTimeMillis() already use it?

Nope. currentTimeMillis() uses gettimeofday(). It's a very old piece of
code. I should probably walk through the Solaris code sometime to see where
gettimeofday and clock_gettime meet up.

> I think a 2-element long array is the way to go.  JSR-310 is already
> taking that approach (well... long + int).  Clock_gettime() also does
> the same thing (the timespec struct is also essentially long + int,
> a.k.a. long long + long).  I guess we save 12 bytes by going with just
> one long?  Is it worth it?

Definitely! Dealing with an array is a PITA as far as I am concerned. I
think most people would take the array and turn it into a single value - so
lets do that for them. In the real-time Java spec there was obviously far
too much 'timespec' influence as they define time objects as a
(millis,nanos) pair and they are a real pain to deal with in my opinion -
the first thing I did (on a previous project) was to change the internal
representation to use a long. :)

Think of the simplest usecase: printing the value - you can't do
System.out.print(System.currentTimeNanos()); if it returns an array (well
you can but you won't see the timestamp :) ).

> Here's my usecase:
>
> http://juliusdavies.ca/logging.html#timestamps
>
> In addition to that, nanosecond precision makes it much more possible
> for log timestamps to become part of a primary key in a natural way
> that preserves chronology.

If you are only interested in comparative ordering then System.nanoTime will
give you as much 'uniqueness' as what we're discussing here. Seeing the date
can be useful, but if your timestamp tracks wall-clock time then your logs
are going to have duplication around DST changes. I suppose it's only once a
year that things get screwed up but someone still has to deal with that. And
ntp updates also mess things up. A logger could output a date +
System.nanoTime value which would aid correlation with wall-clock time while
still giving (modulo bugs!) a monotonic timestamp.

Just my 2c.

> Again, thanks for reading!  If you think it's a good idea, would you
> mind commenting over at the bug?
>
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908

I already added a new section to the evaluation.

> ps.  when it comes to java timestamps, all roads lead to
> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks .
> Thanks for your original blog post.  It was very helpful.

Glad to hear it. It's getting a little dated now. And I never got around to
discussing Solaris or Linux. The sad thing is that what I did discover is
that you simply can't trust the OS to get it right - so many bugs ... at
least on x86. :(

Cheers,
David Holmes


From juliusdavies at gmail.com  Wed Jun 25 05:04:55 2008
From: juliusdavies at gmail.com (Julius Davies)
Date: Wed, 25 Jun 2008 02:04:55 -0700
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEFPHMAA.dcholmes@optusnet.com.au>
References: <598ad5b50806242330y540b1589na98d6c66d170e606@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEFPHMAA.dcholmes@optusnet.com.au>
Message-ID: <598ad5b50806250204y665cd2c5i87dd4a23f0f1fc66@mail.gmail.com>

Hi, David,

Thanks for the reply!  I just have a tiny little note of interest.

I tried running four Java6 processes concurrently.

The 1 micro stayed the same.

But each call took 4 micros instead of 1 micro as it did before.
Throughput stayed the same even though there were 4 running JVM's
instead of 1, and latency increased.  I guess linux is the bottleneck
(64bit 2.6.23 dual-core turion laptop)?  Every nanosecond timestamp
was unique, but they were also all around 1 micro apart.  One last
thing:  performance for currentTimeMillis() went south in the exact
same way:  4 micros per call, instead of 1 micro.

Oh, and you're right:  I don't just want order-of-events.  I want
time-of-day, and then I want order-of-events for everything in the
same second (or log4j's best guess at order-of-events... there's a
whole other can of worms) - so yeah, a hybrid, just like you're
saying.  But having both millis() and the monotonic in the same line -
it does the job, but it's not very friendly for human eyes.  (And how
many zeroes should I pad nanoTime() with?)  (I will admit that sort -n
fixes the zero-padding problem).

The DST thing isn't a problem, because all good logs include TZ, or
GMT offset at the very least.  Or they're in UTC.  Or they use a
cheeky trick with a semi-colon:

(Here's the cheeky trick:)
https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1330



yours,

Julius




On Wed, Jun 25, 2008 at 12:22 AM, David Holmes <dcholmes at optusnet.com.au> wrote:
> Hi Julius,
>
>> Thanks so much for looking at this.  You've got it!  Expose
>> clock_gettime(CLOCK_REALTIME)!  (Or, in the worst case,
>> gettimeofday()).  I would really like that.
>
> Doesn't seem unreasonable. :)
>
>> Yes!  (Do you think the windows "hires simulator" is worth bothering
>> with?  It does have some weird behaviour when NTP is slewing, but how
>> often does that happen?
>>
>> Here's an example on Windows with clock slew (scroll down a little):
>> https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1274
>>
>> Here's the "hires simulator" code:
>> http://juliusdavies.ca/nanotime/src/native/win_nano.c
>
> The cost/benefit ratio takes a dive if you have to play these sorts of
> tricks. And any attempt to convert from "cycles" to "time" is fraught with
> peril unless you're certain you have a well behaved Windows that uses a
> fixed-frequency time source.
>
> Note of course that you could become an OpenJDK contributor and offer up
> what you've already done. In these OpenSource days it's sometimes hard to
> see what the path is for getting small API updates into the system - the
> code ends up being the easy part.
>
>> What I'm really getting at is this:  is it really an update rate of 1
>> micro, or is the update rate really good, and the problem is that the
>> code itself takes 1 micro to execute?  If that's the case, maybe the
>> update rate is actually in the nano range, and even on a 64 core
>> system, the timestamps will tend to never be unique!?  (e.g. within
>> 1000/64 nanos of each other)  That would be awesome!
>
> :) Undoubtedly the call itself takes some time. You'd have to delve into the
> details of the OS and the hardware time source to determine what it's actual
> update rate is. But you'll never be guaranteed unique values.
>
>> I imagine clock_gettime(CLOCK_REALTIME) tracks TOD changes quite well
>> - doesn't System.currentTimeMillis() already use it?
>
> Nope. currentTimeMillis() uses gettimeofday(). It's a very old piece of
> code. I should probably walk through the Solaris code sometime to see where
> gettimeofday and clock_gettime meet up.
>
>> I think a 2-element long array is the way to go.  JSR-310 is already
>> taking that approach (well... long + int).  Clock_gettime() also does
>> the same thing (the timespec struct is also essentially long + int,
>> a.k.a. long long + long).  I guess we save 12 bytes by going with just
>> one long?  Is it worth it?
>
> Definitely! Dealing with an array is a PITA as far as I am concerned. I
> think most people would take the array and turn it into a single value - so
> lets do that for them. In the real-time Java spec there was obviously far
> too much 'timespec' influence as they define time objects as a
> (millis,nanos) pair and they are a real pain to deal with in my opinion -
> the first thing I did (on a previous project) was to change the internal
> representation to use a long. :)
>
> Think of the simplest usecase: printing the value - you can't do
> System.out.print(System.currentTimeNanos()); if it returns an array (well
> you can but you won't see the timestamp :) ).
>
>> Here's my usecase:
>>
>> http://juliusdavies.ca/logging.html#timestamps
>>
>> In addition to that, nanosecond precision makes it much more possible
>> for log timestamps to become part of a primary key in a natural way
>> that preserves chronology.
>
> If you are only interested in comparative ordering then System.nanoTime will
> give you as much 'uniqueness' as what we're discussing here. Seeing the date
> can be useful, but if your timestamp tracks wall-clock time then your logs
> are going to have duplication around DST changes. I suppose it's only once a
> year that things get screwed up but someone still has to deal with that. And
> ntp updates also mess things up. A logger could output a date +
> System.nanoTime value which would aid correlation with wall-clock time while
> still giving (modulo bugs!) a monotonic timestamp.
>
> Just my 2c.
>
>> Again, thanks for reading!  If you think it's a good idea, would you
>> mind commenting over at the bug?
>>
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908
>
> I already added a new section to the evaluation.
>
>> ps.  when it comes to java timestamps, all roads lead to
>> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks .
>> Thanks for your original blog post.  It was very helpful.
>
> Glad to hear it. It's getting a little dated now. And I never got around to
> discussing Solaris or Linux. The sad thing is that what I did discover is
> that you simply can't trust the OS to get it right - so many bugs ... at
> least on x86. :(
>
> Cheers,
> David Holmes
>
>



-- 
yours,

Julius Davies
250-592-2284 (Home)
250-893-4579 (Mobile)
http://juliusdavies.ca/

From dcholmes at optusnet.com.au  Wed Jun 25 05:17:51 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 25 Jun 2008 19:17:51 +1000
Subject: [concurrency-interest] Propose: System.currentTimeNanos()
In-Reply-To: <598ad5b50806250204y665cd2c5i87dd4a23f0f1fc66@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEGAHMAA.dcholmes@optusnet.com.au>

Hi Julius,

I could counter that as long as they are in order and will sort correctly,
then does it really matter what you see?

It just seems to me that having the timestamp be wall-clock time is more a
convenience than a necessity. And that "cheeky trick" for the DST extra hour
is pretty gross :) - and how does the formatter know whether a value is from
the original hour or the extra one? And there's still the more common
ntp-like adjustments that can cause things to go out of order.

Anyway plenty of things for others to comment on if we give them the chance.

Cheers,
David


> -----Original Message-----
> From: Julius Davies [mailto:juliusdavies at gmail.com]
> Sent: Wednesday, 25 June 2008 7:05 PM
> To: concurrency-interest at cs.oswego.edu; dholmes at ieee.org
> Subject: Re: [concurrency-interest] Propose: System.currentTimeNanos()
>
>
> Hi, David,
>
> Thanks for the reply!  I just have a tiny little note of interest.
>
> I tried running four Java6 processes concurrently.
>
> The 1 micro stayed the same.
>
> But each call took 4 micros instead of 1 micro as it did before.
> Throughput stayed the same even though there were 4 running JVM's
> instead of 1, and latency increased.  I guess linux is the bottleneck
> (64bit 2.6.23 dual-core turion laptop)?  Every nanosecond timestamp
> was unique, but they were also all around 1 micro apart.  One last
> thing:  performance for currentTimeMillis() went south in the exact
> same way:  4 micros per call, instead of 1 micro.
>
> Oh, and you're right:  I don't just want order-of-events.  I want
> time-of-day, and then I want order-of-events for everything in the
> same second (or log4j's best guess at order-of-events... there's a
> whole other can of worms) - so yeah, a hybrid, just like you're
> saying.  But having both millis() and the monotonic in the same line -
> it does the job, but it's not very friendly for human eyes.  (And how
> many zeroes should I pad nanoTime() with?)  (I will admit that sort -n
> fixes the zero-padding problem).
>
> The DST thing isn't a problem, because all good logs include TZ, or
> GMT offset at the very least.  Or they're in UTC.  Or they use a
> cheeky trick with a semi-colon:
>
> (Here's the cheeky trick:)
> https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1330
>
>
>
> yours,
>
> Julius
>
>
>
>
> On Wed, Jun 25, 2008 at 12:22 AM, David Holmes
> <dcholmes at optusnet.com.au> wrote:
> > Hi Julius,
> >
> >> Thanks so much for looking at this.  You've got it!  Expose
> >> clock_gettime(CLOCK_REALTIME)!  (Or, in the worst case,
> >> gettimeofday()).  I would really like that.
> >
> > Doesn't seem unreasonable. :)
> >
> >> Yes!  (Do you think the windows "hires simulator" is worth bothering
> >> with?  It does have some weird behaviour when NTP is slewing, but how
> >> often does that happen?
> >>
> >> Here's an example on Windows with clock slew (scroll down a little):
> >> https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1274
> >>
> >> Here's the "hires simulator" code:
> >> http://juliusdavies.ca/nanotime/src/native/win_nano.c
> >
> > The cost/benefit ratio takes a dive if you have to play these sorts of
> > tricks. And any attempt to convert from "cycles" to "time" is
> fraught with
> > peril unless you're certain you have a well behaved Windows that uses a
> > fixed-frequency time source.
> >
> > Note of course that you could become an OpenJDK contributor and offer up
> > what you've already done. In these OpenSource days it's
> sometimes hard to
> > see what the path is for getting small API updates into the system - the
> > code ends up being the easy part.
> >
> >> What I'm really getting at is this:  is it really an update rate of 1
> >> micro, or is the update rate really good, and the problem is that the
> >> code itself takes 1 micro to execute?  If that's the case, maybe the
> >> update rate is actually in the nano range, and even on a 64 core
> >> system, the timestamps will tend to never be unique!?  (e.g. within
> >> 1000/64 nanos of each other)  That would be awesome!
> >
> > :) Undoubtedly the call itself takes some time. You'd have to
> delve into the
> > details of the OS and the hardware time source to determine
> what it's actual
> > update rate is. But you'll never be guaranteed unique values.
> >
> >> I imagine clock_gettime(CLOCK_REALTIME) tracks TOD changes quite well
> >> - doesn't System.currentTimeMillis() already use it?
> >
> > Nope. currentTimeMillis() uses gettimeofday(). It's a very old piece of
> > code. I should probably walk through the Solaris code sometime
> to see where
> > gettimeofday and clock_gettime meet up.
> >
> >> I think a 2-element long array is the way to go.  JSR-310 is already
> >> taking that approach (well... long + int).  Clock_gettime() also does
> >> the same thing (the timespec struct is also essentially long + int,
> >> a.k.a. long long + long).  I guess we save 12 bytes by going with just
> >> one long?  Is it worth it?
> >
> > Definitely! Dealing with an array is a PITA as far as I am concerned. I
> > think most people would take the array and turn it into a
> single value - so
> > lets do that for them. In the real-time Java spec there was
> obviously far
> > too much 'timespec' influence as they define time objects as a
> > (millis,nanos) pair and they are a real pain to deal with in my
> opinion -
> > the first thing I did (on a previous project) was to change the internal
> > representation to use a long. :)
> >
> > Think of the simplest usecase: printing the value - you can't do
> > System.out.print(System.currentTimeNanos()); if it returns an
> array (well
> > you can but you won't see the timestamp :) ).
> >
> >> Here's my usecase:
> >>
> >> http://juliusdavies.ca/logging.html#timestamps
> >>
> >> In addition to that, nanosecond precision makes it much more possible
> >> for log timestamps to become part of a primary key in a natural way
> >> that preserves chronology.
> >
> > If you are only interested in comparative ordering then
> System.nanoTime will
> > give you as much 'uniqueness' as what we're discussing here.
> Seeing the date
> > can be useful, but if your timestamp tracks wall-clock time
> then your logs
> > are going to have duplication around DST changes. I suppose
> it's only once a
> > year that things get screwed up but someone still has to deal
> with that. And
> > ntp updates also mess things up. A logger could output a date +
> > System.nanoTime value which would aid correlation with
> wall-clock time while
> > still giving (modulo bugs!) a monotonic timestamp.
> >
> > Just my 2c.
> >
> >> Again, thanks for reading!  If you think it's a good idea, would you
> >> mind commenting over at the bug?
> >>
> >> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6709908
> >
> > I already added a new section to the evaluation.
> >
> >> ps.  when it comes to java timestamps, all roads lead to
> >> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks .
> >> Thanks for your original blog post.  It was very helpful.
> >
> > Glad to hear it. It's getting a little dated now. And I never
> got around to
> > discussing Solaris or Linux. The sad thing is that what I did
> discover is
> > that you simply can't trust the OS to get it right - so many bugs ... at
> > least on x86. :(
> >
> > Cheers,
> > David Holmes
> >
> >
>
>
>
> --
> yours,
>
> Julius Davies
> 250-592-2284 (Home)
> 250-893-4579 (Mobile)
> http://juliusdavies.ca/


From gkorland at gmail.com  Thu Jun 26 09:48:28 2008
From: gkorland at gmail.com (Guy Korland)
Date: Thu, 26 Jun 2008 16:48:28 +0300
Subject: [concurrency-interest] ReentrantReadWriteLock fair lock
Message-ID: <79be5fa30806260648h6b2a0e16t9895bf0f6e16ac9d@mail.gmail.com>

Hi,

Is there a good reason why
edu.emory.mathcs.backport.java.util.concurrent.ReentrantReadWriteLock
doesn't include a fair constructor?

-- 
Guy Korland
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080626/41cb609e/attachment.html 

