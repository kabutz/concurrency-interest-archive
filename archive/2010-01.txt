From i30817 at gmail.com  Fri Jan  1 10:27:46 2010
From: i30817 at gmail.com (Paulo Levi)
Date: Fri, 1 Jan 2010 15:27:46 +0000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEKHIEAA.davidcholmes@aapt.net.au>
References: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCEEKHIEAA.davidcholmes@aapt.net.au>
Message-ID: <212322091001010727r72f53fbbs983cac81025724d8@mail.gmail.com>

I forgot to "reply to all" and had some responses that were only being sent
to Mr. Andreou. I will transcribe them.

For the record i think this might be a combination of the state Mr. Berlin
says he as seen and the problem in the link in the last reply bellow. I
can't get a thread dump when this state happens but i can do it using a
Shutdown Hook (presumably before the jre shutdowns)

This are the missing responses - i will now try to comment the invokeLater
part of my tasks to see if they terminate now:


---------- Forwarded message ----------
From: Paulo Levi <i30817 at gmail.com>
Date: Thu, Dec 31, 2009 at 2:57 AM
Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
To: Dimitris Andreou <jim.andreou at gmail.com>


Well, after adding the shutdown hook for the ThreadPoolExecutors subclass
and calling shutdownnow on them inside it, it always terminated (when it
almost always didn't in this dual core). Its possible i'm doing something
wrong in the tasks (there is a URL connection opened there and a file
written/read) that the interrupt call of the shutdownnow "fixes". That is
what i suspect.


On Thu, Dec 31, 2009 at 2:45 AM, Dimitris Andreou <jim.andreou at gmail.com>wrote:

> Hi Paulo,
>
> The application can't hang just because a daemon thread hangs. It's
> the other way around - daemon threads live because there is a
> non-daemon thread still active. Are you sure you terminate all
> non-daemon threads when the application is "closed"?
>
> Dimitris
>
> 2009/12/31 Paulo Levi <i30817 at gmail.com>:
> > I have a TPE subclass and i'm seeing (disturbingly only on some dual core
> > processors) a hang when the application is closed and there are lots of
> > tasks to be processed. Specifically no-more tasks appear to be processed,
> > but the (daemon) threads are still alive at the time shutdown hooks are
> > called.
> > (daemon, alive, not interrupted).
> >
> > Threadfactory used by the subclass creates daemon threads, and the class
> is
> > configurated with a keepAliveTime > 0 in this case (to reuse the threads)
> > and a LIFO queue instead of the normal one.
> >
> > I appear to be able to avoid it with a shutdown hook, but i would like to
> > know if all ThreadPoolExecutors are vulnerable to this.
> >
> > (disturbingly too, after this, the debugger says the java process is
> closed,
> > however windows task manager disagrees).
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>


---------- Forwarded message ----------
From: Paulo Levi <i30817 at gmail.com>
Date: Thu, Dec 31, 2009 at 3:11 AM
Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
To: Dimitris Andreou <jim.andreou at gmail.com>


Besides there is the very suspicious fact that i couldn't get a thread dump
when i closed the main window,(CTRL+Break right?) and the profiler/debugger
told me the application was finished but the console didn't terminate (same
thing in the command-line console). That is what blew my mind.

The alive non-demon threads alive at the time the shudown hooks run are :
AWT-Shutdown
AWT-EventQueue-0
DestroyJavaVM

(there are some !isAlive()).



---------- Forwarded message ----------
From: Paulo Levi <i30817 at gmail.com>
Date: Thu, Dec 31, 2009 at 3:17 AM
Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
To: Dimitris Andreou <jim.andreou at gmail.com>


Now seeing that AWT-Shutdown, its possible that one of my tasks posts a
event to the EDT (as described here : http://www.pushing-pixels.org/?p=369).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100101/707dd06a/attachment.html>

From jim.andreou at gmail.com  Fri Jan  1 11:17:21 2010
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Fri, 1 Jan 2010 18:17:21 +0200
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <212322091001010727r72f53fbbs983cac81025724d8@mail.gmail.com>
References: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEKHIEAA.davidcholmes@aapt.net.au>
	<212322091001010727r72f53fbbs983cac81025724d8@mail.gmail.com>
Message-ID: <7d7138c11001010817n3e41af0vc138ce8638197448@mail.gmail.com>

What is defined to happen if a shutdown hook starts a new (non-daemon)
thread? My suspicion on what happens in Paolo's case is that the VM
was about to exist because the last swing window was closed (using
WindowConstants.DISPOSE_ON_CLOSE), thus the EDT died, but then a hook
invoked SwingUtilities.invokeLater which summoned a new EDT (which I
assume it would enter a new event loop, instead of terminating
itself).

Dimitris

2010/1/1 Paulo Levi <i30817 at gmail.com>:
> I forgot to "reply to all" and had some responses that were only being sent
> to Mr. Andreou. I will transcribe them.
>
> For the record i think this might be a combination of the state Mr. Berlin
> says he as seen and the problem in the link in the last reply bellow. I
> can't get a thread dump when this state happens but i can do it using a
> Shutdown Hook (presumably before the jre shutdowns)
>
> This are the missing responses - i will now try to comment the invokeLater
> part of my tasks to see if they terminate now:
>
>
> ---------- Forwarded message ----------
> From: Paulo Levi <i30817 at gmail.com>
> Date: Thu, Dec 31, 2009 at 2:57 AM
> Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> To: Dimitris Andreou <jim.andreou at gmail.com>
>
>
> Well, after adding the shutdown hook for the ThreadPoolExecutors subclass
> and calling shutdownnow on them inside it, it always terminated (when it
> almost always didn't in this dual core). Its possible i'm doing something
> wrong in the tasks (there is a URL connection opened there and a file
> written/read) that the interrupt call of the shutdownnow "fixes". That is
> what i suspect.
>
> On Thu, Dec 31, 2009 at 2:45 AM, Dimitris Andreou <jim.andreou at gmail.com>
> wrote:
>>
>> Hi Paulo,
>>
>> The application can't hang just because a daemon thread hangs. It's
>> the other way around - daemon threads live because there is a
>> non-daemon thread still active. Are you sure you terminate all
>> non-daemon threads when the application is "closed"?
>>
>> Dimitris
>>
>> 2009/12/31 Paulo Levi <i30817 at gmail.com>:
>> > I have a TPE subclass and i'm seeing (disturbingly only on some dual
>> > core
>> > processors) a hang when the application is closed and there are lots of
>> > tasks to be processed. Specifically no-more tasks appear to be
>> > processed,
>> > but the (daemon) threads are still alive at the time shutdown hooks are
>> > called.
>> > (daemon, alive, not interrupted).
>> >
>> > Threadfactory used by the subclass creates daemon threads, and the class
>> > is
>> > configurated with a keepAliveTime > 0 in this case (to reuse the
>> > threads)
>> > and a LIFO queue instead of the normal one.
>> >
>> > I appear to be able to avoid it with a shutdown hook, but i would like
>> > to
>> > know if all ThreadPoolExecutors are vulnerable to this.
>> >
>> > (disturbingly too, after this, the debugger says the java process is
>> > closed,
>> > however windows task manager disagrees).
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>
>
> ---------- Forwarded message ----------
> From: Paulo Levi <i30817 at gmail.com>
> Date: Thu, Dec 31, 2009 at 3:11 AM
> Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> To: Dimitris Andreou <jim.andreou at gmail.com>
>
>
> Besides there is the very suspicious fact that i couldn't get a thread dump
> when i closed the main window,(CTRL+Break right?) and the profiler/debugger
> told me the application was finished but the console didn't terminate (same
> thing in the command-line console). That is what blew my mind.
>
> The alive non-demon threads alive at the time the shudown hooks run are :
> AWT-Shutdown
> AWT-EventQueue-0
> DestroyJavaVM
>
> (there are some !isAlive()).
>
>
>
> ---------- Forwarded message ----------
> From: Paulo Levi <i30817 at gmail.com>
> Date: Thu, Dec 31, 2009 at 3:17 AM
> Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> To: Dimitris Andreou <jim.andreou at gmail.com>
>
>
> Now seeing that AWT-Shutdown, its possible that one of my tasks posts a
> event to the EDT (as described here : http://www.pushing-pixels.org/?p=369
> ).
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From i30817 at gmail.com  Fri Jan  1 13:21:36 2010
From: i30817 at gmail.com (Paulo Levi)
Date: Fri, 1 Jan 2010 18:21:36 +0000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <7d7138c11001010817n3e41af0vc138ce8638197448@mail.gmail.com>
References: <212322090912301738i598a3c74t1d343e6dec026a2b@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCEEKHIEAA.davidcholmes@aapt.net.au>
	<212322091001010727r72f53fbbs983cac81025724d8@mail.gmail.com> 
	<7d7138c11001010817n3e41af0vc138ce8638197448@mail.gmail.com>
Message-ID: <212322091001011021x4b0d8430rf22f09b59b0a3f91@mail.gmail.com>

It doesn't appear to be it :(

I used Xbootclasspath/p: to use a new EventQueue class that made a
system.out.println with a counter on the
private void postEvent(AWTEvent theEvent, int priority) that is used by all
event posting and it doesnt increment after closing.

It appears to be something else that is keeping the AWT thread alive.
Using a shutdown hook to dump the stack of the anomalous (?) alive
AWT-EventQueue-0
gives this
....
EVENT 496
EVENT 497
java.lang.Object.wait(Native Method)
EVENT 498
java.lang.Thread.join(Thread.java:1269)
java.lang.Thread.join(Thread.java:1343)
java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
java.lang.Shutdown.runHooks(Shutdown.java:123)
java.lang.Shutdown.sequence(Shutdown.java:167)
java.lang.Shutdown.exit(Shutdown.java:212)
java.lang.Runtime.exit(Runtime.java:107)
java.lang.System.exit(System.java:954)
javax.swing.JFrame.processWindowEvent(JFrame.java:312)
java.awt.Window.processEvent(Window.java:1961)
java.awt.Component.dispatchEventImpl(Component.java:4790)
java.awt.Container.dispatchEventImpl(Container.java:2261)
java.awt.Window.dispatchEventImpl(Window.java:2671)
java.awt.Component.dispatchEvent(Component.java:4616)
java.awt.EventQueue.dispatchEvent(EventQueue.java:654)
java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:255)
java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:170)
java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:160)
java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
java.awt.EventDispatchThread.run(EventDispatchThread.java:136)


So it appears to be normal that AWT runs the shutdownhooks and waits for
them to die before exiting. Commenting the main shutdown hook of my
application doesn't have a effect.

Then i also replaced the Shutdown auxilialy class to see if the application
progressed beyond the java.lang.Shutdown.sequence call (disabled the event
printing for legibility). I put a dump of all non-daemon alive threads
before the native halt0 method. This is the result in a hanging case.

current thread = AWT-EventQueue-0
Stack of AWT-Shutdown:
java.lang.Object.wait(Native Method)
java.lang.Object.wait(Object.java:502)
sun.awt.AWTAutoShutdown.run(AWTAutoShutdown.java:287)
java.lang.Thread.run(Thread.java:717)

Stack of AWT-EventQueue-0:
java.lang.Thread.getStackTrace(Thread.java:1578)
java.lang.Shutdown.halt(Shutdown.java:148)
java.lang.Shutdown.exit(Shutdown.java:238)
java.lang.Runtime.exit(Runtime.java:107)
java.lang.System.exit(System.java:954)
javax.swing.JFrame.processWindowEvent(JFrame.java:312)
java.awt.Window.processEvent(Window.java:1961)
java.awt.Component.dispatchEventImpl(Component.java:4790)
java.awt.Container.dispatchEventImpl(Container.java:2261)
java.awt.Window.dispatchEventImpl(Window.java:2671)
java.awt.Component.dispatchEvent(Component.java:4616)
java.awt.EventQueue.dispatchEvent(EventQueue.java:653)
java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:255)
java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:170)
java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:160)
java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
java.awt.EventDispatchThread.run(EventDispatchThread.java:136)

Stack of DestroyJavaVM:

(empty)




As i said before it can work around this and i believe now i know what is
the part of the task that is causing this (a opening a url connection on a
daemon thead in the TPE at this time), but i would like to find the cause so
the culprit can be corrected upstream. However i think i reached the limit
of my debugging capabilities.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100101/ba5ccd72/attachment.html>

From davidcholmes at aapt.net.au  Fri Jan  1 18:37:40 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 2 Jan 2010 09:37:40 +1000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <7d7138c11001010817n3e41af0vc138ce8638197448@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEKJIEAA.davidcholmes@aapt.net.au>

Dimitris Andreou writes:
> What is defined to happen if a shutdown hook starts a new (non-daemon)
> thread?

This in itself should have no direct impact on the shutdown process. As long
as all shutdownhook threads terminate then the VM will terminate. If any
hook does not terminate then halt() will need to be called to terminate the
VM.

David Holmes

> My suspicion on what happens in Paolo's case is that the VM
> was about to exist because the last swing window was closed (using
> WindowConstants.DISPOSE_ON_CLOSE), thus the EDT died, but then a hook
> invoked SwingUtilities.invokeLater which summoned a new EDT (which I
> assume it would enter a new event loop, instead of terminating
> itself).
>
> Dimitris
>
> 2010/1/1 Paulo Levi <i30817 at gmail.com>:
> > I forgot to "reply to all" and had some responses that were
> only being sent
> > to Mr. Andreou. I will transcribe them.
> >
> > For the record i think this might be a combination of the state
> Mr. Berlin
> > says he as seen and the problem in the link in the last reply bellow. I
> > can't get a thread dump when this state happens but i can do it using a
> > Shutdown Hook (presumably before the jre shutdowns)
> >
> > This are the missing responses - i will now try to comment the
> invokeLater
> > part of my tasks to see if they terminate now:
> >
> >
> > ---------- Forwarded message ----------
> > From: Paulo Levi <i30817 at gmail.com>
> > Date: Thu, Dec 31, 2009 at 2:57 AM
> > Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> > To: Dimitris Andreou <jim.andreou at gmail.com>
> >
> >
> > Well, after adding the shutdown hook for the
> ThreadPoolExecutors subclass
> > and calling shutdownnow on them inside it, it always terminated (when it
> > almost always didn't in this dual core). Its possible i'm doing
> something
> > wrong in the tasks (there is a URL connection opened there and a file
> > written/read) that the interrupt call of the shutdownnow
> "fixes". That is
> > what i suspect.
> >
> > On Thu, Dec 31, 2009 at 2:45 AM, Dimitris Andreou
> <jim.andreou at gmail.com>
> > wrote:
> >>
> >> Hi Paulo,
> >>
> >> The application can't hang just because a daemon thread hangs. It's
> >> the other way around - daemon threads live because there is a
> >> non-daemon thread still active. Are you sure you terminate all
> >> non-daemon threads when the application is "closed"?
> >>
> >> Dimitris
> >>
> >> 2009/12/31 Paulo Levi <i30817 at gmail.com>:
> >> > I have a TPE subclass and i'm seeing (disturbingly only on some dual
> >> > core
> >> > processors) a hang when the application is closed and there
> are lots of
> >> > tasks to be processed. Specifically no-more tasks appear to be
> >> > processed,
> >> > but the (daemon) threads are still alive at the time
> shutdown hooks are
> >> > called.
> >> > (daemon, alive, not interrupted).
> >> >
> >> > Threadfactory used by the subclass creates daemon threads,
> and the class
> >> > is
> >> > configurated with a keepAliveTime > 0 in this case (to reuse the
> >> > threads)
> >> > and a LIFO queue instead of the normal one.
> >> >
> >> > I appear to be able to avoid it with a shutdown hook, but i
> would like
> >> > to
> >> > know if all ThreadPoolExecutors are vulnerable to this.
> >> >
> >> > (disturbingly too, after this, the debugger says the java process is
> >> > closed,
> >> > however windows task manager disagrees).
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> >
> >> >
> >
> >
> > ---------- Forwarded message ----------
> > From: Paulo Levi <i30817 at gmail.com>
> > Date: Thu, Dec 31, 2009 at 3:11 AM
> > Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> > To: Dimitris Andreou <jim.andreou at gmail.com>
> >
> >
> > Besides there is the very suspicious fact that i couldn't get a
> thread dump
> > when i closed the main window,(CTRL+Break right?) and the
> profiler/debugger
> > told me the application was finished but the console didn't
> terminate (same
> > thing in the command-line console). That is what blew my mind.
> >
> > The alive non-demon threads alive at the time the shudown hooks
> run are :
> > AWT-Shutdown
> > AWT-EventQueue-0
> > DestroyJavaVM
> >
> > (there are some !isAlive()).
> >
> >
> >
> > ---------- Forwarded message ----------
> > From: Paulo Levi <i30817 at gmail.com>
> > Date: Thu, Dec 31, 2009 at 3:17 AM
> > Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit
> > To: Dimitris Andreou <jim.andreou at gmail.com>
> >
> >
> > Now seeing that AWT-Shutdown, its possible that one of my tasks posts a
> > event to the EDT (as described here :
> http://www.pushing-pixels.org/?p=369
> > ).
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Fri Jan  1 18:44:56 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 2 Jan 2010 09:44:56 +1000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <212322091001011021x4b0d8430rf22f09b59b0a3f91@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEKJIEAA.davidcholmes@aapt.net.au>

I think things are getting a little confused. If you have called System.exit
to initiate shutdown then you should expect non-daemon threads to be alive.
The AWT has non-daemon threads. When an orderly shutdown is initiated all
the registered shutdown hooks are executed and once they all terminate then
the shutdown will proceed.

This seems curious:

Stack of AWT-EventQueue-0:

java.lang.Thread.getStackTrace(Thread.java:1578)

java.lang.Shutdown.halt(Shutdown.java:148)

why would halt() be trying to get a stacktrace ?

David Holmes

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Paulo Levi
  Sent: Saturday, 2 January 2010 4:22 AM
  To: Dimitris Andreou
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit


  It doesn't appear to be it :(

  I used Xbootclasspath/p: to use a new EventQueue class that made a
system.out.println with a counter on the
  private void postEvent(AWTEvent theEvent, int priority) that is used by
all event posting and it doesnt increment after closing.

  It appears to be something else that is keeping the AWT thread alive.
  Using a shutdown hook to dump the stack of the anomalous (?) alive
AWT-EventQueue-0
  gives this
  ....
  EVENT 496
  EVENT 497
  java.lang.Object.wait(Native Method)
  EVENT 498
  java.lang.Thread.join(Thread.java:1269)
  java.lang.Thread.join(Thread.java:1343)

java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:10
6)
  java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
  java.lang.Shutdown.runHooks(Shutdown.java:123)
  java.lang.Shutdown.sequence(Shutdown.java:167)
  java.lang.Shutdown.exit(Shutdown.java:212)
  java.lang.Runtime.exit(Runtime.java:107)
  java.lang.System.exit(System.java:954)
  javax.swing.JFrame.processWindowEvent(JFrame.java:312)
  java.awt.Window.processEvent(Window.java:1961)
  java.awt.Component.dispatchEventImpl(Component.java:4790)
  java.awt.Container.dispatchEventImpl(Container.java:2261)
  java.awt.Window.dispatchEventImpl(Window.java:2671)
  java.awt.Component.dispatchEvent(Component.java:4616)
  java.awt.EventQueue.dispatchEvent(EventQueue.java:654)

java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java
:255)

java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:17
0)

java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java
:160)
  java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
  java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
  java.awt.EventDispatchThread.run(EventDispatchThread.java:136)


  So it appears to be normal that AWT runs the shutdownhooks and waits for
them to die before exiting. Commenting the main shutdown hook of my
application doesn't have a effect.

  Then i also replaced the Shutdown auxilialy class to see if the
application progressed beyond the java.lang.Shutdown.sequence call (disabled
the event printing for legibility). I put a dump of all non-daemon alive
threads before the native halt0 method. This is the result in a hanging
case.

  current thread = AWT-EventQueue-0
  Stack of AWT-Shutdown:
  java.lang.Object.wait(Native Method)
  java.lang.Object.wait(Object.java:502)
  sun.awt.AWTAutoShutdown.run(AWTAutoShutdown.java:287)
  java.lang.Thread.run(Thread.java:717)

  Stack of AWT-EventQueue-0:
  java.lang.Thread.getStackTrace(Thread.java:1578)
  java.lang.Shutdown.halt(Shutdown.java:148)
  java.lang.Shutdown.exit(Shutdown.java:238)
  java.lang.Runtime.exit(Runtime.java:107)
  java.lang.System.exit(System.java:954)
  javax.swing.JFrame.processWindowEvent(JFrame.java:312)
  java.awt.Window.processEvent(Window.java:1961)
  java.awt.Component.dispatchEventImpl(Component.java:4790)
  java.awt.Container.dispatchEventImpl(Container.java:2261)
  java.awt.Window.dispatchEventImpl(Window.java:2671)
  java.awt.Component.dispatchEvent(Component.java:4616)
  java.awt.EventQueue.dispatchEvent(EventQueue.java:653)

java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java
:255)

java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:17
0)

java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java
:160)
  java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
  java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
  java.awt.EventDispatchThread.run(EventDispatchThread.java:136)

  Stack of DestroyJavaVM:

  (empty)




  As i said before it can work around this and i believe now i know what is
the part of the task that is causing this (a opening a url connection on a
daemon thead in the TPE at this time), but i would like to find the cause so
the culprit can be corrected upstream. However i think i reached the limit
of my debugging capabilities.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100102/70e61675/attachment-0001.html>

From davidcholmes at aapt.net.au  Fri Jan  1 19:54:43 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 2 Jan 2010 10:54:43 +1000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEKJIEAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKKIEAA.davidcholmes@aapt.net.au>

I missed the fact that you added the thread dump before the halt.

Based on that I don't see anything anomalous.

Based on your original post though I'm still extremely confused as to what
the actual problem is. Seems via the GUI System.exit is called but your app
seems to hang rather than terminate. Not sure what that has to do with the
TPE and whether it continues to process tasks or not.

David

PS. Happy New Year all!
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David Holmes
  Sent: Saturday, 2 January 2010 9:45 AM
  To: Paulo Levi; Dimitris Andreou
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit


  I think things are getting a little confused. If you have called
System.exit to initiate shutdown then you should expect non-daemon threads
to be alive. The AWT has non-daemon threads. When an orderly shutdown is
initiated all the registered shutdown hooks are executed and once they all
terminate then the shutdown will proceed.

  This seems curious:

  Stack of AWT-EventQueue-0:

  java.lang.Thread.getStackTrace(Thread.java:1578)

  java.lang.Shutdown.halt(Shutdown.java:148)

  why would halt() be trying to get a stacktrace ?

  David Holmes

    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Paulo Levi
    Sent: Saturday, 2 January 2010 4:22 AM
    To: Dimitris Andreou
    Cc: concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] ThreadPoolExecutors and System.exit


    It doesn't appear to be it :(

    I used Xbootclasspath/p: to use a new EventQueue class that made a
system.out.println with a counter on the
    private void postEvent(AWTEvent theEvent, int priority) that is used by
all event posting and it doesnt increment after closing.

    It appears to be something else that is keeping the AWT thread alive.
    Using a shutdown hook to dump the stack of the anomalous (?) alive
AWT-EventQueue-0
    gives this
    ....
    EVENT 496
    EVENT 497
    java.lang.Object.wait(Native Method)
    EVENT 498
    java.lang.Thread.join(Thread.java:1269)
    java.lang.Thread.join(Thread.java:1343)

java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:10
6)

java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
    java.lang.Shutdown.runHooks(Shutdown.java:123)
    java.lang.Shutdown.sequence(Shutdown.java:167)
    java.lang.Shutdown.exit(Shutdown.java:212)
    java.lang.Runtime.exit(Runtime.java:107)
    java.lang.System.exit(System.java:954)
    javax.swing.JFrame.processWindowEvent(JFrame.java:312)
    java.awt.Window.processEvent(Window.java:1961)
    java.awt.Component.dispatchEventImpl(Component.java:4790)
    java.awt.Container.dispatchEventImpl(Container.java:2261)
    java.awt.Window.dispatchEventImpl(Window.java:2671)
    java.awt.Component.dispatchEvent(Component.java:4616)
    java.awt.EventQueue.dispatchEvent(EventQueue.java:654)

java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java
:255)

java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:17
0)

java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java
:160)
    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
    java.awt.EventDispatchThread.run(EventDispatchThread.java:136)


    So it appears to be normal that AWT runs the shutdownhooks and waits for
them to die before exiting. Commenting the main shutdown hook of my
application doesn't have a effect.

    Then i also replaced the Shutdown auxilialy class to see if the
application progressed beyond the java.lang.Shutdown.sequence call (disabled
the event printing for legibility). I put a dump of all non-daemon alive
threads before the native halt0 method. This is the result in a hanging
case.

    current thread = AWT-EventQueue-0
    Stack of AWT-Shutdown:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:502)
    sun.awt.AWTAutoShutdown.run(AWTAutoShutdown.java:287)
    java.lang.Thread.run(Thread.java:717)

    Stack of AWT-EventQueue-0:
    java.lang.Thread.getStackTrace(Thread.java:1578)
    java.lang.Shutdown.halt(Shutdown.java:148)
    java.lang.Shutdown.exit(Shutdown.java:238)
    java.lang.Runtime.exit(Runtime.java:107)
    java.lang.System.exit(System.java:954)
    javax.swing.JFrame.processWindowEvent(JFrame.java:312)
    java.awt.Window.processEvent(Window.java:1961)
    java.awt.Component.dispatchEventImpl(Component.java:4790)
    java.awt.Container.dispatchEventImpl(Container.java:2261)
    java.awt.Window.dispatchEventImpl(Window.java:2671)
    java.awt.Component.dispatchEvent(Component.java:4616)
    java.awt.EventQueue.dispatchEvent(EventQueue.java:653)

java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java
:255)

java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:17
0)

java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java
:160)
    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:155)
    java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:147)
    java.awt.EventDispatchThread.run(EventDispatchThread.java:136)

    Stack of DestroyJavaVM:

    (empty)




    As i said before it can work around this and i believe now i know what
is the part of the task that is causing this (a opening a url connection on
a daemon thead in the TPE at this time), but i would like to find the cause
so the culprit can be corrected upstream. However i think i reached the
limit of my debugging capabilities.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100102/37d2a71c/attachment.html>

From i30817 at gmail.com  Fri Jan  1 20:08:21 2010
From: i30817 at gmail.com (Paulo Levi)
Date: Sat, 2 Jan 2010 01:08:21 +0000
Subject: [concurrency-interest] ThreadPoolExecutors and System.exit
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEKKIEAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEKJIEAA.davidcholmes@aapt.net.au> 
	<NFBBKALFDCPFIDBNKAPCAEKKIEAA.davidcholmes@aapt.net.au>
Message-ID: <212322091001011708r406cc6ebp61a25b03bf1bc3f5@mail.gmail.com>

It is possible (likely) that the bug is in the networking stack.

I've made a little (not automatted) test. The test case is not absolutly
reliable in that the hang doesn't occur always.
The JFrame close setting being EXIT_ON_CLOSE or DISPOSE_ON_CLOSE doesn't
matter, but obviously i expected calling exit would stop execution.

http://pastebin.com/m67911719

I doesn't continue to process the tasks, (no print) but doesn't terminate
either.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100102/29a7a5d0/attachment.html>

From jacob at hookom.net  Mon Jan  4 23:11:26 2010
From: jacob at hookom.net (Jacob Hookom)
Date: Mon, 04 Jan 2010 22:11:26 -0600
Subject: [concurrency-interest] Periodic/MutableFuture
Message-ID: <4B42BBEE.10109@hookom.net>

In using Java's ScheduledExecutorService, I was looking for a way to 
match a common usecase in applications whereby I have some variable 
which needs periodic/async updating. The common case is scheduling 
periodic updates from the database into memory with non-blocking reads.

It'd be nice if we had versions which took in Callable<V> to 
scheduleAt/With...(...) and returned a ScheduledFuture<V> by which 
successive calls to get() returned the current of the last execution of 
the Callable<V> given some interval. 

Example:

public <V> ScheduledFuture<V> scheduleAtFixedRate(Callable<V> callable, 
long initialDelay, long period, TimeUnit unit);

(rough code of desired behavior on existing API):

public class PeriodicFuture<V> extends FutureTask<V> {

	private volatile V value;
	private final Callable<V> callable;
	
	public PeriodicFuture(Callable<V> callable) {
		super(callable);
		this.callable = callable;
	}

	public V get() throws InterruptedException, ExecutionException {
		if (!this.isDone()) {
			this.value = super.get();
		}
		return this.value;
	}

	public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {
		if (!this.isDone()) {
			this.value = super.get(timeout, unit);
		}
		return this.value;
	}

	public void run() {
		if (!this.isDone()) {
			super.run();
		}
		try {
			this.value = this.callable.call();
		} catch (Exception e) {
			this.setException(e);
		}
	}
}





From tim at peierls.net  Tue Jan  5 09:48:48 2010
From: tim at peierls.net (Tim Peierls)
Date: Tue, 5 Jan 2010 09:48:48 -0500
Subject: [concurrency-interest] Periodic/MutableFuture
In-Reply-To: <4B42BBEE.10109@hookom.net>
References: <4B42BBEE.10109@hookom.net>
Message-ID: <63b4e4051001050648h69ced318x93bb4e60b7e2924a@mail.gmail.com>

Why not just have your periodic task (a Runnable) write its most recent
value to some known location? For example (not dealing with exceptions or
cancellation):

public abstract class Periodic<T> implements Runnable {

    public final T get() {
        return value;
    }

    public final Periodic<T> updateAtFixedRate(ScheduledExecutorService
exec, long period, TimeUnit unit) {
        value = initialValue();
        exec.scheduleAtFixedRate(this, period, period, unit);
        return this; // for chaining calls
    }

    public final void run() {
        value = updatedValue();
    }

    protected T initialValue() {
        return null;
    }

    protected abstract T updatedValue();

    private volatile T value;
}

// Sample use:

    Periodic<Integer> hitCount = new Periodic<Integer>() {
        protected Integer initialValue() { return 0; }
        protected Integer updatedValue() { return
obtainLatestHitCountFromDataStore(); }
    }.updateAtFixedRate(exec, 30, TimeUnit.SECONDS);

    ...

    int hc = hitCount.get(); // retrieves hit count, could be up to about 30
seconds stale


--tim


On Mon, Jan 4, 2010 at 11:11 PM, Jacob Hookom <jacob at hookom.net> wrote:

> In using Java's ScheduledExecutorService, I was looking for a way to match
> a common usecase in applications whereby I have some variable which needs
> periodic/async updating. The common case is scheduling periodic updates from
> the database into memory with non-blocking reads.
>
> It'd be nice if we had versions which took in Callable<V> to
> scheduleAt/With...(...) and returned a ScheduledFuture<V> by which
> successive calls to get() returned the current of the last execution of the
> Callable<V> given some interval.
> Example:
>
> public <V> ScheduledFuture<V> scheduleAtFixedRate(Callable<V> callable,
> long initialDelay, long period, TimeUnit unit);
>
> (rough code of desired behavior on existing API):
>
> public class PeriodicFuture<V> extends FutureTask<V> {
>
>        private volatile V value;
>        private final Callable<V> callable;
>
>        public PeriodicFuture(Callable<V> callable) {
>                super(callable);
>                this.callable = callable;
>        }
>
>        public V get() throws InterruptedException, ExecutionException {
>                if (!this.isDone()) {
>                        this.value = super.get();
>                }
>                return this.value;
>        }
>
>        public V get(long timeout, TimeUnit unit) throws
> InterruptedException, ExecutionException, TimeoutException {
>                if (!this.isDone()) {
>                        this.value = super.get(timeout, unit);
>                }
>                return this.value;
>        }
>
>        public void run() {
>                if (!this.isDone()) {
>                        super.run();
>                }
>                try {
>                        this.value = this.callable.call();
>                } catch (Exception e) {
>                        this.setException(e);
>                }
>        }
> }
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100105/6e2939b9/attachment.html>

From dl at cs.oswego.edu  Wed Jan  6 19:18:50 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 06 Jan 2010 19:18:50 -0500
Subject: [concurrency-interest] JDK7 schedule and extra166y
In-Reply-To: <4B3AD5A1.4090909@orcaware.com>
References: <4B3AD5A1.4090909@orcaware.com>
Message-ID: <4B45286A.9070504@cs.oswego.edu>

Blair Zajac wrote:
> With the JDK7 pushing out to late next year and getting closures, what 
> are the plans with extra166y?  Will it make it into JDK7?  Will JDK7 see 
> some version of CustomConcurrentHashMap?
> 
> How much will closures effect the collections already in JDK7?
> 

Well, rather than leaving this un-replied-to for so long,
I'll just leave it unanswered :-)

We have no idea where plans for lambda/closure language
enhancements are going, or if they will be done with
enough of a window to overhaul ParallelArray etc to
ship them.

For CustomConcurrentHashMap, we are still leaving it
to the Google Collections folks to experiment with
different caching APIs and algorithms. We'll probably
revisit possible standardization into JDK before
Java7 though.

In the mean-time, expect some improvements soon
in the stuff we ARE shipping (ForkJoin, TransferQueues).
I'll post some notes on these as they get firmer.

-Doug





From vijay at saraswat.org  Wed Jan  6 19:34:28 2010
From: vijay at saraswat.org (Vijay Saraswat)
Date: Wed, 06 Jan 2010 19:34:28 -0500
Subject: [concurrency-interest] JDK7 schedule and extra166y
In-Reply-To: <4B45286A.9070504@cs.oswego.edu>
References: <4B3AD5A1.4090909@orcaware.com> <4B45286A.9070504@cs.oswego.edu>
Message-ID: <4B452C14.7060006@saraswat.org>

The unanswer was useful! Thanks!

Doug Lea wrote:
> Blair Zajac wrote:
>> With the JDK7 pushing out to late next year and getting closures, 
>> what are the plans with extra166y?  Will it make it into JDK7?  Will 
>> JDK7 see some version of CustomConcurrentHashMap?
>>
>> How much will closures effect the collections already in JDK7?
>>
>
> Well, rather than leaving this un-replied-to for so long,
> I'll just leave it unanswered :-)
>
> We have no idea where plans for lambda/closure language
> enhancements are going, or if they will be done with
> enough of a window to overhaul ParallelArray etc to
> ship them.
>
> For CustomConcurrentHashMap, we are still leaving it
> to the Google Collections folks to experiment with
> different caching APIs and algorithms. We'll probably
> revisit possible standardization into JDK before
> Java7 though.
>
> In the mean-time, expect some improvements soon
> in the stuff we ARE shipping (ForkJoin, TransferQueues).
> I'll post some notes on these as they get firmer.
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>



From joe.bowbeer at gmail.com  Wed Jan  6 20:07:05 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 6 Jan 2010 17:07:05 -0800
Subject: [concurrency-interest] JDK7 schedule and extra166y
In-Reply-To: <4B45286A.9070504@cs.oswego.edu>
References: <4B3AD5A1.4090909@orcaware.com> <4B45286A.9070504@cs.oswego.edu>
Message-ID: <31f2a7bd1001061707t288d9a3bg94c1de6a4c0f3307@mail.gmail.com>

On Wed, Jan 6, 2010 at 4:18 PM, Doug Lea wrote:

>
> We'll probably revisit possible standardization into JDK before Java7
> though.
>
>
Will that be after or before you accept the Dahl-Nygaard Senior Prize for
2010?

http://www.aito.org/Dahl-Nygaard/2010.html

Just wondering...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100106/2a764f15/attachment.html>

From vijay at saraswat.org  Wed Jan  6 20:25:30 2010
From: vijay at saraswat.org (Vijay Saraswat)
Date: Wed, 06 Jan 2010 20:25:30 -0500
Subject: [concurrency-interest] JDK7 schedule and extra166y
In-Reply-To: <31f2a7bd1001061707t288d9a3bg94c1de6a4c0f3307@mail.gmail.com>
References: <4B3AD5A1.4090909@orcaware.com> <4B45286A.9070504@cs.oswego.edu>
	<31f2a7bd1001061707t288d9a3bg94c1de6a4c0f3307@mail.gmail.com>
Message-ID: <4B45380A.7050004@saraswat.org>

He probably forgot about that -- a senior moment :-)

Joe Bowbeer wrote:
> On Wed, Jan 6, 2010 at 4:18 PM, Doug Lea wrote:
>
>
>     We'll probably revisit possible standardization into JDK before
>     Java7 though.
>
>
> Will that be after or before you accept the Dahl-Nygaard Senior Prize 
> for 2010?
>
> http://www.aito.org/Dahl-Nygaard/2010.html
>
> Just wondering...

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100106/2ed9a083/attachment.html>

From dawid.weiss at gmail.com  Thu Jan  7 04:53:32 2010
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Thu, 7 Jan 2010 10:53:32 +0100
Subject: [concurrency-interest] HotSpot inlining in hot loops.
Message-ID: <baa8c5731001070153p4583e22dpdcf80e7954e9b2ca@mail.gmail.com>

Hello there,

I have been wondering about one thing... knowing the people reading
this list this I believe it's the right place to ask.

I have been experimenting with various code patterns and
micro-optimisations (and I do realize it's a very subtle topic, but
let me continue). I observed the following interesting behavior from
SUN's HotSpot implementation. Consider the following two loops:

1.
        for (int i = 0; i < list.size(); i++)
        {
            value = list.get(i);
        }

2.
        final int size = list.size();
        final int [] buffer = list.buffer;
        for (int i = 0; i < size; i++)
        {
            value = buffer[i];
        }

The list class is custom and the implementation of size() and get() is trivial:

get == return buffer[index];
size == return elementsCount;

(no boundary checks, no nothing).

Now, the micro-benchmarks (server JVM, with proper warmup, many
benchmark repetitions, no parallel activity from the GC or whatsover)
are
consistently showing that loop (2) is at least TWICE as fast as loop
(1) (on various CPU architectures, multi and single-core systems).
"value"
variable is a public static volatile to force the compiler to actually
read the buffer's content and store is somewhere. Example timings:

testSimpleGetLoop          : time.bench: 1.97, round: 0.20 [+- 0.00],
round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
testDirectBufferLoop       : time.bench: 0.57, round: 0.06 [+- 0.01],
round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00

(note the "round" field above, in brackets are standard deviations
from multiple test runs).

I looked at the assembly dumped by the HotSpot and from it I can
conclude that in case of loop (1) the generated code always attempts
to re-read the fields referenced
through the list field (it is object-scoped, private, final,
non-volatile, direct class access -- no interfaces). See listing (A)
below. My understanding of the JMM was that
in cases such as this one, the compiler can safely assume no side
effects for the current thread and move field references to registers.
This is exactly what happens
in case (2) (see listing (B) below) -- the fields are moved to
regiters and additional loop unrolling is performed by the compiler. I
am definitely missing something here -- is this my
incorrect understanding of the spec or simply the Java compiler does
not do aggresive optimisations?

I also checked with other JVMs. IBM's Java does not show such a major
difference:

testSimpleGetLoop            time.bench: 1.96, round: 0.20 [+- 0.01],
round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
testDirectBufferLoop        time.bench: 1.70, round: 0.17 [+- 0.00],
round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00

JRocki also doesn't:

testSimpleGetLoop             time.bench: 1.02, round: 0.10 [+- 0.01],
round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
testDirectBufferLoop             time.bench: 1.01, round: 0.10 [+-
0.01], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00

I have experimented with other scenarios (closure loops, iterables,
etc.) and the results are often even more intriguing, but I don't want
to start too many threads at once. I will appreciate your comments,
even if they simply redirect me to right fragment of the JVM
specification.

Dawid


(A) simple get loop, pseudo-assembly dump (after warmup).

030   B4: #	B11 B5 &lt;- B3 B8 	Loop: B4-B8 inner  Freq: 220753
030   	MOV    EDX,[EBP + #12] ! Field .buffer
033   	NullCheck EBP
033
033   B5: #	B12 B6 &lt;- B4  Freq: 220753
033   	MOV    EBP,[EDX + #8]
036   	NullCheck EDX
036
036   B6: #	B10 B7 &lt;- B5  Freq: 220752
036   	CMPu   EDI,EBP
038   	Jnb,us B10  P=0.000001 C=-1.000000
038
03a   B7: #	B13 B8 &lt;- B6  Freq: 220752
03a   	MOVSX8 EAX,[EDX + #12 + EDI]	# byte
03f   	MEMBAR-release ! (empty encoding)
03f   	MOV8   [ECX + precise klass Benchmark:
0x048e80e8:Constant:exact *],EAX ! Field  Volatile value
045   	MEMBAR-volatile (unnecessary so empty encoding)
045   	LOCK ADDL [ESP + #0], 0	! membar_volatile
04a   	MOV    EBP,[EBX + #12] ! Field .list
04d   	MOV    EDX,[EBP + #8]	# int ! Field .elementsCount
050   	NullCheck EBP
050
050   B8: #	B4 B9 &lt;- B7  Freq: 220752
050   	INC    EDI
051   	TSTL   #polladdr,EAX	! Safepoint: poll for GC
057   	CMP    EDI,EDX
059   	Jl,s  B4  P=1.000000 C=179200.000000


(B) direct buffer access loop (after warmup).

01a   	MOV    ECX,[ECX + #12] ! Field .list
01d   	MOV    EAX,[ECX + #8]	# int ! Field .elementsCount
020   	NullCheck ECX
020
020   B2: #	B12 B3 &lt;- B1  Freq: 0.999999
020   	TEST   EAX,EAX
022   	Jle    B12  P=0.000000 C=1.000000
022
028   B3: #	B4 &lt;- B2  Freq: 0.999999
028   	MOV    EDI,[ECX + #12] ! Field .buffer
02b   	XOR    EBX,EBX
02d   	MOV    EBP,#360
02d
032   B4: #	B14 B5 &lt;- B3 B6 	Loop: B4-B6 inner stride: not constant
pre of N158 Freq: 1.99999
032   	MOV    EDX,[EDI + #8]
035   	NullCheck EDI
035
035   B5: #	B13 B6 &lt;- B4  Freq: 1.99999
035   	CMPu   EBX,EDX
037   	Jnb,u  B13  P=0.000001 C=-1.000000
037
03d   B6: #	B4 B7 &lt;- B5  Freq: 1.99999
03d   	MOVSX8 ECX,[EDI + #12 + EBX]	# byte
042   	MEMBAR-release ! (empty encoding)
042   	MOV8   [EBP + precise klass : 0x04161d30:Constant:exact *],ECX
! Field  Volatile.value
048   	MEMBAR-volatile (unnecessary so empty encoding)
048   	LOCK ADDL [ESP + #0], 0	! membar_volatile
04d   	INC    EBX
04e   	CMP    EBX,#1
051   	Jl,s  B4	# Loop end  P=0.500000 C=334848.000000

053   B7: #	B9 B8 &lt;- B6  Freq: 0.999994
053   	MOV    ESI,EAX
055   	MIN    ESI,EDX
05b   	SUB    ESI,EBX
05d   	AND    ESI,#-4
060   	ADD    ESI,EBX
062   	CMP    EBX,ESI
064   	Jge,s  B9  P=0.000001 C=-1.000000
      	NOP 	# 10 bytes pad for loops and calls

070   B8: #	B8 B9 &lt;- B7 B8 	Loop: B8-B8 inner stride: not constant
main of N116 Freq: 999993
070   	MOVSX8 ECX,[EDI + #12 + EBX]	# byte
075   	MEMBAR-release ! (empty encoding)
075   	MOV8   [EBP + precise klass : 0x04161d30:Constant:exact *],ECX
! Field  Volatile.value
07b   	MEMBAR-volatile (unnecessary so empty encoding)
07b   	MEMBAR-volatile (unnecessary so empty encoding)
07b   	MOVSX8 ECX,[EDI + #13 + EBX]	# byte
080   	MEMBAR-release ! (empty encoding)
080   	MOV8   [EBP + precise klass : 0x04161d30:Constant:exact *],ECX
! Field  Volatile.value
086   	MEMBAR-volatile (unnecessary so empty encoding)
086   	MEMBAR-volatile (unnecessary so empty encoding)

... (loop further unrolled here) ...

From davidcholmes at aapt.net.au  Thu Jan  7 05:42:16 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 7 Jan 2010 20:42:16 +1000
Subject: [concurrency-interest] HotSpot inlining in hot loops.
In-Reply-To: <baa8c5731001070153p4583e22dpdcf80e7954e9b2ca@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIELCIEAA.davidcholmes@aapt.net.au>

Dawid Weiss writes:
>
> I have been wondering about one thing... knowing the people reading
> this list this I believe it's the right place to ask.

Well there may be some people here who know the answer, but that doesn't
make this the right place to ask. ;-)

A better place to ask is the OpenJDK compiler mailing list:

http://mail.openjdk.java.net/mailman/listinfo/hotspot-compiler-dev

Cheers,
David Holmes

> I have been experimenting with various code patterns and
> micro-optimisations (and I do realize it's a very subtle topic, but
> let me continue). I observed the following interesting behavior from
> SUN's HotSpot implementation. Consider the following two loops:
>
> 1.
>         for (int i = 0; i < list.size(); i++)
>         {
>             value = list.get(i);
>         }
>
> 2.
>         final int size = list.size();
>         final int [] buffer = list.buffer;
>         for (int i = 0; i < size; i++)
>         {
>             value = buffer[i];
>         }
>
> The list class is custom and the implementation of size() and
> get() is trivial:
>
> get == return buffer[index];
> size == return elementsCount;
>
> (no boundary checks, no nothing).
>
> Now, the micro-benchmarks (server JVM, with proper warmup, many
> benchmark repetitions, no parallel activity from the GC or whatsover)
> are
> consistently showing that loop (2) is at least TWICE as fast as loop
> (1) (on various CPU architectures, multi and single-core systems).
> "value"
> variable is a public static volatile to force the compiler to actually
> read the buffer's content and store is somewhere. Example timings:
>
> testSimpleGetLoop          : time.bench: 1.97, round: 0.20 [+- 0.00],
> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
> testDirectBufferLoop       : time.bench: 0.57, round: 0.06 [+- 0.01],
> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>
> (note the "round" field above, in brackets are standard deviations
> from multiple test runs).
>
> I looked at the assembly dumped by the HotSpot and from it I can
> conclude that in case of loop (1) the generated code always attempts
> to re-read the fields referenced
> through the list field (it is object-scoped, private, final,
> non-volatile, direct class access -- no interfaces). See listing (A)
> below. My understanding of the JMM was that
> in cases such as this one, the compiler can safely assume no side
> effects for the current thread and move field references to registers.
> This is exactly what happens
> in case (2) (see listing (B) below) -- the fields are moved to
> regiters and additional loop unrolling is performed by the compiler. I
> am definitely missing something here -- is this my
> incorrect understanding of the spec or simply the Java compiler does
> not do aggresive optimisations?
>
> I also checked with other JVMs. IBM's Java does not show such a major
> difference:
>
> testSimpleGetLoop            time.bench: 1.96, round: 0.20 [+- 0.01],
> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
> testDirectBufferLoop        time.bench: 1.70, round: 0.17 [+- 0.00],
> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>
> JRocki also doesn't:
>
> testSimpleGetLoop             time.bench: 1.02, round: 0.10 [+- 0.01],
> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
> testDirectBufferLoop             time.bench: 1.01, round: 0.10 [+-
> 0.01], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>
> I have experimented with other scenarios (closure loops, iterables,
> etc.) and the results are often even more intriguing, but I don't want
> to start too many threads at once. I will appreciate your comments,
> even if they simply redirect me to right fragment of the JVM
> specification.
>
> Dawid
>
>
> (A) simple get loop, pseudo-assembly dump (after warmup).
>
> 030   B4: #	B11 B5 &lt;- B3 B8 	Loop: B4-B8 inner  Freq: 220753
> 030   	MOV    EDX,[EBP + #12] ! Field .buffer
> 033   	NullCheck EBP
> 033
> 033   B5: #	B12 B6 &lt;- B4  Freq: 220753
> 033   	MOV    EBP,[EDX + #8]
> 036   	NullCheck EDX
> 036
> 036   B6: #	B10 B7 &lt;- B5  Freq: 220752
> 036   	CMPu   EDI,EBP
> 038   	Jnb,us B10  P=0.000001 C=-1.000000
> 038
> 03a   B7: #	B13 B8 &lt;- B6  Freq: 220752
> 03a   	MOVSX8 EAX,[EDX + #12 + EDI]	# byte
> 03f   	MEMBAR-release ! (empty encoding)
> 03f   	MOV8   [ECX + precise klass Benchmark:
> 0x048e80e8:Constant:exact *],EAX ! Field  Volatile value
> 045   	MEMBAR-volatile (unnecessary so empty encoding)
> 045   	LOCK ADDL [ESP + #0], 0	! membar_volatile
> 04a   	MOV    EBP,[EBX + #12] ! Field .list
> 04d   	MOV    EDX,[EBP + #8]	# int ! Field .elementsCount
> 050   	NullCheck EBP
> 050
> 050   B8: #	B4 B9 &lt;- B7  Freq: 220752
> 050   	INC    EDI
> 051   	TSTL   #polladdr,EAX	! Safepoint: poll for GC
> 057   	CMP    EDI,EDX
> 059   	Jl,s  B4  P=1.000000 C=179200.000000
>
>
> (B) direct buffer access loop (after warmup).
>
> 01a   	MOV    ECX,[ECX + #12] ! Field .list
> 01d   	MOV    EAX,[ECX + #8]	# int ! Field .elementsCount
> 020   	NullCheck ECX
> 020
> 020   B2: #	B12 B3 &lt;- B1  Freq: 0.999999
> 020   	TEST   EAX,EAX
> 022   	Jle    B12  P=0.000000 C=1.000000
> 022
> 028   B3: #	B4 &lt;- B2  Freq: 0.999999
> 028   	MOV    EDI,[ECX + #12] ! Field .buffer
> 02b   	XOR    EBX,EBX
> 02d   	MOV    EBP,#360
> 02d
> 032   B4: #	B14 B5 &lt;- B3 B6 	Loop: B4-B6 inner stride:
> not constant
> pre of N158 Freq: 1.99999
> 032   	MOV    EDX,[EDI + #8]
> 035   	NullCheck EDI
> 035
> 035   B5: #	B13 B6 &lt;- B4  Freq: 1.99999
> 035   	CMPu   EBX,EDX
> 037   	Jnb,u  B13  P=0.000001 C=-1.000000
> 037
> 03d   B6: #	B4 B7 &lt;- B5  Freq: 1.99999
> 03d   	MOVSX8 ECX,[EDI + #12 + EBX]	# byte
> 042   	MEMBAR-release ! (empty encoding)
> 042   	MOV8   [EBP + precise klass :
> 0x04161d30:Constant:exact *],ECX
> ! Field  Volatile.value
> 048   	MEMBAR-volatile (unnecessary so empty encoding)
> 048   	LOCK ADDL [ESP + #0], 0	! membar_volatile
> 04d   	INC    EBX
> 04e   	CMP    EBX,#1
> 051   	Jl,s  B4	# Loop end  P=0.500000 C=334848.000000
>
> 053   B7: #	B9 B8 &lt;- B6  Freq: 0.999994
> 053   	MOV    ESI,EAX
> 055   	MIN    ESI,EDX
> 05b   	SUB    ESI,EBX
> 05d   	AND    ESI,#-4
> 060   	ADD    ESI,EBX
> 062   	CMP    EBX,ESI
> 064   	Jge,s  B9  P=0.000001 C=-1.000000
>       	NOP 	# 10 bytes pad for loops and calls
>
> 070   B8: #	B8 B9 &lt;- B7 B8 	Loop: B8-B8 inner stride:
> not constant
> main of N116 Freq: 999993
> 070   	MOVSX8 ECX,[EDI + #12 + EBX]	# byte
> 075   	MEMBAR-release ! (empty encoding)
> 075   	MOV8   [EBP + precise klass :
> 0x04161d30:Constant:exact *],ECX
> ! Field  Volatile.value
> 07b   	MEMBAR-volatile (unnecessary so empty encoding)
> 07b   	MEMBAR-volatile (unnecessary so empty encoding)
> 07b   	MOVSX8 ECX,[EDI + #13 + EBX]	# byte
> 080   	MEMBAR-release ! (empty encoding)
> 080   	MOV8   [EBP + precise klass :
> 0x04161d30:Constant:exact *],ECX
> ! Field  Volatile.value
> 086   	MEMBAR-volatile (unnecessary so empty encoding)
> 086   	MEMBAR-volatile (unnecessary so empty encoding)
>
> ... (loop further unrolled here) ...
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nate at nanocow.com  Fri Jan  8 16:54:25 2010
From: nate at nanocow.com (Nate Nystrom)
Date: Fri, 8 Jan 2010 15:54:25 -0600
Subject: [concurrency-interest] Getting the current FJ task
Message-ID: <a72549cb1001081354x3135e1bdv8d4fe6409ca5a101@mail.gmail.com>

Hi all,

Is there an easy way to get a reference to the currently running
ForkJoinTask?  Something analogous to Thread.currentThread.  I poked
around the javadoc but couldn't find such a method.

Thanks,
Nate

From dl at cs.oswego.edu  Fri Jan  8 19:12:21 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 08 Jan 2010 19:12:21 -0500
Subject: [concurrency-interest] Getting the current FJ task
In-Reply-To: <a72549cb1001081354x3135e1bdv8d4fe6409ca5a101@mail.gmail.com>
References: <a72549cb1001081354x3135e1bdv8d4fe6409ca5a101@mail.gmail.com>
Message-ID: <4B47C9E5.9020408@cs.oswego.edu>

Nate Nystrom wrote:
> Hi all,
> 
> Is there an easy way to get a reference to the currently running
> ForkJoinTask?  Something analogous to Thread.currentThread.  I poked
> around the javadoc but couldn't find such a method.
> 

No. You can get the current ForkJoinWorkerThread just
by a cast: (ForkJoinWorkerThread) Thread.currentThread()
But for ForkJoinTasks,  you should normally write
computation code as methods of ForkJoinTask subclasses, so the
current task is just "this". In cases where you don't
want to do this, you'd have to propagate the current task
manually (or even place it in a ThreadLocal). In principle it
would be possible for the framework to help automate this
at some (relatively small) expense, but I don't know of a
compelling  argument for doing it.

-Doug



From dawid.weiss at gmail.com  Tue Jan 12 14:47:58 2010
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Tue, 12 Jan 2010 20:47:58 +0100
Subject: [concurrency-interest] HotSpot inlining in hot loops.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIELCIEAA.davidcholmes@aapt.net.au>
References: <baa8c5731001070153p4583e22dpdcf80e7954e9b2ca@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCIELCIEAA.davidcholmes@aapt.net.au>
Message-ID: <baa8c5731001121147g3467f4e1w60a76f879b09cfc3@mail.gmail.com>

To complete the thread and for archival reasons -- I did cross-post
the question to hotspot-compiler. It seems like the optimisation is
reasonable and possible under Java memory model, but hotspot currently
does not perform it.

Dawid

On Thu, Jan 7, 2010 at 11:42 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Dawid Weiss writes:
>>
>> I have been wondering about one thing... knowing the people reading
>> this list this I believe it's the right place to ask.
>
> Well there may be some people here who know the answer, but that doesn't
> make this the right place to ask. ;-)
>
> A better place to ask is the OpenJDK compiler mailing list:
>
> http://mail.openjdk.java.net/mailman/listinfo/hotspot-compiler-dev
>
> Cheers,
> David Holmes
>
>> I have been experimenting with various code patterns and
>> micro-optimisations (and I do realize it's a very subtle topic, but
>> let me continue). I observed the following interesting behavior from
>> SUN's HotSpot implementation. Consider the following two loops:
>>
>> 1.
>> ? ? ? ? for (int i = 0; i < list.size(); i++)
>> ? ? ? ? {
>> ? ? ? ? ? ? value = list.get(i);
>> ? ? ? ? }
>>
>> 2.
>> ? ? ? ? final int size = list.size();
>> ? ? ? ? final int [] buffer = list.buffer;
>> ? ? ? ? for (int i = 0; i < size; i++)
>> ? ? ? ? {
>> ? ? ? ? ? ? value = buffer[i];
>> ? ? ? ? }
>>
>> The list class is custom and the implementation of size() and
>> get() is trivial:
>>
>> get == return buffer[index];
>> size == return elementsCount;
>>
>> (no boundary checks, no nothing).
>>
>> Now, the micro-benchmarks (server JVM, with proper warmup, many
>> benchmark repetitions, no parallel activity from the GC or whatsover)
>> are
>> consistently showing that loop (2) is at least TWICE as fast as loop
>> (1) (on various CPU architectures, multi and single-core systems).
>> "value"
>> variable is a public static volatile to force the compiler to actually
>> read the buffer's content and store is somewhere. Example timings:
>>
>> testSimpleGetLoop ? ? ? ? ?: time.bench: 1.97, round: 0.20 [+- 0.00],
>> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>> testDirectBufferLoop ? ? ? : time.bench: 0.57, round: 0.06 [+- 0.01],
>> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>>
>> (note the "round" field above, in brackets are standard deviations
>> from multiple test runs).
>>
>> I looked at the assembly dumped by the HotSpot and from it I can
>> conclude that in case of loop (1) the generated code always attempts
>> to re-read the fields referenced
>> through the list field (it is object-scoped, private, final,
>> non-volatile, direct class access -- no interfaces). See listing (A)
>> below. My understanding of the JMM was that
>> in cases such as this one, the compiler can safely assume no side
>> effects for the current thread and move field references to registers.
>> This is exactly what happens
>> in case (2) (see listing (B) below) -- the fields are moved to
>> regiters and additional loop unrolling is performed by the compiler. I
>> am definitely missing something here -- is this my
>> incorrect understanding of the spec or simply the Java compiler does
>> not do aggresive optimisations?
>>
>> I also checked with other JVMs. IBM's Java does not show such a major
>> difference:
>>
>> testSimpleGetLoop ? ? ? ? ? ?time.bench: 1.96, round: 0.20 [+- 0.01],
>> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>> testDirectBufferLoop ? ? ? ?time.bench: 1.70, round: 0.17 [+- 0.00],
>> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>>
>> JRocki also doesn't:
>>
>> testSimpleGetLoop ? ? ? ? ? ? time.bench: 1.02, round: 0.10 [+- 0.01],
>> round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>> testDirectBufferLoop ? ? ? ? ? ? time.bench: 1.01, round: 0.10 [+-
>> 0.01], round.gc: 0.00 [+- 0.00], GC.calls: 0, GC.time: 0.00
>>
>> I have experimented with other scenarios (closure loops, iterables,
>> etc.) and the results are often even more intriguing, but I don't want
>> to start too many threads at once. I will appreciate your comments,
>> even if they simply redirect me to right fragment of the JVM
>> specification.
>>
>> Dawid
>>
>>
>> (A) simple get loop, pseudo-assembly dump (after warmup).
>>
>> 030 ? B4: # ? B11 B5 &lt;- B3 B8 ? ? ?Loop: B4-B8 inner ?Freq: 220753
>> 030 ? ? ? ? ? MOV ? ?EDX,[EBP + #12] ! Field .buffer
>> 033 ? ? ? ? ? NullCheck EBP
>> 033
>> 033 ? B5: # ? B12 B6 &lt;- B4 ?Freq: 220753
>> 033 ? ? ? ? ? MOV ? ?EBP,[EDX + #8]
>> 036 ? ? ? ? ? NullCheck EDX
>> 036
>> 036 ? B6: # ? B10 B7 &lt;- B5 ?Freq: 220752
>> 036 ? ? ? ? ? CMPu ? EDI,EBP
>> 038 ? ? ? ? ? Jnb,us B10 ?P=0.000001 C=-1.000000
>> 038
>> 03a ? B7: # ? B13 B8 &lt;- B6 ?Freq: 220752
>> 03a ? ? ? ? ? MOVSX8 EAX,[EDX + #12 + EDI] ? ?# byte
>> 03f ? ? ? ? ? MEMBAR-release ! (empty encoding)
>> 03f ? ? ? ? ? MOV8 ? [ECX + precise klass Benchmark:
>> 0x048e80e8:Constant:exact *],EAX ! Field ?Volatile value
>> 045 ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>> 045 ? ? ? ? ? LOCK ADDL [ESP + #0], 0 ! membar_volatile
>> 04a ? ? ? ? ? MOV ? ?EBP,[EBX + #12] ! Field .list
>> 04d ? ? ? ? ? MOV ? ?EDX,[EBP + #8] ? # int ! Field .elementsCount
>> 050 ? ? ? ? ? NullCheck EBP
>> 050
>> 050 ? B8: # ? B4 B9 &lt;- B7 ?Freq: 220752
>> 050 ? ? ? ? ? INC ? ?EDI
>> 051 ? ? ? ? ? TSTL ? #polladdr,EAX ? ?! Safepoint: poll for GC
>> 057 ? ? ? ? ? CMP ? ?EDI,EDX
>> 059 ? ? ? ? ? Jl,s ?B4 ?P=1.000000 C=179200.000000
>>
>>
>> (B) direct buffer access loop (after warmup).
>>
>> 01a ? ? ? ? ? MOV ? ?ECX,[ECX + #12] ! Field .list
>> 01d ? ? ? ? ? MOV ? ?EAX,[ECX + #8] ? # int ! Field .elementsCount
>> 020 ? ? ? ? ? NullCheck ECX
>> 020
>> 020 ? B2: # ? B12 B3 &lt;- B1 ?Freq: 0.999999
>> 020 ? ? ? ? ? TEST ? EAX,EAX
>> 022 ? ? ? ? ? Jle ? ?B12 ?P=0.000000 C=1.000000
>> 022
>> 028 ? B3: # ? B4 &lt;- B2 ?Freq: 0.999999
>> 028 ? ? ? ? ? MOV ? ?EDI,[ECX + #12] ! Field .buffer
>> 02b ? ? ? ? ? XOR ? ?EBX,EBX
>> 02d ? ? ? ? ? MOV ? ?EBP,#360
>> 02d
>> 032 ? B4: # ? B14 B5 &lt;- B3 B6 ? ? ?Loop: B4-B6 inner stride:
>> not constant
>> pre of N158 Freq: 1.99999
>> 032 ? ? ? ? ? MOV ? ?EDX,[EDI + #8]
>> 035 ? ? ? ? ? NullCheck EDI
>> 035
>> 035 ? B5: # ? B13 B6 &lt;- B4 ?Freq: 1.99999
>> 035 ? ? ? ? ? CMPu ? EBX,EDX
>> 037 ? ? ? ? ? Jnb,u ?B13 ?P=0.000001 C=-1.000000
>> 037
>> 03d ? B6: # ? B4 B7 &lt;- B5 ?Freq: 1.99999
>> 03d ? ? ? ? ? MOVSX8 ECX,[EDI + #12 + EBX] ? ?# byte
>> 042 ? ? ? ? ? MEMBAR-release ! (empty encoding)
>> 042 ? ? ? ? ? MOV8 ? [EBP + precise klass :
>> 0x04161d30:Constant:exact *],ECX
>> ! Field ?Volatile.value
>> 048 ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>> 048 ? ? ? ? ? LOCK ADDL [ESP + #0], 0 ! membar_volatile
>> 04d ? ? ? ? ? INC ? ?EBX
>> 04e ? ? ? ? ? CMP ? ?EBX,#1
>> 051 ? ? ? ? ? Jl,s ?B4 ? ? ? ?# Loop end ?P=0.500000 C=334848.000000
>>
>> 053 ? B7: # ? B9 B8 &lt;- B6 ?Freq: 0.999994
>> 053 ? ? ? ? ? MOV ? ?ESI,EAX
>> 055 ? ? ? ? ? MIN ? ?ESI,EDX
>> 05b ? ? ? ? ? SUB ? ?ESI,EBX
>> 05d ? ? ? ? ? AND ? ?ESI,#-4
>> 060 ? ? ? ? ? ADD ? ?ESI,EBX
>> 062 ? ? ? ? ? CMP ? ?EBX,ESI
>> 064 ? ? ? ? ? Jge,s ?B9 ?P=0.000001 C=-1.000000
>> ? ? ? ? ? ? ? NOP ? ? # 10 bytes pad for loops and calls
>>
>> 070 ? B8: # ? B8 B9 &lt;- B7 B8 ? ? ? Loop: B8-B8 inner stride:
>> not constant
>> main of N116 Freq: 999993
>> 070 ? ? ? ? ? MOVSX8 ECX,[EDI + #12 + EBX] ? ?# byte
>> 075 ? ? ? ? ? MEMBAR-release ! (empty encoding)
>> 075 ? ? ? ? ? MOV8 ? [EBP + precise klass :
>> 0x04161d30:Constant:exact *],ECX
>> ! Field ?Volatile.value
>> 07b ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>> 07b ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>> 07b ? ? ? ? ? MOVSX8 ECX,[EDI + #13 + EBX] ? ?# byte
>> 080 ? ? ? ? ? MEMBAR-release ! (empty encoding)
>> 080 ? ? ? ? ? MOV8 ? [EBP + precise klass :
>> 0x04161d30:Constant:exact *],ECX
>> ! Field ?Volatile.value
>> 086 ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>> 086 ? ? ? ? ? MEMBAR-volatile (unnecessary so empty encoding)
>>
>> ... (loop further unrolled here) ...
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From mlists at juma.me.uk  Tue Jan 12 16:04:03 2010
From: mlists at juma.me.uk (Ismael Juma)
Date: Tue, 12 Jan 2010 21:04:03 +0000
Subject: [concurrency-interest] HotSpot inlining in hot loops.
In-Reply-To: <baa8c5731001121147g3467f4e1w60a76f879b09cfc3@mail.gmail.com>
References: <baa8c5731001070153p4583e22dpdcf80e7954e9b2ca@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIELCIEAA.davidcholmes@aapt.net.au>
	<baa8c5731001121147g3467f4e1w60a76f879b09cfc3@mail.gmail.com>
Message-ID: <fa3463561001121304u59644772qf20e46197c38aa92@mail.gmail.com>

Hi Dawid,

On Tue, Jan 12, 2010 at 7:47 PM, Dawid Weiss <dawid.weiss at gmail.com> wrote:
> To complete the thread and for archival reasons -- I did cross-post
> the question to hotspot-compiler. It seems like the optimisation is
> reasonable and possible under Java memory model, but hotspot currently
> does not perform it.

I saw your thread (through Gmane), but I didn't see any reply from
HotSpot developers. Coincidentally, there is a related thread that was
started recently. In case you didn't see it:

http://thread.gmane.org/gmane.comp.java.openjdk.hotspot.compiler.devel/2371

Hope that helps,
Ismael

From dawid.weiss at gmail.com  Tue Jan 12 16:22:50 2010
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Tue, 12 Jan 2010 22:22:50 +0100
Subject: [concurrency-interest] HotSpot inlining in hot loops.
In-Reply-To: <fa3463561001121304u59644772qf20e46197c38aa92@mail.gmail.com>
References: <baa8c5731001070153p4583e22dpdcf80e7954e9b2ca@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCIELCIEAA.davidcholmes@aapt.net.au>
	<baa8c5731001121147g3467f4e1w60a76f879b09cfc3@mail.gmail.com> 
	<fa3463561001121304u59644772qf20e46197c38aa92@mail.gmail.com>
Message-ID: <baa8c5731001121322m47b29aect458df98b33afa6d8@mail.gmail.com>

Hi Ismael,

We had some off-list communications about it too. Thanks for the
thread pointer, it is definitely on topic. It's funny how many
different angles this discussion has; an example of an unsynchronized
busy loop on an object variable _is_ optimized into a jump to the same
location (the condition is evaluated once), so HotSpot is not
consistent here. In any case, my strong belief is that the fact that
the JVM doesn't enforce final fields' assignments doesn't make it any
less suitable for optimization in the case of reader threads; in my
understanding of the JVM spec, such optimizations are allowed and
could take place.

Thanks for discussion, everyone. At least I have some understanding of
the current state of things now.

Dawid

On Tue, Jan 12, 2010 at 10:04 PM, Ismael Juma <mlists at juma.me.uk> wrote:
> Hi Dawid,
>
> On Tue, Jan 12, 2010 at 7:47 PM, Dawid Weiss <dawid.weiss at gmail.com> wrote:
>> To complete the thread and for archival reasons -- I did cross-post
>> the question to hotspot-compiler. It seems like the optimisation is
>> reasonable and possible under Java memory model, but hotspot currently
>> does not perform it.
>
> I saw your thread (through Gmane), but I didn't see any reply from
> HotSpot developers. Coincidentally, there is a related thread that was
> started recently. In case you didn't see it:
>
> http://thread.gmane.org/gmane.comp.java.openjdk.hotspot.compiler.devel/2371
>
> Hope that helps,
> Ismael
>

From robertlazarski at gmail.com  Tue Jan 12 18:26:18 2010
From: robertlazarski at gmail.com (robert lazarski)
Date: Tue, 12 Jan 2010 20:26:18 -0300
Subject: [concurrency-interest] Simple question of how many tasks are in a
	queue
Message-ID: <f87675ee1001121526q662c4313w6d4f921e652723b5@mail.gmail.com>

Hi all,

I have this pool defined:

private static final ThreadFactory factory = new
ONExceptionThreadFactory(new ONExceptionHandler());
private ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 1, 10,
TimeUnit.HOURS,
            new ArrayBlockingQueue<Runnable>(10), factory, new
ONExceptionRejectedExecutionHandler());

Where ONExceptionThreadFactory and ONExceptionRejectedExecutionHandler
are boilerplate implementations that just do some extra logging and
emails.

My question is, how can I determine how many tasks are waiting in the
Queue? I didn't see anything in the javadoc for ThreadPoolExecutor
that would indicate that - sorry if I missed something obvious.

Thanks, robert

From davidcholmes at aapt.net.au  Tue Jan 12 18:31:53 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 13 Jan 2010 09:31:53 +1000
Subject: [concurrency-interest] Simple question of how many tasks are in
	aqueue
In-Reply-To: <f87675ee1001121526q662c4313w6d4f921e652723b5@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAELMIEAA.davidcholmes@aapt.net.au>

Hi Robert,

Use getQueue() to get a reference to the queue, then query its size().

Cheers,
David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of robert
> lazarski
> Sent: Wednesday, 13 January 2010 9:26 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Simple question of how many tasks are in
> aqueue
> 
> 
> Hi all,
> 
> I have this pool defined:
> 
> private static final ThreadFactory factory = new
> ONExceptionThreadFactory(new ONExceptionHandler());
> private ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 1, 10,
> TimeUnit.HOURS,
>             new ArrayBlockingQueue<Runnable>(10), factory, new
> ONExceptionRejectedExecutionHandler());
> 
> Where ONExceptionThreadFactory and ONExceptionRejectedExecutionHandler
> are boilerplate implementations that just do some extra logging and
> emails.
> 
> My question is, how can I determine how many tasks are waiting in the
> Queue? I didn't see anything in the javadoc for ThreadPoolExecutor
> that would indicate that - sorry if I missed something obvious.
> 
> Thanks, robert
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From robertlazarski at gmail.com  Tue Jan 12 18:33:57 2010
From: robertlazarski at gmail.com (robert lazarski)
Date: Tue, 12 Jan 2010 20:33:57 -0300
Subject: [concurrency-interest] Simple question of how many tasks are in
	aqueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAELMIEAA.davidcholmes@aapt.net.au>
References: <f87675ee1001121526q662c4313w6d4f921e652723b5@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAELMIEAA.davidcholmes@aapt.net.au>
Message-ID: <f87675ee1001121533j73156114vd893e64e2c85161@mail.gmail.com>

On Tue, Jan 12, 2010 at 8:31 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Hi Robert,
>
> Use getQueue() to get a reference to the queue, then query its size().
>
> Cheers,
> David
>

Thanks, I seen that but thought it would always return the initial
size of the queue - 10 in this case - and not the number of tasks in
waiting. I'll experiment with that and see.

- R

>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of robert
>> lazarski
>> Sent: Wednesday, 13 January 2010 9:26 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Simple question of how many tasks are in
>> aqueue
>>
>>
>> Hi all,
>>
>> I have this pool defined:
>>
>> private static final ThreadFactory factory = new
>> ONExceptionThreadFactory(new ONExceptionHandler());
>> private ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 1, 10,
>> TimeUnit.HOURS,
>> ? ? ? ? ? ? new ArrayBlockingQueue<Runnable>(10), factory, new
>> ONExceptionRejectedExecutionHandler());
>>
>> Where ONExceptionThreadFactory and ONExceptionRejectedExecutionHandler
>> are boilerplate implementations that just do some extra logging and
>> emails.
>>
>> My question is, how can I determine how many tasks are waiting in the
>> Queue? I didn't see anything in the javadoc for ThreadPoolExecutor
>> that would indicate that - sorry if I missed something obvious.
>>
>> Thanks, robert
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From trustin at gmail.com  Tue Jan 12 20:17:19 2010
From: trustin at gmail.com (=?UTF-8?B?IlRydXN0aW4gTGVlICjsnbTtnazsirkpIg==?=)
Date: Wed, 13 Jan 2010 10:17:19 +0900
Subject: [concurrency-interest] Simple question of how many tasks are in
 a queue
In-Reply-To: <f87675ee1001121526q662c4313w6d4f921e652723b5@mail.gmail.com>
References: <f87675ee1001121526q662c4313w6d4f921e652723b5@mail.gmail.com>
Message-ID: <4B4D1F1F.6000107@gmail.com>

Hi Robert,

You might find my ThreadPoolExecutor extension which keeps track of
memory usage.  It blocks the execute() method if the estimated memory
consumption of the queue exceeds the threshold:

    http://is.gd/69YYG

It supports global limit and per-connection limit, but you will probably
not need the extra complexity resulted in by the per-connection limit
code.  If you are only interested in the number of elements in the
queue, it could be much more simplified.

HTH,
Trustin

robert lazarski wrote:
> Hi all,
> 
> I have this pool defined:
> 
> private static final ThreadFactory factory = new
> ONExceptionThreadFactory(new ONExceptionHandler());
> private ThreadPoolExecutor pool = new ThreadPoolExecutor(1, 1, 10,
> TimeUnit.HOURS,
>             new ArrayBlockingQueue<Runnable>(10), factory, new
> ONExceptionRejectedExecutionHandler());
> 
> Where ONExceptionThreadFactory and ONExceptionRejectedExecutionHandler
> are boilerplate implementations that just do some extra logging and
> emails.
> 
> My question is, how can I determine how many tasks are waiting in the
> Queue? I didn't see anything in the javadoc for ThreadPoolExecutor
> that would indicate that - sorry if I missed something obvious.
> 
> Thanks, robert
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
what we call human nature in actuality is human habit
http://gleamynode.net/


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 260 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100113/53f13ea9/attachment-0001.bin>

From alarmnummer at gmail.com  Mon Jan 18 04:29:20 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Mon, 18 Jan 2010 10:29:20 +0100
Subject: [concurrency-interest] Best way to cause short delays
Message-ID: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>

Hi Guys,

I'm currently working on a transaction back off policy for a software
transactional memory implementation. In some cases a transaction fails
(e.g. because it couldn't acquire all locks for writing the changes).
In these cases I want to back off the transaction by delaying it for a
very short amount of time.

The main back off implementation I use is an exponential one that
delays from a few microseconds to a few milliseconds by calling the
Thread.sleep(ms, ns). The problem is that a delay of a few
microseconds is not possible;
I automatically get a delay of a few milliseconds, so the accuracy is
very low. So what is the best way to let a thread delay for such small
amounts of time?

Peter

From aph at redhat.com  Mon Jan 18 04:56:17 2010
From: aph at redhat.com (Andrew Haley)
Date: Mon, 18 Jan 2010 09:56:17 +0000
Subject: [concurrency-interest] Best way to cause short delays
In-Reply-To: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
References: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
Message-ID: <4B543041.7040605@redhat.com>

On 01/18/2010 09:29 AM, Peter Veentjer wrote:

> I'm currently working on a transaction back off policy for a software
> transactional memory implementation. In some cases a transaction fails
> (e.g. because it couldn't acquire all locks for writing the changes).
> In these cases I want to back off the transaction by delaying it for a
> very short amount of time.
> 
> The main back off implementation I use is an exponential one that
> delays from a few microseconds to a few milliseconds by calling the
> Thread.sleep(ms, ns). The problem is that a delay of a few
> microseconds is not possible;
> I automatically get a delay of a few milliseconds, so the accuracy is
> very low. So what is the best way to let a thread delay for such small
> amounts of time?

I don't think you're going to find any better way to do this than
spinning.  Some CPUs have an internal high-precision timer: for
example, x86 has RDTSC.  However, if your thread gets descheduled and
moved to another processor, Bad Things may happen if you're relying on
a monotonic RDTSC.

Andrew.

From karmazilla at gmail.com  Mon Jan 18 06:35:14 2010
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Mon, 18 Jan 2010 12:35:14 +0100
Subject: [concurrency-interest] Best way to cause short delays
In-Reply-To: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
References: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
Message-ID: <90622e531001180335j28ebc5f6q991f0346d7aa7619@mail.gmail.com>

Maybe you can start with Thread.yield (or maybe spinning before that)
and then degrade into Thread.sleep?

On Mon, Jan 18, 2010 at 10:29 AM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> Hi Guys,
>
> I'm currently working on a transaction back off policy for a software
> transactional memory implementation. In some cases a transaction fails
> (e.g. because it couldn't acquire all locks for writing the changes).
> In these cases I want to back off the transaction by delaying it for a
> very short amount of time.
>
> The main back off implementation I use is an exponential one that
> delays from a few microseconds to a few milliseconds by calling the
> Thread.sleep(ms, ns). The problem is that a delay of a few
> microseconds is not possible;
> I automatically get a delay of a few milliseconds, so the accuracy is
> very low. So what is the best way to let a thread delay for such small
> amounts of time?
>
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

From holger.hoffstaette at googlemail.com  Mon Jan 18 07:10:07 2010
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Mon, 18 Jan 2010 13:10:07 +0100
Subject: [concurrency-interest] Best way to cause short delays
In-Reply-To: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
References: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
Message-ID: <4B544F9F.5000905@googlemail.com>

Peter Veentjer wrote:
> The main back off implementation I use is an exponential one that
> delays from a few microseconds to a few milliseconds by calling the
> Thread.sleep(ms, ns). The problem is that a delay of a few
> microseconds is not possible;
> I automatically get a delay of a few milliseconds, so the accuracy is
> very low. So what is the best way to let a thread delay for such small
> amounts of time?

Required reading:
http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks

Another blog on the the Thread.sleep(ms, ns) misbehaviour:
http://www.sagui.org/~gustavo/blog/code/high-resolution-timer-in-java-5.html

So LockSupport.parkNanos should work better than sleep() as long as CPU,
JVM and OS all play along. After having a good time with rt-Linux and
low-latency messaging last year I can assure you that this is rather
unlikely. :-)

-h

From alarmnummer at gmail.com  Mon Jan 18 09:22:33 2010
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Mon, 18 Jan 2010 15:22:33 +0100
Subject: [concurrency-interest] Best way to cause short delays
In-Reply-To: <4B544F9F.5000905@googlemail.com>
References: <1466c1d61001180129o4786cfd8k37a1b4f638e879c7@mail.gmail.com>
	<4B544F9F.5000905@googlemail.com>
Message-ID: <1466c1d61001180622u52799515q6801d1e33a0b218e@mail.gmail.com>

Thank you (Andrew, Christian and Holger),

I'll research the suggestions.

2010/1/18 Holger Hoffst?tte <holger.hoffstaette at googlemail.com>:
> Peter Veentjer wrote:
>> The main back off implementation I use is an exponential one that
>> delays from a few microseconds to a few milliseconds by calling the
>> Thread.sleep(ms, ns). The problem is that a delay of a few
>> microseconds is not possible;
>> I automatically get a delay of a few milliseconds, so the accuracy is
>> very low. So what is the best way to let a thread delay for such small
>> amounts of time?
>
> Required reading:
> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks
>
> Another blog on the the Thread.sleep(ms, ns) misbehaviour:
> http://www.sagui.org/~gustavo/blog/code/high-resolution-timer-in-java-5.html
>
> So LockSupport.parkNanos should work better than sleep() as long as CPU,
> JVM and OS all play along. After having a good time with rt-Linux and
> low-latency messaging last year I can assure you that this is rather
> unlikely. :-)
>
> -h
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jed at atlassian.com  Tue Jan 19 19:45:30 2010
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Wed, 20 Jan 2010 11:45:30 +1100
Subject: [concurrency-interest] Atomic* questions and code review
Message-ID: <4B56522A.9020101@atlassian.com>

All,

I have found that a lot of our code that uses the Atomic* classes boils 
down to two main usage patterns:

1. Conditional Get. In a loop, get the value and if it is something 
(often null) CAS it and then get again otherwise return the value.
2. Update. Get the value, compute the new value and then CAS from old to 
new. Potential optimise is to use the TTAS pattern here.

I am adding to our concurrent library some code to make all of this a 
lot simpler and more robust. I have two options, one is to have static 
methods and the second is to provide custom extension classes. The 
particular approach isn't that important as the code looks basically the 
same in either case. I will present the sub-classed approach as it is a 
little less cluttered.

For the first (using AtomicReference as an example), I have a method 
called getOrSetAndGetIf(V oldValue, V newValue). Does this name make 
sense? Can anyone think of an improvement?

Here's the code:

    /**
     * Check the current value and if it matches the old value argument, 
set it
     * to the new value and return that instead. If the old value 
argument does
     * not match, ignore both and just return the current value.
     *
     * @param <T> the object type.
     * @param oldValue to check the current value against (reference 
equality
     * check only)
     * @param newValue the new value to set it to
     * @return the current reference value if it doesn't match oldValue or a
     * newly created value.
     */
    public final V getOrSetAndGetIf(final V oldValue, final V newValue) {
        V result = get();
        // loop until invariant is true in case some other thread resets
        // reference to oldValue (although if they are doing that then 
we still
        // cannot guarantee there will be no ABA problem as they could come
        // back and set it after we return)
        while (result == oldValue) {
            // abort if trying to set the same value, otherwise infinite 
loop
            if (result == newValue) {
                return result;
            }
            compareAndSet(oldValue, newValue);
            result = get();
        }
        return result;
    }

This passes all tests I've thrown at it so far. Any 
improvements/suggestions?

The second case is a bit simpler. It uses a Function<V, V> as the 
updated value factory:

    /**
     * Do the actual update. Calls the factory method with the old value 
to do
     * the update logic, then sets the value to that if it hasn't 
changed in the
     * meantime.
     *
     * @return the new updated value.
     */
    public final V update(final Function<V, V> newValueFactory) {
        V oldValue, newValue;
        do {
            oldValue = get();
            newValue = newValueFactory.get(oldValue);
            // test first to implement TTAS optimisation
            if (get() != oldValue) {
                continue;
            }
            // then compare and set
        } while (!compareAndSet(oldValue, newValue));
        return newValue;
    }

Once again, any improvements/suggestions?

All suggestions/comments appreciated.

cheers,
jed.

From jape41 at gmail.com  Mon Jan 25 15:54:26 2010
From: jape41 at gmail.com (JP Fournier)
Date: Mon, 25 Jan 2010 15:54:26 -0500
Subject: [concurrency-interest] question about AtomicReference
Message-ID: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>

Hi All,

I recently spent some long overdue time reading through JCIP.

One thing that was non-intuitive to me was the idea that
AtomicReference could protect more than the reference it contains.
For example on page 331, in the ConcurrentStack example, Node.next
looked unprotected to me:

http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java

I'm assuming that Node.next must be protected by the CAS access to
ConcurrentStack.top.

If I were writing code like this I probably would have naively made
Node.next an AtomicReference too.

Assuming that Node.next is protected by the CAS access to
ConcurrecntStack.top would this hold true if Node.next was something
bigger than reference? Lets say Node.next was an arbitrarily large
array that was mutated within the do loop in the push() method?

Any thoughts appreciated.

jp

From jim.andreou at gmail.com  Mon Jan 25 16:11:59 2010
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Mon, 25 Jan 2010 23:11:59 +0200
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
Message-ID: <7d7138c11001251311ubecdcfap61c6e5992040ce5@mail.gmail.com>

Hi,

You may notice that the next field is updated only in a thread-local
nodes, so needs no protection. After the cas, the field is effectively
immutable.

Dimitris

2010/1/25 JP Fournier <jape41 at gmail.com>:
> Hi All,
>
> I recently spent some long overdue time reading through JCIP.
>
> One thing that was non-intuitive to me was the idea that
> AtomicReference could protect more than the reference it contains.
> For example on page 331, in the ConcurrentStack example, Node.next
> looked unprotected to me:
>
> http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java
>
> I'm assuming that Node.next must be protected by the CAS access to
> ConcurrentStack.top.
>
> If I were writing code like this I probably would have naively made
> Node.next an AtomicReference too.
>
> Assuming that Node.next is protected by the CAS access to
> ConcurrecntStack.top would this hold true if Node.next was something
> bigger than reference? Lets say Node.next was an arbitrarily large
> array that was mutated within the do loop in the push() method?
>
> Any thoughts appreciated.
>
> jp
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From tim at peierls.net  Mon Jan 25 16:29:34 2010
From: tim at peierls.net (Tim Peierls)
Date: Mon, 25 Jan 2010 16:29:34 -0500
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
Message-ID: <63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>

No, it's not particularly intuitive, but it's true. Best not to think of it
as AtomicReference "protecting" Node.next, however. The phrase in the
package doc comment for j.u.c.atomic is "The memory effects for accesses and
updates of atomics generally follow the rules for volatiles", pointing you
to the JLS for further details:

http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4

In the language of *happens-before* introduced in that section, the write to
newHead.next in the push() method *happens-before* a "volatile write"
(top.compareAndSet) and a "volatile read" (top.get) *happens-before* the
read of oldHead.next in the pop() method. So writes to next in push() are
seen by reads of next in pop().

--tim
<http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4>

On Mon, Jan 25, 2010 at 3:54 PM, JP Fournier <jape41 at gmail.com> wrote:

> Hi All,
>
> I recently spent some long overdue time reading through JCIP.
>
> One thing that was non-intuitive to me was the idea that
> AtomicReference could protect more than the reference it contains.
> For example on page 331, in the ConcurrentStack example, Node.next
> looked unprotected to me:
>
> http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java
>
> I'm assuming that Node.next must be protected by the CAS access to
> ConcurrentStack.top.
>
> If I were writing code like this I probably would have naively made
> Node.next an AtomicReference too.
>
> Assuming that Node.next is protected by the CAS access to
> ConcurrecntStack.top would this hold true if Node.next was something
> bigger than reference? Lets say Node.next was an arbitrarily large
> array that was mutated within the do loop in the push() method?
>
> Any thoughts appreciated.
>
> jp
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100125/9d4113ce/attachment.html>

From forax at univ-mlv.fr  Mon Jan 25 16:59:35 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Mon, 25 Jan 2010 22:59:35 +0100
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
Message-ID: <4B5E1447.8040306@univ-mlv.fr>

BTW, I'm not sure that this code is thread-safe because top is not final,
so a NPE in push or pop (on top.get()) is possible.

R?mi

Le 25/01/2010 22:29, Tim Peierls a ?crit :
> No, it's not particularly intuitive, but it's true. Best not to think 
> of it as AtomicReference "protecting" Node.next, however. The phrase 
> in the package doc comment for j.u.c.atomic is "The memory effects for 
> accesses and updates of atomics generally follow the rules for 
> volatiles", pointing you to the JLS for further details:
>
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4
>
> In the language of /happens-before/ introduced in that section, the 
> write to newHead.next in the push() method /happens-before/ a 
> "volatile write" (top.compareAndSet) and a "volatile read" (top.get) 
> /happens-before/ the read of oldHead.next in the pop() method. So 
> writes to next in push() are seen by reads of next in pop().
>
> --tim
>
>
> On Mon, Jan 25, 2010 at 3:54 PM, JP Fournier <jape41 at gmail.com 
> <mailto:jape41 at gmail.com>> wrote:
>
>     Hi All,
>
>     I recently spent some long overdue time reading through JCIP.
>
>     One thing that was non-intuitive to me was the idea that
>     AtomicReference could protect more than the reference it contains.
>     For example on page 331, in the ConcurrentStack example, Node.next
>     looked unprotected to me:
>
>     http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java
>
>     I'm assuming that Node.next must be protected by the CAS access to
>     ConcurrentStack.top.
>
>     If I were writing code like this I probably would have naively made
>     Node.next an AtomicReference too.
>
>     Assuming that Node.next is protected by the CAS access to
>     ConcurrecntStack.top would this hold true if Node.next was something
>     bigger than reference? Lets say Node.next was an arbitrarily large
>     array that was mutated within the do loop in the push() method?
>
>     Any thoughts appreciated.
>
>     jp
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100125/a5f833e2/attachment.html>

From tim at peierls.net  Mon Jan 25 17:02:22 2010
From: tim at peierls.net (Tim Peierls)
Date: Mon, 25 Jan 2010 17:02:22 -0500
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <4B5E1447.8040306@univ-mlv.fr>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
	<4B5E1447.8040306@univ-mlv.fr>
Message-ID: <63b4e4051001251402g7a1a061ey68598bb57d5de78e@mail.gmail.com>

Ugh, I think we have an errata entry (pending) for that.

But if a ConcurrentStack instance is published safely, then it's safe to
use.

--tim

On Mon, Jan 25, 2010 at 4:59 PM, R?mi Forax <forax at univ-mlv.fr> wrote:

>  BTW, I'm not sure that this code is thread-safe because top is not final,
> so a NPE in push or pop (on top.get()) is possible.
>
> R?mi
>
> Le 25/01/2010 22:29, Tim Peierls a ?crit :
>
> No, it's not particularly intuitive, but it's true. Best not to think of it
> as AtomicReference "protecting" Node.next, however. The phrase in the
> package doc comment for j.u.c.atomic is "The memory effects for accesses
> and updates of atomics generally follow the rules for volatiles", pointing
> you to the JLS for further details:
>
>  http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4
>
>  In the language of *happens-before* introduced in that section, the write
> to newHead.next in the push() method *happens-before* a "volatile write"
> (top.compareAndSet) and a "volatile read" (top.get) *happens-before* the
> read of oldHead.next in the pop() method. So writes to next in push() are
> seen by reads of next in pop().
>
>  --tim
>
>
> On Mon, Jan 25, 2010 at 3:54 PM, JP Fournier <jape41 at gmail.com> wrote:
>
>> Hi All,
>>
>> I recently spent some long overdue time reading through JCIP.
>>
>> One thing that was non-intuitive to me was the idea that
>> AtomicReference could protect more than the reference it contains.
>> For example on page 331, in the ConcurrentStack example, Node.next
>> looked unprotected to me:
>>
>> http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java
>>
>> I'm assuming that Node.next must be protected by the CAS access to
>> ConcurrentStack.top.
>>
>> If I were writing code like this I probably would have naively made
>> Node.next an AtomicReference too.
>>
>> Assuming that Node.next is protected by the CAS access to
>> ConcurrecntStack.top would this hold true if Node.next was something
>> bigger than reference? Lets say Node.next was an arbitrarily large
>> array that was mutated within the do loop in the push() method?
>>
>> Any thoughts appreciated.
>>
>> jp
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100125/2cd24023/attachment-0001.html>

From tronje.krop at kom.tu-darmstadt.de  Mon Jan 25 18:09:39 2010
From: tronje.krop at kom.tu-darmstadt.de (Tronje Krop)
Date: Tue, 26 Jan 2010 00:09:39 +0100
Subject: [concurrency-interest] Controlling thread-switches on
	synchronization?
Message-ID: <4B5E24B3.40804@kom.tu-darmstadt.de>

Hi all,

I'm looking for ways to control - or better prevent - thread-switches
on synchronization between two tasks.

I'm developing a performance critical actor-like middleware for Java
based on tasks (aka. micro-threads) that provides primitive paradigms
for Communicating Sequential Processes (CSP) as defined by Hoare.

In general, code written for this middleware will not block, but switch
between task using a view tens to hundreds bytecode instructions. In
rare cases, the CSP-implementation detects that a sender/receiver is
is switching but has not fully initialized its continuation state.

In this rare case I currently uses a simple "Lock" to block the task's
thread. This should only block for a very short period in time as the
thread is hopefully expected to be unblocked by the other task's thread
after a view tens to hundred bytecode instructions - neglecting effects
of garbage collection and thread preemption.

>From what I know about concurrency primitives in Java, this may not be
the "optimal" solution as this might result in a thread switch, which
needs a few thousand machine code instructions as well as a flush of
cache lines with a follow-up of cache misses.

As this is exactly what I intend to prevent by my middleware it seems
far from optimal.

I know, I can use tryLock() to implement busy-waiting and other more
advances strategies of task switching, but I have no experience with
that, and there may be other primitives or techniques I not even know
about.

So my question is: What is best way to go on?

Should I go on with Lock.tryLock(...) or are there better ways to
address the above problem of thread switches? And which tools from
the concurrency library may assist here?

Anny suggestions, hints, and links of this experienced group (that I'm
following now for some time) are very welcome.

Thanks in advance

Tronje


From jape41 at gmail.com  Mon Jan 25 19:58:48 2010
From: jape41 at gmail.com (JP Fournier)
Date: Mon, 25 Jan 2010 19:58:48 -0500
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
Message-ID: <2655639a1001251658n1d0a8120j5602f67cb28774a4@mail.gmail.com>

Thanks for the insight.  This was definitely not intuitive to me.

Is it possible to do something even more un-intuitive by using
something like AtomicInteger to generate some sort of useful
happens-before relationship when reading/writing something external to
the AtomicInteger?

On closer inspection, it sounds like my original assumptions around
the nature of Atomic* are very close to weakCompareAndSet() which
doesn't seem to imply any happens-before behaviour.  Is this correct?

regards,

jp



On Mon, Jan 25, 2010 at 4:29 PM, Tim Peierls <tim at peierls.net> wrote:
> No, it's not particularly intuitive, but it's true. Best not to think of it
> as AtomicReference "protecting" Node.next, however. The phrase in the
> package doc comment for j.u.c.atomic is "The memory effects for accesses and
> updates of atomics generally follow the rules for volatiles", pointing you
> to the JLS for further details:
> http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4
> In the language of happens-before?introduced in that section, the write to
> newHead.next in the push() method?happens-before?a "volatile write"
> (top.compareAndSet) and a "volatile read" (top.get)?happens-before?the read
> of oldHead.next in the pop() method. So writes to next in push() are seen by
> reads of next in pop().
> --tim
>
>
> On Mon, Jan 25, 2010 at 3:54 PM, JP Fournier <jape41 at gmail.com> wrote:
>>
>> Hi All,
>>
>> I recently spent some long overdue time reading through JCIP.
>>
>> One thing that was non-intuitive to me was the idea that
>> AtomicReference could protect more than the reference it contains.
>> For example on page 331, in the ConcurrentStack example, Node.next
>> looked unprotected to me:
>>
>> http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java
>>
>> I'm assuming that Node.next must be protected by the CAS access to
>> ConcurrentStack.top.
>>
>> If I were writing code like this I probably would have naively made
>> Node.next an AtomicReference too.
>>
>> Assuming that Node.next is protected by the CAS access to
>> ConcurrecntStack.top would this hold true if Node.next was something
>> bigger than reference? Lets say Node.next was an arbitrarily large
>> array that was mutated within the do loop in the push() method?
>>
>> Any thoughts appreciated.
>>
>> jp
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From tim at peierls.net  Mon Jan 25 23:31:06 2010
From: tim at peierls.net (Tim Peierls)
Date: Mon, 25 Jan 2010 23:31:06 -0500
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <2655639a1001251658n1d0a8120j5602f67cb28774a4@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
	<2655639a1001251658n1d0a8120j5602f67cb28774a4@mail.gmail.com>
Message-ID: <63b4e4051001252031p299efccco570b489e9f102545@mail.gmail.com>

On Mon, Jan 25, 2010 at 7:58 PM, JP Fournier <jape41 at gmail.com> wrote:

> Is it possible to do something even more un-intuitive by using
> something like AtomicInteger to generate some sort of useful
> happens-before relationship when reading/writing something external to
> the AtomicInteger?
>

Yes, but designs that rely on such constructions can be brittle.



> On closer inspection, it sounds like my original assumptions around
> the nature of Atomic* are very close to weakCompareAndSet() which
> doesn't seem to imply any happens-before behaviour.  Is this correct?
>

Yes.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100125/b024b7b6/attachment.html>

From hans.boehm at hp.com  Tue Jan 26 14:56:55 2010
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 26 Jan 2010 19:56:55 +0000
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <63b4e4051001251402g7a1a061ey68598bb57d5de78e@mail.gmail.com>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
	<4B5E1447.8040306@univ-mlv.fr>
	<63b4e4051001251402g7a1a061ey68598bb57d5de78e@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D0425783BFEE8C@GVW0436EXB.americas.hpqcorp.net>

You mean if it's communicated between threads without a data race?  If that's what you meant, I agree.

I'm not sure what "safe publication" has to do with the current version, since there is no final field.

At the risk of repetition, I still think the easiest way to think of all of this 99% of the time is to ask whether the code allows a data race, i.e. concurrent non-read-only access to the same non-atomic, non-volatile variable.  If it doesn't (and if you don't use a few weird methods like lazySet and weakCompareAndSet, which you should think at least three times about using), you get sequential consistency.  End of story.  And I think that's the case here if the ConcurrentStack is initially shared without introducing a race.  So nothing here should be surprising.

Things get complicated only if you can't play by those rules for performance reasons, or if you have to expose your class to untrusted code that might not play by the rules.

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Tim Peierls
Sent: Monday, January 25, 2010 2:02 PM
To: R?mi Forax
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] question about AtomicReference

Ugh, I think we have an errata entry (pending) for that.

But if a ConcurrentStack instance is published safely, then it's safe to use.

--tim

On Mon, Jan 25, 2010 at 4:59 PM, R?mi Forax <forax at univ-mlv.fr<mailto:forax at univ-mlv.fr>> wrote:
BTW, I'm not sure that this code is thread-safe because top is not final,
so a NPE in push or pop (on top.get()) is possible.

R?mi

Le 25/01/2010 22:29, Tim Peierls a ?crit :
No, it's not particularly intuitive, but it's true. Best not to think of it as AtomicReference "protecting" Node.next, however. The phrase in the package doc comment for j.u.c.atomic is "The memory effects for accesses and updates of atomics generally follow the rules for volatiles", pointing you to the JLS for further details:

http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4

In the language of happens-before introduced in that section, the write to newHead.next in the push() method happens-before a "volatile write" (top.compareAndSet) and a "volatile read" (top.get) happens-before the read of oldHead.next in the pop() method. So writes to next in push() are seen by reads of next in pop().

--tim


On Mon, Jan 25, 2010 at 3:54 PM, JP Fournier <jape41 at gmail.com<mailto:jape41 at gmail.com>> wrote:
Hi All,

I recently spent some long overdue time reading through JCIP.

One thing that was non-intuitive to me was the idea that
AtomicReference could protect more than the reference it contains.
For example on page 331, in the ConcurrentStack example, Node.next
looked unprotected to me:

http://www.javaconcurrencyinpractice.com/listings/ConcurrentStack.java

I'm assuming that Node.next must be protected by the CAS access to
ConcurrentStack.top.

If I were writing code like this I probably would have naively made
Node.next an AtomicReference too.

Assuming that Node.next is protected by the CAS access to
ConcurrecntStack.top would this hold true if Node.next was something
bigger than reference? Lets say Node.next was an arbitrarily large
array that was mutated within the do loop in the push() method?

Any thoughts appreciated.

jp
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100126/cc9ae31e/attachment.html>

From tim at peierls.net  Tue Jan 26 16:08:35 2010
From: tim at peierls.net (Tim Peierls)
Date: Tue, 26 Jan 2010 16:08:35 -0500
Subject: [concurrency-interest] question about AtomicReference
In-Reply-To: <238A96A773B3934685A7269CC8A8D0425783BFEE8C@GVW0436EXB.americas.hpqcorp.net>
References: <2655639a1001251254h440c9513jc3eaa7cfd04b160f@mail.gmail.com>
	<63b4e4051001251329o5f90c64ana5ff6c53ef88507b@mail.gmail.com>
	<4B5E1447.8040306@univ-mlv.fr>
	<63b4e4051001251402g7a1a061ey68598bb57d5de78e@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D0425783BFEE8C@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <63b4e4051001261308t7aa28a68i1e89638e7fe72930@mail.gmail.com>

On Tue, Jan 26, 2010 at 2:56 PM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  At the risk of repetition, I still think the easiest way to think of all
> of this 99% of the time is to ask whether the code allows a data race, i.e.
> concurrent non-read-only access to the same non-atomic, non-volatile
> variable.
>

It might be easier for you, but for the target audience of JCiP, that
characterization is at too low a level to be very helpful. One of the main
goals of JCiP was to provide higher-level guidance than this. If that
guidance is actually wrong, then we need to fix it, but please don't just
dismiss it.

Brian Goetz had an amusing parable about this, but I'll let him decide
whether to share it.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100126/5d9b1bfe/attachment.html>

From eshioji at gmail.com  Wed Jan 27 04:28:11 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Wed, 27 Jan 2010 18:28:11 +0900
Subject: [concurrency-interest] Performance penalty of volatile keyword -
	negligible or significant?
Message-ID: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>

Hi,


There are some books that claim "read/write on a volatile field has
almost the same performance as in non-volatile fields in modern CPUs",
whereas others claim "volatile keyword introduces happens-before
relationship which has a strong impact on performance, and hence, the
performance hit is almost the same as in synchronized keyword".


I understand it is difficult to give a general answer to this kind of
question, but for example more specifically, suppose I have a piece of
code like this:

class Sample {
     private volatile boolean doA;
     private volatile boolean doB;
     private volatile boolean doC;

     public void method(){
          if(doA) A();
          if(doB) B();
          if(doC) C();
     }
}//class

and let's say this piece of code is performance-critical, and it's
nice, but not essential, to be able to switch the flags doA-C between
true/false (so I'm mostly reading the flag). Should I rather do like
this:

class Sample {
     private final boolean doA = checkA();
     private final boolean doB = checkB();
     private final boolean doC = checkC();

or retain the capability to switch after object creation? I understand
it's best to test, but generally, how nervous should I be to add reads
on volatile field?




Regards,
Enno

From karmazilla at gmail.com  Wed Jan 27 05:01:21 2010
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Wed, 27 Jan 2010 11:01:21 +0100
Subject: [concurrency-interest] Performance penalty of volatile keyword
	- negligible or significant?
In-Reply-To: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>
References: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>
Message-ID: <90622e531001270201x4806f793rc337ec4a8bd2d8@mail.gmail.com>

This is a bit of the deep end for me too, so I hope I'm not too wrong here:

When you are mostly reading the variables, then the cache lines that
contain the data will *likely* be in the shared state (assuming a CPU
with a MESI cache coherency protocol, such as x86). Shared means that
the ownership is distributed among the cores. The cores are free to
read the data as often as they can and the performance will be that of
the cache level (L1d, L2, etc.).
But if a core wants to write to the variable, then the cache line must
be marked as exclusive and Request For Ownership messages must be sent
to the other cores. This is a relatively expensive operation. Once the
data has been written, the other cores will likely want to read it.
Before they can do that, the changes must propagate to their caches
and the cache line must be put in a shared state again.

On Wed, Jan 27, 2010 at 10:28 AM, Enno Shioji <eshioji at gmail.com> wrote:
> Hi,
>
>
> There are some books that claim "read/write on a volatile field has
> almost the same performance as in non-volatile fields in modern CPUs",
> whereas others claim "volatile keyword introduces happens-before
> relationship which has a strong impact on performance, and hence, the
> performance hit is almost the same as in synchronized keyword".
>
>
> I understand it is difficult to give a general answer to this kind of
> question, but for example more specifically, suppose I have a piece of
> code like this:
>
> class Sample {
> ? ? private volatile boolean doA;
> ? ? private volatile boolean doB;
> ? ? private volatile boolean doC;
>
> ? ? public void method(){
> ? ? ? ? ?if(doA) A();
> ? ? ? ? ?if(doB) B();
> ? ? ? ? ?if(doC) C();
> ? ? }
> }//class
>
> and let's say this piece of code is performance-critical, and it's
> nice, but not essential, to be able to switch the flags doA-C between
> true/false (so I'm mostly reading the flag). Should I rather do like
> this:
>
> class Sample {
> ? ? private final boolean doA = checkA();
> ? ? private final boolean doB = checkB();
> ? ? private final boolean doC = checkC();
>
> or retain the capability to switch after object creation? I understand
> it's best to test, but generally, how nervous should I be to add reads
> on volatile field?
>
>
>
>
> Regards,
> Enno
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From davidcholmes at aapt.net.au  Wed Jan 27 05:16:13 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 27 Jan 2010 20:16:13 +1000
Subject: [concurrency-interest] Performance penalty of volatile keyword
	-negligible or significant?
In-Reply-To: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCENEIEAA.davidcholmes@aapt.net.au>

Enno Shioji writes:
> There are some books that claim "read/write on a volatile field has
> almost the same performance as in non-volatile fields in modern CPUs",
> whereas others claim "volatile keyword introduces happens-before
> relationship which has a strong impact on performance, and hence, the
> performance hit is almost the same as in synchronized keyword".

This is an apples to oranges comparison. Both volatile and synchronized have
the same memory synchronization requirements: volatile-read ==
monitor-enter; volatile-write == monitor-exit. So in addition use of
synchronized has the overhead of actually locking/unlocking, not to mention
the fact it can also block! Use of volatile variables instead of locking is
only applicable to limited situations and the choice would not generally be
made based on the assumed cost of memory synchronization.

As for the actual memory synchronization costs: it varies. Not only across
architectures, but across different chips in a family - so it is a moving
target. (Aside: the Hotspot VM has changed the actual hardware instructions
used for this a number of times as the cost of such operations changes on
the current popular chips.) As a general rule, reads are cheap, writes may
not be.

I would generally opt for lock-free (ie use of volatile in specialized
algorithms) over locking (use of synchronized or
java.util.concurrent.locks.Lock) if given the choice, as lock-free scales
better in general. Of course you don't often have that choice.

> I understand it is difficult to give a general answer to this kind of
> question, but for example more specifically, suppose I have a piece of
> code like this:
>
> class Sample {
>      private volatile boolean doA;
>      private volatile boolean doB;
>      private volatile boolean doC;
>
>      public void method(){
>           if(doA) A();
>           if(doB) B();
>           if(doC) C();
>      }
> }//class
>
> and let's say this piece of code is performance-critical, and it's
> nice, but not essential, to be able to switch the flags doA-C between
> true/false (so I'm mostly reading the flag). Should I rather do like
> this:
>
> class Sample {
>      private final boolean doA = checkA();
>      private final boolean doB = checkB();
>      private final boolean doC = checkC();
>
> or retain the capability to switch after object creation? I understand
> it's best to test, but generally, how nervous should I be to add reads
> on volatile field?

The zeroeth rule of correct synchronization is to avoid the need for
synchronization: don't share mutable objects. So if you can get away with an
immutable design then use it. Usually semantic requirements will dictate
this rather then personal preference.

If you must synchronize then avoid premature optimization: do it the simple
correct way and then see if a performance issue exists, and only then look
for more complex, faster ways.

I would not be concerned with using volatiles if needed.

Just my 2c. :)

Cheers,
David Holmes


From karmazilla at gmail.com  Wed Jan 27 05:19:01 2010
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Wed, 27 Jan 2010 11:19:01 +0100
Subject: [concurrency-interest] Performance penalty of volatile keyword
	- negligible or significant?
In-Reply-To: <90622e531001270201x4806f793rc337ec4a8bd2d8@mail.gmail.com>
References: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>
	<90622e531001270201x4806f793rc337ec4a8bd2d8@mail.gmail.com>
Message-ID: <90622e531001270219l72e1ce42pe76828c84da74425@mail.gmail.com>

Send vs. Save miss-click.

So in essence, read-mostly volatiles are *probably* going to have a
negligible performance impact. What can muddy the picture is false
sharing; that is, when shared read-heavy data is on the same cache
line as some write-heavy piece of data. For instance, if your
"volatile boolean doC" have a write-heavy usage pattern, then it could
impact the access times of doB and/or doA. I think it's easier to put
fields that should not be falsely shared in different objects, than to
try and game how the JVM will align them in memory.
The cost of an RFO depends on the processor. I think it's usually more
than a L2 access (maybe double that) but not as much as reading from
main memory (which is also loads more expensive than a L2 access).

On Wed, Jan 27, 2010 at 11:01 AM, Christian Vest Hansen
<karmazilla at gmail.com> wrote:
> This is a bit of the deep end for me too, so I hope I'm not too wrong here:
>
> When you are mostly reading the variables, then the cache lines that
> contain the data will *likely* be in the shared state (assuming a CPU
> with a MESI cache coherency protocol, such as x86). Shared means that
> the ownership is distributed among the cores. The cores are free to
> read the data as often as they can and the performance will be that of
> the cache level (L1d, L2, etc.).
> But if a core wants to write to the variable, then the cache line must
> be marked as exclusive and Request For Ownership messages must be sent
> to the other cores. This is a relatively expensive operation. Once the
> data has been written, the other cores will likely want to read it.
> Before they can do that, the changes must propagate to their caches
> and the cache line must be put in a shared state again.
>
> On Wed, Jan 27, 2010 at 10:28 AM, Enno Shioji <eshioji at gmail.com> wrote:
>> Hi,
>>
>>
>> There are some books that claim "read/write on a volatile field has
>> almost the same performance as in non-volatile fields in modern CPUs",
>> whereas others claim "volatile keyword introduces happens-before
>> relationship which has a strong impact on performance, and hence, the
>> performance hit is almost the same as in synchronized keyword".
>>
>>
>> I understand it is difficult to give a general answer to this kind of
>> question, but for example more specifically, suppose I have a piece of
>> code like this:
>>
>> class Sample {
>> ? ? private volatile boolean doA;
>> ? ? private volatile boolean doB;
>> ? ? private volatile boolean doC;
>>
>> ? ? public void method(){
>> ? ? ? ? ?if(doA) A();
>> ? ? ? ? ?if(doB) B();
>> ? ? ? ? ?if(doC) C();
>> ? ? }
>> }//class
>>
>> and let's say this piece of code is performance-critical, and it's
>> nice, but not essential, to be able to switch the flags doA-C between
>> true/false (so I'm mostly reading the flag). Should I rather do like
>> this:
>>
>> class Sample {
>> ? ? private final boolean doA = checkA();
>> ? ? private final boolean doB = checkB();
>> ? ? private final boolean doC = checkC();
>>
>> or retain the capability to switch after object creation? I understand
>> it's best to test, but generally, how nervous should I be to add reads
>> on volatile field?
>>
>>
>>
>>
>> Regards,
>> Enno
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Venlig hilsen / Kind regards,
> Christian Vest Hansen.
>



-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.


From davidcholmes at aapt.net.au  Wed Jan 27 05:24:33 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 27 Jan 2010 20:24:33 +1000
Subject: [concurrency-interest] Performance penalty of volatile keyword
	-negligible or significant?
In-Reply-To: <b20868901001270128w26c7da3asb688097dcc87a3e6@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>

PS. See

http://gee.cs.oswego.edu/dl/jmm/cookbook.html

The JSR-133 Cookbook for Compiler Writers


Probably could do with some fresher data though.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Enno
> Shioji
> Sent: Wednesday, 27 January 2010 7:28 PM
> To: concurrency-interest
> Subject: [concurrency-interest] Performance penalty of volatile keyword
> -negligible or significant?
> 
> 
> Hi,
> 
> 
> There are some books that claim "read/write on a volatile field has
> almost the same performance as in non-volatile fields in modern CPUs",
> whereas others claim "volatile keyword introduces happens-before
> relationship which has a strong impact on performance, and hence, the
> performance hit is almost the same as in synchronized keyword".
> 
> 
> I understand it is difficult to give a general answer to this kind of
> question, but for example more specifically, suppose I have a piece of
> code like this:
> 
> class Sample {
>      private volatile boolean doA;
>      private volatile boolean doB;
>      private volatile boolean doC;
> 
>      public void method(){
>           if(doA) A();
>           if(doB) B();
>           if(doC) C();
>      }
> }//class
> 
> and let's say this piece of code is performance-critical, and it's
> nice, but not essential, to be able to switch the flags doA-C between
> true/false (so I'm mostly reading the flag). Should I rather do like
> this:
> 
> class Sample {
>      private final boolean doA = checkA();
>      private final boolean doB = checkB();
>      private final boolean doC = checkC();
> 
> or retain the capability to switch after object creation? I understand
> it's best to test, but generally, how nervous should I be to add reads
> on volatile field?
> 
> 
> 
> 
> Regards,
> Enno
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From forax at univ-mlv.fr  Wed Jan 27 06:33:32 2010
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Wed, 27 Jan 2010 12:33:32 +0100
Subject: [concurrency-interest] Performance penalty of volatile keyword
 -negligible or significant?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
Message-ID: <4B60248C.2040208@univ-mlv.fr>

Jochen Theodorou (a Groovy guy) did some measurements of the cost of a 
volatile read.
It seems that there is an hidden cost because a volatile read disable 
some optimisations.
http://wiki.jvmlangsummit.com/images/8/8c/Theodorou_Groovy.pdf

R?mi

Le 27/01/2010 11:24, David Holmes a ?crit :
> PS. See
>
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html
>
> The JSR-133 Cookbook for Compiler Writers
>
>
> Probably could do with some fresher data though.
>
> David
>
>    
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Enno
>> Shioji
>> Sent: Wednesday, 27 January 2010 7:28 PM
>> To: concurrency-interest
>> Subject: [concurrency-interest] Performance penalty of volatile keyword
>> -negligible or significant?
>>
>>
>> Hi,
>>
>>
>> There are some books that claim "read/write on a volatile field has
>> almost the same performance as in non-volatile fields in modern CPUs",
>> whereas others claim "volatile keyword introduces happens-before
>> relationship which has a strong impact on performance, and hence, the
>> performance hit is almost the same as in synchronized keyword".
>>
>>
>> I understand it is difficult to give a general answer to this kind of
>> question, but for example more specifically, suppose I have a piece of
>> code like this:
>>
>> class Sample {
>>       private volatile boolean doA;
>>       private volatile boolean doB;
>>       private volatile boolean doC;
>>
>>       public void method(){
>>            if(doA) A();
>>            if(doB) B();
>>            if(doC) C();
>>       }
>> }//class
>>
>> and let's say this piece of code is performance-critical, and it's
>> nice, but not essential, to be able to switch the flags doA-C between
>> true/false (so I'm mostly reading the flag). Should I rather do like
>> this:
>>
>> class Sample {
>>       private final boolean doA = checkA();
>>       private final boolean doB = checkB();
>>       private final boolean doC = checkC();
>>
>> or retain the capability to switch after object creation? I understand
>> it's best to test, but generally, how nervous should I be to add reads
>> on volatile field?
>>
>>
>>
>>
>> Regards,
>> Enno
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>      
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    


From eshioji at gmail.com  Wed Jan 27 08:56:17 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Wed, 27 Jan 2010 22:56:17 +0900
Subject: [concurrency-interest] Performance penalty of volatile keyword
	-negligible or significant?
In-Reply-To: <4B60248C.2040208@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
	<4B60248C.2040208@univ-mlv.fr>
Message-ID: <b20868901001270556g78d36e29te4d6da90991c805b@mail.gmail.com>

Wow, so many great answers... :) Thank you!

I think I have a much better understanding on this now. Thanks, as always!



Regards,
Enno


On Wed, Jan 27, 2010 at 8:33 PM, R?mi Forax <forax at univ-mlv.fr> wrote:
> Jochen Theodorou (a Groovy guy) did some measurements of the cost of a
> volatile read.
> It seems that there is an hidden cost because a volatile read disable some
> optimisations.
> http://wiki.jvmlangsummit.com/images/8/8c/Theodorou_Groovy.pdf
>
> R?mi
>
> Le 27/01/2010 11:24, David Holmes a ?crit :
>>
>> PS. See
>>
>> http://gee.cs.oswego.edu/dl/jmm/cookbook.html
>>
>> The JSR-133 Cookbook for Compiler Writers
>>
>>
>> Probably could do with some fresher data though.
>>
>> David
>>
>>
>>>
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Enno
>>> Shioji
>>> Sent: Wednesday, 27 January 2010 7:28 PM
>>> To: concurrency-interest
>>> Subject: [concurrency-interest] Performance penalty of volatile keyword
>>> -negligible or significant?
>>>
>>>
>>> Hi,
>>>
>>>
>>> There are some books that claim "read/write on a volatile field has
>>> almost the same performance as in non-volatile fields in modern CPUs",
>>> whereas others claim "volatile keyword introduces happens-before
>>> relationship which has a strong impact on performance, and hence, the
>>> performance hit is almost the same as in synchronized keyword".
>>>
>>>
>>> I understand it is difficult to give a general answer to this kind of
>>> question, but for example more specifically, suppose I have a piece of
>>> code like this:
>>>
>>> class Sample {
>>> ? ? ?private volatile boolean doA;
>>> ? ? ?private volatile boolean doB;
>>> ? ? ?private volatile boolean doC;
>>>
>>> ? ? ?public void method(){
>>> ? ? ? ? ? if(doA) A();
>>> ? ? ? ? ? if(doB) B();
>>> ? ? ? ? ? if(doC) C();
>>> ? ? ?}
>>> }//class
>>>
>>> and let's say this piece of code is performance-critical, and it's
>>> nice, but not essential, to be able to switch the flags doA-C between
>>> true/false (so I'm mostly reading the flag). Should I rather do like
>>> this:
>>>
>>> class Sample {
>>> ? ? ?private final boolean doA = checkA();
>>> ? ? ?private final boolean doB = checkB();
>>> ? ? ?private final boolean doC = checkC();
>>>
>>> or retain the capability to switch after object creation? I understand
>>> it's best to test, but generally, how nervous should I be to add reads
>>> on volatile field?
>>>
>>>
>>>
>>>
>>> Regards,
>>> Enno
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From Hans.Boehm at hp.com  Wed Jan 27 14:32:16 2010
From: Hans.Boehm at hp.com (Hans Boehm)
Date: Wed, 27 Jan 2010 11:32:16 -0800 (PST)
Subject: [concurrency-interest] Performance penalty of volatile keyword
 -negligible or significant?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
Message-ID: <alpine.LNX.1.10.1001271125350.2272@192.168.2.2>

In particular, you should be careful with the PowerPC recipe for
LoadLoad and LoadStore.  Although the implementation described there
usually does the right thing, a fully correct one is much slower.
See WG21 paper N2745 for the C++ discussion, where the situation
is essentially the same.

Hans

On Wed, 27 Jan 2010, David Holmes wrote:

> PS. See
>
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html
>
> The JSR-133 Cookbook for Compiler Writers
>
>
> Probably could do with some fresher data though.
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Enno
>> Shioji
>> Sent: Wednesday, 27 January 2010 7:28 PM
>> To: concurrency-interest
>> Subject: [concurrency-interest] Performance penalty of volatile keyword
>> -negligible or significant?
>>
>>
>> Hi,
>>
>>
>> There are some books that claim "read/write on a volatile field has
>> almost the same performance as in non-volatile fields in modern CPUs",
>> whereas others claim "volatile keyword introduces happens-before
>> relationship which has a strong impact on performance, and hence, the
>> performance hit is almost the same as in synchronized keyword".
>>
>>
>> I understand it is difficult to give a general answer to this kind of
>> question, but for example more specifically, suppose I have a piece of
>> code like this:
>>
>> class Sample {
>>      private volatile boolean doA;
>>      private volatile boolean doB;
>>      private volatile boolean doC;
>>
>>      public void method(){
>>           if(doA) A();
>>           if(doB) B();
>>           if(doC) C();
>>      }
>> }//class
>>
>> and let's say this piece of code is performance-critical, and it's
>> nice, but not essential, to be able to switch the flags doA-C between
>> true/false (so I'm mostly reading the flag). Should I rather do like
>> this:
>>
>> class Sample {
>>      private final boolean doA = checkA();
>>      private final boolean doB = checkB();
>>      private final boolean doC = checkC();
>>
>> or retain the capability to switch after object creation? I understand
>> it's best to test, but generally, how nervous should I be to add reads
>> on volatile field?
>>
>>
>>
>>
>> Regards,
>> Enno
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From brian at briangoetz.com  Thu Jan 28 13:15:23 2010
From: brian at briangoetz.com (Brian Goetz)
Date: Thu, 28 Jan 2010 13:15:23 -0500
Subject: [concurrency-interest] Performance penalty of volatile keyword
 -negligible or significant?
In-Reply-To: <4B60248C.2040208@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCOENEIEAA.davidcholmes@aapt.net.au>
	<4B60248C.2040208@univ-mlv.fr>
Message-ID: <4B61D43B.1050504@briangoetz.com>

Jochen presented this at the JVM Language Summit.  He made a number of fairly 
basic benchmarking mistakes, so I would not take these numbers at face value.

It is true that volatile accesses do inhibit some code-motion optimizations 
(to preserve JMM ordering guarantees.)

R?mi Forax wrote:
> Jochen Theodorou (a Groovy guy) did some measurements of the cost of a 
> volatile read.
> It seems that there is an hidden cost because a volatile read disable 
> some optimisations.
> http://wiki.jvmlangsummit.com/images/8/8c/Theodorou_Groovy.pdf
> 
> R?mi
> 
> Le 27/01/2010 11:24, David Holmes a ?crit :
>> PS. See
>>
>> http://gee.cs.oswego.edu/dl/jmm/cookbook.html
>>
>> The JSR-133 Cookbook for Compiler Writers
>>
>>
>> Probably could do with some fresher data though.
>>
>> David
>>
>>   
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Enno
>>> Shioji
>>> Sent: Wednesday, 27 January 2010 7:28 PM
>>> To: concurrency-interest
>>> Subject: [concurrency-interest] Performance penalty of volatile keyword
>>> -negligible or significant?
>>>
>>>
>>> Hi,
>>>
>>>
>>> There are some books that claim "read/write on a volatile field has
>>> almost the same performance as in non-volatile fields in modern CPUs",
>>> whereas others claim "volatile keyword introduces happens-before
>>> relationship which has a strong impact on performance, and hence, the
>>> performance hit is almost the same as in synchronized keyword".
>>>
>>>
>>> I understand it is difficult to give a general answer to this kind of
>>> question, but for example more specifically, suppose I have a piece of
>>> code like this:
>>>
>>> class Sample {
>>>       private volatile boolean doA;
>>>       private volatile boolean doB;
>>>       private volatile boolean doC;
>>>
>>>       public void method(){
>>>            if(doA) A();
>>>            if(doB) B();
>>>            if(doC) C();
>>>       }
>>> }//class
>>>
>>> and let's say this piece of code is performance-critical, and it's
>>> nice, but not essential, to be able to switch the flags doA-C between
>>> true/false (so I'm mostly reading the flag). Should I rather do like
>>> this:
>>>
>>> class Sample {
>>>       private final boolean doA = checkA();
>>>       private final boolean doB = checkB();
>>>       private final boolean doC = checkC();
>>>
>>> or retain the capability to switch after object creation? I understand
>>> it's best to test, but generally, how nervous should I be to add reads
>>> on volatile field?
>>>
>>>
>>>
>>>
>>> Regards,
>>> Enno
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>      
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>    
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

