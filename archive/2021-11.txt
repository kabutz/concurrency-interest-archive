Subject: [concurrency-interest] Lock.lock("info")..
From: "Kedar C. Raybagkar via Concurrency-interest" <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 07:43
To: concurrency-interest@cs.oswego.edu
Reply-To: "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>

Hi,

Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

Regards,
-Kedar.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: David Holmes via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 07:57
To: "'Kedar C. Raybagkar'" <kedar.raybagkar@gmail.com>
CC: David Holmes <davidcholmes@aapt.net.au>, concurrency-interest@cs.oswego.edu
Reply-To: dholmes@ieee.org

Hi Kedar,

 

The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.

 

Cheers,

David

 

From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
Sent: Monday, 22 November 2021 3:43 PM
To: concurrency-interest@cs.oswego.edu
Subject: [concurrency-interest] Lock.lock("info")..

 

Hi,

 

Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

 

Regards,

-Kedar.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: "Kedar C. Raybagkar via Concurrency-interest" <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 08:22
To: dholmes@ieee.org
CC: concurrency-interest@cs.oswego.edu
Reply-To: "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>

Thank you, David.

That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.

Regards,
-Kedar.

On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:

    Hi Kedar,

     

    The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.

     

    Cheers,

    David

     

    From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
    Sent: Monday, 22 November 2021 3:43 PM
    To: concurrency-interest@cs.oswego.edu
    Subject: [concurrency-interest] Lock.lock("info")..

     

    Hi,

     

    Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

     

    Regards,

    -Kedar.


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: David Holmes via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 08:29
To: "'Kedar C. Raybagkar'" <kedar.raybagkar@gmail.com>
CC: David Holmes <davidcholmes@aapt.net.au>, concurrency-interest@cs.oswego.edu
Reply-To: dholmes@ieee.org

Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.

 

David

 

From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
Sent: Monday, 22 November 2021 4:23 PM
To: dholmes@ieee.org
Cc: concurrency-interest@cs.oswego.edu
Subject: Re: [concurrency-interest] Lock.lock("info")..

 

Thank you, David.

 

That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.

 

Regards,

-Kedar.

 

On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:

    Hi Kedar,

     

    The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.

     

    Cheers,

    David

     

    From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
    Sent: Monday, 22 November 2021 3:43 PM
    To: concurrency-interest@cs.oswego.edu
    Subject: [concurrency-interest] Lock.lock("info")..

     

    Hi,

     

    Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

     

    Regards,

    -Kedar.


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: David Holmes via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 11:20
To: "'Kedar C. Raybagkar'" <kedar.raybagkar@gmail.com>
CC: David Holmes <davidcholmes@aapt.net.au>, concurrency-interest@cs.oswego.edu
Reply-To: dholmes@ieee.org

Hi Kedar,

 

I added back the c-I list.

 

So IIUC you want to store information about the last operation with the lock, so that next time if you can’t get it (?) because teardown has not finished, you could query the lock to see what the previous operation was, and report that back. In other words your lock is the only common object you have for different messages in the same session so you want to use it to channel the data through. In such a situation I would expect you to map a session id to a Session object which includes the lock and any other data about the session you might want to track. If that is the “Holder” that you previously referred to then that is the best way to do this IMO.

 

It isn’t the job of a Lock to act as a data repository, so I would not consider adding the kind of API you have suggested.

 

Cheers,

David

 

From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
Sent: Monday, 22 November 2021 6:18 PM
To: dholmes@ieee.org
Subject: Re: [concurrency-interest] Lock.lock("info")..

 

Consider a simple servlet.

 

This servlet creates an AsyncFuture and waits for say 3 seconds max before sending the response out to the caller. If the response is generated within 3 seconds it is sent out, else we send a long running response (with reference to the future) to the caller. The caller if receives the response it displays it else if it finds a long running response then again calls the server by passing the reference id.

 

Now we have this future execute the job in two distinct ways. #1 waits till the entire operation is complete and #2 with no wait for the tear down process.

 

Teardown process serializes some data and it takes some time before it completes but the response is already generated and can be sent to the caller.

 

In case of #2 what we do is on the queue we push the response early on and then continue to do teardown. The caller waits on the queue and as soon as the response is received it returns that to the caller.

 

Usually the user takes time to send further requests to the server and in normal scenarios the tear down is finished by the time the next request comes. When the next request comes to the server we have to ensure that the prior requests teardown is complete. So we have used Lock to latch on and release it once teardown is complete. The latch is acquired as per the session (UUID reference) so that when the next request comes it gets the same latch and waits on the latch to be released.

 

As we need to keep all these Locks in a map based on the UUID (session identifier) we need to hold on to the encapsulated lock DTO that holds the purpose of what was being done before.

 

Hope I am able to explain it to your satisfaction.

 

Thanks & Regards,

 

 

On Mon, 22 Nov 2021 at 12:01, David Holmes <davidcholmes@aapt.net.au> wrote:

    Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.

     

    David

     

    From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
    Sent: Monday, 22 November 2021 4:23 PM
    To: dholmes@ieee.org
    Cc: concurrency-interest@cs.oswego.edu
    Subject: Re: [concurrency-interest] Lock.lock("info")..

     

    Thank you, David.

     

    That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.

     

    Regards,

    -Kedar.

     

    On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:

        Hi Kedar,

         

        The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.

         

        Cheers,

        David

         

        From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
        Sent: Monday, 22 November 2021 3:43 PM
        To: concurrency-interest@cs.oswego.edu
        Subject: [concurrency-interest] Lock.lock("info")..

         

        Hi,

         

        Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

         

        Regards,

        -Kedar.


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: "Kedar C. Raybagkar via Concurrency-interest" <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 11:30
To: dholmes@ieee.org
CC: concurrency-interest@cs.oswego.edu
Reply-To: "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>

Yes, that was the holder that I was referring to.

I understand that Lock is not a repo but a small String that captures the reason for acquiring the lock would have helped. I have it already addressed; so just wanted to check if there is a possibility to have that API so that we don't have to create similar holders that capture the reason as the reasons are dynamic in nature.

Do appreciate the follow ups! We can close this thread.

Thanks & Regards,
-Kedar.

On Mon, 22 Nov 2021 at 14:51, David Holmes <davidcholmes@aapt.net.au> wrote:

    Hi Kedar,

     

    I added back the c-I list.

     

    So IIUC you want to store information about the last operation with the lock, so that next time if you can’t get it (?) because teardown has not finished, you could query the lock to see what the previous operation was, and report that back. In other words your lock is the only common object you have for different messages in the same session so you want to use it to channel the data through. In such a situation I would expect you to map a session id to a Session object which includes the lock and any other data about the session you might want to track. If that is the “Holder” that you previously referred to then that is the best way to do this IMO.

     

    It isn’t the job of a Lock to act as a data repository, so I would not consider adding the kind of API you have suggested.

     

    Cheers,

    David

     

    From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
    Sent: Monday, 22 November 2021 6:18 PM
    To: dholmes@ieee.org
    Subject: Re: [concurrency-interest] Lock.lock("info")..

     

    Consider a simple servlet.

     

    This servlet creates an AsyncFuture and waits for say 3 seconds max before sending the response out to the caller. If the response is generated within 3 seconds it is sent out, else we send a long running response (with reference to the future) to the caller. The caller if receives the response it displays it else if it finds a long running response then again calls the server by passing the reference id.

     

    Now we have this future execute the job in two distinct ways. #1 waits till the entire operation is complete and #2 with no wait for the tear down process.

     

    Teardown process serializes some data and it takes some time before it completes but the response is already generated and can be sent to the caller.

     

    In case of #2 what we do is on the queue we push the response early on and then continue to do teardown. The caller waits on the queue and as soon as the response is received it returns that to the caller.

     

    Usually the user takes time to send further requests to the server and in normal scenarios the tear down is finished by the time the next request comes. When the next request comes to the server we have to ensure that the prior requests teardown is complete. So we have used Lock to latch on and release it once teardown is complete. The latch is acquired as per the session (UUID reference) so that when the next request comes it gets the same latch and waits on the latch to be released.

     

    As we need to keep all these Locks in a map based on the UUID (session identifier) we need to hold on to the encapsulated lock DTO that holds the purpose of what was being done before.

     

    Hope I am able to explain it to your satisfaction.

     

    Thanks & Regards,

     

     

    On Mon, 22 Nov 2021 at 12:01, David Holmes <davidcholmes@aapt.net.au> wrote:

        Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.

         

        David

         

        From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
        Sent: Monday, 22 November 2021 4:23 PM
        To: dholmes@ieee.org
        Cc: concurrency-interest@cs.oswego.edu
        Subject: Re: [concurrency-interest] Lock.lock("info")..

         

        Thank you, David.

         

        That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.

         

        Regards,

        -Kedar.

         

        On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:

            Hi Kedar,

             

            The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.

             

            Cheers,

            David

             

            From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
            Sent: Monday, 22 November 2021 3:43 PM
            To: concurrency-interest@cs.oswego.edu
            Subject: [concurrency-interest] Lock.lock("info")..

             

            Hi,

             

            Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?

             

            Regards,

            -Kedar.


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 18:22
To: "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>
CC: concurrency-interest@cs.oswego.edu, dholmes@ieee.org
Reply-To: Gregg Wonderly <gergg@cox.net>

For me, there is a whole list of “Reporting” based interfaces that I typically use.  These names are an example, but they help me separate implementations out so that logging vs console and historical data repos etc, can all be inserted into my “Reporting”.

public interface Report {
String getDetails();
String getLocation();
bool hasLocation();
}	

public interface Reportable {
Report getReport();
}

public interface Reporter {
String getName();
void report( Reportable obj );
}

public class ReportingManager {
static ReportingManager getDefault();
static Iterator<Reporter> getAll();
static Reporter getReporter( String which );
static void add( Reporter reporter );
static void remove( Reporter reporter );
static void removeAll();
static void report( Reportable rep );
static void report( Report rep );
static void reportTo( String rptr, Reportable rep );
static void reportTo( String rptr, Report rep );
}

With these kinds of interfaces, logging and all kinds of details get abstracted into something that is pluggable and extendable.  Your lock holder can be a Reportable for example, so that it can be any object your need to report something about.  This kind of API might be interesting to the OpenJDK, but is just not something that is explicitly about concurrency as Doug indicates.

Gregg Wonderly

> On Nov 22, 2021, at 3:30 AM, Kedar C. Raybagkar via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
> Yes, that was the holder that I was referring to.
>
> I understand that Lock is not a repo but a small String that captures the reason for acquiring the lock would have helped. I have it already addressed; so just wanted to check if there is a possibility to have that API so that we don't have to create similar holders that capture the reason as the reasons are dynamic in nature.
>
> Do appreciate the follow ups! We can close this thread.
>
> Thanks & Regards,
> -Kedar.
>
> On Mon, 22 Nov 2021 at 14:51, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>     Hi Kedar,
>
>      
>
>     I added back the c-I list.
>
>      
>
>     So IIUC you want to store information about the last operation with the lock, so that next time if you can’t get it (?) because teardown has not finished, you could query the lock to see what the previous operation was, and report that back. In other words your lock is the only common object you have for different messages in the same session so you want to use it to channel the data through. In such a situation I would expect you to map a session id to a Session object which includes the lock and any other data about the session you might want to track. If that is the “Holder” that you previously referred to then that is the best way to do this IMO.
>
>      
>
>     It isn’t the job of a Lock to act as a data repository, so I would not consider adding the kind of API you have suggested.
>
>      
>
>     Cheers,
>
>     David
>
>      
>
>     From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>     Sent: Monday, 22 November 2021 6:18 PM
>     To: dholmes@ieee.org
>     Subject: Re: [concurrency-interest] Lock.lock("info")..
>
>      
>
>     Consider a simple servlet.
>
>      
>
>     This servlet creates an AsyncFuture and waits for say 3 seconds max before sending the response out to the caller. If the response is generated within 3 seconds it is sent out, else we send a long running response (with reference to the future) to the caller. The caller if receives the response it displays it else if it finds a long running response then again calls the server by passing the reference id.
>
>      
>
>     Now we have this future execute the job in two distinct ways. #1 waits till the entire operation is complete and #2 with no wait for the tear down process.
>
>      
>
>     Teardown process serializes some data and it takes some time before it completes but the response is already generated and can be sent to the caller.
>
>      
>
>     In case of #2 what we do is on the queue we push the response early on and then continue to do teardown. The caller waits on the queue and as soon as the response is received it returns that to the caller.
>
>      
>
>     Usually the user takes time to send further requests to the server and in normal scenarios the tear down is finished by the time the next request comes. When the next request comes to the server we have to ensure that the prior requests teardown is complete. So we have used Lock to latch on and release it once teardown is complete. The latch is acquired as per the session (UUID reference) so that when the next request comes it gets the same latch and waits on the latch to be released.
>
>      
>
>     As we need to keep all these Locks in a map based on the UUID (session identifier) we need to hold on to the encapsulated lock DTO that holds the purpose of what was being done before.
>
>      
>
>     Hope I am able to explain it to your satisfaction.
>
>      
>
>     Thanks & Regards,
>
>      
>
>      
>
>     On Mon, 22 Nov 2021 at 12:01, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>         Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.
>
>          
>
>         David
>
>          
>
>         From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>         Sent: Monday, 22 November 2021 4:23 PM
>         To: dholmes@ieee.org
>         Cc: concurrency-interest@cs.oswego.edu
>         Subject: Re: [concurrency-interest] Lock.lock("info")..
>
>          
>
>         Thank you, David.
>
>          
>
>         That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.
>
>          
>
>         Regards,
>
>         -Kedar.
>
>          
>
>         On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>             Hi Kedar,
>
>              
>
>             The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.
>
>              
>
>             Cheers,
>
>             David
>
>              
>
>             From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
>             Sent: Monday, 22 November 2021 3:43 PM
>             To: concurrency-interest@cs.oswego.edu
>             Subject: [concurrency-interest] Lock.lock("info")..
>
>              
>
>             Hi,
>
>              
>
>             Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?
>
>              
>
>             Regards,
>
>             -Kedar.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: Nathan Reynolds via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 22:06
To: Gregg Wonderly <gergg@cox.net>
CC: concurrency-interest <concurrency-interest@cs.oswego.edu>, "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>, dholmes@ieee.org
Reply-To: Nathan Reynolds <numeralnathan@gmail.com>

Why not create a logger that holds some number of debug log messages in memory for each thread until a warning or error is logged?  When a warning or error is logged, then the logger writes all the messages regardless of the log level.  Hence, the code does debug logging calls and the logger takes care of the rest.

In the case of locks, change to a try lock with a timeout.  When the timeout happens, log an error.  The logger does the rest.  I find this to be very helpful since the debug log messages can be captured from ancestor and preceding sibling methods.  This way I get much more contextual information than what can be captured right before attempting to acquire the lock.

On Mon, Nov 22, 2021 at 9:23 AM Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:

    For me, there is a whole list of “Reporting” based interfaces that I typically use.  These names are an example, but they help me separate implementations out so that logging vs console and historical data repos etc, can all be inserted into my “Reporting”.

    public interface Report {
    String getDetails();
    String getLocation();
    bool hasLocation();
    }	

    public interface Reportable {
    Report getReport();
    }

    public interface Reporter {
    String getName();
    void report( Reportable obj );
    }

    public class ReportingManager {
    static ReportingManager getDefault();
    static Iterator<Reporter> getAll();
    static Reporter getReporter( String which );
    static void add( Reporter reporter );
    static void remove( Reporter reporter );
    static void removeAll();
    static void report( Reportable rep );
    static void report( Report rep );
    static void reportTo( String rptr, Reportable rep );
    static void reportTo( String rptr, Report rep );
    }

    With these kinds of interfaces, logging and all kinds of details get abstracted into something that is pluggable and extendable.  Your lock holder can be a Reportable for example, so that it can be any object your need to report something about.  This kind of API might be interesting to the OpenJDK, but is just not something that is explicitly about concurrency as Doug indicates.

    Gregg Wonderly

>     On Nov 22, 2021, at 3:30 AM, Kedar C. Raybagkar via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>     Yes, that was the holder that I was referring to.
>
>     I understand that Lock is not a repo but a small String that captures the reason for acquiring the lock would have helped. I have it already addressed; so just wanted to check if there is a possibility to have that API so that we don't have to create similar holders that capture the reason as the reasons are dynamic in nature.
>
>     Do appreciate the follow ups! We can close this thread.
>
>     Thanks & Regards,
>     -Kedar.
>
>     On Mon, 22 Nov 2021 at 14:51, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>         Hi Kedar,
>
>          
>
>         I added back the c-I list.
>
>          
>
>         So IIUC you want to store information about the last operation with the lock, so that next time if you can’t get it (?) because teardown has not finished, you could query the lock to see what the previous operation was, and report that back. In other words your lock is the only common object you have for different messages in the same session so you want to use it to channel the data through. In such a situation I would expect you to map a session id to a Session object which includes the lock and any other data about the session you might want to track. If that is the “Holder” that you previously referred to then that is the best way to do this IMO.
>
>          
>
>         It isn’t the job of a Lock to act as a data repository, so I would not consider adding the kind of API you have suggested.
>
>          
>
>         Cheers,
>
>         David
>
>          
>
>         From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>         Sent: Monday, 22 November 2021 6:18 PM
>         To: dholmes@ieee.org
>         Subject: Re: [concurrency-interest] Lock.lock("info")..
>
>          
>
>         Consider a simple servlet.
>
>          
>
>         This servlet creates an AsyncFuture and waits for say 3 seconds max before sending the response out to the caller. If the response is generated within 3 seconds it is sent out, else we send a long running response (with reference to the future) to the caller. The caller if receives the response it displays it else if it finds a long running response then again calls the server by passing the reference id.
>
>          
>
>         Now we have this future execute the job in two distinct ways. #1 waits till the entire operation is complete and #2 with no wait for the tear down process.
>
>          
>
>         Teardown process serializes some data and it takes some time before it completes but the response is already generated and can be sent to the caller.
>
>          
>
>         In case of #2 what we do is on the queue we push the response early on and then continue to do teardown. The caller waits on the queue and as soon as the response is received it returns that to the caller.
>
>          
>
>         Usually the user takes time to send further requests to the server and in normal scenarios the tear down is finished by the time the next request comes. When the next request comes to the server we have to ensure that the prior requests teardown is complete. So we have used Lock to latch on and release it once teardown is complete. The latch is acquired as per the session (UUID reference) so that when the next request comes it gets the same latch and waits on the latch to be released.
>
>          
>
>         As we need to keep all these Locks in a map based on the UUID (session identifier) we need to hold on to the encapsulated lock DTO that holds the purpose of what was being done before.
>
>          
>
>         Hope I am able to explain it to your satisfaction.
>
>          
>
>         Thanks & Regards,
>
>          
>
>          
>
>         On Mon, 22 Nov 2021 at 12:01, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>             Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.
>
>              
>
>             David
>
>              
>
>             From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>             Sent: Monday, 22 November 2021 4:23 PM
>             To: dholmes@ieee.org
>             Cc: concurrency-interest@cs.oswego.edu
>             Subject: Re: [concurrency-interest] Lock.lock("info")..
>
>              
>
>             Thank you, David.
>
>              
>
>             That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.
>
>              
>
>             Regards,
>
>             -Kedar.
>
>              
>
>             On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:
>
>                 Hi Kedar,
>
>                  
>
>                 The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.
>
>                  
>
>                 Cheers,
>
>                 David
>
>                  
>
>                 From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
>                 Sent: Monday, 22 November 2021 3:43 PM
>                 To: concurrency-interest@cs.oswego.edu
>                 Subject: [concurrency-interest] Lock.lock("info")..
>
>                  
>
>                 Hi,
>
>                  
>
>                 Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?
>
>                  
>
>                 Regards,
>
>                 -Kedar.
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest@cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: [concurrency-interest] ThreadLocal performance degradation
From: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-22, 23:27
To: "concurrency-interest@cs.oswego.edu" <concurrency-interest@cs.oswego.edu>
Reply-To: Margaret Figura <margaret.figura@infovista.com>

Hi all,

This has been discussed before in this thread: https://cs.oswego.edu/pipermail/concurrency-interest/2018-October/016628.html
...and I think Gil Tene's comment about collisions exacerbating the issue is especially helpful: https://cs.oswego.edu/pipermail/concurrency-interest/2018-October/016685.html

What happens is that the ThreadLocalMap can degenerate to a linear scan when there are many collisions, and performance drops significantly on affected Threads in all code using any ThreadLocal instance (even code unrelated to the 'problem').

Is there interest in fixing this? Within the same discussion, Peter Levart had asked if there was a reproducer. I've managed to create one that works quickly on at least my two test systems. Does that help?
Reproducer and sample output: https://gist.github.com/megfigura/67d6972aa19fa1a4a1e13b4c7e7380fb

Watching as it runs, what's most concerning is how it suddenly blows up. Monitoring for the longest run of non-nulls in the ThreadLocalMap, the number will be small, small, small, but then it suddenly explodes - 6...23...48...29863... ...257207. Runs of non-nulls mean that long linear-scans will happen with some of the ThreadLocal API calls.

I ran into this issue indirectly by creating very many instances of the otherwise wonderful ChronicleMap which has a per-instance ThreadLocal. I captured a heapdump of our process in the bad state, and it showed a continuous run of non-null entries in the ThreadLocalMap of about 8000 items and another of about 4000. The backing array of the map is 64k, so there is a good chance a new instance will end up within one of these large blocks, doing a linear-scan. I could see the same behavior on different worker threads in the same process. The problem will reproduce again in new instances of the process within a few days in a specific customer environment. Once in the bad state, we saw an unrelated area which was using ReentrantReadWriteLock (which also uses ThreadLocal) show up as the hottest methods in the profiler.

In my case, the solution is to reduce the rate we are creating ChronicleMap/ThreadLocal instances, but a workaround is to trigger a GC via JMX periodically or to get the internal ThreadLocal via reflection and schedule it to be .remove()'d on all threads that accessed it.

Thanks a lot!
Meg
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Lock.lock("info")..
From: Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-23, 01:04
To: Nathan Reynolds <numeralnathan@gmail.com>
CC: concurrency-interest <concurrency-interest@cs.oswego.edu>, "Kedar C. Raybagkar" <kedar.raybagkar@gmail.com>, dholmes@ieee.org
Reply-To: Gregg Wonderly <gergg@cox.net>

You can use a Logger behind Reporter for sure.  The detail is that I also want to have a remote Handler that I can tap into and watch what’s happening and manipulate the levels of the loggers from that interface.  But, java.util.logging’s Level in Logger instances is not associated with the tie into the handler.  What I typically need, is for a Logger’s level to actually be a guard that the Handler level checks.  Thus instead of all Handler’s being filtered by the Logger level, I want each Handler to have a level for each Logger so that I can filter Loggers by Handler, instead of just at the Logger level.  Yes, you can use the Handler level to filter things toward that logger, but that doesn’t allow individual remote management instances to turn on and off logging levels that they need for their specific observations in a larger domain of Logger use.

Gregg Wonderly

> On Nov 22, 2021, at 2:06 PM, Nathan Reynolds <numeralnathan@gmail.com> wrote:
>
> Why not create a logger that holds some number of debug log messages in memory for each thread until a warning or error is logged?  When a warning or error is logged, then the logger writes all the messages regardless of the log level.  Hence, the code does debug logging calls and the logger takes care of the rest.
>
> In the case of locks, change to a try lock with a timeout.  When the timeout happens, log an error.  The logger does the rest.  I find this to be very helpful since the debug log messages can be captured from ancestor and preceding sibling methods.  This way I get much more contextual information than what can be captured right before attempting to acquire the lock.
>
> On Mon, Nov 22, 2021 at 9:23 AM Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>     For me, there is a whole list of “Reporting” based interfaces that I typically use.  These names are an example, but they help me separate implementations out so that logging vs console and historical data repos etc, can all be inserted into my “Reporting”.
>
>     public interface Report {
>     String getDetails();
>     String getLocation();
>     bool hasLocation();
>     }	
>
>     public interface Reportable {
>     Report getReport();
>     }
>
>     public interface Reporter {
>     String getName();
>     void report( Reportable obj );
>     }
>
>     public class ReportingManager {
>     static ReportingManager getDefault();
>     static Iterator<Reporter> getAll();
>     static Reporter getReporter( String which );
>     static void add( Reporter reporter );
>     static void remove( Reporter reporter );
>     static void removeAll();
>     static void report( Reportable rep );
>     static void report( Report rep );
>     static void reportTo( String rptr, Reportable rep );
>     static void reportTo( String rptr, Report rep );
>     }
>
>     With these kinds of interfaces, logging and all kinds of details get abstracted into something that is pluggable and extendable.  Your lock holder can be a Reportable for example, so that it can be any object your need to report something about.  This kind of API might be interesting to the OpenJDK, but is just not something that is explicitly about concurrency as Doug indicates.
>
>     Gregg Wonderly
>
>>     On Nov 22, 2021, at 3:30 AM, Kedar C. Raybagkar via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>>
>>     Yes, that was the holder that I was referring to.
>>
>>     I understand that Lock is not a repo but a small String that captures the reason for acquiring the lock would have helped. I have it already addressed; so just wanted to check if there is a possibility to have that API so that we don't have to create similar holders that capture the reason as the reasons are dynamic in nature.
>>
>>     Do appreciate the follow ups! We can close this thread.
>>
>>     Thanks & Regards,
>>     -Kedar.
>>
>>     On Mon, 22 Nov 2021 at 14:51, David Holmes <davidcholmes@aapt.net.au> wrote:
>>
>>         Hi Kedar,
>>
>>          
>>
>>         I added back the c-I list.
>>
>>          
>>
>>         So IIUC you want to store information about the last operation with the lock, so that next time if you can’t get it (?) because teardown has not finished, you could query the lock to see what the previous operation was, and report that back. In other words your lock is the only common object you have for different messages in the same session so you want to use it to channel the data through. In such a situation I would expect you to map a session id to a Session object which includes the lock and any other data about the session you might want to track. If that is the “Holder” that you previously referred to then that is the best way to do this IMO.
>>
>>          
>>
>>         It isn’t the job of a Lock to act as a data repository, so I would not consider adding the kind of API you have suggested.
>>
>>          
>>
>>         Cheers,
>>
>>         David
>>
>>          
>>
>>         From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>>         Sent: Monday, 22 November 2021 6:18 PM
>>         To: dholmes@ieee.org
>>         Subject: Re: [concurrency-interest] Lock.lock("info")..
>>
>>          
>>
>>         Consider a simple servlet.
>>
>>          
>>
>>         This servlet creates an AsyncFuture and waits for say 3 seconds max before sending the response out to the caller. If the response is generated within 3 seconds it is sent out, else we send a long running response (with reference to the future) to the caller. The caller if receives the response it displays it else if it finds a long running response then again calls the server by passing the reference id.
>>
>>          
>>
>>         Now we have this future execute the job in two distinct ways. #1 waits till the entire operation is complete and #2 with no wait for the tear down process.
>>
>>          
>>
>>         Teardown process serializes some data and it takes some time before it completes but the response is already generated and can be sent to the caller.
>>
>>          
>>
>>         In case of #2 what we do is on the queue we push the response early on and then continue to do teardown. The caller waits on the queue and as soon as the response is received it returns that to the caller.
>>
>>          
>>
>>         Usually the user takes time to send further requests to the server and in normal scenarios the tear down is finished by the time the next request comes. When the next request comes to the server we have to ensure that the prior requests teardown is complete. So we have used Lock to latch on and release it once teardown is complete. The latch is acquired as per the session (UUID reference) so that when the next request comes it gets the same latch and waits on the latch to be released.
>>
>>          
>>
>>         As we need to keep all these Locks in a map based on the UUID (session identifier) we need to hold on to the encapsulated lock DTO that holds the purpose of what was being done before.
>>
>>          
>>
>>         Hope I am able to explain it to your satisfaction.
>>
>>          
>>
>>         Thanks & Regards,
>>
>>          
>>
>>          
>>
>>         On Mon, 22 Nov 2021 at 12:01, David Holmes <davidcholmes@aapt.net.au> wrote:
>>
>>             Why do you need a Holder object for the lock and condition? The lock and condition belong in the object whose state they are guarding and interacting with. If you are using external synchronization you wont have an encapsulated lock/condition but the code using the lock/condition has to know what it is actually dealing with and so that code can report that information.
>>
>>              
>>
>>             David
>>
>>              
>>
>>             From: Kedar C. Raybagkar <kedar.raybagkar@gmail.com>
>>             Sent: Monday, 22 November 2021 4:23 PM
>>             To: dholmes@ieee.org
>>             Cc: concurrency-interest@cs.oswego.edu
>>             Subject: Re: [concurrency-interest] Lock.lock("info")..
>>
>>              
>>
>>             Thank you, David.
>>
>>              
>>
>>             That is what I have been doing but instead of having to encapsulate into a separate holder object I thought it might be a good idea to have some API directly available. As encapsulation also results in having a Value Object with hashcode and equals being overridden and then keeping them in a map with thread ids. This is already within the lock; just an additional string or object parameter may help solve the problem.
>>
>>              
>>
>>             Regards,
>>
>>             -Kedar.
>>
>>              
>>
>>             On Mon, 22 Nov 2021 at 11:28, David Holmes <davidcholmes@aapt.net.au> wrote:
>>
>>                 Hi Kedar,
>>
>>                  
>>
>>                 The Lock and Condition should be encapsulated inside the object that needs them, and that object can provide whatever additional information the callers needs on its own methods.
>>
>>                  
>>
>>                 Cheers,
>>
>>                 David
>>
>>                  
>>
>>                 From: Concurrency-interest <concurrency-interest-bounces@cs.oswego.edu> On Behalf Of Kedar C. Raybagkar via Concurrency-interest
>>                 Sent: Monday, 22 November 2021 3:43 PM
>>                 To: concurrency-interest@cs.oswego.edu
>>                 Subject: [concurrency-interest] Lock.lock("info")..
>>
>>                  
>>
>>                 Hi,
>>
>>                  
>>
>>                 Can we have something API where we can provide why we have acquired a lock so that when waiting on a condition if timed out we can fetch the info and return it to the caller?
>>
>>                  
>>
>>                 Regards,
>>
>>                 -Kedar.
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest@cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest@cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] ThreadLocal performance degradation
From: Andrew Haley via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-23, 12:19
To: concurrency-interest@cs.oswego.edu
Reply-To: Andrew Haley <aph@redhat.com>

On 11/22/21 21:27, Margaret Figura via Concurrency-interest wrote:
> Is there interest in fixing this?

This is, arguably, abuse of ThreadLocals, and the failure to remove
the ThreadLocal is a bug.

Having said that, some thing like a Bagwell Ideal Hash Tree would perform
better in such degenerate cases. It'd be interesting to try, and see if
it makes more reasonable code any worse.

Note that Gil's comment

> We’ve created a tweaked implementation of ThreadLocal that avoids
> the weakref get()-strengthening problem

is now out of date, since

8256167: Convert JDK use of `Reference::get` to `Reference::refersTo`

-- 
Andrew Haley  (he/him)
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
https://keybase.io/andrewhaley
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Doug Lea via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-24, 22:36
To: concurrency-interest@cs.oswego.edu
Reply-To: Doug Lea <dl@cs.oswego.edu>


On 11/24/21 2:15 PM, Dr Heinz M. Kabutz via Concurrency-interest wrote:
> Every time I see the example in RecursiveTask I have to cringe:

The initial rationale was that "nearly everyone" knows Fibonacci so it doesn't need much explanation it. but you are right that even more people know factorial, and the BigInteger version fits RecursiveTask without needing caveats, so we should use it; thanks. Here's a javadoc'd version of your example. Any objections to using it?


 *
 * <pre> {@code
 * public class FactorialTask extends RecursiveTask<BigInteger> {
 *   private final int from, to;
 *   public FactorialTask(int n) { this(0, n); }
 *   private FactorialTask(int from, int to) { this.from = from; this.to = to; }
 *   protected BigInteger compute() {
 *     if (from == to)                        // base case
 *       return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from);
 *     int mid = (from + to) >>> 1;           // split in half
 *     FactorialTask leftTask = (new FactorialTask(from, mid)).fork();
 *     FactorialTask rightTask = new FactorialTask(mid + 1, to);
 *     BigInteger right = rightTask.invoke(); // perform half the work locally
 *     BigInteger left = leftTask.join();
 *     return left.multiply(right);
 *   }
 * }}</pre>
 *

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Remi Forax via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-24, 23:11
To: Doug Lea <dl@cs.oswego.edu>
CC: concurrency-interest <concurrency-interest@cs.oswego.edu>
Reply-To: Remi Forax <forax@univ-mlv.fr>



----- Original Message -----
> > From: "concurrency-interest" <concurrency-interest@cs.oswego.edu>
> > To: "concurrency-interest" <concurrency-interest@cs.oswego.edu>
> > Sent: Mercredi 24 Novembre 2021 21:36:36
> > Subject: Re: [concurrency-interest] Better RecursiveTask Example

> > On 11/24/21 2:15 PM, Dr Heinz M. Kabutz via Concurrency-interest wrote:
>> >> Every time I see the example in RecursiveTask I have to cringe:
> > 
> > The initial rationale was that "nearly everyone" knows Fibonacci so it
> > doesn't need much explanation it. but you are right that even more
> > people know factorial, and the BigInteger version fits RecursiveTask
> > without needing caveats, so we should use it; thanks. Here's a javadoc'd
> > version of your example. Any objections to using it?
> > 
> > 
> >  *
> >  * <pre> {@code
> >  * public class FactorialTask extends RecursiveTask<BigInteger> {
> >  *   private final int from, to;
> >  *   public FactorialTask(int n) { this(0, n); }
> >  *   private FactorialTask(int from, int to) { this.from = from;
> > this.to = to; }
> >  *   protected BigInteger compute() {
> >  *     if (from == to)                        // base case
> >  *       return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from);
> >  *     int mid = (from + to) >>> 1;           // split in half
> >  *     FactorialTask leftTask = (new FactorialTask(from, mid)).fork();
> >  *     FactorialTask rightTask = new FactorialTask(mid + 1, to);
> >  *     BigInteger right = rightTask.invoke(); // perform half the work
> > locally
> >  *     BigInteger left = leftTask.join();
> >  *     return left.multiply(right);
> >  *   }
> >  * }}</pre>
> >  *

Same version, with classical formatting applied

 *
 * <pre> {@code
 * public class FactorialTask extends RecursiveTask<BigInteger> {
 *   private final int from;
 *   private final int to;
 *
 *   public FactorialTask(int n) {
 *     this(0, n);
 *   }
 *
 *   private FactorialTask(int from, int to) {
 *     this.from = from;
 *     this.to = to;
 *   }
 *
 *   protected BigInteger compute() {
 *     if (from == to) {                      // base case
 *       return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from);
 *     }
 *     int mid = (from + to) >>> 1;           // split in half
 *     FactorialTask leftTask = new FactorialTask(from, mid).fork();
 *     FactorialTask rightTask = new FactorialTask(mid + 1, to);
 *     BigInteger right = rightTask.invoke(); // perform half the work locally
 *     BigInteger left = leftTask.join();
 *     return left.multiply(right);
 *   }
 * }</pre>
 *

We can also use 'var', every declarations are aligned but it is perhaps harder to understand

 *   protected BigInteger compute() {
 *     if (from == to) {               // base case
 *       return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from);
 *     }
 *     var mid = (from + to) >>> 1;    // split in half
 *     var leftTask = new FactorialTask(from, mid).fork();
 *     var rightTask = new FactorialTask(mid + 1, to);
 *     var right = rightTask.invoke(); // perform half the work locally
 *     var left = leftTask.join();
 *     return left.multiply(right);
 *   }


Rémi

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Tim Peierls via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-25, 00:07
To: Remi Forax <forax@univ-mlv.fr>
CC: Doug Lea <dl@cs.oswego.edu>, concurrency-interest <concurrency-interest@cs.oswego.edu>
Reply-To: Tim Peierls <tim@peierls.net>

On Wed, Nov 24, 2021 at 4:14 PM Remi Forax via Concurrency-interest concurrency-interest@cs.oswego.edu wrote:

    We can also use ‘var’, every declarations are aligned but it is perhaps harder to understand

* protected BigInteger compute() { * if (from == to) { // base case * return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from); * } * var mid = (from + to) >>> 1; // split in half * var leftTask = new FactorialTask(from, mid).fork(); * var rightTask = new FactorialTask(mid + 1, to); * var right = rightTask.invoke(); // perform half the work locally * var left = leftTask.join(); * return left.multiply(right); * }

It is definitely harder to understand. My brain is wired to look on the left for types. Lines that are all lower-case initial before an = don’t look like sources of type information to me.

—tim


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Nathan Reynolds via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-25, 00:28
To: Tim Peierls <tim@peierls.net>
CC: Doug Lea <dl@cs.oswego.edu>, concurrency-interest <concurrency-interest@cs.oswego.edu>
Reply-To: Nathan Reynolds <numeralnathan@gmail.com>

We are looking for clarity.  Too many examples online don't include package or variable types and it makes it very hard to understand how to adapt the code to my needs.

On Wed, Nov 24, 2021, 3:09 PM Tim Peierls via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:

    On Wed, Nov 24, 2021 at 4:14 PM Remi Forax via Concurrency-interest concurrency-interest@cs.oswego.edu wrote:

        We can also use ‘var’, every declarations are aligned but it is perhaps harder to understand

    * protected BigInteger compute() { * if (from == to) { // base case * return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from); * } * var mid = (from + to) >>> 1; // split in half * var leftTask = new FactorialTask(from, mid).fork(); * var rightTask = new FactorialTask(mid + 1, to); * var right = rightTask.invoke(); // perform half the work locally * var left = leftTask.join(); * return left.multiply(right); * }

    It is definitely harder to understand. My brain is wired to look on the left for types. Lines that are all lower-case initial before an = don’t look like sources of type information to me.

    —tim

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 01:30
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)

https://bit.ly/3oVFeTD


Alex

On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:

    Every time I see the example in RecursiveTask I have to cringe:

    https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html

    For a classic example, here is a task computing Fibonacci numbers:


      class Fibonacci extends RecursiveTask<Integer> {
        final int n;
        Fibonacci(int n) { this.n = n; }
        protected Integer compute() {
          if (n <= 1)
            return n;
          Fibonacci f1 = new Fibonacci(n - 1);
          f1.fork();
          Fibonacci f2 = new Fibonacci(n - 2);
          return f2.compute() + f1.join();
        }
      }
    However, besides being a dumb way to compute Fibonacci functions (there
    is a simple fast linear algorithm that you'd use in practice), this is
    likely to perform poorly because the smallest subtasks are too small to
    be worthwhile splitting up. Instead, as is the case for nearly all
    fork/join applications, you'd pick some minimum granularity size (for
    example 10 here) for which you always sequentially solve rather than
    subdividing.



    Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
    algorithm isn't fast either. Since we overflow even Long after about
    fibonacci(90), we would need BigInteger. And there the add is linear,
    meaning that the "fast linear" algorithm referred to here is probably
    going to end up as "slow quadratic".

    To me, this example sends the completely wrong message. Let's take the
    worst possible algorithm and parallelize it. Great. That means if we use
    1000 processors, we can solve the problem of n+10 in the same time as n
    with a single processor.

    I do realize this is meant to illustrate a point, but it doesn't do it
    very well IME. I would like to propose to change this to a slightly
    better example, for example a Factorial calculation:

    public class FactorialTask extends RecursiveTask<BigInteger> {
         private final int from, to;

         public FactorialTask(int n) {
             this(0, n);
         }

         private FactorialTask(int from, int to) {
             this.from = from;
             this.to = to;
         }

         protected BigInteger compute() {
             if (from == to) {
                 if (from == 0) return BigInteger.ONE;
                 return BigInteger.valueOf(from);
             }
             int mid = (from + to) >>> 1;
             FactorialTask leftTask = new FactorialTask(from, mid);
             FactorialTask rightTask = new FactorialTask(mid + 1, to);
             leftTask.fork();
             BigInteger right = rightTask.invoke();
             BigInteger left = leftTask.join();
             return left.multiply(right);
         }
    }

    This is actually a *lot* faster than the stream version:

         public static BigInteger factorialStream(int n) {
             return IntStream.rangeClosed(1, n)
                     .mapToObj(BigInteger::valueOf)
                     .reduce(BigInteger.ONE, BigInteger::multiply);
         }

    (this has to do more with the algorithms used by BigInteger's multiply
    method than the parallelization, but that also has an effect.


    Alternatively, if we have to have Fibonacci, could we at least change it
    to Dijkstra's Sum of Squares? I believe there are slightly better
    algorithms, but this one works very nicely with parallelisation:

    public class FibonacciTask extends RecursiveTask<BigInteger> {
         private final int n;

         public FibonacciTask(int n) {
             this.n = n;
         }

         @Override
         protected BigInteger compute() {
             return switch (n) {
                 case 0 -> BigInteger.ZERO;
                 case 1 -> BigInteger.ONE;
                 default -> {
                     // Dijkstra's Sum of Squares Algorithm
                     int half = (n + 1) / 2;
                     FibonacciTask f0_task = new FibonacciTask(half - 1);
                     f0_task.fork();
                     FibonacciTask f1_task = new FibonacciTask(half);
                     BigInteger f1 = f1_task.invoke();
                     BigInteger f0 = f0_task.join();

                     if (n % 2 == 1) {
                         yield f0.multiply(f0).add(f1.multiply(f1));
                     } else {
                         yield f0.shiftLeft(1).add(f1).multiply(f1);
                     }
                 }
             };
         }
    }

    Please let me know if you agree with this change (or propose a different
    example). I would be happy to make the change. I presume it would need
    to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
    and then we can sync that over to CVS? (My preference would be GitHub)




    Regards

    Heinz
    -- 
    Dr Heinz M. Kabutz (PhD CompSci)
    Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
    Java Champion - www.javachampions.org
    JavaOne Rock Star Speaker
    Tel: +30 69 75 595 262
    Skype: kabutz

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 02:17
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

https://bit.ly/3HVgEey - but maybe this is less silly, as we can actually make subcomputations wait for the previous task to complete.

Alex

On Wed, 24 Nov 2021, 23:30 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:

    I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)

    https://bit.ly/3oVFeTD


    Alex

    On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:

        Every time I see the example in RecursiveTask I have to cringe:

        https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html

        For a classic example, here is a task computing Fibonacci numbers:


          class Fibonacci extends RecursiveTask<Integer> {
            final int n;
            Fibonacci(int n) { this.n = n; }
            protected Integer compute() {
              if (n <= 1)
                return n;
              Fibonacci f1 = new Fibonacci(n - 1);
              f1.fork();
              Fibonacci f2 = new Fibonacci(n - 2);
              return f2.compute() + f1.join();
            }
          }
        However, besides being a dumb way to compute Fibonacci functions (there
        is a simple fast linear algorithm that you'd use in practice), this is
        likely to perform poorly because the smallest subtasks are too small to
        be worthwhile splitting up. Instead, as is the case for nearly all
        fork/join applications, you'd pick some minimum granularity size (for
        example 10 here) for which you always sequentially solve rather than
        subdividing.



        Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
        algorithm isn't fast either. Since we overflow even Long after about
        fibonacci(90), we would need BigInteger. And there the add is linear,
        meaning that the "fast linear" algorithm referred to here is probably
        going to end up as "slow quadratic".

        To me, this example sends the completely wrong message. Let's take the
        worst possible algorithm and parallelize it. Great. That means if we use
        1000 processors, we can solve the problem of n+10 in the same time as n
        with a single processor.

        I do realize this is meant to illustrate a point, but it doesn't do it
        very well IME. I would like to propose to change this to a slightly
        better example, for example a Factorial calculation:

        public class FactorialTask extends RecursiveTask<BigInteger> {
             private final int from, to;

             public FactorialTask(int n) {
                 this(0, n);
             }

             private FactorialTask(int from, int to) {
                 this.from = from;
                 this.to = to;
             }

             protected BigInteger compute() {
                 if (from == to) {
                     if (from == 0) return BigInteger.ONE;
                     return BigInteger.valueOf(from);
                 }
                 int mid = (from + to) >>> 1;
                 FactorialTask leftTask = new FactorialTask(from, mid);
                 FactorialTask rightTask = new FactorialTask(mid + 1, to);
                 leftTask.fork();
                 BigInteger right = rightTask.invoke();
                 BigInteger left = leftTask.join();
                 return left.multiply(right);
             }
        }

        This is actually a *lot* faster than the stream version:

             public static BigInteger factorialStream(int n) {
                 return IntStream.rangeClosed(1, n)
                         .mapToObj(BigInteger::valueOf)
                         .reduce(BigInteger.ONE, BigInteger::multiply);
             }

        (this has to do more with the algorithms used by BigInteger's multiply
        method than the parallelization, but that also has an effect.


        Alternatively, if we have to have Fibonacci, could we at least change it
        to Dijkstra's Sum of Squares? I believe there are slightly better
        algorithms, but this one works very nicely with parallelisation:

        public class FibonacciTask extends RecursiveTask<BigInteger> {
             private final int n;

             public FibonacciTask(int n) {
                 this.n = n;
             }

             @Override
             protected BigInteger compute() {
                 return switch (n) {
                     case 0 -> BigInteger.ZERO;
                     case 1 -> BigInteger.ONE;
                     default -> {
                         // Dijkstra's Sum of Squares Algorithm
                         int half = (n + 1) / 2;
                         FibonacciTask f0_task = new FibonacciTask(half - 1);
                         f0_task.fork();
                         FibonacciTask f1_task = new FibonacciTask(half);
                         BigInteger f1 = f1_task.invoke();
                         BigInteger f0 = f0_task.join();

                         if (n % 2 == 1) {
                             yield f0.multiply(f0).add(f1.multiply(f1));
                         } else {
                             yield f0.shiftLeft(1).add(f1).multiply(f1);
                         }
                     }
                 };
             }
        }

        Please let me know if you agree with this change (or propose a different
        example). I would be happy to make the change. I presume it would need
        to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
        and then we can sync that over to CVS? (My preference would be GitHub)




        Regards

        Heinz
        -- 
        Dr Heinz M. Kabutz (PhD CompSci)
        Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
        Java Champion - www.javachampions.org
        JavaOne Rock Star Speaker
        Tel: +30 69 75 595 262
        Skype: kabutz

        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest@cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] ThreadLocal performance degradation
From: Shevek via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-25, 02:55
To: "concurrency-interest@cs.oswego.edu" <concurrency-interest@cs.oswego.edu>
Reply-To: Shevek <shevek@anarres.org>

We hit this performance degradation hard, like a brick wall. We had to work around it by using ThreadLocal<Map<...>>, reducing our creation/allocation rate, or using context objects containing HashMap(s).

S.

On 11/22/21 1:27 PM, Margaret Figura via Concurrency-interest wrote:
> Hi all,
>
> This has been discussed before in this thread: https://cs.oswego.edu/pipermail/concurrency-interest/2018-October/016628.html
> ...and I think Gil Tene's comment about collisions exacerbating the issue is especially helpful: https://cs.oswego.edu/pipermail/concurrency-interest/2018-October/016685.html
>
> What happens is that the ThreadLocalMap can degenerate to a linear scan when there are many collisions, and performance drops significantly on affected Threads in all code using any ThreadLocal instance (even code unrelated to the 'problem').
>
> Is there interest in fixing this? Within the same discussion, Peter Levart had asked if there was a reproducer. I've managed to create one that works quickly on at least my two test systems. Does that help?
> Reproducer and sample output: https://gist.github.com/megfigura/67d6972aa19fa1a4a1e13b4c7e7380fb
>
> Watching as it runs, what's most concerning is how it suddenly blows up. Monitoring for the longest run of non-nulls in the ThreadLocalMap, the number will be small, small, small, but then it suddenly explodes - 6...23...48...29863... ...257207. Runs of non-nulls mean that long linear-scans will happen with some of the ThreadLocal API calls.
>
> I ran into this issue indirectly by creating very many instances of the otherwise wonderful ChronicleMap which has a per-instance ThreadLocal. I captured a heapdump of our process in the bad state, and it showed a continuous run of non-null entries in the ThreadLocalMap of about 8000 items and another of about 4000. The backing array of the map is 64k, so there is a good chance a new instance will end up within one of these large blocks, doing a linear-scan. I could see the same behavior on different worker threads in the same process. The problem will reproduce again in new instances of the process within a few days in a specific customer environment. Once in the bad state, we saw an unrelated area which was using ReentrantReadWriteLock (which also uses ThreadLocal) show up as the hottest methods in the profiler.
>
> In my case, the solution is to reduce the rate we are creating ChronicleMap/ThreadLocal instances, but a workaround is to trigger a GC via JMX periodically or to get the internal ThreadLocal via reflection and schedule it to be .remove()'d on all threads that accessed it.
>
> Thanks a lot!
> Meg
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Joe Bowbeer via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-25, 07:12
To: Doug Lea <dl@cs.oswego.edu>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Joe Bowbeer <joe.bowbeer@gmail.com>

I like this original version without the vars. Maybe vars would be friendlier in an IDE with intellisense, but these javadoc examples should be obvious to the unassisted eye.

I would however remove all the extra space before the comments, but whatever style is used in the rest of the javadoc should prevail here.

On Wed, Nov 24, 2021 at 12:38 PM Doug Lea via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:


    On 11/24/21 2:15 PM, Dr Heinz M. Kabutz via Concurrency-interest wrote:
    > Every time I see the example in RecursiveTask I have to cringe:

    The initial rationale was that "nearly everyone" knows Fibonacci so it
    doesn't need much explanation it. but you are right that even more
    people know factorial, and the BigInteger version fits RecursiveTask
    without needing caveats, so we should use it; thanks. Here's a javadoc'd
    version of your example. Any objections to using it?


      *
      * <pre> {@code
      * public class FactorialTask extends RecursiveTask<BigInteger> {
      *   private final int from, to;
      *   public FactorialTask(int n) { this(0, n); }
      *   private FactorialTask(int from, int to) { this.from = from;
    this.to = to; }
      *   protected BigInteger compute() {
      *     if (from == to)                        // base case
      *       return (from == 0) ? BigInteger.ONE : BigInteger.valueOf(from);
      *     int mid = (from + to) >>> 1;           // split in half
      *     FactorialTask leftTask = (new FactorialTask(from, mid)).fork();
      *     FactorialTask rightTask = new FactorialTask(mid + 1, to);
      *     BigInteger right = rightTask.invoke(); // perform half the work
    locally
      *     BigInteger left = leftTask.join();
      *     return left.multiply(right);
      *   }
      * }}</pre>
      *

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 11:40
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

Hmmm, yes. I lost focus there.

I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.


Alex

On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

    Hi Alex,

    my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.

    The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409

    Regards

    Heinz
    -- 
    Dr Heinz M. Kabutz (PhD CompSci)
    Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
    Java Champion - www.javachampions.org
    JavaOne Rock Star Speaker
    Tel: +30 69 75 595 262
    Skype: kabutz

    On 2021/11/25 01:30, Alex Otenko wrote:
>     I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>
>     https://bit.ly/3oVFeTD
>
>
>     Alex
>
>     On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>         Every time I see the example in RecursiveTask I have to cringe:
>
>         https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>
>         For a classic example, here is a task computing Fibonacci numbers:
>
>
>           class Fibonacci extends RecursiveTask<Integer> {
>             final int n;
>             Fibonacci(int n) { this.n = n; }
>             protected Integer compute() {
>               if (n <= 1)
>                 return n;
>               Fibonacci f1 = new Fibonacci(n - 1);
>               f1.fork();
>               Fibonacci f2 = new Fibonacci(n - 2);
>               return f2.compute() + f1.join();
>             }
>           }
>         However, besides being a dumb way to compute Fibonacci functions (there
>         is a simple fast linear algorithm that you'd use in practice), this is
>         likely to perform poorly because the smallest subtasks are too small to
>         be worthwhile splitting up. Instead, as is the case for nearly all
>         fork/join applications, you'd pick some minimum granularity size (for
>         example 10 here) for which you always sequentially solve rather than
>         subdividing.
>
>
>
>         Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>         algorithm isn't fast either. Since we overflow even Long after about
>         fibonacci(90), we would need BigInteger. And there the add is linear,
>         meaning that the "fast linear" algorithm referred to here is probably
>         going to end up as "slow quadratic".
>
>         To me, this example sends the completely wrong message. Let's take the
>         worst possible algorithm and parallelize it. Great. That means if we use
>         1000 processors, we can solve the problem of n+10 in the same time as n
>         with a single processor.
>
>         I do realize this is meant to illustrate a point, but it doesn't do it
>         very well IME. I would like to propose to change this to a slightly
>         better example, for example a Factorial calculation:
>
>         public class FactorialTask extends RecursiveTask<BigInteger> {
>              private final int from, to;
>
>              public FactorialTask(int n) {
>                  this(0, n);
>              }
>
>              private FactorialTask(int from, int to) {
>                  this.from = from;
>                  this.to = to;
>              }
>
>              protected BigInteger compute() {
>                  if (from == to) {
>                      if (from == 0) return BigInteger.ONE;
>                      return BigInteger.valueOf(from);
>                  }
>                  int mid = (from + to) >>> 1;
>                  FactorialTask leftTask = new FactorialTask(from, mid);
>                  FactorialTask rightTask = new FactorialTask(mid + 1, to);
>                  leftTask.fork();
>                  BigInteger right = rightTask.invoke();
>                  BigInteger left = leftTask.join();
>                  return left.multiply(right);
>              }
>         }
>
>         This is actually a *lot* faster than the stream version:
>
>              public static BigInteger factorialStream(int n) {
>                  return IntStream.rangeClosed(1, n)
>                          .mapToObj(BigInteger::valueOf)
>                          .reduce(BigInteger.ONE, BigInteger::multiply);
>              }
>
>         (this has to do more with the algorithms used by BigInteger's multiply
>         method than the parallelization, but that also has an effect.
>
>
>         Alternatively, if we have to have Fibonacci, could we at least change it
>         to Dijkstra's Sum of Squares? I believe there are slightly better
>         algorithms, but this one works very nicely with parallelisation:
>
>         public class FibonacciTask extends RecursiveTask<BigInteger> {
>              private final int n;
>
>              public FibonacciTask(int n) {
>                  this.n = n;
>              }
>
>              @Override
>              protected BigInteger compute() {
>                  return switch (n) {
>                      case 0 -> BigInteger.ZERO;
>                      case 1 -> BigInteger.ONE;
>                      default -> {
>                          // Dijkstra's Sum of Squares Algorithm
>                          int half = (n + 1) / 2;
>                          FibonacciTask f0_task = new FibonacciTask(half - 1);
>                          f0_task.fork();
>                          FibonacciTask f1_task = new FibonacciTask(half);
>                          BigInteger f1 = f1_task.invoke();
>                          BigInteger f0 = f0_task.join();
>
>                          if (n % 2 == 1) {
>                              yield f0.multiply(f0).add(f1.multiply(f1));
>                          } else {
>                              yield f0.shiftLeft(1).add(f1).multiply(f1);
>                          }
>                      }
>                  };
>              }
>         }
>
>         Please let me know if you agree with this change (or propose a different
>         example). I would be happy to make the change. I presume it would need
>         to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>         and then we can sync that over to CVS? (My preference would be GitHub)
>
>
>
>
>         Regards
>
>         Heinz
>         -- 
>         Dr Heinz M. Kabutz (PhD CompSci)
>         Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>         Java Champion - www.javachampions.org
>         JavaOne Rock Star Speaker
>         Tel: +30 69 75 595 262
>         Skype: kabutz
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest@cs.oswego.edu
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 12:17
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

Do we want to also nudge the reader towards considering how to split tasks into equally sized, if possible? 

https://bit.ly/3HOH9Ca - factorial is, of course, better done by ensuring both branches multiply numbers of similar magnitude.

Alex

On Thu, 25 Nov 2021, 09:40 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:

    Hmmm, yes. I lost focus there.

    I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.


    Alex

    On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

        Hi Alex,

        my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.

        The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409

        Regards

        Heinz
        -- 
        Dr Heinz M. Kabutz (PhD CompSci)
        Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
        Java Champion - www.javachampions.org
        JavaOne Rock Star Speaker
        Tel: +30 69 75 595 262
        Skype: kabutz

        On 2021/11/25 01:30, Alex Otenko wrote:
>         I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>
>         https://bit.ly/3oVFeTD
>
>
>         Alex
>
>         On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>             Every time I see the example in RecursiveTask I have to cringe:
>
>             https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>
>             For a classic example, here is a task computing Fibonacci numbers:
>
>
>               class Fibonacci extends RecursiveTask<Integer> {
>                 final int n;
>                 Fibonacci(int n) { this.n = n; }
>                 protected Integer compute() {
>                   if (n <= 1)
>                     return n;
>                   Fibonacci f1 = new Fibonacci(n - 1);
>                   f1.fork();
>                   Fibonacci f2 = new Fibonacci(n - 2);
>                   return f2.compute() + f1.join();
>                 }
>               }
>             However, besides being a dumb way to compute Fibonacci functions (there
>             is a simple fast linear algorithm that you'd use in practice), this is
>             likely to perform poorly because the smallest subtasks are too small to
>             be worthwhile splitting up. Instead, as is the case for nearly all
>             fork/join applications, you'd pick some minimum granularity size (for
>             example 10 here) for which you always sequentially solve rather than
>             subdividing.
>
>
>
>             Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>             algorithm isn't fast either. Since we overflow even Long after about
>             fibonacci(90), we would need BigInteger. And there the add is linear,
>             meaning that the "fast linear" algorithm referred to here is probably
>             going to end up as "slow quadratic".
>
>             To me, this example sends the completely wrong message. Let's take the
>             worst possible algorithm and parallelize it. Great. That means if we use
>             1000 processors, we can solve the problem of n+10 in the same time as n
>             with a single processor.
>
>             I do realize this is meant to illustrate a point, but it doesn't do it
>             very well IME. I would like to propose to change this to a slightly
>             better example, for example a Factorial calculation:
>
>             public class FactorialTask extends RecursiveTask<BigInteger> {
>                  private final int from, to;
>
>                  public FactorialTask(int n) {
>                      this(0, n);
>                  }
>
>                  private FactorialTask(int from, int to) {
>                      this.from = from;
>                      this.to = to;
>                  }
>
>                  protected BigInteger compute() {
>                      if (from == to) {
>                          if (from == 0) return BigInteger.ONE;
>                          return BigInteger.valueOf(from);
>                      }
>                      int mid = (from + to) >>> 1;
>                      FactorialTask leftTask = new FactorialTask(from, mid);
>                      FactorialTask rightTask = new FactorialTask(mid + 1, to);
>                      leftTask.fork();
>                      BigInteger right = rightTask.invoke();
>                      BigInteger left = leftTask.join();
>                      return left.multiply(right);
>                  }
>             }
>
>             This is actually a *lot* faster than the stream version:
>
>                  public static BigInteger factorialStream(int n) {
>                      return IntStream.rangeClosed(1, n)
>                              .mapToObj(BigInteger::valueOf)
>                              .reduce(BigInteger.ONE, BigInteger::multiply);
>                  }
>
>             (this has to do more with the algorithms used by BigInteger's multiply
>             method than the parallelization, but that also has an effect.
>
>
>             Alternatively, if we have to have Fibonacci, could we at least change it
>             to Dijkstra's Sum of Squares? I believe there are slightly better
>             algorithms, but this one works very nicely with parallelisation:
>
>             public class FibonacciTask extends RecursiveTask<BigInteger> {
>                  private final int n;
>
>                  public FibonacciTask(int n) {
>                      this.n = n;
>                  }
>
>                  @Override
>                  protected BigInteger compute() {
>                      return switch (n) {
>                          case 0 -> BigInteger.ZERO;
>                          case 1 -> BigInteger.ONE;
>                          default -> {
>                              // Dijkstra's Sum of Squares Algorithm
>                              int half = (n + 1) / 2;
>                              FibonacciTask f0_task = new FibonacciTask(half - 1);
>                              f0_task.fork();
>                              FibonacciTask f1_task = new FibonacciTask(half);
>                              BigInteger f1 = f1_task.invoke();
>                              BigInteger f0 = f0_task.join();
>
>                              if (n % 2 == 1) {
>                                  yield f0.multiply(f0).add(f1.multiply(f1));
>                              } else {
>                                  yield f0.shiftLeft(1).add(f1).multiply(f1);
>                              }
>                          }
>                      };
>                  }
>             }
>
>             Please let me know if you agree with this change (or propose a different
>             example). I would be happy to make the change. I presume it would need
>             to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>             and then we can sync that over to CVS? (My preference would be GitHub)
>
>
>
>
>             Regards
>
>             Heinz
>             -- 
>             Dr Heinz M. Kabutz (PhD CompSci)
>             Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>             Java Champion - www.javachampions.org
>             JavaOne Rock Star Speaker
>             Tel: +30 69 75 595 262
>             Skype: kabutz
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest@cs.oswego.edu
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 17:18
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

Hi Heinz,

I don't mind even if my version is left as an exercise for the reader - after all, I have no feel of what the target audience can grasp.

As for your finding - that's interesting. I don't get 12x worse with (2x1024x1024)!, I always get mine a little faster (and for some inputs quite a bit faster).

(HotSpot, 15.0.1+9-18 on Mac)

Alex


On Thu, 25 Nov 2021, 11:00 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

    With my current simple algorithm, the magnitude of the two halves are similar. Whilst your algorithm is better in terms of keeping the two halves of closer size, it also complicates the demo program too much in my opinion. Your point should perhaps be added as a comment? A bigger concern to me would be the many tasks that we would fork. It would be better to have a depth threshold after which we stop forking. This is already mentioned in the comment.

    My algorithm for (2 * 1024 * 1024)! ends up like this:

    left size 2,733,860 bits, right size 2,746,476 bits
    left size 4,340,409 bits, right size 4,864,687 bits
    left size 5,062,576 bits, right size 5,191,086 bits
    left size 5,286,644 bits, right size 5,362,796 bits
    left size 5,426,123 bits, right size 5,480,336 bits
    left size 9,205,096 bits, right size 10,253,661 bits
    left size 10,649,440 bits, right size 10,906,459 bits
    left size 19,458,756 bits, right size 21,555,898 bits
    2097152 bits 41014654
    fjTime = 4798ms

    Your algorithm for the same input has an almost equal number of bits for the two numbers:

    left size 320,424 bits, right size 320,433 bits
    left size 640,852 bits, right size 640,861 bits
    left size 640,847 bits, right size 640,857 bits
    left size 1,281,703 bits, right size 1,281,713 bits
    left size 2,563,415 bits, right size 2,563,424 bits
    left size 5,126,828 bits, right size 5,126,839 bits
    left size 10,253,656 bits, right size 10,253,667 bits
    left size 20,507,332 bits, right size 20,507,322 bits
    2097152 bits 41014654
    fjTime = 69867ms

    However, mine also happens to be about 12x faster, but I suspect that has more to do with the computational time complexity than with Fork/Join. With BigInteger, we want to get to large numbers as quickly as possible, so that we can start using Karatsuba and Toom Cook 3.

    Regards

    Heinz
    -- 
    Dr Heinz M. Kabutz (PhD CompSci)
    Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
    Java Champion - www.javachampions.org
    JavaOne Rock Star Speaker
    Tel: +30 69 75 595 262
    Skype: kabutz

    On 2021/11/25 12:17, Alex Otenko wrote:
>     Do we want to also nudge the reader towards considering how to split tasks into equally sized, if possible? 
>
>     https://bit.ly/3HOH9Ca - factorial is, of course, better done by ensuring both branches multiply numbers of similar magnitude.
>
>     Alex
>
>     On Thu, 25 Nov 2021, 09:40 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:
>
>         Hmmm, yes. I lost focus there.
>
>         I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.
>
>
>         Alex
>
>         On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:
>
>             Hi Alex,
>
>             my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.
>
>             The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409
>
>             Regards
>
>             Heinz
>             -- 
>             Dr Heinz M. Kabutz (PhD CompSci)
>             Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>             Java Champion - www.javachampions.org
>             JavaOne Rock Star Speaker
>             Tel: +30 69 75 595 262
>             Skype: kabutz
>
>             On 2021/11/25 01:30, Alex Otenko wrote:
>>             I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>>
>>             https://bit.ly/3oVFeTD
>>
>>
>>             Alex
>>
>>             On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>                 Every time I see the example in RecursiveTask I have to cringe:
>>
>>                 https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>>
>>                 For a classic example, here is a task computing Fibonacci numbers:
>>
>>
>>                   class Fibonacci extends RecursiveTask<Integer> {
>>                     final int n;
>>                     Fibonacci(int n) { this.n = n; }
>>                     protected Integer compute() {
>>                       if (n <= 1)
>>                         return n;
>>                       Fibonacci f1 = new Fibonacci(n - 1);
>>                       f1.fork();
>>                       Fibonacci f2 = new Fibonacci(n - 2);
>>                       return f2.compute() + f1.join();
>>                     }
>>                   }
>>                 However, besides being a dumb way to compute Fibonacci functions (there
>>                 is a simple fast linear algorithm that you'd use in practice), this is
>>                 likely to perform poorly because the smallest subtasks are too small to
>>                 be worthwhile splitting up. Instead, as is the case for nearly all
>>                 fork/join applications, you'd pick some minimum granularity size (for
>>                 example 10 here) for which you always sequentially solve rather than
>>                 subdividing.
>>
>>
>>
>>                 Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>>                 algorithm isn't fast either. Since we overflow even Long after about
>>                 fibonacci(90), we would need BigInteger. And there the add is linear,
>>                 meaning that the "fast linear" algorithm referred to here is probably
>>                 going to end up as "slow quadratic".
>>
>>                 To me, this example sends the completely wrong message. Let's take the
>>                 worst possible algorithm and parallelize it. Great. That means if we use
>>                 1000 processors, we can solve the problem of n+10 in the same time as n
>>                 with a single processor.
>>
>>                 I do realize this is meant to illustrate a point, but it doesn't do it
>>                 very well IME. I would like to propose to change this to a slightly
>>                 better example, for example a Factorial calculation:
>>
>>                 public class FactorialTask extends RecursiveTask<BigInteger> {
>>                      private final int from, to;
>>
>>                      public FactorialTask(int n) {
>>                          this(0, n);
>>                      }
>>
>>                      private FactorialTask(int from, int to) {
>>                          this.from = from;
>>                          this.to = to;
>>                      }
>>
>>                      protected BigInteger compute() {
>>                          if (from == to) {
>>                              if (from == 0) return BigInteger.ONE;
>>                              return BigInteger.valueOf(from);
>>                          }
>>                          int mid = (from + to) >>> 1;
>>                          FactorialTask leftTask = new FactorialTask(from, mid);
>>                          FactorialTask rightTask = new FactorialTask(mid + 1, to);
>>                          leftTask.fork();
>>                          BigInteger right = rightTask.invoke();
>>                          BigInteger left = leftTask.join();
>>                          return left.multiply(right);
>>                      }
>>                 }
>>
>>                 This is actually a *lot* faster than the stream version:
>>
>>                      public static BigInteger factorialStream(int n) {
>>                          return IntStream.rangeClosed(1, n)
>>                                  .mapToObj(BigInteger::valueOf)
>>                                  .reduce(BigInteger.ONE, BigInteger::multiply);
>>                      }
>>
>>                 (this has to do more with the algorithms used by BigInteger's multiply
>>                 method than the parallelization, but that also has an effect.
>>
>>
>>                 Alternatively, if we have to have Fibonacci, could we at least change it
>>                 to Dijkstra's Sum of Squares? I believe there are slightly better
>>                 algorithms, but this one works very nicely with parallelisation:
>>
>>                 public class FibonacciTask extends RecursiveTask<BigInteger> {
>>                      private final int n;
>>
>>                      public FibonacciTask(int n) {
>>                          this.n = n;
>>                      }
>>
>>                      @Override
>>                      protected BigInteger compute() {
>>                          return switch (n) {
>>                              case 0 -> BigInteger.ZERO;
>>                              case 1 -> BigInteger.ONE;
>>                              default -> {
>>                                  // Dijkstra's Sum of Squares Algorithm
>>                                  int half = (n + 1) / 2;
>>                                  FibonacciTask f0_task = new FibonacciTask(half - 1);
>>                                  f0_task.fork();
>>                                  FibonacciTask f1_task = new FibonacciTask(half);
>>                                  BigInteger f1 = f1_task.invoke();
>>                                  BigInteger f0 = f0_task.join();
>>
>>                                  if (n % 2 == 1) {
>>                                      yield f0.multiply(f0).add(f1.multiply(f1));
>>                                  } else {
>>                                      yield f0.shiftLeft(1).add(f1).multiply(f1);
>>                                  }
>>                              }
>>                          };
>>                      }
>>                 }
>>
>>                 Please let me know if you agree with this change (or propose a different
>>                 example). I would be happy to make the change. I presume it would need
>>                 to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>>                 and then we can sync that over to CVS? (My preference would be GitHub)
>>
>>
>>
>>
>>                 Regards
>>
>>                 Heinz
>>                 -- 
>>                 Dr Heinz M. Kabutz (PhD CompSci)
>>                 Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>>                 Java Champion - www.javachampions.org
>>                 JavaOne Rock Star Speaker
>>                 Tel: +30 69 75 595 262
>>                 Skype: kabutz
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest@cs.oswego.edu
>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 17:26
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

Eg HotSpot as before:

Factorial, straightforward: 6.281s
Factorial, try equalize work: 6.051s

GraalVM EE 20.2.0 (JDK 11 based):
5.278s and 4.668s respectively for the same task.

Alex

On Thu, 25 Nov 2021, 15:18 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:

    Hi Heinz,

    I don't mind even if my version is left as an exercise for the reader - after all, I have no feel of what the target audience can grasp.

    As for your finding - that's interesting. I don't get 12x worse with (2x1024x1024)!, I always get mine a little faster (and for some inputs quite a bit faster).

    (HotSpot, 15.0.1+9-18 on Mac)

    Alex


    On Thu, 25 Nov 2021, 11:00 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

        With my current simple algorithm, the magnitude of the two halves are similar. Whilst your algorithm is better in terms of keeping the two halves of closer size, it also complicates the demo program too much in my opinion. Your point should perhaps be added as a comment? A bigger concern to me would be the many tasks that we would fork. It would be better to have a depth threshold after which we stop forking. This is already mentioned in the comment.

        My algorithm for (2 * 1024 * 1024)! ends up like this:

        left size 2,733,860 bits, right size 2,746,476 bits
        left size 4,340,409 bits, right size 4,864,687 bits
        left size 5,062,576 bits, right size 5,191,086 bits
        left size 5,286,644 bits, right size 5,362,796 bits
        left size 5,426,123 bits, right size 5,480,336 bits
        left size 9,205,096 bits, right size 10,253,661 bits
        left size 10,649,440 bits, right size 10,906,459 bits
        left size 19,458,756 bits, right size 21,555,898 bits
        2097152 bits 41014654
        fjTime = 4798ms

        Your algorithm for the same input has an almost equal number of bits for the two numbers:

        left size 320,424 bits, right size 320,433 bits
        left size 640,852 bits, right size 640,861 bits
        left size 640,847 bits, right size 640,857 bits
        left size 1,281,703 bits, right size 1,281,713 bits
        left size 2,563,415 bits, right size 2,563,424 bits
        left size 5,126,828 bits, right size 5,126,839 bits
        left size 10,253,656 bits, right size 10,253,667 bits
        left size 20,507,332 bits, right size 20,507,322 bits
        2097152 bits 41014654
        fjTime = 69867ms

        However, mine also happens to be about 12x faster, but I suspect that has more to do with the computational time complexity than with Fork/Join. With BigInteger, we want to get to large numbers as quickly as possible, so that we can start using Karatsuba and Toom Cook 3.

        Regards

        Heinz
        -- 
        Dr Heinz M. Kabutz (PhD CompSci)
        Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
        Java Champion - www.javachampions.org
        JavaOne Rock Star Speaker
        Tel: +30 69 75 595 262
        Skype: kabutz

        On 2021/11/25 12:17, Alex Otenko wrote:
>         Do we want to also nudge the reader towards considering how to split tasks into equally sized, if possible? 
>
>         https://bit.ly/3HOH9Ca - factorial is, of course, better done by ensuring both branches multiply numbers of similar magnitude.
>
>         Alex
>
>         On Thu, 25 Nov 2021, 09:40 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:
>
>             Hmmm, yes. I lost focus there.
>
>             I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.
>
>
>             Alex
>
>             On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:
>
>                 Hi Alex,
>
>                 my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.
>
>                 The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409
>
>                 Regards
>
>                 Heinz
>                 -- 
>                 Dr Heinz M. Kabutz (PhD CompSci)
>                 Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>                 Java Champion - www.javachampions.org
>                 JavaOne Rock Star Speaker
>                 Tel: +30 69 75 595 262
>                 Skype: kabutz
>
>                 On 2021/11/25 01:30, Alex Otenko wrote:
>>                 I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>>
>>                 https://bit.ly/3oVFeTD
>>
>>
>>                 Alex
>>
>>                 On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>                     Every time I see the example in RecursiveTask I have to cringe:
>>
>>                     https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>>
>>                     For a classic example, here is a task computing Fibonacci numbers:
>>
>>
>>                       class Fibonacci extends RecursiveTask<Integer> {
>>                         final int n;
>>                         Fibonacci(int n) { this.n = n; }
>>                         protected Integer compute() {
>>                           if (n <= 1)
>>                             return n;
>>                           Fibonacci f1 = new Fibonacci(n - 1);
>>                           f1.fork();
>>                           Fibonacci f2 = new Fibonacci(n - 2);
>>                           return f2.compute() + f1.join();
>>                         }
>>                       }
>>                     However, besides being a dumb way to compute Fibonacci functions (there
>>                     is a simple fast linear algorithm that you'd use in practice), this is
>>                     likely to perform poorly because the smallest subtasks are too small to
>>                     be worthwhile splitting up. Instead, as is the case for nearly all
>>                     fork/join applications, you'd pick some minimum granularity size (for
>>                     example 10 here) for which you always sequentially solve rather than
>>                     subdividing.
>>
>>
>>
>>                     Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>>                     algorithm isn't fast either. Since we overflow even Long after about
>>                     fibonacci(90), we would need BigInteger. And there the add is linear,
>>                     meaning that the "fast linear" algorithm referred to here is probably
>>                     going to end up as "slow quadratic".
>>
>>                     To me, this example sends the completely wrong message. Let's take the
>>                     worst possible algorithm and parallelize it. Great. That means if we use
>>                     1000 processors, we can solve the problem of n+10 in the same time as n
>>                     with a single processor.
>>
>>                     I do realize this is meant to illustrate a point, but it doesn't do it
>>                     very well IME. I would like to propose to change this to a slightly
>>                     better example, for example a Factorial calculation:
>>
>>                     public class FactorialTask extends RecursiveTask<BigInteger> {
>>                          private final int from, to;
>>
>>                          public FactorialTask(int n) {
>>                              this(0, n);
>>                          }
>>
>>                          private FactorialTask(int from, int to) {
>>                              this.from = from;
>>                              this.to = to;
>>                          }
>>
>>                          protected BigInteger compute() {
>>                              if (from == to) {
>>                                  if (from == 0) return BigInteger.ONE;
>>                                  return BigInteger.valueOf(from);
>>                              }
>>                              int mid = (from + to) >>> 1;
>>                              FactorialTask leftTask = new FactorialTask(from, mid);
>>                              FactorialTask rightTask = new FactorialTask(mid + 1, to);
>>                              leftTask.fork();
>>                              BigInteger right = rightTask.invoke();
>>                              BigInteger left = leftTask.join();
>>                              return left.multiply(right);
>>                          }
>>                     }
>>
>>                     This is actually a *lot* faster than the stream version:
>>
>>                          public static BigInteger factorialStream(int n) {
>>                              return IntStream.rangeClosed(1, n)
>>                                      .mapToObj(BigInteger::valueOf)
>>                                      .reduce(BigInteger.ONE, BigInteger::multiply);
>>                          }
>>
>>                     (this has to do more with the algorithms used by BigInteger's multiply
>>                     method than the parallelization, but that also has an effect.
>>
>>
>>                     Alternatively, if we have to have Fibonacci, could we at least change it
>>                     to Dijkstra's Sum of Squares? I believe there are slightly better
>>                     algorithms, but this one works very nicely with parallelisation:
>>
>>                     public class FibonacciTask extends RecursiveTask<BigInteger> {
>>                          private final int n;
>>
>>                          public FibonacciTask(int n) {
>>                              this.n = n;
>>                          }
>>
>>                          @Override
>>                          protected BigInteger compute() {
>>                              return switch (n) {
>>                                  case 0 -> BigInteger.ZERO;
>>                                  case 1 -> BigInteger.ONE;
>>                                  default -> {
>>                                      // Dijkstra's Sum of Squares Algorithm
>>                                      int half = (n + 1) / 2;
>>                                      FibonacciTask f0_task = new FibonacciTask(half - 1);
>>                                      f0_task.fork();
>>                                      FibonacciTask f1_task = new FibonacciTask(half);
>>                                      BigInteger f1 = f1_task.invoke();
>>                                      BigInteger f0 = f0_task.join();
>>
>>                                      if (n % 2 == 1) {
>>                                          yield f0.multiply(f0).add(f1.multiply(f1));
>>                                      } else {
>>                                          yield f0.shiftLeft(1).add(f1).multiply(f1);
>>                                      }
>>                                  }
>>                              };
>>                          }
>>                     }
>>
>>                     Please let me know if you agree with this change (or propose a different
>>                     example). I would be happy to make the change. I presume it would need
>>                     to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>>                     and then we can sync that over to CVS? (My preference would be GitHub)
>>
>>
>>
>>
>>                     Regards
>>
>>                     Heinz
>>                     -- 
>>                     Dr Heinz M. Kabutz (PhD CompSci)
>>                     Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>>                     Java Champion - www.javachampions.org
>>                     JavaOne Rock Star Speaker
>>                     Tel: +30 69 75 595 262
>>                     Skype: kabutz
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest@cs.oswego.edu
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-25, 17:43
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

The usual: all forked, none volunteered to pick up the task they all depend on.

This seems to fix it:

https://bit.ly/3laU9Iu

(But this is moot now, your suggestion is just as good)

Alex

On Thu, 25 Nov 2021, 10:35 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

    Hi Alex,

    there is some strange resource deadlock in your Fibonacci solution that occurs when there are more than 2 cores. Try run your Main with -XX:+ActiveProcessorCount=8 to see what I mean:

    https://tio.run/##pVVNbxMxEL3nVww9oHWjWmnEAZpugFatBBKkajlUIA624yZOdu3I9gaqKn@dMN7vfBSCsJTNzng8b/z8PDtjS3YyG8/Xa5UujPUwQwdNmZ/SCzX5oL2cSDvotCczrxIqjBaZtVJ7em3s/KNR@saY5LDIL8zN/xx5K/HNqaUsQjuLjCdKgEiYc/CJKQ1PHYDS6zzz@Lc0agwpzkV33io9@fYdGMnjAO4enZcpNZmnC5zziY6OEFcljCfyxhohnTPWQQxH0IXbTHuVSjqRvnyNCN0THhEyeDa9MGlqclJumGVJIhPl0hKgTVlAudwXW2d/MDbCpKDi3gDU@avw7HbJc8ha/oBrxU2kCFKaLjIfyueY7K8r3vS2lwCswrKS4sToCTzkubEeXbGrHiDSEMfQI2Clz6yGXlF6voDhrts2R/u0ZYvGrreKrlJ8dKomU@n8SMsL5SMEHQ6HGA/qRc4Hmri8KqXKKTEBO@bINT8Wg2qKw3EMvDZFMJtZFkzWMrubwRtm2ISsrcBA4OAlKAIvcLvQKghHKKcB2s2Ww205NhFWneZZcsy3j6e4HuEkQf70Uo8dbFyk8zA1rA4tnGCRvrnp1fZbHr7jEaWSyut3vaMHHH6qHEVNVBCrYsm70VJaq8aySGCNl8LLcVFzrbwmTaGsc@iTpw3y4lY9dPT5apvIQo1vQZy1A79e3Y62GRbUZdxbJnzEqitSjJLlsJOtU8DRbmUlrTbgVjdJQxApoSjoeUQGe5c1lQ1BZ6GY9jRlY7bwEbJxgrnpDN2hC6F7HNWmIDTNEq8WyWPj5ORwXN4/CJb/JwzrH7i7XRj2DzDiMBixB0YQ0rrqqIs@VXpp5nnrR8p5v4wkrQaB7aJf17k/KBwrHm7pbwQWlJ03jNOiYWz2i73NYKuBBO0GUEZ25dl5TsOrTvHrrNbrX2bhldFufXJ/f/ZeeOwS9bft0uCHL379Gw


    Regards

    Heinz
    -- 
    Dr Heinz M. Kabutz (PhD CompSci)
    Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
    Java Champion - www.javachampions.org
    JavaOne Rock Star Speaker
    Tel: +30 69 75 595 262
    Skype: kabutz

    On 2021/11/25 11:40, Alex Otenko wrote:
>     Hmmm, yes. I lost focus there.
>
>     I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.
>
>
>     Alex
>
>     On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:
>
>         Hi Alex,
>
>         my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.
>
>         The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409
>
>         Regards
>
>         Heinz
>         -- 
>         Dr Heinz M. Kabutz (PhD CompSci)
>         Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>         Java Champion - www.javachampions.org
>         JavaOne Rock Star Speaker
>         Tel: +30 69 75 595 262
>         Skype: kabutz
>
>         On 2021/11/25 01:30, Alex Otenko wrote:
>>         I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>>
>>         https://bit.ly/3oVFeTD
>>
>>
>>         Alex
>>
>>         On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>             Every time I see the example in RecursiveTask I have to cringe:
>>
>>             https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>>
>>             For a classic example, here is a task computing Fibonacci numbers:
>>
>>
>>               class Fibonacci extends RecursiveTask<Integer> {
>>                 final int n;
>>                 Fibonacci(int n) { this.n = n; }
>>                 protected Integer compute() {
>>                   if (n <= 1)
>>                     return n;
>>                   Fibonacci f1 = new Fibonacci(n - 1);
>>                   f1.fork();
>>                   Fibonacci f2 = new Fibonacci(n - 2);
>>                   return f2.compute() + f1.join();
>>                 }
>>               }
>>             However, besides being a dumb way to compute Fibonacci functions (there
>>             is a simple fast linear algorithm that you'd use in practice), this is
>>             likely to perform poorly because the smallest subtasks are too small to
>>             be worthwhile splitting up. Instead, as is the case for nearly all
>>             fork/join applications, you'd pick some minimum granularity size (for
>>             example 10 here) for which you always sequentially solve rather than
>>             subdividing.
>>
>>
>>
>>             Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>>             algorithm isn't fast either. Since we overflow even Long after about
>>             fibonacci(90), we would need BigInteger. And there the add is linear,
>>             meaning that the "fast linear" algorithm referred to here is probably
>>             going to end up as "slow quadratic".
>>
>>             To me, this example sends the completely wrong message. Let's take the
>>             worst possible algorithm and parallelize it. Great. That means if we use
>>             1000 processors, we can solve the problem of n+10 in the same time as n
>>             with a single processor.
>>
>>             I do realize this is meant to illustrate a point, but it doesn't do it
>>             very well IME. I would like to propose to change this to a slightly
>>             better example, for example a Factorial calculation:
>>
>>             public class FactorialTask extends RecursiveTask<BigInteger> {
>>                  private final int from, to;
>>
>>                  public FactorialTask(int n) {
>>                      this(0, n);
>>                  }
>>
>>                  private FactorialTask(int from, int to) {
>>                      this.from = from;
>>                      this.to = to;
>>                  }
>>
>>                  protected BigInteger compute() {
>>                      if (from == to) {
>>                          if (from == 0) return BigInteger.ONE;
>>                          return BigInteger.valueOf(from);
>>                      }
>>                      int mid = (from + to) >>> 1;
>>                      FactorialTask leftTask = new FactorialTask(from, mid);
>>                      FactorialTask rightTask = new FactorialTask(mid + 1, to);
>>                      leftTask.fork();
>>                      BigInteger right = rightTask.invoke();
>>                      BigInteger left = leftTask.join();
>>                      return left.multiply(right);
>>                  }
>>             }
>>
>>             This is actually a *lot* faster than the stream version:
>>
>>                  public static BigInteger factorialStream(int n) {
>>                      return IntStream.rangeClosed(1, n)
>>                              .mapToObj(BigInteger::valueOf)
>>                              .reduce(BigInteger.ONE, BigInteger::multiply);
>>                  }
>>
>>             (this has to do more with the algorithms used by BigInteger's multiply
>>             method than the parallelization, but that also has an effect.
>>
>>
>>             Alternatively, if we have to have Fibonacci, could we at least change it
>>             to Dijkstra's Sum of Squares? I believe there are slightly better
>>             algorithms, but this one works very nicely with parallelisation:
>>
>>             public class FibonacciTask extends RecursiveTask<BigInteger> {
>>                  private final int n;
>>
>>                  public FibonacciTask(int n) {
>>                      this.n = n;
>>                  }
>>
>>                  @Override
>>                  protected BigInteger compute() {
>>                      return switch (n) {
>>                          case 0 -> BigInteger.ZERO;
>>                          case 1 -> BigInteger.ONE;
>>                          default -> {
>>                              // Dijkstra's Sum of Squares Algorithm
>>                              int half = (n + 1) / 2;
>>                              FibonacciTask f0_task = new FibonacciTask(half - 1);
>>                              f0_task.fork();
>>                              FibonacciTask f1_task = new FibonacciTask(half);
>>                              BigInteger f1 = f1_task.invoke();
>>                              BigInteger f0 = f0_task.join();
>>
>>                              if (n % 2 == 1) {
>>                                  yield f0.multiply(f0).add(f1.multiply(f1));
>>                              } else {
>>                                  yield f0.shiftLeft(1).add(f1).multiply(f1);
>>                              }
>>                          }
>>                      };
>>                  }
>>             }
>>
>>             Please let me know if you agree with this change (or propose a different
>>             example). I would be happy to make the change. I presume it would need
>>             to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>>             and then we can sync that over to CVS? (My preference would be GitHub)
>>
>>
>>
>>
>>             Regards
>>
>>             Heinz
>>             -- 
>>             Dr Heinz M. Kabutz (PhD CompSci)
>>             Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>>             Java Champion - www.javachampions.org
>>             JavaOne Rock Star Speaker
>>             Tel: +30 69 75 595 262
>>             Skype: kabutz
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest@cs.oswego.edu
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Doug Lea via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-25, 20:40
To: concurrency-interest@cs.oswego.edu
Reply-To: Doug Lea <dl@cs.oswego.edu>

Thanks for the comments and suggestions. Here's pass 2, that tries to find a better compromise across concerns. Also pasted below without the leading "*"s in case you'd like to experience the copy-paste-run that people will be able to do.

 * <p>For example, here is a task-based program for computing Factorials:
 *
 * <pre> {@code
 * import java.util.concurrent.RecursiveTask;
 * import java.math.BigInteger;
 * public class Factorial {
 *   static class FactorialTask extends RecursiveTask<BigInteger> {
 *     private final int from, to;
 *     FactorialTask(int from, int to) { this.from = from; this.to = to; }
 *     protected BigInteger compute() {
 *       if (from == to) {                       // base case
 *         return BigInteger.valueOf(from);
 *       } else if (to - from < 2) {             // too small to parallelize
 *          return BigInteger.valueOf(from).multiply(BigInteger.valueOf(to));
 *       } else {                                // split in half
 *         int mid = from + (to - from) / 2;
 *         FactorialTask leftTask = new FactorialTask(from, mid);
 *         leftTask.fork();         // perform about half the work locally
 *         return new FactorialTask(mid + 1, to).compute()
 *                .multiply(leftTask.join());
 *       }
 *     }
 *   }
 *   static BigInteger factorial(int n) { // uses ForkJoinPool.commonPool()
 *     return (n <= 1) ? BigInteger.ONE : new FactorialTask(1, n).invoke();
 *   }
 *   public static void main(String[] args) {
 *     System.out.println(factorial(Integer.parseInt(args[0])));
 *   }
 * }}</pre>


import java.util.concurrent.RecursiveTask;
import java.math.BigInteger;
public class Factorial {
  static class FactorialTask extends RecursiveTask<BigInteger> {
    private final int from, to;
    FactorialTask(int from, int to) { this.from = from; this.to = to; }
    protected BigInteger compute() {
      if (to == from) {                       // base case
        return BigInteger.valueOf(from);
      } else if (to - from < 2) {             // too small to parallelize
         return BigInteger.valueOf(from).multiply(BigInteger.valueOf(to));
      } else {                                // split in half
        int mid = from + (to - from) / 2;
        FactorialTask leftTask = new FactorialTask(from, mid);
        leftTask.fork();         // perform about half the work locally
        return new FactorialTask(mid + 1, to).compute()
               .multiply(leftTask.join());
      }
    }
  }
  static BigInteger factorial(int n) { // uses ForkJoinPool.commonPool()
    return (n <= 1) ? BigInteger.ONE : new FactorialTask(1, n).invoke();
  }
  public static void main(String[] args) {
    System.out.println(factorial(Integer.parseInt(args[0])));
  }
}

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Nathan Reynolds via Concurrency-interest <concurrency-interest@cs.oswego.edu>
Date: 2021-11-26, 08:13
To: Doug Lea <dl@cs.oswego.edu>
CC: concurrency-interest <concurrency-interest@cs.oswego.edu>
Reply-To: Nathan Reynolds <numeralnathan@gmail.com>

No "else"s are needed in compute() since the blocks have a return.

On Thu, Nov 25, 2021 at 11:43 AM Doug Lea via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:

    Thanks for the comments and suggestions. Here's pass 2, that tries to
    find a better compromise across concerns. Also pasted below without the
    leading "*"s in case you'd like to experience the copy-paste-run that
    people will be able to do.

      * <p>For example, here is a task-based program for computing Factorials:
      *
      * <pre> {@code
      * import java.util.concurrent.RecursiveTask;
      * import java.math.BigInteger;
      * public class Factorial {
      *   static class FactorialTask extends RecursiveTask<BigInteger> {
      *     private final int from, to;
      *     FactorialTask(int from, int to) { this.from = from; this.to = to; }
      *     protected BigInteger compute() {
      *       if (from == to) {                       // base case
      *         return BigInteger.valueOf(from);
      *       } else if (to - from < 2) {             // too small to
    parallelize
      *          return
    BigInteger.valueOf(from).multiply(BigInteger.valueOf(to));
      *       } else {                                // split in half
      *         int mid = from + (to - from) / 2;
      *         FactorialTask leftTask = new FactorialTask(from, mid);
      *         leftTask.fork();         // perform about half the work locally
      *         return new FactorialTask(mid + 1, to).compute()
      *                .multiply(leftTask.join());
      *       }
      *     }
      *   }
      *   static BigInteger factorial(int n) { // uses ForkJoinPool.commonPool()
      *     return (n <= 1) ? BigInteger.ONE : new FactorialTask(1, n).invoke();
      *   }
      *   public static void main(String[] args) {
      *     System.out.println(factorial(Integer.parseInt(args[0])));
      *   }
      * }}</pre>


    import java.util.concurrent.RecursiveTask;
    import java.math.BigInteger;
    public class Factorial {
       static class FactorialTask extends RecursiveTask<BigInteger> {
         private final int from, to;
         FactorialTask(int from, int to) { this.from = from; this.to = to; }
         protected BigInteger compute() {
           if (to == from) {                       // base case
             return BigInteger.valueOf(from);
           } else if (to - from < 2) {             // too small to parallelize
              return BigInteger.valueOf(from).multiply(BigInteger.valueOf(to));
           } else {                                // split in half
             int mid = from + (to - from) / 2;
             FactorialTask leftTask = new FactorialTask(from, mid);
             leftTask.fork();         // perform about half the work locally
             return new FactorialTask(mid + 1, to).compute()
                    .multiply(leftTask.join());
           }
         }
       }
       static BigInteger factorial(int n) { // uses ForkJoinPool.commonPool()
         return (n <= 1) ? BigInteger.ONE : new FactorialTask(1, n).invoke();
       }
       public static void main(String[] args) {
         System.out.println(factorial(Integer.parseInt(args[0])));
       }
    }

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-26, 10:54
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

I am curious what's going on with your 12x observation. 

I may be wrong about why I observe the speedup, but my original thinking was to do that.

Looking at the naive split of work, I was thinking: the left multiplies lots of small numbers, and the right multiplies lots of big numbers; that means the left is going to be done sooner, and some CPU will be idling. Of course, given that there will be a lot of tiny work created, it won't really be idling so much, so let's call it a hunch that it will have tendency to idle.

So I thought let's split the work into even and odd numbers. Then left and right will be going through the numbers of similar magnitude, and bound to do similar amount of work. Then to keep splitting work, choose all divisible by 4 out of even, and those that aren't; then of those divisible by 4, half are divisible by 8, and half aren't, etc. Of course, no divisibility test is needed - just keep adding a step, so the procedure of splitting work is extensible to the odd numbers, too.

If you notice, we actually end up multiplying a very small number and a very large one at first - say, 1 and 1 million. This is kind of against my premise that we multiply mostly equally sized numbers. But the idea is that the other branch is doing roughly the same - multiplying 2 and 1000001. This certainly is going to get us into Karatsuba range very quickly, too.

So I really wonder why you are getting 12x worse numbers. If this isn't due to some additional debugging that wasn't commented out, I wonder if there is some strange fact about how the work gets split - eg do we get to juggle work that gets blocked more?

Alex

On Thu, 25 Nov 2021, 11:00 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

    With my current simple algorithm, the magnitude of the two halves are similar. Whilst your algorithm is better in terms of keeping the two halves of closer size, it also complicates the demo program too much in my opinion. Your point should perhaps be added as a comment? A bigger concern to me would be the many tasks that we would fork. It would be better to have a depth threshold after which we stop forking. This is already mentioned in the comment.

    My algorithm for (2 * 1024 * 1024)! ends up like this:

    left size 2,733,860 bits, right size 2,746,476 bits
    left size 4,340,409 bits, right size 4,864,687 bits
    left size 5,062,576 bits, right size 5,191,086 bits
    left size 5,286,644 bits, right size 5,362,796 bits
    left size 5,426,123 bits, right size 5,480,336 bits
    left size 9,205,096 bits, right size 10,253,661 bits
    left size 10,649,440 bits, right size 10,906,459 bits
    left size 19,458,756 bits, right size 21,555,898 bits
    2097152 bits 41014654
    fjTime = 4798ms

    Your algorithm for the same input has an almost equal number of bits for the two numbers:

    left size 320,424 bits, right size 320,433 bits
    left size 640,852 bits, right size 640,861 bits
    left size 640,847 bits, right size 640,857 bits
    left size 1,281,703 bits, right size 1,281,713 bits
    left size 2,563,415 bits, right size 2,563,424 bits
    left size 5,126,828 bits, right size 5,126,839 bits
    left size 10,253,656 bits, right size 10,253,667 bits
    left size 20,507,332 bits, right size 20,507,322 bits
    2097152 bits 41014654
    fjTime = 69867ms

    However, mine also happens to be about 12x faster, but I suspect that has more to do with the computational time complexity than with Fork/Join. With BigInteger, we want to get to large numbers as quickly as possible, so that we can start using Karatsuba and Toom Cook 3.

    Regards

    Heinz
    -- 
    Dr Heinz M. Kabutz (PhD CompSci)
    Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
    Java Champion - www.javachampions.org
    JavaOne Rock Star Speaker
    Tel: +30 69 75 595 262
    Skype: kabutz

    On 2021/11/25 12:17, Alex Otenko wrote:
>     Do we want to also nudge the reader towards considering how to split tasks into equally sized, if possible? 
>
>     https://bit.ly/3HOH9Ca - factorial is, of course, better done by ensuring both branches multiply numbers of similar magnitude.
>
>     Alex
>
>     On Thu, 25 Nov 2021, 09:40 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:
>
>         Hmmm, yes. I lost focus there.
>
>         I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.
>
>
>         Alex
>
>         On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:
>
>             Hi Alex,
>
>             my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.
>
>             The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409
>
>             Regards
>
>             Heinz
>             -- 
>             Dr Heinz M. Kabutz (PhD CompSci)
>             Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>             Java Champion - www.javachampions.org
>             JavaOne Rock Star Speaker
>             Tel: +30 69 75 595 262
>             Skype: kabutz
>
>             On 2021/11/25 01:30, Alex Otenko wrote:
>>             I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>>
>>             https://bit.ly/3oVFeTD
>>
>>
>>             Alex
>>
>>             On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>                 Every time I see the example in RecursiveTask I have to cringe:
>>
>>                 https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>>
>>                 For a classic example, here is a task computing Fibonacci numbers:
>>
>>
>>                   class Fibonacci extends RecursiveTask<Integer> {
>>                     final int n;
>>                     Fibonacci(int n) { this.n = n; }
>>                     protected Integer compute() {
>>                       if (n <= 1)
>>                         return n;
>>                       Fibonacci f1 = new Fibonacci(n - 1);
>>                       f1.fork();
>>                       Fibonacci f2 = new Fibonacci(n - 2);
>>                       return f2.compute() + f1.join();
>>                     }
>>                   }
>>                 However, besides being a dumb way to compute Fibonacci functions (there
>>                 is a simple fast linear algorithm that you'd use in practice), this is
>>                 likely to perform poorly because the smallest subtasks are too small to
>>                 be worthwhile splitting up. Instead, as is the case for nearly all
>>                 fork/join applications, you'd pick some minimum granularity size (for
>>                 example 10 here) for which you always sequentially solve rather than
>>                 subdividing.
>>
>>
>>
>>                 Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>>                 algorithm isn't fast either. Since we overflow even Long after about
>>                 fibonacci(90), we would need BigInteger. And there the add is linear,
>>                 meaning that the "fast linear" algorithm referred to here is probably
>>                 going to end up as "slow quadratic".
>>
>>                 To me, this example sends the completely wrong message. Let's take the
>>                 worst possible algorithm and parallelize it. Great. That means if we use
>>                 1000 processors, we can solve the problem of n+10 in the same time as n
>>                 with a single processor.
>>
>>                 I do realize this is meant to illustrate a point, but it doesn't do it
>>                 very well IME. I would like to propose to change this to a slightly
>>                 better example, for example a Factorial calculation:
>>
>>                 public class FactorialTask extends RecursiveTask<BigInteger> {
>>                      private final int from, to;
>>
>>                      public FactorialTask(int n) {
>>                          this(0, n);
>>                      }
>>
>>                      private FactorialTask(int from, int to) {
>>                          this.from = from;
>>                          this.to = to;
>>                      }
>>
>>                      protected BigInteger compute() {
>>                          if (from == to) {
>>                              if (from == 0) return BigInteger.ONE;
>>                              return BigInteger.valueOf(from);
>>                          }
>>                          int mid = (from + to) >>> 1;
>>                          FactorialTask leftTask = new FactorialTask(from, mid);
>>                          FactorialTask rightTask = new FactorialTask(mid + 1, to);
>>                          leftTask.fork();
>>                          BigInteger right = rightTask.invoke();
>>                          BigInteger left = leftTask.join();
>>                          return left.multiply(right);
>>                      }
>>                 }
>>
>>                 This is actually a *lot* faster than the stream version:
>>
>>                      public static BigInteger factorialStream(int n) {
>>                          return IntStream.rangeClosed(1, n)
>>                                  .mapToObj(BigInteger::valueOf)
>>                                  .reduce(BigInteger.ONE, BigInteger::multiply);
>>                      }
>>
>>                 (this has to do more with the algorithms used by BigInteger's multiply
>>                 method than the parallelization, but that also has an effect.
>>
>>
>>                 Alternatively, if we have to have Fibonacci, could we at least change it
>>                 to Dijkstra's Sum of Squares? I believe there are slightly better
>>                 algorithms, but this one works very nicely with parallelisation:
>>
>>                 public class FibonacciTask extends RecursiveTask<BigInteger> {
>>                      private final int n;
>>
>>                      public FibonacciTask(int n) {
>>                          this.n = n;
>>                      }
>>
>>                      @Override
>>                      protected BigInteger compute() {
>>                          return switch (n) {
>>                              case 0 -> BigInteger.ZERO;
>>                              case 1 -> BigInteger.ONE;
>>                              default -> {
>>                                  // Dijkstra's Sum of Squares Algorithm
>>                                  int half = (n + 1) / 2;
>>                                  FibonacciTask f0_task = new FibonacciTask(half - 1);
>>                                  f0_task.fork();
>>                                  FibonacciTask f1_task = new FibonacciTask(half);
>>                                  BigInteger f1 = f1_task.invoke();
>>                                  BigInteger f0 = f0_task.join();
>>
>>                                  if (n % 2 == 1) {
>>                                      yield f0.multiply(f0).add(f1.multiply(f1));
>>                                  } else {
>>                                      yield f0.shiftLeft(1).add(f1).multiply(f1);
>>                                  }
>>                              }
>>                          };
>>                      }
>>                 }
>>
>>                 Please let me know if you agree with this change (or propose a different
>>                 example). I would be happy to make the change. I presume it would need
>>                 to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>>                 and then we can sync that over to CVS? (My preference would be GitHub)
>>
>>
>>
>>
>>                 Regards
>>
>>                 Heinz
>>                 -- 
>>                 Dr Heinz M. Kabutz (PhD CompSci)
>>                 Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>>                 Java Champion - www.javachampions.org
>>                 JavaOne Rock Star Speaker
>>                 Tel: +30 69 75 595 262
>>                 Skype: kabutz
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest@cs.oswego.edu
>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


-------------------------

Subject: Re: [concurrency-interest] Better RecursiveTask Example
From: Alex Otenko <oleksandr.otenko@gmail.com>
Date: 2021-11-27, 14:28
To: "Dr Heinz M. Kabutz" <heinz@javaspecialists.eu>
CC: Margaret Figura via Concurrency-interest <concurrency-interest@cs.oswego.edu>

For the curious:

This is not all we can do here.

Half of even numbers are 2x(odd), and the other half are 2x(even). So we can break this down some more, and end up multiplying only odd numbers.

We can see that each sub-chain of odd numbers multiplies the same odd numbers, so we can also try to multiply the odd numbers only once. So we end up with only half the multiplications needed to compute the factorial. Unfortunately, the latter seems to not be faster - only burning less CPU.

Alex

On Fri, 26 Nov 2021, 08:54 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:

    I am curious what's going on with your 12x observation. 

    I may be wrong about why I observe the speedup, but my original thinking was to do that.

    Looking at the naive split of work, I was thinking: the left multiplies lots of small numbers, and the right multiplies lots of big numbers; that means the left is going to be done sooner, and some CPU will be idling. Of course, given that there will be a lot of tiny work created, it won't really be idling so much, so let's call it a hunch that it will have tendency to idle.

    So I thought let's split the work into even and odd numbers. Then left and right will be going through the numbers of similar magnitude, and bound to do similar amount of work. Then to keep splitting work, choose all divisible by 4 out of even, and those that aren't; then of those divisible by 4, half are divisible by 8, and half aren't, etc. Of course, no divisibility test is needed - just keep adding a step, so the procedure of splitting work is extensible to the odd numbers, too.

    If you notice, we actually end up multiplying a very small number and a very large one at first - say, 1 and 1 million. This is kind of against my premise that we multiply mostly equally sized numbers. But the idea is that the other branch is doing roughly the same - multiplying 2 and 1000001. This certainly is going to get us into Karatsuba range very quickly, too.

    So I really wonder why you are getting 12x worse numbers. If this isn't due to some additional debugging that wasn't commented out, I wonder if there is some strange fact about how the work gets split - eg do we get to juggle work that gets blocked more?

    Alex

    On Thu, 25 Nov 2021, 11:00 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:

        With my current simple algorithm, the magnitude of the two halves are similar. Whilst your algorithm is better in terms of keeping the two halves of closer size, it also complicates the demo program too much in my opinion. Your point should perhaps be added as a comment? A bigger concern to me would be the many tasks that we would fork. It would be better to have a depth threshold after which we stop forking. This is already mentioned in the comment.

        My algorithm for (2 * 1024 * 1024)! ends up like this:

        left size 2,733,860 bits, right size 2,746,476 bits
        left size 4,340,409 bits, right size 4,864,687 bits
        left size 5,062,576 bits, right size 5,191,086 bits
        left size 5,286,644 bits, right size 5,362,796 bits
        left size 5,426,123 bits, right size 5,480,336 bits
        left size 9,205,096 bits, right size 10,253,661 bits
        left size 10,649,440 bits, right size 10,906,459 bits
        left size 19,458,756 bits, right size 21,555,898 bits
        2097152 bits 41014654
        fjTime = 4798ms

        Your algorithm for the same input has an almost equal number of bits for the two numbers:

        left size 320,424 bits, right size 320,433 bits
        left size 640,852 bits, right size 640,861 bits
        left size 640,847 bits, right size 640,857 bits
        left size 1,281,703 bits, right size 1,281,713 bits
        left size 2,563,415 bits, right size 2,563,424 bits
        left size 5,126,828 bits, right size 5,126,839 bits
        left size 10,253,656 bits, right size 10,253,667 bits
        left size 20,507,332 bits, right size 20,507,322 bits
        2097152 bits 41014654
        fjTime = 69867ms

        However, mine also happens to be about 12x faster, but I suspect that has more to do with the computational time complexity than with Fork/Join. With BigInteger, we want to get to large numbers as quickly as possible, so that we can start using Karatsuba and Toom Cook 3.

        Regards

        Heinz
        -- 
        Dr Heinz M. Kabutz (PhD CompSci)
        Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
        Java Champion - www.javachampions.org
        JavaOne Rock Star Speaker
        Tel: +30 69 75 595 262
        Skype: kabutz

        On 2021/11/25 12:17, Alex Otenko wrote:
>         Do we want to also nudge the reader towards considering how to split tasks into equally sized, if possible? 
>
>         https://bit.ly/3HOH9Ca - factorial is, of course, better done by ensuring both branches multiply numbers of similar magnitude.
>
>         Alex
>
>         On Thu, 25 Nov 2021, 09:40 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:
>
>             Hmmm, yes. I lost focus there.
>
>             I think there are two different problems: seeing that it is Fibonacci, and seeing how recursion works. I am not sure how much importance to give to the former.
>
>
>             Alex
>
>             On Thu, 25 Nov 2021, 05:38 Dr Heinz M. Kabutz, <heinz@javaspecialists.eu> wrote:
>
>                 Hi Alex,
>
>                 my second example was a recursive logarithmic complexity Fibonacci. However, I do think that the logarithmic Fibonacci demos are too complicated for most readers to follow. But Factorial most people know.
>
>                 The parallel performance of the Factorial is limited by the final large numbers that need to be multiplied together, and this is (currently) happening in parallel. I've got a PR in the works to add parallelMultiply() to BigInteger: https://github.com/openjdk/jdk/pull/6409
>
>                 Regards
>
>                 Heinz
>                 -- 
>                 Dr Heinz M. Kabutz (PhD CompSci)
>                 Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>                 Java Champion - www.javachampions.org
>                 JavaOne Rock Star Speaker
>                 Tel: +30 69 75 595 262
>                 Skype: kabutz
>
>                 On 2021/11/25 01:30, Alex Otenko wrote:
>>                 I presume logarithmic cost Fibonacci is not considered, because there's little point doing it recursively? (Although can still show off parallel computations)
>>
>>                 https://bit.ly/3oVFeTD
>>
>>
>>                 Alex
>>
>>                 On Wed, 24 Nov 2021, 19:19 Dr Heinz M. Kabutz via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>                     Every time I see the example in RecursiveTask I have to cringe:
>>
>>                     https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/concurrent/RecursiveTask.html
>>
>>                     For a classic example, here is a task computing Fibonacci numbers:
>>
>>
>>                       class Fibonacci extends RecursiveTask<Integer> {
>>                         final int n;
>>                         Fibonacci(int n) { this.n = n; }
>>                         protected Integer compute() {
>>                           if (n <= 1)
>>                             return n;
>>                           Fibonacci f1 = new Fibonacci(n - 1);
>>                           f1.fork();
>>                           Fibonacci f2 = new Fibonacci(n - 2);
>>                           return f2.compute() + f1.join();
>>                         }
>>                       }
>>                     However, besides being a dumb way to compute Fibonacci functions (there
>>                     is a simple fast linear algorithm that you'd use in practice), this is
>>                     likely to perform poorly because the smallest subtasks are too small to
>>                     be worthwhile splitting up. Instead, as is the case for nearly all
>>                     fork/join applications, you'd pick some minimum granularity size (for
>>                     example 10 here) for which you always sequentially solve rather than
>>                     subdividing.
>>
>>
>>
>>                     Indeed, it is a dumb way to compute Fibonacci, but the "fast linear"
>>                     algorithm isn't fast either. Since we overflow even Long after about
>>                     fibonacci(90), we would need BigInteger. And there the add is linear,
>>                     meaning that the "fast linear" algorithm referred to here is probably
>>                     going to end up as "slow quadratic".
>>
>>                     To me, this example sends the completely wrong message. Let's take the
>>                     worst possible algorithm and parallelize it. Great. That means if we use
>>                     1000 processors, we can solve the problem of n+10 in the same time as n
>>                     with a single processor.
>>
>>                     I do realize this is meant to illustrate a point, but it doesn't do it
>>                     very well IME. I would like to propose to change this to a slightly
>>                     better example, for example a Factorial calculation:
>>
>>                     public class FactorialTask extends RecursiveTask<BigInteger> {
>>                          private final int from, to;
>>
>>                          public FactorialTask(int n) {
>>                              this(0, n);
>>                          }
>>
>>                          private FactorialTask(int from, int to) {
>>                              this.from = from;
>>                              this.to = to;
>>                          }
>>
>>                          protected BigInteger compute() {
>>                              if (from == to) {
>>                                  if (from == 0) return BigInteger.ONE;
>>                                  return BigInteger.valueOf(from);
>>                              }
>>                              int mid = (from + to) >>> 1;
>>                              FactorialTask leftTask = new FactorialTask(from, mid);
>>                              FactorialTask rightTask = new FactorialTask(mid + 1, to);
>>                              leftTask.fork();
>>                              BigInteger right = rightTask.invoke();
>>                              BigInteger left = leftTask.join();
>>                              return left.multiply(right);
>>                          }
>>                     }
>>
>>                     This is actually a *lot* faster than the stream version:
>>
>>                          public static BigInteger factorialStream(int n) {
>>                              return IntStream.rangeClosed(1, n)
>>                                      .mapToObj(BigInteger::valueOf)
>>                                      .reduce(BigInteger.ONE, BigInteger::multiply);
>>                          }
>>
>>                     (this has to do more with the algorithms used by BigInteger's multiply
>>                     method than the parallelization, but that also has an effect.
>>
>>
>>                     Alternatively, if we have to have Fibonacci, could we at least change it
>>                     to Dijkstra's Sum of Squares? I believe there are slightly better
>>                     algorithms, but this one works very nicely with parallelisation:
>>
>>                     public class FibonacciTask extends RecursiveTask<BigInteger> {
>>                          private final int n;
>>
>>                          public FibonacciTask(int n) {
>>                              this.n = n;
>>                          }
>>
>>                          @Override
>>                          protected BigInteger compute() {
>>                              return switch (n) {
>>                                  case 0 -> BigInteger.ZERO;
>>                                  case 1 -> BigInteger.ONE;
>>                                  default -> {
>>                                      // Dijkstra's Sum of Squares Algorithm
>>                                      int half = (n + 1) / 2;
>>                                      FibonacciTask f0_task = new FibonacciTask(half - 1);
>>                                      f0_task.fork();
>>                                      FibonacciTask f1_task = new FibonacciTask(half);
>>                                      BigInteger f1 = f1_task.invoke();
>>                                      BigInteger f0 = f0_task.join();
>>
>>                                      if (n % 2 == 1) {
>>                                          yield f0.multiply(f0).add(f1.multiply(f1));
>>                                      } else {
>>                                          yield f0.shiftLeft(1).add(f1).multiply(f1);
>>                                      }
>>                                  }
>>                              };
>>                          }
>>                     }
>>
>>                     Please let me know if you agree with this change (or propose a different
>>                     example). I would be happy to make the change. I presume it would need
>>                     to be done in the CVS? Or can I do it in the OpenJDK GitHub repository
>>                     and then we can sync that over to CVS? (My preference would be GitHub)
>>
>>
>>
>>
>>                     Regards
>>
>>                     Heinz
>>                     -- 
>>                     Dr Heinz M. Kabutz (PhD CompSci)
>>                     Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
>>                     Java Champion - www.javachampions.org
>>                     JavaOne Rock Star Speaker
>>                     Tel: +30 69 75 595 262
>>                     Skype: kabutz
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest@cs.oswego.edu
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
