From viktor.klang at gmail.com  Sun Oct  2 07:09:27 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 2 Oct 2016 13:09:27 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
Message-ID: <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>

On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> 100s of threads blocked on various locks is no different from 100s of
> callbacks waiting in the queue.
>

As I'm certain you already know this, so for the casual list reader, the
answer is "yes, and no." Thread wakeup latency tends to be in the thousands
of us, this is rarely the case for callbacks. Also, Threads tend to be a
much more finite resource than callbacks, so a problem which can be
expressed with N callbacks may not be expressible with the same N of
threads on the same machine.


>
> Callbacks require a different expression of the same problem. For example,
> local variables are no longer local variables - they become instance
> variables. Without language support for closures it becomes a nightmare.
>
> But most importantly, such common algorithmic constructs as loops and
> return from function, no longer look like that. A sequence of calls to
> potentially blocking methods turn into a pyramid of doom - just take a look
> at node.js projects.
>
> It’s a tradeoff.
>
> Alex
>
> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
>
> The completely less thread consuming alternative is call backs.   VMS
> used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It
> was a very performant and friendly way to allow small software modules to
> be written to do one thing and used as the call back for asynchronous
> system behaviors.  I wrote several symbionts which did serial connection
> scripting for connecting to printers remotely, process management for
> process reuse on PMDF to speed up mail flow through the queue, and other
> things interactive with the kernel to alter process priorities to provide
> longer term scheduling on a heavily loaded 750.  It was like heaven to have
> such a small amount of code (lots of functions, some global data).   I am
> just suggesting that this kind of thing was a major part of more than one
> programming environment.
>
> Having 100’s of threads blocked on various locks adds considerably to the
> overhead of scheduling but also complicates cache use and burdens the
> developer with looping and waiting in ways that increase bugs in code that
> should not exist.  I really feel that the kind of thing that
> CompletionStage provides to the developer is something that should of been
> more prevalent from the start.  It inverts programatic logic in some cases,
> but call backs really are the best way to react to asynchronous eventing in
> software.
>
> Gregg Wonderly
>
> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:
>
> My limited understanding is that the original API was
> only CompletableFuture and that CompletionStage introduced as a compromise.
> It did not appear to be an attempt to strictly follow an
> interface-implementation separation, e.g. collections. As you said #toCompletableFuture()
> may throw an UOE, which means some use-cases can't rely on CompletionState
> which limits its usefulness. In my case that would be an AsyncLoadingCache
> with a synchronous LoadingCache view. I think having to code that the
> resulting implementation would be worse if it called toCompletableFuture,
> caught the exception, and then adapted as you said.
>
> When the new future class was introduced it was stated,
>
> "In other words, we (j.u.c) are not now in a position to dictate a common
> interface for all SettableFuture, FutureValue, Promise, ListenableFuture,
> etc like APIs. And as we've seen, different audiences want/need different
> subsets of this API exposed as interfaces for their usages, and are in any
> case unlikely to want change all their existing interfaces. However, what
> we can do is provide a common underlying implementation that is as fast,
> scalable, space-conserving, carefully-specified, and reliable as possible.
> It should then be easy and attractive for others creating or reworking
> higher-level APIs to relay all functionality to the CompletableFuture
> implementation."  - Doug Lea, '12
>
> I've gradually come to terms using CF as part of an API and haven't
> experienced a downside yet.
>
> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> (Sorry to re-open this discussion)
>>
>> The separation of a read-only CompletionStage from CompletableFuture is
>> great.  I'm a fan of the scala style Promise/Future split as described in
>> http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
>> re-add (safe, read-only) blocking methods like join.  Java is not Node.js,
>> where there are no threads but there is a universal event loop.  Java
>> programmers are used to Future, where the *only* way to use a future's
>> value is to block waiting for it.  The existing CompletionStage methods are
>> a better scaling alternative to blocking all the time, but blocking is
>> almost always eventually necessary in Java.  For example, junit test
>> methods that start any asynchronous computation need to block until the
>> computation is done, before returning.
>>
>> As Viktor has pointed out, users can always implement blocking themselves
>> by writing
>>
>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
>> stage) {
>>         CompletableFuture<T> f = new CompletableFuture<>();
>>         stage.handle((T t, Throwable ex) -> {
>>                          if (ex != null) f.completeExceptionally(ex);
>>                          else f.complete(t);
>>                          return null;
>>                      });
>>         return f;
>>     }
>>
>>     static <T> T join(CompletionStage<T> stage) {
>>         return toCompletableFuture(stage).join();
>>     }
>>
>> but unlike Viktor, I think it's unreasonable to not provide this for
>> users (especially when we can do so more efficiently).  What is happening
>> instead is API providers not using CompletionStage as return values in
>> public APIs because of the lack of convenient blocking, and instead
>> returning CompletableFuture, which is a tragic software engineering failure.
>>
>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
>> from throwing UnsupportedOperationException, and implement join as:
>>
>>     public default T join() { return toCompletableFuture().join(); }
>>
>> There is a risk of multiple-inheritance conflict with Future if we add
>> e.g. isDone(), but there are no current plans to turn those Future methods
>> into default methods, and even if we did in some future release, it would
>> be only a source, not binary incompatibility, so far less serious.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161002/95c1c8fe/attachment.html>

From oleksandr.otenko at gmail.com  Sun Oct  2 09:03:11 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sun, 2 Oct 2016 14:03:11 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
Message-ID: <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>

Did you really mean thousands of microseconds (ie on the order of milliseconds)?

But my point actually was that without language support for things like closures, switching from threaded model to actors/callbacks is not cheap refactoring. You may get the benefits of reduced latency, but will it be measurable (like, if it’s thousands of nanoseconds, not microseconds, then for context switches to account for 1% of CPU time one needs to switch thousands of times a second - some people may even experience that level of contention), and will the switch of expression justify it (loops become tail recursion, etc).

The interviewing process shows engineers generally find it difficult to grasp. Your mileage may vary :)

Alex

> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> 
> 
> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> 100s of threads blocked on various locks is no different from 100s of callbacks waiting in the queue.
> 
> As I'm certain you already know this, so for the casual list reader, the answer is "yes, and no." Thread wakeup latency tends to be in the thousands of us, this is rarely the case for callbacks. Also, Threads tend to be a much more finite resource than callbacks, so a problem which can be expressed with N callbacks may not be expressible with the same N of threads on the same machine.
>  
> 
> Callbacks require a different expression of the same problem. For example, local variables are no longer local variables - they become instance variables. Without language support for closures it becomes a nightmare.
> 
> But most importantly, such common algorithmic constructs as loops and return from function, no longer look like that. A sequence of calls to potentially blocking methods turn into a pyramid of doom - just take a look at node.js projects.
> 
> It’s a tradeoff.
> 
> Alex
> 
>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>> wrote:
>> 
>> The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.
>> 
>> Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  
>> 
>> Gregg Wonderly
>> 
>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com <mailto:ben.manes at gmail.com>> wrote:
>>> 
>>> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
>>> 
>>> When the new future class was introduced it was stated,
>>> 
>>> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>> 
>>> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
>>> 
>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>>> (Sorry to re-open this discussion)
>>> 
>>> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
>>> 
>>> As Viktor has pointed out, users can always implement blocking themselves by writing
>>> 
>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>         stage.handle((T t, Throwable ex) -> {
>>>                          if (ex != null) f.completeExceptionally(ex);
>>>                          else f.complete(t);
>>>                          return null;
>>>                      });
>>>         return f;
>>>     }
>>> 
>>>     static <T> T join(CompletionStage<T> stage) {
>>>         return toCompletableFuture(stage).join();
>>>     }
>>> 
>>> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
>>> 
>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
>>> 
>>>     public default T join() { return toCompletableFuture().join(); }
>>> 
>>> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> 
> 
> -- 
> Cheers,
> √

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161002/10895d80/attachment-0001.html>

From viktor.klang at gmail.com  Mon Oct  3 09:57:31 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 3 Oct 2016 15:57:31 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
Message-ID: <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>

On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Did you really mean thousands of microseconds (ie on the order of
> milliseconds)?
>

Apologies, I misremembered, it's in the order of hundreds of us.


>
> But my point actually was that without language support for things like
> closures, switching from threaded model to actors/callbacks is not cheap
> refactoring. You may get the benefits of reduced latency, but will it be
> measurable (like, if it’s thousands of nanoseconds, not microseconds, then
> for context switches to account for 1% of CPU time one needs to switch
> thousands of times a second - some people may even experience that level of
> contention), and will the switch of expression justify it (loops become
> tail recursion, etc).
>

Having context-switches in the order of 100s of us means effectively
capping throughput to < 10k ops/s


>
> The interviewing process shows engineers generally find it difficult to
> grasp. Your mileage may vary :)
>

The iterviewing process shows engineers generally find threads and locks
difficult to grasp, yet it is baked into java.lang.Object ;)


>
> Alex
>
> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>
>
> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> 100s of threads blocked on various locks is no different from 100s of
>> callbacks waiting in the queue.
>>
>
> As I'm certain you already know this, so for the casual list reader, the
> answer is "yes, and no." Thread wakeup latency tends to be in the thousands
> of us, this is rarely the case for callbacks. Also, Threads tend to be a
> much more finite resource than callbacks, so a problem which can be
> expressed with N callbacks may not be expressible with the same N of
> threads on the same machine.
>
>
>>
>> Callbacks require a different expression of the same problem. For
>> example, local variables are no longer local variables - they become
>> instance variables. Without language support for closures it becomes a
>> nightmare.
>>
>> But most importantly, such common algorithmic constructs as loops and
>> return from function, no longer look like that. A sequence of calls to
>> potentially blocking methods turn into a pyramid of doom - just take a look
>> at node.js projects.
>>
>> It’s a tradeoff.
>>
>> Alex
>>
>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
>>
>> The completely less thread consuming alternative is call backs.   VMS
>> used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It
>> was a very performant and friendly way to allow small software modules to
>> be written to do one thing and used as the call back for asynchronous
>> system behaviors.  I wrote several symbionts which did serial connection
>> scripting for connecting to printers remotely, process management for
>> process reuse on PMDF to speed up mail flow through the queue, and other
>> things interactive with the kernel to alter process priorities to provide
>> longer term scheduling on a heavily loaded 750.  It was like heaven to have
>> such a small amount of code (lots of functions, some global data).   I am
>> just suggesting that this kind of thing was a major part of more than one
>> programming environment.
>>
>> Having 100’s of threads blocked on various locks adds considerably to the
>> overhead of scheduling but also complicates cache use and burdens the
>> developer with looping and waiting in ways that increase bugs in code that
>> should not exist.  I really feel that the kind of thing that
>> CompletionStage provides to the developer is something that should of been
>> more prevalent from the start.  It inverts programatic logic in some cases,
>> but call backs really are the best way to react to asynchronous eventing in
>> software.
>>
>> Gregg Wonderly
>>
>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:
>>
>> My limited understanding is that the original API was
>> only CompletableFuture and that CompletionStage introduced as a compromise.
>> It did not appear to be an attempt to strictly follow an
>> interface-implementation separation, e.g. collections. As you said #toCompletableFuture()
>> may throw an UOE, which means some use-cases can't rely on CompletionState
>> which limits its usefulness. In my case that would be an AsyncLoadingCache
>> with a synchronous LoadingCache view. I think having to code that the
>> resulting implementation would be worse if it called toCompletableFuture,
>> caught the exception, and then adapted as you said.
>>
>> When the new future class was introduced it was stated,
>>
>> "In other words, we (j.u.c) are not now in a position to dictate a common
>> interface for all SettableFuture, FutureValue, Promise, ListenableFuture,
>> etc like APIs. And as we've seen, different audiences want/need different
>> subsets of this API exposed as interfaces for their usages, and are in any
>> case unlikely to want change all their existing interfaces. However, what
>> we can do is provide a common underlying implementation that is as fast,
>> scalable, space-conserving, carefully-specified, and reliable as possible.
>> It should then be easy and attractive for others creating or reworking
>> higher-level APIs to relay all functionality to the CompletableFuture
>> implementation."  - Doug Lea, '12
>>
>> I've gradually come to terms using CF as part of an API and haven't
>> experienced a downside yet.
>>
>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
>> wrote:
>>
>>> (Sorry to re-open this discussion)
>>>
>>> The separation of a read-only CompletionStage from CompletableFuture is
>>> great.  I'm a fan of the scala style Promise/Future split as described in
>>> http://docs.scala-lang.org/overviews/core/futures.html, but: we need to
>>> re-add (safe, read-only) blocking methods like join.  Java is not Node.js,
>>> where there are no threads but there is a universal event loop.  Java
>>> programmers are used to Future, where the *only* way to use a future's
>>> value is to block waiting for it.  The existing CompletionStage methods are
>>> a better scaling alternative to blocking all the time, but blocking is
>>> almost always eventually necessary in Java.  For example, junit test
>>> methods that start any asynchronous computation need to block until the
>>> computation is done, before returning.
>>>
>>> As Viktor has pointed out, users can always implement blocking
>>> themselves by writing
>>>
>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
>>> stage) {
>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>         stage.handle((T t, Throwable ex) -> {
>>>                          if (ex != null) f.completeExceptionally(ex);
>>>                          else f.complete(t);
>>>                          return null;
>>>                      });
>>>         return f;
>>>     }
>>>
>>>     static <T> T join(CompletionStage<T> stage) {
>>>         return toCompletableFuture(stage).join();
>>>     }
>>>
>>> but unlike Viktor, I think it's unreasonable to not provide this for
>>> users (especially when we can do so more efficiently).  What is happening
>>> instead is API providers not using CompletionStage as return values in
>>> public APIs because of the lack of convenient blocking, and instead
>>> returning CompletableFuture, which is a tragic software engineering failure.
>>>
>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
>>> from throwing UnsupportedOperationException, and implement join as:
>>>
>>>     public default T join() { return toCompletableFuture().join(); }
>>>
>>> There is a risk of multiple-inheritance conflict with Future if we add
>>> e.g. isDone(), but there are no current plans to turn those Future methods
>>> into default methods, and even if we did in some future release, it would
>>> be only a source, not binary incompatibility, so far less serious.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> √
>
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161003/85f5eaf6/attachment-0001.html>

From aph at redhat.com  Mon Oct  3 10:05:37 2016
From: aph at redhat.com (Andrew Haley)
Date: Mon, 3 Oct 2016 15:05:37 +0100
Subject: [concurrency-interest] We need to add blocking methods to
 CompletionStage!
In-Reply-To: <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
Message-ID: <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>

On 03/10/16 14:57, Viktor Klang wrote:
> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
> 
>> Did you really mean thousands of microseconds (ie on the order of
>> milliseconds)?
>>
> 
> Apologies, I misremembered, it's in the order of hundreds of us.

That sounds like a seriously deficient system, unless there is some
swapping to disk going on.  I would expect a microsecond per system
call.  Simple inter-process communication without bothering the
kernel is about 50ns or so.

Andrew.

From vitalyd at gmail.com  Mon Oct  3 10:12:56 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 3 Oct 2016 10:12:56 -0400
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>
Message-ID: <CAHjP37H=ALrU5LAkk_2EteNooroV+O5CrudcyJFq4eWdOnRxPg@mail.gmail.com>

On Monday, October 3, 2016, Andrew Haley <aph at redhat.com> wrote:

> On 03/10/16 14:57, Viktor Klang wrote:
> > On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com
> <javascript:;>>
> > wrote:
> >
> >> Did you really mean thousands of microseconds (ie on the order of
> >> milliseconds)?
> >>
> >
> > Apologies, I misremembered, it's in the order of hundreds of us.
>
> That sounds like a seriously deficient system, unless there is some
> swapping to disk going on.  I would expect a microsecond per system
> call.  Simple inter-process communication without bothering the
> kernel is about 50ns or so.

Right, I was about to say something similar.

I think Viktor must be counting/including some sort of worst case where the
runnable thread cache misses in each access to bring its working/hot set
back into the caches.  Even then I don't think it'll be 100s of us.

>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <javascript:;>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161003/6109b1ef/attachment.html>

From oleksandr.otenko at gmail.com  Mon Oct  3 10:37:55 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Mon, 3 Oct 2016 15:37:55 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
Message-ID: <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>


> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> 
> 
> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> Did you really mean thousands of microseconds (ie on the order of milliseconds)?
> 
> Apologies, I misremembered, it's in the order of hundreds of us.
>  
> 
> But my point actually was that without language support for things like closures, switching from threaded model to actors/callbacks is not cheap refactoring. You may get the benefits of reduced latency, but will it be measurable (like, if it’s thousands of nanoseconds, not microseconds, then for context switches to account for 1% of CPU time one needs to switch thousands of times a second - some people may even experience that level of contention), and will the switch of expression justify it (loops become tail recursion, etc).
> 
> Having context-switches in the order of 100s of us means effectively capping throughput to < 10k ops/s

:) ...per thread

yes, there are cases like that.


> 
> The interviewing process shows engineers generally find it difficult to grasp. Your mileage may vary :)
> 
> The iterviewing process shows engineers generally find threads and locks difficult to grasp, yet it is baked into java.lang.Object ;)

:) sure! It’s good to have a powerful toolset. I was only commenting on the statement "how easy it is to use async”.

In reality suddenly people struggle to do simple non-concurrent things, like returning a value from a method (because you don’t return it anymore, but pass it to a continuation), or organizing a loop (hello, recursion and fixpoints), or comprehending what would be a method with 20 lines of code, because it turns into 5 functions 4 lines each, and passing a dozen arguments through all of them just because the last method in the chain of invocations needs it.


Alex



>  
> 
> Alex
> 
>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>> 
>> 
>> 
>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> 100s of threads blocked on various locks is no different from 100s of callbacks waiting in the queue.
>> 
>> As I'm certain you already know this, so for the casual list reader, the answer is "yes, and no." Thread wakeup latency tends to be in the thousands of us, this is rarely the case for callbacks. Also, Threads tend to be a much more finite resource than callbacks, so a problem which can be expressed with N callbacks may not be expressible with the same N of threads on the same machine.
>>  
>> 
>> Callbacks require a different expression of the same problem. For example, local variables are no longer local variables - they become instance variables. Without language support for closures it becomes a nightmare.
>> 
>> But most importantly, such common algorithmic constructs as loops and return from function, no longer look like that. A sequence of calls to potentially blocking methods turn into a pyramid of doom - just take a look at node.js projects.
>> 
>> It’s a tradeoff.
>> 
>> Alex
>> 
>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>> wrote:
>>> 
>>> The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.
>>> 
>>> Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  
>>> 
>>> Gregg Wonderly
>>> 
>>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com <mailto:ben.manes at gmail.com>> wrote:
>>>> 
>>>> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
>>>> 
>>>> When the new future class was introduced it was stated,
>>>> 
>>>> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>> 
>>>> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
>>>> 
>>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>>>> (Sorry to re-open this discussion)
>>>> 
>>>> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
>>>> 
>>>> As Viktor has pointed out, users can always implement blocking themselves by writing
>>>> 
>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>         stage.handle((T t, Throwable ex) -> {
>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>                          else f.complete(t);
>>>>                          return null;
>>>>                      });
>>>>         return f;
>>>>     }
>>>> 
>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>         return toCompletableFuture(stage).join();
>>>>     }
>>>> 
>>>> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
>>>> 
>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
>>>> 
>>>>     public default T join() { return toCompletableFuture().join(); }
>>>> 
>>>> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> 
>> 
>> -- 
>> Cheers,
>> √
> 
> 
> 
> 
> -- 
> Cheers,
> √

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161003/272fd672/attachment-0001.html>

From viktor.klang at gmail.com  Tue Oct  4 13:10:50 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 4 Oct 2016 19:10:50 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CAHjP37H=ALrU5LAkk_2EteNooroV+O5CrudcyJFq4eWdOnRxPg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>
 <CAHjP37H=ALrU5LAkk_2EteNooroV+O5CrudcyJFq4eWdOnRxPg@mail.gmail.com>
Message-ID: <CANPzfU8T5529-pd91uQVZ1g2NxoZQDJZ7XmWinc1SWH-LnkmSg@mail.gmail.com>

On Mon, Oct 3, 2016 at 4:12 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

>
>
> On Monday, October 3, 2016, Andrew Haley <aph at redhat.com> wrote:
>
>> On 03/10/16 14:57, Viktor Klang wrote:
>> > On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com
>> >
>> > wrote:
>> >
>> >> Did you really mean thousands of microseconds (ie on the order of
>> >> milliseconds)?
>> >>
>> >
>> > Apologies, I misremembered, it's in the order of hundreds of us.
>>
>> That sounds like a seriously deficient system, unless there is some
>> swapping to disk going on.  I would expect a microsecond per system
>> call.  Simple inter-process communication without bothering the
>> kernel is about 50ns or so.
>
> Right, I was about to say something similar.
>
> I think Viktor must be counting/including some sort of worst case where
> the runnable thread cache misses in each access to bring its working/hot
> set back into the caches.  Even then I don't think it'll be 100s of us.
>

Our numbers were for a Linux setup on a multi-socket server running
4x12cores of Opterons.

Unfortunately I cannot seem to find more details, but Martin Thompson
posted this for instance:
https://mechanical-sympathy.blogspot.com/2011/08/inter-thread-latency.html?showComment=1368795527233#c7971873839985553900


>
>> Andrew.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> Sent from my phone
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161004/b1de3932/attachment.html>

From viktor.klang at gmail.com  Tue Oct  4 13:12:52 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 4 Oct 2016 19:12:52 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
Message-ID: <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>

On Mon, Oct 3, 2016 at 4:37 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

>
> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>
>
> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> Did you really mean thousands of microseconds (ie on the order of
>> milliseconds)?
>>
>
> Apologies, I misremembered, it's in the order of hundreds of us.
>
>
>>
>> But my point actually was that without language support for things like
>> closures, switching from threaded model to actors/callbacks is not cheap
>> refactoring. You may get the benefits of reduced latency, but will it be
>> measurable (like, if it’s thousands of nanoseconds, not microseconds, then
>> for context switches to account for 1% of CPU time one needs to switch
>> thousands of times a second - some people may even experience that level of
>> contention), and will the switch of expression justify it (loops become
>> tail recursion, etc).
>>
>
> Having context-switches in the order of 100s of us means effectively
> capping throughput to < 10k ops/s
>
>
> :) ...per thread
>

No, I don't think it can be broken down like that, because typically wakeup
is not non-contended and without coherence cost.


>
> yes, there are cases like that.
>

Yes, unfortunately, I may add!


>
>
>
>> The interviewing process shows engineers generally find it difficult to
>> grasp. Your mileage may vary :)
>>
>
> The iterviewing process shows engineers generally find threads and locks
> difficult to grasp, yet it is baked into java.lang.Object ;)
>
>
> :) sure! It’s good to have a powerful toolset. I was only commenting on
> the statement "how easy it is to use async”.
>

Easy implies familiarity :)


>
> In reality suddenly people struggle to do simple non-concurrent things,
> like returning a value from a method (because you don’t return it anymore,
> but pass it to a continuation), or organizing a loop (hello, recursion and
> fixpoints), or comprehending what would be a method with 20 lines of code,
> because it turns into 5 functions 4 lines each, and passing a dozen
> arguments through all of them just because the last method in the chain of
> invocations needs it.
>

Not to mention that most humans don't get programming *at all*? :-)


>
>
> Alex
>
>
>
>
>
>>
>> Alex
>>
>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com> wrote:
>>
>>
>>
>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com>
>>  wrote:
>>
>>> 100s of threads blocked on various locks is no different from 100s of
>>> callbacks waiting in the queue.
>>>
>>
>> As I'm certain you already know this, so for the casual list reader, the
>> answer is "yes, and no." Thread wakeup latency tends to be in the thousands
>> of us, this is rarely the case for callbacks. Also, Threads tend to be a
>> much more finite resource than callbacks, so a problem which can be
>> expressed with N callbacks may not be expressible with the same N of
>> threads on the same machine.
>>
>>
>>>
>>> Callbacks require a different expression of the same problem. For
>>> example, local variables are no longer local variables - they become
>>> instance variables. Without language support for closures it becomes a
>>> nightmare.
>>>
>>> But most importantly, such common algorithmic constructs as loops and
>>> return from function, no longer look like that. A sequence of calls to
>>> potentially blocking methods turn into a pyramid of doom - just take a look
>>> at node.js projects.
>>>
>>> It’s a tradeoff.
>>>
>>> Alex
>>>
>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
>>>
>>> The completely less thread consuming alternative is call backs.   VMS
>>> used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It
>>> was a very performant and friendly way to allow small software modules to
>>> be written to do one thing and used as the call back for asynchronous
>>> system behaviors.  I wrote several symbionts which did serial connection
>>> scripting for connecting to printers remotely, process management for
>>> process reuse on PMDF to speed up mail flow through the queue, and other
>>> things interactive with the kernel to alter process priorities to provide
>>> longer term scheduling on a heavily loaded 750.  It was like heaven to have
>>> such a small amount of code (lots of functions, some global data).   I am
>>> just suggesting that this kind of thing was a major part of more than one
>>> programming environment.
>>>
>>> Having 100’s of threads blocked on various locks adds considerably to
>>> the overhead of scheduling but also complicates cache use and burdens the
>>> developer with looping and waiting in ways that increase bugs in code that
>>> should not exist.  I really feel that the kind of thing that
>>> CompletionStage provides to the developer is something that should of been
>>> more prevalent from the start.  It inverts programatic logic in some cases,
>>> but call backs really are the best way to react to asynchronous eventing in
>>> software.
>>>
>>> Gregg Wonderly
>>>
>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com> wrote:
>>>
>>> My limited understanding is that the original API was
>>> only CompletableFuture and that CompletionStage introduced as a compromise.
>>> It did not appear to be an attempt to strictly follow an
>>> interface-implementation separation, e.g. collections. As you said #toCompletableFuture()
>>> may throw an UOE, which means some use-cases can't rely on CompletionState
>>> which limits its usefulness. In my case that would be an AsyncLoadingCache
>>> with a synchronous LoadingCache view. I think having to code that the
>>> resulting implementation would be worse if it called toCompletableFuture,
>>> caught the exception, and then adapted as you said.
>>>
>>> When the new future class was introduced it was stated,
>>>
>>> "In other words, we (j.u.c) are not now in a position to dictate a
>>> common interface for all SettableFuture, FutureValue, Promise,
>>> ListenableFuture, etc like APIs. And as we've seen, different audiences
>>> want/need different subsets of this API exposed as interfaces for their
>>> usages, and are in any case unlikely to want change all their existing
>>> interfaces. However, what we can do is provide a common underlying
>>> implementation that is as fast, scalable, space-conserving,
>>> carefully-specified, and reliable as possible. It should then be easy and
>>> attractive for others creating or reworking higher-level APIs to relay all
>>> functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>
>>> I've gradually come to terms using CF as part of an API and haven't
>>> experienced a downside yet.
>>>
>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
>>> wrote:
>>>
>>>> (Sorry to re-open this discussion)
>>>>
>>>> The separation of a read-only CompletionStage from CompletableFuture is
>>>> great.  I'm a fan of the scala style Promise/Future split as described in
>>>> http://docs.scala-lang.org/overviews/core/futures.html, but: we need
>>>> to re-add (safe, read-only) blocking methods like join.  Java is not
>>>> Node.js, where there are no threads but there is a universal event loop.
>>>> Java programmers are used to Future, where the *only* way to use a future's
>>>> value is to block waiting for it.  The existing CompletionStage methods are
>>>> a better scaling alternative to blocking all the time, but blocking is
>>>> almost always eventually necessary in Java.  For example, junit test
>>>> methods that start any asynchronous computation need to block until the
>>>> computation is done, before returning.
>>>>
>>>> As Viktor has pointed out, users can always implement blocking
>>>> themselves by writing
>>>>
>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
>>>> stage) {
>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>         stage.handle((T t, Throwable ex) -> {
>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>                          else f.complete(t);
>>>>                          return null;
>>>>                      });
>>>>         return f;
>>>>     }
>>>>
>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>         return toCompletableFuture(stage).join();
>>>>     }
>>>>
>>>> but unlike Viktor, I think it's unreasonable to not provide this for
>>>> users (especially when we can do so more efficiently).  What is happening
>>>> instead is API providers not using CompletionStage as return values in
>>>> public APIs because of the lack of convenient blocking, and instead
>>>> returning CompletableFuture, which is a tragic software engineering failure.
>>>>
>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
>>>> from throwing UnsupportedOperationException, and implement join as:
>>>>
>>>>     public default T join() { return toCompletableFuture().join(); }
>>>>
>>>> There is a risk of multiple-inheritance conflict with Future if we add
>>>> e.g. isDone(), but there are no current plans to turn those Future methods
>>>> into default methods, and even if we did in some future release, it would
>>>> be only a source, not binary incompatibility, so far less serious.
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Cheers,
>> √
>>
>>
>>
>
>
> --
> Cheers,
> √
>
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161004/2ec7e9d3/attachment-0001.html>

From aph at redhat.com  Tue Oct  4 13:21:52 2016
From: aph at redhat.com (Andrew Haley)
Date: Tue, 4 Oct 2016 18:21:52 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU8T5529-pd91uQVZ1g2NxoZQDJZ7XmWinc1SWH-LnkmSg@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>
 <CAHjP37H=ALrU5LAkk_2EteNooroV+O5CrudcyJFq4eWdOnRxPg@mail.gmail.com>
 <CANPzfU8T5529-pd91uQVZ1g2NxoZQDJZ7XmWinc1SWH-LnkmSg@mail.gmail.com>
Message-ID: <26f6f303-506c-a50d-1179-73f90b213f25@redhat.com>

On 04/10/16 18:10, Viktor Klang wrote:
> On Mon, Oct 3, 2016 at 4:12 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
>>
>>
>> On Monday, October 3, 2016, Andrew Haley <aph at redhat.com> wrote:
>>
>>> On 03/10/16 14:57, Viktor Klang wrote:
>>>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com
>>>>
>>>> wrote:
>>>>
>>>>> Did you really mean thousands of microseconds (ie on the order of
>>>>> milliseconds)?
>>>>>
>>>>
>>>> Apologies, I misremembered, it's in the order of hundreds of us.
>>>
>>> That sounds like a seriously deficient system, unless there is some
>>> swapping to disk going on.  I would expect a microsecond per system
>>> call.  Simple inter-process communication without bothering the
>>> kernel is about 50ns or so.
>>
>> Right, I was about to say something similar.
>>
>> I think Viktor must be counting/including some sort of worst case where
>> the runnable thread cache misses in each access to bring its working/hot
>> set back into the caches.  Even then I don't think it'll be 100s of us.
>>
> 
> Our numbers were for a Linux setup on a multi-socket server running
> 4x12cores of Opterons.
> 
> Unfortunately I cannot seem to find more details, but Martin Thompson
> posted this for instance:
> https://mechanical-sympathy.blogspot.com/2011/08/inter-thread-latency.html?showComment=1368795527233#c7971873839985553900

Which says

   However if you wish to operate in a low-latency environment then
   busy spinning is often the only option. I've seen the cost to wake
   up a thread often be up ~16us on some versions of Linux and even
   into the milliseconds when virtualised.

I can believe that: virtualization can do some very weird things.
16us is pretty horrible, though.  As the questioner says, "it cost
about 2 to 5 microseconds to wake up a thread", which is what I
believe.  I occasionally nag kernel architects about this: I've never
received any really convincing explanation about why it has to be as
bad as it is.

Andrew.

From viktor.klang at gmail.com  Tue Oct  4 13:59:15 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 4 Oct 2016 19:59:15 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <26f6f303-506c-a50d-1179-73f90b213f25@redhat.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <d9df96ff-3a0f-6434-05e4-e8873ae8378b@redhat.com>
 <CAHjP37H=ALrU5LAkk_2EteNooroV+O5CrudcyJFq4eWdOnRxPg@mail.gmail.com>
 <CANPzfU8T5529-pd91uQVZ1g2NxoZQDJZ7XmWinc1SWH-LnkmSg@mail.gmail.com>
 <26f6f303-506c-a50d-1179-73f90b213f25@redhat.com>
Message-ID: <CANPzfU8HF0y67CCK3YCSOocYmO0gEa76NaCRjz1g7JK4YQDhFw@mail.gmail.com>

On Oct 4, 2016 12:21 PM, "Andrew Haley" <aph at redhat.com> wrote:
>
> On 04/10/16 18:10, Viktor Klang wrote:
> > On Mon, Oct 3, 2016 at 4:12 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:
> >
> >>
> >>
> >> On Monday, October 3, 2016, Andrew Haley <aph at redhat.com> wrote:
> >>
> >>> On 03/10/16 14:57, Viktor Klang wrote:
> >>>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <
oleksandr.otenko at gmail.com
> >>>>
> >>>> wrote:
> >>>>
> >>>>> Did you really mean thousands of microseconds (ie on the order of
> >>>>> milliseconds)?
> >>>>>
> >>>>
> >>>> Apologies, I misremembered, it's in the order of hundreds of us.
> >>>
> >>> That sounds like a seriously deficient system, unless there is some
> >>> swapping to disk going on.  I would expect a microsecond per system
> >>> call.  Simple inter-process communication without bothering the
> >>> kernel is about 50ns or so.
> >>
> >> Right, I was about to say something similar.
> >>
> >> I think Viktor must be counting/including some sort of worst case where
> >> the runnable thread cache misses in each access to bring its
working/hot
> >> set back into the caches.  Even then I don't think it'll be 100s of us.
> >>
> >
> > Our numbers were for a Linux setup on a multi-socket server running
> > 4x12cores of Opterons.
> >
> > Unfortunately I cannot seem to find more details, but Martin Thompson
> > posted this for instance:
> >
https://mechanical-sympathy.blogspot.com/2011/08/inter-thread-latency.html?showComment=1368795527233#c7971873839985553900
>
> Which says
>
>    However if you wish to operate in a low-latency environment then
>    busy spinning is often the only option. I've seen the cost to wake
>    up a thread often be up ~16us on some versions of Linux and even
>    into the milliseconds when virtualised.

And also: "On top of the wakeup cost you must also consider the cache
pollution and thread migration that results."

Specifically on multisocket machines it can be quite pricey.

>
> I can believe that: virtualization can do some very weird things.
> 16us is pretty horrible, though.  As the questioner says, "it cost
> about 2 to 5 microseconds to wake up a thread", which is what I
> believe.  I occasionally nag kernel architects about this: I've never
> received any really convincing explanation about why it has to be as
> bad as it is.
>
> Andrew.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161004/1ddb0e03/attachment.html>

From forax at univ-mlv.fr  Tue Oct  4 16:32:01 2016
From: forax at univ-mlv.fr (Remi Forax)
Date: Tue, 4 Oct 2016 22:32:01 +0200 (CEST)
Subject: [concurrency-interest] Deprecate all
	java.util.concurrent.*FieldUpdater
Message-ID: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>

Given that Java 9 introduces a faster way to emit things like compareAndSet by using the VarHandke API and that AtomiReference (and likes) are now rewritten to use VarHandles directly,
i think it's time to deprecate all *FieldUpdater with something saying that they have been superseded by the VarHandle API.

Rémi
substitute dr deprecator

From martinrb at google.com  Tue Oct  4 17:19:33 2016
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 4 Oct 2016 14:19:33 -0700
Subject: [concurrency-interest] Deprecate all
	java.util.concurrent.*FieldUpdater
In-Reply-To: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
Message-ID: <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>

VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
complete (where is accumulateAndGet?), and when do you deprecate something
when the replacement won't be ubiquitous for 10 years?

On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> Given that Java 9 introduces a faster way to emit things like
> compareAndSet by using the VarHandke API and that AtomiReference (and
> likes) are now rewritten to use VarHandles directly,
> i think it's time to deprecate all *FieldUpdater with something saying
> that they have been superseded by the VarHandle API.
>
> Rémi
> substitute dr deprecator
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161004/b65e573d/attachment.html>

From aph at redhat.com  Wed Oct  5 04:19:26 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 5 Oct 2016 09:19:26 +0100
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
Message-ID: <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>

On 04/10/16 22:19, Martin Buchholz wrote:
> VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
> complete (where is accumulateAndGet?), and when do you deprecate something
> when the replacement won't be ubiquitous for 10 years?

Surely you have to start somewhere: deprecation is no more than saying
to programmers "Don't use this, use that."  And if you were leaning
over someone's shoulder that's what you would say.

Andrew.



From jsampson at guidewire.com  Wed Oct  5 10:39:10 2016
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 5 Oct 2016 14:39:10 +0000
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>
Message-ID: <BN3PR05MB25162EBB74D923EF9EA05884D1C40@BN3PR05MB2516.namprd05.prod.outlook.com>

Deprecation is stronger than that -- it says "we're going to remove this, or we wish we could remove it because it's broken, so you'd better change your code a.s.a.p." -- e.g. Thread.stop(). My interpretation of Martin's comment is that using deprecation for things that aren't actually broken just means that people will live with lots of deprecation warnings in their code for years to come. This could be a perfect case for introducing a weaker alternative to deprecation, saying "here's something better that you should really be using for new code" -- e.g. ArrayList vs. Vector. I remember the Guava team talking about that a lot. Don't know if they ever implemented it.

Cheers,
Justin


-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Andrew Haley
Sent: Wednesday, October 05, 2016 1:19 AM
To: Martin Buchholz; Remi Forax; Paul Sandoz
Cc: core-libs-dev; concurrency-interest
Subject: Re: [concurrency-interest] Deprecate all java.util.concurrent.*FieldUpdater

On 04/10/16 22:19, Martin Buchholz wrote:
> VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
> complete (where is accumulateAndGet?), and when do you deprecate something
> when the replacement won't be ubiquitous for 10 years?

Surely you have to start somewhere: deprecation is no more than saying
to programmers "Don't use this, use that."  And if you were leaning
over someone's shoulder that's what you would say.

Andrew.


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From forax at univ-mlv.fr  Wed Oct  5 10:39:07 2016
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 05 Oct 2016 14:39:07 +0000
Subject: [concurrency-interest] Deprecate all
	java.util.concurrent.*FieldUpdater
In-Reply-To: <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
Message-ID: <73C87358-90FB-4C00-9CC8-78C43360FFE5@univ-mlv.fr>

Hi Martin,

On October 4, 2016 11:19:33 PM GMT+02:00, Martin Buchholz <martinrb at google.com> wrote:
>VarHandle is a reasonable replacement for FieldUpdaters, but it's not
>yet
>complete (where is accumulateAndGet?), 

Seems to be a feature for me :)
I've never liked the methods that takes a lambda in FieldUpdater. If you're using a FieldUpdater, you are in the basement, you want a reliable performance model. The JLS does not guarantee that a side effect free lambda will be always inlined.

>and when do you deprecate
>something
>when the replacement won't be ubiquitous for 10 years?

The right answer is the one from Andrew but i can not resisrt, here is my answer:
when you hope that it will not take 10 years to be ubiquitous.

regards,
Remi

>
>On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>> Given that Java 9 introduces a faster way to emit things like
>> compareAndSet by using the VarHandke API and that AtomiReference (and
>> likes) are now rewritten to use VarHandles directly,
>> i think it's time to deprecate all *FieldUpdater with something
>saying
>> that they have been superseded by the VarHandle API.
>>
>> Rémi
>> substitute dr deprecator
>>

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.

From david.lloyd at redhat.com  Wed Oct  5 11:02:57 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 5 Oct 2016 10:02:57 -0500
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
Message-ID: <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>

I'm sure I recall an email from the past few months which proposed that 
*FieldUpdater are still going to be recommended in many cases over 
VarHandle because the latter is probably too low-level for casual uses. 
It was (IIRC) an argument in favor of more advanced fence methods or 
something like that.

Am I imagining it?

On 10/04/2016 04:19 PM, Martin Buchholz wrote:
> VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
> complete (where is accumulateAndGet?), and when do you deprecate something
> when the replacement won't be ubiquitous for 10 years?
>
> On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>> Given that Java 9 introduces a faster way to emit things like
>> compareAndSet by using the VarHandke API and that AtomiReference (and
>> likes) are now rewritten to use VarHandles directly,
>> i think it's time to deprecate all *FieldUpdater with something saying
>> that they have been superseded by the VarHandle API.
>>
>> Rémi
>> substitute dr deprecator
>>

-- 
- DML

From aaron.grunthal at infinite-source.de  Wed Oct  5 11:36:41 2016
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Wed, 5 Oct 2016 17:36:41 +0200
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>
Message-ID: <f625e3d8-1f56-e30c-5e3f-571df5026441@infinite-source.de>

Maybe a @see on each method to inform the reader about varhandle
alternatives without deprecation?

On 05.10.2016 17:02, David M. Lloyd wrote:
> I'm sure I recall an email from the past few months which proposed that
> *FieldUpdater are still going to be recommended in many cases over
> VarHandle because the latter is probably too low-level for casual uses.
> It was (IIRC) an argument in favor of more advanced fence methods or
> something like that.
> 
> Am I imagining it?
> 
> On 10/04/2016 04:19 PM, Martin Buchholz wrote:
>> VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
>> complete (where is accumulateAndGet?), and when do you deprecate
>> something
>> when the replacement won't be ubiquitous for 10 years?
>>
>> On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>>
>>> Given that Java 9 introduces a faster way to emit things like
>>> compareAndSet by using the VarHandke API and that AtomiReference (and
>>> likes) are now rewritten to use VarHandles directly,
>>> i think it's time to deprecate all *FieldUpdater with something saying
>>> that they have been superseded by the VarHandle API.
>>>
>>> Rémi
>>> substitute dr deprecator
>>>
> 


From aph at redhat.com  Wed Oct  5 11:38:22 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 5 Oct 2016 16:38:22 +0100
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <BN3PR05MB25162EBB74D923EF9EA05884D1C40@BN3PR05MB2516.namprd05.prod.outlook.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>
 <BN3PR05MB25162EBB74D923EF9EA05884D1C40@BN3PR05MB2516.namprd05.prod.outlook.com>
Message-ID: <ca8a9ae0-fede-1385-6618-271aad22c193@redhat.com>

On 05/10/16 15:39, Justin Sampson wrote:

> Deprecation is stronger than that -- it says "we're going to remove
> this, or we wish we could remove it because it's broken, so you'd
> better change your code a.s.a.p." -- e.g. Thread.stop().

We're moving Java to support some new ways of working, and these
changes will inevitably mean that some parts of the project will be
obsolescent.  We need to be able to flag the old ways of doing things
in some way.

Deprecation is a way to do that.  I don't think that Thread.stop() is
a typical case.

> My interpretation of Martin's comment is that using deprecation for
> things that aren't actually broken just means that people will live
> with lots of deprecation warnings in their code for years to
> come. This could be a perfect case for introducing a weaker
> alternative to deprecation, saying "here's something better that you
> should really be using for new code" -- e.g. ArrayList vs. Vector. I
> remember the Guava team talking about that a lot. Don't know if they
> ever implemented it.

OK.  But we really do need a way to do that.

Andrew.

From martinrb at google.com  Wed Oct  5 12:11:36 2016
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 5 Oct 2016 09:11:36 -0700
Subject: [concurrency-interest] Deprecate all
	java.util.concurrent.*FieldUpdater
In-Reply-To: <f625e3d8-1f56-e30c-5e3f-571df5026441@infinite-source.de>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>
 <f625e3d8-1f56-e30c-5e3f-571df5026441@infinite-source.de>
Message-ID: <CA+kOe0-fHrDpJJ1CdryKVq95iosyxfT3-GOMjXAR6Qq_JkBocA@mail.gmail.com>

We already added the gentlest deprecation of all of j.u.c.atomic in the
package-info:

Instances of Atomic classes
 * maintain values that are accessed and updated using methods
 * otherwise available for fields using associated atomic {@link
 * java.lang.invoke.VarHandle} operations.

I'm generally negative on deprecation of stuff that works perfectly well in
favor of other stuff that, even though it is clearly better in some way, is
not as ubiquitous and hence less portable/compatible.  Better compatibility
trumps most other kinds of "better" for most users.



On Wed, Oct 5, 2016 at 8:36 AM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Maybe a @see on each method to inform the reader about varhandle
> alternatives without deprecation?
>
> On 05.10.2016 17:02, David M. Lloyd wrote:
> > I'm sure I recall an email from the past few months which proposed that
> > *FieldUpdater are still going to be recommended in many cases over
> > VarHandle because the latter is probably too low-level for casual uses.
> > It was (IIRC) an argument in favor of more advanced fence methods or
> > something like that.
> >
> > Am I imagining it?
> >
> > On 10/04/2016 04:19 PM, Martin Buchholz wrote:
> >> VarHandle is a reasonable replacement for FieldUpdaters, but it's not
> yet
> >> complete (where is accumulateAndGet?), and when do you deprecate
> >> something
> >> when the replacement won't be ubiquitous for 10 years?
> >>
> >> On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr> wrote:
> >>
> >>> Given that Java 9 introduces a faster way to emit things like
> >>> compareAndSet by using the VarHandke API and that AtomiReference (and
> >>> likes) are now rewritten to use VarHandles directly,
> >>> i think it's time to deprecate all *FieldUpdater with something saying
> >>> that they have been superseded by the VarHandle API.
> >>>
> >>> Rémi
> >>> substitute dr deprecator
> >>>
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161005/b9f4dfd9/attachment.html>

From aaron.grunthal at infinite-source.de  Wed Oct  5 12:51:57 2016
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Wed, 5 Oct 2016 18:51:57 +0200
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <CA+kOe0-fHrDpJJ1CdryKVq95iosyxfT3-GOMjXAR6Qq_JkBocA@mail.gmail.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>
 <f625e3d8-1f56-e30c-5e3f-571df5026441@infinite-source.de>
 <CA+kOe0-fHrDpJJ1CdryKVq95iosyxfT3-GOMjXAR6Qq_JkBocA@mail.gmail.com>
Message-ID: <6f3bd4d7-1895-30b8-ecf5-c97a4fb2f6c4@infinite-source.de>

In my experience users rarely read the package-info compared to class or
method infos. I guess the way IDEs display javadocs should be blamed for
that.

And they're even less likely to notice changes to those even if they
have read them in the past.

The attention ranking is members > class >> package, simply because the
way content-assist with automatic javadoc display works. Members are
most frequently accessed when typing, classes only when declaring
variables, packages get inserted automatically so you never focus on
those and thus don't see the javadocs.

Maybe IDE projects could be convinced to stack the member/class/package
hierarchy in a single view so they get equal visibility?


Also, the "using methods otherwise available" phrasing seems a bit weak
to me. I could read it as the field updaters as abstraction over the
varhandles (which they are) and conclude from that that i don't have to
concern myself with those details.
It does nothing to encourage the user to at least explore them as
alternative.



On 05.10.2016 18:11, Martin Buchholz wrote:
> We already added the gentlest deprecation of all of j.u.c.atomic in the
> package-info:
> 
> Instances of Atomic classes
>  * maintain values that are accessed and updated using methods
>  * otherwise available for fields using associated atomic {@link
>  * java.lang.invoke.VarHandle} operations.
> 
> I'm generally negative on deprecation of stuff that works perfectly well
> in favor of other stuff that, even though it is clearly better in some
> way, is not as ubiquitous and hence less portable/compatible.  Better
> compatibility trumps most other kinds of "better" for most users.
> 
> 
> 
> On Wed, Oct 5, 2016 at 8:36 AM, Aaron Grunthal
> <aaron.grunthal at infinite-source.de
> <mailto:aaron.grunthal at infinite-source.de>> wrote:
> 
>     Maybe a @see on each method to inform the reader about varhandle
>     alternatives without deprecation?
> 
>     On 05.10.2016 17:02, David M. Lloyd wrote:
>     > I'm sure I recall an email from the past few months which proposed that
>     > *FieldUpdater are still going to be recommended in many cases over
>     > VarHandle because the latter is probably too low-level for casual uses.
>     > It was (IIRC) an argument in favor of more advanced fence methods or
>     > something like that.
>     >
>     > Am I imagining it?
>     >
>     > On 10/04/2016 04:19 PM, Martin Buchholz wrote:
>     >> VarHandle is a reasonable replacement for FieldUpdaters, but it's not yet
>     >> complete (where is accumulateAndGet?), and when do you deprecate
>     >> something
>     >> when the replacement won't be ubiquitous for 10 years?
>     >>
>     >> On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>> wrote:
>     >>
>     >>> Given that Java 9 introduces a faster way to emit things like
>     >>> compareAndSet by using the VarHandke API and that AtomiReference (and
>     >>> likes) are now rewritten to use VarHandles directly,
>     >>> i think it's time to deprecate all *FieldUpdater with something saying
>     >>> that they have been superseded by the VarHandle API.
>     >>>
>     >>> Rémi
>     >>> substitute dr deprecator
>     >>>
>     >
> 
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 


From vladimir.x.ivanov at oracle.com  Wed Oct  5 12:55:00 2016
From: vladimir.x.ivanov at oracle.com (Vladimir Ivanov)
Date: Wed, 5 Oct 2016 19:55:00 +0300
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <ca8a9ae0-fede-1385-6618-271aad22c193@redhat.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>
 <BN3PR05MB25162EBB74D923EF9EA05884D1C40@BN3PR05MB2516.namprd05.prod.outlook.com>
 <ca8a9ae0-fede-1385-6618-271aad22c193@redhat.com>
Message-ID: <5dab5b50-bf80-0566-9a4a-f09b5523a092@oracle.com>

>> My interpretation of Martin's comment is that using deprecation for
>> things that aren't actually broken just means that people will live
>> with lots of deprecation warnings in their code for years to
>> come. This could be a perfect case for introducing a weaker
>> alternative to deprecation, saying "here's something better that you
>> should really be using for new code" -- e.g. ArrayList vs. Vector. I
>> remember the Guava team talking about that a lot. Don't know if they
>> ever implemented it.
>
> OK.  But we really do need a way to do that.
Doesn't enhanced deprecation [1] in 9 cover that?

Best regards,
Vladimir Ivanov

[1] http://openjdk.java.net/jeps/277

From aph at redhat.com  Wed Oct  5 13:04:25 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 5 Oct 2016 18:04:25 +0100
Subject: [concurrency-interest] Deprecate all
 java.util.concurrent.*FieldUpdater
In-Reply-To: <5dab5b50-bf80-0566-9a4a-f09b5523a092@oracle.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <148dc564-1632-ecb1-8f65-05de407cfbfd@redhat.com>
 <BN3PR05MB25162EBB74D923EF9EA05884D1C40@BN3PR05MB2516.namprd05.prod.outlook.com>
 <ca8a9ae0-fede-1385-6618-271aad22c193@redhat.com>
 <5dab5b50-bf80-0566-9a4a-f09b5523a092@oracle.com>
Message-ID: <64dfab7d-a4d7-e096-a53a-dd9f4cc603e0@redhat.com>

On 05/10/16 17:55, Vladimir Ivanov wrote:
>>> My interpretation of Martin's comment is that using deprecation for
>>> things that aren't actually broken just means that people will live
>>> with lots of deprecation warnings in their code for years to
>>> come. This could be a perfect case for introducing a weaker
>>> alternative to deprecation, saying "here's something better that you
>>> should really be using for new code" -- e.g. ArrayList vs. Vector. I
>>> remember the Guava team talking about that a lot. Don't know if they
>>> ever implemented it.
>>
>> OK.  But we really do need a way to do that.
> Doesn't enhanced deprecation [1] in 9 cover that?

I can't tell.  As far as I can see there is no annotation which
covers exactly our case:

    the API is flawed and is impractical to fix,

    usage of the API is likely to lead to errors,

    the API has been superseded by another API,

    the API is obsolete,

    the API is experimental and is subject to incompatible changes,

    or any combination of the above.

we'd perhaps need to add

    this method over here is better in most cases

...which perhaps implies obsolescent rather than obsolete.

Andrew.


From forax at univ-mlv.fr  Wed Oct  5 14:23:54 2016
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 05 Oct 2016 18:23:54 +0000
Subject: [concurrency-interest] Deprecate
	all	java.util.concurrent.*FieldUpdater
In-Reply-To: <CA+kOe0-fHrDpJJ1CdryKVq95iosyxfT3-GOMjXAR6Qq_JkBocA@mail.gmail.com>
References: <1761941304.1037912.1475613121083.JavaMail.zimbra@u-pem.fr>
 <CA+kOe090sCv5v8MukaESSj75ZtaDfQVUXk+XGUFuaPfjPXfVCw@mail.gmail.com>
 <e190c44a-448f-f543-85c5-67f3a9d9dc29@redhat.com>
 <f625e3d8-1f56-e30c-5e3f-571df5026441@infinite-source.de>
 <CA+kOe0-fHrDpJJ1CdryKVq95iosyxfT3-GOMjXAR6Qq_JkBocA@mail.gmail.com>
Message-ID: <4C0DF76E-4EBD-49A9-B60B-8119DAB77F6B@univ-mlv.fr>

FieldUpdaters do not work well, at least for me !

War story/Anectode:
Updaters do unnecessary runtime casts that may lead to a deoptimization storm. I've spend several weeks in 2006/2007 on the core part of a language runtime trying to avoid that issue until I use unsafe and the problem disappear. 
I've only understood what was the root cause when Paul starts to develop the VarHandle API.

Remi


On October 5, 2016 6:11:36 PM GMT+02:00, Martin Buchholz <martinrb at google.com> wrote:
>We already added the gentlest deprecation of all of j.u.c.atomic in the
>package-info:
>
>Instances of Atomic classes
> * maintain values that are accessed and updated using methods
> * otherwise available for fields using associated atomic {@link
> * java.lang.invoke.VarHandle} operations.
>
>I'm generally negative on deprecation of stuff that works perfectly
>well in
>favor of other stuff that, even though it is clearly better in some
>way, is
>not as ubiquitous and hence less portable/compatible.  Better
>compatibility
>trumps most other kinds of "better" for most users.
>
>
>
>On Wed, Oct 5, 2016 at 8:36 AM, Aaron Grunthal <
>aaron.grunthal at infinite-source.de> wrote:
>
>> Maybe a @see on each method to inform the reader about varhandle
>> alternatives without deprecation?
>>
>> On 05.10.2016 17:02, David M. Lloyd wrote:
>> > I'm sure I recall an email from the past few months which proposed
>that
>> > *FieldUpdater are still going to be recommended in many cases over
>> > VarHandle because the latter is probably too low-level for casual
>uses.
>> > It was (IIRC) an argument in favor of more advanced fence methods
>or
>> > something like that.
>> >
>> > Am I imagining it?
>> >
>> > On 10/04/2016 04:19 PM, Martin Buchholz wrote:
>> >> VarHandle is a reasonable replacement for FieldUpdaters, but it's
>not
>> yet
>> >> complete (where is accumulateAndGet?), and when do you deprecate
>> >> something
>> >> when the replacement won't be ubiquitous for 10 years?
>> >>
>> >> On Tue, Oct 4, 2016 at 1:32 PM, Remi Forax <forax at univ-mlv.fr>
>wrote:
>> >>
>> >>> Given that Java 9 introduces a faster way to emit things like
>> >>> compareAndSet by using the VarHandke API and that AtomiReference
>(and
>> >>> likes) are now rewritten to use VarHandles directly,
>> >>> i think it's time to deprecate all *FieldUpdater with something
>saying
>> >>> that they have been superseded by the VarHandle API.
>> >>>
>> >>> Rémi
>> >>> substitute dr deprecator
>> >>>
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>------------------------------------------------------------------------
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161005/7f2d69fa/attachment-0001.html>

From martinrb at google.com  Wed Oct  5 15:49:52 2016
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 5 Oct 2016 12:49:52 -0700
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
Message-ID: <CA+kOe0_DN+5u-_YC_RFRVEFoMd=pb9cQy=+T5sgNs=nY4sTDtg@mail.gmail.com>

Tough crowd...

I still think (and Doug privately agreed with me) that my initial proposal
makes the source code a little cleaner.  I can't understand why some of the
benchmark results report this as a pessimization, except for bad
benchmarking luck or a hotspot bug optimizing nested loops.

---

On a platform with native strong cas, a single loop using strong cas should
be the smallest cleanest code.  We could have a horrible

static final boolean HAS_STRONG_CAS = ???

if (HAS_STRONG_CAS) useStrongCasLoop(); else useWeakCasLoop();

---

I still don't want to give up the property that we never call the update
function twice due to spurious weak cas failure.

---

The platform where a weak cas loop might really matter is powerpc, because
there the volatile read can be unusually expensive, so saving one may be
worthwhile.  But no one has been benchmarking powerpc!  Maybe we've been
staring too much at the CAS and not enough at the read.  Perhaps instead of
calling get() we should be calling a weaker version like getOpaque()?

---

If we produced specialized implementations for each platform, that might be
optimal, but probably these getAndUpdate methods aren't worth the effort to
do that...  In performance critical situations callers will be writing
their own cas loops.

---

Thanks to Peter Paul and Andrew for awesome benchmarking prowess, but do we
have a concrete proposal for improving j.u.c. ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161005/f7561f70/attachment.html>

From vitalyd at gmail.com  Wed Oct  5 16:51:40 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 5 Oct 2016 16:51:40 -0400
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_DN+5u-_YC_RFRVEFoMd=pb9cQy=+T5sgNs=nY4sTDtg@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CA+kOe0_DN+5u-_YC_RFRVEFoMd=pb9cQy=+T5sgNs=nY4sTDtg@mail.gmail.com>
Message-ID: <CAHjP37F53MCSuqYcDKjkHHeHDzSvpiHW7tfxHNer4N16GUGRrw@mail.gmail.com>

On Wed, Oct 5, 2016 at 3:49 PM, Martin Buchholz <martinrb at google.com> wrote:

> Tough crowd...
>
> I still think (and Doug privately agreed with me) that my initial proposal
> makes the source code a little cleaner.  I can't understand why some of the
> benchmark results report this as a pessimization, except for bad
> benchmarking luck or a hotspot bug optimizing nested loops.
>
I think Aleksey's "canonical" version upthread is the cleanest.

>
> ---
>
> On a platform with native strong cas, a single loop using strong cas
> should be the smallest cleanest code.  We could have a horrible
>
> static final boolean HAS_STRONG_CAS = ???
>
> if (HAS_STRONG_CAS) useStrongCasLoop(); else useWeakCasLoop();
>
> ---
>
> I still don't want to give up the property that we never call the update
> function twice due to spurious weak cas failure.
>
Why is this so important? A spurious weak cas failure such that the
cacheline was stolen yet somehow came back with the same value as before
would seem like a very rare situation.  I also don't understand why you'd
want to paint yourself into a corner with promising, either explicitly or
via impl detail, to not call the update function on spurious failures.
What if a few years down the road being able to call it multiple times
actually allows better code generation? I just don't see enough gain here
to lock yourself in like that.

And as you say below, if someone really wanted to avoid calling their
update function multiple times on spurious weak cas failures, they could
open code their own loop.  It will be a very niche case, I suspect.

>
> ---
>
> The platform where a weak cas loop might really matter is powerpc, because
> there the volatile read can be unusually expensive, so saving one may be
> worthwhile.  But no one has been benchmarking powerpc!  Maybe we've been
> staring too much at the CAS and not enough at the read.  Perhaps instead of
> calling get() we should be calling a weaker version like getOpaque()?
>
> ---
>
> If we produced specialized implementations for each platform, that might
> be optimal, but probably these getAndUpdate methods aren't worth the effort
> to do that...  In performance critical situations callers will be writing
> their own cas loops.
>
> ---
>
> Thanks to Peter Paul and Andrew for awesome benchmarking prowess, but do
> we have a concrete proposal for improving j.u.c. ?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161005/f35cb4d9/attachment.html>

From aph at redhat.com  Thu Oct  6 03:07:43 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 6 Oct 2016 08:07:43 +0100
Subject: [concurrency-interest] The very best CAS loop
In-Reply-To: <CA+kOe0_DN+5u-_YC_RFRVEFoMd=pb9cQy=+T5sgNs=nY4sTDtg@mail.gmail.com>
References: <CA+kOe0_qumMdTmuAKcuEtOBXh-HM4SPYnrH4Mo3KDRYz34c6og@mail.gmail.com>
 <CA+kOe0_DN+5u-_YC_RFRVEFoMd=pb9cQy=+T5sgNs=nY4sTDtg@mail.gmail.com>
Message-ID: <f42d4b26-6959-3fd5-72c3-0c69c6d840fe@redhat.com>

On 05/10/16 20:49, Martin Buchholz wrote:
> I still think (and Doug privately agreed with me) that my initial proposal
> makes the source code a little cleaner.  I can't understand why some of the
> benchmark results report this as a pessimization, except for bad
> benchmarking luck or a hotspot bug optimizing nested loops.

I think I already explained that: there's another loop, and it's badly
predicted.  Also, the spurious failure is so rare that on the odd
occasions it does fail you hit an uncommon trap.  There's no point
simulating a strong CAS with a weak one.

Andrew.

From oleksandr.otenko at gmail.com  Thu Oct  6 05:15:25 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 6 Oct 2016 10:15:25 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
 <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>
Message-ID: <09423E04-2568-41CA-8576-7E1253B90151@gmail.com>

> Not to mention that most humans don't get programming *at all*? :-)


That of course is an argument of a completely different magnitude.

I am talking about cases like these (poked around just yesterday):

SMTP protocol, library offers to register callbacks for individual commands.

smtp.on(‘mail’, …);
smtp.on(‘rcpt’, …);
smtp.on(‘data’, …);
smtp.on(‘message-start’, (stream) -> stream.on(‘end’, ...));
smtp.on(‘message-end’, …);

What essentially would have been a bunch of expressions in program order, becomes a concurrency problem. Since the control got inverted, now technically you must be prepared to process MAIL FROM, RCPT TO, DATA out of order. Also, for the same reason, now one needs to introduce a barrier to join the events on message body stream end and the message-end as signalled by SMTP protocol - but without callbacks that would not be needed, as the order would be program order.


Alex

> On 4 Oct 2016, at 18:12, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> 
> 
> On Mon, Oct 3, 2016 at 4:37 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> 
>> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>> 
>> 
>> 
>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> Did you really mean thousands of microseconds (ie on the order of milliseconds)?
>> 
>> Apologies, I misremembered, it's in the order of hundreds of us.
>>  
>> 
>> But my point actually was that without language support for things like closures, switching from threaded model to actors/callbacks is not cheap refactoring. You may get the benefits of reduced latency, but will it be measurable (like, if it’s thousands of nanoseconds, not microseconds, then for context switches to account for 1% of CPU time one needs to switch thousands of times a second - some people may even experience that level of contention), and will the switch of expression justify it (loops become tail recursion, etc).
>> 
>> Having context-switches in the order of 100s of us means effectively capping throughput to < 10k ops/s
> 
> :) ...per thread
> 
> No, I don't think it can be broken down like that, because typically wakeup is not non-contended and without coherence cost.
>  
> 
> yes, there are cases like that.
> 
> Yes, unfortunately, I may add!
>  
> 
> 
>> 
>> The interviewing process shows engineers generally find it difficult to grasp. Your mileage may vary :)
>> 
>> The iterviewing process shows engineers generally find threads and locks difficult to grasp, yet it is baked into java.lang.Object ;)
> 
> :) sure! It’s good to have a powerful toolset. I was only commenting on the statement "how easy it is to use async”.
> 
> Easy implies familiarity :)
>  
> 
> In reality suddenly people struggle to do simple non-concurrent things, like returning a value from a method (because you don’t return it anymore, but pass it to a continuation), or organizing a loop (hello, recursion and fixpoints), or comprehending what would be a method with 20 lines of code, because it turns into 5 functions 4 lines each, and passing a dozen arguments through all of them just because the last method in the chain of invocations needs it.
> 
> Not to mention that most humans don't get programming *at all*? :-)
>  
> 
> 
> Alex
> 
> 
> 
>>  
>> 
>> Alex
>> 
>>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>>> 
>>> 
>>> 
>>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>>> 100s of threads blocked on various locks is no different from 100s of callbacks waiting in the queue.
>>> 
>>> As I'm certain you already know this, so for the casual list reader, the answer is "yes, and no." Thread wakeup latency tends to be in the thousands of us, this is rarely the case for callbacks. Also, Threads tend to be a much more finite resource than callbacks, so a problem which can be expressed with N callbacks may not be expressible with the same N of threads on the same machine.
>>>  
>>> 
>>> Callbacks require a different expression of the same problem. For example, local variables are no longer local variables - they become instance variables. Without language support for closures it becomes a nightmare.
>>> 
>>> But most importantly, such common algorithmic constructs as loops and return from function, no longer look like that. A sequence of calls to potentially blocking methods turn into a pyramid of doom - just take a look at node.js projects.
>>> 
>>> It’s a tradeoff.
>>> 
>>> Alex
>>> 
>>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>> wrote:
>>>> 
>>>> The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.
>>>> 
>>>> Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  
>>>> 
>>>> Gregg Wonderly
>>>> 
>>>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com <mailto:ben.manes at gmail.com>> wrote:
>>>>> 
>>>>> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
>>>>> 
>>>>> When the new future class was introduced it was stated,
>>>>> 
>>>>> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>>> 
>>>>> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
>>>>> 
>>>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>>>>> (Sorry to re-open this discussion)
>>>>> 
>>>>> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
>>>>> 
>>>>> As Viktor has pointed out, users can always implement blocking themselves by writing
>>>>> 
>>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>>         stage.handle((T t, Throwable ex) -> {
>>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>>                          else f.complete(t);
>>>>>                          return null;
>>>>>                      });
>>>>>         return f;
>>>>>     }
>>>>> 
>>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>>         return toCompletableFuture(stage).join();
>>>>>     }
>>>>> 
>>>>> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
>>>>> 
>>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
>>>>> 
>>>>>     public default T join() { return toCompletableFuture().join(); }
>>>>> 
>>>>> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>> 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> 
>>> 
>>> 
>>> -- 
>>> Cheers,
>>> √
>> 
>> 
>> 
>> 
>> -- 
>> Cheers,
>> √
> 
> 
> 
> 
> -- 
> Cheers,
> √

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161006/899179fb/attachment-0001.html>

From viktor.klang at gmail.com  Thu Oct  6 15:22:38 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 6 Oct 2016 21:22:38 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <09423E04-2568-41CA-8576-7E1253B90151@gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
 <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>
 <09423E04-2568-41CA-8576-7E1253B90151@gmail.com>
Message-ID: <CANPzfU-w6Z=VaGoQ7vihcm1_ANj+-s-Wa24u0fA2Mq3othC61A@mail.gmail.com>

On Thu, Oct 6, 2016 at 11:15 AM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Not to mention that most humans don't get programming *at all*? :-)
>
>
> That of course is an argument of a completely different magnitude.
>

Yes, but who gets to choose the scope?


>
> I am talking about cases like these (poked around just yesterday):
>
> SMTP protocol, library offers to register callbacks for individual
> commands.
>
> smtp.on(‘mail’, …);
> smtp.on(‘rcpt’, …);
> smtp.on(‘data’, …);
> smtp.on(‘message-start’, (stream) -> stream.on(‘end’, ...));
> smtp.on(‘message-end’, …);
>
> What essentially would have been a bunch of expressions in program order,
> becomes a concurrency problem. Since the control got inverted, now
> technically you must be prepared to process MAIL FROM, RCPT TO, DATA out of
> order. Also, for the same reason, now one needs to introduce a barrier to
> join the events on message body stream end and the message-end as signalled
> by SMTP protocol - but without callbacks that would not be needed, as the
> order would be *program* order.
>

I'm pretty sure that the jury is in when it comes to the utility of *that
style* of async APIs.
(I mean, even SAX would be nicer than that API)


>
>
> Alex
>
> On 4 Oct 2016, at 18:12, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>
>
> On Mon, Oct 3, 2016 at 4:37 PM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>>
>> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com> wrote:
>>
>>
>>
>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com>
>> wrote:
>>
>>> Did you really mean thousands of microseconds (ie on the order of
>>> milliseconds)?
>>>
>>
>> Apologies, I misremembered, it's in the order of hundreds of us.
>>
>>
>>>
>>> But my point actually was that without language support for things like
>>> closures, switching from threaded model to actors/callbacks is not cheap
>>> refactoring. You may get the benefits of reduced latency, but will it be
>>> measurable (like, if it’s thousands of nanoseconds, not microseconds, then
>>> for context switches to account for 1% of CPU time one needs to switch
>>> thousands of times a second - some people may even experience that level of
>>> contention), and will the switch of expression justify it (loops become
>>> tail recursion, etc).
>>>
>>
>> Having context-switches in the order of 100s of us means effectively
>> capping throughput to < 10k ops/s
>>
>>
>> :) ...per thread
>>
>
> No, I don't think it can be broken down like that, because typically
> wakeup is not non-contended and without coherence cost.
>
>
>>
>> yes, there are cases like that.
>>
>
> Yes, unfortunately, I may add!
>
>
>>
>>
>>
>>> The interviewing process shows engineers generally find it difficult to
>>> grasp. Your mileage may vary :)
>>>
>>
>> The iterviewing process shows engineers generally find threads and locks
>> difficult to grasp, yet it is baked into java.lang.Object ;)
>>
>>
>> :) sure! It’s good to have a powerful toolset. I was only commenting on
>> the statement "how easy it is to use async”.
>>
>
> Easy implies familiarity :)
>
>
>>
>> In reality suddenly people struggle to do simple non-concurrent things,
>> like returning a value from a method (because you don’t return it anymore,
>> but pass it to a continuation), or organizing a loop (hello, recursion and
>> fixpoints), or comprehending what would be a method with 20 lines of code,
>> because it turns into 5 functions 4 lines each, and passing a dozen
>> arguments through all of them just because the last method in the chain of
>> invocations needs it.
>>
>
> Not to mention that most humans don't get programming *at all*? :-)
>
>
>>
>>
>> Alex
>>
>>
>>
>>
>>
>>>
>>> Alex
>>>
>>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>
>>>
>>>
>>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com
>>> > wrote:
>>>
>>>> 100s of threads blocked on various locks is no different from 100s of
>>>> callbacks waiting in the queue.
>>>>
>>>
>>> As I'm certain you already know this, so for the casual list reader, the
>>> answer is "yes, and no." Thread wakeup latency tends to be in the thousands
>>> of us, this is rarely the case for callbacks. Also, Threads tend to be a
>>> much more finite resource than callbacks, so a problem which can be
>>> expressed with N callbacks may not be expressible with the same N of
>>> threads on the same machine.
>>>
>>>
>>>>
>>>> Callbacks require a different expression of the same problem. For
>>>> example, local variables are no longer local variables - they become
>>>> instance variables. Without language support for closures it becomes a
>>>> nightmare.
>>>>
>>>> But most importantly, such common algorithmic constructs as loops and
>>>> return from function, no longer look like that. A sequence of calls to
>>>> potentially blocking methods turn into a pyramid of doom - just take a look
>>>> at node.js projects.
>>>>
>>>> It’s a tradeoff.
>>>>
>>>> Alex
>>>>
>>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
>>>>
>>>> The completely less thread consuming alternative is call backs.   VMS
>>>> used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It
>>>> was a very performant and friendly way to allow small software modules to
>>>> be written to do one thing and used as the call back for asynchronous
>>>> system behaviors.  I wrote several symbionts which did serial connection
>>>> scripting for connecting to printers remotely, process management for
>>>> process reuse on PMDF to speed up mail flow through the queue, and other
>>>> things interactive with the kernel to alter process priorities to provide
>>>> longer term scheduling on a heavily loaded 750.  It was like heaven to have
>>>> such a small amount of code (lots of functions, some global data).   I am
>>>> just suggesting that this kind of thing was a major part of more than one
>>>> programming environment.
>>>>
>>>> Having 100’s of threads blocked on various locks adds considerably to
>>>> the overhead of scheduling but also complicates cache use and burdens the
>>>> developer with looping and waiting in ways that increase bugs in code that
>>>> should not exist.  I really feel that the kind of thing that
>>>> CompletionStage provides to the developer is something that should of been
>>>> more prevalent from the start.  It inverts programatic logic in some cases,
>>>> but call backs really are the best way to react to asynchronous eventing in
>>>> software.
>>>>
>>>> Gregg Wonderly
>>>>
>>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com>
>>>> wrote:
>>>>
>>>> My limited understanding is that the original API was
>>>> only CompletableFuture and that CompletionStage introduced as a compromise.
>>>> It did not appear to be an attempt to strictly follow an
>>>> interface-implementation separation, e.g. collections. As you said #toCompletableFuture()
>>>> may throw an UOE, which means some use-cases can't rely on CompletionState
>>>> which limits its usefulness. In my case that would be an AsyncLoadingCache
>>>> with a synchronous LoadingCache view. I think having to code that the
>>>> resulting implementation would be worse if it called toCompletableFuture,
>>>> caught the exception, and then adapted as you said.
>>>>
>>>> When the new future class was introduced it was stated,
>>>>
>>>> "In other words, we (j.u.c) are not now in a position to dictate a
>>>> common interface for all SettableFuture, FutureValue, Promise,
>>>> ListenableFuture, etc like APIs. And as we've seen, different audiences
>>>> want/need different subsets of this API exposed as interfaces for their
>>>> usages, and are in any case unlikely to want change all their existing
>>>> interfaces. However, what we can do is provide a common underlying
>>>> implementation that is as fast, scalable, space-conserving,
>>>> carefully-specified, and reliable as possible. It should then be easy and
>>>> attractive for others creating or reworking higher-level APIs to relay all
>>>> functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>>
>>>> I've gradually come to terms using CF as part of an API and haven't
>>>> experienced a downside yet.
>>>>
>>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
>>>> wrote:
>>>>
>>>>> (Sorry to re-open this discussion)
>>>>>
>>>>> The separation of a read-only CompletionStage from CompletableFuture
>>>>> is great.  I'm a fan of the scala style Promise/Future split as described
>>>>> in http://docs.scala-lang.org/overviews/core/futures.html, but: we
>>>>> need to re-add (safe, read-only) blocking methods like join.  Java is not
>>>>> Node.js, where there are no threads but there is a universal event loop.
>>>>> Java programmers are used to Future, where the *only* way to use a future's
>>>>> value is to block waiting for it.  The existing CompletionStage methods are
>>>>> a better scaling alternative to blocking all the time, but blocking is
>>>>> almost always eventually necessary in Java.  For example, junit test
>>>>> methods that start any asynchronous computation need to block until the
>>>>> computation is done, before returning.
>>>>>
>>>>> As Viktor has pointed out, users can always implement blocking
>>>>> themselves by writing
>>>>>
>>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
>>>>> stage) {
>>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>>         stage.handle((T t, Throwable ex) -> {
>>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>>                          else f.complete(t);
>>>>>                          return null;
>>>>>                      });
>>>>>         return f;
>>>>>     }
>>>>>
>>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>>         return toCompletableFuture(stage).join();
>>>>>     }
>>>>>
>>>>> but unlike Viktor, I think it's unreasonable to not provide this for
>>>>> users (especially when we can do so more efficiently).  What is happening
>>>>> instead is API providers not using CompletionStage as return values in
>>>>> public APIs because of the lack of convenient blocking, and instead
>>>>> returning CompletableFuture, which is a tragic software engineering failure.
>>>>>
>>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
>>>>> from throwing UnsupportedOperationException, and implement join as:
>>>>>
>>>>>     public default T join() { return toCompletableFuture().join(); }
>>>>>
>>>>> There is a risk of multiple-inheritance conflict with Future if we add
>>>>> e.g. isDone(), but there are no current plans to turn those Future methods
>>>>> into default methods, and even if we did in some future release, it would
>>>>> be only a source, not binary incompatibility, so far less serious.
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> Cheers,
>>> √
>>>
>>>
>>>
>>
>>
>> --
>> Cheers,
>> √
>>
>>
>>
>
>
> --
> Cheers,
> √
>
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161006/56b6becd/attachment-0001.html>

From oleksandr.otenko at gmail.com  Fri Oct  7 03:25:51 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Fri, 7 Oct 2016 08:25:51 +0100
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU-w6Z=VaGoQ7vihcm1_ANj+-s-Wa24u0fA2Mq3othC61A@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
 <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>
 <09423E04-2568-41CA-8576-7E1253B90151@gmail.com>
 <CANPzfU-w6Z=VaGoQ7vihcm1_ANj+-s-Wa24u0fA2Mq3othC61A@mail.gmail.com>
Message-ID: <73407F04-419F-43E4-AAD7-A2C27619E4B4@gmail.com>

It’s not about the style of the API. It is about inversion of control that you can’t hide whatever async API you use - it is just a property of continuation-passing transform.


Alex


> On 6 Oct 2016, at 20:22, Viktor Klang <viktor.klang at gmail.com> wrote:
> 
> 
> 
> On Thu, Oct 6, 2016 at 11:15 AM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> Not to mention that most humans don't get programming *at all*? :-)
> 
> 
> That of course is an argument of a completely different magnitude.
> 
> Yes, but who gets to choose the scope?
>  
> 
> I am talking about cases like these (poked around just yesterday):
> 
> SMTP protocol, library offers to register callbacks for individual commands.
> 
> smtp.on(‘mail’, …);
> smtp.on(‘rcpt’, …);
> smtp.on(‘data’, …);
> smtp.on(‘message-start’, (stream) -> stream.on(‘end’, ...));
> smtp.on(‘message-end’, …);
> 
> What essentially would have been a bunch of expressions in program order, becomes a concurrency problem. Since the control got inverted, now technically you must be prepared to process MAIL FROM, RCPT TO, DATA out of order. Also, for the same reason, now one needs to introduce a barrier to join the events on message body stream end and the message-end as signalled by SMTP protocol - but without callbacks that would not be needed, as the order would be program order.
> 
> I'm pretty sure that the jury is in when it comes to the utility of *that style* of async APIs.
> (I mean, even SAX would be nicer than that API)
>  
> 
> 
> Alex
> 
>> On 4 Oct 2016, at 18:12, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>> 
>> 
>> 
>> On Mon, Oct 3, 2016 at 4:37 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> 
>>> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>>> 
>>> 
>>> 
>>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>>> Did you really mean thousands of microseconds (ie on the order of milliseconds)?
>>> 
>>> Apologies, I misremembered, it's in the order of hundreds of us.
>>>  
>>> 
>>> But my point actually was that without language support for things like closures, switching from threaded model to actors/callbacks is not cheap refactoring. You may get the benefits of reduced latency, but will it be measurable (like, if it’s thousands of nanoseconds, not microseconds, then for context switches to account for 1% of CPU time one needs to switch thousands of times a second - some people may even experience that level of contention), and will the switch of expression justify it (loops become tail recursion, etc).
>>> 
>>> Having context-switches in the order of 100s of us means effectively capping throughput to < 10k ops/s
>> 
>> :) ...per thread
>> 
>> No, I don't think it can be broken down like that, because typically wakeup is not non-contended and without coherence cost.
>>  
>> 
>> yes, there are cases like that.
>> 
>> Yes, unfortunately, I may add!
>>  
>> 
>> 
>>> 
>>> The interviewing process shows engineers generally find it difficult to grasp. Your mileage may vary :)
>>> 
>>> The iterviewing process shows engineers generally find threads and locks difficult to grasp, yet it is baked into java.lang.Object ;)
>> 
>> :) sure! It’s good to have a powerful toolset. I was only commenting on the statement "how easy it is to use async”.
>> 
>> Easy implies familiarity :)
>>  
>> 
>> In reality suddenly people struggle to do simple non-concurrent things, like returning a value from a method (because you don’t return it anymore, but pass it to a continuation), or organizing a loop (hello, recursion and fixpoints), or comprehending what would be a method with 20 lines of code, because it turns into 5 functions 4 lines each, and passing a dozen arguments through all of them just because the last method in the chain of invocations needs it.
>> 
>> Not to mention that most humans don't get programming *at all*? :-)
>>  
>> 
>> 
>> Alex
>> 
>> 
>> 
>>>  
>>> 
>>> Alex
>>> 
>>>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>>>> 
>>>> 
>>>> 
>>>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>>>> 100s of threads blocked on various locks is no different from 100s of callbacks waiting in the queue.
>>>> 
>>>> As I'm certain you already know this, so for the casual list reader, the answer is "yes, and no." Thread wakeup latency tends to be in the thousands of us, this is rarely the case for callbacks. Also, Threads tend to be a much more finite resource than callbacks, so a problem which can be expressed with N callbacks may not be expressible with the same N of threads on the same machine.
>>>>  
>>>> 
>>>> Callbacks require a different expression of the same problem. For example, local variables are no longer local variables - they become instance variables. Without language support for closures it becomes a nightmare.
>>>> 
>>>> But most importantly, such common algorithmic constructs as loops and return from function, no longer look like that. A sequence of calls to potentially blocking methods turn into a pyramid of doom - just take a look at node.js projects.
>>>> 
>>>> It’s a tradeoff.
>>>> 
>>>> Alex
>>>> 
>>>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>> wrote:
>>>>> 
>>>>> The completely less thread consuming alternative is call backs.   VMS used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It was a very performant and friendly way to allow small software modules to be written to do one thing and used as the call back for asynchronous system behaviors.  I wrote several symbionts which did serial connection scripting for connecting to printers remotely, process management for process reuse on PMDF to speed up mail flow through the queue, and other things interactive with the kernel to alter process priorities to provide longer term scheduling on a heavily loaded 750.  It was like heaven to have such a small amount of code (lots of functions, some global data).   I am just suggesting that this kind of thing was a major part of more than one programming environment.
>>>>> 
>>>>> Having 100’s of threads blocked on various locks adds considerably to the overhead of scheduling but also complicates cache use and burdens the developer with looping and waiting in ways that increase bugs in code that should not exist.  I really feel that the kind of thing that CompletionStage provides to the developer is something that should of been more prevalent from the start.  It inverts programatic logic in some cases, but call backs really are the best way to react to asynchronous eventing in software.  
>>>>> 
>>>>> Gregg Wonderly
>>>>> 
>>>>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com <mailto:ben.manes at gmail.com>> wrote:
>>>>>> 
>>>>>> My limited understanding is that the original API was only CompletableFuture and that CompletionStage introduced as a compromise. It did not appear to be an attempt to strictly follow an interface-implementation separation, e.g. collections. As you said #toCompletableFuture() may throw an UOE, which means some use-cases can't rely on CompletionState which limits its usefulness. In my case that would be an AsyncLoadingCache with a synchronous LoadingCache view. I think having to code that the resulting implementation would be worse if it called toCompletableFuture, caught the exception, and then adapted as you said. 
>>>>>> 
>>>>>> When the new future class was introduced it was stated,
>>>>>> 
>>>>>> "In other words, we (j.u.c) are not now in a position to dictate a common interface for all SettableFuture, FutureValue, Promise, ListenableFuture, etc like APIs. And as we've seen, different audiences want/need different subsets of this API exposed as interfaces for their usages, and are in any case unlikely to want change all their existing interfaces. However, what we can do is provide a common underlying implementation that is as fast, scalable, space-conserving, carefully-specified, and reliable as possible. It should then be easy and attractive for others creating or reworking higher-level APIs to relay all functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>>>> 
>>>>>> I've gradually come to terms using CF as part of an API and haven't experienced a downside yet.
>>>>>> 
>>>>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com <mailto:martinrb at google.com>> wrote:
>>>>>> (Sorry to re-open this discussion)
>>>>>> 
>>>>>> The separation of a read-only CompletionStage from CompletableFuture is great.  I'm a fan of the scala style Promise/Future split as described in http://docs.scala-lang.org/overviews/core/futures.html <http://docs.scala-lang.org/overviews/core/futures.html>, but: we need to re-add (safe, read-only) blocking methods like join.  Java is not Node.js, where there are no threads but there is a universal event loop.  Java programmers are used to Future, where the *only* way to use a future's value is to block waiting for it.  The existing CompletionStage methods are a better scaling alternative to blocking all the time, but blocking is almost always eventually necessary in Java.  For example, junit test methods that start any asynchronous computation need to block until the computation is done, before returning.
>>>>>> 
>>>>>> As Viktor has pointed out, users can always implement blocking themselves by writing
>>>>>> 
>>>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T> stage) {
>>>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>>>         stage.handle((T t, Throwable ex) -> {
>>>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>>>                          else f.complete(t);
>>>>>>                          return null;
>>>>>>                      });
>>>>>>         return f;
>>>>>>     }
>>>>>> 
>>>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>>>         return toCompletableFuture(stage).join();
>>>>>>     }
>>>>>> 
>>>>>> but unlike Viktor, I think it's unreasonable to not provide this for users (especially when we can do so more efficiently).  What is happening instead is API providers not using CompletionStage as return values in public APIs because of the lack of convenient blocking, and instead returning CompletableFuture, which is a tragic software engineering failure.
>>>>>> 
>>>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture from throwing UnsupportedOperationException, and implement join as:
>>>>>> 
>>>>>>     public default T join() { return toCompletableFuture().join(); }
>>>>>> 
>>>>>> There is a risk of multiple-inheritance conflict with Future if we add e.g. isDone(), but there are no current plans to turn those Future methods into default methods, and even if we did in some future release, it would be only a source, not binary incompatibility, so far less serious.
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> 
>>>> 
>>>> 
>>>> 
>>>> -- 
>>>> Cheers,
>>>> √
>>> 
>>> 
>>> 
>>> 
>>> -- 
>>> Cheers,
>>> √
>> 
>> 
>> 
>> 
>> -- 
>> Cheers,
>> √
> 
> 
> 
> 
> -- 
> Cheers,
> √

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/43846334/attachment-0001.html>

From viktor.klang at gmail.com  Fri Oct  7 09:40:59 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 15:40:59 +0200
Subject: [concurrency-interest] We need to add blocking methods to
	CompletionStage!
In-Reply-To: <CANPzfU_rCuEg1V26FY=s1SkQr-9Wp61x-pybv4j9TRgGaw0w7Q@mail.gmail.com>
References: <CA+kOe0-hQp7CpLdSf3TycbP30TGs7z4C6SS8PF_xCtBz3Nu_eA@mail.gmail.com>
 <96D342D8-8820-49F3-A80A-07EAFCC8AC16@cox.net>
 <C79EEB8E-6D08-4117-A574-863AFB146B0F@gmail.com>
 <CANPzfU9T5LySe78vJFYHO5iqut3bPUsZiSztbi=VvVzLoFqzYw@mail.gmail.com>
 <5D41EB9F-511F-426B-9964-BB839F21360A@gmail.com>
 <CANPzfU-LnevMznQ_pez4_nk5hc0k=mPWU-UeULH-9QOKntbqBQ@mail.gmail.com>
 <C915005F-4BB3-423E-B2F6-F0DCB028D167@gmail.com>
 <CANPzfU993RNBR_yK9c6Z0X8LW9XOWeSpPDWVpWsQU89+-rvWmA@mail.gmail.com>
 <09423E04-2568-41CA-8576-7E1253B90151@gmail.com>
 <CANPzfU-w6Z=VaGoQ7vihcm1_ANj+-s-Wa24u0fA2Mq3othC61A@mail.gmail.com>
 <73407F04-419F-43E4-AAD7-A2C27619E4B4@gmail.com>
 <CANPzfU_rCuEg1V26FY=s1SkQr-9Wp61x-pybv4j9TRgGaw0w7Q@mail.gmail.com>
Message-ID: <CANPzfU-vtcXBpcv2w2E6nGkyWsBZoyj8moNxR7+tk8bfqBRh9A@mail.gmail.com>

I'd disagree, but that's a longer and quite different discussion than the
one originally posted.

Perhaps over beverage somewhere, sometime?

-- 
Cheers,
√

On Oct 7, 2016 02:25, "Alex Otenko" <oleksandr.otenko at gmail.com> wrote:

It’s not about the style of the API. It is about inversion of control that
you can’t hide whatever async API you use - it is just a property of
continuation-passing transform.


Alex


On 6 Oct 2016, at 20:22, Viktor Klang <viktor.klang at gmail.com> wrote:



On Thu, Oct 6, 2016 at 11:15 AM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Not to mention that most humans don't get programming *at all*? :-)
>
>
> That of course is an argument of a completely different magnitude.
>

Yes, but who gets to choose the scope?


>
> I am talking about cases like these (poked around just yesterday):
>
> SMTP protocol, library offers to register callbacks for individual
> commands.
>
> smtp.on(‘mail’, …);
> smtp.on(‘rcpt’, …);
> smtp.on(‘data’, …);
> smtp.on(‘message-start’, (stream) -> stream.on(‘end’, ...));
> smtp.on(‘message-end’, …);
>
> What essentially would have been a bunch of expressions in program order,
> becomes a concurrency problem. Since the control got inverted, now
> technically you must be prepared to process MAIL FROM, RCPT TO, DATA out of
> order. Also, for the same reason, now one needs to introduce a barrier to
> join the events on message body stream end and the message-end as signalled
> by SMTP protocol - but without callbacks that would not be needed, as the
> order would be *program* order.
>

I'm pretty sure that the jury is in when it comes to the utility of *that
style* of async APIs.
(I mean, even SAX would be nicer than that API)


>
>
> Alex
>
> On 4 Oct 2016, at 18:12, Viktor Klang <viktor.klang at gmail.com> wrote:
>
>
>
> On Mon, Oct 3, 2016 at 4:37 PM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>>
>> On 3 Oct 2016, at 14:57, Viktor Klang <viktor.klang at gmail.com> wrote:
>>
>>
>>
>> On Sun, Oct 2, 2016 at 3:03 PM, Alex Otenko <oleksandr.otenko at gmail.com>
>> wrote:
>>
>>> Did you really mean thousands of microseconds (ie on the order of
>>> milliseconds)?
>>>
>>
>> Apologies, I misremembered, it's in the order of hundreds of us.
>>
>>
>>>
>>> But my point actually was that without language support for things like
>>> closures, switching from threaded model to actors/callbacks is not cheap
>>> refactoring. You may get the benefits of reduced latency, but will it be
>>> measurable (like, if it’s thousands of nanoseconds, not microseconds, then
>>> for context switches to account for 1% of CPU time one needs to switch
>>> thousands of times a second - some people may even experience that level of
>>> contention), and will the switch of expression justify it (loops become
>>> tail recursion, etc).
>>>
>>
>> Having context-switches in the order of 100s of us means effectively
>> capping throughput to < 10k ops/s
>>
>>
>> :) ...per thread
>>
>
> No, I don't think it can be broken down like that, because typically
> wakeup is not non-contended and without coherence cost.
>
>
>>
>> yes, there are cases like that.
>>
>
> Yes, unfortunately, I may add!
>
>
>>
>>
>>
>>> The interviewing process shows engineers generally find it difficult to
>>> grasp. Your mileage may vary :)
>>>
>>
>> The iterviewing process shows engineers generally find threads and locks
>> difficult to grasp, yet it is baked into java.lang.Object ;)
>>
>>
>> :) sure! It’s good to have a powerful toolset. I was only commenting on
>> the statement "how easy it is to use async”.
>>
>
> Easy implies familiarity :)
>
>
>>
>> In reality suddenly people struggle to do simple non-concurrent things,
>> like returning a value from a method (because you don’t return it anymore,
>> but pass it to a continuation), or organizing a loop (hello, recursion and
>> fixpoints), or comprehending what would be a method with 20 lines of code,
>> because it turns into 5 functions 4 lines each, and passing a dozen
>> arguments through all of them just because the last method in the chain of
>> invocations needs it.
>>
>
> Not to mention that most humans don't get programming *at all*? :-)
>
>
>>
>>
>> Alex
>>
>>
>>
>>
>>
>>>
>>> Alex
>>>
>>> On 2 Oct 2016, at 12:09, Viktor Klang <viktor.klang at gmail.com> wrote:
>>>
>>>
>>>
>>> On Wed, Sep 28, 2016 at 8:38 PM, Alex Otenko <oleksandr.otenko at gmail.com
>>> > wrote:
>>>
>>>> 100s of threads blocked on various locks is no different from 100s of
>>>> callbacks waiting in the queue.
>>>>
>>>
>>> As I'm certain you already know this, so for the casual list reader, the
>>> answer is "yes, and no." Thread wakeup latency tends to be in the thousands
>>> of us, this is rarely the case for callbacks. Also, Threads tend to be a
>>> much more finite resource than callbacks, so a problem which can be
>>> expressed with N callbacks may not be expressible with the same N of
>>> threads on the same machine.
>>>
>>>
>>>>
>>>> Callbacks require a different expression of the same problem. For
>>>> example, local variables are no longer local variables - they become
>>>> instance variables. Without language support for closures it becomes a
>>>> nightmare.
>>>>
>>>> But most importantly, such common algorithmic constructs as loops and
>>>> return from function, no longer look like that. A sequence of calls to
>>>> potentially blocking methods turn into a pyramid of doom - just take a look
>>>> at node.js projects.
>>>>
>>>> It’s a tradeoff.
>>>>
>>>> Alex
>>>>
>>>> On 28 Sep 2016, at 16:05, Gregg Wonderly <gergg at cox.net> wrote:
>>>>
>>>> The completely less thread consuming alternative is call backs.   VMS
>>>> used/uses Asynchronous System Traps (ASTs) for everything in the 1980s.  It
>>>> was a very performant and friendly way to allow small software modules to
>>>> be written to do one thing and used as the call back for asynchronous
>>>> system behaviors.  I wrote several symbionts which did serial connection
>>>> scripting for connecting to printers remotely, process management for
>>>> process reuse on PMDF to speed up mail flow through the queue, and other
>>>> things interactive with the kernel to alter process priorities to provide
>>>> longer term scheduling on a heavily loaded 750.  It was like heaven to have
>>>> such a small amount of code (lots of functions, some global data).   I am
>>>> just suggesting that this kind of thing was a major part of more than one
>>>> programming environment.
>>>>
>>>> Having 100’s of threads blocked on various locks adds considerably to
>>>> the overhead of scheduling but also complicates cache use and burdens the
>>>> developer with looping and waiting in ways that increase bugs in code that
>>>> should not exist.  I really feel that the kind of thing that
>>>> CompletionStage provides to the developer is something that should of been
>>>> more prevalent from the start.  It inverts programatic logic in some cases,
>>>> but call backs really are the best way to react to asynchronous eventing in
>>>> software.
>>>>
>>>> Gregg Wonderly
>>>>
>>>> On Sep 21, 2016, at 4:25 PM, Benjamin Manes <ben.manes at gmail.com>
>>>> wrote:
>>>>
>>>> My limited understanding is that the original API was
>>>> only CompletableFuture and that CompletionStage introduced as a compromise.
>>>> It did not appear to be an attempt to strictly follow an
>>>> interface-implementation separation, e.g. collections. As you said #toCompletableFuture()
>>>> may throw an UOE, which means some use-cases can't rely on CompletionState
>>>> which limits its usefulness. In my case that would be an AsyncLoadingCache
>>>> with a synchronous LoadingCache view. I think having to code that the
>>>> resulting implementation would be worse if it called toCompletableFuture,
>>>> caught the exception, and then adapted as you said.
>>>>
>>>> When the new future class was introduced it was stated,
>>>>
>>>> "In other words, we (j.u.c) are not now in a position to dictate a
>>>> common interface for all SettableFuture, FutureValue, Promise,
>>>> ListenableFuture, etc like APIs. And as we've seen, different audiences
>>>> want/need different subsets of this API exposed as interfaces for their
>>>> usages, and are in any case unlikely to want change all their existing
>>>> interfaces. However, what we can do is provide a common underlying
>>>> implementation that is as fast, scalable, space-conserving,
>>>> carefully-specified, and reliable as possible. It should then be easy and
>>>> attractive for others creating or reworking higher-level APIs to relay all
>>>> functionality to the CompletableFuture implementation."  - Doug Lea, '12
>>>>
>>>> I've gradually come to terms using CF as part of an API and haven't
>>>> experienced a downside yet.
>>>>
>>>> On Wed, Sep 21, 2016 at 1:43 PM, Martin Buchholz <martinrb at google.com>
>>>> wrote:
>>>>
>>>>> (Sorry to re-open this discussion)
>>>>>
>>>>> The separation of a read-only CompletionStage from CompletableFuture
>>>>> is great.  I'm a fan of the scala style Promise/Future split as described
>>>>> in http://docs.scala-lang.org/overviews/core/futures.html, but: we
>>>>> need to re-add (safe, read-only) blocking methods like join.  Java is not
>>>>> Node.js, where there are no threads but there is a universal event loop.
>>>>> Java programmers are used to Future, where the *only* way to use a future's
>>>>> value is to block waiting for it.  The existing CompletionStage methods are
>>>>> a better scaling alternative to blocking all the time, but blocking is
>>>>> almost always eventually necessary in Java.  For example, junit test
>>>>> methods that start any asynchronous computation need to block until the
>>>>> computation is done, before returning.
>>>>>
>>>>> As Viktor has pointed out, users can always implement blocking
>>>>> themselves by writing
>>>>>
>>>>>     static <T> CompletableFuture<T> toCompletableFuture(CompletionStage<T>
>>>>> stage) {
>>>>>         CompletableFuture<T> f = new CompletableFuture<>();
>>>>>         stage.handle((T t, Throwable ex) -> {
>>>>>                          if (ex != null) f.completeExceptionally(ex);
>>>>>                          else f.complete(t);
>>>>>                          return null;
>>>>>                      });
>>>>>         return f;
>>>>>     }
>>>>>
>>>>>     static <T> T join(CompletionStage<T> stage) {
>>>>>         return toCompletableFuture(stage).join();
>>>>>     }
>>>>>
>>>>> but unlike Viktor, I think it's unreasonable to not provide this for
>>>>> users (especially when we can do so more efficiently).  What is happening
>>>>> instead is API providers not using CompletionStage as return values in
>>>>> public APIs because of the lack of convenient blocking, and instead
>>>>> returning CompletableFuture, which is a tragic software engineering failure.
>>>>>
>>>>> Re-adding join is easy.  We discourage CompletionStage.toCompletableFuture
>>>>> from throwing UnsupportedOperationException, and implement join as:
>>>>>
>>>>>     public default T join() { return toCompletableFuture().join(); }
>>>>>
>>>>> There is a risk of multiple-inheritance conflict with Future if we add
>>>>> e.g. isDone(), but there are no current plans to turn those Future methods
>>>>> into default methods, and even if we did in some future release, it would
>>>>> be only a source, not binary incompatibility, so far less serious.
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> Cheers,
>>> √
>>>
>>>
>>>
>>
>>
>> --
>> Cheers,
>> √
>>
>>
>>
>
>
> --
> Cheers,
> √
>
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/373c8f84/attachment-0001.html>

From andrew at intelerad.com  Fri Oct  7 13:47:36 2016
From: andrew at intelerad.com (Andrew Trumper)
Date: Fri, 7 Oct 2016 13:47:36 -0400
Subject: [concurrency-interest] Continuations / Fibers
Message-ID: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>

Hi All,

I've been meaning to ask for some time. Is there anyone working on 
adding continuations to the jvm with the idea of using them to allow the 
writing of single-threaded threaded Actors more naturally? ie: instead 
of callbacks just blocking? like:

Typical modern style:
someAsyncTask()
   .mapAsync(result -> someOtherAsyncTask(result))
   .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
   .onSuccess(yetAnotherResult -> System.out.println( "" + 
yetAnotherResult))
   .setInvoker(thisThread);

converts to:

// doesn't block the current (actor? EDT?) thread
Object result = yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
System.out.println( "" + result );

- Andrew

-- 

This email or any attachments may contain confidential or legally 
privileged information intended for the sole use of the addressees. Any 
use, redistribution, disclosure, or reproduction of this information, 
except as intended, is prohibited. If you received this email in error, 
please notify the sender and remove all copies of the message, including 
any attachments.


From markus at headcrashing.eu  Fri Oct  7 13:55:02 2016
From: markus at headcrashing.eu (Markus KARG)
Date: Fri, 7 Oct 2016 19:55:02 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
Message-ID: <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>

I don't understand the question. Why not just using CompletableFuture to build Fibers?

-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Andrew Trumper
Sent: Freitag, 7. Oktober 2016 19:48
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Continuations / Fibers

Hi All,

I've been meaning to ask for some time. Is there anyone working on adding continuations to the jvm with the idea of using them to allow the writing of single-threaded threaded Actors more naturally? ie: instead of callbacks just blocking? like:

Typical modern style:
someAsyncTask()
   .mapAsync(result -> someOtherAsyncTask(result))
   .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
   .onSuccess(yetAnotherResult -> System.out.println( "" +
yetAnotherResult))
   .setInvoker(thisThread);

converts to:

// doesn't block the current (actor? EDT?) thread Object result = yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
System.out.println( "" + result );

- Andrew

-- 

This email or any attachments may contain confidential or legally privileged information intended for the sole use of the addressees. Any use, redistribution, disclosure, or reproduction of this information, except as intended, is prohibited. If you received this email in error, please notify the sender and remove all copies of the message, including any attachments.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Fri Oct  7 14:09:43 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 20:09:43 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
Message-ID: <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>

>From a syntax PoV, there's things like Scala Async (which is syntactically
similar to C# async/await): https://github.com/scala/async

On Fri, Oct 7, 2016 at 7:55 PM, Markus KARG <markus at headcrashing.eu> wrote:

> I don't understand the question. Why not just using CompletableFuture to
> build Fibers?
>
> -----Original Message-----
> From: Concurrency-interest [mailto:concurrency-interest-
> bounces at cs.oswego.edu] On Behalf Of Andrew Trumper
> Sent: Freitag, 7. Oktober 2016 19:48
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Continuations / Fibers
>
> Hi All,
>
> I've been meaning to ask for some time. Is there anyone working on adding
> continuations to the jvm with the idea of using them to allow the writing
> of single-threaded threaded Actors more naturally? ie: instead of callbacks
> just blocking? like:
>
> Typical modern style:
> someAsyncTask()
>    .mapAsync(result -> someOtherAsyncTask(result))
>    .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
>    .onSuccess(yetAnotherResult -> System.out.println( "" +
> yetAnotherResult))
>    .setInvoker(thisThread);
>
> converts to:
>
> // doesn't block the current (actor? EDT?) thread Object result =
> yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
> System.out.println( "" + result );
>
> - Andrew
>
> --
>
> This email or any attachments may contain confidential or legally
> privileged information intended for the sole use of the addressees. Any
> use, redistribution, disclosure, or reproduction of this information,
> except as intended, is prohibited. If you received this email in error,
> please notify the sender and remove all copies of the message, including
> any attachments.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/ac6ed303/attachment.html>

From andrew at intelerad.com  Fri Oct  7 16:40:25 2016
From: andrew at intelerad.com (Andrew Trumper)
Date: Fri, 7 Oct 2016 16:40:25 -0400
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
Message-ID: <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>

I guess I'm wondering if anyone is working on bringing async/await like 
functionality to java. Either by continuation support in the JVM or 
whatever magic Scala uses.

Given that java has a great concurrency community around it, I'm 
surprised I never seem to hear anything about it.

- Andrew

On 10/07/2016 02:09 PM, Viktor Klang wrote:
> From a syntax PoV, there's things like Scala Async (which is
> syntactically similar to C# async/await): https://github.com/scala/async
>
> On Fri, Oct 7, 2016 at 7:55 PM, Markus KARG <markus at headcrashing.eu
> <mailto:markus at headcrashing.eu>> wrote:
>
>     I don't understand the question. Why not just using
>     CompletableFuture to build Fibers?
>
>     -----Original Message-----
>     From: Concurrency-interest
>     [mailto:concurrency-interest-bounces at cs.oswego.edu
>     <mailto:concurrency-interest-bounces at cs.oswego.edu>] On Behalf Of
>     Andrew Trumper
>     Sent: Freitag, 7. Oktober 2016 19:48
>     To: concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>
>     Subject: [concurrency-interest] Continuations / Fibers
>
>     Hi All,
>
>     I've been meaning to ask for some time. Is there anyone working on
>     adding continuations to the jvm with the idea of using them to allow
>     the writing of single-threaded threaded Actors more naturally? ie:
>     instead of callbacks just blocking? like:
>
>     Typical modern style:
>     someAsyncTask()
>        .mapAsync(result -> someOtherAsyncTask(result))
>        .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
>        .onSuccess(yetAnotherResult -> System.out.println( "" +
>     yetAnotherResult))
>        .setInvoker(thisThread);
>
>     converts to:
>
>     // doesn't block the current (actor? EDT?) thread Object result =
>     yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
>     System.out.println( "" + result );
>
>     - Andrew
>
>     --
>
>     This email or any attachments may contain confidential or legally
>     privileged information intended for the sole use of the addressees.
>     Any use, redistribution, disclosure, or reproduction of this
>     information, except as intended, is prohibited. If you received this
>     email in error, please notify the sender and remove all copies of
>     the message, including any attachments.
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
> --
> Cheers,
> √

-- 

This email or any attachments may contain confidential or legally 
privileged information intended for the sole use of the addressees. Any 
use, redistribution, disclosure, or reproduction of this information, 
except as intended, is prohibited. If you received this email in error, 
please notify the sender and remove all copies of the message, including 
any attachments.


From viktor.klang at gmail.com  Fri Oct  7 16:46:29 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 22:46:29 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
Message-ID: <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>

By no means exhaustive:

Scala uses no magic, only macros. So no need for language support.

Quasar does bytecode rewriting to implement fibers:
https://github.com/puniverse/quasar

Kotlin is also looking into coroutines:
https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html







On Fri, Oct 7, 2016 at 10:40 PM, Andrew Trumper <andrew at intelerad.com>
wrote:

> I guess I'm wondering if anyone is working on bringing async/await like
> functionality to java. Either by continuation support in the JVM or
> whatever magic Scala uses.
>
> Given that java has a great concurrency community around it, I'm surprised
> I never seem to hear anything about it.
>
> - Andrew
>
> On 10/07/2016 02:09 PM, Viktor Klang wrote:
>
>> From a syntax PoV, there's things like Scala Async (which is
>> syntactically similar to C# async/await): https://github.com/scala/async
>>
>> On Fri, Oct 7, 2016 at 7:55 PM, Markus KARG <markus at headcrashing.eu
>> <mailto:markus at headcrashing.eu>> wrote:
>>
>>     I don't understand the question. Why not just using
>>     CompletableFuture to build Fibers?
>>
>>     -----Original Message-----
>>     From: Concurrency-interest
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu
>>     <mailto:concurrency-interest-bounces at cs.oswego.edu>] On Behalf Of
>>     Andrew Trumper
>>     Sent: Freitag, 7. Oktober 2016 19:48
>>     To: concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>
>>     Subject: [concurrency-interest] Continuations / Fibers
>>
>>     Hi All,
>>
>>     I've been meaning to ask for some time. Is there anyone working on
>>     adding continuations to the jvm with the idea of using them to allow
>>     the writing of single-threaded threaded Actors more naturally? ie:
>>     instead of callbacks just blocking? like:
>>
>>     Typical modern style:
>>     someAsyncTask()
>>        .mapAsync(result -> someOtherAsyncTask(result))
>>        .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
>>        .onSuccess(yetAnotherResult -> System.out.println( "" +
>>     yetAnotherResult))
>>        .setInvoker(thisThread);
>>
>>     converts to:
>>
>>     // doesn't block the current (actor? EDT?) thread Object result =
>>     yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
>>     System.out.println( "" + result );
>>
>>     - Andrew
>>
>>     --
>>
>>     This email or any attachments may contain confidential or legally
>>     privileged information intended for the sole use of the addressees.
>>     Any use, redistribution, disclosure, or reproduction of this
>>     information, except as intended, is prohibited. If you received this
>>     email in error, please notify the sender and remove all copies of
>>     the message, including any attachments.
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>> --
>> Cheers,
>> √
>>
>
> --
>
> This email or any attachments may contain confidential or legally
> privileged information intended for the sole use of the addressees. Any
> use, redistribution, disclosure, or reproduction of this information,
> except as intended, is prohibited. If you received this email in error,
> please notify the sender and remove all copies of the message, including
> any attachments.
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/aeb6fc93/attachment-0001.html>

From headius at headius.com  Fri Oct  7 17:07:00 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 16:07:00 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
Message-ID: <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>

On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com> wrote:

> Scala uses no magic, only macros. So no need for language support.
>
> Quasar does bytecode rewriting to implement fibers: https://github.com/
> puniverse/quasar
>
> Kotlin is also looking into coroutines: https://kotlin.
> link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html
>

Coroutine options built atop threads suffer from inter-thread signaling
delays, thread count limits, and high memory use.

Options that use trampolines and bytecode rewriting impact performance and
can't suspend/resume across call boundaries that haven't been likewise
rewritten (so you have to rewrite everything).

True coroutines on the JVM would provide a lightweight way to cooperatively
share a thread of execution across multiple call stacks. Go has 'em, Ruby
has 'em, Python has 'em...the JVM needs 'em.

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/e6e07068/attachment.html>

From viktor.klang at gmail.com  Fri Oct  7 17:18:01 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 23:18:01 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
Message-ID: <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>

On Fri, Oct 7, 2016 at 11:07 PM, Charles Oliver Nutter <headius at headius.com>
wrote:

> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> Scala uses no magic, only macros. So no need for language support.
>>
>> Quasar does bytecode rewriting to implement fibers:
>> https://github.com/puniverse/quasar
>>
>> Kotlin is also looking into coroutines: https://kotlin.lin
>> k/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html
>>
>
> Coroutine options built atop threads suffer from inter-thread signaling
> delays, thread count limits, and high memory use.
>

There will need to be inter-thread signalling anyway, though?


>
> Options that use trampolines and bytecode rewriting impact performance and
> can't suspend/resume across call boundaries that haven't been likewise
> rewritten (so you have to rewrite everything).
>

Not only that, you'd have to rewrite all usage of monitorenter/monitorexit.


>
> True coroutines on the JVM would provide a lightweight way to
> cooperatively share a thread of execution across multiple call stacks. Go
> has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>

You still have problems with native code calls…


>
> - Charlie
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/ab3e0b50/attachment.html>

From viktor.klang at gmail.com  Fri Oct  7 17:18:21 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 23:18:21 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
Message-ID: <CANPzfU-6i3NWM2iH9sRpOjGFDuWVsDOhu+t+KC8Q5KGdQ1cHjw@mail.gmail.com>

Sorry if I sound negative, JVM-native coroutines would be interesting.

On Fri, Oct 7, 2016 at 11:18 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>
> On Fri, Oct 7, 2016 at 11:07 PM, Charles Oliver Nutter <
> headius at headius.com> wrote:
>
>> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com>
>> wrote:
>>
>>> Scala uses no magic, only macros. So no need for language support.
>>>
>>> Quasar does bytecode rewriting to implement fibers:
>>> https://github.com/puniverse/quasar
>>>
>>> Kotlin is also looking into coroutines: https://kotlin.lin
>>> k/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html
>>>
>>
>> Coroutine options built atop threads suffer from inter-thread signaling
>> delays, thread count limits, and high memory use.
>>
>
> There will need to be inter-thread signalling anyway, though?
>
>
>>
>> Options that use trampolines and bytecode rewriting impact performance
>> and can't suspend/resume across call boundaries that haven't been likewise
>> rewritten (so you have to rewrite everything).
>>
>
> Not only that, you'd have to rewrite all usage of monitorenter/monitorexit.
>
>
>>
>> True coroutines on the JVM would provide a lightweight way to
>> cooperatively share a thread of execution across multiple call stacks. Go
>> has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>>
>
> You still have problems with native code calls…
>
>
>>
>> - Charlie
>>
>
>
>
> --
> Cheers,
> √
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/f53a8ad7/attachment.html>

From vitalyd at gmail.com  Fri Oct  7 17:27:21 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 7 Oct 2016 17:27:21 -0400
Subject: [concurrency-interest]  Continuations / Fibers
In-Reply-To: <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
Message-ID: <CAHjP37HRRjxhc+WjB-esOu4KB2bqU+4mJqXt371MSSvmBkw5wQ@mail.gmail.com>

On Friday, October 7, 2016, Charles Oliver Nutter <headius at headius.com
<javascript:_e(%7B%7D,'cvml','headius at headius.com');>> wrote:

> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> Scala uses no magic, only macros. So no need for language support.
>>
>> Quasar does bytecode rewriting to implement fibers:
>> https://github.com/puniverse/quasar
>>
>> Kotlin is also looking into coroutines: https://kotlin.lin
>> k/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html
>>
>
> Coroutine options built atop threads suffer from inter-thread signaling
> delays, thread count limits, and high memory use.
>
> Options that use trampolines and bytecode rewriting impact performance and
> can't suspend/resume across call boundaries that haven't been likewise
> rewritten (so you have to rewrite everything).
>
> True coroutines on the JVM would provide a lightweight way to
> cooperatively share a thread of execution across multiple call stacks. Go
> has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>
Looks like C++ will also be getting them.

I think John Rose may have some JVM thoughts on this - this might be a good
post to the mlvm list rather than concurrency-interest (although I
understand why this list was picked).

>
> - Charlie
>


-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/5c6b10f6/attachment-0001.html>

From headius at headius.com  Fri Oct  7 17:29:27 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 16:29:27 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
Message-ID: <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>

On Fri, Oct 7, 2016 at 4:18 PM, Viktor Klang <viktor.klang at gmail.com> wrote:

> There will need to be inter-thread signalling anyway, though?
>

No. A thread can switch from one fiber to another without involving the
thread scheduler.


>  Not only that, you'd have to rewrite all usage of
> monitorenter/monitorexit.
>

Locking is a challenge but it's no better with thread-based coroutines. In
fact, it might be easier, since we wouldn't have to pass ownership of a
lock around to different threads.

 You still have problems with native code calls…
>

Not necessarily. If the JVM can see the size of the native frames on the
stack, it would be able to reify and swap them out too. JNI-wise, if
suspend/resume only happen from managed code, I think safepoint entry/exit
should be balanced on the stack too.

This definitely gets into some deeper magic than I know, but there has
already been a prototype of coroutines for OpenJDK, and it worked pretty
well: https://wiki.openjdk.java.net/display/mlvm/Coroutines

As with tail calling, someone needs to champion this work at a JVMS/JLS/JCP
level. It might be possible to do something short term through JEPs.

> Sorry if I sound negative, JVM-native coroutines would be interesting.

Not at all :-) These are fair points. Coroutines solve many problems of
thread-based "microthreading" and introduce some others of their own. At
least we'd have a very interesting and unusual hammer to add to our bag of
hammers!

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/41756e6e/attachment.html>

From nathanila at gmail.com  Fri Oct  7 17:40:44 2016
From: nathanila at gmail.com (Nathan & Ila Reynolds)
Date: Fri, 7 Oct 2016 15:40:44 -0600
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
Message-ID: <00fd01d220e3$7674b230$635e1690$@gmail.com>

Here’s a 10,000-foot view of a possible implementation of fibers in the JVM.

 

At the lowest level, a thread is simply a CPU state (i.e. registers), stack space and wakeup condition.  At a higher level, there are Java objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these objects are accessed through a CPU register so the CPU state handles this.

 

To implement fibers, it seems that when a fiber is going to block, then the registers have to be saved (e.g. x86 pusha instruction), the instruction pointer changed in the saved stated and the wakeup condition has to be created.  (The registers have a pointer to the stack.)  Once this is completed, the thread can go find another runnable fiber.  The thread then loads the registers for that fiber (e.g. x86 popa instruction) and starts executing.

 

>From the above 10,000-foot view, it seems like we could implement fibers in Java with no visible side-effects from Java.  The visible side-effects would be visible through JNI.  Am I missing something?

However, it doesn’t seem like we have saved that much.  We still have stack space allocated to each fiber.  We still have context switching but it is implemented in user space and not kernel space.  I don’t think this is going to make much difference since context switching is no longer costly.  Hence, I am not sure what this implementation of fibers really buys us that plain old threads don’t already accomplish.  Is there a better way to implement fibers which mitigates the cost of threads?

 

-Nathan

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Charles Oliver Nutter
Sent: Friday, October 07, 2016 3:07 PM
To: Viktor Klang <viktor.klang at gmail.com>
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Continuations / Fibers

 

On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

Scala uses no magic, only macros. So no need for language support.

 

Quasar does bytecode rewriting to implement fibers: https://github.com/puniverse/quasar

 

Kotlin is also looking into coroutines: https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html

 

Coroutine options built atop threads suffer from inter-thread signaling delays, thread count limits, and high memory use.

 

Options that use trampolines and bytecode rewriting impact performance and can't suspend/resume across call boundaries that haven't been likewise rewritten (so you have to rewrite everything).

 

True coroutines on the JVM would provide a lightweight way to cooperatively share a thread of execution across multiple call stacks. Go has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.

 

- Charlie

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/9ab066d4/attachment.html>

From viktor.klang at gmail.com  Fri Oct  7 17:41:01 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 23:41:01 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
Message-ID: <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>

On Fri, Oct 7, 2016 at 11:29 PM, Charles Oliver Nutter <headius at headius.com>
wrote:

> On Fri, Oct 7, 2016 at 4:18 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> There will need to be inter-thread signalling anyway, though?
>>
>
> No. A thread can switch from one fiber to another without involving the
> thread scheduler.
>

Sure, but if you want to be able to leverage multithreading then you're
going to need a jvm-level fiber scheduler anyway.
(compare to Erlang for instance).


>
>
>>  Not only that, you'd have to rewrite all usage of
>> monitorenter/monitorexit.
>>
>
> Locking is a challenge but it's no better with thread-based coroutines. In
> fact, it might be easier, since we wouldn't have to pass ownership of a
> lock around to different threads.
>

That will definitely affect runtime behavior tho.
(Same tradeoffs which the OS-level scheduler has when it comes to
thread-to-core migration/retention)


>
>  You still have problems with native code calls…
>>
>
> Not necessarily. If the JVM can see the size of the native frames on the
> stack, it would be able to reify and swap them out too. JNI-wise, if
> suspend/resume only happen from managed code, I think safepoint entry/exit
> should be balanced on the stack too.
>

Sure, but the vm wouldn't know if the native call ever hands back control
or not.


>
> This definitely gets into some deeper magic than I know, but there has
> already been a prototype of coroutines for OpenJDK, and it worked pretty
> well: https://wiki.openjdk.java.net/display/mlvm/Coroutines
>
>
Thanks for the link!


> As with tail calling, someone needs to champion this work at a
> JVMS/JLS/JCP level. It might be possible to do something short term through
> JEPs.
>

Sadly I'd expect a 5 year battle… :S


>
> > Sorry if I sound negative, JVM-native coroutines would be interesting.
>
> Not at all :-) These are fair points. Coroutines solve many problems of
> thread-based "microthreading" and introduce some others of their own. At
> least we'd have a very interesting and unusual hammer to add to our bag of
> hammers!
>

Worth having a look at the thoughts behind Scheduler Activations as well.


>
> - Charlie
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/52326ce8/attachment-0001.html>

From headius at headius.com  Fri Oct  7 17:52:05 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 16:52:05 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
 <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
Message-ID: <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>

On Fri, Oct 7, 2016 at 4:41 PM, Viktor Klang <viktor.klang at gmail.com> wrote:

> On Fri, Oct 7, 2016 at 11:29 PM, Charles Oliver Nutter <
> headius at headius.com> wrote:
>>
>> No. A thread can switch from one fiber to another without involving the
>> thread scheduler.
>>
>
> Sure, but if you want to be able to leverage multithreading then you're
> going to need a jvm-level fiber scheduler anyway.
> (compare to Erlang for instance).
>

Fibers, at least in the Ruby (and I believe win32) sense, do not cross
thread boundaries. Coroutines could or could not (but it would be more
useful if they could).

But that's largely beside the point. Yes, if you want to hand a coroutine
off from one thread to another, or have a group of N threads driving those
coroutines, they'll have to coordinate. But you could also have any number
of coroutines running on the *same* thread, without any handoff. You're
timeslicing one thread into many threadlets without any hand-off (other
than the overhead of suspend/resume).


>  Locking is a challenge but it's no better with thread-based coroutines.
> In fact, it might be easier, since we wouldn't have to pass ownership of a
> lock around to different threads.
>
> That will definitely affect runtime behavior tho.
> (Same tradeoffs which the OS-level scheduler has when it comes to
> thread-to-core migration/retention)
>

I think just about everything relating to true coroutines will affect
runtime behavior in some way :-)

It's a fundamentally new concurrency mechanism from anything we have on JVM
right now (other than the bytecode-rewriting options, which are a close
approximation).


>
>
>> Not necessarily. If the JVM can see the size of the native frames on the
>> stack, it would be able to reify and swap them out too. JNI-wise, if
>> suspend/resume only happen from managed code, I think safepoint entry/exit
>> should be balanced on the stack too.
>>
>
> Sure, but the vm wouldn't know if the native call ever hands back control
> or not.
>

If the native call never hands control back, it would never be able to
resume. My point above was that suspend/resume would only be problematic
when JNI code upcalls back into the JVM, but in that case it will have
exited its safepoint anyway. It should be possible to save off those native
frames without causing headaches for the GC.

At least, that's my impression of how it would work. Devil's in the details!

> As with tail calling, someone needs to champion this work at a
>> JVMS/JLS/JCP level. It might be possible to do something short term through
>> JEPs.
>>
>
> Sadly I'd expect a 5 year battle… :S
>

Yes...I wish someone with a pressing need would have started ten years ago.
I would have, if I weren't already working on a rather language
implementation :-D


> Not at all :-) These are fair points. Coroutines solve many problems of
>> thread-based "microthreading" and introduce some others of their own. At
>> least we'd have a very interesting and unusual hammer to add to our bag of
>> hammers!
>>
>
> Worth having a look at the thoughts behind Scheduler Activations as well.
>

I'll have a look, thanks!

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/46530ca5/attachment.html>

From viktor.klang at gmail.com  Fri Oct  7 17:58:24 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 7 Oct 2016 23:58:24 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
 <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
 <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>
Message-ID: <CANPzfU-3nO60YUynzhT6gv+LJV28EjkJzv_XXTp1_Ge+qh2z5g@mail.gmail.com>

On Fri, Oct 7, 2016 at 11:52 PM, Charles Oliver Nutter <headius at headius.com>
wrote:

>
> On Fri, Oct 7, 2016 at 4:41 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> On Fri, Oct 7, 2016 at 11:29 PM, Charles Oliver Nutter <
>> headius at headius.com> wrote:
>>>
>>> No. A thread can switch from one fiber to another without involving the
>>> thread scheduler.
>>>
>>
>> Sure, but if you want to be able to leverage multithreading then you're
>> going to need a jvm-level fiber scheduler anyway.
>> (compare to Erlang for instance).
>>
>
> Fibers, at least in the Ruby (and I believe win32) sense, do not cross
> thread boundaries. Coroutines could or could not (but it would be more
> useful if they could).
>
> But that's largely beside the point. Yes, if you want to hand a coroutine
> off from one thread to another, or have a group of N threads driving those
> coroutines, they'll have to coordinate. But you could also have any number
> of coroutines running on the *same* thread, without any handoff. You're
> timeslicing one thread into many threadlets without any hand-off (other
> than the overhead of suspend/resume).
>

Well of course—I've spent the past 7-8 years implementing Actors on the JVM
;)


>
>
>>  Locking is a challenge but it's no better with thread-based coroutines.
>> In fact, it might be easier, since we wouldn't have to pass ownership of a
>> lock around to different threads.
>>
>> That will definitely affect runtime behavior tho.
>> (Same tradeoffs which the OS-level scheduler has when it comes to
>> thread-to-core migration/retention)
>>
>
> I think just about everything relating to true coroutines will affect
> runtime behavior in some way :-)
>

Perhaps we need to define "true coroutines" so we're talking about the same
semantics :-)


>
> It's a fundamentally new concurrency mechanism from anything we have on
> JVM right now (other than the bytecode-rewriting options, which are a close
> approximation).
>

It's not really *that* new, I'd say, but I'm getting old ;)


>
>
>>
>>
>>> Not necessarily. If the JVM can see the size of the native frames on the
>>> stack, it would be able to reify and swap them out too. JNI-wise, if
>>> suspend/resume only happen from managed code, I think safepoint entry/exit
>>> should be balanced on the stack too.
>>>
>>
>> Sure, but the vm wouldn't know if the native call ever hands back control
>> or not.
>>
>
> If the native call never hands control back, it would never be able to
> resume.
>

Yes, but now all coroutines that are parked behind it will be starved
forver—unless—you migrate fibers across threads. :)


> My point above was that suspend/resume would only be problematic when JNI
> code upcalls back into the JVM, but in that case it will have exited its
> safepoint anyway. It should be possible to save off those native frames
> without causing headaches for the GC.
>

GC interaction is above my paygrade ^^


>
> At least, that's my impression of how it would work. Devil's in the
> details!
>
>> As with tail calling, someone needs to champion this work at a
>>> JVMS/JLS/JCP level. It might be possible to do something short term through
>>> JEPs.
>>>
>>
>> Sadly I'd expect a 5 year battle… :S
>>
>
> Yes...I wish someone with a pressing need would have started ten years
> ago. I would have, if I weren't already working on a rather language
> implementation :-D
>

Haha :)


>
>
>> Not at all :-) These are fair points. Coroutines solve many problems of
>>> thread-based "microthreading" and introduce some others of their own. At
>>> least we'd have a very interesting and unusual hammer to add to our bag of
>>> hammers!
>>>
>>
>> Worth having a look at the thoughts behind Scheduler Activations as well.
>>
>
> I'll have a look, thanks!
>
> - Charlie
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/3b79e072/attachment-0001.html>

From headius at headius.com  Fri Oct  7 18:03:56 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 17:03:56 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <00fd01d220e3$7674b230$635e1690$@gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
Message-ID: <CAE-f1xTWqy=H21yXyOavMMesoiXs1uuq+Ljb4LwGWnG25gticQ@mail.gmail.com>

On Fri, Oct 7, 2016 at 4:40 PM, Nathan & Ila Reynolds <nathanila at gmail.com>
 wrote:

> Here’s a 10,000-foot view of a possible implementation of fibers in the
> JVM.
>
>
>
> At the lowest level, a thread is simply a CPU state (i.e. registers),
> stack space and wakeup condition.  At a higher level, there are Java
> objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these
> objects are accessed through a CPU register so the CPU state handles this.
>
>
>
> To implement fibers, it seems that when a fiber is going to block, then
> the registers have to be saved (e.g. x86 pusha instruction), the
> instruction pointer changed in the saved stated and the wakeup condition
> has to be created.  (The registers have a pointer to the stack.)  Once this
> is completed, the thread can go find another runnable fiber.  The thread
> then loads the registers for that fiber (e.g. x86 popa instruction) and
> starts executing.
>

Yeah, that's about the size of it.


>  From the above 10,000-foot view, it seems like we could implement fibers
> in Java with no visible side-effects from Java.  The visible side-effects
> would be visible through JNI.  Am I missing something?
>

>From Java with no assistance from the JVM? I don't think so.
Bytecode-rewriting options fundamentally change how methods execute and
can't interact with non-rewritten code, so I don't consider them a complete
(or particularly useful) solution. At some level, someone's going to have
to get into the dirty native bits and actually save those registers off and
deal with saving any frames that happen to be shared (most coroutines are
not root threads; they are multiple tentacles dangling off of the same root
thread, so you need to manage partial stacks).

Have a look at the MLVM link I posted...there's a paper that describes
Lukas's approach (lazy stack reification, etc). He helped us implement
JRuby Fibers atop Coroutines at one point, and the single-thread speed of
Fiber switching improved something like 8x.


> However, it doesn’t seem like we have saved that much.  We still have
> stack space allocated to each fiber.  We still have context switching but
> it is implemented in user space and not kernel space.  I don’t think this
> is going to make much difference since context switching is no longer
> costly.  Hence, I am not sure what this implementation of fibers really
> buys us that plain old threads don’t already accomplish.  Is there a better
> way to implement fibers which mitigates the cost of threads?
>

Stack space for fibers can be smaller than for threads, since you have a
smaller downstream set of calls to make. In C Ruby, the default Fiber stack
size is something 1/10 the default Thread stack size. But yes, both options
have to have stack space *somewhere*.

Context switching in user space can *definitely* be cheaper than switching
threads at the kernel level, because the kernel doesn't have to be
involved, you don't fight the thread-scheduler to get your coroutine to
start running again, you don't have to do nasty tricks to keep threads hot,
etc. If you reduce the cost of switching enough, coroutine A's last
blocking instruction might be followed only very quickly by coroutine B's
first non-blocking instruction. Experimentally, the prototype coroutine
support made a huge different for single-thread coroutine scheduling in
JRuby. I've never managed to get thread-to-thread context switching to even
come close.

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/07ecc1be/attachment.html>

From oleksandr.otenko at gmail.com  Fri Oct  7 18:12:29 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Fri, 7 Oct 2016 23:12:29 +0100
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <00fd01d220e3$7674b230$635e1690$@gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
Message-ID: <EA4A71BA-DA1B-4BA5-BBFB-C330B0C7730D@gmail.com>

Anything can be written in assembly :-)

It offers a different way to separate concerns.

A glaring example, although far from what people usually see async is useful for:

try to write an Iterator for a binary Tree. You’ll need to spend a lot of mental effort to maintain the state - are you in the left leaf? where’s the parent node? oh, you need a pointer to the parent now? have you visited its right subtree? This is because you have to take into account the concern of the consumer of the data - they will need sequential access to the Tree nodes.


Now, with co-routines it becomes trivial to express and easy to understand - because the concern of how exactly the values are consumed, are separated from the concern of how the values are produced.

function* depthfirst(t){
  if (t === undefined) return;
  yield t.value;
  yield* depthfirst(t.left);
  yield* depthfirst(t.right);
}

Alex

> On 7 Oct 2016, at 22:40, Nathan & Ila Reynolds <nathanila at gmail.com> wrote:
> 
> Here’s a 10,000-foot view of a possible implementation of fibers in the JVM.
>  
> At the lowest level, a thread is simply a CPU state (i.e. registers), stack space and wakeup condition.  At a higher level, there are Java objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these objects are accessed through a CPU register so the CPU state handles this.
>  
> To implement fibers, it seems that when a fiber is going to block, then the registers have to be saved (e.g. x86 pusha instruction), the instruction pointer changed in the saved stated and the wakeup condition has to be created.  (The registers have a pointer to the stack.)  Once this is completed, the thread can go find another runnable fiber.  The thread then loads the registers for that fiber (e.g. x86 popa instruction) and starts executing.
>  
> From the above 10,000-foot view, it seems like we could implement fibers in Java with no visible side-effects from Java.  The visible side-effects would be visible through JNI.  Am I missing something?
> However, it doesn’t seem like we have saved that much.  We still have stack space allocated to each fiber.  We still have context switching but it is implemented in user space and not kernel space.  I don’t think this is going to make much difference since context switching is no longer costly.  Hence, I am not sure what this implementation of fibers really buys us that plain old threads don’t already accomplish.  Is there a better way to implement fibers which mitigates the cost of threads?
>  
> -Nathan
>  
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Charles Oliver Nutter
> Sent: Friday, October 07, 2016 3:07 PM
> To: Viktor Klang <viktor.klang at gmail.com>
> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Continuations / Fibers
>  
> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>> Scala uses no magic, only macros. So no need for language support.
>>  
>> Quasar does bytecode rewriting to implement fibers: https://github.com/puniverse/quasar <https://github.com/puniverse/quasar>
>>  
>> Kotlin is also looking into coroutines: https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html <https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html> 
> Coroutine options built atop threads suffer from inter-thread signaling delays, thread count limits, and high memory use.
>  
> Options that use trampolines and bytecode rewriting impact performance and can't suspend/resume across call boundaries that haven't been likewise rewritten (so you have to rewrite everything).
>  
> True coroutines on the JVM would provide a lightweight way to cooperatively share a thread of execution across multiple call stacks. Go has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>  
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/c46a9ce0/attachment-0001.html>

From headius at headius.com  Fri Oct  7 18:14:39 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 17:14:39 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU-3nO60YUynzhT6gv+LJV28EjkJzv_XXTp1_Ge+qh2z5g@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
 <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
 <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>
 <CANPzfU-3nO60YUynzhT6gv+LJV28EjkJzv_XXTp1_Ge+qh2z5g@mail.gmail.com>
Message-ID: <CAE-f1xSvK43VcA6+Tg3Us3OFBxnUarKMm34w_jSHyeZtNckhiQ@mail.gmail.com>

On Fri, Oct 7, 2016 at 4:58 PM, Viktor Klang <viktor.klang at gmail.com> wrote:

> Perhaps we need to define "true coroutines" so we're talking about the
> same semantics :-)
>

"True" coroutines, to me, are swappable stacks of execution that can hand
off control on the same thread (or possibly on a different thread), do not
involve the thread-scheduler to do so, but DO have real native execution
stacks with calls deepening that stack. It is not possible to implement
them in Java on today's JVMs.

It's a fundamentally new concurrency mechanism from anything we have on JVM
>> right now (other than the bytecode-rewriting options, which are a close
>> approximation).
>>
>
> It's not really *that* new, I'd say, but I'm getting old ;)
>

>From a VM perspective...it's very peculiar. I usually have a VM perspective
:-)

At a user level, coroutines simulated with threads or bytecode rewriting at
least *look* the same as true coroutines. The interesting differences are
not at that level.

If the native call never hands control back, it would never be able to
>> resume.
>>
>
> Yes, but now all coroutines that are parked behind it will be starved
> forver—unless—you migrate fibers across threads. :)
>

If you make a blocking call to *anything* that doesn't suspend, you're
blocking a thread. This is no different in calls from Java to Java; a
coroutine that calls a method that loops forever or a thread that calls an
actor that loops forever both get stuck. This is not unique to coroutines;
badly-behaved code is still badly-behaved code.

Perhaps this isn't clear: coroutines are *cooperatively* scheduled, not
*preemptively* scheduled like a thread. And just like in any
cooperatively-scheduled system, you have to cooperate :-)

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/d260a747/attachment.html>

From viktor.klang at gmail.com  Fri Oct  7 18:17:16 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sat, 8 Oct 2016 00:17:16 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CAE-f1xSvK43VcA6+Tg3Us3OFBxnUarKMm34w_jSHyeZtNckhiQ@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
 <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
 <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>
 <CANPzfU-3nO60YUynzhT6gv+LJV28EjkJzv_XXTp1_Ge+qh2z5g@mail.gmail.com>
 <CAE-f1xSvK43VcA6+Tg3Us3OFBxnUarKMm34w_jSHyeZtNckhiQ@mail.gmail.com>
Message-ID: <CANPzfU_7fUP2n9fUpTKOpRAEmN1gvLXcaiitaa4nHstSPEW6yw@mail.gmail.com>

On Sat, Oct 8, 2016 at 12:14 AM, Charles Oliver Nutter <headius at headius.com>
wrote:

> On Fri, Oct 7, 2016 at 4:58 PM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> Perhaps we need to define "true coroutines" so we're talking about the
>> same semantics :-)
>>
>
> "True" coroutines, to me, are swappable stacks of execution that can hand
> off control on the same thread (or possibly on a different thread), do not
> involve the thread-scheduler to do so, but DO have real native execution
> stacks with calls deepening that stack. It is not possible to implement
> them in Java on today's JVMs.
>
> It's a fundamentally new concurrency mechanism from anything we have on
>>> JVM right now (other than the bytecode-rewriting options, which are a close
>>> approximation).
>>>
>>
>> It's not really *that* new, I'd say, but I'm getting old ;)
>>
>
> From a VM perspective...it's very peculiar. I usually have a VM
> perspective :-)
>
> At a user level, coroutines simulated with threads or bytecode rewriting
> at least *look* the same as true coroutines. The interesting differences
> are not at that level.
>
> If the native call never hands control back, it would never be able to
>>> resume.
>>>
>>
>> Yes, but now all coroutines that are parked behind it will be starved
>> forver—unless—you migrate fibers across threads. :)
>>
>
> If you make a blocking call to *anything* that doesn't suspend, you're
> blocking a thread. This is no different in calls from Java to Java; a
> coroutine that calls a method that loops forever or a thread that calls an
> actor that loops forever both get stuck. This is not unique to coroutines;
> badly-behaved code is still badly-behaved code.
>
> Perhaps this isn't clear: coroutines are *cooperatively* scheduled, not
> *preemptively* scheduled like a thread. And just like in any
> cooperatively-scheduled system, you have to cooperate :-)
>

You can still instrument the instructions and do reduction counting á la
Erlang to address fairness and "timeslicing" so you get opportunity for
preemption.


>
> - Charlie
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161008/f9d2599f/attachment.html>

From headius at headius.com  Fri Oct  7 18:20:16 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 17:20:16 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <EA4A71BA-DA1B-4BA5-BBFB-C330B0C7730D@gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
 <EA4A71BA-DA1B-4BA5-BBFB-C330B0C7730D@gmail.com>
Message-ID: <CAE-f1xSEGgUoec8xg1gxbda3fTquiBMu=b-4j9gCS7Rj8n-FaQ@mail.gmail.com>

On Fri, Oct 7, 2016 at 5:12 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> try to write an Iterator for a binary Tree. You’ll need to spend a lot of
> mental effort to maintain the state - are you in the left leaf? where’s the
> parent node? oh, you need a pointer to the parent now? have you visited its
> right subtree? This is because you have to take into account the concern of
> the consumer of the data - they will need sequential access to the Tree
> nodes.
>
>
> Now, with co-routines it becomes trivial to express and easy to understand
> - because the concern of how exactly the values are consumed, are separated
> from the concern of how the values are produced.
>

I think it's helpful to think of coroutines as allowing you to reify stack
state as just another piece of your application. By suspending, you yank
off an execution context and swap in another on the same thread. When the
coroutine is later resumed, you have the same stack state as before ready
for execution. It's continuation-passing style, which of course can be
converted to a hand-written state machine that tracks all that
execution-contextual state. With coroutines, you don't have to write that
state machine :-)

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/db2d4a5c/attachment.html>

From headius at headius.com  Fri Oct  7 18:21:20 2016
From: headius at headius.com (Charles Oliver Nutter)
Date: Fri, 7 Oct 2016 17:21:20 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <CANPzfU_7fUP2n9fUpTKOpRAEmN1gvLXcaiitaa4nHstSPEW6yw@mail.gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <CANPzfU-B-_OguVH3tnDRtUJncMAPZDKG5Fgd3TXLsqE=eREZDg@mail.gmail.com>
 <CAE-f1xQ=rSw2SckdOQg66jO+X3+1YeUuoKkS15j9eKY9XiFE5Q@mail.gmail.com>
 <CANPzfU9K3n1XPvwg9-QS3U4K05+yTq28tC6pxJ43ydaThu+vKA@mail.gmail.com>
 <CAE-f1xSz9T6wO8vM8jUs4RVwTjm=vGe8Zx6FG89L=Cn_WqDnBw@mail.gmail.com>
 <CANPzfU-3nO60YUynzhT6gv+LJV28EjkJzv_XXTp1_Ge+qh2z5g@mail.gmail.com>
 <CAE-f1xSvK43VcA6+Tg3Us3OFBxnUarKMm34w_jSHyeZtNckhiQ@mail.gmail.com>
 <CANPzfU_7fUP2n9fUpTKOpRAEmN1gvLXcaiitaa4nHstSPEW6yw@mail.gmail.com>
Message-ID: <CAE-f1xQwUQEAtn2nJ7dYVYF=w=yXDhfP6HKg=Ewy8_upqoO_Yw@mail.gmail.com>

On Fri, Oct 7, 2016 at 5:17 PM, Viktor Klang <viktor.klang at gmail.com> wrote:

> You can still instrument the instructions and do reduction counting á la
> Erlang to address fairness and "timeslicing" so you get opportunity for
> preemption.
>

EOUTOFSCOPE

- Charlie
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161007/a8be6aeb/attachment-0001.html>

From davidcholmes at aapt.net.au  Fri Oct  7 18:54:37 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 8 Oct 2016 08:54:37 +1000
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <00fd01d220e3$7674b230$635e1690$@gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
Message-ID: <01b901d220ed$c83dff90$58b9feb0$@aapt.net.au>

How are thread-blocking operations expected to be handled? One of the arguments I’ve seen for fibers is to avoid using numerous threads in thread-per-connection style designs, but that implies the ability to block a fiber on I/O but not the thread. And that requires a complete rewrite of I/O libraries – or a new set for fibers to use.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan & Ila Reynolds
Sent: Saturday, October 8, 2016 7:41 AM
To: 'Charles Oliver Nutter' <headius at headius.com>; 'Viktor Klang' <viktor.klang at gmail.com>
Cc: 'concurrency-interest' <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Continuations / Fibers

 

Here’s a 10,000-foot view of a possible implementation of fibers in the JVM.

 

At the lowest level, a thread is simply a CPU state (i.e. registers), stack space and wakeup condition.  At a higher level, there are Java objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these objects are accessed through a CPU register so the CPU state handles this.

 

To implement fibers, it seems that when a fiber is going to block, then the registers have to be saved (e.g. x86 pusha instruction), the instruction pointer changed in the saved stated and the wakeup condition has to be created.  (The registers have a pointer to the stack.)  Once this is completed, the thread can go find another runnable fiber.  The thread then loads the registers for that fiber (e.g. x86 popa instruction) and starts executing.

 

>From the above 10,000-foot view, it seems like we could implement fibers in Java with no visible side-effects from Java.  The visible side-effects would be visible through JNI.  Am I missing something?

 

However, it doesn’t seem like we have saved that much.  We still have stack space allocated to each fiber.  We still have context switching but it is implemented in user space and not kernel space.  I don’t think this is going to make much difference since context switching is no longer costly.  Hence, I am not sure what this implementation of fibers really buys us that plain old threads don’t already accomplish.  Is there a better way to implement fibers which mitigates the cost of threads?

 

-Nathan

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Charles Oliver Nutter
Sent: Friday, October 07, 2016 3:07 PM
To: Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> >
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >
Subject: Re: [concurrency-interest] Continuations / Fibers

 

On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com> > wrote:

Scala uses no magic, only macros. So no need for language support.

 

Quasar does bytecode rewriting to implement fibers: https://github.com/puniverse/quasar

 

Kotlin is also looking into coroutines: https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html

 

Coroutine options built atop threads suffer from inter-thread signaling delays, thread count limits, and high memory use.

 

Options that use trampolines and bytecode rewriting impact performance and can't suspend/resume across call boundaries that haven't been likewise rewritten (so you have to rewrite everything).

 

True coroutines on the JVM would provide a lightweight way to cooperatively share a thread of execution across multiple call stacks. Go has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.

 

- Charlie

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161008/3a888a26/attachment.html>

From rk at rkuhn.info  Sat Oct  8 03:35:52 2016
From: rk at rkuhn.info (Roland Kuhn)
Date: Sat, 8 Oct 2016 09:35:52 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <01b901d220ed$c83dff90$58b9feb0$@aapt.net.au>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
 <01b901d220ed$c83dff90$58b9feb0$@aapt.net.au>
Message-ID: <0976C9D7-420B-40AD-B1A9-A43AA1BC6C9F@rkuhn.info>

One aspect I'd like to highlight is what Alex already mentioned with the tree traversal example: coroutines are not light-weight threads nor are they intended to be, they are cooperating routines. One more generic and recent example is https://github.com/storm-enroute/coroutines (for Scala). Sure, async–await can be implemented on top of that in about 20 lines of code, but that is not the point.

The point is to provide us with a code composition mechanism that easily transports accumulated state across several process steps, and IO is one example that benefits from this. But as David notes this would imply cooperation from the IO layer, meaning new interfaces and preferably also new syntax to make it clear what happens.

Going in the direction of green threads is not beneficial for the reasons outlined earlier (heavy weight, the reason why they were abandoned in Java 1.2 already). Threads already perfectly provide the tools for preemptive multitasking, the abstraction we seek for cooperative asynchronous composition shall be much more targeted and thereby also easier to grasp.

Regards, Roland 

Sent from my iPhone

> On 8 Oct 2016, at 00:54, David Holmes <davidcholmes at aapt.net.au> wrote:
> 
> How are thread-blocking operations expected to be handled? One of the arguments I’ve seen for fibers is to avoid using numerous threads in thread-per-connection style designs, but that implies the ability to block a fiber on I/O but not the thread. And that requires a complete rewrite of I/O libraries – or a new set for fibers to use.
>  
> David
>  
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan & Ila Reynolds
> Sent: Saturday, October 8, 2016 7:41 AM
> To: 'Charles Oliver Nutter' <headius at headius.com>; 'Viktor Klang' <viktor.klang at gmail.com>
> Cc: 'concurrency-interest' <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Continuations / Fibers
>  
> Here’s a 10,000-foot view of a possible implementation of fibers in the JVM.
>  
> At the lowest level, a thread is simply a CPU state (i.e. registers), stack space and wakeup condition.  At a higher level, there are Java objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these objects are accessed through a CPU register so the CPU state handles this.
>  
> To implement fibers, it seems that when a fiber is going to block, then the registers have to be saved (e.g. x86 pusha instruction), the instruction pointer changed in the saved stated and the wakeup condition has to be created.  (The registers have a pointer to the stack.)  Once this is completed, the thread can go find another runnable fiber.  The thread then loads the registers for that fiber (e.g. x86 popa instruction) and starts executing.
>  
> From the above 10,000-foot view, it seems like we could implement fibers in Java with no visible side-effects from Java.  The visible side-effects would be visible through JNI.  Am I missing something?
>  
> However, it doesn’t seem like we have saved that much.  We still have stack space allocated to each fiber.  We still have context switching but it is implemented in user space and not kernel space.  I don’t think this is going to make much difference since context switching is no longer costly.  Hence, I am not sure what this implementation of fibers really buys us that plain old threads don’t already accomplish.  Is there a better way to implement fibers which mitigates the cost of threads?
>  
> -Nathan
>  
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Charles Oliver Nutter
> Sent: Friday, October 07, 2016 3:07 PM
> To: Viktor Klang <viktor.klang at gmail.com>
> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Continuations / Fibers
>  
> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com> wrote:
> Scala uses no magic, only macros. So no need for language support.
>  
> Quasar does bytecode rewriting to implement fibers: https://github.com/puniverse/quasar
>  
> Kotlin is also looking into coroutines: https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html
>  
> Coroutine options built atop threads suffer from inter-thread signaling delays, thread count limits, and high memory use.
>  
> Options that use trampolines and bytecode rewriting impact performance and can't suspend/resume across call boundaries that haven't been likewise rewritten (so you have to rewrite everything).
>  
> True coroutines on the JVM would provide a lightweight way to cooperatively share a thread of execution across multiple call stacks. Go has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>  
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161008/4efb33fa/attachment-0001.html>

From kirk at kodewerk.com  Sat Oct  8 04:50:27 2016
From: kirk at kodewerk.com (kirk at kodewerk.com)
Date: Sat, 8 Oct 2016 10:50:27 +0200
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <00fd01d220e3$7674b230$635e1690$@gmail.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
Message-ID: <83BD5DBB-D7C6-493F-91D2-C71637C8498F@kodewerk.com>

Hi Nathan,

Nice overview. Sun already tried this with LWPs in Solaris. An LWP was tied to a thread. The OS scheduled the thread but was also involved with which LWP the thread was running. Sound familiar? (https://docs.oracle.com/cd/E19455-01/806-5257/mtintro-72944/index.html <https://docs.oracle.com/cd/E19455-01/806-5257/mtintro-72944/index.html>)

I wouldn’t exactly call that effort a stunning success as the cost of moving between LWPs did out-weigh the benefits of keeping a thread live and avoiding the cost of the context switch. FWIW, the context switching is exceptionally cheap as it’s simply a remapping of which bits of silicon are the current register set. At worst it’s an exchange packet that gets mapped in and out of the processor. IIRC the cost is about 40 cycles for the later and far few for the former. The real cost, the thing that can drive costs up are the some times > 80,000 clocks are needed to realign the caches to the current working thread. I’m not sure that fibres will solve this particular problem. In fact it seems like it would bury the problem in a way that would make it difficult to see. Right now the CPU counters clearly display context switching counts. As I’ve always said, context switching isn’t a problem, it’s a symptom of an underlying problem. Hiding the symptoms does seem like a great idea.

As you most likely know, problems with the thread scheduling come from all over the place. We have problems as a result of the activity being bound to core 0. If we unbind we end up with other problems due to odd harmonics that can occur. The net result is that there are a ton of cycles that left on the table simply because the thread scheduler isn’t able to get runnable threads into idle cores. There was a study done by a group at UBC (I’ll see if I can dig up the link) that dug deeper into this problem. Anyways, this suggests that done right, we might see some benefits from fibers. Or, it suggests that we might see greater benefits in coming up with alternative ways to schedule threads.

Don’t know, just thinking out loud.

Kind regards,
Kirk

PS, the link for a decade of wasted cores. https://www.ece.ubc.ca/~sasha/papers/eurosys16-final29.pdf <https://www.ece.ubc.ca/~sasha/papers/eurosys16-final29.pdf>

> On Oct 7, 2016, at 11:40 PM, Nathan & Ila Reynolds <nathanila at gmail.com> wrote:
> 
> Here’s a 10,000-foot view of a possible implementation of fibers in the JVM.
>  
> At the lowest level, a thread is simply a CPU state (i.e. registers), stack space and wakeup condition.  At a higher level, there are Java objects tied to the thread (i.e. Thread and ThreadLocals).  On x86, these objects are accessed through a CPU register so the CPU state handles this.
>  
> To implement fibers, it seems that when a fiber is going to block, then the registers have to be saved (e.g. x86 pusha instruction), the instruction pointer changed in the saved stated and the wakeup condition has to be created.  (The registers have a pointer to the stack.)  Once this is completed, the thread can go find another runnable fiber.  The thread then loads the registers for that fiber (e.g. x86 popa instruction) and starts executing.
>  
> From the above 10,000-foot view, it seems like we could implement fibers in Java with no visible side-effects from Java.  The visible side-effects would be visible through JNI.  Am I missing something?
> However, it doesn’t seem like we have saved that much.  We still have stack space allocated to each fiber.  We still have context switching but it is implemented in user space and not kernel space.  I don’t think this is going to make much difference since context switching is no longer costly.  Hence, I am not sure what this implementation of fibers really buys us that plain old threads don’t already accomplish.  Is there a better way to implement fibers which mitigates the cost of threads?
>  
> -Nathan
>  
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Charles Oliver Nutter
> Sent: Friday, October 07, 2016 3:07 PM
> To: Viktor Klang <viktor.klang at gmail.com>
> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Continuations / Fibers
>  
> On Fri, Oct 7, 2016 at 3:46 PM, Viktor Klang <viktor.klang at gmail.com <mailto:viktor.klang at gmail.com>> wrote:
>> Scala uses no magic, only macros. So no need for language support.
>>  
>> Quasar does bytecode rewriting to implement fibers: https://github.com/puniverse/quasar <https://github.com/puniverse/quasar>
>>  
>> Kotlin is also looking into coroutines: https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html <https://kotlin.link/articles/Andrey-Breslav-Kotlin-Coroutines-JVMLS-2016.html> 
> Coroutine options built atop threads suffer from inter-thread signaling delays, thread count limits, and high memory use.
>  
> Options that use trampolines and bytecode rewriting impact performance and can't suspend/resume across call boundaries that haven't been likewise rewritten (so you have to rewrite everything).
>  
> True coroutines on the JVM would provide a lightweight way to cooperatively share a thread of execution across multiple call stacks. Go has 'em, Ruby has 'em, Python has 'em...the JVM needs 'em.
>  
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161008/f1563ebf/attachment.html>

