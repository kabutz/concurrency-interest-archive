From rl.stpuu at gmail.com  Fri Feb  9 09:00:01 2018
From: rl.stpuu at gmail.com (Roussanka Loukanova)
Date: Fri, 9 Feb 2018 15:00:01 +0100
Subject: [concurrency-interest] CfP: AI aspects of Reasoning, Information,
	and Memory 2018 (AIRIM'18)
Message-ID: <CACAe74gGS8MNkcewLHPgz3fDFHV1wUc5sB4tfECjAH1eono6ng@mail.gmail.com>

CALL FOR PAPERS

3rd International Workshop on
AI aspects of Reasoning, Information, and Memory 2018 (AIRIM'18)

https://www.fedcsis.org/2018/airim

Poznan, Poland, 9-12 September, 2018

-------------------------------------------------------------
SCOPE
There is general realization that computational models of languages and
reasoning can be improved by integration of heterogeneous resources of
information, e.g., multidimensional diagrams, images, language, syntax,
semantics, quantitative data, memory. While the event targets promotion of
integrated computational approaches, we invite contributions from any
individual areas related to information, language, memory, reasoning.

TOPICS

We welcome submissions of papers on the following topics, without limiting
to them, across approaches, methods, theories, and applications:

- Reasoning systems --- theories and applications
- Proof systems and model checkers
- Theories of computation and information
- Interactive computation and reasoning
- Computation and reasoning with heterogeneous information
- Space and time in information, language, memory, and reasoning
- Partiality, underspecification, vagueness, and possibilities
- Detection of and reasoning with inconsistency
- Logic and language --- approaches, theories, methods
- Computational morphology, syntax, semantics, and interfaces between these
- Constraint-based and type-theoretic approaches and grammars
- Logical approaches to multilingual processing
- Logical and computational foundations in machine learning and information
retrieval
- Mathematics for linguistics and cognitive science
- Reasoning, information, and memory in computational neuroscience and life
sciences
- Interdisciplinary approaches to information, language, memory, and
reasoning

IMPORTANT DATES

- Paper submission (strict deadline): May 15 2018 23:59:59 pm HST
- Position paper submission: June 12, 2018
- Authors notification: June 24, 2018
- Final paper submission and registration: July 03, 2018
- Final deadline for discounted fee: August 01, 2018
- Conference dates: September 9-12, 2018

PAPER SUBMISSION and PUBLICATIONS

The publication rules, status, and the submission page for AIRIM'18 are the
same as for AAIA'18 | FedCSIS:

https://www.fedcsis.org/2018/airim

https://www.fedcsis.org/2018/instructions

- Authors should submit draft papers (as Postscript, PDF or MSWord file)
- The total length of a paper should not exceed 10 pages IEEE style
(including tables, figures and references). IEEE style templates are
available at:
https://fedcsis.org/2018/for_authors
- Papers will be refereed and accepted on the basis of their scientific
merit and relevance to the workshop
- Preprints containing accepted papers will be published on a USB memory
stick provided to the FedCSIS participants
- Only papers presented at the conference will be published in Conference
Proceedings and submitted for inclusion in the IEEE XploreÂ® database
- Conference proceedings will be published in a volume with ISBN, ISSN and
DOI numbers and posted at the conference WWW site
- Conference proceedings will be indexed in BazEkon and submitted for
indexation in: Thomson Reuters - Conference Proceedings Citation Index,
SciVerse Scopus, Inspec, Index Copernicus, DBLP Computer Science
Bibliography and Google Scholar
- Extended versions of selected papers presented during the conference will
be published as Special Issue(s)
- Organizers reserve right to move accepted papers between FedCSIS events

Event Chairs

- Grabowski, Adam, Institute of Informatics, University of Bialystok,
Bialystok, Poland
- Ishihara, Hajime, Japan Advanced Institute of Science and Technology,
Japan
- Loukanova, Roussanka, Stockholm University, Sweden
- Schwarzweller, Christoph, Institute of Informatics, University of Gdansk,
Poland
- van den Herik, Jaap, Leiden University, The Netherlands

CONTACT INFORMATION
Roussanka Loukanova (rloukanova at gmail.com)
-------------------------------------------------------------
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180209/33e9b75c/attachment.html>

From nathanila at gmail.com  Mon Feb 12 14:27:53 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Mon, 12 Feb 2018 12:27:53 -0700
Subject: [concurrency-interest] Fencing Sanity Check
Message-ID: <fd38df1f-7c05-64a5-2918-279305072d30@gmail.com>

I asked the following question on StackOverflow. 
https://stackoverflow.com/questions/48754065/re-ordering-of-assignments-and-adding-a-fence 
Please respond there.

The following code looks a little strange because I have simplified it 
down to the bare essentials.Â  I think the code has an ordering problem.Â  
I am looking at the first table in 
http://g.oswego.edu/dl/jmm/cookbook.html and it seems the normal store 
can be reordered with the volatile store in change().

Can the assignment to m_normal in change() move ahead of the assignment 
of m_volatile?Â  In other words, can get() return null?

How should I solve this?Â  Would adding "value = m_volatile;" after 
"m_volatile = value;" prevent the assignment of m_normal happening 
before the assignment of m_volatile?

private volatile Object m_volatile;
private          Object m_normal;

....

public void change()
{
    Object value;

    value = m_normal;

    if (value == null)
       return;

    m_volatile = value;
    m_normal   = null;
}

public Object get()
{
    Object value;

    value = m_normal;

    if (value == null)
       value = m_volatile;
    
    return(value);
}

-- -Nathan

From andy at stagirite.com  Thu Feb 15 14:34:07 2018
From: andy at stagirite.com (andy at stagirite.com)
Date: Thu, 15 Feb 2018 12:34:07 -0700
Subject: [concurrency-interest] hypervisor-based single cpu instances and
	atomic CAS
Message-ID: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180215/5137add8/attachment.html>

From gil at azul.com  Thu Feb 15 15:33:15 2018
From: gil at azul.com (Gil Tene)
Date: Thu, 15 Feb 2018 20:33:15 +0000
Subject: [concurrency-interest] hypervisor-based single cpu instances
	and	atomic CAS
In-Reply-To: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
Message-ID: <0AD9BDFF-9BFE-43BF-9D87-CD8B6027B636@azul.com>



> On Feb 15, 2018, at 11:34 AM, andy--- via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> It seems that AWS ec2-instances give huge price breaks per cpu for workloads that can be split to work on a set of "virtual" single-cpu instances.

I would start with examining this premise (that single-cpu instances are heavily discounted). I think it is wrong.

E.g. here is a snapshot of the current on-demand pricing of low end instances on AWS (the ones that have 1 vcore options):

vCPU	ECU	Memory (GiB)	Instance Storage (GB)	Linux/UNIX Usage
General Purpose - Current Generation
t2.nano	1	Variable	0.5	EBS Only	$0.0058 per Hour
t2.micro	1	Variable	1	EBS Only	$0.0116 per Hour
t2.small	1	Variable	2	EBS Only	$0.023 per Hour
t2.medium	2	Variable	4	EBS Only	$0.0464 per Hour
t2.large	2	Variable	8	EBS Only	$0.0928 per Hour
t2.xlarge	4	Variable	16	EBS Only	$0.1856 per Hour

As you can see, the price is actually pretty linear per GB. And the smallest amount of GB/vcore you can get (when going above 1 vcore) is 2GB. The only option for renting <2GB instances is with single vcore instance, and the price remains linear per GB down to 0.5GB.

So a way to look at the above is "single CPU instances give you an options of renting instances with less than 2GB of memory". Or "<4GB AWS instances have only 1 vcore".

You may look at the above and think "If I can keep my cpu-intensive workloads to <0.5GB of memory, I can get a lot of cheap CPU using single vcore instances". But that would be misleading. The physical machines AWS uses have at least 2GB physical memory per hyperthread (and likely a lot more, ranging to 4GB-8GB per hyperthread on modern HW). E.g. A single 2 socket, 48 vcore, 128GB physical machine has enough memory to hold 256 t2.nano instances. If you actually rented 256 those instances you are not getting 256 vcores, and may only get 48 (or even less).

> I am wondering about my heavy use of lockfree datastructures in j.u.c and atomic variables in particular for these workloads, and the underlying CAS call that these are mostly based on on (relative to such single cpu machines in the hypervisor sense).  Are these CAS calls overly costly, making lockfree algorithms an anti-pattern on such machines?  Should traditional synchronized be preferred?  Is there any difference between hypervisor machines that have one cpu and real machines that do (if any such still exist), relative to the CAS?

It may be interesting to have a mode where the JIT is told the JVM is limited to a single core (either because the OS has only 1 vcore, or because the JVM is limited to a single core via isolcpus of cpu sets). The JIT would then be able to emit non-atomic CAS, and remove cpu-ordeing instructions for e.g. volatile writes, neither of which will affect program order, and both of which could result in faster execution of the same logic.

> 
> The answers to this question might also imply that nodejs is more performant than java on such single cpu machine instances in the cloud.  Which would come as a big surprise.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180215/1d87386d/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 874 bytes
Desc: Message signed with OpenPGP
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180215/1d87386d/attachment.sig>

From nathanila at gmail.com  Thu Feb 15 15:40:35 2018
From: nathanila at gmail.com (Nathan and Ila Reynolds)
Date: Thu, 15 Feb 2018 13:40:35 -0700
Subject: [concurrency-interest] hypervisor-based single cpu instances
 and atomic CAS
In-Reply-To: <0AD9BDFF-9BFE-43BF-9D87-CD8B6027B636@azul.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
 <0AD9BDFF-9BFE-43BF-9D87-CD8B6027B636@azul.com>
Message-ID: <5f39d50f-363d-2727-a7a6-2b4ec0204211@gmail.com>



On 2/15/2018 1:33 PM, Gil Tene via Concurrency-interest wrote:
>
>
>> On Feb 15, 2018, at 11:34 AM, andy--- via Concurrency-interest 
>> <concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>>
>> It seems that AWS ec2-instances give huge price breaks per cpu for 
>> workloads that can be split to work on a set of "virtual" single-cpu 
>> instances.
>
> I would start with examining this premise (that single-cpu instances 
> are heavily discounted). I think it is wrong.
>
> E.g. here is a snapshot of the current on-demand pricing of low end 
> instances on AWS (the ones that have 1 vcore options):
>
>
> 	vCPU 	ECU 	Memory (GiB) 	Instance Storage (GB) 	Linux/UNIX Usage
> General Purpose - Current Generation
> t2.nano 	1 	Variable 	0.5 	EBS Only 	$0.0058 per Hour
> t2.micro 	1 	Variable 	1 	EBS Only 	$0.0116 per Hour
> t2.small 	1 	Variable 	2 	EBS Only 	$0.023 per Hour
> t2.medium 	2 	Variable 	4 	EBS Only 	$0.0464 per Hour
> t2.large 	2 	Variable 	8 	EBS Only 	$0.0928 per Hour
> t2.xlarge 	4 	Variable 	16 	EBS Only 	$0.1856 per Hour
>
>
> As you can see, the price is actually pretty linear per GB. And the 
> smallest amount of GB/vcore you can get (when going above 1 vcore) is 
> 2GB. The only option for renting <2GB instances is with single vcore 
> instance, and the price remains linear per GB down to 0.5GB.
>
> So a way to look at the above is "single CPU instances give you an 
> options of renting instances with less than 2GB of memory". Or "<4GB 
> AWS instances have only 1 vcore".
>
> You may look at the above and think "If I can keep my cpu-intensive 
> workloads to <0.5GB of memory, I can get a lot of cheap CPU using 
> single vcore instances". But that would be misleading. The physical 
> machines AWS uses have at least 2GB physical memory per hyperthread 
> (and likely a lot more, ranging to 4GB-8GB per hyperthread on modern 
> HW). E.g. A single 2 socket, 48 vcore, 128GB physical machine has 
> enough memory to hold 256 t2.nano instances. If you actually rented 
> 256 those instances you are not getting 256 vcores, and may only get 
> 48 (or even less).
>
>> I am wondering about my heavy use of lockfree datastructures in j.u.c 
>> and atomic variables in particular for these workloads, and the 
>> underlying CAS call that these are mostly based on on (relative to 
>> such single cpu machines in the hypervisor sense). Are these CAS 
>> calls overly costly, making lockfree algorithms an anti-pattern on 
>> such machines?Â  Should traditional synchronized be preferred?Â  Is 
>> there any difference between hypervisor machines that have one cpu 
>> and real machines that do (if any such still exist), relative to the CAS?
>
> It may be interesting to have a mode where the JIT is told the JVM is 
> limited to a single core (either because the OS has only 1 vcore, or 
> because the JVM is limited to a single core via isolcpus of cpu sets). 
> The JIT would then be able to emit non-atomic CAS, and remove 
> cpu-ordeing instructions for e.g. volatile writes, neither of which 
> will affect program order, and both of which could result in faster 
> execution of the same logic.
JIT can already do single hardware thread optimizations.Â  This used to 
default on.Â  Many years ago, I asked for it to default to off since 
affinity masking (I think) confused the JVM into making the wrong 
decision and cause crashes.Â  If the feature defaults to off, there is a 
command line switch to turn it on.

I have not heard if JIT can do single core (2 hardware threads) 
optimizations.Â  There might be some optimizations to do here but they 
will be limited compared to single hardware thread.
>
>>
>> The answers to this question might also imply that nodejs is more 
>> performant than java on such single cpu machine instances in the 
>> cloud.Â  Which would come as a big surprise.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
-Nathan

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180215/b0741b53/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Feb 15 19:50:44 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Feb 2018 10:50:44 +1000
Subject: [concurrency-interest] hypervisor-based single cpu instances
	and atomic CAS
In-Reply-To: <5f39d50f-363d-2727-a7a6-2b4ec0204211@gmail.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
 <0AD9BDFF-9BFE-43BF-9D87-CD8B6027B636@azul.com>
 <5f39d50f-363d-2727-a7a6-2b4ec0204211@gmail.com>
Message-ID: <00bf01d3a6c0$2d2de330$8789a990$@aapt.net.au>

It's ironic that we are (were?) on the verge of ripping out os::is_MP() and always assuming MP at build and runtime.

 

BTW with single-core we can get rid of memory ordering instructions, but CAS must still be atomic wrt. any possible context
switching.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan and Ila Reynolds via
Concurrency-interest
Sent: Friday, February 16, 2018 6:41 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] hypervisor-based single cpu instances and atomic CAS

 

 

 

On 2/15/2018 1:33 PM, Gil Tene via Concurrency-interest wrote:

 





On Feb 15, 2018, at 11:34 AM, andy--- via Concurrency-interest <concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu> > wrote:

 

It seems that AWS ec2-instances give huge price breaks per cpu for workloads that can be split to work on a set of "virtual"
single-cpu instances.

 

I would start with examining this premise (that single-cpu instances are heavily discounted). I think it is wrong.

 

E.g. here is a snapshot of the current on-demand pricing of low end instances on AWS (the ones that have 1 vcore options):

 

	
vCPU

ECU

Memory (GiB)

Instance Storage (GB)

Linux/UNIX Usage


General Purpose - Current Generation


t2.nano

1

Variable

0.5

EBS Only

$0.0058 per Hour


t2.micro

1

Variable

1

EBS Only

$0.0116 per Hour


t2.small

1

Variable

2

EBS Only

$0.023 per Hour


t2.medium

2

Variable

4

EBS Only

$0.0464 per Hour


t2.large

2

Variable

8

EBS Only

$0.0928 per Hour


t2.xlarge

4

Variable

16

EBS Only

$0.1856 per Hour

 

As you can see, the price is actually pretty linear per GB. And the smallest amount of GB/vcore you can get (when going above 1
vcore) is 2GB. The only option for renting <2GB instances is with single vcore instance, and the price remains linear per GB down to
0.5GB.

 

So a way to look at the above is "single CPU instances give you an options of renting instances with less than 2GB of memory". Or
"<4GB AWS instances have only 1 vcore".

 

You may look at the above and think "If I can keep my cpu-intensive workloads to <0.5GB of memory, I can get a lot of cheap CPU
using single vcore instances". But that would be misleading. The physical machines AWS uses have at least 2GB physical memory per
hyperthread (and likely a lot more, ranging to 4GB-8GB per hyperthread on modern HW). E.g. A single 2 socket, 48 vcore, 128GB
physical machine has enough memory to hold 256 t2.nano instances. If you actually rented 256 those instances you are not getting 256
vcores, and may only get 48 (or even less).





I am wondering about my heavy use of lockfree datastructures in j.u.c and atomic variables in particular for these workloads, and
the underlying CAS call that these are mostly based on on (relative to such single cpu machines in the hypervisor sense).  Are these
CAS calls overly costly, making lockfree algorithms an anti-pattern on such machines?  Should traditional synchronized be preferred?
Is there any difference between hypervisor machines that have one cpu and real machines that do (if any such still exist), relative
to the CAS?

 

It may be interesting to have a mode where the JIT is told the JVM is limited to a single core (either because the OS has only 1
vcore, or because the JVM is limited to a single core via isolcpus of cpu sets). The JIT would then be able to emit non-atomic CAS,
and remove cpu-ordeing instructions for e.g. volatile writes, neither of which will affect program order, and both of which could
result in faster execution of the same logic. 

JIT can already do single hardware thread optimizations.  This used to default on.  Many years ago, I asked for it to default to off
since affinity masking (I think) confused the JVM into making the wrong decision and cause crashes.  If the feature defaults to off,
there is a command line switch to turn it on.

I have not heard if JIT can do single core (2 hardware threads) optimizations.  There might be some optimizations to do here but
they will be limited compared to single hardware thread.







 

The answers to this question might also imply that nodejs is more performant than java on such single cpu machine instances in the
cloud.  Which would come as a big surprise.

 

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest







_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





-- 
-Nathan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180216/0290aa0d/attachment.html>

From aph at redhat.com  Mon Feb 19 04:23:14 2018
From: aph at redhat.com (Andrew Haley)
Date: Mon, 19 Feb 2018 09:23:14 +0000
Subject: [concurrency-interest] hypervisor-based single cpu instances
 and atomic CAS
In-Reply-To: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
Message-ID: <47c96ce1-6f07-4180-4b7f-6247dcf1c692@redhat.com>

On 15/02/18 19:34, andy--- via Concurrency-interest wrote:
> I am wondering about my heavy use of lockfree datastructures in j.u.c and atomic 
> variables in particular for these workloads, and the underlying CAS call that 
> these are mostly based on on (relative to such single cpu machines in the 
> hypervisor sense).  Are these CAS calls overly costly, making lockfree 
> algorithms an anti-pattern on such machines?  Should traditional synchronized be 
> preferred?
Just one note: synchronized uses CAS under the hood.  If locks are (mostly)
uncontended, the cost of synchronized is the cost of a CAS or two.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From aph at redhat.com  Mon Feb 19 04:25:50 2018
From: aph at redhat.com (Andrew Haley)
Date: Mon, 19 Feb 2018 09:25:50 +0000
Subject: [concurrency-interest] hypervisor-based single cpu instances
 and atomic CAS
In-Reply-To: <00bf01d3a6c0$2d2de330$8789a990$@aapt.net.au>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
 <0AD9BDFF-9BFE-43BF-9D87-CD8B6027B636@azul.com>
 <5f39d50f-363d-2727-a7a6-2b4ec0204211@gmail.com>
 <00bf01d3a6c0$2d2de330$8789a990$@aapt.net.au>
Message-ID: <c99f0afb-393b-7498-ca0d-9aadf5ada3f1@redhat.com>

On 16/02/18 00:50, David Holmes via Concurrency-interest wrote:

> It's ironic that we are (were?) on the verge of ripping out
> os::is_MP() and always assuming MP at build and runtime.

Not really, IMO.  With virtualization and containers you get
migration, and you might migrate a running VM to another with more
virtul CPUs.  Even if there's some performance advantage it's not
safe to do this in general.

-- 
Andrew Haley
Java Platform Lead Engineer
Red Hat UK Ltd. <https://www.redhat.com>
EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671

From viktor.klang at gmail.com  Mon Feb 19 04:50:48 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 19 Feb 2018 10:50:48 +0100
Subject: [concurrency-interest] hypervisor-based single cpu instances
 and atomic CAS
In-Reply-To: <47c96ce1-6f07-4180-4b7f-6247dcf1c692@redhat.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
 <47c96ce1-6f07-4180-4b7f-6247dcf1c692@redhat.com>
Message-ID: <CANPzfU8gLaNewX3XdBJy6ocOAfi1Mtmc-CfRSE-kVR_punWaLA@mail.gmail.com>

IMO the cost of synchronized is not the cost of implementing the feature
itself, since it is eclipsed by the cost of running it in production. ðŸ˜Š

-- 
Cheers,
âˆš

On Feb 19, 2018 10:26, "Andrew Haley via Concurrency-interest" <
concurrency-interest at cs.oswego.edu> wrote:

> On 15/02/18 19:34, andy--- via Concurrency-interest wrote:
> > I am wondering about my heavy use of lockfree datastructures in j.u.c
> and atomic
> > variables in particular for these workloads, and the underlying CAS call
> that
> > these are mostly based on on (relative to such single cpu machines in the
> > hypervisor sense).  Are these CAS calls overly costly, making lockfree
> > algorithms an anti-pattern on such machines?  Should traditional
> synchronized be
> > preferred?
> Just one note: synchronized uses CAS under the hood.  If locks are (mostly)
> uncontended, the cost of synchronized is the cost of a CAS or two.
>
> --
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180219/b2163303/attachment.html>

From oleksandr.otenko at gmail.com  Tue Feb 20 04:27:35 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 20 Feb 2018 09:27:35 +0000
Subject: [concurrency-interest] Fencing Sanity Check
In-Reply-To: <fd38df1f-7c05-64a5-2918-279305072d30@gmail.com>
References: <fd38df1f-7c05-64a5-2918-279305072d30@gmail.com>
Message-ID: <1C5F81E1-CE0F-4A2E-BF78-CAFCE1577903@gmail.com>

No, it wonâ€™t help the API as a whole. get() is racy whatever you do to change().

Alex


> On 12 Feb 2018, at 19:27, Nathan and Ila Reynolds via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> I asked the following question on StackOverflow. https://stackoverflow.com/questions/48754065/re-ordering-of-assignments-and-adding-a-fence Please respond there.
> 
> The following code looks a little strange because I have simplified it down to the bare essentials.  I think the code has an ordering problem.  I am looking at the first table in http://g.oswego.edu/dl/jmm/cookbook.html and it seems the normal store can be reordered with the volatile store in change().
> 
> Can the assignment to m_normal in change() move ahead of the assignment of m_volatile?  In other words, can get() return null?
> 
> How should I solve this?  Would adding "value = m_volatile;" after "m_volatile = value;" prevent the assignment of m_normal happening before the assignment of m_volatile?
> 
> private volatile Object m_volatile;
> private          Object m_normal;
> 
> ....
> 
> public void change()
> {
>   Object value;
> 
>   value = m_normal;
> 
>   if (value == null)
>      return;
> 
>   m_volatile = value;
>   m_normal   = null;
> }
> 
> public Object get()
> {
>   Object value;
> 
>   value = m_normal;
> 
>   if (value == null)
>      value = m_volatile;
>      return(value);
> }
> 
> -- -Nathan
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at gmail.com  Thu Feb 22 05:19:03 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 22 Feb 2018 10:19:03 +0000
Subject: [concurrency-interest] hypervisor-based single cpu instances
	and atomic CAS
In-Reply-To: <47c96ce1-6f07-4180-4b7f-6247dcf1c692@redhat.com>
References: <20180215123407.85d28b18474c37ff99588e0f81557839.1a9b8da42d.wbe@email26.godaddy.com>
 <47c96ce1-6f07-4180-4b7f-6247dcf1c692@redhat.com>
Message-ID: <C9A081B0-6AE8-4BCB-8B56-11F2BB8146F6@gmail.com>

JVM may even bias the locks, so they remain owned by the thread until a contender claims them.

A single-CPU process wonâ€™t have the contenders, so the synchronized blocks will be a few reads.


Alex


> On 19 Feb 2018, at 09:23, Andrew Haley via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> On 15/02/18 19:34, andy--- via Concurrency-interest wrote:
>> I am wondering about my heavy use of lockfree datastructures in j.u.c and atomic 
>> variables in particular for these workloads, and the underlying CAS call that 
>> these are mostly based on on (relative to such single cpu machines in the 
>> hypervisor sense).  Are these CAS calls overly costly, making lockfree 
>> algorithms an anti-pattern on such machines?  Should traditional synchronized be 
>> preferred?
> Just one note: synchronized uses CAS under the hood.  If locks are (mostly)
> uncontended, the cost of synchronized is the cost of a CAS or two.
> 
> -- 
> Andrew Haley
> Java Platform Lead Engineer
> Red Hat UK Ltd. <https://www.redhat.com>
> EAC8 43EB D3EF DB98 CC77 2FAD A5CD 6035 332F A671
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From james at lightbend.com  Wed Feb 28 22:37:46 2018
From: james at lightbend.com (James Roper)
Date: Thu, 1 Mar 2018 14:37:46 +1100
Subject: [concurrency-interest] Reactive Streams Utility API
Message-ID: <CABY0rKMNfKWgi6i=z71Xdcppp59UU1LkxtrziuBx84LrtS25Ew@mail.gmail.com>

Hi all,

We (Lightbend) would like to put forward a proposal for a Reactive Streams
utility API for building instances of juc.Flow interfaces. The rationale,
goals and non goals for this, along with our proposed approach to the API,
and an actual (incomplete) API proposal, TCK, with implementation examples
both in Akka Streams and RxJava, can be found here:

https://github.com/lightbend/reactive-streams-utils

We are also concurrently discussing this on the core-libs-dev mailing list:

http://mail.openjdk.java.net/mailman/listinfo/core-libs-dev

We would love to hear any feedback that people have.

Regards,

-- 
*James Roper*
*Senior Octonaut*

Lightbend <https://www.lightbend.com/> â€“ Build reactive apps!
Twitter: @jroper <https://twitter.com/jroper>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180301/e23b8f11/attachment.html>

