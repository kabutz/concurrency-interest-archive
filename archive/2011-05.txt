From martinrb at google.com  Tue May  3 02:33:21 2011
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 2 May 2011 23:33:21 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
	j.u.c.locks lock implementations.
Message-ID: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>

AQS, RL, and RRWL guarantee that await methods never return
spuriously, but this is not clearly specified.

How about:

Index: ReentrantLock.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/locks/ReentrantLock.java,v
retrieving revision 1.94
diff -u -r1.94 ReentrantLock.java
--- ReentrantLock.java	15 Mar 2011 19:47:04 -0000	1.94
+++ ReentrantLock.java	3 May 2011 06:25:22 -0000
@@ -466,6 +466,9 @@
      * but for <em>fair</em> locks favors those threads that have been
      * waiting the longest.
      *
+     * <li>None of the condition {@linkplain Condition#await() waiting}
+     * methods ever return due to a &quot;<em>spurious wakeup</em>&quot;.
+     *
      * </ul>
      *
      * @return the Condition object
Index: ReentrantReadWriteLock.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/locks/ReentrantReadWriteLock.java,v
retrieving revision 1.95
diff -u -r1.95 ReentrantReadWriteLock.java
--- ReentrantReadWriteLock.java	1 May 2011 22:53:17 -0000	1.95
+++ ReentrantReadWriteLock.java	3 May 2011 06:25:23 -0000
@@ -1141,6 +1141,9 @@
          * but for <em>fair</em> locks favors those threads that have been
          * waiting the longest.
          *
+         * <li>None of the condition {@linkplain Condition#await() waiting}
+         * methods ever return due to a &quot;<em>spurious wakeup</em>&quot;.
+         *
          * </ul>
          *
          * @return the Condition object

From davidcholmes at aapt.net.au  Tue May  3 06:10:09 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 3 May 2011 20:10:09 +1000
Subject: [concurrency-interest] Specifying "No spurious wakeups"
	inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>

Hi Martin,

This question was last raised (by Doug) in May 2004. We deliberately chose
to not guarantee this as part of the spec for these classes as it might
encourage people to program in a way that would only work with Condition
objects from these specific Lock implementations (eg. not waiting in a
loop).

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Martin
> Buchholz
> Sent: Tuesday, 3 May 2011 4:33 PM
> To: concurrency-interest; Doug Lea
> Subject: [concurrency-interest] Specifying "No spurious wakeups"
> inj.u.c.locks lock implementations.
>
>
> AQS, RL, and RRWL guarantee that await methods never return
> spuriously, but this is not clearly specified.
>
> How about:
>
> Index: ReentrantLock.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/lo
> cks/ReentrantLock.java,v
> retrieving revision 1.94
> diff -u -r1.94 ReentrantLock.java
> --- ReentrantLock.java	15 Mar 2011 19:47:04 -0000	1.94
> +++ ReentrantLock.java	3 May 2011 06:25:22 -0000
> @@ -466,6 +466,9 @@
>       * but for <em>fair</em> locks favors those threads that have been
>       * waiting the longest.
>       *
> +     * <li>None of the condition {@linkplain Condition#await() waiting}
> +     * methods ever return due to a &quot;<em>spurious wakeup</em>&quot;.
> +     *
>       * </ul>
>       *
>       * @return the Condition object
> Index: ReentrantReadWriteLock.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/lo
> cks/ReentrantReadWriteLock.java,v
> retrieving revision 1.95
> diff -u -r1.95 ReentrantReadWriteLock.java
> --- ReentrantReadWriteLock.java	1 May 2011 22:53:17 -0000	1.95
> +++ ReentrantReadWriteLock.java	3 May 2011 06:25:23 -0000
> @@ -1141,6 +1141,9 @@
>           * but for <em>fair</em> locks favors those threads that
> have been
>           * waiting the longest.
>           *
> +         * <li>None of the condition {@linkplain
> Condition#await() waiting}
> +         * methods ever return due to a &quot;<em>spurious
> wakeup</em>&quot;.
> +         *
>           * </ul>
>           *
>           * @return the Condition object
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From martinrb at google.com  Tue May  3 10:12:53 2011
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 3 May 2011 07:12:53 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
Message-ID: <BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>

In practice, people will absolutely write code that depends on the
absence of spurious wakeups.

Most obviously, our own existing tests like
src/test/tck/ReentrantLockTest do so.  These don't call await in a
loop, and any test for inspection methods like hasQueuedWaiters would
have to at least use a retry loop, since e.g. hasQueuedWaiters could
spuriously return false at any time, but presumably we need to test
that it does actually return true sometime.

Freedom from spurious wakeups in concrete implementations is an
important reliability feature, in the spirit of Java, can never
practically be weakened, and so should be specified.  Education for
users ("always call await in a loop") is important, but belongs
elsewhere.

Martin

From al-javaconcurrencyinterest at none.at  Tue May  3 13:34:52 2011
From: al-javaconcurrencyinterest at none.at (Aleksandar Lazic)
Date: Tue, 3 May 2011 19:34:52 +0200
Subject: [concurrency-interest] Maybe a FAQ: Is it possible to make a
	JavaDeamon without c code
Message-ID: <20110503173452.GA6333@none.at>

Dear list member,

I hope I'am right here, if not please accept my apologize, and please
can you redirect me to the right place, thank you.

I asked myself if there is possible to make a java deamon without the
native C-code which is used in the both popular java deamon wrappers, as
far as I know.

http://wrapper.tanukisoftware.com/doc/english/download.jsp
http://commons.apache.org/daemon/

As far as I know there is

http://download.oracle.com/javase/6/docs/api/java/lang/Thread.html#setDaemon%28boolean%29

but does this make all the same stuff as a 'normal' Deamon does?

http://www.netzmafia.de/skripten/unix/linux-daemon-howto.html#s4

Maybe I think in the wrong way?

Many thanks for your attention.

Best regards

Aleks

From davidcholmes at aapt.net.au  Tue May  3 18:39:23 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 May 2011 08:39:23 +1000
Subject: [concurrency-interest] Specifying "No spurious wakeups"
	inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMENJIMAA.davidcholmes@aapt.net.au>

Martin,

> In practice, people will absolutely write code that depends on the
> absence of spurious wakeups.

They shouldn't and we should not encourage them to do so.

> Most obviously, our own existing tests like
> src/test/tck/ReentrantLockTest do so.  These don't call await in a
> loop, and any test for inspection methods like hasQueuedWaiters would
> have to at least use a retry loop, since e.g. hasQueuedWaiters could
> spuriously return false at any time, but presumably we need to test
> that it does actually return true sometime.

These are tests of our actual implementation. I don't believe we intend them
to be tests for other people's implementations but perhaps that is a grey
area. It is ok for tests of our implementation to rely on the expected
behaviour of that implementation. And of course our tests also want to
ensure that we do not in fact have spurious wakeups.

> Freedom from spurious wakeups in concrete implementations is an
> important reliability feature, in the spirit of Java, can never
> practically be weakened, and so should be specified.  Education for
> users ("always call await in a loop") is important, but belongs
> elsewhere.

Implementations should strive to remove spurious wakeups, but programmers
should always expect them. You do not want people to write Condition using
code that is dependent on the semantics of a particular implementation of
Condition.

David

> Martin
>


From davidcholmes at aapt.net.au  Tue May  3 18:50:10 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 May 2011 08:50:10 +1000
Subject: [concurrency-interest] Maybe a FAQ: Is it possible to make
	aJavaDeamon without c code
In-Reply-To: <20110503173452.GA6333@none.at>
Message-ID: <NFBBKALFDCPFIDBNKAPCGENKIMAA.davidcholmes@aapt.net.au>

Hi Aleks,

Aleksandar Lazic writes:
> I hope I'am right here, if not please accept my apologize, and please
> can you redirect me to the right place, thank you.

Not really the right place, but not too far off-topic either :)

> I asked myself if there is possible to make a java deamon without the
> native C-code which is used in the both popular java deamon wrappers, as
> far as I know.
>
> http://wrapper.tanukisoftware.com/doc/english/download.jsp
> http://commons.apache.org/daemon/
>
> As far as I know there is
>
> http://download.oracle.com/javase/6/docs/api/java/lang/Thread.html
> #setDaemon%28boolean%29
>
> but does this make all the same stuff as a 'normal' Deamon does?
>
> http://www.netzmafia.de/skripten/unix/linux-daemon-howto.html#s4
>
> Maybe I think in the wrong way?

What is a "daemon" for you? You need to answer that question to understand
what is needed in Java to implement one. If it is:

"Under Unix based operating systems non interactive server applications are
called daemons and are controlled by the operating system with a set of
specified signals."

then you definitely need a C wrapper to do the signal management.

The Java Thread notion of daemon is unrelated to UNIX daemons - daemon
threads won't force the Java Virtual machine to remain alive.

Hope this helps a little.

David Holmes

> Many thanks for your attention.
>
> Best regards
>
> Aleks
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From mbien at fh-landshut.de  Tue May  3 21:55:09 2011
From: mbien at fh-landshut.de (Michael Bien)
Date: Wed, 04 May 2011 03:55:09 +0200
Subject: [concurrency-interest] Runnables in ThreadPools
Message-ID: <4DC0B1FD.5010009@fh-landshut.de>

  Hello,

is it guaranteed that a Runnable in a ThreadPool is bound to the same 
Thread from start to finish?
I am using Executors.newFixedThreadPool(...) with a custom thread 
factory injecting a per-thread resource required for task execution.

a bit context:
The resource is a OpenCL command queue used within the task. The task 
doesn't do any explicit computation. It waits for external events most 
of the time. All computation is done via CL (on GPUs or other devices). 
Its important that there is only one queue per thread and that a task 
(->Runnable) never moves to a different queue (since every queue may be 
on a different device).

regards,
michael

-- 
http://michael-bien.com/


From david.lloyd at redhat.com  Tue May  3 22:02:47 2011
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 03 May 2011 21:02:47 -0500
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
	<BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
Message-ID: <4DC0B3C7.8080908@redhat.com>

On 05/03/2011 09:12 AM, Martin Buchholz wrote:
> In practice, people will absolutely write code that depends on the
> absence of spurious wakeups.
>
> Most obviously, our own existing tests like
> src/test/tck/ReentrantLockTest do so.  These don't call await in a
> loop, and any test for inspection methods like hasQueuedWaiters would
> have to at least use a retry loop, since e.g. hasQueuedWaiters could
> spuriously return false at any time, but presumably we need to test
> that it does actually return true sometime.
>
> Freedom from spurious wakeups in concrete implementations is an
> important reliability feature, in the spirit of Java, can never
> practically be weakened, and so should be specified.  Education for
> users ("always call await in a loop") is important, but belongs
> elsewhere.

The advantages to allowing spurious wakeups is more important I think. 
Since many existing implementations of conditions of various stripes can 
be spuriously awoken, wrapping these or creating new implementations 
based on them will become difficult if not impossible to do if spurious 
wakeups are forbidden at the API level.

I've been in the situation once or twice where being allowed to wake up 
spuriously has saved my bacon.  On the other hand, I've never been 
burned by it; coding for it is simple and it's really second nature for 
me at this point.  If it's well-documented, then users have no excuse 
for fudging it up in my opinion.  And it's not like Java is the first 
platform to ever have this particular idiosyncrasy - far from it.
-- 
- DML

From davidcholmes at aapt.net.au  Tue May  3 22:11:28 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 May 2011 12:11:28 +1000
Subject: [concurrency-interest] Runnables in ThreadPools
In-Reply-To: <4DC0B1FD.5010009@fh-landshut.de>
Message-ID: <NFBBKALFDCPFIDBNKAPCAENMIMAA.davidcholmes@aapt.net.au>

Michael Bien writes:
> is it guaranteed that a Runnable in a ThreadPool is bound to the same
> Thread from start to finish?

For ThreadPoolExecutor - yes. A worker takes a Runnable from a queue and
starts executing run(). Unless the Runnable (or the code it calls) resubmits
itself to the executor, all the code executed from run() will be executed by
that worker thread.

David Holmes

> I am using Executors.newFixedThreadPool(...) with a custom thread
> factory injecting a per-thread resource required for task execution.
>
> a bit context:
> The resource is a OpenCL command queue used within the task. The task
> doesn't do any explicit computation. It waits for external events most
> of the time. All computation is done via CL (on GPUs or other devices).
> Its important that there is only one queue per thread and that a task
> (->Runnable) never moves to a different queue (since every queue may be
> on a different device).
>
> regards,
> michael
>
> --
> http://michael-bien.com/
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From martinrb at google.com  Wed May  4 00:55:27 2011
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 3 May 2011 21:55:27 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <4DC0B3C7.8080908@redhat.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
	<BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
	<4DC0B3C7.8080908@redhat.com>
Message-ID: <BANLkTi=ZFuL1F+=DUZK64gZoXPExXwaAZA@mail.gmail.com>

On Tue, May 3, 2011 at 19:02, David M. Lloyd <david.lloyd at redhat.com> wrote:
> On 05/03/2011 09:12 AM, Martin Buchholz wrote:
>>
> The advantages to allowing spurious wakeups is more important I think. Since
> many existing implementations of conditions of various stripes can be
> spuriously awoken, wrapping these or creating new implementations based on
> them will become difficult if not impossible to do if spurious wakeups are
> forbidden at the API level.

I'm not proposing to change the specs for the Condition interface,
only the concrete classes (most notably ReentrantLock and
ReentrantReadWriteLock) that in fact are carefully implemented to
avoid spurious wakeups.  Other implementations of Condition could
continue to allow spurious wakeups.

> I've been in the situation once or twice where being allowed to wake up
> spuriously has saved my bacon. ?On the other hand, I've never been burned by
> it; coding for it is simple and it's really second nature for me at this
> point. ?If it's well-documented, then users have no excuse for fudging it up
> in my opinion. ?And it's not like Java is the first platform to ever have
> this particular idiosyncrasy - far from it.

For the specific case of awaiting methods, you are right that you
pretty much have to call await in a loop.  But consider
getWaitQueueLength
http://download.oracle.com/javase/7/docs/api/java/util/concurrent/locks/ReentrantLock.html#getWaitQueueLength%28java.util.concurrent.locks.Condition%29
In the absence of spurious wakeups, getWaitQueueLength can be a much
more reliable indicator of the actual number of waiters.  And more
reliable is always better.  Even if the only use is monitoring and
debugging.

Martin


From martinrb at google.com  Wed May  4 01:00:12 2011
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 3 May 2011 22:00:12 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMENJIMAA.davidcholmes@aapt.net.au>
References: <BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMENJIMAA.davidcholmes@aapt.net.au>
Message-ID: <BANLkTikDAWsQro+KPuUCT1mcQFEa_F2UjA@mail.gmail.com>

On Tue, May 3, 2011 at 15:39, David Holmes <davidcholmes at aapt.net.au> wrote:

>> Most obviously, our own existing tests like
>> src/test/tck/ReentrantLockTest do so.
>
> These are tests of our actual implementation.

Being tck tests, they are (in theory) only allowed to test conformance
to the specification.

> I don't believe we intend them
> to be tests for other people's implementations but perhaps that is a grey
> area. It is ok for tests of our implementation to rely on the expected
> behaviour of that implementation. And of course our tests also want to
> ensure that we do not in fact have spurious wakeups.

That's the distinction between tck tests and jtreg tests.

Martin

From joe.bowbeer at gmail.com  Wed May  4 01:42:59 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 3 May 2011 22:42:59 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTi=ZFuL1F+=DUZK64gZoXPExXwaAZA@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
	<BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
	<4DC0B3C7.8080908@redhat.com>
	<BANLkTi=ZFuL1F+=DUZK64gZoXPExXwaAZA@mail.gmail.com>
Message-ID: <BANLkTim_he3oa-n84rDZi8ZSZxv+wgYpSw@mail.gmail.com>

On Tue, May 3, 2011 at 9:55 PM, Martin Buchholz wrote:

>
> I'm not proposing to change the specs for the Condition interface,
> only the concrete classes (most notably ReentrantLock and
> ReentrantReadWriteLock) that in fact are carefully implemented to
> avoid spurious wakeups.  Other implementations of Condition could
> continue to allow spurious wakeups.


Can this comment be confined to the "Implementation Notes" sections of these
classes?

What about the other implementations of ReentrantLock and friends, such as
the back-ports?

Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110503/ef2abc1c/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed May  4 02:06:15 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 May 2011 16:06:15 +1000
Subject: [concurrency-interest] Specifying "No spurious wakeups"
	inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTikDAWsQro+KPuUCT1mcQFEa_F2UjA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEOAIMAA.davidcholmes@aapt.net.au>

Martin Buchholz writes:
> On Tue, May 3, 2011 at 15:39, David Holmes
> <davidcholmes at aapt.net.au> wrote:
>
> >> Most obviously, our own existing tests like
> >> src/test/tck/ReentrantLockTest do so.
> >
> > These are tests of our actual implementation.
>
> Being tck tests, they are (in theory) only allowed to test conformance
> to the specification.

Then it may be that our tests needs to be moved or changed if they assume no
spurious wakeups - depending on whether these should be TCK or JTREG tests.

David
------


> > I don't believe we intend them
> > to be tests for other people's implementations but perhaps that
> is a grey
> > area. It is ok for tests of our implementation to rely on the expected
> > behaviour of that implementation. And of course our tests also want to
> > ensure that we do not in fact have spurious wakeups.
>
> That's the distinction between tck tests and jtreg tests.
>
> Martin
>


From davidcholmes at aapt.net.au  Wed May  4 02:09:49 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 May 2011 16:09:49 +1000
Subject: [concurrency-interest] Specifying "No spurious wakeups"
	inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTim_he3oa-n84rDZi8ZSZxv+wgYpSw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEOAIMAA.davidcholmes@aapt.net.au>

Joe,

While the placement of the comment as an implementation note would allow
other implementations to not be required to eliminate spurious wakeups that
only further reinforces the fact that nobody should be writing code that
assumes no spurious wakeups else they are tying their code to these specific
implementations (not just ReentrantLock but the JDKs ReentrantLock).

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Joe Bowbeer
  Sent: Wednesday, 4 May 2011 3:43 PM
  To: concurrency-interest
  Subject: Re: [concurrency-interest] Specifying "No spurious wakeups"
inj.u.c.locks lock implementations.


  On Tue, May 3, 2011 at 9:55 PM, Martin Buchholz wrote:



    I'm not proposing to change the specs for the Condition interface,
    only the concrete classes (most notably ReentrantLock and
    ReentrantReadWriteLock) that in fact are carefully implemented to
    avoid spurious wakeups.  Other implementations of Condition could
    continue to allow spurious wakeups.


  Can this comment be confined to the "Implementation Notes" sections of
these classes?



  What about the other implementations of ReentrantLock and friends, such as
the back-ports?


  Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110504/c9452d5e/attachment.html>

From al-javaconcurrencyinterest at none.at  Wed May  4 02:17:38 2011
From: al-javaconcurrencyinterest at none.at (Aleksandar Lazic)
Date: Wed, 4 May 2011 08:17:38 +0200
Subject: [concurrency-interest] Maybe a FAQ: Is it possible to make
 aJavaDeamon without c code
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGENKIMAA.davidcholmes@aapt.net.au>
References: <20110503173452.GA6333@none.at>
	<NFBBKALFDCPFIDBNKAPCGENKIMAA.davidcholmes@aapt.net.au>
Message-ID: <20110504061738.GA4946@none.at>

Hi David,


On Mit 04.05.2011 08:50, David Holmes wrote:
>Hi Aleks,
>
>Aleksandar Lazic writes:

[snipp]

>> Maybe I think in the wrong way?
>
>What is a "daemon" for you? You need to answer that question to
>understand what is needed in Java to implement one. If it is:
>
>"Under Unix based operating systems non interactive server applications
>are called daemons and are controlled by the operating system with a
>set of specified signals."

Yes.

>then you definitely need a C wrapper to do the signal management.
>
>The Java Thread notion of daemon is unrelated to UNIX daemons - daemon
>threads won't force the Java Virtual machine to remain alive.
>
>Hope this helps a little.

Yes.

Thank you for the clarification

Cheers
Aleks

From dl at cs.oswego.edu  Wed May  4 07:50:31 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 04 May 2011 07:50:31 -0400
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 in	j.u.c.locks lock implementations.
In-Reply-To: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
Message-ID: <4DC13D87.40506@cs.oswego.edu>

On 05/03/11 02:33, Martin Buchholz wrote:
> AQS, RL, and RRWL guarantee that await methods never return
> spuriously, but this is not clearly specified.

It IS clearly specified for AQS -- see the specs for
AbstractQueuedSynchronizer.ConditionObject.await ...
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
... which is important to spec for anyone building any
kind of lock/sync using AQS.

So the questions are:

1. We know (but do not specify) that ReentrantLock is built
using AQS in a way that preserves the no-spurious-wakeup property.
Should we add this to specs? The main reasons not to are
to allow alternative implementations and to avoid promoting
the bad programming practice of not rechecking conditions
upon return from await.

2. Depending on the outcome of (1), we might want to
weaken or strengthen some TCK tests that are inconsistent
about assuming lack of spurious wakeups.

-Doug



From mbien at fh-landshut.de  Wed May  4 07:58:58 2011
From: mbien at fh-landshut.de (Michael Bien)
Date: Wed, 04 May 2011 13:58:58 +0200
Subject: [concurrency-interest] Runnables in ThreadPools
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAENMIMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAENMIMAA.davidcholmes@aapt.net.au>
Message-ID: <4DC13F82.7040003@fh-landshut.de>

  thank you very much David!

-michael

On 05/04/2011 04:11 AM, David Holmes wrote:
> Michael Bien writes:
>> is it guaranteed that a Runnable in a ThreadPool is bound to the same
>> Thread from start to finish?
> For ThreadPoolExecutor - yes. A worker takes a Runnable from a queue and
> starts executing run(). Unless the Runnable (or the code it calls) resubmits
> itself to the executor, all the code executed from run() will be executed by
> that worker thread.
>
> David Holmes
>
>> I am using Executors.newFixedThreadPool(...) with a custom thread
>> factory injecting a per-thread resource required for task execution.
>>
>> a bit context:
>> The resource is a OpenCL command queue used within the task. The task
>> doesn't do any explicit computation. It waits for external events most
>> of the time. All computation is done via CL (on GPUs or other devices).
>> Its important that there is only one queue per thread and that a task
>> (->Runnable) never moves to a different queue (since every queue may be
>> on a different device).
>>
>> regards,
>> michael
>>
>> --
>> http://michael-bien.com/
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-- 
http://michael-bien.com/


From martinrb at google.com  Wed May  4 08:21:14 2011
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 May 2011 05:21:14 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups"
 inj.u.c.locks lock implementations.
In-Reply-To: <BANLkTim_he3oa-n84rDZi8ZSZxv+wgYpSw@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCENIIMAA.davidcholmes@aapt.net.au>
	<BANLkTik+qs5ksGbox1odwnEiYsy2fmJmdw@mail.gmail.com>
	<4DC0B3C7.8080908@redhat.com>
	<BANLkTi=ZFuL1F+=DUZK64gZoXPExXwaAZA@mail.gmail.com>
	<BANLkTim_he3oa-n84rDZi8ZSZxv+wgYpSw@mail.gmail.com>
Message-ID: <BANLkTim1uNfeO3=x0B7CL5f8QcLwh=SK+w@mail.gmail.com>

On Tue, May 3, 2011 at 22:42, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> On Tue, May 3, 2011 at 9:55 PM, Martin Buchholz wrote:
>>
>> I'm not proposing to change the specs for the Condition interface,
>> only the concrete classes (most notably ReentrantLock and
>> ReentrantReadWriteLock) that in fact are carefully implemented to
>> avoid spurious wakeups. ?Other implementations of Condition could
>> continue to allow spurious wakeups.
>
> Can this comment be confined to the "Implementation Notes" sections of these
> classes?

I really do want to specify the historic behavior of the ReentrantLock
implementation (in practice, all current Java implementations use
Doug's code).  But perhaps I hadn't thought enough about user-defined
subclasses of ReentrantLock.  I only want to constrain the
ReentrantLock implementation, not subclasses (like java.util.Random?)

> What about the other implementations of ReentrantLock and friends, such as
> the back-ports?

It's a good question.  Intrinsic Object.wait is explicit about
permitting spurious wakeups.  If you implement a ReentrantLock on top
of intrinsic locks, and Dawid Kurzyniec's backport appears to do so,
it's hard to eliminate spurious wakeups.

Martin


From martinrb at google.com  Wed May  4 09:01:59 2011
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 May 2011 06:01:59 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
 j.u.c.locks lock implementations.
In-Reply-To: <4DC13D87.40506@cs.oswego.edu>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<4DC13D87.40506@cs.oswego.edu>
Message-ID: <BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>

On Wed, May 4, 2011 at 04:50, Doug Lea <dl at cs.oswego.edu> wrote:
> On 05/03/11 02:33, Martin Buchholz wrote:
>>
>> AQS, RL, and RRWL guarantee that await methods never return
>> spuriously, but this is not clearly specified.
>
> It IS clearly specified for AQS -- see the specs for
> AbstractQueuedSynchronizer.ConditionObject.await ...
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
> ... which is important to spec for anyone building any
> kind of lock/sync using AQS.

It's not clear enough for me.  I'd like to see a paragraph about
spurious-wakeup-freedom added to the ConditionObject or AQS class doc.
 And this includes not just the awaiting methods, but also the
protected monitoring methods.

Or am I to interpret """Note that because timeouts and interrupts may
occur at any time""" that spurious wakeups can *not* occur at any
time?

(As often happens, I'm starting to regret opening this unexpectedly
controversial thread)

From dl at cs.oswego.edu  Wed May  4 09:09:25 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 04 May 2011 09:09:25 -0400
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
 j.u.c.locks lock implementations.
In-Reply-To: <BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>	<4DC13D87.40506@cs.oswego.edu>
	<BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>
Message-ID: <4DC15005.7040305@cs.oswego.edu>

On 05/04/11 09:01, Martin Buchholz wrote:
> On Wed, May 4, 2011 at 04:50, Doug Lea<dl at cs.oswego.edu>  wrote:
>> On 05/03/11 02:33, Martin Buchholz wrote:
>>>
>>> AQS, RL, and RRWL guarantee that await methods never return
>>> spuriously, but this is not clearly specified.
>>
>> It IS clearly specified for AQS -- see the specs for
>> AbstractQueuedSynchronizer.ConditionObject.await ...
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
>> ... which is important to spec for anyone building any
>> kind of lock/sync using AQS.
>
> It's not clear enough for me.

Pasting...
<paste>
Implements interruptible condition wait.

    1. If current thread is interrupted, throw InterruptedException.
    2. Save lock state returned by AbstractQueuedSynchronizer.getState().
    3. Invoke AbstractQueuedSynchronizer.release(int) with saved state as 
argument, throwing IllegalMonitorStateException if it fails.
    4. Block until signalled or interrupted.
    5. Reacquire by invoking specialized version of 
AbstractQueuedSynchronizer.acquire(int) with saved state as argument.
    6. If interrupted while blocked in step 4, throw InterruptedException.
</paste>

What part of step #4 is not clear? :-)

-Doug

From martinrb at google.com  Wed May  4 10:34:14 2011
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 May 2011 07:34:14 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
 j.u.c.locks lock implementations.
In-Reply-To: <4DC15005.7040305@cs.oswego.edu>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<4DC13D87.40506@cs.oswego.edu>
	<BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>
	<4DC15005.7040305@cs.oswego.edu>
Message-ID: <BANLkTi=GNemkDtd4Ah0CmKuWwKQU1odNdQ@mail.gmail.com>

On Wed, May 4, 2011 at 06:09, Doug Lea <dl at cs.oswego.edu> wrote:
> On 05/04/11 09:01, Martin Buchholz wrote:
>>
>> On Wed, May 4, 2011 at 04:50, Doug Lea<dl at cs.oswego.edu> ?wrote:
>>>
>>> On 05/03/11 02:33, Martin Buchholz wrote:
>>>>
>>>> AQS, RL, and RRWL guarantee that await methods never return
>>>> spuriously, but this is not clearly specified.
>>>
>>> It IS clearly specified for AQS -- see the specs for
>>> AbstractQueuedSynchronizer.ConditionObject.await ...
>>>
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
>>> ... which is important to spec for anyone building any
>>> kind of lock/sync using AQS.
>>
>> It's not clear enough for me.
>
> Pasting...
> <paste>
> Implements interruptible condition wait.
>
> ? 1. If current thread is interrupted, throw InterruptedException.
1.5. Enqueue the current thread in the wait queue for this condition
> ? 2. Save lock state returned by AbstractQueuedSynchronizer.getState().
> ? 3. Invoke AbstractQueuedSynchronizer.release(int) with saved state as
> argument, throwing IllegalMonitorStateException if it fails.

> ? 4. Block until signalled or interrupted.

4. Blocks (by (if necessary) repeatedly calling park() while
continually being enqueued in the condition wait queue (which can be
reliably monitored using the protected monitoring methods)) until
signalled or interrupted.

> ? 5. Reacquire by invoking specialized version of
> AbstractQueuedSynchronizer.acquire(int) with saved state as argument.
> ? 6. If interrupted while blocked in step 4, throw InterruptedException.
> </paste>
>
> What part of step #4 is not clear? :-)

You could at least make it explicit, e.g.

"Even though all blocking operations use the underlying park()
primitive, which is subject to spurious wakeups, any spurious wakeups
from park() do not cause early return from any of the awaiting methods
(in such cases park is invoked repeatedly), nor do they affect the
results of the protected monitoring operations."


From dl at cs.oswego.edu  Thu May  5 06:46:13 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 05 May 2011 06:46:13 -0400
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
 j.u.c.locks lock implementations.
In-Reply-To: <BANLkTi=GNemkDtd4Ah0CmKuWwKQU1odNdQ@mail.gmail.com>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>	<4DC13D87.40506@cs.oswego.edu>	<BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>	<4DC15005.7040305@cs.oswego.edu>
	<BANLkTi=GNemkDtd4Ah0CmKuWwKQU1odNdQ@mail.gmail.com>
Message-ID: <4DC27FF5.5080601@cs.oswego.edu>

On 05/04/11 10:34, Martin Buchholz wrote:
> On Wed, May 4, 2011 at 06:09, Doug Lea<dl at cs.oswego.edu>  wrote:
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
>>>> ... which is important to spec for anyone building any
>>>> kind of lock/sync using AQS.
>>>
>>> It's not clear enough for me.
>>
>> Implements interruptible condition wait.
>>
>>    1. If current thread is interrupted, throw InterruptedException.
> 1.5. Enqueue the current thread in the wait queue for this condition
>>    2. Save lock state returned by AbstractQueuedSynchronizer.getState().
>>    3. Invoke AbstractQueuedSynchronizer.release(int) with saved state as
>> argument, throwing IllegalMonitorStateException if it fails.
>
>>    4. Block until signalled or interrupted.
>
> 4. Blocks (by (if necessary) repeatedly calling park() while
> continually being enqueued in the condition wait queue (which can be
> reliably monitored using the protected monitoring methods)) until
> signalled or interrupted.

Sorry, I don't see a reason to say exactly how this blocks.

You are right though that the steps do not explicitly mention
relation to wait queue. We could add:

   The current thread is enqueued on the wait queue for this
   condition while waiting.

This is preferable to your proposed step 1.5 because we
cannot promise a visible ordering of enqueue/dequeue wrt
steps 2-5.

-Doug

From karmazilla at gmail.com  Fri May  6 10:57:21 2011
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Fri, 6 May 2011 16:57:21 +0200
Subject: [concurrency-interest] LockSupport.getBlocker(null) crashes hotspot
Message-ID: <BANLkTi=A34pAZV830CpomPnCXH=BGG-L2w@mail.gmail.com>

Hi,

As the title says, calling LockSupport.getBlocker with a null argument
crashes hotspot.
I have tested this with Java6 on Linux 32bit and Mac OS X Snow Leopard.

Adding a check to getBlocket that returns null if the provided `t`
argument is null would fix it, I think.

-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.

From dl at cs.oswego.edu  Fri May  6 11:11:30 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 06 May 2011 11:11:30 -0400
Subject: [concurrency-interest] LockSupport.getBlocker(null) crashes
	hotspot
In-Reply-To: <BANLkTi=A34pAZV830CpomPnCXH=BGG-L2w@mail.gmail.com>
References: <BANLkTi=A34pAZV830CpomPnCXH=BGG-L2w@mail.gmail.com>
Message-ID: <4DC40FA2.6010109@cs.oswego.edu>

On 05/06/11 10:57, Christian Vest Hansen wrote:
> Hi,
>
> As the title says, calling LockSupport.getBlocker with a null argument
> crashes hotspot.


Thanks very much! Fixed and hopefully integrated soon into OpenJDK.

*** LockSupport.java	2011/03/15 19:47:04	1.23
--- LockSupport.java	2011/05/06 15:09:36	1.25
***************
*** 247,255 ****
--- 247,258 ----
        * different blocker object.
        *
        * @return the blocker
+      * @throws NullPointerException if argument is null
        * @since 1.6
        */
       public static Object getBlocker(Thread t) {
+         if (t == null)
+             throw new NullPointerException();
           return unsafe.getObjectVolatile(t, parkBlockerOffset);
       }


	

-Doug


From forax at univ-mlv.fr  Fri May  6 11:44:00 2011
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 06 May 2011 17:44:00 +0200
Subject: [concurrency-interest] LockSupport.getBlocker(null)
	crashes	hotspot
In-Reply-To: <4DC40FA2.6010109@cs.oswego.edu>
References: <BANLkTi=A34pAZV830CpomPnCXH=BGG-L2w@mail.gmail.com>
	<4DC40FA2.6010109@cs.oswego.edu>
Message-ID: <4DC41740.8050609@univ-mlv.fr>

For unsafe.get/put, null means that the offset is an absolute address :(

R?mi

On 05/06/2011 05:11 PM, Doug Lea wrote:
> On 05/06/11 10:57, Christian Vest Hansen wrote:
>> Hi,
>>
>> As the title says, calling LockSupport.getBlocker with a null argument
>> crashes hotspot.
>
>
> Thanks very much! Fixed and hopefully integrated soon into OpenJDK.
>
> *** LockSupport.java    2011/03/15 19:47:04    1.23
> --- LockSupport.java    2011/05/06 15:09:36    1.25
> ***************
> *** 247,255 ****
> --- 247,258 ----
>        * different blocker object.
>        *
>        * @return the blocker
> +      * @throws NullPointerException if argument is null
>        * @since 1.6
>        */
>       public static Object getBlocker(Thread t) {
> +         if (t == null)
> +             throw new NullPointerException();
>           return unsafe.getObjectVolatile(t, parkBlockerOffset);
>       }
>
>
>
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Fri May  6 18:59:09 2011
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 6 May 2011 15:59:09 -0700
Subject: [concurrency-interest] Specifying "No spurious wakeups" in
 j.u.c.locks lock implementations.
In-Reply-To: <4DC27FF5.5080601@cs.oswego.edu>
References: <BANLkTim3ZDbYOOLwYiPASc=N2QVjdYqeAQ@mail.gmail.com>
	<4DC13D87.40506@cs.oswego.edu>
	<BANLkTimdZEyYq7OBGEqB4Pv4NY6Zqnep=Q@mail.gmail.com>
	<4DC15005.7040305@cs.oswego.edu>
	<BANLkTi=GNemkDtd4Ah0CmKuWwKQU1odNdQ@mail.gmail.com>
	<4DC27FF5.5080601@cs.oswego.edu>
Message-ID: <BANLkTik2AYXbzEoDKnZccMWjheAKb4FcrQ@mail.gmail.com>

On Thu, May 5, 2011 at 03:46, Doug Lea <dl at cs.oswego.edu> wrote:
> On 05/04/11 10:34, Martin Buchholz wrote:
>>
>> On Wed, May 4, 2011 at 06:09, Doug Lea<dl at cs.oswego.edu> ?wrote:
>>>>>
>>>>>
>>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/locks/AbstractQueuedSynchronizer.ConditionObject.html#await%28%29
>>>>> ... which is important to spec for anyone building any
>>>>> kind of lock/sync using AQS.
>>>>
>>>> It's not clear enough for me.
>>>
>>> Implements interruptible condition wait.
>>>
>>> ? 1. If current thread is interrupted, throw InterruptedException.
>>
>> 1.5. Enqueue the current thread in the wait queue for this condition
>>>
>>> ? 2. Save lock state returned by AbstractQueuedSynchronizer.getState().
>>> ? 3. Invoke AbstractQueuedSynchronizer.release(int) with saved state as
>>> argument, throwing IllegalMonitorStateException if it fails.
>>
>>> ? 4. Block until signalled or interrupted.
>>
>> 4. Blocks (by (if necessary) repeatedly calling park() while
>> continually being enqueued in the condition wait queue (which can be
>> reliably monitored using the protected monitoring methods)) until
>> signalled or interrupted.
>
> Sorry, I don't see a reason to say exactly how this blocks.

Yeah, my description was only half-serious, but helps me understand it.

The javadoc for AQS.ConditionObject is in a pseudocode/spec Twilight
Zone, different from other classes.

> You are right though that the steps do not explicitly mention
> relation to wait queue. We could add:
>
> ?The current thread is enqueued on the wait queue for this
> ?condition while waiting.
>
> This is preferable to your proposed step 1.5 because we
> cannot promise a visible ordering of enqueue/dequeue wrt
> steps 2-5.

I think 1.5 has to come before 3 (else another thread could sneak in
and observe this thread as apparently not waiting), and 2 is
unobservable, so I think it's reasonable to present them in this
order.  In the unlikely event that there is a performance advantage to
reordering 2 and 3, the "as-if" rule applies.

Martin


From sachingupta107 at gmail.com  Sun May  8 15:58:38 2011
From: sachingupta107 at gmail.com (Sachin Gupta)
Date: Mon, 9 May 2011 01:28:38 +0530
Subject: [concurrency-interest] Understanding ReusableBarrier Problem (from
	'Little Book of Semaphores')
Message-ID: <BANLkTinLLvWeL9AHBaJ774qHN4tp+u74+g@mail.gmail.com>

Hi,

While going through a concurrency problem in 'Little Book Of
Semaphores' I am stuck at following problem.

The problem statement is as follows:

Often a set of cooperating threads will perform a series of steps in a
loop and synchronize at a barrier after each step. For this
application we need a reusable barrier that locks itself after all the
threads have passed through.

Given Solution is:

1 # rendezvous
2
3 mutex.wait()
4     count += 1
5     if count == n:
6         turnstile2.wait() # lock the second
7         turnstile.signal() # unlock the first
8 mutex.signal()
9
10 turnstile.wait() # first turnstile
11 turnstile.signal()
12
13 # critical point
14
15 mutex.wait()
16     count -= 1
17     if count == 0:
18         turnstile.wait() # lock the first
19         turnstile2.signal() # unlock the second
20 mutex.signal()
21
22 turnstile2.wait() # second turnstile
23 turnstile2.signal()

Suppose we use this barrier for 2 threads and we pump 100 threads
through this barrier. when second thread has unlocked turnstile(7) and
reaches line 9, now, thread 3 come along and,
it increments count,
count > n so it releases mutex,
since turnstile is unlocked it reaches critical point also,
similarly, thread 4, thread 5, thread 6 can execute critical point,
executing it more than 2 times.
what is stopping them from passing through the barrier ahead of thread
2? Or is my understanding wrong here?

--
Thanks,
Sachin

From ted_yu at yahoo.com  Sun May  8 16:22:12 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 8 May 2011 13:22:12 -0700 (PDT)
Subject: [concurrency-interest] [seajug] BlockingQueue that is aware of
	sizes of elements
In-Reply-To: <BANLkTi=CD1kAwwKD8FZxFRQU6pWDX9P5=g@mail.gmail.com>
References: <008601cbfb95$b8459d60$9701a8c0@Taku> <4DA88C24.9010102@gmail.com>
	<045101cbfed1$7c407490$9701a8c0@Taku>
	<224805.55436.qm@web30802.mail.mud.yahoo.com>
	<B99874B312BE5C459BDB81526661EC548AF6F6D9@EXCH13.GHCMASTER.GHC.ORG>
	<124420.27843.qm@web30804.mail.mud.yahoo.com>
	<829758.42836.qm@web30808.mail.mud.yahoo.com>
	<BANLkTi=CD1kAwwKD8FZxFRQU6pWDX9P5=g@mail.gmail.com>
Message-ID: <76152.14877.qm@web30808.mail.mud.yahoo.com>

Joe:
See https://issues.apache.org/jira/browse/HBASE-3813 for background for my question.
I am looking forward to expert advice from concurrency-interest.


________________________________
From: Joe Bowbeer <joe.bowbeer at gmail.com>
To: seajug at yahoogroups.com
Sent: Sunday, May 8, 2011 11:05 AM
Subject: Re: [seajug] BlockingQueue that is aware of sizes of elements


? 
For the most expert advise, I suggest posting this to concurrency-interest. ?I've noticed that these questions also do fairly well on stackoverflow.com, even though none of the JCiP authors frequent SO as far as I know.


In answer to your question, it doesn't look difficult to clone LinkedBlockingQueue and to re-purpose the capacity and count to serve your needs. ?The performance of a linked queue isn't stellar, but it is space-efficient and you are, after all, trying to cap total size...

One question: what happens if one thread tries to enqueue an oversize element while a subsequent thread tries to enqueue a smaller element? ?Should the smaller element be enqueued even though the request happened after the oversize element was blocked? ?(It will be enqueued if you use the modified LBQ that I suggested.)

Also: You'll need to make sure that notFull is signalled every time anything is removed from the queue, because you don't know how large the waiting oversized element is.

Joe


On Sun, May 8, 2011 at 6:52 AM, Ted Yu?wrote:


>
>
>BlockingQueue's blocking behavior is determined by the number of elements.
>
>I wonder if there is an implementation of BlockingQueue that is aware 
of the sizes of the objects it holds. Meaning it would block if the next
 element to be queued would cause total estimated heap consumed to 
exceed pre-determined threshold.
>
>Thanks
__._,_.___
Reply to sender | Reply to group | Reply via web post | Start a New Topic Messages in this topic (5) 
Recent Activity: 	* New Members 1  	* New Polls 1   
Visit Your Group 
 
Switch to: Text-Only, Daily Digest ? Unsubscribe ? Terms of Use
. 

__,_._,___ 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/4db8ec6b/attachment.html>

From davidcholmes at aapt.net.au  Sun May  8 17:56:10 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 07:56:10 +1000
Subject: [concurrency-interest] Understanding ReusableBarrier Problem
	(from'Little Book of Semaphores')
In-Reply-To: <BANLkTinLLvWeL9AHBaJ774qHN4tp+u74+g@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>

> Suppose we use this barrier for 2 threads and we pump 100 threads
> though this barrier

A barrier has to be set for the number of threads that will be "pumped
through it" - if you are going to pump 100 threads then you need to define
the barrier to expect 100 threads.

David Holmes


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Sachin
> Gupta
> Sent: Monday, 9 May 2011 5:59 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Understanding ReusableBarrier Problem
> (from'Little Book of Semaphores')
>
>
> Hi,
>
> While going through a concurrency problem in 'Little Book Of
> Semaphores' I am stuck at following problem.
>
> The problem statement is as follows:
>
> Often a set of cooperating threads will perform a series of steps in a
> loop and synchronize at a barrier after each step. For this
> application we need a reusable barrier that locks itself after all the
> threads have passed through.
>
> Given Solution is:
>
> 1 # rendezvous
> 2
> 3 mutex.wait()
> 4     count += 1
> 5     if count == n:
> 6         turnstile2.wait() # lock the second
> 7         turnstile.signal() # unlock the first
> 8 mutex.signal()
> 9
> 10 turnstile.wait() # first turnstile
> 11 turnstile.signal()
> 12
> 13 # critical point
> 14
> 15 mutex.wait()
> 16     count -= 1
> 17     if count == 0:
> 18         turnstile.wait() # lock the first
> 19         turnstile2.signal() # unlock the second
> 20 mutex.signal()
> 21
> 22 turnstile2.wait() # second turnstile
> 23 turnstile2.signal()
>
> Suppose we use this barrier for 2 threads and we pump 100 threads
> through this barrier. when second thread has unlocked turnstile(7) and
> reaches line 9, now, thread 3 come along and,
> it increments count,
> count > n so it releases mutex,
> since turnstile is unlocked it reaches critical point also,
> similarly, thread 4, thread 5, thread 6 can execute critical point,
> executing it more than 2 times.
> what is stopping them from passing through the barrier ahead of thread
> 2? Or is my understanding wrong here?
>
> --
> Thanks,
> Sachin
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Sun May  8 18:05:13 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 08:05:13 +1000
Subject: [concurrency-interest] [seajug] BlockingQueue that is aware
	ofsizes of elements
In-Reply-To: <76152.14877.qm@web30808.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEPIIMAA.davidcholmes@aapt.net.au>

Making a synchronizer for abritrary conditions is not that difficult, but it assumes that you can detect the necessary conditions for blocking and signalling and that you control all the code that affects the change of state so that you can introduce the necessary signalling. I'm unclear from the description exactly what those conditions would be in this case (as the object to be enqueued already exists it already consumes heap).

And as Joe states one policy decision with such things is whether blocked requests prevent new requests from proceeding (similar to the question as to whether a blocked writer prevents additional readers in a read/write lock).

Regardless this will be a custom built BlockingQueue - in fact not really a BlockingQueue as you won't be fulfilling the BQ API.

Cheers,
David Holmes
 
 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted Yu
Sent: Monday, 9 May 2011 6:22 AM
To: concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Subject: Re: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


  Joe:
  See https://issues.apache.org/jira/browse/HBASE-3813 for background for my question.
  I am looking forward to expert advice from concurrency-interest.



------------------------------------------------------------------------------
  From: Joe Bowbeer <joe.bowbeer at gmail.com>
  To: seajug at yahoogroups.com
  Sent: Sunday, May 8, 2011 11:05 AM
  Subject: Re: [seajug] BlockingQueue that is aware of sizes of elements


    
  For the most expert advise, I suggest posting this to concurrency-interest.  I've noticed that these questions also do fairly well on stackoverflow.com, even though none of the JCiP authors frequent SO as far as I know.


  In answer to your question, it doesn't look difficult to clone LinkedBlockingQueue and to re-purpose the capacity and count to serve your needs.  The performance of a linked queue isn't stellar, but it is space-efficient and you are, after all, trying to cap total size...


  One question: what happens if one thread tries to enqueue an oversize element while a subsequent thread tries to enqueue a smaller element?  Should the smaller element be enqueued even though the request happened after the oversize element was blocked?  (It will be enqueued if you use the modified LBQ that I suggested.)


  Also: You'll need to make sure that notFull is signalled every time anything is removed from the queue, because you don't know how large the waiting oversized element is.


  Joe


  On Sun, May 8, 2011 at 6:52 AM, Ted Yu wrote:




    BlockingQueue's blocking behavior is determined by the number of elements.

    I wonder if there is an implementation of BlockingQueue that is aware of the sizes of the objects it holds. Meaning it would block if the next element to be queued would cause total estimated heap consumed to exceed pre-determined threshold.

    Thanks
  __._,_.___
  Reply to sender | Reply to group | Reply via web post | Start a New Topic 
  Messages in this topic (5) 
  Recent Activity: a.. New Members 1 a.. New Polls 1 
  Visit Your Group 
   Switch to: Text-Only, Daily Digest ? Unsubscribe ? Terms of Use.
   
  __,_._,___


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110509/4ee882ac/attachment.html>

From joe.bowbeer at gmail.com  Sun May  8 18:21:50 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 8 May 2011 15:21:50 -0700
Subject: [concurrency-interest] Understanding ReusableBarrier Problem
 (from'Little Book of Semaphores')
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
References: <BANLkTinLLvWeL9AHBaJ774qHN4tp+u74+g@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
Message-ID: <BANLkTim8_+eRdKagPxPowvymz5f5oA5Exg@mail.gmail.com>

The best solution for Java programmers, of course, is to construct this
two-phase barrier from two CyclicBarrier(100) instances.

On Sun, May 8, 2011 at 2:56 PM, David Holmes wrote:

> > Suppose we use this barrier for 2 threads and we pump 100 threads
> > though this barrier
>
> A barrier has to be set for the number of threads that will be "pumped
> through it" - if you are going to pump 100 threads then you need to define
> the barrier to expect 100 threads.
>
> David Holmes
>
>
> > -----Original Message-----
> >
> > While going through a concurrency problem in 'Little Book Of
> > Semaphores' I am stuck at following problem.
> >
> > The problem statement is as follows:
> >
> > Often a set of cooperating threads will perform a series of steps in a
> > loop and synchronize at a barrier after each step. For this
> > application we need a reusable barrier that locks itself after all the
> > threads have passed through.
> >
> > Given Solution is:
> >
> > 1 # rendezvous
> > 2
> > 3 mutex.wait()
> > 4     count += 1
> > 5     if count == n:
> > 6         turnstile2.wait() # lock the second
> > 7         turnstile.signal() # unlock the first
> > 8 mutex.signal()
> > 9
> > 10 turnstile.wait() # first turnstile
> > 11 turnstile.signal()
> > 12
> > 13 # critical point
> > 14
> > 15 mutex.wait()
> > 16     count -= 1
> > 17     if count == 0:
> > 18         turnstile.wait() # lock the first
> > 19         turnstile2.signal() # unlock the second
> > 20 mutex.signal()
> > 21
> > 22 turnstile2.wait() # second turnstile
> > 23 turnstile2.signal()
> >
> > Suppose we use this barrier for 2 threads and we pump 100 threads
> > through this barrier. when second thread has unlocked turnstile(7) and
> > reaches line 9, now, thread 3 come along and,
> > it increments count,
> > count > n so it releases mutex,
> > since turnstile is unlocked it reaches critical point also,
> > similarly, thread 4, thread 5, thread 6 can execute critical point,
> > executing it more than 2 times.
> > what is stopping them from passing through the barrier ahead of thread
> > 2? Or is my understanding wrong here?
> >
> > --
> > Thanks,
> > Sachin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/b9e13b78/attachment-0001.html>

From joe.bowbeer at gmail.com  Sun May  8 18:31:45 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 8 May 2011 15:31:45 -0700
Subject: [concurrency-interest] Understanding ReusableBarrier Problem
 (from'Little Book of Semaphores')
In-Reply-To: <BANLkTim8_+eRdKagPxPowvymz5f5oA5Exg@mail.gmail.com>
References: <BANLkTinLLvWeL9AHBaJ774qHN4tp+u74+g@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
	<BANLkTim8_+eRdKagPxPowvymz5f5oA5Exg@mail.gmail.com>
Message-ID: <BANLkTiny8aSo7HfoKbFFtSLe4+k2ZHV5fQ@mail.gmail.com>

PS - I wasn't aware of the little book (thanks).  The second edition is
online:

http://greenteapress.com/semaphores/
http://greenteapress.com/semaphores/downey08semaphores.pdf

<http://greenteapress.com/semaphores/downey08semaphores.pdf>It would be
interesting to see how easily these problems are solved using
java.util.concurrent.

Joe

On Sun, May 8, 2011 at 3:21 PM, Joe Bowbeer wrote:

> The best solution for Java programmers, of course, is to construct this
> two-phase barrier from two CyclicBarrier(100) instances.
>
> On Sun, May 8, 2011 at 2:56 PM, David Holmes wrote:
>
>> > Suppose we use this barrier for 2 threads and we pump 100 threads
>> > though this barrier
>>
>> A barrier has to be set for the number of threads that will be "pumped
>> through it" - if you are going to pump 100 threads then you need to define
>> the barrier to expect 100 threads.
>>
>> David Holmes
>>
>>
>> > -----Original Message-----
>> >
>> > While going through a concurrency problem in 'Little Book Of
>> > Semaphores' I am stuck at following problem.
>> >
>> > The problem statement is as follows:
>> >
>> > Often a set of cooperating threads will perform a series of steps in a
>> > loop and synchronize at a barrier after each step. For this
>> > application we need a reusable barrier that locks itself after all the
>> > threads have passed through.
>> >
>> > Given Solution is:
>> >
>> > 1 # rendezvous
>> > 2
>> > 3 mutex.wait()
>> > 4     count += 1
>> > 5     if count == n:
>> > 6         turnstile2.wait() # lock the second
>> > 7         turnstile.signal() # unlock the first
>> > 8 mutex.signal()
>> > 9
>> > 10 turnstile.wait() # first turnstile
>> > 11 turnstile.signal()
>> > 12
>> > 13 # critical point
>> > 14
>> > 15 mutex.wait()
>> > 16     count -= 1
>> > 17     if count == 0:
>> > 18         turnstile.wait() # lock the first
>> > 19         turnstile2.signal() # unlock the second
>> > 20 mutex.signal()
>> > 21
>> > 22 turnstile2.wait() # second turnstile
>> > 23 turnstile2.signal()
>> >
>> > Suppose we use this barrier for 2 threads and we pump 100 threads
>> > through this barrier. when second thread has unlocked turnstile(7) and
>> > reaches line 9, now, thread 3 come along and,
>> > it increments count,
>> > count > n so it releases mutex,
>> > since turnstile is unlocked it reaches critical point also,
>> > similarly, thread 4, thread 5, thread 6 can execute critical point,
>> > executing it more than 2 times.
>> > what is stopping them from passing through the barrier ahead of thread
>> > 2? Or is my understanding wrong here?
>> >
>> > --
>> > Thanks,
>> > Sachin
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/87c01492/attachment.html>

From davidcholmes at aapt.net.au  Sun May  8 18:33:13 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 08:33:13 +1000
Subject: [concurrency-interest] Understanding ReusableBarrier
	Problem(from'Little Book of Semaphores')
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEPKIMAA.davidcholmes@aapt.net.au>

I wrote:
> > Suppose we use this barrier for 2 threads and we pump 100 threads
> > though this barrier
>
> A barrier has to be set for the number of threads that will be "pumped
> through it" - if you are going to pump 100 threads then you need to define
> the barrier to expect 100 threads.

Except this is not a regular barrier but some specific two-phase barrier,
the semantics of which I know nothing about as I haven't see the "Little
book of semaphores".

David

> David Holmes
>
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Sachin
> > Gupta
> > Sent: Monday, 9 May 2011 5:59 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] Understanding ReusableBarrier Problem
> > (from'Little Book of Semaphores')
> >
> >
> > Hi,
> >
> > While going through a concurrency problem in 'Little Book Of
> > Semaphores' I am stuck at following problem.
> >
> > The problem statement is as follows:
> >
> > Often a set of cooperating threads will perform a series of steps in a
> > loop and synchronize at a barrier after each step. For this
> > application we need a reusable barrier that locks itself after all the
> > threads have passed through.
> >
> > Given Solution is:
> >
> > 1 # rendezvous
> > 2
> > 3 mutex.wait()
> > 4     count += 1
> > 5     if count == n:
> > 6         turnstile2.wait() # lock the second
> > 7         turnstile.signal() # unlock the first
> > 8 mutex.signal()
> > 9
> > 10 turnstile.wait() # first turnstile
> > 11 turnstile.signal()
> > 12
> > 13 # critical point
> > 14
> > 15 mutex.wait()
> > 16     count -= 1
> > 17     if count == 0:
> > 18         turnstile.wait() # lock the first
> > 19         turnstile2.signal() # unlock the second
> > 20 mutex.signal()
> > 21
> > 22 turnstile2.wait() # second turnstile
> > 23 turnstile2.signal()
> >
> > Suppose we use this barrier for 2 threads and we pump 100 threads
> > through this barrier. when second thread has unlocked turnstile(7) and
> > reaches line 9, now, thread 3 come along and,
> > it increments count,
> > count > n so it releases mutex,
> > since turnstile is unlocked it reaches critical point also,
> > similarly, thread 4, thread 5, thread 6 can execute critical point,
> > executing it more than 2 times.
> > what is stopping them from passing through the barrier ahead of thread
> > 2? Or is my understanding wrong here?
> >
> > --
> > Thanks,
> > Sachin
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Sun May  8 18:40:04 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 08:40:04 +1000
Subject: [concurrency-interest] Understanding
	ReusableBarrierProblem(from'Little Book of Semaphores')
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEPKIMAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEPLIMAA.davidcholmes@aapt.net.au>

Thanks to the link Joe provided I have now seen the book (at least skimmed
the relevant part) and as it states there:

"You can assume that there are n threads and that this value is stored in a
variable, n, that is accessible from all threads."

So this is a regular barrier and my original comment stands.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David
> Holmes
> Sent: Monday, 9 May 2011 8:33 AM
> To: Sachin Gupta; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Understanding
> ReusableBarrierProblem(from'Little Book of Semaphores')
>
>
> I wrote:
> > > Suppose we use this barrier for 2 threads and we pump 100 threads
> > > though this barrier
> >
> > A barrier has to be set for the number of threads that will be "pumped
> > through it" - if you are going to pump 100 threads then you
> need to define
> > the barrier to expect 100 threads.
>
> Except this is not a regular barrier but some specific two-phase barrier,
> the semantics of which I know nothing about as I haven't see the "Little
> book of semaphores".
>
> David
>
> > David Holmes
> >
> >
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu
> > > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Sachin
> > > Gupta
> > > Sent: Monday, 9 May 2011 5:59 AM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: [concurrency-interest] Understanding ReusableBarrier Problem
> > > (from'Little Book of Semaphores')
> > >
> > >
> > > Hi,
> > >
> > > While going through a concurrency problem in 'Little Book Of
> > > Semaphores' I am stuck at following problem.
> > >
> > > The problem statement is as follows:
> > >
> > > Often a set of cooperating threads will perform a series of steps in a
> > > loop and synchronize at a barrier after each step. For this
> > > application we need a reusable barrier that locks itself after all the
> > > threads have passed through.
> > >
> > > Given Solution is:
> > >
> > > 1 # rendezvous
> > > 2
> > > 3 mutex.wait()
> > > 4     count += 1
> > > 5     if count == n:
> > > 6         turnstile2.wait() # lock the second
> > > 7         turnstile.signal() # unlock the first
> > > 8 mutex.signal()
> > > 9
> > > 10 turnstile.wait() # first turnstile
> > > 11 turnstile.signal()
> > > 12
> > > 13 # critical point
> > > 14
> > > 15 mutex.wait()
> > > 16     count -= 1
> > > 17     if count == 0:
> > > 18         turnstile.wait() # lock the first
> > > 19         turnstile2.signal() # unlock the second
> > > 20 mutex.signal()
> > > 21
> > > 22 turnstile2.wait() # second turnstile
> > > 23 turnstile2.signal()
> > >
> > > Suppose we use this barrier for 2 threads and we pump 100 threads
> > > through this barrier. when second thread has unlocked turnstile(7) and
> > > reaches line 9, now, thread 3 come along and,
> > > it increments count,
> > > count > n so it releases mutex,
> > > since turnstile is unlocked it reaches critical point also,
> > > similarly, thread 4, thread 5, thread 6 can execute critical point,
> > > executing it more than 2 times.
> > > what is stopping them from passing through the barrier ahead of thread
> > > 2? Or is my understanding wrong here?
> > >
> > > --
> > > Thanks,
> > > Sachin
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at cs.oswego.edu
> > > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From joe.bowbeer at gmail.com  Sun May  8 18:40:19 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 8 May 2011 15:40:19 -0700
Subject: [concurrency-interest] Understanding ReusableBarrier
 Problem(from'Little Book of Semaphores')
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEPKIMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCOEPKIMAA.davidcholmes@aapt.net.au>
Message-ID: <BANLkTimpRAhTbNFVHNxcA0r=cHY8Fk04vQ@mail.gmail.com>

David,

I think you hit the nail on the head (while blindfolded with one hand tied
behind your back).

The problem is a two-phase barrier where N threads are supposed to wait at
barrier #1 for all threads to arrive, then all execute some code, then wait
again for all threads to arrive at barrier #2 before continuing.

On Sun, May 8, 2011 at 3:33 PM, David Holmes wrote:

> I wrote:
> > > Suppose we use this barrier for 2 threads and we pump 100 threads
> > > though this barrier
> >
> > A barrier has to be set for the number of threads that will be "pumped
> > through it" - if you are going to pump 100 threads then you need to
> define
> > the barrier to expect 100 threads.
>
> Except this is not a regular barrier but some specific two-phase barrier,
> the semantics of which I know nothing about as I haven't see the "Little
> book of semaphores".
>
> David
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/d359b6f9/attachment.html>

From joe.bowbeer at gmail.com  Sun May  8 19:34:44 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 8 May 2011 16:34:44 -0700
Subject: [concurrency-interest] Understanding ReusableBarrier Problem
 (from'Little Book of Semaphores')
In-Reply-To: <BANLkTim8_+eRdKagPxPowvymz5f5oA5Exg@mail.gmail.com>
References: <BANLkTinLLvWeL9AHBaJ774qHN4tp+u74+g@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEPIIMAA.davidcholmes@aapt.net.au>
	<BANLkTim8_+eRdKagPxPowvymz5f5oA5Exg@mail.gmail.com>
Message-ID: <BANLkTikRi09GKsWXK_XeCoVMa8cFnRP8yg@mail.gmail.com>

Correction:

Only one instance of CyclicBarrier(N) is needed.

The barrier is reusable (cyclic) and will reset after all threads pass
through.  So each thread can wait on the same barrier instance on entry and
on exit from their critical section.

Joe

On Sun, May 8, 2011 at 3:21 PM, Joe Bowbeer wrote:

> The best solution for Java programmers, of course, is to construct this
> two-phase barrier from two CyclicBarrier(100) instances.
>
> On Sun, May 8, 2011 at 2:56 PM, David Holmes wrote:
>
>> > Suppose we use this barrier for 2 threads and we pump 100 threads
>> > though this barrier
>>
>> A barrier has to be set for the number of threads that will be "pumped
>> through it" - if you are going to pump 100 threads then you need to define
>> the barrier to expect 100 threads.
>>
>> David Holmes
>>
>>
>> > -----Original Message-----
>> >
>> > While going through a concurrency problem in 'Little Book Of
>> > Semaphores' I am stuck at following problem.
>> >
>> > The problem statement is as follows:
>> >
>> > Often a set of cooperating threads will perform a series of steps in a
>> > loop and synchronize at a barrier after each step. For this
>> > application we need a reusable barrier that locks itself after all the
>> > threads have passed through.
>> >
>> > Given Solution is:
>> >
>> > 1 # rendezvous
>> > 2
>> > 3 mutex.wait()
>> > 4     count += 1
>> > 5     if count == n:
>> > 6         turnstile2.wait() # lock the second
>> > 7         turnstile.signal() # unlock the first
>> > 8 mutex.signal()
>> > 9
>> > 10 turnstile.wait() # first turnstile
>> > 11 turnstile.signal()
>> > 12
>> > 13 # critical point
>> > 14
>> > 15 mutex.wait()
>> > 16     count -= 1
>> > 17     if count == 0:
>> > 18         turnstile.wait() # lock the first
>> > 19         turnstile2.signal() # unlock the second
>> > 20 mutex.signal()
>> > 21
>> > 22 turnstile2.wait() # second turnstile
>> > 23 turnstile2.signal()
>> >
>> > Suppose we use this barrier for 2 threads and we pump 100 threads
>> > through this barrier. when second thread has unlocked turnstile(7) and
>> > reaches line 9, now, thread 3 come along and,
>> > it increments count,
>> > count > n so it releases mutex,
>> > since turnstile is unlocked it reaches critical point also,
>> > similarly, thread 4, thread 5, thread 6 can execute critical point,
>> > executing it more than 2 times.
>> > what is stopping them from passing through the barrier ahead of thread
>> > 2? Or is my understanding wrong here?
>> >
>> > --
>> > Thanks,
>> > Sachin
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/93d465a4/attachment-0001.html>

From ted_yu at yahoo.com  Sun May  8 19:40:58 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 8 May 2011 16:40:58 -0700 (PDT)
Subject: [concurrency-interest] BlockingQueue that is aware of sizes of
	elements
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEPIIMAA.davidcholmes@aapt.net.au>
References: <76152.14877.qm@web30808.mail.mud.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCOEPIIMAA.davidcholmes@aapt.net.au>
Message-ID: <975129.52186.qm@web30805.mail.mud.yahoo.com>

>> as the object to be enqueued 
already exists it already consumes heap
The rationale was to block future client requests through this:

??????? callQueue.put(call);????????????? // queue the call; maybe blocked here


Cheers



________________________________
From: David Holmes <davidcholmes at aapt.net.au>
To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Sent: Sunday, May 8, 2011 3:05 PM
Subject: RE: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


? 
Making 
a synchronizer for abritrary conditions is not that difficult, but it assumes 
that you can detect the necessary conditions for blocking and signalling and 
that you control all the code that affects the change of state so that you can 
introduce the necessary signalling. I'm unclear from the description exactly 
what those conditions would be in this case (as the object to be enqueued 
already exists it already consumes heap).
?
And as 
Joe states one policy decision with such things is whether blocked requests 
prevent new requests from proceeding (similar to the question as to whether a 
blocked writer prevents additional readers in a read/write 
lock).
?
Regardless this will be a custom built BlockingQueue - in fact not 
really a BlockingQueue as you won't be fulfilling the BQ 
API.
?
Cheers,
David 
Holmes
?
?-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu 
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted 
Yu
Sent: Monday, 9 May 2011 6:22 AM
To: concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Subject: Re: [concurrency-interest] [seajug] 
BlockingQueue that is aware ofsizes of elements

 
Joe:
>See https://issues.apache.org/jira/browse/HBASE-3813 for background  for my question.
>I am looking forward to expert advice from concurrency-interest.
>
>
>
>________________________________
> From: Joe Bowbeer  <joe.bowbeer at gmail.com>
>To: seajug at yahoogroups.com
>Sent: Sunday, May 8, 2011 11:05  AM
>Subject: Re: [seajug]  BlockingQueue that is aware of sizes of elements
>
>
>?
>For the most expert advise, I suggest posting this to  concurrency-interest. ?I've noticed that these questions also do fairly  well on stackoverflow.com, even though none of the JCiP authors  frequent SO as far as I know.
>
>
>In answer to your question, it doesn't look difficult to clone  LinkedBlockingQueue and to re-purpose the capacity and count to serve your  needs. ?The performance of a linked queue isn't stellar, but it is  space-efficient and you are, after all, trying to cap total size...
>
>
>One question: what happens if one thread tries to enqueue an oversize  element while a subsequent thread tries to enqueue a smaller element?  ?Should the smaller element be enqueued even though the request happened  after the oversize element was blocked? ?(It will be enqueued if you use  the modified LBQ that I suggested.)
>
>
>Also: You'll need to make sure that notFull is signalled every time  anything is removed from the queue, because you don't know how large the  waiting oversized element is.
>
>
>Joe
>
>
>On Sun, May 8, 2011 at 6:52 AM, Ted  Yu?wrote:
>
>
>>
>>
>>BlockingQueue's  blocking behavior is determined by the number of elements.
>>
>>I wonder 
    if there is an implementation of BlockingQueue that is aware of the sizes of 
    the objects it holds. Meaning it would block if the next element to be 
    queued would cause total estimated heap consumed to exceed pre-determined 
    threshold.
>>
>>Thanks
>__._,_.___
>Reply  to sender | Reply  to group | Reply via web  post | Start a New Topic Messages in this topic (5) 
>Recent Activity: 	* New Members 1 	* New Polls 1  
>Visit Your Group 
> 
>Switch to: Text-Only, Daily  Digest ? Unsubscribe ? Terms of Use
>. 
>
>__,_._,___ 
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/48d3ddbb/attachment.html>

From davidcholmes at aapt.net.au  Sun May  8 20:31:12 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 10:31:12 +1000
Subject: [concurrency-interest] BlockingQueue that is aware of sizes of
	elements
In-Reply-To: <975129.52186.qm@web30805.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEPMIMAA.davidcholmes@aapt.net.au>

Hi Ted,

So if I'm generalizing this right, the idea is to check the resource requirements of "call" against the available resources and block at this point if there are insufficient resources (whatever they may be). The hard part of such a scheme is that the state you are blocking on is external to the queue. So when another "call" completes, your system will have to make a callback to the queue to inform it that further resources are available.

It might be simpler/cleaner to introduce a "resource barrier" prior to the queue. For example, if you can quantify resources in a generic way and simply refer to available units of resources then you may be able to use a semaphore initialized to the number of resources units available. Each job then tries to take it's required number of resource units, blocking if not available, and returning them upon job completion. A FIFOSemaphore would work best for this assuming you want "calls" processed in arrival order. If you allow barging then a semaphore won't be suitable without additional anti-starvation controls.

The devil is in the details, as always.

David

  -----Original Message-----
  From: Ted Yu [mailto:ted_yu at yahoo.com]
  Sent: Monday, 9 May 2011 9:41 AM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Cc: joe.bowbeer at gmail.com
  Subject: Re: [concurrency-interest] BlockingQueue that is aware of sizes of elements


  >> as the object to be enqueued already exists it already consumes heap
  The rationale was to block future client requests through this:

          callQueue.put(call);              // queue the call; maybe blocked here



  Cheers



------------------------------------------------------------------------------
  From: David Holmes <davidcholmes at aapt.net.au>
  To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
  Cc: joe.bowbeer at gmail.com
  Sent: Sunday, May 8, 2011 3:05 PM
  Subject: RE: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


  ? 
  Making a synchronizer for abritrary conditions is not that difficult, but it assumes that you can detect the necessary conditions for blocking and signalling and that you control all the code that affects the change of state so that you can introduce the necessary signalling. I'm unclear from the description exactly what those conditions would be in this case (as the object to be enqueued already exists it already consumes heap).

  And as Joe states one policy decision with such things is whether blocked requests prevent new requests from proceeding (similar to the question as to whether a blocked writer prevents additional readers in a read/write lock).

  Regardless this will be a custom built BlockingQueue - in fact not really a BlockingQueue as you won't be fulfilling the BQ API.

  Cheers,
  David Holmes

   -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted Yu
  Sent: Monday, 9 May 2011 6:22 AM
  To: concurrency-interest at cs.oswego.edu
  Cc: joe.bowbeer at gmail.com
  Subject: Re: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


    Joe:
    See https://issues.apache.org/jira/browse/HBASE-3813 for background for my question.
    I am looking forward to expert advice from concurrency-interest.



----------------------------------------------------------------------------
    From: Joe Bowbeer <joe.bowbeer at gmail.com>
    To: seajug at yahoogroups.com
    Sent: Sunday, May 8, 2011 11:05 AM
    Subject: Re: [seajug] BlockingQueue that is aware of sizes of elements


      
    For the most expert advise, I suggest posting this to concurrency-interest.  I've noticed that these questions also do fairly well on stackoverflow.com, even though none of the JCiP authors frequent SO as far as I know.


    In answer to your question, it doesn't look difficult to clone LinkedBlockingQueue and to re-purpose the capacity and count to serve your needs.  The performance of a linked queue isn't stellar, but it is space-efficient and you are, after all, trying to cap total size...


    One question: what happens if one thread tries to enqueue an oversize element while a subsequent thread tries to enqueue a smaller element?  Should the smaller element be enqueued even though the request happened after the oversize element was blocked?  (It will be enqueued if you use the modified LBQ that I suggested.)


    Also: You'll need to make sure that notFull is signalled every time anything is removed from the queue, because you don't know how large the waiting oversized element is.


    Joe


    On Sun, May 8, 2011 at 6:52 AM, Ted Yu wrote:




      BlockingQueue's blocking behavior is determined by the number of elements.

      I wonder if there is an implementation of BlockingQueue that is aware of the sizes of the objects it holds. Meaning it would block if the next element to be queued would cause total estimated heap consumed to exceed pre-determined threshold.

      Thanks
    __._,_.___
    Reply to sender | Reply to group | Reply via web post | Start a New Topic 
    Messages in this topic (5) 
    Recent Activity: a.. New Members 1 a.. New Polls 1 
    Visit Your Group 
     Switch to: Text-Only, Daily Digest ? Unsubscribe ? Terms of Use.
     
    __,_._,___





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110509/78b6d6bb/attachment-0001.html>

From ted_yu at yahoo.com  Sun May  8 21:01:49 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 8 May 2011 18:01:49 -0700 (PDT)
Subject: [concurrency-interest] BlockingQueue that is aware of sizes of
	elements
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEPMIMAA.davidcholmes@aapt.net.au>
References: <975129.52186.qm@web30805.mail.mud.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCMEPMIMAA.davidcholmes@aapt.net.au>
Message-ID: <518914.29731.qm@web30808.mail.mud.yahoo.com>

>> the state you are blocking on is external to the queue
My plan was to ask the type E to implement a new interface, temporarily called Sizeable:

public class LinkedBlockingQueueBySize<E extends Sizeable> extends AbstractQueue<E>
??????? implements BlockingQueue<E>, java.io.Serializable {
where:
public interface Sizeable {
? // returns estimated size in heap in bytes
? long getSize();
}

LinkedBlockingQueueBySize would maintain the total estimated size of elements in it. put() would examine the size of element to see if threshold would be exceeded if the new element is enqueued. If so, put() blocks.
take() would subtract the size of the returned element from total estimated size.

FYI offer() or add() isn't used on callQueue which is declared as:
? protected BlockingQueue<Call> callQueue; // queued calls


Please comment.



________________________________
From: David Holmes <davidcholmes at aapt.net.au>
To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Sent: Sunday, May 8, 2011 5:31 PM
Subject: RE: [concurrency-interest] BlockingQueue that is aware of sizes of elements


? 
Hi 
Ted,
?
So if 
I'm generalizing this right, the idea is to check the resource requirements of 
"call" against the available resources and block at this point if there are 
insufficient resources (whatever they may be). The hard part of such a scheme is 
that the state you are blocking on is external to the queue. So when another 
"call" completes, your system will have to make a callback to the queue to 
inform it that further resources are available.
?
It 
might be simpler/cleaner to introduce a "resource barrier" prior to the queue. 
For example, if you can quantify resources in a generic way and simply refer to 
available units of resources then you may be able to use a semaphore initialized 
to the number of resources units available. Each job then tries to take it's 
required number of resource units, blocking if not available, and returning them 
upon job completion. A FIFOSemaphore would work best for this assuming you want 
"calls" processed in arrival order. If you allow barging then a semaphore won't 
be suitable without additional anti-starvation controls.
?
The 
devil is in the details, as always.
?
David
?
-----Original Message-----
>From: Ted Yu  [mailto:ted_yu at yahoo.com]
>Sent: Monday, 9 May 2011 9:41  AM
>To: dholmes at ieee.org;  concurrency-interest at cs.oswego.edu
>Cc: joe.bowbeer at gmail.com
>Subject: Re: [concurrency-interest]  BlockingQueue that is aware of sizes of elements
>
>
>>> as the object to be enqueued already exists it  already consumes heap
>The rationale was to block future client requests through  this:
>
>???????  callQueue.put(call);?????????????  // queue the call; maybe blocked here
>
>
>
>Cheers
>
>
>
>________________________________
> From: David Holmes  <davidcholmes at aapt.net.au>
>To: Ted Yu <ted_yu at yahoo.com>;  concurrency-interest at cs.oswego.edu
>Cc: joe.bowbeer at gmail.com
>Sent: Sunday, May 8, 2011 3:05  PM
>Subject: RE:  [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of  elements
>
>
>? 
>Making a synchronizer for abritrary conditions is not that  difficult, but it assumes that you can detect the necessary conditions for  blocking and signalling and that you control all the code that affects the  change of state so that you can introduce the necessary signalling. I'm  unclear from the description exactly what those conditions would be in this  case (as the object to be enqueued already exists it already consumes  heap).
>?
>And as Joe states one policy decision with such things is whether  blocked requests prevent new requests from proceeding (similar to the question  as to whether a blocked writer prevents additional readers in a read/write  lock).
>?
>Regardless this will be a custom built BlockingQueue - in fact not  really a BlockingQueue as you won't be fulfilling the BQ  API.
>?
>Cheers,
>David Holmes
>?
>?-----Original  Message-----
>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted  Yu
>Sent: Monday, 9 May 2011 6:22 AM
>To: concurrency-interest at cs.oswego.edu
>Cc: joe.bowbeer at gmail.com
>Subject: Re: [concurrency-interest] [seajug]  BlockingQueue that is aware ofsizes of  elements
>
> 
>Joe:
>>See https://issues.apache.org/jira/browse/HBASE-3813 for  background for my question.
>>I am looking forward to expert advice from concurrency-interest.
>>
>>
>>
>>________________________________
>> From: Joe Bowbeer  <joe.bowbeer at gmail.com>
>>To: seajug at yahoogroups.com
>>Sent: Sunday, May 8, 2011 11:05  AM
>>Subject: Re: [seajug]  BlockingQueue that is aware of sizes of elements
>>
>>
>>?
>>For the most expert advise, I suggest posting this to  concurrency-interest. ?I've noticed that these questions also do fairly  well on stackoverflow.com, even though none of the JCiP authors  frequent SO as far as I know.
>>
>>
>>In answer to your question, it doesn't look difficult to clone  LinkedBlockingQueue and to re-purpose the capacity and count to serve your  needs. ?The performance of a linked queue isn't stellar, but it is  space-efficient and you are, after all, trying to cap total size...
>>
>>
>>One question: what happens if one thread tries to enqueue an oversize  element while a subsequent thread tries to enqueue a smaller element?  ?Should the smaller element be enqueued even though the request  happened after the oversize element was blocked? ?(It will be enqueued  if you use the modified LBQ that I suggested.)
>>
>>
>>Also: You'll need to make sure that notFull is signalled every time  anything is removed from the queue, because you don't know how large the  waiting oversized element is.
>>
>>
>>Joe
>>
>>
>>On Sun, May 8, 2011 at 6:52 AM, Ted  Yu?wrote:
>>
>>
>>>
>>>
>>>BlockingQueue's  blocking behavior is determined by the number of elements.
>>>
>>>I wonder 
      if there is an implementation of BlockingQueue that is aware of the sizes 
      of the objects it holds. Meaning it would block if the next element to be 
      queued would cause total estimated heap consumed to exceed pre-determined 
      threshold.
>>>
>>>Thanks
>>__._,_.___
>>Reply  to sender | Reply  to group | Reply via web  post | Start a New Topic Messages in this topic (5) 
>>Recent Activity: 	* New Members 1 	* New Polls 1  
>>Visit Your Group 
>> 
>>Switch to: Text-Only, Daily  Digest ? Unsubscribe ? Terms of Use
>>. 
>>
>>__,_._,___ 
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/cfe6588d/attachment-0001.html>

From davidcholmes at aapt.net.au  Sun May  8 21:18:11 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 9 May 2011 11:18:11 +1000
Subject: [concurrency-interest] BlockingQueue that is aware of sizes of
	elements
In-Reply-To: <518914.29731.qm@web30808.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEPOIMAA.davidcholmes@aapt.net.au>

Hi Ted,

I'm not following the details here. Suppose the threshold is 100 and a job of size 100 arrives. put() will succeed and set the size estimate to 100. A subsequent take will change the size estimate back to zero and start processing the job. Now a second job of 100 can arrive and we go through the same steps - resulting in two jobs of size 100 executing concurrently, then a third and a fourth etc. This is only modifying the way in which jobs get through the queue, it doesn't control the number/size of jobs concurrently active.

David

-----Original Message-----
From: Ted Yu [mailto:ted_yu at yahoo.com]
Sent: Monday, 9 May 2011 11:02 AM
To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Subject: Re: [concurrency-interest] BlockingQueue that is aware of sizes of elements


  >> the state you are blocking on is external to the queue
  My plan was to ask the type E to implement a new interface, temporarily called Sizeable:

  public class LinkedBlockingQueueBySize<E extends Sizeable> extends AbstractQueue<E>
          implements BlockingQueue<E>, java.io.Serializable {
  where:
  public interface Sizeable {
    // returns estimated size in heap in bytes
    long getSize();
  }


  LinkedBlockingQueueBySize would maintain the total estimated size of elements in it. put() would examine the size of element to see if threshold would be exceeded if the new element is enqueued. If so, put() blocks.
  take() would subtract the size of the returned element from total estimated size.


  FYI offer() or add() isn't used on callQueue which is declared as:
    protected BlockingQueue<Call> callQueue; // queued calls



  Please comment.




------------------------------------------------------------------------------
  From: David Holmes <davidcholmes at aapt.net.au>
  To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
  Cc: joe.bowbeer at gmail.com
  Sent: Sunday, May 8, 2011 5:31 PM
  Subject: RE: [concurrency-interest] BlockingQueue that is aware of sizes of elements


  ? 
  Hi Ted,

  So if I'm generalizing this right, the idea is to check the resource requirements of "call" against the available resources and block at this point if there are insufficient resources (whatever they may be). The hard part of such a scheme is that the state you are blocking on is external to the queue. So when another "call" completes, your system will have to make a callback to the queue to inform it that further resources are available.

  It might be simpler/cleaner to introduce a "resource barrier" prior to the queue. For example, if you can quantify resources in a generic way and simply refer to available units of resources then you may be able to use a semaphore initialized to the number of resources units available. Each job then tries to take it's required number of resource units, blocking if not available, and returning them upon job completion. A FIFOSemaphore would work best for this assuming you want "calls" processed in arrival order. If you allow barging then a semaphore won't be suitable without additional anti-starvation controls.

  The devil is in the details, as always.

  David

    -----Original Message-----
    From: Ted Yu [mailto:ted_yu at yahoo.com]
    Sent: Monday, 9 May 2011 9:41 AM
    To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
    Cc: joe.bowbeer at gmail.com
    Subject: Re: [concurrency-interest] BlockingQueue that is aware of sizes of elements


    >> as the object to be enqueued already exists it already consumes heap
    The rationale was to block future client requests through this:

            callQueue.put(call);              // queue the call; maybe blocked here



    Cheers



----------------------------------------------------------------------------
    From: David Holmes <davidcholmes at aapt.net.au>
    To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
    Cc: joe.bowbeer at gmail.com
    Sent: Sunday, May 8, 2011 3:05 PM
    Subject: RE: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


    ? 
    Making a synchronizer for abritrary conditions is not that difficult, but it assumes that you can detect the necessary conditions for blocking and signalling and that you control all the code that affects the change of state so that you can introduce the necessary signalling. I'm unclear from the description exactly what those conditions would be in this case (as the object to be enqueued already exists it already consumes heap).

    And as Joe states one policy decision with such things is whether blocked requests prevent new requests from proceeding (similar to the question as to whether a blocked writer prevents additional readers in a read/write lock).

    Regardless this will be a custom built BlockingQueue - in fact not really a BlockingQueue as you won't be fulfilling the BQ API.

    Cheers,
    David Holmes

     -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted Yu
    Sent: Monday, 9 May 2011 6:22 AM
    To: concurrency-interest at cs.oswego.edu
    Cc: joe.bowbeer at gmail.com
    Subject: Re: [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of elements


      Joe:
      See https://issues.apache.org/jira/browse/HBASE-3813 for background for my question.
      I am looking forward to expert advice from concurrency-interest.



--------------------------------------------------------------------------
      From: Joe Bowbeer <joe.bowbeer at gmail.com>
      To: seajug at yahoogroups.com
      Sent: Sunday, May 8, 2011 11:05 AM
      Subject: Re: [seajug] BlockingQueue that is aware of sizes of elements


        
      For the most expert advise, I suggest posting this to concurrency-interest.  I've noticed that these questions also do fairly well on stackoverflow.com, even though none of the JCiP authors frequent SO as far as I know.


      In answer to your question, it doesn't look difficult to clone LinkedBlockingQueue and to re-purpose the capacity and count to serve your needs.  The performance of a linked queue isn't stellar, but it is space-efficient and you are, after all, trying to cap total size...


      One question: what happens if one thread tries to enqueue an oversize element while a subsequent thread tries to enqueue a smaller element?  Should the smaller element be enqueued even though the request happened after the oversize element was blocked?  (It will be enqueued if you use the modified LBQ that I suggested.)


      Also: You'll need to make sure that notFull is signalled every time anything is removed from the queue, because you don't know how large the waiting oversized element is.


      Joe


      On Sun, May 8, 2011 at 6:52 AM, Ted Yu wrote:




        BlockingQueue's blocking behavior is determined by the number of elements.

        I wonder if there is an implementation of BlockingQueue that is aware of the sizes of the objects it holds. Meaning it would block if the next element to be queued would cause total estimated heap consumed to exceed pre-determined threshold.

        Thanks
      __._,_.___
      Reply to sender | Reply to group | Reply via web post | Start a New Topic 
      Messages in this topic (5) 
      Recent Activity: a.. New Members 1 a.. New Polls 1 
      Visit Your Group 
       Switch to: Text-Only, Daily Digest ? Unsubscribe ? Terms of Use.
       
      __,_._,___








-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110509/c09cc6c0/attachment-0001.html>

From ted_yu at yahoo.com  Sun May  8 23:49:40 2011
From: ted_yu at yahoo.com (Ted Yu)
Date: Sun, 8 May 2011 20:49:40 -0700 (PDT)
Subject: [concurrency-interest] BlockingQueue that is aware of sizes of
	elements
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEPOIMAA.davidcholmes@aapt.net.au>
References: <518914.29731.qm@web30808.mail.mud.yahoo.com>
	<NFBBKALFDCPFIDBNKAPCAEPOIMAA.davidcholmes@aapt.net.au>
Message-ID: <640525.3310.qm@web30808.mail.mud.yahoo.com>

David:
Thanks for your hint.
After studying HBaseServer code some more, I found places where the increment and decrement of total estimated size of elements should be executed (not considering timing of garbage collection). This means, I can approximately account for the length of time a Call is in scope.



________________________________
From: David Holmes <davidcholmes at aapt.net.au>
To: Ted Yu <ted_yu at yahoo.com>; concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Sent: Sunday, May 8, 2011 6:18 PM
Subject: RE: [concurrency-interest] BlockingQueue that is aware of sizes of elements


? 
Hi 
Ted,
?
I'm 
not following the details here. Suppose the threshold is 100 and a job of size 
100 arrives. put() will succeed and set the size estimate to 100. A subsequent 
take will change the size estimate back to zero and start processing the job. 
Now a second job of 100 can arrive and we go through the same steps - resulting 
in two jobs of size 100 executing concurrently, then a third and a fourth etc. 
This is only?modifying the way in which jobs get through the queue, it 
doesn't control the number/size of jobs concurrently active.
?
David
?
-----Original 
Message-----
From: Ted Yu [mailto:ted_yu at yahoo.com]
Sent: Monday, 9 May 2011 11:02 AM
To: dholmes at ieee.org; 
concurrency-interest at cs.oswego.edu
Cc: joe.bowbeer at gmail.com
Subject: Re: [concurrency-interest] 
BlockingQueue that is aware of sizes of elements

 
>> the state you are blocking on is external to  the queue
>My plan was to ask the type E to implement a new interface, temporarily  called Sizeable:
>
>public class LinkedBlockingQueueBySize<E extends Sizeable> extends  AbstractQueue<E>
>??????? 
  implements BlockingQueue<E>, java.io.Serializable {
>where:
>public interface Sizeable {
>? // returns estimated size in heap 
  in bytes
>? long getSize();
>}
>
>
>LinkedBlockingQueueBySize would maintain the total estimated size of  elements in it. put() would examine the size of element to see if threshold  would be exceeded if the new element is enqueued. If so, put() blocks.
>take() would subtract the size of the returned element from total  estimated size.
>
>
>FYI offer() or add() isn't used on callQueue which is declared  as:
>? protected BlockingQueue<Call> callQueue; // queued  calls
>
>
>
>Please comment.
>
>
>
>
>________________________________
> From: David Holmes  <davidcholmes at aapt.net.au>
>To: Ted Yu <ted_yu at yahoo.com>;  concurrency-interest at cs.oswego.edu
>Cc: joe.bowbeer at gmail.com
>Sent: Sunday, May 8, 2011 5:31  PM
>Subject: RE:  [concurrency-interest] BlockingQueue that is aware of sizes of  elements
>
>
>? 
>Hi Ted,
>?
>So if I'm generalizing this right, the idea is to check the  resource requirements of "call" against the available resources and block at  this point if there are insufficient resources (whatever they may be). The  hard part of such a scheme is that the state you are blocking on is external  to the queue. So when another "call" completes, your system will have to make  a callback to the queue to inform it that further resources are  available.
>?
>It might be simpler/cleaner to introduce a "resource barrier" prior  to the queue. For example, if you can quantify resources in a generic way and  simply refer to available units of resources then you may be able to use a  semaphore initialized to the number of resources units available. Each job  then tries to take it's required number of resource units, blocking if not  available, and returning them upon job completion. A FIFOSemaphore would work  best for this assuming you want "calls" processed in arrival order. If you  allow barging then a semaphore won't be suitable without additional  anti-starvation controls.
>?
>The devil is in the details, as always.
>?
>David
>?
>-----Original Message-----
>>From: Ted Yu  [mailto:ted_yu at yahoo.com]
>>Sent: Monday, 9 May 2011 9:41  AM
>>To: dholmes at ieee.org;  concurrency-interest at cs.oswego.edu
>>Cc: joe.bowbeer at gmail.com
>>Subject: Re: [concurrency-interest]  BlockingQueue that is aware of sizes of elements
>>
>>
>>>> as the object to be enqueued already exists  it already consumes heap
>>The rationale was to block future client requests through  this:
>>
>>???????  callQueue.put(call);?????????????  // queue the call; maybe blocked here
>>
>>
>>
>>Cheers
>>
>>
>>
>>________________________________
>> From: David Holmes  <davidcholmes at aapt.net.au>
>>To: Ted Yu <ted_yu at yahoo.com>;  concurrency-interest at cs.oswego.edu
>>Cc: joe.bowbeer at gmail.com
>>Sent: Sunday, May 8, 2011 3:05  PM
>>Subject: RE:  [concurrency-interest] [seajug] BlockingQueue that is aware ofsizes of  elements
>>
>>
>>? 
>>Making a synchronizer for abritrary conditions is not that  difficult, but it assumes that you can detect the necessary conditions for  blocking and signalling and that you control all the code that affects the  change of state so that you can introduce the necessary signalling. I'm  unclear from the description exactly what those conditions would be in this  case (as the object to be enqueued already exists it already consumes  heap).
>>?
>>And as Joe states one policy decision with such things is whether  blocked requests prevent new requests from proceeding (similar to the  question as to whether a blocked writer prevents additional readers in a  read/write lock).
>>?
>>Regardless this will be a custom built BlockingQueue - in fact  not really a BlockingQueue as you won't be fulfilling the BQ  API.
>>?
>>Cheers,
>>David Holmes
>>?
>>?-----Original  Message-----
>>From: concurrency-interest-bounces at cs.oswego.edu  [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ted  Yu
>>Sent: Monday, 9 May 2011 6:22 AM
>>To: concurrency-interest at cs.oswego.edu
>>Cc: joe.bowbeer at gmail.com
>>Subject: Re: [concurrency-interest] [seajug]  BlockingQueue that is aware ofsizes of  elements
>>
>> 
>>Joe:
>>>See https://issues.apache.org/jira/browse/HBASE-3813 for  background for my question.
>>>I am looking forward to expert advice from concurrency-interest.
>>>
>>>
>>>
>>>________________________________
>>> From: Joe Bowbeer  <joe.bowbeer at gmail.com>
>>>To: seajug at yahoogroups.com
>>>Sent: Sunday, May 8, 2011 11:05  AM
>>>Subject: Re: [seajug]  BlockingQueue that is aware of sizes of elements
>>>
>>>
>>>?
>>>For the most expert advise, I suggest posting this to  concurrency-interest. ?I've noticed that these questions also do  fairly well on stackoverflow.com, even though none of the JCiP authors  frequent SO as far as I know.
>>>
>>>
>>>In answer to your question, it doesn't look difficult to clone  LinkedBlockingQueue and to re-purpose the capacity and count to serve your  needs. ?The performance of a linked queue isn't stellar, but it is  space-efficient and you are, after all, trying to cap total size...
>>>
>>>
>>>One question: what happens if one thread tries to enqueue an oversize  element while a subsequent thread tries to enqueue a smaller element?  ?Should the smaller element be enqueued even though the request  happened after the oversize element was blocked? ?(It will be  enqueued if you use the modified LBQ that I suggested.)
>>>
>>>
>>>Also: You'll need to make sure that notFull is signalled every time  anything is removed from the queue, because you don't know how large the  waiting oversized element is.
>>>
>>>
>>>Joe
>>>
>>>
>>>On Sun, May 8, 2011 at 6:52 AM, Ted  Yu?wrote:
>>>
>>>
>>>>
>>>>
>>>>BlockingQueue's  blocking behavior is determined by the number of elements.
>>>>
>>>>I 
        wonder if there is an implementation of BlockingQueue that is aware of 
        the sizes of the objects it holds. Meaning it would block if the next 
        element to be queued would cause total estimated heap consumed to exceed 
        pre-determined 
      threshold.
>>>>
>>>>Thanks
>>>__._,_.___
>>>Reply  to sender | Reply  to group | Reply via web  post | Start a New Topic Messages in this topic (5) 
>>>Recent Activity: 	* New Members 1 	* New Polls 1  
>>>Visit Your Group 
>>> 
>>>Switch to: Text-Only, Daily  Digest ? Unsubscribe ? Terms  of Use
>>>. 
>>>
>>>__,_._,___ 
>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110508/a7d39dfa/attachment-0001.html>

From davidcholmes at aapt.net.au  Mon May  9 16:02:46 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 10 May 2011 06:02:46 +1000
Subject: [concurrency-interest] Understanding ReusableBarrier Problem
	(from'Little Book of Semaphores')
In-Reply-To: <BANLkTi=KG2YO-79Rbf6pzHPZDKgPxQ+4WQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEAKINAA.davidcholmes@aapt.net.au>

Hi Sachin,

> We are setting up barrier to expect 2 threads, that is it should let
> two threads through when second arrives, and it should block third one
> until fourth one arrives. so if we pump 100 threads through it, it
> should allow threads in pairs to go through it.

Thanks for clarifying that.

> However in above example, i think it will break and let all threads
> through if 2nd thread sleeps at line number 9.

The expectation, in the book examples, is that the barrier is used in a loop
by the same set of N threads. It isn't a general purpose barrier that can be
used by multiple sets of N threads.

David

> On Mon, May 9, 2011 at 3:26 AM, David Holmes
> <davidcholmes at aapt.net.au> wrote:
> >> Suppose we use this barrier for 2 threads and we pump 100 threads
> >> though this barrier
> >
> > A barrier has to be set for the number of threads that will be "pumped
> > through it" - if you are going to pump 100 threads then you
> need to define
> > the barrier to expect 100 threads.
> >
> > David Holmes
> >
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Sachin
> >> Gupta
> >> Sent: Monday, 9 May 2011 5:59 AM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] Understanding ReusableBarrier Problem
> >> (from'Little Book of Semaphores')
> >>
> >>
> >> Hi,
> >>
> >> While going through a concurrency problem in 'Little Book Of
> >> Semaphores' I am stuck at following problem.
> >>
> >> The problem statement is as follows:
> >>
> >> Often a set of cooperating threads will perform a series of steps in a
> >> loop and synchronize at a barrier after each step. For this
> >> application we need a reusable barrier that locks itself after all the
> >> threads have passed through.
> >>
> >> Given Solution is:
> >>
> >> 1 # rendezvous
> >> 2
> >> 3 mutex.wait()
> >> 4 ? ? count += 1
> >> 5 ? ? if count == n:
> >> 6 ? ? ? ? turnstile2.wait() # lock the second
> >> 7 ? ? ? ? turnstile.signal() # unlock the first
> >> 8 mutex.signal()
> >> 9
> >> 10 turnstile.wait() # first turnstile
> >> 11 turnstile.signal()
> >> 12
> >> 13 # critical point
> >> 14
> >> 15 mutex.wait()
> >> 16 ? ? count -= 1
> >> 17 ? ? if count == 0:
> >> 18 ? ? ? ? turnstile.wait() # lock the first
> >> 19 ? ? ? ? turnstile2.signal() # unlock the second
> >> 20 mutex.signal()
> >> 21
> >> 22 turnstile2.wait() # second turnstile
> >> 23 turnstile2.signal()
> >>
> >> Suppose we use this barrier for 2 threads and we pump 100 threads
> >> through this barrier. when second thread has unlocked turnstile(7) and
> >> reaches line 9, now, thread 3 come along and,
> >> it increments count,
> >> count > n so it releases mutex,
> >> since turnstile is unlocked it reaches critical point also,
> >> similarly, thread 4, thread 5, thread 6 can execute critical point,
> >> executing it more than 2 times.
> >> what is stopping them from passing through the barrier ahead of thread
> >> 2? Or is my understanding wrong here?
> >>
> >> --
> >> Thanks,
> >> Sachin
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> >
>



From martinrb at google.com  Mon May 16 16:07:27 2011
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 16 May 2011 13:07:27 -0700
Subject: [concurrency-interest] How to write a threadpool that honors
	locality hints?
Message-ID: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com>

Hey threadpool experts,

It often happens that sets of tasks submitted to a thread pool are
related in some way, and there is likely to be a performance boost if
these tasks are executed on the same thread (or perhaps also the same
core on a NUMA system).  Is there any code/design out there for doing
that?

E.g. implement an interface like this:

public interface AffinityExecutor extends Executor {

  /**
   * @param hint locality scheduling hint
   */
  void execute(int hint, Runnable command);
}

From aleksey.shipilev at gmail.com  Mon May 16 16:27:33 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Tue, 17 May 2011 00:27:33 +0400
Subject: [concurrency-interest] How to write a threadpool that honors
 locality hints?
In-Reply-To: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com>
References: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com>
Message-ID: <BANLkTimCYdG4vBn-Q5bVsm_eFXBCq9CbnQ@mail.gmail.com>

Hi Martin,

FWIW, Java code is unaware about hw layout, so that would be tricky
without JVM support. Naive and straight-forward way will be doing
native thread management via JNI, with all the horrors and
idiosyncrasies of doing that.

However, you might trick your application into having multiple
threadpools with single threads, and submitting all the Runnable's
based on "hints", then pray your OS will not migrate that thread very
often. I think you can hide a number of those under AffinityExecutor
facade. I wonder if you need balancing between those pools, i.e. when
one pool is overpopulated, should it overflow to others?

-Aleksey.

On Tue, May 17, 2011 at 12:07 AM, Martin Buchholz <martinrb at google.com> wrote:
> Hey threadpool experts,
>
> It often happens that sets of tasks submitted to a thread pool are
> related in some way, and there is likely to be a performance boost if
> these tasks are executed on the same thread (or perhaps also the same
> core on a NUMA system). ?Is there any code/design out there for doing
> that?
>
> E.g. implement an interface like this:
>
> public interface AffinityExecutor extends Executor {
>
> ?/**
> ? * @param hint locality scheduling hint
> ? */
> ?void execute(int hint, Runnable command);
> }
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From aleksey.shipilev at gmail.com  Mon May 16 16:37:57 2011
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Tue, 17 May 2011 00:37:57 +0400
Subject: [concurrency-interest] How to write a threadpool that honors
 locality hints?
In-Reply-To: <BANLkTimCYdG4vBn-Q5bVsm_eFXBCq9CbnQ@mail.gmail.com>
References: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com>
	<BANLkTimCYdG4vBn-Q5bVsm_eFXBCq9CbnQ@mail.gmail.com>
Message-ID: <BANLkTikCdrTc+EhdBZzgX65QN8fnRjLM8A@mail.gmail.com>

Re-read my note, deserves some code (dirty, buggy, incomplete, etc.,
but still explains the idea):

public class AffinityExecutor extends Executor {

  private ExecutorService[] executors = ... //

  public AffinityExecutor(int initialCapacity) {
      this.count = initialCapacity;
      for(int c = 0; c < this.count; c++) {
           executors[c] = ... // single-threaded ThreadPoolExecutor
with bounded queue
      }
  }

 ?void execute(int hint, Runnable command) {
      for (int c = 0; c < count; c++) {
	 try {
             executors[(hint+c) % this.count].execute(command);
             return;
	 } catch(RejectedExecutionException e) {
             // whoops;
         }
      }
      throw new RejectedExecutionException("Whoops");
  }
}

On Tue, May 17, 2011 at 12:27 AM, Aleksey Shipilev
<aleksey.shipilev at gmail.com> wrote:
> Hi Martin,
>
> FWIW, Java code is unaware about hw layout, so that would be tricky
> without JVM support. Naive and straight-forward way will be doing
> native thread management via JNI, with all the horrors and
> idiosyncrasies of doing that.
>
> However, you might trick your application into having multiple
> threadpools with single threads, and submitting all the Runnable's
> based on "hints", then pray your OS will not migrate that thread very
> often. I think you can hide a number of those under AffinityExecutor
> facade. I wonder if you need balancing between those pools, i.e. when
> one pool is overpopulated, should it overflow to others?
>
> -Aleksey.
>
> On Tue, May 17, 2011 at 12:07 AM, Martin Buchholz <martinrb at google.com> wrote:
>> Hey threadpool experts,
>>
>> It often happens that sets of tasks submitted to a thread pool are
>> related in some way, and there is likely to be a performance boost if
>> these tasks are executed on the same thread (or perhaps also the same
>> core on a NUMA system). ?Is there any code/design out there for doing
>> that?
>>
>> E.g. implement an interface like this:
>>
>> public interface AffinityExecutor extends Executor {
>>
>> ?/**
>> ? * @param hint locality scheduling hint
>> ? */
>> ?void execute(int hint, Runnable command);
>> }
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From martinrb at google.com  Mon May 16 19:23:45 2011
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 16 May 2011 16:23:45 -0700
Subject: [concurrency-interest] How to write a threadpool that honors
 locality hints?
In-Reply-To: <BANLkTikCdrTc+EhdBZzgX65QN8fnRjLM8A@mail.gmail.com>
References: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com>
	<BANLkTimCYdG4vBn-Q5bVsm_eFXBCq9CbnQ@mail.gmail.com>
	<BANLkTikCdrTc+EhdBZzgX65QN8fnRjLM8A@mail.gmail.com>
Message-ID: <BANLkTimz4W8uDDmY1p-5sH47_dtTQtawWQ@mail.gmail.com>

On Mon, May 16, 2011 at 13:37, Aleksey Shipilev
<aleksey.shipilev at gmail.com> wrote:
> ?public AffinityExecutor(int initialCapacity) {
> ? ? ?this.count = initialCapacity;
> ? ? ?for(int c = 0; c < this.count; c++) {
> ? ? ? ? ? executors[c] = ... // single-threaded ThreadPoolExecutor
> with bounded queue

yeah, I was also thinking of going down this road, but then you have
to figure out how to limit the size of the data structure - you can't
have a single threaded pool for every hint value.  Managing those
pools is going to be hairy - you have to worry about cleanup and
expiry, and how to minimize the overhead.  You don't want to have an
exception thrown during normal task submission.  You don't want lots
of single-threaded executors sitting idle.

Martin

> ? ? ?}
> ?}
>
> ??void execute(int hint, Runnable command) {
> ? ? ?for (int c = 0; c < count; c++) {
> ? ? ? ? try {
> ? ? ? ? ? ? executors[(hint+c) % this.count].execute(command);
> ? ? ? ? ? ? return;
> ? ? ? ? } catch(RejectedExecutionException e) {
> ? ? ? ? ? ? // whoops;
> ? ? ? ? }
> ? ? ?}
> ? ? ?throw new RejectedExecutionException("Whoops");
> ?}
> }
>
> On Tue, May 17, 2011 at 12:27 AM, Aleksey Shipilev
> <aleksey.shipilev at gmail.com> wrote:
>> Hi Martin,
>>
>> FWIW, Java code is unaware about hw layout, so that would be tricky
>> without JVM support. Naive and straight-forward way will be doing
>> native thread management via JNI, with all the horrors and
>> idiosyncrasies of doing that.
>>
>> However, you might trick your application into having multiple
>> threadpools with single threads, and submitting all the Runnable's
>> based on "hints", then pray your OS will not migrate that thread very
>> often. I think you can hide a number of those under AffinityExecutor
>> facade. I wonder if you need balancing between those pools, i.e. when
>> one pool is overpopulated, should it overflow to others?
>>
>> -Aleksey.
>>
>> On Tue, May 17, 2011 at 12:07 AM, Martin Buchholz <martinrb at google.com> wrote:
>>> Hey threadpool experts,
>>>
>>> It often happens that sets of tasks submitted to a thread pool are
>>> related in some way, and there is likely to be a performance boost if
>>> these tasks are executed on the same thread (or perhaps also the same
>>> core on a NUMA system). ?Is there any code/design out there for doing
>>> that?
>>>
>>> E.g. implement an interface like this:
>>>
>>> public interface AffinityExecutor extends Executor {
>>>
>>> ?/**
>>> ? * @param hint locality scheduling hint
>>> ? */
>>> ?void execute(int hint, Runnable command);
>>> }
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>


From g.arora at iontrading.com  Tue May 17 02:39:58 2011
From: g.arora at iontrading.com (Gaurav Arora)
Date: Tue, 17 May 2011 12:09:58 +0530
Subject: [concurrency-interest] How to write a threadpool that honors
 locality hints?
In-Reply-To: <BANLkTimz4W8uDDmY1p-5sH47_dtTQtawWQ@mail.gmail.com>
References: <BANLkTimTu4rMwmukCbz3ccpEepSPLpgVbQ@mail.gmail.com><BANLkTimCYdG4vBn-Q5bVsm_eFXBCq9CbnQ@mail.gmail.com><BANLkTikCdrTc+EhdBZzgX65QN8fnRjLM8A@mail.gmail.com>
	<BANLkTimz4W8uDDmY1p-5sH47_dtTQtawWQ@mail.gmail.com>
Message-ID: <aa17212efce53779d603abdba79b269a@mail.gmail.com>

Martin,

We use a similar approach to what Aleksey has suggested because we want
certain tasks to be executed one after the other always but with an
acceptable loss of parallelism. The problem of executors sitting idle was
discussed for long with an "overflow" suggestion made but there was no
guaranteed way that we could move a whole set of tasks to another executor
without ugly and costly locking (or atleast not one that we could figure
out).

It was eventually decided to let the application manage the problem of
single executors sitting idle by increasing the size of the thread pool
and resizing the pool when load was reduced/increased (again costly
locking but now atleast the application decides when to lock). Not the
best solution but the only one that could satisfy all needs.

If you do come up with a cleaner design, please do share. I feel this is
something that should be a part of the j.u.c. classes because it's a very
basic need and I see a lot of people requesting it.

--Gaurav

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin
Buchholz
Sent: Tuesday, May 17, 2011 4:54 AM
To: Aleksey Shipilev
Cc: concurrency-interest
Subject: Re: [concurrency-interest] How to write a threadpool that honors
locality hints?

On Mon, May 16, 2011 at 13:37, Aleksey Shipilev
<aleksey.shipilev at gmail.com> wrote:
> ?public AffinityExecutor(int initialCapacity) {
> ? ? ?this.count = initialCapacity;
> ? ? ?for(int c = 0; c < this.count; c++) {
> ? ? ? ? ? executors[c] = ... // single-threaded ThreadPoolExecutor
> with bounded queue

yeah, I was also thinking of going down this road, but then you have
to figure out how to limit the size of the data structure - you can't
have a single threaded pool for every hint value.  Managing those
pools is going to be hairy - you have to worry about cleanup and
expiry, and how to minimize the overhead.  You don't want to have an
exception thrown during normal task submission.  You don't want lots
of single-threaded executors sitting idle.

Martin

> ? ? ?}
> ?}
>
> ??void execute(int hint, Runnable command) {
> ? ? ?for (int c = 0; c < count; c++) {
> ? ? ? ? try {
> ? ? ? ? ? ? executors[(hint+c) % this.count].execute(command);
> ? ? ? ? ? ? return;
> ? ? ? ? } catch(RejectedExecutionException e) {
> ? ? ? ? ? ? // whoops;
> ? ? ? ? }
> ? ? ?}
> ? ? ?throw new RejectedExecutionException("Whoops");
> ?}
> }
>
> On Tue, May 17, 2011 at 12:27 AM, Aleksey Shipilev
> <aleksey.shipilev at gmail.com> wrote:
>> Hi Martin,
>>
>> FWIW, Java code is unaware about hw layout, so that would be tricky
>> without JVM support. Naive and straight-forward way will be doing
>> native thread management via JNI, with all the horrors and
>> idiosyncrasies of doing that.
>>
>> However, you might trick your application into having multiple
>> threadpools with single threads, and submitting all the Runnable's
>> based on "hints", then pray your OS will not migrate that thread very
>> often. I think you can hide a number of those under AffinityExecutor
>> facade. I wonder if you need balancing between those pools, i.e. when
>> one pool is overpopulated, should it overflow to others?
>>
>> -Aleksey.
>>
>> On Tue, May 17, 2011 at 12:07 AM, Martin Buchholz <martinrb at google.com>
wrote:
>>> Hey threadpool experts,
>>>
>>> It often happens that sets of tasks submitted to a thread pool are
>>> related in some way, and there is likely to be a performance boost if
>>> these tasks are executed on the same thread (or perhaps also the same
>>> core on a NUMA system). ?Is there any code/design out there for doing
>>> that?
>>>
>>> E.g. implement an interface like this:
>>>
>>> public interface AffinityExecutor extends Executor {
>>>
>>> ?/**
>>> ? * @param hint locality scheduling hint
>>> ? */
>>> ?void execute(int hint, Runnable command);
>>> }
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Tue May 17 03:12:06 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 17 May 2011 17:12:06 +1000
Subject: [concurrency-interest] How to write a threadpool that honors
	locality hints?
In-Reply-To: <aa17212efce53779d603abdba79b269a@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECOINAA.davidcholmes@aapt.net.au>

Guarav,

Can you clarify what this "basic need" is please? If tasks have dependencies and have to be executed serially then single-threaded executor seems the best fit for such task groups, but without defining such groups in detail with knowledge of associations, expected arrival rates and associated execution costs, there's no way to do any kind of reasonable load balancing.

The CPU/node affinity issue is completely different, and as there are no such APIs available to use then it is hard to determine how they could be exposed through an Executor. The Real-time Specification for Java is adding basic "processor set" support to aid in affinity - though perversely such affinity bindings are anethema to real-time scheduling guarantees. More general NUMA type locality support is even harder - even the POSIX folk abandoned that effort.

Anyway I'm just curious what the actual need is here. If it can be well defined and easily implemented and is useful to enough people then perhaps it may be a candidate for Java 8 or 9.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gaurav
> Arora
> Sent: Tuesday, 17 May 2011 4:40 PM
> To: Martin Buchholz; Aleksey Shipilev
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] How to write a threadpool that
> honors locality hints?
> 
> 
> Martin,
> 
> We use a similar approach to what Aleksey has suggested because we want
> certain tasks to be executed one after the other always but with an
> acceptable loss of parallelism. The problem of executors sitting idle was
> discussed for long with an "overflow" suggestion made but there was no
> guaranteed way that we could move a whole set of tasks to another executor
> without ugly and costly locking (or atleast not one that we could figure
> out).
> 
> It was eventually decided to let the application manage the problem of
> single executors sitting idle by increasing the size of the thread pool
> and resizing the pool when load was reduced/increased (again costly
> locking but now atleast the application decides when to lock). Not the
> best solution but the only one that could satisfy all needs.
> 
> If you do come up with a cleaner design, please do share. I feel this is
> something that should be a part of the j.u.c. classes because it's a very
> basic need and I see a lot of people requesting it.
> 
> --Gaurav
> 
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin
> Buchholz
> Sent: Tuesday, May 17, 2011 4:54 AM
> To: Aleksey Shipilev
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] How to write a threadpool that honors
> locality hints?
> 
> On Mon, May 16, 2011 at 13:37, Aleksey Shipilev
> <aleksey.shipilev at gmail.com> wrote:
> >  public AffinityExecutor(int initialCapacity) {
> >      this.count = initialCapacity;
> >      for(int c = 0; c < this.count; c++) {
> >           executors[c] = ... // single-threaded ThreadPoolExecutor
> > with bounded queue
> 
> yeah, I was also thinking of going down this road, but then you have
> to figure out how to limit the size of the data structure - you can't
> have a single threaded pool for every hint value.  Managing those
> pools is going to be hairy - you have to worry about cleanup and
> expiry, and how to minimize the overhead.  You don't want to have an
> exception thrown during normal task submission.  You don't want lots
> of single-threaded executors sitting idle.
> 
> Martin
> 
> >      }
> >  }
> >
> >   void execute(int hint, Runnable command) {
> >      for (int c = 0; c < count; c++) {
> >         try {
> >             executors[(hint+c) % this.count].execute(command);
> >             return;
> >         } catch(RejectedExecutionException e) {
> >             // whoops;
> >         }
> >      }
> >      throw new RejectedExecutionException("Whoops");
> >  }
> > }
> >
> > On Tue, May 17, 2011 at 12:27 AM, Aleksey Shipilev
> > <aleksey.shipilev at gmail.com> wrote:
> >> Hi Martin,
> >>
> >> FWIW, Java code is unaware about hw layout, so that would be tricky
> >> without JVM support. Naive and straight-forward way will be doing
> >> native thread management via JNI, with all the horrors and
> >> idiosyncrasies of doing that.
> >>
> >> However, you might trick your application into having multiple
> >> threadpools with single threads, and submitting all the Runnable's
> >> based on "hints", then pray your OS will not migrate that thread very
> >> often. I think you can hide a number of those under AffinityExecutor
> >> facade. I wonder if you need balancing between those pools, i.e. when
> >> one pool is overpopulated, should it overflow to others?
> >>
> >> -Aleksey.
> >>
> >> On Tue, May 17, 2011 at 12:07 AM, Martin Buchholz <martinrb at google.com>
> wrote:
> >>> Hey threadpool experts,
> >>>
> >>> It often happens that sets of tasks submitted to a thread pool are
> >>> related in some way, and there is likely to be a performance boost if
> >>> these tasks are executed on the same thread (or perhaps also the same
> >>> core on a NUMA system).  Is there any code/design out there for doing
> >>> that?
> >>>
> >>> E.g. implement an interface like this:
> >>>
> >>> public interface AffinityExecutor extends Executor {
> >>>
> >>>  /**
> >>>   * @param hint locality scheduling hint
> >>>   */
> >>>  void execute(int hint, Runnable command);
> >>> }
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From concurrency-interest at cs.oswego.edu  Thu May 19 19:40:27 2011
From: concurrency-interest at cs.oswego.edu (concurrency-interest at cs.oswego.edu)
Date: Thu, 19 May 2011 15:40:27 -0800
Subject: [concurrency-interest] Let's talk online
Message-ID: <2376368357.BDBFPL34645683@uneds.bolcm.tv>

If you're looking for a hot female partner for emailing, chatting in icq, or skypationship, I'm here at your service! 

I'm looking forward to your email! www.rusukrdating.ru


From blue at cs.oswego.edu  Thu May 19 20:50:23 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 01:50:23 +0100
Subject: [concurrency-interest] Don't be shy to chat with a girl
Message-ID: <7803236575.X5LM434Z702284@twdyuicanm.hlfcmv.info>

Wanna find a pretty virtual girlfriend? I'm ready to communicate with you in any possible way: email, icq, Skype, or even real life meetings! 

I'm looking forward to your email! www.rusukrdating.ru


From concurrency-interest at cs.oswego.edu  Thu May 19 22:22:40 2011
From: concurrency-interest at cs.oswego.edu (concurrency-interest at cs.oswego.edu)
Date: Fri, 20 May 2011 09:22:40 +0700
Subject: [concurrency-interest] I'm looking for your info
Message-ID: <5284312599.9SD1DBND881362@myizdkmq.fqkwflesgpsjmod.net>

Don't you think it's time for a virtual romance with a cute girl? If so, I'm ready to start chatting with you! 

Looking forward to hearing from you! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 01:43:35 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 08:43:35 +0300
Subject: [concurrency-interest] Get ready for unforgettable romance
Message-ID: <8046897403.VD63YUZO368523@qwxcohhct.vxhemiah.va>

If you're looking for a hot female partner for emailing, chatting in icq, or skypationship, I'm here at your service! 

Let's start communicating today! www.rusukrdating.ru


From concurrency-interest at cs.oswego.edu  Fri May 20 03:14:16 2011
From: concurrency-interest at cs.oswego.edu (concurrency-interest at cs.oswego.edu)
Date: Fri, 20 May 2011 12:44:16 +0530
Subject: [concurrency-interest] I would like to meet you
Message-ID: <8003972488.FQQKAZPP174131@xxsxmvpkah.wfnnlfegieodx.tv>

Wanna have some romantic relationships with a cute girl? I'm a yummy blonde, and I'll be happy to start our correspondence! 

Looking forward to hearing from you! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 04:16:33 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 05:16:33 -0300
Subject: [concurrency-interest] It would be nice to talk to yoU!
Message-ID: <7339438450.HC4CEV87153919@ugfhdunuqsg.mwpnrpf.info>

Hi! I'm a nice girl interested in some virtual chatting, emailing, or skyping with a nice guy.  

Drop her a line! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 05:32:03 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 12:32:03 +0300
Subject: [concurrency-interest] Do you want to meet a nice friend?
Message-ID: <2632914834.1IUCZP71826715@vywcwp.wtrfizfln.biz>

Hi! I'm a nice girl interested in some virtual chatting, emailing, or skyping with a nice guy.  

Start our virtual relationships now! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 06:39:49 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 16:39:49 +0600
Subject: [concurrency-interest] Let's make friends
Message-ID: <6152898098.2N4VD2CA598343@xoazcuzeoyuehm.uugzdtfvqnrwd.com>

I'm a hot girl, and I'm eager to make friends with a cool guy for communicating in cyberspace and/or in real life! 

Start our virtual relationships now! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 07:48:30 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 13:48:30 +0200
Subject: [concurrency-interest] My greetings to you!
Message-ID: <0272895335.6MW4TJZS233066@gwsynvybollbh.mzdfrwkj.biz>

Don't you think it's time for a virtual romance with a cute girl? If so, I'm ready to start chatting with you! 

Hope to hear from you soon! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 08:47:28 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 13:47:28 +0100
Subject: [concurrency-interest] I'm eager to be your friend
Message-ID: <1878606133.Y0HZVJG5684045@pviyb.nqhsahdiuu.org>

Wanna find a pretty virtual girlfriend? I'm ready to communicate with you in any possible way: email, icq, Skype, or even real life meetings! 

Let's start communicating today! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 11:37:16 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 12:37:16 -0300
Subject: [concurrency-interest] I wish to be your pen friend
Message-ID: <0581264906.32DGWCET872128@imqtadzeneviqm.hlfscdbnhgfg.org>

Have you ever thought of meeting your true love in the Internet? A hot beautiful woman is waiting for you to chat with her! 

Let's start communicating today! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 14:19:59 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 20:19:59 +0200
Subject: [concurrency-interest] Get to know me!
Message-ID: <6418497154.S2CBJOGI862906@ssqtxx.gnqvujcaros.su>

Hi! I'm a nice girl interested in some virtual chatting, emailing, or skyping with a nice guy.  

Hope to hear from you soon! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 15:17:53 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 21:17:53 +0200
Subject: [concurrency-interest] Let's communicate online
Message-ID: <9943660090.2K4PWP07908823@cpaoccscvu.cmswqkof.com>

If you're looking for a hot female partner for emailing, chatting in icq, or skypationship, I'm here at your service! 

Looking forward to hearing from you! www.rusukrdating.ru


From blue at cs.oswego.edu  Fri May 20 16:19:21 2011
From: blue at cs.oswego.edu (blue at cs.oswego.edu)
Date: Fri, 20 May 2011 17:19:21 -0300
Subject: [concurrency-interest] Feel a real virtual pleasure!
Message-ID: <4658524459.VDR07PZ4329067@xahdtwr.lrmzzumnqaz.org>

Don't you think it's time for a virtual romance with a cute girl? If so, I'm ready to start chatting with you! 

Let's start communicating today! www.rusukrdating.ru


From dl at cs.oswego.edu  Fri May 20 18:56:13 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 20 May 2011 18:56:13 -0400
Subject: [concurrency-interest] Sorry for the spam
Message-ID: <4DD6F18D.6040508@cs.oswego.edu>


Apparently some spam network found a new way to infiltrate mailman
lists. The problem should now be fixed by adding another
layer of filtering on this list. If you try to post and suspect
that it got filtered out and so didn't make it to the list,
please let me know.

-Doug


From hawkinsp at cs.stanford.edu  Mon May 23 13:22:54 2011
From: hawkinsp at cs.stanford.edu (Peter Hawkins)
Date: Mon, 23 May 2011 13:22:54 -0400
Subject: [concurrency-interest] Examples of ConcurrentMap client code
Message-ID: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>

Hi...

I'm a grad student working on techniques for helping programmers work
with concurrent associative containers, such as ConcurrentHashMap or
ConcurrentSkipListMap. To help evaluate my research, I'm looking for
real-world code that uses the Java concurrent map classes.

After looking at a bunch of open source java code, I've come to two conclusions:
a) the predominant use for ConcurrentHashMap is to build concurrent caches.
b) much code that uses ConcurrentHashMap is racy. Programmers often do
things like a get followed by a put, erroneously believing that such
code is atomic, very similar to the way in which people often misuse
the SynchronizedMap classes.

I'm rather hoping I'm wrong about both points, perhaps because I
haven't been looking in the right place.

I'd like to find examples of code that contains non-trivial (more than
just a single ConcurrentMap used by itself as a cache) and, ideally,
correct uses of concurrent maps. I'm hoping people on this list can
point me towards some examples, and I'd greatly appreciate any
pointers you may have.

Thanks,
Peter

From joe.bowbeer at gmail.com  Mon May 23 13:43:59 2011
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 23 May 2011 10:43:59 -0700
Subject: [concurrency-interest] Examples of ConcurrentMap client code
In-Reply-To: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
References: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
Message-ID: <BANLkTinfiKV2jfcMgnJvUnqrJrJ5z=G=Lw@mail.gmail.com>

Peter,

You're likely to find some advanced uses of CHM if you search for uses of
the ConcurrentMap methods: putIfAbsent, two-arg remove, and the replace
methods.

The memoizing examples in JCiP may have influenced some programmers:

http://jcip.net/listings.html

One of the thorny problems in the Memoizer approach is how to address
transient errors and retries.

Joe

On Mon, May 23, 2011 at 10:22 AM, Peter Hawkins wrote:

> Hi...
>
> I'm a grad student working on techniques for helping programmers work
> with concurrent associative containers, such as ConcurrentHashMap or
> ConcurrentSkipListMap. To help evaluate my research, I'm looking for
> real-world code that uses the Java concurrent map classes.
>
> After looking at a bunch of open source java code, I've come to two
> conclusions:
> a) the predominant use for ConcurrentHashMap is to build concurrent caches.
> b) much code that uses ConcurrentHashMap is racy. Programmers often do
> things like a get followed by a put, erroneously believing that such
> code is atomic, very similar to the way in which people often misuse
> the SynchronizedMap classes.
>
> I'm rather hoping I'm wrong about both points, perhaps because I
> haven't been looking in the right place.
>
> I'd like to find examples of code that contains non-trivial (more than
> just a single ConcurrentMap used by itself as a cache) and, ideally,
> correct uses of concurrent maps. I'm hoping people on this list can
> point me towards some examples, and I'd greatly appreciate any
> pointers you may have.
>
> Thanks,
> Peter
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110523/1856784d/attachment.html>

From davidcholmes at aapt.net.au  Mon May 23 18:06:49 2011
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 24 May 2011 08:06:49 +1000
Subject: [concurrency-interest] Examples of ConcurrentMap client code
In-Reply-To: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEFIINAA.davidcholmes@aapt.net.au>

Hi Peter,

Peter Hawkins writes:
> I'm a grad student working on techniques for helping programmers work
> with concurrent associative containers, such as ConcurrentHashMap or
> ConcurrentSkipListMap. To help evaluate my research, I'm looking for
> real-world code that uses the Java concurrent map classes.
>
> After looking at a bunch of open source java code, I've come to
> two conclusions:
> a) the predominant use for ConcurrentHashMap is to build
> concurrent caches.
> b) much code that uses ConcurrentHashMap is racy. Programmers often do
> things like a get followed by a put, erroneously believing that such
> code is atomic, very similar to the way in which people often misuse
> the SynchronizedMap classes.
>
> I'm rather hoping I'm wrong about both points, perhaps because I
> haven't been looking in the right place.

I'm not sure why (a) is a surprise as by definition a Map is a look-up table
which is a cache: you use a key to find a value rather than recomputing the
value directly.

(b) is sadly not a surprise. I suspect much of that code previously used
Hashtable in the same racy way. One would hope that in updating/using CHM
they would notice putIfAbsent.

David Holmes
------------

> I'd like to find examples of code that contains non-trivial (more than
> just a single ConcurrentMap used by itself as a cache) and, ideally,
> correct uses of concurrent maps. I'm hoping people on this list can
> point me towards some examples, and I'd greatly appreciate any
> pointers you may have.
>
> Thanks,
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ktlam at cs.hku.hk  Mon May 23 19:54:12 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Tue, 24 May 2011 07:54:12 +0800
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
	volatile fields
Message-ID: <00f101cc19a4$b9359810$f8b10893@ktlampc>

Dear all, 

I really have a big trouble or doubt in how ConcurrentHashMap gets some
inconsistency scenarios solved. Hope you could help answering. Thank you in
advance.

The problems seem hard to describe. Please suffer me writing more text to
illustrate.

As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
often used as a concurrent or scalable cache, e.g. for caching HTTP
responses (web pages) generated from querying a database. Suppose the
application tier has cached an entry (keyX, pageX) in the map. So further
requests retrieving pageX can be served by the cache entry, without going
thru the database tier. However, the application receives a request to
modify the database records related to pageX. So it evicts the cache entry
of pageX from the map by calling remove(keyX). In the meanwhile, there can
be other incoming retrieval requests that perform get(keyX) concurrently.

Suppose all threads draw requests from a single queue to serve.

Thread T1 draws an update request, calls remove(keyX) to evict cache, then
updates the data for pageX in the database. 

Thread T2 draws a retrieval request, and calls get(keyX).


Suppose T2's action happens after T1's all actions in real time. Then we
should expect T2 get null for keyX. This is the desired behavior for
avoiding some inconsistency between the cache and database. And T2 will
contact the database for up-to-date data and make a new cache entry for
pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
at get(keyX) and contacts the database, the data update performed by T1 may
not have started or finished, and needs one more level of synchronization,
say wrapping the Java code calling the SQL commands in synchronized.)


Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
get()?

In the CHM code (appended below), I know that there is a volatile write to
the count field in remove() and volatile read from count in get(). This sets
up a happens-before order on the two operations.

But I think all would agree the JMM enforces sequential consistency (SC) for
volatile accesses. In SC, a read may not return the last write by another
thread in real-time saying. Or you may imagine the JVM is running in a
distributed setting, like Terracotta. So T2 may be reading its (stale) value
under its cache, or the updates of T1 are still in its store buffer, not yet
flushed to main memory. So even if T1's remove() has just done "count = c"
(write-volatile), T2 may not yet see it at checking "count != 0"
(read-volatile), and can return the old hash entry of pageX in the map, that
was cached in processor cache (If T2 does not see the new count, it must
also not see the delete of pageX). May I ask if this is one acceptable
behavior of CHM?


Best regards,
King Tin


Append some code of CHM for easier discussion:
---------------------------------------------

        Object get(Object key, int hash) {
            if (count != 0) { // read-volatile
                HashEntry e = getFirst(hash);
                while (e != null) {
                    if (e.hash == hash && key.equals(e.key)) {
                        Object v = e.value;
                        if (v != null)
                            return v;
                        return readValueUnderLock(e); // recheck
                    }
                    e = e.next;
                }
            }
            return null;
        }

        Object remove(Object key, int hash, Object value) {
            lock();
            try {
                int c = count - 1;
                HashEntry[] tab = table;
                int index = hash & (tab.length - 1);
                HashEntry first = tab[index];
                HashEntry e = first;
                while (e != null && (e.hash != hash || !key.equals(e.key)))
                    e = e.next;

                Object oldValue = null;
                if (e != null) {
                    Object v = e.value;
                    if (value == null || value.equals(v)) {
                        oldValue = v;
                        // All entries following removed node can stay
                        // in list, but all preceding ones need to be
                        // cloned.
                        ++modCount;
                        HashEntry newFirst = e.next;
                        for (HashEntry p = first; p != e; p = p.next)
                            newFirst = new HashEntry(p.key, p.hash,
                                                          newFirst,
p.value);
                        tab[index] = newFirst;
                        count = c; // write-volatile
                    }
                }
                return oldValue;
            } finally {
                unlock();
            }
        } 



From jbaker at zyasoft.com  Mon May 23 20:05:22 2011
From: jbaker at zyasoft.com (Jim Baker)
Date: Mon, 23 May 2011 18:05:22 -0600
Subject: [concurrency-interest] Examples of ConcurrentMap client code
In-Reply-To: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
References: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
Message-ID: <BANLkTintPuZnpbeQJ-5hCvfcy07Gtm-kqg@mail.gmail.com>

Peter,

Take a look at Jython's use of CHM. Jython uses CHM for the following:

   - Implement __dict__ (the underlying attribute namespace in Python for
   all types of objects, including module namespaces) and Python dict/set
   types with volatile semantics, which match the CPython memory model,
   especially as an artifact of the GIL
   - Enable the use of a common Python idiom of iterating over a dict and
   possibly removing keys (there's no equivalent of Iterator#remove in Python)
   - Ensure dict.setdefault is atomic by mapping to putIfAbsent
   - Expose undocumented dict.setifabsent (following advice from Guido van
   Rossum, need to doc at some point other than in the code!); looking at the
   code, I know we planned to also expose to user code replace and remove from
   ConcurrentMap.

The Jython runtime itself tends to use other maps for caches, especially
Google Guava to get at weakref key and/or value.

In the future, using invokedynamic and profiling, we should be able to move
some of the usage of CHM to some sort of storage class. But even then it
will continue to be an important part of Jython's implementation. Therefore
the planned reduction in CHM overhead is great news for Jython.

We would certainly appreciate any feedback you might have on our usage. Hope
this helps!

- Jim

On Mon, May 23, 2011 at 11:22 AM, Peter Hawkins <hawkinsp at cs.stanford.edu>wrote:

> Hi...
>
> I'm a grad student working on techniques for helping programmers work
> with concurrent associative containers, such as ConcurrentHashMap or
> ConcurrentSkipListMap. To help evaluate my research, I'm looking for
> real-world code that uses the Java concurrent map classes.
>
> After looking at a bunch of open source java code, I've come to two
> conclusions:
> a) the predominant use for ConcurrentHashMap is to build concurrent caches.
> b) much code that uses ConcurrentHashMap is racy. Programmers often do
> things like a get followed by a put, erroneously believing that such
> code is atomic, very similar to the way in which people often misuse
> the SynchronizedMap classes.
>
> I'm rather hoping I'm wrong about both points, perhaps because I
> haven't been looking in the right place.
>
> I'd like to find examples of code that contains non-trivial (more than
> just a single ConcurrentMap used by itself as a cache) and, ideally,
> correct uses of concurrent maps. I'm hoping people on this list can
> point me towards some examples, and I'd greatly appreciate any
> pointers you may have.
>
> Thanks,
> Peter
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110523/a57ca150/attachment.html>

From vitalyd at gmail.com  Mon May 23 20:47:26 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 23 May 2011 20:47:26 -0400
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
 volatile fields
In-Reply-To: <BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
References: <00f101cc19a4$b9359810$f8b10893@ktlampc>
	<BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
Message-ID: <BANLkTinyrb2n9GpCswhyc0oPh2yscjDpgg@mail.gmail.com>

+ mailgroup, which I accidentally left out

On Mon, May 23, 2011 at 8:45 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Hi King,
>
> I have to be honest in that I'm not 100% sure of what your exact question
> is as there seem to be a few issues in your scenario.  I'll assume that your
> question is strictly about the effects/visibility of T1's remove followed by
> T2's get, happening strictly one after the other, in global processor order
> so to speak (i.e. T2's read of "count" happens after T1's write to count).
>
> In this case, T2 will observe the modification by T1.  Assuming only T1 and
> T2 are running on separate cores/procs (just to simplify this case), then
> T2's read of count will ensure that it observes all actions performed by T1
> prior to writing to count -- ie. this creates the happens-before edge.  For
> x86(64), Hotspot will emit some mem fencing instruction (lock addl, mfence,
> etc depends on version) before the store to count by T1.  What this actually
> means is that the writing cpu will not continue executing the instruction
> stream until the store buffer has drained to L1, and once it hits L1, the
> standard cache coherency protocol (again, using x86/64 in this example) will
> ensure coherence across cores reading this data.  Therefore, if T2's read
> happens after this serializing action by T1, it'll observe the change in
> state made by T1.
>
> The problem with this scenario is that this is probably not the right way
> to reason about your overall use-case, because you cannot guarantee (absent
> mutual exclusion) that T2 executes its actions strictly after T1 completes.
>  In practice, you'll probably have cases where T2 has already started the
> get() and T1 is somewhere along in its remove().  In that case, CHM only
> guarantees that concurrent reader/writer(s) will not corrupt the invariants
> of the CHM itself -- there's no guarantee whatsoever as to what you can
> observe in this case: T2 may see null or it may see the old (i.e. about to
> be removed) value.  In practice, if you want to avoid the issue of multiple
> threads working on the same piece of data concurrently and you don't want to
> introduce mutual exclusion, you'll want to ensure that a given key X is
> always handled by the same thread (or just 1 thread at any given point in
> time).  One way to do that would be to create a collection of single-thread
> executor services, and then hash your key mod # of executors, and then place
> your workitem on that associated queue.
>
> Let me know if I misunderstood your question :)
>
> Vitaly
>
> On Mon, May 23, 2011 at 7:54 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:
>
>> Dear all,
>>
>> I really have a big trouble or doubt in how ConcurrentHashMap gets some
>> inconsistency scenarios solved. Hope you could help answering. Thank you
>> in
>> advance.
>>
>> The problems seem hard to describe. Please suffer me writing more text to
>> illustrate.
>>
>> As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
>> often used as a concurrent or scalable cache, e.g. for caching HTTP
>> responses (web pages) generated from querying a database. Suppose the
>> application tier has cached an entry (keyX, pageX) in the map. So further
>> requests retrieving pageX can be served by the cache entry, without going
>> thru the database tier. However, the application receives a request to
>> modify the database records related to pageX. So it evicts the cache entry
>> of pageX from the map by calling remove(keyX). In the meanwhile, there can
>> be other incoming retrieval requests that perform get(keyX) concurrently.
>>
>> Suppose all threads draw requests from a single queue to serve.
>>
>> Thread T1 draws an update request, calls remove(keyX) to evict cache, then
>> updates the data for pageX in the database.
>>
>> Thread T2 draws a retrieval request, and calls get(keyX).
>>
>>
>> Suppose T2's action happens after T1's all actions in real time. Then we
>> should expect T2 get null for keyX. This is the desired behavior for
>> avoiding some inconsistency between the cache and database. And T2 will
>> contact the database for up-to-date data and make a new cache entry for
>> pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
>> at get(keyX) and contacts the database, the data update performed by T1
>> may
>> not have started or finished, and needs one more level of synchronization,
>> say wrapping the Java code calling the SQL commands in synchronized.)
>>
>>
>> Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
>> get()?
>>
>> In the CHM code (appended below), I know that there is a volatile write to
>> the count field in remove() and volatile read from count in get(). This
>> sets
>> up a happens-before order on the two operations.
>>
>> But I think all would agree the JMM enforces sequential consistency (SC)
>> for
>> volatile accesses. In SC, a read may not return the last write by another
>> thread in real-time saying. Or you may imagine the JVM is running in a
>> distributed setting, like Terracotta. So T2 may be reading its (stale)
>> value
>> under its cache, or the updates of T1 are still in its store buffer, not
>> yet
>> flushed to main memory. So even if T1's remove() has just done "count = c"
>> (write-volatile), T2 may not yet see it at checking "count != 0"
>> (read-volatile), and can return the old hash entry of pageX in the map,
>> that
>> was cached in processor cache (If T2 does not see the new count, it must
>> also not see the delete of pageX). May I ask if this is one acceptable
>> behavior of CHM?
>>
>>
>> Best regards,
>> King Tin
>>
>>
>> Append some code of CHM for easier discussion:
>> ---------------------------------------------
>>
>>        Object get(Object key, int hash) {
>>            if (count != 0) { // read-volatile
>>                HashEntry e = getFirst(hash);
>>                while (e != null) {
>>                    if (e.hash == hash && key.equals(e.key)) {
>>                        Object v = e.value;
>>                        if (v != null)
>>                            return v;
>>                        return readValueUnderLock(e); // recheck
>>                    }
>>                    e = e.next;
>>                }
>>            }
>>            return null;
>>        }
>>
>>        Object remove(Object key, int hash, Object value) {
>>            lock();
>>            try {
>>                int c = count - 1;
>>                HashEntry[] tab = table;
>>                int index = hash & (tab.length - 1);
>>                HashEntry first = tab[index];
>>                HashEntry e = first;
>>                while (e != null && (e.hash != hash || !key.equals(e.key)))
>>                    e = e.next;
>>
>>                Object oldValue = null;
>>                if (e != null) {
>>                    Object v = e.value;
>>                    if (value == null || value.equals(v)) {
>>                        oldValue = v;
>>                        // All entries following removed node can stay
>>                        // in list, but all preceding ones need to be
>>                        // cloned.
>>                        ++modCount;
>>                        HashEntry newFirst = e.next;
>>                        for (HashEntry p = first; p != e; p = p.next)
>>                            newFirst = new HashEntry(p.key, p.hash,
>>                                                          newFirst,
>> p.value);
>>                        tab[index] = newFirst;
>>                        count = c; // write-volatile
>>                    }
>>                }
>>                return oldValue;
>>            } finally {
>>                unlock();
>>            }
>>        }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110523/a37e43b7/attachment-0001.html>

From ktlam at cs.hku.hk  Mon May 23 21:55:14 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Tue, 24 May 2011 09:55:14 +0800
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
	volatile fields
In-Reply-To: <BANLkTinyrb2n9GpCswhyc0oPh2yscjDpgg@mail.gmail.com>
References: <00f101cc19a4$b9359810$f8b10893@ktlampc><BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
	<BANLkTinyrb2n9GpCswhyc0oPh2yscjDpgg@mail.gmail.com>
Message-ID: <010101cc19b5$a3329d40$f8b10893@ktlampc>

Dear Vitaly,

 

You have understood my question(s) although I really expressed quite poorly.


Thank you so much for your detailed explanation. And I have some follow-up
comments/questions.

 

In response to "The problem with this scenario is that this is probably not
the right way to reason about your overall use-case, because you cannot
guarantee (absent mutual exclusion) that T2 executes its actions strictly
after T1 completes.  In practice, you'll probably have cases where T2 has
already started the get() and T1 is somewhere along in its remove().", I
would see in this way. Since the remove operation is not completely finished
(somewhere before "count = c; // write-volatile"), T2 (somewhere after "if
(count != 0) // read-volatile") still getting the old hash entry is
acceptable and makes sense. But if T2 starts get() after T1's remove()
ended, but still reporting the old entry, then this is what we mind as
"something wrong". Right? 

 

About your 2nd paragraph, so do you mean by the read/write barriers around
the volatile accesses plus the cache coherence of the multiprocessor
hardware, if T1 does finish the volatile write, any thread which performs a
volatile read on that field will always get it along with the updates
happened before it?

 

Best regards,

King Tin

 

  _____  

From: Vitaly Davidovich [mailto:vitalyd at gmail.com] 
Sent: Tuesday, May 24, 2011 8:47 AM
To: King Tin Lam; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ConcurrentHashMap and JMM semantics of
volatile fields

 

+ mailgroup, which I accidentally left out

On Mon, May 23, 2011 at 8:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

Hi King,

 

I have to be honest in that I'm not 100% sure of what your exact question is
as there seem to be a few issues in your scenario.  I'll assume that your
question is strictly about the effects/visibility of T1's remove followed by
T2's get, happening strictly one after the other, in global processor order
so to speak (i.e. T2's read of "count" happens after T1's write to count).

 

In this case, T2 will observe the modification by T1.  Assuming only T1 and
T2 are running on separate cores/procs (just to simplify this case), then
T2's read of count will ensure that it observes all actions performed by T1
prior to writing to count -- ie. this creates the happens-before edge.  For
x86(64), Hotspot will emit some mem fencing instruction (lock addl, mfence,
etc depends on version) before the store to count by T1.  What this actually
means is that the writing cpu will not continue executing the instruction
stream until the store buffer has drained to L1, and once it hits L1, the
standard cache coherency protocol (again, using x86/64 in this example) will
ensure coherence across cores reading this data.  Therefore, if T2's read
happens after this serializing action by T1, it'll observe the change in
state made by T1.

 

The problem with this scenario is that this is probably not the right way to
reason about your overall use-case, because you cannot guarantee (absent
mutual exclusion) that T2 executes its actions strictly after T1 completes.
In practice, you'll probably have cases where T2 has already started the
get() and T1 is somewhere along in its remove().  In that case, CHM only
guarantees that concurrent reader/writer(s) will not corrupt the invariants
of the CHM itself -- there's no guarantee whatsoever as to what you can
observe in this case: T2 may see null or it may see the old (i.e. about to
be removed) value.  In practice, if you want to avoid the issue of multiple
threads working on the same piece of data concurrently and you don't want to
introduce mutual exclusion, you'll want to ensure that a given key X is
always handled by the same thread (or just 1 thread at any given point in
time).  One way to do that would be to create a collection of single-thread
executor services, and then hash your key mod # of executors, and then place
your workitem on that associated queue.

 

Let me know if I misunderstood your question :)

 

Vitaly

 

On Mon, May 23, 2011 at 7:54 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:

Dear all,

I really have a big trouble or doubt in how ConcurrentHashMap gets some
inconsistency scenarios solved. Hope you could help answering. Thank you in
advance.

The problems seem hard to describe. Please suffer me writing more text to
illustrate.

As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
often used as a concurrent or scalable cache, e.g. for caching HTTP
responses (web pages) generated from querying a database. Suppose the
application tier has cached an entry (keyX, pageX) in the map. So further
requests retrieving pageX can be served by the cache entry, without going
thru the database tier. However, the application receives a request to
modify the database records related to pageX. So it evicts the cache entry
of pageX from the map by calling remove(keyX). In the meanwhile, there can
be other incoming retrieval requests that perform get(keyX) concurrently.

Suppose all threads draw requests from a single queue to serve.

Thread T1 draws an update request, calls remove(keyX) to evict cache, then
updates the data for pageX in the database.

Thread T2 draws a retrieval request, and calls get(keyX).


Suppose T2's action happens after T1's all actions in real time. Then we
should expect T2 get null for keyX. This is the desired behavior for
avoiding some inconsistency between the cache and database. And T2 will
contact the database for up-to-date data and make a new cache entry for
pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
at get(keyX) and contacts the database, the data update performed by T1 may
not have started or finished, and needs one more level of synchronization,
say wrapping the Java code calling the SQL commands in synchronized.)


Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
get()?

In the CHM code (appended below), I know that there is a volatile write to
the count field in remove() and volatile read from count in get(). This sets
up a happens-before order on the two operations.

But I think all would agree the JMM enforces sequential consistency (SC) for
volatile accesses. In SC, a read may not return the last write by another
thread in real-time saying. Or you may imagine the JVM is running in a
distributed setting, like Terracotta. So T2 may be reading its (stale) value
under its cache, or the updates of T1 are still in its store buffer, not yet
flushed to main memory. So even if T1's remove() has just done "count = c"
(write-volatile), T2 may not yet see it at checking "count != 0"
(read-volatile), and can return the old hash entry of pageX in the map, that
was cached in processor cache (If T2 does not see the new count, it must
also not see the delete of pageX). May I ask if this is one acceptable
behavior of CHM?


Best regards,
King Tin


Append some code of CHM for easier discussion:
---------------------------------------------

       Object get(Object key, int hash) {
           if (count != 0) { // read-volatile
               HashEntry e = getFirst(hash);
               while (e != null) {
                   if (e.hash == hash && key.equals(e.key)) {
                       Object v = e.value;
                       if (v != null)
                           return v;
                       return readValueUnderLock(e); // recheck
                   }
                   e = e.next;
               }
           }
           return null;
       }

       Object remove(Object key, int hash, Object value) {
           lock();
           try {
               int c = count - 1;
               HashEntry[] tab = table;
               int index = hash & (tab.length - 1);
               HashEntry first = tab[index];
               HashEntry e = first;
               while (e != null && (e.hash != hash || !key.equals(e.key)))
                   e = e.next;

               Object oldValue = null;
               if (e != null) {
                   Object v = e.value;
                   if (value == null || value.equals(v)) {
                       oldValue = v;
                       // All entries following removed node can stay
                       // in list, but all preceding ones need to be
                       // cloned.
                       ++modCount;
                       HashEntry newFirst = e.next;
                       for (HashEntry p = first; p != e; p = p.next)
                           newFirst = new HashEntry(p.key, p.hash,
                                                         newFirst,
p.value);
                       tab[index] = newFirst;
                       count = c; // write-volatile
                   }
               }
               return oldValue;
           } finally {
               unlock();
           }
       }


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





-- 
Vitaly
617-548-7007 (mobile)




-- 
Vitaly
617-548-7007 (mobile)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110524/331acb39/attachment-0001.html>

From vitalyd at gmail.com  Mon May 23 22:18:15 2011
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 23 May 2011 22:18:15 -0400
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
 volatile fields
In-Reply-To: <BANLkTim1ANOUJ7BMyYGj+gmSGbOO9CU=zg@mail.gmail.com>
References: <00f101cc19a4$b9359810$f8b10893@ktlampc>
	<BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
	<BANLkTinyrb2n9GpCswhyc0oPh2yscjDpgg@mail.gmail.com>
	<010101cc19b5$a3329d40$f8b10893@ktlampc>
	<BANLkTim1ANOUJ7BMyYGj+gmSGbOO9CU=zg@mail.gmail.com>
Message-ID: <BANLkTi=KQ1YGda+4zWJvOW5A+H3OmS6h0Q@mail.gmail.com>

On Mon, May 23, 2011 at 10:17 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> No problem.
>
> "I would see in this way. Since the remove operation is not completely
> finished (somewhere before ?count = c; // write-volatile?), T2 (somewhere
> after ?if (count != 0) // read-volatile?) still getting the old hash entry
> is acceptable and makes sense. But if T2 starts get() after T1?s remove()
> ended, but still reporting the old entry, then this is what we mind as
> ?something wrong?. Right?"
>
> If T2 starts after T1 (i.e. T1's stores have drained to its L1 before T2
> begins get()), then you should see the effects of T1's remove() -- this is
> guaranteed (absent any JVM bugs) by the happens-before edge that's created
> by the volatile write to "count" and the subsequent volatile read of it.
>  I'm again simplifying the example to just 2 threads (T1 and T2) running at
> the same time that are operating on the CHM.
>
> "About your 2nd paragraph, so do you mean by the read/write barriers
> around the volatile accesses plus the cache coherence of the multiprocessor
> hardware, if T1 does finish the volatile write, any thread which performs a
> volatile read on that field will always get it along with the updates
> happened before it?"
>
> Yes.  On x86(64), there's no global order of memory visibility -- only per
> cpu order.  In other words, if cpu1 writes A, B, C (in that order) to memory
> and cpu2 writes D, E, F (in that order), then for cpu3 all that's guaranteed
> is that it will observe A before B before C, and D before E before F -- it
> does not guarantee anything about the "global" order of stores between cpu1
> and cpu2 (e.g. cpu3 may see A, D, E, B, C, F, in that order).  So your "any
> thread which performs volatile read" really means that it's observing the
> aforementioned order, so if there are 2 mutating cpus, then the actual
> "global" order is undefined except that each processor's order is enforced.
>  Hope that makes sense ...
>
> Vitaly
>
> On Mon, May 23, 2011 at 9:55 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:
>
>>  Dear Vitaly,
>>
>>
>>
>> You have understood my question(s) although I really expressed quite
>> poorly.
>>
>> Thank you so much for your detailed explanation. And I have some follow-up
>> comments/questions.
>>
>>
>>
>> In response to ?The problem with this scenario is that this is probably
>> not the right way to reason about your overall use-case, because you cannot
>> guarantee (absent mutual exclusion) that T2 executes its actions strictly
>> after T1 completes.  In practice, you'll probably have cases where T2 has
>> already started the get() and T1 is somewhere along in its remove().?, I
>> would see in this way. Since the remove operation is not completely finished
>> (somewhere before ?count = c; // write-volatile?), T2 (somewhere after ?if
>> (count != 0) // read-volatile?) still getting the old hash entry is
>> acceptable and makes sense. But if T2 starts get() after T1?s remove()
>> ended, but still reporting the old entry, then this is what we mind as
>> ?something wrong?. Right?
>>
>>
>>
>> About your 2nd paragraph, so do you mean by the read/write barriers
>> around the volatile accesses plus the cache coherence of the multiprocessor
>> hardware, if T1 does finish the volatile write, any thread which performs a
>> volatile read on that field will always get it along with the updates
>> happened before it?
>>
>>
>>
>> Best regards,
>>
>> King Tin
>>
>>
>>   ------------------------------
>>
>> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
>> *Sent:* Tuesday, May 24, 2011 8:47 AM
>> *To:* King Tin Lam; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] ConcurrentHashMap and JMM semantics
>> of volatile fields
>>
>>
>>
>> + mailgroup, which I accidentally left out
>>
>> On Mon, May 23, 2011 at 8:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>> Hi King,
>>
>>
>>
>> I have to be honest in that I'm not 100% sure of what your exact question
>> is as there seem to be a few issues in your scenario.  I'll assume that your
>> question is strictly about the effects/visibility of T1's remove followed by
>> T2's get, happening strictly one after the other, in global processor order
>> so to speak (i.e. T2's read of "count" happens after T1's write to count).
>>
>>
>>
>> In this case, T2 will observe the modification by T1.  Assuming only T1
>> and T2 are running on separate cores/procs (just to simplify this case),
>> then T2's read of count will ensure that it observes all actions performed
>> by T1 prior to writing to count -- ie. this creates the happens-before edge.
>>  For x86(64), Hotspot will emit some mem fencing instruction (lock addl,
>> mfence, etc depends on version) before the store to count by T1.  What this
>> actually means is that the writing cpu will not continue executing the
>> instruction stream until the store buffer has drained to L1, and once it
>> hits L1, the standard cache coherency protocol (again, using x86/64 in this
>> example) will ensure coherence across cores reading this data.  Therefore,
>> if T2's read happens after this serializing action by T1, it'll observe the
>> change in state made by T1.
>>
>>
>>
>> The problem with this scenario is that this is probably not the right way
>> to reason about your overall use-case, because you cannot guarantee (absent
>> mutual exclusion) that T2 executes its actions strictly after T1 completes.
>>  In practice, you'll probably have cases where T2 has already started the
>> get() and T1 is somewhere along in its remove().  In that case, CHM only
>> guarantees that concurrent reader/writer(s) will not corrupt the invariants
>> of the CHM itself -- there's no guarantee whatsoever as to what you can
>> observe in this case: T2 may see null or it may see the old (i.e. about to
>> be removed) value.  In practice, if you want to avoid the issue of multiple
>> threads working on the same piece of data concurrently and you don't want to
>> introduce mutual exclusion, you'll want to ensure that a given key X is
>> always handled by the same thread (or just 1 thread at any given point in
>> time).  One way to do that would be to create a collection of single-thread
>> executor services, and then hash your key mod # of executors, and then place
>> your workitem on that associated queue.
>>
>>
>>
>> Let me know if I misunderstood your question :)
>>
>>
>>
>> Vitaly
>>
>>
>>
>> On Mon, May 23, 2011 at 7:54 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:
>>
>> Dear all,
>>
>> I really have a big trouble or doubt in how ConcurrentHashMap gets some
>> inconsistency scenarios solved. Hope you could help answering. Thank you
>> in
>> advance.
>>
>> The problems seem hard to describe. Please suffer me writing more text to
>> illustrate.
>>
>> As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
>> often used as a concurrent or scalable cache, e.g. for caching HTTP
>> responses (web pages) generated from querying a database. Suppose the
>> application tier has cached an entry (keyX, pageX) in the map. So further
>> requests retrieving pageX can be served by the cache entry, without going
>> thru the database tier. However, the application receives a request to
>> modify the database records related to pageX. So it evicts the cache entry
>> of pageX from the map by calling remove(keyX). In the meanwhile, there can
>> be other incoming retrieval requests that perform get(keyX) concurrently.
>>
>> Suppose all threads draw requests from a single queue to serve.
>>
>> Thread T1 draws an update request, calls remove(keyX) to evict cache, then
>> updates the data for pageX in the database.
>>
>> Thread T2 draws a retrieval request, and calls get(keyX).
>>
>>
>> Suppose T2's action happens after T1's all actions in real time. Then we
>> should expect T2 get null for keyX. This is the desired behavior for
>> avoiding some inconsistency between the cache and database. And T2 will
>> contact the database for up-to-date data and make a new cache entry for
>> pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
>> at get(keyX) and contacts the database, the data update performed by T1
>> may
>> not have started or finished, and needs one more level of synchronization,
>> say wrapping the Java code calling the SQL commands in synchronized.)
>>
>>
>> Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
>> get()?
>>
>> In the CHM code (appended below), I know that there is a volatile write to
>> the count field in remove() and volatile read from count in get(). This
>> sets
>> up a happens-before order on the two operations.
>>
>> But I think all would agree the JMM enforces sequential consistency (SC)
>> for
>> volatile accesses. In SC, a read may not return the last write by another
>> thread in real-time saying. Or you may imagine the JVM is running in a
>> distributed setting, like Terracotta. So T2 may be reading its (stale)
>> value
>> under its cache, or the updates of T1 are still in its store buffer, not
>> yet
>> flushed to main memory. So even if T1's remove() has just done "count = c"
>> (write-volatile), T2 may not yet see it at checking "count != 0"
>> (read-volatile), and can return the old hash entry of pageX in the map,
>> that
>> was cached in processor cache (If T2 does not see the new count, it must
>> also not see the delete of pageX). May I ask if this is one acceptable
>> behavior of CHM?
>>
>>
>> Best regards,
>> King Tin
>>
>>
>> Append some code of CHM for easier discussion:
>> ---------------------------------------------
>>
>>        Object get(Object key, int hash) {
>>            if (count != 0) { // read-volatile
>>                HashEntry e = getFirst(hash);
>>                while (e != null) {
>>                    if (e.hash == hash && key.equals(e.key)) {
>>                        Object v = e.value;
>>                        if (v != null)
>>                            return v;
>>                        return readValueUnderLock(e); // recheck
>>                    }
>>                    e = e.next;
>>                }
>>            }
>>            return null;
>>        }
>>
>>        Object remove(Object key, int hash, Object value) {
>>            lock();
>>            try {
>>                int c = count - 1;
>>                HashEntry[] tab = table;
>>                int index = hash & (tab.length - 1);
>>                HashEntry first = tab[index];
>>                HashEntry e = first;
>>                while (e != null && (e.hash != hash || !key.equals(e.key)))
>>                    e = e.next;
>>
>>                Object oldValue = null;
>>                if (e != null) {
>>                    Object v = e.value;
>>                    if (value == null || value.equals(v)) {
>>                        oldValue = v;
>>                        // All entries following removed node can stay
>>                        // in list, but all preceding ones need to be
>>                        // cloned.
>>                        ++modCount;
>>                        HashEntry newFirst = e.next;
>>                        for (HashEntry p = first; p != e; p = p.next)
>>                            newFirst = new HashEntry(p.key, p.hash,
>>                                                          newFirst,
>> p.value);
>>                        tab[index] = newFirst;
>>                        count = c; // write-volatile
>>                    }
>>                }
>>                return oldValue;
>>            } finally {
>>                unlock();
>>            }
>>        }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>   --
>> Vitaly
>> 617-548-7007 (mobile)
>>
>>
>>
>>
>> --
>> Vitaly
>> 617-548-7007 (mobile)
>>
>
>
>
> --
> Vitaly
> 617-548-7007 (mobile)
>



-- 
Vitaly
617-548-7007 (mobile)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110523/b1daa743/attachment-0001.html>

From ktlam at cs.hku.hk  Mon May 23 23:48:04 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Tue, 24 May 2011 11:48:04 +0800
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
	volatile fields
In-Reply-To: <BANLkTi=KQ1YGda+4zWJvOW5A+H3OmS6h0Q@mail.gmail.com>
References: <00f101cc19a4$b9359810$f8b10893@ktlampc><BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com><BANLkTinyrb2n9GpCswhyc0oPh2yscjDpgg@mail.gmail.com><010101cc19b5$a3329d40$f8b10893@ktlampc><BANLkTim1ANOUJ7BMyYGj+gmSGbOO9CU=zg@mail.gmail.com>
	<BANLkTi=KQ1YGda+4zWJvOW5A+H3OmS6h0Q@mail.gmail.com>
Message-ID: <011501cc19c5$650b68c0$f8b10893@ktlampc>

> If T2 starts after T1 (i.e. T1's stores have drained to its L1 before T2
begins get()), then you should see the effects of T1's remove() -- this is
guaranteed (absent any JVM bugs) by the happens-before edge that's created
by the volatile write to "count" and the subsequent volatile read of it.

 

Thanks a lot. But as you also mentioned in previous post, in reality, since
we don't have ordering of which of the threads go first, so it can happen
that get() returns old entry while remove() is happening concurrently, e.g.
T2 issued the read-volatile earlier than T1's volatile-write; such
read-volatile operation will never wait until T1's write-volatile finishes
because T2 even does not know T1 is going to perform the write. Am I right?

 

If you suffer me asking more, please read further below.

 

Indeed, I am a researcher on distributed or clustered JVM, something similar
to Terracotta. So my concern is more on a correct distributed JMM
implementation. 

 

I always heard about JMM enforces sequential consistency (SC) on volatile
access. Well, this is a model. But eventually, in the implementation (or
protocol) on a shared-memory system, by means of the added read/write
barriers, I feel the actual outcome has become able to guarantee strict (or
atomic) consistency -- a read operation can always return the result of the
latest write operation which occurred on that data item. Well, T2's volatile
read always gets T1's volatile write immediately (as T1's stores have
drained to its L1) if T2 really issues the bus transaction later than T1.
This is stricter than necessary (as the model does not require), but it
turns no other implementation choices on a shared-memory system. Not sure if
you agree my view?

 

But in a distributed setting, we won't have read/write barrier instructions
and shared memory, all to do is to send messages for synchronization ---
i.e. software DSM, implementing SC or (Lazy) Release Consistency (RC). Since
network communication is costly operation, we can't afford a protocol that
allows T2's volatile read gets T1's volatile write as immediately as in a
shared-memory; this can only be achieved if we force volatile-read to send
message to last volatile writer and waits for its reply. Indeed, since Java
volatile access follows SC, in a model viewpoint, there is no such strict
need, we can do it in a relaxed way --- just let T2 keeps reading its own
cached entries until arrival of updates from a remote T1 (as long as the
updates respect program order of T1). Again, since we don't have ordering of
which of the threads go first, and we don't even have a global clock in such
environment, it is normal and acceptable for a thread's get() returns old
entry even there is some thread finishes a remove() concurrently. Am I
right?

 

This is really really my last question: in the new JMM, volatile access
semantics get changed/fixed---"To be somewhat more precise, a volatile read
has acquire semantics and a volatile write has release semantics." (quote
from David Dice's Weblog
http://blogs.oracle.com/dave/entry/java_memory_model_concerns_on). But I get
a bit confused. What acquire (lock) and release (unlock) enforce is RC, not
SC. Does this imply the new JMM has changed to RC instead of SC for volatile
access? I guess not. Well, they are different. In RC, when you acquire, you
will wait for the last acquirer to release the lock and getting all updates
happened before from it. But volatile read won't wait, I supposed, so when
the read sees no cache invalidations coming at the current time point, it
will simply proceed with its own cached values. But once it sees
invalidation/update due to the volatile field that it is now reading, it
will also see the changes happened before the volatile write. And this
guarantee (or behavior) is common to both SC and RC.

 

Thank you again for your patience.

I really appreciated your helpful answers.

 

Best regards,

King Tin

 

  _____  

From: Vitaly Davidovich [mailto:vitalyd at gmail.com] 
Sent: Tuesday, May 24, 2011 10:18 AM
To: King Tin Lam; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ConcurrentHashMap and JMM semantics of
volatile fields

 

 

On Mon, May 23, 2011 at 10:17 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

No problem.

 

"I would see in this way. Since the remove operation is not completely
finished (somewhere before "count = c; // write-volatile"), T2 (somewhere
after "if (count != 0) // read-volatile") still getting the old hash entry
is acceptable and makes sense. But if T2 starts get() after T1's remove()
ended, but still reporting the old entry, then this is what we mind as
"something wrong". Right?"

If T2 starts after T1 (i.e. T1's stores have drained to its L1 before T2
begins get()), then you should see the effects of T1's remove() -- this is
guaranteed (absent any JVM bugs) by the happens-before edge that's created
by the volatile write to "count" and the subsequent volatile read of it.
I'm again simplifying the example to just 2 threads (T1 and T2) running at
the same time that are operating on the CHM.

 

"About your 2nd paragraph, so do you mean by the read/write barriers around
the volatile accesses plus the cache coherence of the multiprocessor
hardware, if T1 does finish the volatile write, any thread which performs a
volatile read on that field will always get it along with the updates
happened before it?"

 

Yes.  On x86(64), there's no global order of memory visibility -- only per
cpu order.  In other words, if cpu1 writes A, B, C (in that order) to memory
and cpu2 writes D, E, F (in that order), then for cpu3 all that's guaranteed
is that it will observe A before B before C, and D before E before F -- it
does not guarantee anything about the "global" order of stores between cpu1
and cpu2 (e.g. cpu3 may see A, D, E, B, C, F, in that order).  So your "any
thread which performs volatile read" really means that it's observing the
aforementioned order, so if there are 2 mutating cpus, then the actual
"global" order is undefined except that each processor's order is enforced.
Hope that makes sense ...

 

Vitaly

 

On Mon, May 23, 2011 at 9:55 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:

Dear Vitaly,

 

You have understood my question(s) although I really expressed quite poorly.


Thank you so much for your detailed explanation. And I have some follow-up
comments/questions.

 

In response to "The problem with this scenario is that this is probably not
the right way to reason about your overall use-case, because you cannot
guarantee (absent mutual exclusion) that T2 executes its actions strictly
after T1 completes.  In practice, you'll probably have cases where T2 has
already started the get() and T1 is somewhere along in its remove().", I
would see in this way. Since the remove operation is not completely finished
(somewhere before "count = c; // write-volatile"), T2 (somewhere after "if
(count != 0) // read-volatile") still getting the old hash entry is
acceptable and makes sense. But if T2 starts get() after T1's remove()
ended, but still reporting the old entry, then this is what we mind as
"something wrong". Right? 

 

About your 2nd paragraph, so do you mean by the read/write barriers around
the volatile accesses plus the cache coherence of the multiprocessor
hardware, if T1 does finish the volatile write, any thread which performs a
volatile read on that field will always get it along with the updates
happened before it?

 

Best regards,

King Tin

 

  _____  

From: Vitaly Davidovich [mailto:vitalyd at gmail.com] 
Sent: Tuesday, May 24, 2011 8:47 AM
To: King Tin Lam; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] ConcurrentHashMap and JMM semantics of
volatile fields

 

+ mailgroup, which I accidentally left out

On Mon, May 23, 2011 at 8:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

Hi King,

 

I have to be honest in that I'm not 100% sure of what your exact question is
as there seem to be a few issues in your scenario.  I'll assume that your
question is strictly about the effects/visibility of T1's remove followed by
T2's get, happening strictly one after the other, in global processor order
so to speak (i.e. T2's read of "count" happens after T1's write to count).

 

In this case, T2 will observe the modification by T1.  Assuming only T1 and
T2 are running on separate cores/procs (just to simplify this case), then
T2's read of count will ensure that it observes all actions performed by T1
prior to writing to count -- ie. this creates the happens-before edge.  For
x86(64), Hotspot will emit some mem fencing instruction (lock addl, mfence,
etc depends on version) before the store to count by T1.  What this actually
means is that the writing cpu will not continue executing the instruction
stream until the store buffer has drained to L1, and once it hits L1, the
standard cache coherency protocol (again, using x86/64 in this example) will
ensure coherence across cores reading this data.  Therefore, if T2's read
happens after this serializing action by T1, it'll observe the change in
state made by T1.

 

The problem with this scenario is that this is probably not the right way to
reason about your overall use-case, because you cannot guarantee (absent
mutual exclusion) that T2 executes its actions strictly after T1 completes.
In practice, you'll probably have cases where T2 has already started the
get() and T1 is somewhere along in its remove().  In that case, CHM only
guarantees that concurrent reader/writer(s) will not corrupt the invariants
of the CHM itself -- there's no guarantee whatsoever as to what you can
observe in this case: T2 may see null or it may see the old (i.e. about to
be removed) value.  In practice, if you want to avoid the issue of multiple
threads working on the same piece of data concurrently and you don't want to
introduce mutual exclusion, you'll want to ensure that a given key X is
always handled by the same thread (or just 1 thread at any given point in
time).  One way to do that would be to create a collection of single-thread
executor services, and then hash your key mod # of executors, and then place
your workitem on that associated queue.

 

Let me know if I misunderstood your question :)

 

Vitaly

 

On Mon, May 23, 2011 at 7:54 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:

Dear all,

I really have a big trouble or doubt in how ConcurrentHashMap gets some
inconsistency scenarios solved. Hope you could help answering. Thank you in
advance.

The problems seem hard to describe. Please suffer me writing more text to
illustrate.

As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
often used as a concurrent or scalable cache, e.g. for caching HTTP
responses (web pages) generated from querying a database. Suppose the
application tier has cached an entry (keyX, pageX) in the map. So further
requests retrieving pageX can be served by the cache entry, without going
thru the database tier. However, the application receives a request to
modify the database records related to pageX. So it evicts the cache entry
of pageX from the map by calling remove(keyX). In the meanwhile, there can
be other incoming retrieval requests that perform get(keyX) concurrently.

Suppose all threads draw requests from a single queue to serve.

Thread T1 draws an update request, calls remove(keyX) to evict cache, then
updates the data for pageX in the database.

Thread T2 draws a retrieval request, and calls get(keyX).


Suppose T2's action happens after T1's all actions in real time. Then we
should expect T2 get null for keyX. This is the desired behavior for
avoiding some inconsistency between the cache and database. And T2 will
contact the database for up-to-date data and make a new cache entry for
pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
at get(keyX) and contacts the database, the data update performed by T1 may
not have started or finished, and needs one more level of synchronization,
say wrapping the Java code calling the SQL commands in synchronized.)


Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
get()?

In the CHM code (appended below), I know that there is a volatile write to
the count field in remove() and volatile read from count in get(). This sets
up a happens-before order on the two operations.

But I think all would agree the JMM enforces sequential consistency (SC) for
volatile accesses. In SC, a read may not return the last write by another
thread in real-time saying. Or you may imagine the JVM is running in a
distributed setting, like Terracotta. So T2 may be reading its (stale) value
under its cache, or the updates of T1 are still in its store buffer, not yet
flushed to main memory. So even if T1's remove() has just done "count = c"
(write-volatile), T2 may not yet see it at checking "count != 0"
(read-volatile), and can return the old hash entry of pageX in the map, that
was cached in processor cache (If T2 does not see the new count, it must
also not see the delete of pageX). May I ask if this is one acceptable
behavior of CHM?


Best regards,
King Tin


Append some code of CHM for easier discussion:
---------------------------------------------

       Object get(Object key, int hash) {
           if (count != 0) { // read-volatile
               HashEntry e = getFirst(hash);
               while (e != null) {
                   if (e.hash == hash && key.equals(e.key)) {
                       Object v = e.value;
                       if (v != null)
                           return v;
                       return readValueUnderLock(e); // recheck
                   }
                   e = e.next;
               }
           }
           return null;
       }

       Object remove(Object key, int hash, Object value) {
           lock();
           try {
               int c = count - 1;
               HashEntry[] tab = table;
               int index = hash & (tab.length - 1);
               HashEntry first = tab[index];
               HashEntry e = first;
               while (e != null && (e.hash != hash || !key.equals(e.key)))
                   e = e.next;

               Object oldValue = null;
               if (e != null) {
                   Object v = e.value;
                   if (value == null || value.equals(v)) {
                       oldValue = v;
                       // All entries following removed node can stay
                       // in list, but all preceding ones need to be
                       // cloned.
                       ++modCount;
                       HashEntry newFirst = e.next;
                       for (HashEntry p = first; p != e; p = p.next)
                           newFirst = new HashEntry(p.key, p.hash,
                                                         newFirst,
p.value);
                       tab[index] = newFirst;
                       count = c; // write-volatile
                   }
               }
               return oldValue;
           } finally {
               unlock();
           }
       }


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





-- 
Vitaly
617-548-7007 (mobile)




-- 
Vitaly
617-548-7007 (mobile)




-- 
Vitaly
617-548-7007 (mobile)




-- 
Vitaly
617-548-7007 (mobile)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110524/3d938b10/attachment-0001.html>

From ktlam at cs.hku.hk  Tue May 24 00:20:50 2011
From: ktlam at cs.hku.hk (King Tin Lam)
Date: Tue, 24 May 2011 12:20:50 +0800
Subject: [concurrency-interest] ConcurrentHashMap and JMM semantics of
	volatile fields
In-Reply-To: <BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
References: <00f101cc19a4$b9359810$f8b10893@ktlampc>
	<BANLkTim7B+v7mU0sZ+hsbStSC9P_h_x6TA@mail.gmail.com>
Message-ID: <012001cc19c9$f9f16710$f8b10893@ktlampc>

Hi, Vitaly,

 

Btw, in response to your last paragraph, thank you for your suggested way of
avoiding mutual exclusion. 

But this requires the request dispatcher to have some application-level
knowledge. In my scenario, the load dispatcher is situated at the front end
Apache Web server which assigns requests to Tomcat instances running on
different nodes, then it will not know what the keys are to get, unless we
modify its load balancer, say mod_jk.

 

By writing the following pseudo-code (suppose to be called by a Java
servlet), I express my ways of handling. Appreciate your views / advices if
any.

 

Well, if a write request coming (to modify data k in DB), it will
synchronize on lock L chosen by some hash function.

In read-request, if it get v equal to null, this implies the entry never
exists or deleted, it has to create a new record by calling getFromDB(k).

This method synchronizes on L with write-request, so it won't read until the
writer finishes, and if the reader is reading, the writer won't update the
DB.

There seems possibility that read-request could still return a v of old
image while the data in DB has been updated, as our previous discussion
reveals.

But since it's not a time-critical application like stock price updates, we
could accept that at some point after a completed write-request(k), future
read-requests will return the update.

 

 

V read-request(String k) {

   V v = map.get(k)

   if (v == null) {

      V v_new = getFromDB(k)

      v = map.putIfAbsent(k, v_new)

      if (v == null) // put succeeded

         v = v_new

   }

   return v

}

 

V getFromDB(String k) {

   int i = someHash(k)  // a fine-grained locking scheme

   L = tableOfLocks[i]

   synchronized(L){

      executeQuery("SELECT ... FROM ... WHERE key = " + k)

      generate v by looping the recordset returned

   }

   return v

}

 

void write-request(String k){

   int i = someHash(k)  // a fine-grained locking scheme

   L = tableOfLocks[i]

   synchronized(L){

      map.remove(k)  // evict cache

      executeUpdate("UPDATE ... SET ... WHERE key = " + k)

   }

}

 

Best regards,

King Tin

 

  _____  

From: Vitaly Davidovich [mailto:vitalyd at gmail.com] 
Sent: Tuesday, May 24, 2011 8:46 AM
To: King Tin Lam
Subject: Re: [concurrency-interest] ConcurrentHashMap and JMM semantics of
volatile fields

 

Hi King,

 

I have to be honest in that I'm not 100% sure of what your exact question is
as there seem to be a few issues in your scenario.  I'll assume that your
question is strictly about the effects/visibility of T1's remove followed by
T2's get, happening strictly one after the other, in global processor order
so to speak (i.e. T2's read of "count" happens after T1's write to count).

 

In this case, T2 will observe the modification by T1.  Assuming only T1 and
T2 are running on separate cores/procs (just to simplify this case), then
T2's read of count will ensure that it observes all actions performed by T1
prior to writing to count -- ie. this creates the happens-before edge.  For
x86(64), Hotspot will emit some mem fencing instruction (lock addl, mfence,
etc depends on version) before the store to count by T1.  What this actually
means is that the writing cpu will not continue executing the instruction
stream until the store buffer has drained to L1, and once it hits L1, the
standard cache coherency protocol (again, using x86/64 in this example) will
ensure coherence across cores reading this data.  Therefore, if T2's read
happens after this serializing action by T1, it'll observe the change in
state made by T1.

 

The problem with this scenario is that this is probably not the right way to
reason about your overall use-case, because you cannot guarantee (absent
mutual exclusion) that T2 executes its actions strictly after T1 completes.
In practice, you'll probably have cases where T2 has already started the
get() and T1 is somewhere along in its remove().  In that case, CHM only
guarantees that concurrent reader/writer(s) will not corrupt the invariants
of the CHM itself -- there's no guarantee whatsoever as to what you can
observe in this case: T2 may see null or it may see the old (i.e. about to
be removed) value.  In practice, if you want to avoid the issue of multiple
threads working on the same piece of data concurrently and you don't want to
introduce mutual exclusion, you'll want to ensure that a given key X is
always handled by the same thread (or just 1 thread at any given point in
time).  One way to do that would be to create a collection of single-thread
executor services, and then hash your key mod # of executors, and then place
your workitem on that associated queue.

 

Let me know if I misunderstood your question :)

 

Vitaly

 

On Mon, May 23, 2011 at 7:54 PM, King Tin Lam <ktlam at cs.hku.hk> wrote:

Dear all,

I really have a big trouble or doubt in how ConcurrentHashMap gets some
inconsistency scenarios solved. Hope you could help answering. Thank you in
advance.

The problems seem hard to describe. Please suffer me writing more text to
illustrate.

As just the last post by Peter also mentioned, ConcurrentHashMap (CHM) is
often used as a concurrent or scalable cache, e.g. for caching HTTP
responses (web pages) generated from querying a database. Suppose the
application tier has cached an entry (keyX, pageX) in the map. So further
requests retrieving pageX can be served by the cache entry, without going
thru the database tier. However, the application receives a request to
modify the database records related to pageX. So it evicts the cache entry
of pageX from the map by calling remove(keyX). In the meanwhile, there can
be other incoming retrieval requests that perform get(keyX) concurrently.

Suppose all threads draw requests from a single queue to serve.

Thread T1 draws an update request, calls remove(keyX) to evict cache, then
updates the data for pageX in the database.

Thread T2 draws a retrieval request, and calls get(keyX).


Suppose T2's action happens after T1's all actions in real time. Then we
should expect T2 get null for keyX. This is the desired behavior for
avoiding some inconsistency between the cache and database. And T2 will
contact the database for up-to-date data and make a new cache entry for
pageX in the map via a new putIfAbsent call. (Of course, when T2 sees null
at get(keyX) and contacts the database, the data update performed by T1 may
not have started or finished, and needs one more level of synchronization,
say wrapping the Java code calling the SQL commands in synchronized.)


Well, my question is: will effect of T1's remove() ALWAYS be seen by T2's
get()?

In the CHM code (appended below), I know that there is a volatile write to
the count field in remove() and volatile read from count in get(). This sets
up a happens-before order on the two operations.

But I think all would agree the JMM enforces sequential consistency (SC) for
volatile accesses. In SC, a read may not return the last write by another
thread in real-time saying. Or you may imagine the JVM is running in a
distributed setting, like Terracotta. So T2 may be reading its (stale) value
under its cache, or the updates of T1 are still in its store buffer, not yet
flushed to main memory. So even if T1's remove() has just done "count = c"
(write-volatile), T2 may not yet see it at checking "count != 0"
(read-volatile), and can return the old hash entry of pageX in the map, that
was cached in processor cache (If T2 does not see the new count, it must
also not see the delete of pageX). May I ask if this is one acceptable
behavior of CHM?


Best regards,
King Tin


Append some code of CHM for easier discussion:
---------------------------------------------

       Object get(Object key, int hash) {
           if (count != 0) { // read-volatile
               HashEntry e = getFirst(hash);
               while (e != null) {
                   if (e.hash == hash && key.equals(e.key)) {
                       Object v = e.value;
                       if (v != null)
                           return v;
                       return readValueUnderLock(e); // recheck
                   }
                   e = e.next;
               }
           }
           return null;
       }

       Object remove(Object key, int hash, Object value) {
           lock();
           try {
               int c = count - 1;
               HashEntry[] tab = table;
               int index = hash & (tab.length - 1);
               HashEntry first = tab[index];
               HashEntry e = first;
               while (e != null && (e.hash != hash || !key.equals(e.key)))
                   e = e.next;

               Object oldValue = null;
               if (e != null) {
                   Object v = e.value;
                   if (value == null || value.equals(v)) {
                       oldValue = v;
                       // All entries following removed node can stay
                       // in list, but all preceding ones need to be
                       // cloned.
                       ++modCount;
                       HashEntry newFirst = e.next;
                       for (HashEntry p = first; p != e; p = p.next)
                           newFirst = new HashEntry(p.key, p.hash,
                                                         newFirst,
p.value);
                       tab[index] = newFirst;
                       count = c; // write-volatile
                   }
               }
               return oldValue;
           } finally {
               unlock();
           }
       }


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




-- 
Vitaly
617-548-7007 (mobile)

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110524/d414f0a7/attachment-0001.html>

From dl at cs.oswego.edu  Tue May 24 07:26:25 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 24 May 2011 07:26:25 -0400
Subject: [concurrency-interest] Examples of ConcurrentMap client code
In-Reply-To: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
References: <BANLkTinmjFxA6-55hLnO=dqBFeQ+qTD_yg@mail.gmail.com>
Message-ID: <4DDB95E1.5040507@cs.oswego.edu>

On 05/23/11 13:22, Peter Hawkins wrote:
> a) the predominant use for ConcurrentHashMap is to build concurrent caches.

Background: Bearing this in mind, we've periodically looked
into providing better support for various caching usages.
The in-progress extra166y.CustomConcurrentHashMap class
(http://gee.cs.oswego.edu/dl/jsr166/dist/extra166ydocs/extra166y/CustomConcurrentHashMap.html)
contains some of this -- computeIfAbsent, optional
weak or soft refs, customizable equality/identify checks.
However, it still lacks support for custom eviction policies.
We tabled this last year after the folks developing Google/Guava
MapMaker (that internally uses a variant of CCHM)
(http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html) 
volunteered to further investigate
such policies, at least within the kinds of use cases they run into.

We'll surely reconsider CCHM (or some variant) for JDK8.
So, if you discover wrong or error-prone map usages that might
benefit from additional API support, please let us know!

-Doug





From karmazilla at gmail.com  Tue May 24 07:55:48 2011
From: karmazilla at gmail.com (Christian Vest Hansen)
Date: Tue, 24 May 2011 13:55:48 +0200
Subject: [concurrency-interest] Flat-combining and operations with timeout
Message-ID: <BANLkTi=9ziS09ya06iP-9yA+2k89WVpQLA@mail.gmail.com>

Hi,

I have been trying to implement a concurrent data structure using the
flat-combining technique. I ave run into problems, however, because I must
be able to support operations that block, with and without a timeout.
Further, the Thread.getState method must return the correct WAITING and
TIMED_WAITING values for threads that are blocked in these methods.

This appears to complicate the algorithm, because it in part relies on
threads occasionally waking up and trying to become the combiner thread.
This apparently makes dead-locks, of sorts, possible if all threads only
call blocking operations.
For those that block with a timeout, the problem is that they can wake up to
unlucky interleavings where they miss a reply signal from the combiner
thread.
Furthermore, threads must also correctly deal with interruption. In my code,
I am using LockSupport directly, and it will automatically unpack threads
that become interrupted. So this too, introduces the risk of a missed signal
from the combiner thread.

Do anyone on this list have any experience with, or tips for, implementing
these blocking behaviours in flat-combining? The papers and the code I have
read seems to only concern itself with spin-waiting for replies, which also
makes it easy to recheck the combiner lock.

It should be noted that my specific use case permit the presence of
dedicated threads. So, lacking other ideas, I am planning on experimenting
with a dedicated combiner thread, so requesting threads never need to make
attempts at a combiner-lock. This, however, makes me worry a bit for the
cost of having all operations involving at least two threads communicating.

-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110524/9b562a06/attachment.html>

From alex at puredanger.com  Tue May 24 13:35:11 2011
From: alex at puredanger.com (Alex Miller)
Date: Tue, 24 May 2011 12:35:11 -0500
Subject: [concurrency-interest] Examples of ConcurrentMap client code
Message-ID: <BANLkTi=8++SVJCyrqdHAcvtu3PWX3pMUFA@mail.gmail.com>

On Tue, 24 May 2011 07:26:25,  Doug Lea <dl at cs.oswego.edu> wrote:
> We'll surely reconsider CCHM (or some variant) for JDK8.

Along these lines, I was wondering if Doug or anyone else in the JSR
166 loop is tracking the JSR 107 progress.  They are revived and
actively working on defining a Java SE/EE Cache API which covers this
same territory.  It occurred to me that it would be weird to get to a
point where we have a cache in juc and another in javax.cache.  JSR
107 aims to include transactions so maybe that's the point of
departure.  Even so, sure would be nice to have a consistent interface
for the Cache proper.

Mailing list: http://groups.google.com/group/jsr107
API: https://github.com/jsr107/jsr107spec

Alex

From ben_manes at yahoo.com  Tue May 24 14:46:33 2011
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 24 May 2011 11:46:33 -0700 (PDT)
Subject: [concurrency-interest] Flat-combining and operations with
	timeout
References: <277722.93054.qm@web38802.mail.mud.yahoo.com>
	<BANLkTi=dR0Gnp-HStLSgUg30mXUMthsvcg@mail.gmail.com> 
Message-ID: <17829.47735.qm@web38802.mail.mud.yahoo.com>





________________________________
From: Ben Manes <ben_manes at yahoo.com>
To: Christian Vest Hansen <karmazilla at gmail.com>
Sent: Tuesday, May 24, 2011 11:38 AM
Subject: Re: [concurrency-interest] Flat-combining and operations with timeout


In your particular case of an object pool, JDK7's TransferQueue might be the most appropriate. This enables canceling opposing operations without incurring high contention overhead.


If I recall the flat combining papers correctly, the idea was to avoid lock thrashing by having the thread that holds the lock perform a batch of work. This is otherwise known as lock amortization, as it spreads the penalty across callers and exchanges the contention penalty with the victim performing a slightly larger amount of work. I've used the non-blocking version successfully. I read the flat-combining papers as a discussion of the blocking variant, but I might have been mistaken. I simplified the mental picture down to a combination of lock amortization, elimination, and reservations (e.g. TransferQueue's dual data structures). The approach would be to bypass the lock by transferring around it and using a blocking construct (e.g. SynchronousQueue). A very rough form might look like,


SynchronousQueue transfer = ...

Lock lock = ...

Task task = ... // e.g. add operation


for (;;) {

? if (lock.tryLock()) {

? ? try {

? ? ? int tasks = 0;

? ? ? do {

? ? ? ? task.run();

? ? ? ? task.notify();

? ? ? } while (++transfers <= AMORTIZED_TASKS_THRESHOLD
? ? ? ? ? ? && (task = transfer.poll(1, TimeUnit.MILLISECONDS)) != null);

? ? } finally {

? ? ? lock.unlock();

? ? }

? }

? boolean transferred = transfer.offer(e, 1, TimeUnit.MILLISECONDS);

? if (transfered) {

? ? task.await();

? }

}


In this way the lock is not being thrashed against, but instead barged for acquisition. If not obtained then the awaiting threads block on a per-thread reservation instead of piling against the lock. The thread under the lock can drain the pending tasks, notify them, and exit the critical section. This may be a simplified version of their idea of flat combining, or I might have misinterpreted the papers.


________________________________
From: Christian Vest Hansen <karmazilla at gmail.com>
To: Ben Manes <ben_manes at yahoo.com>
Sent: Tuesday, May 24, 2011 10:40 AM
Subject: Re: [concurrency-interest] Flat-combining and operations with timeout


What do you mean by "transferring?the task around the lock"?

Spin-waiting is unfortunately not enough to satisfy the promises of the API that I'm implementing: Blocking operations must really block if they can't make progress, so that the State of the thread is reflecting what is going on. I don't see a way to "fake" the state of arbitrary threads.

For reference, my code is here if you want to have a look for context:
https://github.com/chrisvest/stormpot/tree/master/src/main/java/stormpot/whirlpool


On Tue, May 24, 2011 at 18:18, Ben Manes <ben_manes at yahoo.com> wrote:

My understanding of flat combining is that its a blocking form of lock amortization (the asynchronous version is simpler). The complexity seems to be required if building a synchronous queue, but otherwise the synchronous queue can be used as a construct in other data structures. If the task can be transfered around the lock then the thread can spin waiting for a reply on its task object. At that point I think it becomes easy to reason about the remaining issues.
>
>On Tue May 24th, 2011 4:55 AM PDT Christian Vest Hansen wrote:
>
>>Hi,
>>
>>I have been trying to implement a concurrent data structure using the
>>flat-combining technique. I ave run into problems, however, because I must
>>be able to support operations that block, with and without a timeout.
>>Further, the Thread.getState method must return the correct WAITING and
>>TIMED_WAITING values for threads that are blocked in these methods.
>>
>>This appears to complicate the algorithm, because it in part relies on
>>threads occasionally waking up and trying to become the combiner thread.
>>This apparently makes dead-locks, of sorts, possible if all threads only
>>call blocking operations.
>>For those that block with a timeout, the problem is that they can wake up to
>>unlucky interleavings where they miss a reply signal from the combiner
>>thread.
>>Furthermore, threads must also correctly deal with interruption. In my code,
>>I am using LockSupport directly, and it will automatically unpack threads
>>that become interrupted. So this too, introduces the risk of a missed signal
>>from the combiner thread.
>>
>>Do anyone on this list have any experience with, or tips for, implementing
>>these blocking behaviours in flat-combining? The papers and the code I have
>>read seems to only concern itself with spin-waiting for replies, which also
>>makes it easy to recheck the combiner lock.
>>
>>It should be noted that my specific use case permit the presence of
>>dedicated threads. So, lacking other ideas, I am planning on experimenting
>>with a dedicated combiner thread, so requesting threads never need to make
>>attempts at a combiner-lock. This, however, makes me worry a bit for the
>>cost of having all operations involving at least two threads communicating.
>>
>>--
>>Venlig hilsen / Kind regards,
>>Christian Vest Hansen.
>
>


-- 
Venlig hilsen / Kind regards,
Christian Vest Hansen.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20110524/f4725380/attachment.html>

From dl at cs.oswego.edu  Thu May 26 07:02:31 2011
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 26 May 2011 07:02:31 -0400
Subject: [concurrency-interest] Examples of ConcurrentMap client code
In-Reply-To: <BANLkTi=8++SVJCyrqdHAcvtu3PWX3pMUFA@mail.gmail.com>
References: <BANLkTi=8++SVJCyrqdHAcvtu3PWX3pMUFA@mail.gmail.com>
Message-ID: <4DDE3347.5000304@cs.oswego.edu>

On 05/24/11 13:35, Alex Miller wrote:
> Along these lines, I was wondering if Doug or anyone else in the JSR
> 166 loop is tracking the JSR 107 progress.  They are revived and
> actively working on defining a Java SE/EE Cache API which covers this
> same territory.  It occurred to me that it would be weird to get to a
> point where we have a cache in juc and another in javax.cache.

I hadn't looked at this for a long time. Thanks for
the reminder, especially in light of the likely
move to recast into some sort of j.u.c.Cache API.
I'll look into possible ties.
If anyone on this list is also involved in JSR107,
please get in touch with me.

-Doug

