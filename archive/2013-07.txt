From aleksey.shipilev at oracle.com  Mon Jul  1 04:22:34 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 01 Jul 2013 12:22:34 +0400
Subject: [concurrency-interest] RRWL with 'bad' Thread.getId()
	implementations
In-Reply-To: <51CAC5EA.9080209@cs.oswego.edu>
References: <CDEF4299.2F31E%chris.w.dennis@gmail.com>
	<51CA21B5.2010907@cs.oswego.edu> <51CAC5EA.9080209@cs.oswego.edu>
Message-ID: <51D13C4A.7050409@oracle.com>

On 06/26/2013 02:43 PM, Doug Lea wrote:
> On 06/25/13 19:03, Doug Lea wrote:
>> We can and probably should fix this immediately by grabbing
>> the Thread's "tid" field instead of calling getId.
> 
> I committed this change to jsr166 sources, so it should eventually
> make its way into OpenJDK.

It did, into JDK 8:
 http://hg.openjdk.java.net/jdk8/tl/jdk/rev/1919c226b427

There is a possibility to backport this to JDK 7. Is there an interest
in doing so? (Granted, since the issue is very isolated, we can skip
this part).

-Aleksey.





From dl at cs.oswego.edu  Mon Jul  1 16:26:50 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 01 Jul 2013 16:26:50 -0400
Subject: [concurrency-interest] Interface CompletionStage
Message-ID: <51D1E60A.2090606@cs.oswego.edu>


Last fall there was a lot of controversy about the API for
class CompletableFuture. Some people demanded the
presence of some methods, and some people demanded the
absence of these same methods. I think it is safe to
say that everyone with strong feelings about this
was disappointed. At the time, I tabled this discussion
pending further thought about possible options.

Just barely (I hope!) before it is too late to do anything
about this, I returned try to come to a better resolution.
Here it is:

New interface CompletionStage contains (almost) only the "fluent"
methods (thenApply, etc). These are uncontroversial.
Class CompletableFuture implements CompletionStage, and
also the methods people may or may not want to
allow users to use in any layered framework. However
CompletableFuture is structured such that any subclass
can disable any of the non-CompletionStage methods
by overriding to throw UnsupoortedOperationException,
and still use it for a perfectly compliant CompletionStage
implementation. For example, for those who want/need to
disable cancel(), might write...

  class MyCompletableFuture extends CompletableFuture { ...
     public boolean cancel(...) { throw new UnsupportedOperationException(); }
   }
   class MyCompletionStage implements CompletionStage {
     MyCompletableFuture cf ...
     // delegate all methods to cf;
   }

And many variations like this in which you may choose to
create and use a (strictly internal) intentionally-crippled
CompletableFuture subclass, but a perfectly fine and compliant
CompletionStage implementation. Design-by-UOE is an odd extension
policy, but better than all the others I know in coping with this issue.

Very little needed to be changed to pull this off.
 From the CompletableFuture view, just a few small additions
that do not impact any existing usages:

1. Interface CompletionStage requires one more method
than just the stage-based fluent ones, toCompletableFuture()
that ensures a completely safe common currency for interoperability.
(It is safe, because it may be implemented as a new stage,
not necessarily a (casted) "return this", as it is in
CompletableFuture itself.)

2. New method CompletionStage.whenCompleted plays a role similar to
that of CompletableFuture.exceptionally but avoids a type-system
problem: Method exceptionally() relied on the availability to users of
method completeExceptionally to get the equivalent of an
exception rethrow if so desired. Without it being available
in CompletionStage, there is no sanctioned way to do this.
But whenCompleted can now be used in the rethrow case.
Also, the whenCompleted and handle methods are available
in async versions.

3. The little utility method CompletableFuture.isCompletedExceptionally
should have been there to begin with, but is now hard to
live without.

That's it. I think that these small changes will make
a big difference in how CompletableFuture can be used
as basic infrastructure for layered frameworks. And
I cannot imagine any way in which it makes things worse
for anyone. But if you do, please let me know very soon.
(But please(!) do not ask for "just one more" change or feature!)

If I don't hear any soon, I'll try to make a case for it as
a late change for JDK8.


javadocs:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html

Sources:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletionStage.java?view=log
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log






From anmiller at redhat.com  Mon Jul  1 19:02:46 2013
From: anmiller at redhat.com (Andrig Miller)
Date: Mon, 1 Jul 2013 19:02:46 -0400 (EDT)
Subject: [concurrency-interest] Java performance issue when running
	virtualized (KVM)
In-Reply-To: <4720352.1795.1372719075939.JavaMail.andrig@worklaptop.miller.org>
Message-ID: <13586306.1825.1372719762251.JavaMail.andrig@worklaptop.miller.org>

I work for Red Hat, in the middle-ware business unit, and recently we had a customer open a case against our KVM based virtualization, where they were experiencing an unusual performance issue, compared to bare metal performance. The application happened to be a Java application, so I was asked to look into it. 

There was a simple application that they submitted with the issue that could reproduce the issue. In my testing, I found that it was much better when running on Java 7 (they were using Java 6), but that there was still a substantial performance degradation. After looking at the source, and doing some thread dumps, I discovered that they were using ReentrantReadWriteLock on some data that they were protecting before pushing that data into a PriorityBlockingQueue. 

It turns out that Java 7 was better then Java 6, because the internal locking in the PriorityBlockingQueue was using ReentrantLock, and the implementation of ReentrantLock was changed between Java 6 and 7 to no longer use the native method sun.misc.Unsafe.park. In the reproducer code, there actually wasn't a reason to use the ReentrantReadWriteLock, and when I removed that, we were able to get almost bare metal performance again. 

Our Red Hat Enterprise Linux KVM guys had determined that the futex() system call that comes from the sun.misc.Unsafe.park native method was the culprit of the performance degradation (there is no an easy fix that crosses all CPU architecture at the OS level, by the way). Since ReentrantLock no longer uses that native method, and exists entirely in user space, performance is great. 

So, I was wondering why ReentrantLock was modified to no longer use sun.misc.Unsafe.park, and ReentrantReadWriteLock was not? If ReentrantReadWriteLock could be modified in a similar way, and no longer use the native method making the futex() system call, then we could get near to bare metal performance even with not changing their code. Was this an oversight, or is there something about ReentrantReadWriteLock that prevents it from being modified in a similar way to ReentrantLock (all in user space, and no system calls). 

Thanks. 

-- 

Andrig (Andy) Miller 
Global Platform Director for JBoss Middle-ware 
Red Hat, Inc. 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130701/e6d21fa2/attachment.html>

From davidcholmes at aapt.net.au  Mon Jul  1 20:25:45 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 2 Jul 2013 10:25:45 +1000
Subject: [concurrency-interest] Java performance issue when
	runningvirtualized (KVM)
In-Reply-To: <13586306.1825.1372719762251.JavaMail.andrig@worklaptop.miller.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>

Hi Andy,

All of the blocking in ReentrantLock and ReentrantReadWriteLock is done via AbstractQueuedSynchronizer which uses LockSupport.park which uses Unsafe.park which will go into the VM's Parker implementation and use pthread_condwait, which in turn will use the futex implementation on Linux.

So I am unclear what difference you are actually seeing in the source code ??

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andrig Miller
  Sent: Tuesday, 2 July 2013 9:03 AM
  To: Concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Java performance issue when runningvirtualized (KVM)


  I work for Red Hat, in the middle-ware business unit, and recently we had a customer open a case against our KVM based virtualization, where they were experiencing an unusual performance issue, compared to bare metal performance.  The application happened to be a Java application, so I was asked to look into it.

  There was a simple application that they submitted with the issue that could reproduce the issue.  In my testing, I found that it was much better when running on Java 7 (they were using Java 6), but that there was still a substantial performance degradation.  After looking at the source, and doing some thread dumps, I discovered that they were using ReentrantReadWriteLock on some data that they were protecting before pushing that data into a PriorityBlockingQueue.

  It turns out that Java 7 was better then Java 6, because the internal locking in the PriorityBlockingQueue was using ReentrantLock, and the implementation of ReentrantLock was changed between Java 6 and 7 to no longer use the native method sun.misc.Unsafe.park.  In the reproducer code, there actually wasn't a reason to use the ReentrantReadWriteLock, and when I removed that, we were able to get almost bare metal performance again.

  Our Red Hat Enterprise Linux KVM guys had determined that the futex() system call that comes from the sun.misc.Unsafe.park native method was the culprit of the performance degradation (there is no an easy fix that crosses all CPU architecture at the OS level, by the way).  Since ReentrantLock no longer uses that native method, and exists entirely in user space, performance is great.

  So, I was wondering why ReentrantLock was modified to no longer use sun.misc.Unsafe.park, and ReentrantReadWriteLock was not?  If ReentrantReadWriteLock could be modified in a similar way, and no longer use the native method making the futex() system call, then we could get near to bare metal performance even with not changing their code.  Was this an oversight, or is there something about ReentrantReadWriteLock that prevents it from being modified in a similar way to ReentrantLock (all in user space, and no system calls).

  Thanks.

  -- 

  Andrig (Andy) Miller
  Global Platform Director for JBoss Middle-ware
  Red Hat, Inc.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/40b220cb/attachment.html>

From zhong.j.yu at gmail.com  Mon Jul  1 21:06:23 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 1 Jul 2013 20:06:23 -0500
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <51D1E60A.2090606@cs.oswego.edu>
References: <51D1E60A.2090606@cs.oswego.edu>
Message-ID: <CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>

On Mon, Jul 1, 2013 at 3:26 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> Last fall there was a lot of controversy about the API for
> class CompletableFuture. Some people demanded the
> presence of some methods, and some people demanded the
> absence of these same methods. I think it is safe to
> say that everyone with strong feelings about this
> was disappointed. At the time, I tabled this discussion
> pending further thought about possible options.
>
> Just barely (I hope!) before it is too late to do anything
> about this, I returned try to come to a better resolution.
> Here it is:
>
> New interface CompletionStage contains (almost) only the "fluent"
> methods (thenApply, etc). These are uncontroversial.
> Class CompletableFuture implements CompletionStage, and
> also the methods people may or may not want to
> allow users to use in any layered framework. However
> CompletableFuture is structured such that any subclass
> can disable any of the non-CompletionStage methods
> by overriding to throw UnsupoortedOperationException,
> and still use it for a perfectly compliant CompletionStage
> implementation. For example, for those who want/need to
> disable cancel(), might write...
>
>  class MyCompletableFuture extends CompletableFuture { ...
>     public boolean cancel(...) { throw new UnsupportedOperationException();
> }
>   }
>   class MyCompletionStage implements CompletionStage {
>     MyCompletableFuture cf ...
>     // delegate all methods to cf;
>   }
>
> And many variations like this in which you may choose to
> create and use a (strictly internal) intentionally-crippled
> CompletableFuture subclass, but a perfectly fine and compliant
> CompletionStage implementation. Design-by-UOE is an odd extension
> policy, but better than all the others I know in coping with this issue.
>
> Very little needed to be changed to pull this off.
> From the CompletableFuture view, just a few small additions
> that do not impact any existing usages:
>
> 1. Interface CompletionStage requires one more method
> than just the stage-based fluent ones, toCompletableFuture()
> that ensures a completely safe common currency for interoperability.
> (It is safe, because it may be implemented as a new stage,
> not necessarily a (casted) "return this", as it is in
> CompletableFuture itself.)
>
> 2. New method CompletionStage.whenCompleted plays a role similar to
> that of CompletableFuture.exceptionally but avoids a type-system
> problem: Method exceptionally() relied on the availability to users of
> method completeExceptionally to get the equivalent of an
> exception rethrow if so desired. Without it being available
> in CompletionStage, there is no sanctioned way to do this.
> But whenCompleted can now be used in the rethrow case.
> Also, the whenCompleted and handle methods are available
> in async versions.
>
> 3. The little utility method CompletableFuture.isCompletedExceptionally
> should have been there to begin with, but is now hard to
> live without.
>
> That's it. I think that these small changes will make
> a big difference in how CompletableFuture can be used
> as basic infrastructure for layered frameworks. And
> I cannot imagine any way in which it makes things worse
> for anyone. But if you do, please let me know very soon.
> (But please(!) do not ask for "just one more" change or feature!)

Just a trivial naming suggestion:

    whenComplete(BiConsumer<T,Throwable>)
=>
    finallyAccept(BiConsumer<T,Throwable>)


    handle(BiFunction<T,Throwable,U>)
=>
    finallyApply(BiFunction<T,Throwable,U>)


i.e. thenFoo() works on successful completion, finallyFoo() works on
any completion. The resulting chained calls make more sense to java
programmers:

    compute()
        .thenApply(...)
        .thenApply(...)
        .finallyAccept(...); // free resources

(would be nice too to have a finallyRun() method...)

Zhong Yu


>
> If I don't hear any soon, I'll try to make a case for it as
> a late change for JDK8.
>
>
> javadocs:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>
> Sources:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletionStage.java?view=log
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From jed at atlassian.com  Mon Jul  1 21:41:31 2013
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Tue, 2 Jul 2013 11:41:31 +1000
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
Message-ID: <CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>

many of these names are synonyms for established names on other
abstractions.

For instance

  CompletionStage<T> {
    <U> CompletionStage<U> thenApply(Function<? super T, ? extends U>)
  }

has the same semantics as:

  Stream<T> {
    <U> Stream<U> map(Function<? super T, ? extends U>)
  }

or

  Optional<T> {
    <U> Optional<U> map(Function<? super T, ? extends U>)
  }

and

CompletionStage<T> {
  <U> CompletionStage<U> thenCompose(Function<? super T, ? extends
CompletionStage<U>>)
}

has the same semantics as:

  Stream<T> {
    <U> Stream<U> flatMap(Function<? super T, ? extends U>)
  }

or

  Optional<T> {
    <U> Stream<U> flatMap(Function<? super T, ? extends U>)
  }

and further:

and

CompletionStage<T> {
  <U> CompletionStage<U> thenAccept(Function<? super T, ? extends
CompletionStage<U>>)
}

has the same semantics as:

  Stream<T> {
    <U> Stream<U> foreach(Function<? super T, ? extends U>)
  }

or

  Optional<T> {
    <U> Stream<U> foreach(Function<? super T, ? extends U>)
  }

It would be nice not to have a bespoke naming scheme for every functor that
exists. While it isn't possible to abstract across these things in Java, it
does reduce the (re)learning burden to use each one.

There is one combinator that I find quite troubling:


http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html#handle(java.util.function.BiFunction)

It takes a BiFunction that takes either a T (and therefore a null
Throwable) or a Throwable (and therefore a null T). Could we please not use
nulls for a disjoint union? We could either supply two Functions (one trom
T and one from Throwable) or perhaps use a proper disjoint union type* as
the function argument. Lets not accept nulls if we can avoid them.

(BTW. this combinator is the catamorphism for Future, and is often known as
'fold')

cheers,
jed.

* for instance you are welcome to:
https://bitbucket.org/atlassian/fugue/src/master/src/main/java/com/atlassian/fugue/Either.java



On 2 July 2013 11:06, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> On Mon, Jul 1, 2013 at 3:26 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> >
> > Last fall there was a lot of controversy about the API for
> > class CompletableFuture. Some people demanded the
> > presence of some methods, and some people demanded the
> > absence of these same methods. I think it is safe to
> > say that everyone with strong feelings about this
> > was disappointed. At the time, I tabled this discussion
> > pending further thought about possible options.
> >
> > Just barely (I hope!) before it is too late to do anything
> > about this, I returned try to come to a better resolution.
> > Here it is:
> >
> > New interface CompletionStage contains (almost) only the "fluent"
> > methods (thenApply, etc). These are uncontroversial.
> > Class CompletableFuture implements CompletionStage, and
> > also the methods people may or may not want to
> > allow users to use in any layered framework. However
> > CompletableFuture is structured such that any subclass
> > can disable any of the non-CompletionStage methods
> > by overriding to throw UnsupoortedOperationException,
> > and still use it for a perfectly compliant CompletionStage
> > implementation. For example, for those who want/need to
> > disable cancel(), might write...
> >
> >  class MyCompletableFuture extends CompletableFuture { ...
> >     public boolean cancel(...) { throw new
> UnsupportedOperationException();
> > }
> >   }
> >   class MyCompletionStage implements CompletionStage {
> >     MyCompletableFuture cf ...
> >     // delegate all methods to cf;
> >   }
> >
> > And many variations like this in which you may choose to
> > create and use a (strictly internal) intentionally-crippled
> > CompletableFuture subclass, but a perfectly fine and compliant
> > CompletionStage implementation. Design-by-UOE is an odd extension
> > policy, but better than all the others I know in coping with this issue.
> >
> > Very little needed to be changed to pull this off.
> > From the CompletableFuture view, just a few small additions
> > that do not impact any existing usages:
> >
> > 1. Interface CompletionStage requires one more method
> > than just the stage-based fluent ones, toCompletableFuture()
> > that ensures a completely safe common currency for interoperability.
> > (It is safe, because it may be implemented as a new stage,
> > not necessarily a (casted) "return this", as it is in
> > CompletableFuture itself.)
> >
> > 2. New method CompletionStage.whenCompleted plays a role similar to
> > that of CompletableFuture.exceptionally but avoids a type-system
> > problem: Method exceptionally() relied on the availability to users of
> > method completeExceptionally to get the equivalent of an
> > exception rethrow if so desired. Without it being available
> > in CompletionStage, there is no sanctioned way to do this.
> > But whenCompleted can now be used in the rethrow case.
> > Also, the whenCompleted and handle methods are available
> > in async versions.
> >
> > 3. The little utility method CompletableFuture.isCompletedExceptionally
> > should have been there to begin with, but is now hard to
> > live without.
> >
> > That's it. I think that these small changes will make
> > a big difference in how CompletableFuture can be used
> > as basic infrastructure for layered frameworks. And
> > I cannot imagine any way in which it makes things worse
> > for anyone. But if you do, please let me know very soon.
> > (But please(!) do not ask for "just one more" change or feature!)
>
> Just a trivial naming suggestion:
>
>     whenComplete(BiConsumer<T,Throwable>)
> =>
>     finallyAccept(BiConsumer<T,Throwable>)
>
>
>     handle(BiFunction<T,Throwable,U>)
> =>
>     finallyApply(BiFunction<T,Throwable,U>)
>
>
> i.e. thenFoo() works on successful completion, finallyFoo() works on
> any completion. The resulting chained calls make more sense to java
> programmers:
>
>     compute()
>         .thenApply(...)
>         .thenApply(...)
>         .finallyAccept(...); // free resources
>
> (would be nice too to have a finallyRun() method...)
>
> Zhong Yu
>
>
> >
> > If I don't hear any soon, I'll try to make a case for it as
> > a late change for JDK8.
> >
> >
> > javadocs:
> >
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html
> >
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> >
> > Sources:
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletionStage.java?view=log
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
> >
> >
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/25e0bc25/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jul  2 06:34:10 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 02 Jul 2013 06:34:10 -0400
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
Message-ID: <51D2ACA2.7010605@cs.oswego.edu>

On 07/01/13 21:41, Jed Wesley-Smith wrote:
> many of these names are synonyms for established names on other abstractions.
>
> For instance
>
>    CompletionStage<T> {
>      <U> CompletionStage<U> thenApply(Function<? super T, ? extends U>)
>    }
>
> has the same semantics as:
>
>    Stream<T> {
>      <U> Stream<U> map(Function<? super T, ? extends U>)
>    }
>
> or
>
>    Optional<T> {
>      <U> Optional<U> map(Function<? super T, ? extends U>)
>    }
>

And so on. Yes. We are fully aware of this.
The thought that some nice unifying scheme might emerge
after other JDK8 features were nailed down was one of
the main reasons for tabling this in the first place.
But as it turns out, I think we are doing people like
you a favor by intentionally not using an identical
scheme: Because Java does not have higher-order
generics, it is impossible to express notions like
Monads. Additionally, because of the multiple-match
rules for lambda-literals, you cannot always safely
overload methods that take function-type parameters.
(While the "map" example is not such a big deal,
others like "flatMap" are.)

So, rather than present some crippled approximation
of a higher-order concept, we use simple descriptive
names that are intended to be easy to write trivial
adaptors for in a mix-in interface (or any of
several variants) for people who choose to use some
approximation of the intended higher-order types, or
adapt to use in other source languages.

>
> There is one combinator that I find quite troubling:
>
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html#handle(java.util.function.BiFunction)
>
> It takes a BiFunction that takes either a T (and therefore a null Throwable) or
> a Throwable (and therefore a null T).

I'm not a big fan of this either, and was pretty close to choosing the
two-method approach to overcome the javac-won't-let-me-rethrow problem of
"exceptionally()". But I chose this instead because it is the only
reasonable path to support stages of the form: take some action if either
an exception or an unexpected result. Any other way to support this would
be too painful and slow.

-Doug



From aph at redhat.com  Tue Jul  2 09:40:27 2013
From: aph at redhat.com (Andrew Haley)
Date: Tue, 02 Jul 2013 14:40:27 +0100
Subject: [concurrency-interest] Java performance issue when
 runningvirtualized (KVM)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>
Message-ID: <51D2D84B.2090709@redhat.com>

Indeed.  And IME actual futex() calls are very rare except in highly contended
situations where the system call overhead is going to be the lest of your worries.

I wonder if there might be a bug where futex() gets called pointlessly.
This might even be a libc bug: libc goes to a fair bit of effort avoid
calling futex() unless it's very likely that it will block.

Andrew.

On 07/02/2013 01:25 AM, David Holmes wrote:
> Hi Andy,
> 
> All of the blocking in ReentrantLock and ReentrantReadWriteLock is done via AbstractQueuedSynchronizer which uses LockSupport.park which uses Unsafe.park which will go into the VM's Parker implementation and use pthread_condwait, which in turn will use the futex implementation on Linux.
> 
> So I am unclear what difference you are actually seeing in the source code ??
> 
> David Holmes
>   -----Original Message-----
>   From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andrig Miller
>   Sent: Tuesday, 2 July 2013 9:03 AM
>   To: Concurrency-interest at cs.oswego.edu
>   Subject: [concurrency-interest] Java performance issue when runningvirtualized (KVM)
> 
> 
>   I work for Red Hat, in the middle-ware business unit, and recently we had a customer open a case against our KVM based virtualization, where they were experiencing an unusual performance issue, compared to bare metal performance.  The application happened to be a Java application, so I was asked to look into it.
> 
>   There was a simple application that they submitted with the issue that could reproduce the issue.  In my testing, I found that it was much better when running on Java 7 (they were using Java 6), but that there was still a substantial performance degradation.  After looking at the source, and doing some thread dumps, I discovered that they were using ReentrantReadWriteLock on some data that they were protecting before pushing that data into a PriorityBlockingQueue.
> 
>   It turns out that Java 7 was better then Java 6, because the internal locking in the PriorityBlockingQueue was using ReentrantLock, and the implementation of ReentrantLock was changed between Java 6 and 7 to no longer use the native method sun.misc.Unsafe.park.  In the reproducer code, there actually wasn't a reason to use the ReentrantReadWriteLock, and when I removed that, we were able to get almost bare metal performance again.
> 
>   Our Red Hat Enterprise Linux KVM guys had determined that the futex() system call that comes from the sun.misc.Unsafe.park native method was the culprit of the performance degradation (there is no an easy fix that crosses all CPU architecture at the OS level, by the way).  Since ReentrantLock no longer uses that native method, and exists entirely in user space, performance is great.
> 
>   So, I was wondering why ReentrantLock was modified to no longer use sun.misc.Unsafe.park, and ReentrantReadWriteLock was not?  If ReentrantReadWriteLock could be modified in a similar way, and no longer use the native method making the futex() system call, then we could get near to bare metal performance even with not changing their code.  Was this an oversight, or is there something about ReentrantReadWriteLock that prevents it from being modified in a similar way to ReentrantLock (all in user space, and no system calls).
> 
>   Thanks.
> 
>   -- 
> 
>   Andrig (Andy) Miller
>   Global Platform Director for JBoss Middle-ware
>   Red Hat, Inc.
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From anmiller at redhat.com  Tue Jul  2 10:35:34 2013
From: anmiller at redhat.com (Andrig Miller)
Date: Tue, 2 Jul 2013 10:35:34 -0400 (EDT)
Subject: [concurrency-interest] Java performance issue when
 runningvirtualized (KVM)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>
Message-ID: <4964337.352.1372775730460.JavaMail.andrig@worklaptop.miller.org>

Well, I should have looked at the source of the methods, but here is what I see when only using ReentrantLock in their supplied reproducer code, from a thread dump: 

"Thread-350" prio=10 tid=0x00007f54b8437000 nid=0x21a6 waiting on condition [0x00007f546b7f6000] 
java.lang.Thread.State: TIMED_WAITING (sleeping) 
at java.lang.Thread.sleep(Native Method) 
at java.lang.Thread.sleep(Thread.java:338) 
at test.MultiThreadedQ$4.run(MultiThreadedQ.java:252) 
at java.lang.Thread.run(Thread.java:722) 

That's what all the blocked threads look like. When using ReentrantReadWriteLock or JDK 6, all the threads look like this: 

"Thread-350" prio=10 tid=0x00007f17e43af000 nid=0x1cbb waiting on condition [0x00007f15c9ad9000] 
java.lang.Thread.State: WAITING (parking) 
at sun.misc.Unsafe.park(Native Method) 
- parking to wait for <0x0000000688a020a0> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync) 
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:968) 
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1284) 
at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:627) 
at test.MultiThreadedQ$4.run(MultiThreadedQ.java:229) 
at java.lang.Thread.run(Thread.java:679) 

In the first case with JDK 7, we are not using the ReentrantReadWriteLock, but only depending on the internal locking of the PriorityBlockingQueue, which using ReentrantLock internally. 

So, my assumption was the implementation was different, so I should have looked at the code. 

Andy 

----- Original Message -----

> From: "David Holmes" <davidcholmes at aapt.net.au>
> To: "Andrig Miller" <anmiller at redhat.com>,
> Concurrency-interest at cs.oswego.edu
> Sent: Monday, July 1, 2013 6:25:45 PM
> Subject: RE: [concurrency-interest] Java performance issue when
> runningvirtualized (KVM)

> ?
> Hi Andy,

> All of the blocking in ReentrantLock and ReentrantReadWriteLock is
> done via AbstractQueuedSynchronizer which uses LockSupport.park
> which uses Unsafe.park which will go into the VM's Parker
> implementation and use pthread_condwait, which in turn will use the
> futex implementation on Linux.

> So I am unclear what difference you are actually seeing in the source
> code ??

> David Holmes
> > -----Original Message-----
> 
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of
> > Andrig Miller
> 
> > Sent: Tuesday, 2 July 2013 9:03 AM
> 
> > To: Concurrency-interest at cs.oswego.edu
> 
> > Subject: [concurrency-interest] Java performance issue when
> > runningvirtualized (KVM)
> 

> > I work for Red Hat, in the middle-ware business unit, and recently
> > we
> > had a customer open a case against our KVM based virtualization,
> > where they were experiencing an unusual performance issue, compared
> > to bare metal performance. The application happened to be a Java
> > application, so I was asked to look into it.
> 

> > There was a simple application that they submitted with the issue
> > that could reproduce the issue. In my testing, I found that it was
> > much better when running on Java 7 (they were using Java 6), but
> > that there was still a substantial performance degradation. After
> > looking at the source, and doing some thread dumps, I discovered
> > that they were using ReentrantReadWriteLock on some data that they
> > were protecting before pushing that data into a
> > PriorityBlockingQueue.
> 

> > It turns out that Java 7 was better then Java 6, because the
> > internal
> > locking in the PriorityBlockingQueue was using ReentrantLock, and
> > the implementation of ReentrantLock was changed between Java 6 and
> > 7
> > to no longer use the native method sun.misc.Unsafe.park. In the
> > reproducer code, there actually wasn't a reason to use the
> > ReentrantReadWriteLock, and when I removed that, we were able to
> > get
> > almost bare metal performance again.
> 

> > Our Red Hat Enterprise Linux KVM guys had determined that the
> > futex()
> > system call that comes from the sun.misc.Unsafe.park native method
> > was the culprit of the performance degradation (there is no an easy
> > fix that crosses all CPU architecture at the OS level, by the way).
> > Since ReentrantLock no longer uses that native method, and exists
> > entirely in user space, performance is great.
> 

> > So, I was wondering why ReentrantLock was modified to no longer use
> > sun.misc.Unsafe.park, and ReentrantReadWriteLock was not? If
> > ReentrantReadWriteLock could be modified in a similar way, and no
> > longer use the native method making the futex() system call, then
> > we
> > could get near to bare metal performance even with not changing
> > their code. Was this an oversight, or is there something about
> > ReentrantReadWriteLock that prevents it from being modified in a
> > similar way to ReentrantLock (all in user space, and no system
> > calls).
> 

> > Thanks.
> 

> > --
> 

> > Andrig (Andy) Miller
> 
> > Global Platform Director for JBoss Middle-ware
> 
> > Red Hat, Inc.
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/1dd488f3/attachment.html>

From dragonsinth at gmail.com  Tue Jul  2 11:18:43 2013
From: dragonsinth at gmail.com (Scott Blum)
Date: Tue, 2 Jul 2013 11:18:43 -0400
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <51D2ACA2.7010605@cs.oswego.edu>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
	<51D2ACA2.7010605@cs.oswego.edu>
Message-ID: <CALuNCpjr3TaW=dqp9V2Hc89v9wjvvi9=SvajPgP4h6bzGyKxoQ@mail.gmail.com>

On Tue, Jul 2, 2013 at 6:34 AM, Doug Lea <dl at cs.oswego.edu> wrote:

I'm not a big fan of this either, and was pretty close to choosing the
> two-method approach to overcome the javac-won't-let-me-rethrow problem of
> "exceptionally()". But I chose this instead because it is the only
> reasonable path to support stages of the form: take some action if either
> an exception or an unexpected result. Any other way to support this would
> be too painful and slow.

Are you referring to the problem of not being able to propagate a checked
exception from a local method (when it would be safe to do so) on account
not being able to declare a throws because you?re implementing an
interface? If so, there?s a way around it, although it's kind of a hack.
This is what we do at my company:

  /**
   * Throws the {@link Throwable} which may be a checked exception.
This is generally not allowed in
   * Java but okay in the VM. You probably don't want to use this
unless you're really sure. You've
   * been warned.
   *
   * @param throwable the exception to throw.
   * @return this method does not return, the return type is specified
to allow the caller to easily
   *         construct a statement that completes abruptly.
   *         e.g., {@code throw throwUnchecked(new IOException())}
   */
  public static RuntimeException throwUnchecked(Throwable throwable) {
    throw Exceptions.<RuntimeException>doThrowUnchecked(throwable);
  }

  @SuppressWarnings("unchecked")
  private static <T extends Throwable> T doThrowUnchecked(Throwable
throwable) throws T {
    throw (T) throwable;
  }
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/e304b327/attachment-0001.html>

From gustav.r.akesson at gmail.com  Tue Jul  2 15:16:13 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Tue, 2 Jul 2013 21:16:13 +0200
Subject: [concurrency-interest]  ABQ and interrupts
Message-ID: <CAKEw5+5hnXAOBuOfOZB-JD=UXPYQsgRAt45pkCUnX5wrBO8QGA@mail.gmail.com>

Hi,

I saw that for Java 6 ABQ#put (goes for other blocking methods as well) the
implementation is...

    public void put(E e) throws InterruptedException {
        if (e == null) throw new NullPointerException();
        final E[] items = this.items;
        final ReentrantLock lock = this.lock;
        lock.lockInterruptibly();
        try {
            try {
                while (count == items.length)
                    notFull.await();*            } catch
(InterruptedException ie) {
                notFull.signal(); // propagate to non-interrupted thread
                throw ie;
            }*
            insert(e);
        } finally {
            lock.unlock();
        }
    }


...whereas in Java 7 it is...

    public void put(E e) throws InterruptedException {
        checkNotNull(e);
        final ReentrantLock lock = this.lock;
        lock.lockInterruptibly();
        try {
            while (count == items.length)
                notFull.await();
            insert(e);
        } finally {
            lock.unlock();
        }
    }

How come that the interrupt is no longer handling by signaling any
potential thread waiting at the condition?  First of all, I'm not
completely sure I understand why that handling was there in the first place
(seemed like an unncessary thread-wakeup to me), but I would like to know
why it was handled and why it was removed.

Any ideas?


Best Regards,

Gustav ?kesson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/6546859d/attachment.html>

From cpovirk at google.com  Tue Jul  2 15:40:50 2013
From: cpovirk at google.com (Chris Povirk)
Date: Tue, 2 Jul 2013 15:40:50 -0400
Subject: [concurrency-interest] ABQ and interrupts
In-Reply-To: <CAKEw5+5hnXAOBuOfOZB-JD=UXPYQsgRAt45pkCUnX5wrBO8QGA@mail.gmail.com>
References: <CAKEw5+5hnXAOBuOfOZB-JD=UXPYQsgRAt45pkCUnX5wrBO8QGA@mail.gmail.com>
Message-ID: <CAEvq2nrwEcnmPHEguapnsATPGFWRSj=sBT14wXjjrnmjLGc=Wg@mail.gmail.com>

Prior discussion: http://markmail.org/message/zya4rc2dfnzdsevi

From stanimir at riflexo.com  Tue Jul  2 16:38:59 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 2 Jul 2013 23:38:59 +0300
Subject: [concurrency-interest] Java performance issue when
 runningvirtualized (KVM)
In-Reply-To: <4964337.352.1372775730460.JavaMail.andrig@worklaptop.miller.org>
References: <NFBBKALFDCPFIDBNKAPCGEIGJOAA.davidcholmes@aapt.net.au>
	<4964337.352.1372775730460.JavaMail.andrig@worklaptop.miller.org>
Message-ID: <CAEJX8orqYS0za7bvvxCaWa37YWFVBfhxdtXiY0Wn0xvg6siorw@mail.gmail.com>

The sleep call in the same inner class looks quite bothering, esp if you
sleep while the lock is obtained.
That would explain everything you observe.
And yes if you experience unexpected contention jstack is your friend.

Stanimir

On Tue, Jul 2, 2013 at 5:35 PM, Andrig Miller <anmiller at redhat.com> wrote:

> Well, I should have looked at the source of the methods, but here is what
> I see when only using ReentrantLock in their supplied reproducer code, from
> a thread dump:
>
> "Thread-350" prio=10 tid=0x00007f54b8437000 nid=0x21a6 waiting on
> condition [0x00007f546b7f6000]
>    java.lang.Thread.State: TIMED_WAITING (sleeping)
>     at java.lang.Thread.sleep(Native Method)
>     at java.lang.Thread.sleep(Thread.java:338)
>     at test.MultiThreadedQ$4.run(MultiThreadedQ.java:252)
>     at java.lang.Thread.run(Thread.java:722)
>
> That's what all the blocked threads look like.  When using
> ReentrantReadWriteLock or JDK 6, all the threads look like this:
>
> "Thread-350" prio=10 tid=0x00007f17e43af000 nid=0x1cbb waiting on
> condition [0x00007f15c9ad9000]
>    java.lang.Thread.State: WAITING (parking)
>     at sun.misc.Unsafe.park(Native Method)
>     - parking to wait for  <0x0000000688a020a0> (a
> java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
>     at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
>     at
> java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
>     at
> java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:968)
>     at
> java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1284)
>     at
> java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:627)
>     at test.MultiThreadedQ$4.run(MultiThreadedQ.java:229)
>     at java.lang.Thread.run(Thread.java:679)
>
> In the first case with JDK 7, we are not using the ReentrantReadWriteLock,
> but only depending on the internal locking of the PriorityBlockingQueue,
> which using ReentrantLock internally.
>
> So, my assumption was the implementation was different, so I should have
> looked at the code.
>
> Andy
>
> ------------------------------
>
> *From: *"David Holmes" <davidcholmes at aapt.net.au>
> *To: *"Andrig Miller" <anmiller at redhat.com>,
> Concurrency-interest at cs.oswego.edu
> *Sent: *Monday, July 1, 2013 6:25:45 PM
> *Subject: *RE: [concurrency-interest] Java performance issue when
> runningvirtualized (KVM)
>
> 
> Hi Andy,
>
> All of the blocking in ReentrantLock and ReentrantReadWriteLock is done
> via AbstractQueuedSynchronizer which uses LockSupport.park which uses
> Unsafe.park which will go into the VM's Parker implementation and use
> pthread_condwait, which in turn will use the futex implementation on Linux.
>
> So I am unclear what difference you are actually seeing in the source code
> ??
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Andrig Miller
> *Sent:* Tuesday, 2 July 2013 9:03 AM
> *To:* Concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Java performance issue when
> runningvirtualized (KVM)
>
> I work for Red Hat, in the middle-ware business unit, and recently we had
> a customer open a case against our KVM based virtualization, where they
> were experiencing an unusual performance issue, compared to bare metal
> performance.  The application happened to be a Java application, so I was
> asked to look into it.
>
> There was a simple application that they submitted with the issue that
> could reproduce the issue.  In my testing, I found that it was much better
> when running on Java 7 (they were using Java 6), but that there was still a
> substantial performance degradation.  After looking at the source, and
> doing some thread dumps, I discovered that they were using
> ReentrantReadWriteLock on some data that they were protecting before
> pushing that data into a PriorityBlockingQueue.
>
> It turns out that Java 7 was better then Java 6, because the internal
> locking in the PriorityBlockingQueue was using ReentrantLock, and the
> implementation of ReentrantLock was changed between Java 6 and 7 to no
> longer use the native method sun.misc.Unsafe.park.  In the reproducer code,
> there actually wasn't a reason to use the ReentrantReadWriteLock, and when
> I removed that, we were able to get almost bare metal performance again.
>
> Our Red Hat Enterprise Linux KVM guys had determined that the futex()
> system call that comes from the sun.misc.Unsafe.park native method was the
> culprit of the performance degradation (there is no an easy fix that
> crosses all CPU architecture at the OS level, by the way).  Since
> ReentrantLock no longer uses that native method, and exists entirely in
> user space, performance is great.
>
> So, I was wondering why ReentrantLock was modified to no longer use
> sun.misc.Unsafe.park, and ReentrantReadWriteLock was not?  If
> ReentrantReadWriteLock could be modified in a similar way, and no longer
> use the native method making the futex() system call, then we could get
> near to bare metal performance even with not changing their code.  Was this
> an oversight, or is there something about ReentrantReadWriteLock that
> prevents it from being modified in a similar way to ReentrantLock (all in
> user space, and no system calls).
>
> Thanks.
>
> --
> Andrig (Andy) Miller
> Global Platform Director for JBoss Middle-ware
> Red Hat, Inc.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/14e7a257/attachment.html>

From stanimir at riflexo.com  Tue Jul  2 17:00:00 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 3 Jul 2013 00:00:00 +0300
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CALuNCpjr3TaW=dqp9V2Hc89v9wjvvi9=SvajPgP4h6bzGyKxoQ@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
	<51D2ACA2.7010605@cs.oswego.edu>
	<CALuNCpjr3TaW=dqp9V2Hc89v9wjvvi9=SvajPgP4h6bzGyKxoQ@mail.gmail.com>
Message-ID: <CAEJX8oq5RaQi-2WySgK7VZ+bziGqCWhhXFyQcTr0Z_7jNWei3w@mail.gmail.com>

This is a nice hack, but it's also available via Unsafe.throwException

On Tue, Jul 2, 2013 at 6:18 PM, Scott Blum <dragonsinth at gmail.com> wrote:

> On Tue, Jul 2, 2013 at 6:34 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> I'm not a big fan of this either, and was pretty close to choosing the
>> two-method approach to overcome the javac-won't-let-me-rethrow problem of
>> "exceptionally()". But I chose this instead because it is the only
>> reasonable path to support stages of the form: take some action if either
>> an exception or an unexpected result. Any other way to support this would
>> be too painful and slow.
>
> Are you referring to the problem of not being able to propagate a checked
> exception from a local method (when it would be safe to do so) on account
> not being able to declare a throws because you?re implementing an
> interface? If so, there?s a way around it, although it's kind of a hack.
> This is what we do at my company:
>
>   /**
>    * Throws the {@link Throwable} which may be a checked exception. This is generally not allowed in
>    * Java but okay in the VM. You probably don't want to use this unless you're really sure. You've
>    * been warned.
>    *
>    * @param throwable the exception to throw.
>    * @return this method does not return, the return type is specified to allow the caller to easily
>    *         construct a statement that completes abruptly.
>    *         e.g., {@code throw throwUnchecked(new IOException())}
>    */
>   public static RuntimeException throwUnchecked(Throwable throwable) {
>     throw Exceptions.<RuntimeException>doThrowUnchecked(throwable);
>   }
>
>   @SuppressWarnings("unchecked")
>   private static <T extends Throwable> T doThrowUnchecked(Throwable throwable) throws T {
>     throw (T) throwable;
>   }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/b5a86966/attachment-0001.html>

From viktor.klang at gmail.com  Tue Jul  2 17:13:29 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 2 Jul 2013 23:13:29 +0200
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CAEJX8oq5RaQi-2WySgK7VZ+bziGqCWhhXFyQcTr0Z_7jNWei3w@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
	<51D2ACA2.7010605@cs.oswego.edu>
	<CALuNCpjr3TaW=dqp9V2Hc89v9wjvvi9=SvajPgP4h6bzGyKxoQ@mail.gmail.com>
	<CAEJX8oq5RaQi-2WySgK7VZ+bziGqCWhhXFyQcTr0Z_7jNWei3w@mail.gmail.com>
Message-ID: <CANPzfU-75GiFYr5QGQGato3-PB-4JvrmSJ7H_vy-K4T6D=qTzg@mail.gmail.com>

Which does not exist on Android... (the throwsException method)


On Tue, Jul 2, 2013 at 11:00 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> This is a nice hack, but it's also available via Unsafe.throwException
>
> On Tue, Jul 2, 2013 at 6:18 PM, Scott Blum <dragonsinth at gmail.com> wrote:
>
>>  On Tue, Jul 2, 2013 at 6:34 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> I'm not a big fan of this either, and was pretty close to choosing the
>>> two-method approach to overcome the javac-won't-let-me-rethrow problem of
>>> "exceptionally()". But I chose this instead because it is the only
>>> reasonable path to support stages of the form: take some action if either
>>> an exception or an unexpected result. Any other way to support this would
>>> be too painful and slow.
>>
>> Are you referring to the problem of not being able to propagate a checked
>> exception from a local method (when it would be safe to do so) on account
>> not being able to declare a throws because you?re implementing an
>> interface? If so, there?s a way around it, although it's kind of a hack.
>> This is what we do at my company:
>>
>>   /**
>>    * Throws the {@link Throwable} which may be a checked exception. This is generally not allowed in
>>    * Java but okay in the VM. You probably don't want to use this unless you're really sure. You've
>>    * been warned.
>>    *
>>    * @param throwable the exception to throw.
>>    * @return this method does not return, the return type is specified to allow the caller to easily
>>    *         construct a statement that completes abruptly.
>>    *         e.g., {@code throw throwUnchecked(new IOException())}
>>    */
>>   public static RuntimeException throwUnchecked(Throwable throwable) {
>>     throw Exceptions.<RuntimeException>doThrowUnchecked(throwable);
>>   }
>>
>>   @SuppressWarnings("unchecked")
>>   private static <T extends Throwable> T doThrowUnchecked(Throwable throwable) throws T {
>>     throw (T) throwable;
>>   }
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130702/d0187ae8/attachment.html>

From stanimir at riflexo.com  Tue Jul  2 17:16:23 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 3 Jul 2013 00:16:23 +0300
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CANPzfU-75GiFYr5QGQGato3-PB-4JvrmSJ7H_vy-K4T6D=qTzg@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
	<51D2ACA2.7010605@cs.oswego.edu>
	<CALuNCpjr3TaW=dqp9V2Hc89v9wjvvi9=SvajPgP4h6bzGyKxoQ@mail.gmail.com>
	<CAEJX8oq5RaQi-2WySgK7VZ+bziGqCWhhXFyQcTr0Z_7jNWei3w@mail.gmail.com>
	<CANPzfU-75GiFYr5QGQGato3-PB-4JvrmSJ7H_vy-K4T6D=qTzg@mail.gmail.com>
Message-ID: <CAEJX8or9_XoFbjv9Gr0UPrpH0WPrDS2CJ4qMud6iMEgyzbEa0w@mail.gmail.com>

Thread.stop(Throwable) does, though (but it has the security checks), so I
was not my 1st choice.

Stanimir

On Wed, Jul 3, 2013 at 12:13 AM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

> Which does not exist on Android... (the throwsException method)
>
>
> On Tue, Jul 2, 2013 at 11:00 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> This is a nice hack, but it's also available via Unsafe.throwException
>>
>> On Tue, Jul 2, 2013 at 6:18 PM, Scott Blum <dragonsinth at gmail.com> wrote:
>>
>>>  On Tue, Jul 2, 2013 at 6:34 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>
>>> I'm not a big fan of this either, and was pretty close to choosing the
>>>> two-method approach to overcome the javac-won't-let-me-rethrow problem
>>>> of
>>>> "exceptionally()". But I chose this instead because it is the only
>>>> reasonable path to support stages of the form: take some action if
>>>> either
>>>> an exception or an unexpected result. Any other way to support this
>>>> would
>>>> be too painful and slow.
>>>
>>> Are you referring to the problem of not being able to propagate a
>>> checked exception from a local method (when it would be safe to do so) on
>>> account not being able to declare a throws because you?re implementing an
>>> interface? If so, there?s a way around it, although it's kind of a hack.
>>> This is what we do at my company:
>>>
>>>   /**
>>>    * Throws the {@link Throwable} which may be a checked exception. This is generally not allowed in
>>>    * Java but okay in the VM. You probably don't want to use this unless you're really sure. You've
>>>    * been warned.
>>>    *
>>>    * @param throwable the exception to throw.
>>>    * @return this method does not return, the return type is specified to allow the caller to easily
>>>    *         construct a statement that completes abruptly.
>>>    *         e.g., {@code throw throwUnchecked(new IOException())}
>>>    */
>>>   public static RuntimeException throwUnchecked(Throwable throwable) {
>>>     throw Exceptions.<RuntimeException>doThrowUnchecked(throwable);
>>>   }
>>>
>>>   @SuppressWarnings("unchecked")
>>>   private static <T extends Throwable> T doThrowUnchecked(Throwable throwable) throws T {
>>>     throw (T) throwable;
>>>   }
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/d9a681d7/attachment.html>

From davidcholmes at aapt.net.au  Wed Jul  3 07:08:39 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 3 Jul 2013 21:08:39 +1000
Subject: [concurrency-interest] Java performance issue when
	runningvirtualized (KVM)
In-Reply-To: <4964337.352.1372775730460.JavaMail.andrig@worklaptop.miller.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEJFJOAA.davidcholmes@aapt.net.au>

So can you clarify which version had what performance on which VM?

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andrig Miller
  Sent: Wednesday, 3 July 2013 12:36 AM
  To: dholmes at ieee.org
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Java performance issue when runningvirtualized (KVM)


  Well, I should have looked at the source of the methods, but here is what I see when only using ReentrantLock in their supplied reproducer code, from a thread dump:

  "Thread-350" prio=10 tid=0x00007f54b8437000 nid=0x21a6 waiting on condition [0x00007f546b7f6000]
     java.lang.Thread.State: TIMED_WAITING (sleeping)
      at java.lang.Thread.sleep(Native Method)
      at java.lang.Thread.sleep(Thread.java:338)
      at test.MultiThreadedQ$4.run(MultiThreadedQ.java:252)
      at java.lang.Thread.run(Thread.java:722)

  That's what all the blocked threads look like.  When using ReentrantReadWriteLock or JDK 6, all the threads look like this:

  "Thread-350" prio=10 tid=0x00007f17e43af000 nid=0x1cbb waiting on condition [0x00007f15c9ad9000]
     java.lang.Thread.State: WAITING (parking)
      at sun.misc.Unsafe.park(Native Method)
      - parking to wait for  <0x0000000688a020a0> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
      at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
      at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
      at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:968)
      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1284)
      at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:627)
      at test.MultiThreadedQ$4.run(MultiThreadedQ.java:229)
      at java.lang.Thread.run(Thread.java:679)

  In the first case with JDK 7, we are not using the ReentrantReadWriteLock, but only depending on the internal locking of the PriorityBlockingQueue, which using ReentrantLock internally.

  So, my assumption was the implementation was different, so I should have looked at the code.

  Andy


------------------------------------------------------------------------------

    From: "David Holmes" <davidcholmes at aapt.net.au>
    To: "Andrig Miller" <anmiller at redhat.com>, Concurrency-interest at cs.oswego.edu
    Sent: Monday, July 1, 2013 6:25:45 PM
    Subject: RE: [concurrency-interest] Java performance issue when runningvirtualized (KVM)

    ? 
    Hi Andy,

    All of the blocking in ReentrantLock and ReentrantReadWriteLock is done via AbstractQueuedSynchronizer which uses LockSupport.park which uses Unsafe.park which will go into the VM's Parker implementation and use pthread_condwait, which in turn will use the futex implementation on Linux.

    So I am unclear what difference you are actually seeing in the source code ??

    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andrig Miller
      Sent: Tuesday, 2 July 2013 9:03 AM
      To: Concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Java performance issue when runningvirtualized (KVM)


      I work for Red Hat, in the middle-ware business unit, and recently we had a customer open a case against our KVM based virtualization, where they were experiencing an unusual performance issue, compared to bare metal performance.  The application happened to be a Java application, so I was asked to look into it.

      There was a simple application that they submitted with the issue that could reproduce the issue.  In my testing, I found that it was much better when running on Java 7 (they were using Java 6), but that there was still a substantial performance degradation.  After looking at the source, and doing some thread dumps, I discovered that they were using ReentrantReadWriteLock on some data that they were protecting before pushing that data into a PriorityBlockingQueue.

      It turns out that Java 7 was better then Java 6, because the internal locking in the PriorityBlockingQueue was using ReentrantLock, and the implementation of ReentrantLock was changed between Java 6 and 7 to no longer use the native method sun.misc.Unsafe.park.  In the reproducer code, there actually wasn't a reason to use the ReentrantReadWriteLock, and when I removed that, we were able to get almost bare metal performance again.

      Our Red Hat Enterprise Linux KVM guys had determined that the futex() system call that comes from the sun.misc.Unsafe.park native method was the culprit of the performance degradation (there is no an easy fix that crosses all CPU architecture at the OS level, by the way).  Since ReentrantLock no longer uses that native method, and exists entirely in user space, performance is great.

      So, I was wondering why ReentrantLock was modified to no longer use sun.misc.Unsafe.park, and ReentrantReadWriteLock was not?  If ReentrantReadWriteLock could be modified in a similar way, and no longer use the native method making the futex() system call, then we could get near to bare metal performance even with not changing their code.  Was this an oversight, or is there something about ReentrantReadWriteLock that prevents it from being modified in a similar way to ReentrantLock (all in user space, and no system calls).

      Thanks.

      -- 

      Andrig (Andy) Miller
      Global Platform Director for JBoss Middle-ware
      Red Hat, Inc.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/c3c5be3f/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Jul  3 07:15:50 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 3 Jul 2013 21:15:50 +1000
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CAEJX8or9_XoFbjv9Gr0UPrpH0WPrDS2CJ4qMud6iMEgyzbEa0w@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>

We just removed Thread.stop(Throwable) from Java 8. And good riddance to it.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Stanimir Simeonoff
  Sent: Wednesday, 3 July 2013 7:16 AM
  To: viktor ?lang
  Cc: Doug Lea; Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Interface CompletionStage


  Thread.stop(Throwable) does, though (but it has the security checks), so I was not my 1st choice.

  Stanimir


  On Wed, Jul 3, 2013 at 12:13 AM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

    Which does not exist on Android... (the throwsException method)



    On Tue, Jul 2, 2013 at 11:00 PM, Stanimir Simeonoff <stanimir at riflexo.com> wrote:

      This is a nice hack, but it's also available via Unsafe.throwException


      On Tue, Jul 2, 2013 at 6:18 PM, Scott Blum <dragonsinth at gmail.com> wrote:

        On Tue, Jul 2, 2013 at 6:34 AM, Doug Lea <dl at cs.oswego.edu> wrote:



          I'm not a big fan of this either, and was pretty close to choosing the
          two-method approach to overcome the javac-won't-let-me-rethrow problem of
          "exceptionally()". But I chose this instead because it is the only
          reasonable path to support stages of the form: take some action if either
          an exception or an unexpected result. Any other way to support this would
          be too painful and slow.


        Are you referring to the problem of not being able to propagate a checked exception from a local method (when it would be safe to do so) on account not being able to declare a throws because you?re implementing an interface? If so, there?s a way around it, although it's kind of a hack. This is what we do at my company:

  /**
   * Throws the {@link Throwable} which may be a checked exception. This is generally not allowed in
   * Java but okay in the VM. You probably don't want to use this unless you're really sure. You've
   * been warned.
   *
   * @param throwable the exception to throw.
   * @return this method does not return, the return type is specified to allow the caller to easily
   *         construct a statement that completes abruptly.
   *         e.g., {@code throw throwUnchecked(new IOException())}
   */
  public static RuntimeException throwUnchecked(Throwable throwable) {
    throw Exceptions.<RuntimeException>doThrowUnchecked(throwable);
  }

  @SuppressWarnings("unchecked")
  private static <T extends Throwable> T doThrowUnchecked(Throwable throwable) throws T {
    throw (T) throwable;
  }

        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest




      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest







    -- 

    Viktor Klang
    Director of Engineering
    Typesafe

    Twitter: @viktorklang

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/4a753a2e/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jul  3 07:20:05 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 03 Jul 2013 12:20:05 +0100
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<CAKh+yi9X3kxYY4MYNc-DR5nr4Ci7nCmm5pscc+aOtCu3Gr+C1Q@mail.gmail.com>
Message-ID: <51D408E5.8090703@oracle.com>

Nit picking, but Either[A,X].fold(A=>U,X=>U):U is not a catamorphism. It 
is a join.

The catamorphism will deal with any nesting of Either, and turn it into 
innermost X: Either[A,Either[A,.......X]]=>X, with X having at least one 
distinguished point.

But there are many discords throughout the collections API, so little 
point fixing just one.


Alex


On 02/07/2013 02:41, Jed Wesley-Smith wrote:
> ...
> There is one combinator that I find quite troubling:
>
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html#handle(java.util.function.BiFunction) 
> <http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html#handle%28java.util.function.BiFunction%29>
>
> It takes a BiFunction that takes either a T (and therefore a null 
> Throwable) or a Throwable (and therefore a null T). Could we please 
> not use nulls for a disjoint union? We could either supply two 
> Functions (one trom T and one from Throwable) or perhaps use a proper 
> disjoint union type* as the function argument. Lets not accept nulls 
> if we can avoid them.
>
> (BTW. this combinator is the catamorphism for Future, and is often 
> known as 'fold')
>
> cheers,
> jed.
>
> * for instance you are welcome to:
> https://bitbucket.org/atlassian/fugue/src/master/src/main/java/com/atlassian/fugue/Either.java
>
>
> On 2 July 2013 11:06, Zhong Yu <zhong.j.yu at gmail.com 
> <mailto:zhong.j.yu at gmail.com>> wrote:
>
>     On Mon, Jul 1, 2013 at 3:26 PM, Doug Lea <dl at cs.oswego.edu
>     <mailto:dl at cs.oswego.edu>> wrote:
>     >
>     > Last fall there was a lot of controversy about the API for
>     > class CompletableFuture. Some people demanded the
>     > presence of some methods, and some people demanded the
>     > absence of these same methods. I think it is safe to
>     > say that everyone with strong feelings about this
>     > was disappointed. At the time, I tabled this discussion
>     > pending further thought about possible options.
>     >
>     > Just barely (I hope!) before it is too late to do anything
>     > about this, I returned try to come to a better resolution.
>     > Here it is:
>     >
>     > New interface CompletionStage contains (almost) only the "fluent"
>     > methods (thenApply, etc). These are uncontroversial.
>     > Class CompletableFuture implements CompletionStage, and
>     > also the methods people may or may not want to
>     > allow users to use in any layered framework. However
>     > CompletableFuture is structured such that any subclass
>     > can disable any of the non-CompletionStage methods
>     > by overriding to throw UnsupoortedOperationException,
>     > and still use it for a perfectly compliant CompletionStage
>     > implementation. For example, for those who want/need to
>     > disable cancel(), might write...
>     >
>     >  class MyCompletableFuture extends CompletableFuture { ...
>     >     public boolean cancel(...) { throw new
>     UnsupportedOperationException();
>     > }
>     >   }
>     >   class MyCompletionStage implements CompletionStage {
>     >     MyCompletableFuture cf ...
>     >     // delegate all methods to cf;
>     >   }
>     >
>     > And many variations like this in which you may choose to
>     > create and use a (strictly internal) intentionally-crippled
>     > CompletableFuture subclass, but a perfectly fine and compliant
>     > CompletionStage implementation. Design-by-UOE is an odd extension
>     > policy, but better than all the others I know in coping with
>     this issue.
>     >
>     > Very little needed to be changed to pull this off.
>     > From the CompletableFuture view, just a few small additions
>     > that do not impact any existing usages:
>     >
>     > 1. Interface CompletionStage requires one more method
>     > than just the stage-based fluent ones, toCompletableFuture()
>     > that ensures a completely safe common currency for interoperability.
>     > (It is safe, because it may be implemented as a new stage,
>     > not necessarily a (casted) "return this", as it is in
>     > CompletableFuture itself.)
>     >
>     > 2. New method CompletionStage.whenCompleted plays a role similar to
>     > that of CompletableFuture.exceptionally but avoids a type-system
>     > problem: Method exceptionally() relied on the availability to
>     users of
>     > method completeExceptionally to get the equivalent of an
>     > exception rethrow if so desired. Without it being available
>     > in CompletionStage, there is no sanctioned way to do this.
>     > But whenCompleted can now be used in the rethrow case.
>     > Also, the whenCompleted and handle methods are available
>     > in async versions.
>     >
>     > 3. The little utility method
>     CompletableFuture.isCompletedExceptionally
>     > should have been there to begin with, but is now hard to
>     > live without.
>     >
>     > That's it. I think that these small changes will make
>     > a big difference in how CompletableFuture can be used
>     > as basic infrastructure for layered frameworks. And
>     > I cannot imagine any way in which it makes things worse
>     > for anyone. But if you do, please let me know very soon.
>     > (But please(!) do not ask for "just one more" change or feature!)
>
>     Just a trivial naming suggestion:
>
>         whenComplete(BiConsumer<T,Throwable>)
>     =>
>         finallyAccept(BiConsumer<T,Throwable>)
>
>
>         handle(BiFunction<T,Throwable,U>)
>     =>
>         finallyApply(BiFunction<T,Throwable,U>)
>
>
>     i.e. thenFoo() works on successful completion, finallyFoo() works on
>     any completion. The resulting chained calls make more sense to java
>     programmers:
>
>         compute()
>             .thenApply(...)
>             .thenApply(...)
>             .finallyAccept(...); // free resources
>
>     (would be nice too to have a finallyRun() method...)
>
>     Zhong Yu
>
>
>     >
>     > If I don't hear any soon, I'll try to make a case for it as
>     > a late change for JDK8.
>     >
>     >
>     > javadocs:
>     >
>     http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html
>     >
>     http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
>     >
>     > Sources:
>     >
>     http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletionStage.java?view=log
>     >
>     http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log
>     >
>     >
>     >
>     >
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/16a5a6ad/attachment-0001.html>

From aph at redhat.com  Wed Jul  3 08:09:26 2013
From: aph at redhat.com (Andrew Haley)
Date: Wed, 03 Jul 2013 13:09:26 +0100
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
Message-ID: <51D41476.3070204@redhat.com>

On 07/03/2013 12:15 PM, David Holmes wrote:
> We just removed Thread.stop(Throwable) from Java 8.

Hallelujah!

Andrew.


From viktor.klang at gmail.com  Wed Jul  3 08:35:54 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 3 Jul 2013 14:35:54 +0200
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <51D1E60A.2090606@cs.oswego.edu>
References: <51D1E60A.2090606@cs.oswego.edu>
Message-ID: <CANPzfU_yLZR3SWciDO0edBhc-Dff+WptALHQswRvEZT2Dy6QSw@mail.gmail.com>

Doug,

I like the direction; this creates an opportunity for integration of many
distinct Future implementations (Guava, Netty etc).

As for the naming I'd personally go with the monadic names aligning with
Stream & Optional, but these might make more sense in their context (async
computation / flow).

Cheers,
?


On Mon, Jul 1, 2013 at 10:26 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> Last fall there was a lot of controversy about the API for
> class CompletableFuture. Some people demanded the
> presence of some methods, and some people demanded the
> absence of these same methods. I think it is safe to
> say that everyone with strong feelings about this
> was disappointed. At the time, I tabled this discussion
> pending further thought about possible options.
>
> Just barely (I hope!) before it is too late to do anything
> about this, I returned try to come to a better resolution.
> Here it is:
>
> New interface CompletionStage contains (almost) only the "fluent"
> methods (thenApply, etc). These are uncontroversial.
> Class CompletableFuture implements CompletionStage, and
> also the methods people may or may not want to
> allow users to use in any layered framework. However
> CompletableFuture is structured such that any subclass
> can disable any of the non-CompletionStage methods
> by overriding to throw UnsupoortedOperationException,
> and still use it for a perfectly compliant CompletionStage
> implementation. For example, for those who want/need to
> disable cancel(), might write...
>
>  class MyCompletableFuture extends CompletableFuture { ...
>     public boolean cancel(...) { throw new UnsupportedOperationException(*
> *); }
>   }
>   class MyCompletionStage implements CompletionStage {
>     MyCompletableFuture cf ...
>     // delegate all methods to cf;
>   }
>
> And many variations like this in which you may choose to
> create and use a (strictly internal) intentionally-crippled
> CompletableFuture subclass, but a perfectly fine and compliant
> CompletionStage implementation. Design-by-UOE is an odd extension
> policy, but better than all the others I know in coping with this issue.
>
> Very little needed to be changed to pull this off.
> From the CompletableFuture view, just a few small additions
> that do not impact any existing usages:
>
> 1. Interface CompletionStage requires one more method
> than just the stage-based fluent ones, toCompletableFuture()
> that ensures a completely safe common currency for interoperability.
> (It is safe, because it may be implemented as a new stage,
> not necessarily a (casted) "return this", as it is in
> CompletableFuture itself.)
>
> 2. New method CompletionStage.whenCompleted plays a role similar to
> that of CompletableFuture.**exceptionally but avoids a type-system
> problem: Method exceptionally() relied on the availability to users of
> method completeExceptionally to get the equivalent of an
> exception rethrow if so desired. Without it being available
> in CompletionStage, there is no sanctioned way to do this.
> But whenCompleted can now be used in the rethrow case.
> Also, the whenCompleted and handle methods are available
> in async versions.
>
> 3. The little utility method CompletableFuture.**isCompletedExceptionally
> should have been there to begin with, but is now hard to
> live without.
>
> That's it. I think that these small changes will make
> a big difference in how CompletableFuture can be used
> as basic infrastructure for layered frameworks. And
> I cannot imagine any way in which it makes things worse
> for anyone. But if you do, please let me know very soon.
> (But please(!) do not ask for "just one more" change or feature!)
>
> If I don't hear any soon, I'll try to make a case for it as
> a late change for JDK8.
>
>
> javadocs:
> http://gee.cs.oswego.edu/dl/**jsr166/dist/docs/java/util/**
> concurrent/CompletionStage.**html<http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletionStage.html>
> http://gee.cs.oswego.edu/dl/**jsr166/dist/docs/java/util/**
> concurrent/CompletableFuture.**html<http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html>
>
> Sources:
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> main/java/util/concurrent/**CompletionStage.java?view=log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletionStage.java?view=log>
> http://gee.cs.oswego.edu/cgi-**bin/viewcvs.cgi/jsr166/src/**
> main/java/util/concurrent/**CompletableFuture.java?view=**log<http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?view=log>
>
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/bb0a4917/attachment.html>

From anmiller at redhat.com  Wed Jul  3 09:30:40 2013
From: anmiller at redhat.com (Andrig Miller)
Date: Wed, 3 Jul 2013 09:30:40 -0400 (EDT)
Subject: [concurrency-interest] Java performance issue when
 runningvirtualized (KVM)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEJFJOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEJFJOAA.davidcholmes@aapt.net.au>
Message-ID: <20609239.481.1372858238857.JavaMail.andrig@worklaptop.miller.org>

The best performance was with JDK 7 (Red Hat's OpenJDK 7 implementation on RHEL 6.4), which is the first thread dump example. 

Andy 

----- Original Message -----

> From: "David Holmes" <davidcholmes at aapt.net.au>
> To: "Andrig Miller" <anmiller at redhat.com>
> Cc: Concurrency-interest at cs.oswego.edu
> Sent: Wednesday, July 3, 2013 5:08:39 AM
> Subject: RE: [concurrency-interest] Java performance issue when
> runningvirtualized (KVM)

> ?
> So can you clarify which version had what performance on which VM?

> David
> > -----Original Message-----
> 
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of
> > Andrig Miller
> 
> > Sent: Wednesday, 3 July 2013 12:36 AM
> 
> > To: dholmes at ieee.org
> 
> > Cc: Concurrency-interest at cs.oswego.edu
> 
> > Subject: Re: [concurrency-interest] Java performance issue when
> > runningvirtualized (KVM)
> 

> > Well, I should have looked at the source of the methods, but here
> > is
> > what I see when only using ReentrantLock in their supplied
> > reproducer code, from a thread dump:
> 

> > "Thread-350" prio=10 tid=0x00007f54b8437000 nid=0x21a6 waiting on
> > condition [0x00007f546b7f6000]
> 
> > java.lang.Thread.State: TIMED_WAITING (sleeping)
> 
> > at java.lang.Thread.sleep(Native Method)
> 
> > at java.lang.Thread.sleep(Thread.java:338)
> 
> > at test.MultiThreadedQ$4.run(MultiThreadedQ.java:252)
> 
> > at java.lang.Thread.run(Thread.java:722)
> 

> > That's what all the blocked threads look like. When using
> > ReentrantReadWriteLock or JDK 6, all the threads look like this:
> 

> > "Thread-350" prio=10 tid=0x00007f17e43af000 nid=0x1cbb waiting on
> > condition [0x00007f15c9ad9000]
> 
> > java.lang.Thread.State: WAITING (parking)
> 
> > at sun.misc.Unsafe.park(Native Method)
> 
> > - parking to wait for <0x0000000688a020a0> (a
> > java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
> 
> > at
> > java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
> 
> > at
> > java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
> 
> > at
> > java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:968)
> 
> > at
> > java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1284)
> 
> > at
> > java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:627)
> 
> > at test.MultiThreadedQ$4.run(MultiThreadedQ.java:229)
> 
> > at java.lang.Thread.run(Thread.java:679)
> 

> > In the first case with JDK 7, we are not using the
> > ReentrantReadWriteLock, but only depending on the internal locking
> > of the PriorityBlockingQueue, which using ReentrantLock internally.
> 

> > So, my assumption was the implementation was different, so I should
> > have looked at the code.
> 

> > Andy
> 

> > ----- Original Message -----
> 

> > > From: "David Holmes" <davidcholmes at aapt.net.au>
> > 
> 
> > > To: "Andrig Miller" <anmiller at redhat.com>,
> > > Concurrency-interest at cs.oswego.edu
> > 
> 
> > > Sent: Monday, July 1, 2013 6:25:45 PM
> > 
> 
> > > Subject: RE: [concurrency-interest] Java performance issue when
> > > runningvirtualized (KVM)
> > 
> 

> > > ?
> > 
> 
> > > Hi Andy,
> > 
> 

> > > All of the blocking in ReentrantLock and ReentrantReadWriteLock
> > > is
> > > done via AbstractQueuedSynchronizer which uses LockSupport.park
> > > which uses Unsafe.park which will go into the VM's Parker
> > > implementation and use pthread_condwait, which in turn will use
> > > the
> > > futex implementation on Linux.
> > 
> 

> > > So I am unclear what difference you are actually seeing in the
> > > source
> > > code ??
> > 
> 

> > > David Holmes
> > 
> 
> > > > -----Original Message-----
> > > 
> > 
> 
> > > > From: concurrency-interest-bounces at cs.oswego.edu
> > > > [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> > > > Of
> > > > Andrig Miller
> > > 
> > 
> 
> > > > Sent: Tuesday, 2 July 2013 9:03 AM
> > > 
> > 
> 
> > > > To: Concurrency-interest at cs.oswego.edu
> > > 
> > 
> 
> > > > Subject: [concurrency-interest] Java performance issue when
> > > > runningvirtualized (KVM)
> > > 
> > 
> 

> > > > I work for Red Hat, in the middle-ware business unit, and
> > > > recently
> > > > we
> > > > had a customer open a case against our KVM based
> > > > virtualization,
> > > > where they were experiencing an unusual performance issue,
> > > > compared
> > > > to bare metal performance. The application happened to be a
> > > > Java
> > > > application, so I was asked to look into it.
> > > 
> > 
> 

> > > > There was a simple application that they submitted with the
> > > > issue
> > > > that could reproduce the issue. In my testing, I found that it
> > > > was
> > > > much better when running on Java 7 (they were using Java 6),
> > > > but
> > > > that there was still a substantial performance degradation.
> > > > After
> > > > looking at the source, and doing some thread dumps, I
> > > > discovered
> > > > that they were using ReentrantReadWriteLock on some data that
> > > > they
> > > > were protecting before pushing that data into a
> > > > PriorityBlockingQueue.
> > > 
> > 
> 

> > > > It turns out that Java 7 was better then Java 6, because the
> > > > internal
> > > > locking in the PriorityBlockingQueue was using ReentrantLock,
> > > > and
> > > > the implementation of ReentrantLock was changed between Java 6
> > > > and
> > > > 7
> > > > to no longer use the native method sun.misc.Unsafe.park. In the
> > > > reproducer code, there actually wasn't a reason to use the
> > > > ReentrantReadWriteLock, and when I removed that, we were able
> > > > to
> > > > get
> > > > almost bare metal performance again.
> > > 
> > 
> 

> > > > Our Red Hat Enterprise Linux KVM guys had determined that the
> > > > futex()
> > > > system call that comes from the sun.misc.Unsafe.park native
> > > > method
> > > > was the culprit of the performance degradation (there is no an
> > > > easy
> > > > fix that crosses all CPU architecture at the OS level, by the
> > > > way).
> > > > Since ReentrantLock no longer uses that native method, and
> > > > exists
> > > > entirely in user space, performance is great.
> > > 
> > 
> 

> > > > So, I was wondering why ReentrantLock was modified to no longer
> > > > use
> > > > sun.misc.Unsafe.park, and ReentrantReadWriteLock was not? If
> > > > ReentrantReadWriteLock could be modified in a similar way, and
> > > > no
> > > > longer use the native method making the futex() system call,
> > > > then
> > > > we
> > > > could get near to bare metal performance even with not changing
> > > > their code. Was this an oversight, or is there something about
> > > > ReentrantReadWriteLock that prevents it from being modified in
> > > > a
> > > > similar way to ReentrantLock (all in user space, and no system
> > > > calls).
> > > 
> > 
> 

> > > > Thanks.
> > > 
> > 
> 

> > > > --
> > > 
> > 
> 

> > > > Andrig (Andy) Miller
> > > 
> > 
> 
> > > > Global Platform Director for JBoss Middle-ware
> > > 
> > 
> 
> > > > Red Hat, Inc.
> > > 
> > 
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/d0ab72f1/attachment-0001.html>

From kyungkoo_yoon at tmax.co.kr  Wed Jul  3 11:10:12 2013
From: kyungkoo_yoon at tmax.co.kr (Yoon Kyung Koo)
Date: Thu, 4 Jul 2013 00:10:12 +0900
Subject: [concurrency-interest] java.lang.StackOverflowError on
	ConcurrentHashMap V8
Message-ID: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>

Hi, all.

I'm testing the ConcurrentHashMap V8 (backport) on JDK 6 and JDK 7.
I'm experiencing this problem..

ConcurrentHashMapPerfTest for JDK 8 in JDK 6 -----------------
.testV8 prestart took 18 (ms)
Exception in thread "Thread-31" java.lang.StackOverflowError
	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2144)
	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)

my test program only does this routine in 200 threads. have some body experience such problem?

        public void run() {
            try {
                Thread.sleep(500L);

                for (int i = 0; i < 500000; i++) {
                    Integer str = null;
                    if (!doPut) {
                        while (true) {
                            str = map.get(new Integer(i));
                            if (str != null) {
                                break;
                            }
                            Thread.sleep(250L);
                        }
                    }

                    map.put(new Integer(i), str == null ? new Integer(i) : new Integer(str.intValue() + 1));
                }
            } catch(InterruptedException e){
                unexpectedException();
            }
            latch.countDown();
        }


-- 
--------------------
 Software Innovation Driver
 Researcher & Executive Director / WAS Lab / TmaxSoft R&D Center
 PGP  http://www.javadom.com/personal/yoonforhatgmaildotcom.asc





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/85249d87/attachment.html>

From nathan.reynolds at oracle.com  Wed Jul  3 12:27:06 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 03 Jul 2013 09:27:06 -0700
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D41476.3070204@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com>
Message-ID: <51D450DA.6060500@oracle.com>

The Java Debug Inferface's (JDI) API ThreadReference.stop() probably 
uses Thread.stop() or similar functionality.  Is ThreadReference.stop() 
being removed too?

http://docs.oracle.com/javase/7/docs/jdk/api/jpda/jdi/com/sun/jdi/ThreadReference.html#stop%28com.sun.jdi.ObjectReference%29

The Java Debug Wire Protocol (JDWP) has a Stop command in the 
ThreadReference Command Set.  Is this being removed too?

http://docs.oracle.com/javase/7/docs/platform/jpda/jdwp/jdwp-protocol.html#JDWP_ThreadReference_Stop

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/3/2013 5:09 AM, Andrew Haley wrote:
> On 07/03/2013 12:15 PM, David Holmes wrote:
>> We just removed Thread.stop(Throwable) from Java 8.
> Hallelujah!
>
> Andrew.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/29034480/attachment.html>

From Alan.Bateman at oracle.com  Wed Jul  3 13:35:57 2013
From: Alan.Bateman at oracle.com (Alan Bateman)
Date: Wed, 03 Jul 2013 18:35:57 +0100
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D450DA.6060500@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
Message-ID: <51D460FD.7090803@oracle.com>

On 03/07/2013 17:27, Nathan Reynolds wrote:
> The Java Debug Inferface's (JDI) API ThreadReference.stop() probably 
> uses Thread.stop() or similar functionality.  Is 
> ThreadReference.stop() being removed too?

Just to clarify that we haven't quite removed Thread.stop(Throwable) in 
8, rather than it has been changed to always throw UOE.

TBD as to whether there is a need to retire it from the debugger/tooling 
APIs. A small complication there is the debugger and debuggee being 
different versions.

-Alan.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/4c140a30/attachment.html>

From russel at winder.org.uk  Wed Jul  3 16:18:23 2013
From: russel at winder.org.uk (Russel Winder)
Date: Wed, 03 Jul 2013 21:18:23 +0100
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D450DA.6060500@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
Message-ID: <1372882703.7993.6.camel@anglides.winder.org.uk>

The day a deprecated thing actually gets removed from the Java
distribution will be a day for real celebration.

-- 
Russel.
=============================================================================
Dr Russel Winder      t: +44 20 7585 2200   voip: sip:russel.winder at ekiga.net
41 Buckmaster Road    m: +44 7770 465 077   xmpp: russel at winder.org.uk
London SW11 1EN, UK   w: www.russel.org.uk  skype: russel_winder
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/8c5be3eb/attachment.bin>

From forax at univ-mlv.fr  Wed Jul  3 16:52:37 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 03 Jul 2013 22:52:37 +0200
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <1372882703.7993.6.camel@anglides.winder.org.uk>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
Message-ID: <51D48F15.2060906@univ-mlv.fr>

On 07/03/2013 10:18 PM, Russel Winder wrote:
> The day a deprecated thing actually gets removed from the Java
> distribution will be a day for real celebration.

May I ask why ?
Given the ridiculous number of deprecated methods/classes etc. compared 
to existing ones,
what is the point of actually removing something.

R?mi


From viktor.klang at gmail.com  Wed Jul  3 17:14:23 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 3 Jul 2013 23:14:23 +0200
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D48F15.2060906@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
Message-ID: <CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>

What would be interesting IMO is a JDK Reloaded, based on what we've
learned since Java was conceived.

Cheers,
?



On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> On 07/03/2013 10:18 PM, Russel Winder wrote:
>
>> The day a deprecated thing actually gets removed from the Java
>> distribution will be a day for real celebration.
>>
>
> May I ask why ?
> Given the ridiculous number of deprecated methods/classes etc. compared to
> existing ones,
> what is the point of actually removing something.
>
> R?mi
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/146bd597/attachment-0001.html>

From studdugie at gmail.com  Wed Jul  3 17:35:14 2013
From: studdugie at gmail.com (Dane Foster)
Date: Wed, 3 Jul 2013 17:35:14 -0400
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
Message-ID: <CA+WxinJkDe08eNoeWBixc+=C50aWs+Km_3ggj-gq92e9Sb80fA@mail.gmail.com>

It's a dream of mine too. Unfortunately, it's probably a pipe dream. But,
if it ever happens, on the day of its GA release I'll get Duke tattooed on
my forehead!

Dane


On Wed, Jul 3, 2013 at 5:14 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:

> What would be interesting IMO is a JDK Reloaded, based on what we've
> learned since Java was conceived.
>
> Cheers,
> ?
>
>
>
> On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>> On 07/03/2013 10:18 PM, Russel Winder wrote:
>>
>>> The day a deprecated thing actually gets removed from the Java
>>> distribution will be a day for real celebration.
>>>
>>
>> May I ask why ?
>> Given the ridiculous number of deprecated methods/classes etc. compared
>> to existing ones,
>> what is the point of actually removing something.
>>
>> R?mi
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/aa5ccb90/attachment.html>

From nathan.reynolds at oracle.com  Wed Jul  3 18:13:48 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 03 Jul 2013 15:13:48 -0700
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
Message-ID: <51D4A21C.4030509@oracle.com>

20 years later...

What would be interesting IMO is a JDK Revolutions, based on what we've 
learned since Java Reloaded was conceived.

Cheers,
?

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/3/2013 2:14 PM, ?iktor ?lang wrote:
> What would be interesting IMO is a JDK Reloaded, based on what we've 
> learned since Java was conceived.
>
> Cheers,
> ?
>
>
>
> On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     On 07/03/2013 10:18 PM, Russel Winder wrote:
>
>         The day a deprecated thing actually gets removed from the Java
>         distribution will be a day for real celebration.
>
>
>     May I ask why ?
>     Given the ridiculous number of deprecated methods/classes etc.
>     compared to existing ones,
>     what is the point of actually removing something.
>
>     R?mi
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/57f125ea/attachment.html>

From viktor.klang at gmail.com  Wed Jul  3 18:24:03 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 4 Jul 2013 00:24:03 +0200
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D4A21C.4030509@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
	<51D4A21C.4030509@oracle.com>
Message-ID: <CANPzfU_fKhsBzxTcvYhfXAjDyTiLDU_AnoTkYxfYxT-7wQOLxA@mail.gmail.com>

Haha, let's just hope that JDK Reloaded doesn't end with:

/**
 * To be concluded
 */

Cheers,
?


On Thu, Jul 4, 2013 at 12:13 AM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  20 years later...
>
> What would be interesting IMO is a JDK Revolutions, based on what we've
> learned since Java Reloaded was conceived.
>
>  Cheers,
> ?
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 7/3/2013 2:14 PM, ?iktor ?lang wrote:
>
> What would be interesting IMO is a JDK Reloaded, based on what we've
> learned since Java was conceived.
>
>  Cheers,
> ?
>
>
>
> On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>> On 07/03/2013 10:18 PM, Russel Winder wrote:
>>
>>> The day a deprecated thing actually gets removed from the Java
>>> distribution will be a day for real celebration.
>>>
>>
>>  May I ask why ?
>> Given the ridiculous number of deprecated methods/classes etc. compared
>> to existing ones,
>> what is the point of actually removing something.
>>
>> R?mi
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
>  --
> *Viktor Klang*
> *Director of Engineering*
>  Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/1c199226/attachment.html>

From gregg at wonderly.org  Wed Jul  3 18:25:14 2013
From: gregg at wonderly.org (Gregg Wonderly)
Date: Wed, 03 Jul 2013 17:25:14 -0500
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <CA+WxinJkDe08eNoeWBixc+=C50aWs+Km_3ggj-gq92e9Sb80fA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
	<CA+WxinJkDe08eNoeWBixc+=C50aWs+Km_3ggj-gq92e9Sb80fA@mail.gmail.com>
Message-ID: <51D4A4CA.3060201@wonderly.org>

If software is functioning on a particular release of the JVM, then why would it 
matter if a later version no longer supported a feature it was using?  It would 
seem that at the point that a software company wanted to move to a newer 
release, that they'd be prepared to "change" their software to work with the new 
release.  Removing cruft allows the JVM, overall, to stop having to "support" 
that cruft, and thus allows it to be morphed more completely.  I know that many 
people would prefer to have a .jar (I would too), that I could carry around on 
an thumb drive, and run anywhere at any time.  The silly issue, is that neither 
Sun, nor Oracle has really endorsed the desktop work or "small software" world 
in their practices of dealing with suggestions through the JCP or otherwise.  
Things that 10,000,000 people need for small applications never seem to be as 
import as what 1,000 paying-a-license-fee enterprise companies want.

So, in the end, I'm guessing that Java, will eventually die as a fat, obese, 
overweight language with far to many things "broken" and not fixable, because we 
are too concerned about those 100,000,000 .jar files that people carry around, 
instead of being interested in the true capabilities of the language.

The other JVM languages have demonstrated that all kinds of things are 
possible.  The problem is, that we'd all like to not have to "learn" another 
language and "rewrite" all of our software and libraries to run there, once 
again...  If we fix java periodically, I think that's a lot less rewriting than 
if we eventually let Java "die" because we don't keep up with cleaning out the 
cruft.

Gregg Wonderly

On 7/3/2013 4:35 PM, Dane Foster wrote:
> It's a dream of mine too. Unfortunately, it's probably a pipe dream. But, if 
> it ever happens, on the day of its GA release I'll get Duke tattooed on my 
> forehead!
>
> Dane
>
>
> On Wed, Jul 3, 2013 at 5:14 PM, ?iktor ?lang <viktor.klang at gmail.com 
> <mailto:viktor.klang at gmail.com>> wrote:
>
>     What would be interesting IMO is a JDK Reloaded, based on what we've
>     learned since Java was conceived.
>
>     Cheers,
>     ?
>
>
>
>     On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr
>     <mailto:forax at univ-mlv.fr>> wrote:
>
>         On 07/03/2013 10:18 PM, Russel Winder wrote:
>
>             The day a deprecated thing actually gets removed from the Java
>             distribution will be a day for real celebration.
>
>
>         May I ask why ?
>         Given the ridiculous number of deprecated methods/classes etc.
>         compared to existing ones,
>         what is the point of actually removing something.
>
>         R?mi
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>     -- 
>     *Viktor Klang*
>     /Director of Engineering/
>     Typesafe <http://www.typesafe.com/>
>
>     Twitter: @viktorklang
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/98849bb8/attachment-0001.html>

From gregg at wonderly.org  Wed Jul  3 18:31:16 2013
From: gregg at wonderly.org (Gregg Wonderly)
Date: Wed, 03 Jul 2013 17:31:16 -0500
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D4A21C.4030509@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
	<51D4A21C.4030509@oracle.com>
Message-ID: <51D4A634.6010805@wonderly.org>

And if you look at .Net in particular, there is a great example of how "old 
APIs", in the form of the original DOS functions, and structures, have continued 
to be supported in some way, and now, it's a huge ball of rubber bands and yarn, 
all tangled up and extremely difficult and time consuming to use because they've 
never removed the cruft.

New APIs are getting rolled on top of old stuff and wrapped around and between 
things and creating countless problems for the developer to figure out what is 
actually the current state of the art.   There really should only ever be the 
best current way, and the previous best visible in any language platform.   
Having too many ways is what eventually complicates and chance and successfully 
changing the technologies, because there will be breakage is such complex and 
weird ways.

I'll also point out things like the JMM enabled JVM/Jit optimizations as an 
example of things that Java gets advantages with, but which developers have no 
way to clearly understand and prevent, because "old code" still has the "old 
way" of doing it spread all over the place...

Gregg Wonderly

On 7/3/2013 5:13 PM, Nathan Reynolds wrote:
> 20 years later...
>
> What would be interesting IMO is a JDK Revolutions, based on what we've 
> learned since Java Reloaded was conceived.
>
> Cheers,
> ?
>
> Nathan Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> 
> | Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 7/3/2013 2:14 PM, ?iktor ?lang wrote:
>> What would be interesting IMO is a JDK Reloaded, based on what we've learned 
>> since Java was conceived.
>>
>> Cheers,
>> ?
>>
>>
>>
>> On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr 
>> <mailto:forax at univ-mlv.fr>> wrote:
>>
>>     On 07/03/2013 10:18 PM, Russel Winder wrote:
>>
>>         The day a deprecated thing actually gets removed from the Java
>>         distribution will be a day for real celebration.
>>
>>
>>     May I ask why ?
>>     Given the ridiculous number of deprecated methods/classes etc. compared
>>     to existing ones,
>>     what is the point of actually removing something.
>>
>>     R?mi
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> -- 
>> *Viktor Klang*
>> /Director of Engineering/
>> Typesafe <http://www.typesafe.com/>
>>
>> Twitter: @viktorklang
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/af79c793/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jul  3 18:33:18 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 03 Jul 2013 23:33:18 +0100
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <CANPzfU_fKhsBzxTcvYhfXAjDyTiLDU_AnoTkYxfYxT-7wQOLxA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
	<CANPzfU-2irZnu=LxXH5r_8uLUjJtxho4-KRvSYGe_uqbrrKfnw@mail.gmail.com>
	<51D4A21C.4030509@oracle.com>
	<CANPzfU_fKhsBzxTcvYhfXAjDyTiLDU_AnoTkYxfYxT-7wQOLxA@mail.gmail.com>
Message-ID: <51D4A6AE.20406@oracle.com>

You mean, JDK Overridden ends with:

/**
  * TODO: conclude
  */


Alex

On 03/07/2013 23:24, ?iktor ?lang wrote:
> Haha, let's just hope that JDK Reloaded doesn't end with:
>
> /**
>  * To be concluded
>  */
>
> Cheers,
> ?
>
>
> On Thu, Jul 4, 2013 at 12:13 AM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     20 years later...
>
>     What would be interesting IMO is a JDK Revolutions, based on what
>     we've learned since Java Reloaded was conceived.
>
>     Cheers,
>     ?
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 7/3/2013 2:14 PM, ?iktor ?lang wrote:
>>     What would be interesting IMO is a JDK Reloaded, based on what
>>     we've learned since Java was conceived.
>>
>>     Cheers,
>>     ?
>>
>>
>>
>>     On Wed, Jul 3, 2013 at 10:52 PM, Remi Forax <forax at univ-mlv.fr
>>     <mailto:forax at univ-mlv.fr>> wrote:
>>
>>         On 07/03/2013 10:18 PM, Russel Winder wrote:
>>
>>             The day a deprecated thing actually gets removed from the
>>             Java
>>             distribution will be a day for real celebration.
>>
>>
>>         May I ask why ?
>>         Given the ridiculous number of deprecated methods/classes
>>         etc. compared to existing ones,
>>         what is the point of actually removing something.
>>
>>         R?mi
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     -- 
>>     *Viktor Klang*
>>     /Director of Engineering/
>>     Typesafe <http://www.typesafe.com/>
>>
>>     Twitter: @viktorklang
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/29daba3a/attachment-0001.html>

From martinrb at google.com  Wed Jul  3 21:41:42 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 3 Jul 2013 18:41:42 -0700
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
Message-ID: <CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>

This is not a known problem.  Can you identify whether your problem is
specific to the backport?  Do you have a small complete test case?


On Wed, Jul 3, 2013 at 8:10 AM, Yoon Kyung Koo <kyungkoo_yoon at tmax.co.kr>wrote:

> Hi, all.
>
> I'm testing the ConcurrentHashMap V8 (backport) on JDK 6 and JDK 7.
> I'm experiencing this problem..
>
> ConcurrentHashMapPerfTest for JDK 8 in JDK 6 -----------------
> .testV8 prestart took 18 (ms)
> Exception in thread "Thread-31" java.lang.StackOverflowError
> at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2144)
> at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
> at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
> at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
>
> my test program only does this routine in 200 threads. have some body
> experience such problem?
>
>         public void run() {
>             try {
>                 Thread.sleep(500L);
>
>                 for (int i = 0; i < 500000; i++) {
>                     Integer str = null;
>                     if (!doPut) {
>                         while (true) {
>                             str = map.get(new Integer(i));
>                             if (str != null) {
>                                 break;
>                             }
>                             Thread.sleep(250L);
>                         }
>                     }
>
>                     map.put(new Integer(i), str == null ? new Integer(i) :
> new Integer(str.intValue() + 1));
>                 }
>             } catch(InterruptedException e){
>                 unexpectedException();
>             }
>             latch.countDown();
>         }
>
>
> --
> --------------------
>  Software Innovation Driver
>  Researcher & Executive Director / WAS Lab / TmaxSoft R&D Center
>  PGP  http://www.javadom.com/personal/yoonforhatgmaildotcom.asc
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130703/194eea6d/attachment.html>

From kyungkoo_yoon at tmax.co.kr  Wed Jul  3 21:45:22 2013
From: kyungkoo_yoon at tmax.co.kr (Yoon Kyung Koo)
Date: Thu, 4 Jul 2013 10:45:22 +0900
Subject: [concurrency-interest] java.lang.StackOverflowError on
	ConcurrentHashMap V8
In-Reply-To: <CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
Message-ID: <F21B3138-5CD7-4A21-B4EE-E1830621BC76@tmax.co.kr>

I'll attach the test case. It's simple.



Thanks.

-- 
--------------------
 Software Innovation Driver
 Researcher & Executive Director / WAS Lab / TmaxSoft R&D Center
 PGP  http://www.javadom.com/personal/yoonforhatgmaildotcom.asc





2013. 7. 4., ?? 10:41, Martin Buchholz <martinrb at google.com> ??:

> This is not a known problem.  Can you identify whether your problem is specific to the backport?  Do you have a small complete test case?
> 
> 
> On Wed, Jul 3, 2013 at 8:10 AM, Yoon Kyung Koo <kyungkoo_yoon at tmax.co.kr> wrote:
> Hi, all.
> 
> I'm testing the ConcurrentHashMap V8 (backport) on JDK 6 and JDK 7.
> I'm experiencing this problem..
> 
> ConcurrentHashMapPerfTest for JDK 8 in JDK 6 -----------------
> .testV8 prestart took 18 (ms)
> Exception in thread "Thread-31" java.lang.StackOverflowError
> 	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2144)
> 	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
> 	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
> 	at ConcurrentHashMapV8$ForwardingNode.find(ConcurrentHashMapV8.java:2153)
> 
> my test program only does this routine in 200 threads. have some body experience such problem?
> 
>         public void run() {
>             try {
>                 Thread.sleep(500L);
> 
>                 for (int i = 0; i < 500000; i++) {
>                     Integer str = null;
>                     if (!doPut) {
>                         while (true) {
>                             str = map.get(new Integer(i));
>                             if (str != null) {
>                                 break;
>                             }
>                             Thread.sleep(250L);
>                         }
>                     }
> 
>                     map.put(new Integer(i), str == null ? new Integer(i) : new Integer(str.intValue() + 1));
>                 }
>             } catch(InterruptedException e){
>                 unexpectedException();
>             }
>             latch.countDown();
>         }
> 
> 
> -- 
> --------------------
>  Software Innovation Driver
>  Researcher & Executive Director / WAS Lab / TmaxSoft R&D Center
>  PGP  http://www.javadom.com/personal/yoonforhatgmaildotcom.asc
> 
> 
> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/a75a2f2a/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: ConcurrentHashMapPerfTest.java
Type: application/octet-stream
Size: 5846 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/a75a2f2a/attachment.obj>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/a75a2f2a/attachment-0001.html>

From russel at winder.org.uk  Thu Jul  4 02:51:45 2013
From: russel at winder.org.uk (Russel Winder)
Date: Thu, 04 Jul 2013 07:51:45 +0100
Subject: [concurrency-interest] Removing Thread.stop()
In-Reply-To: <51D48F15.2060906@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCMEJFJOAA.davidcholmes@aapt.net.au>
	<51D41476.3070204@redhat.com> <51D450DA.6060500@oracle.com>
	<1372882703.7993.6.camel@anglides.winder.org.uk>
	<51D48F15.2060906@univ-mlv.fr>
Message-ID: <1372920705.7993.20.camel@anglides.winder.org.uk>

On Wed, 2013-07-03 at 22:52 +0200, Remi Forax wrote:
[?]
> May I ask why ?

The term deprecated is supposed to mean "no longer available, will be
removed in the next release" (or at least a reasonably short time soon).
It is supposed to be a tool for getting rid of incorrect API components
(cf. the dread Thread methods).

Major releases should be points at which breaking changes are
introduced. Java does actually do this, thereby breaking backward
compatibility, whilst at the same time saying we must maintain backward
compatibility. It's the hypocrisy that is the real problem with Java
development.  

If Thread.stop cannot be removed from Java, it is a signal by OpenJDK,
Oracle, IBM, and JCP EC that Java is officially bloatware destined for
an early grave (*), to be replaced by Scala, Kotlin, Ceylon, Groovy,
etc.

> Given the ridiculous number of deprecated methods/classes etc. compared 
> to existing ones,
> what is the point of actually removing something.

Because they are deprecated (and have been for 15 years or more in some
cases) and so are being advertised as "will go". They therefore should
go.

(*) Somewhat like COBOL, no-one uses it, but there is an awful lot of it
around.

-- 
Russel.
=============================================================================
Dr Russel Winder      t: +44 20 7585 2200   voip: sip:russel.winder at ekiga.net
41 Buckmaster Road    m: +44 7770 465 077   xmpp: russel at winder.org.uk
London SW11 1EN, UK   w: www.russel.org.uk  skype: russel_winder
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/029f235c/attachment-0001.bin>

From valentin.male.kovalenko at gmail.com  Thu Jul  4 06:19:23 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Thu, 4 Jul 2013 14:19:23 +0400
Subject: [concurrency-interest] Removing Thread.stop()
Message-ID: <CAO-wXw+6uVvAPLwUGVAknVSpp9z_=qcQcaxQFSGqgQ=05gu4Zg@mail.gmail.com>

Russel Winder wrote:
>>Major releases should be points at which breaking changes are
introduced.

I completely agree with you! It would be nice if API that was deprecated in
major_release_N would be removed in major_release_N+1 or at least as soon
as we hit the EOL of major_release_N
--
Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/2625b097/attachment.html>

From dl at cs.oswego.edu  Thu Jul  4 06:59:03 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 04 Jul 2013 06:59:03 -0400
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
Message-ID: <51D55577.6050706@cs.oswego.edu>

On 07/03/13 21:41, Martin Buchholz wrote:
> This is not a known problem.  Can you identify whether your problem is specific
> to the backport?
>

I can't replicate this on the current CHMV8 (which does include
an update since my "refresh" post), but I don't have stock
JDK6 handy to test on.
Martin and I will take this off-list to try to further investigate.

-Doug



From dl at cs.oswego.edu  Thu Jul  4 08:22:52 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 04 Jul 2013 08:22:52 -0400
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
Message-ID: <51D5691C.1040701@cs.oswego.edu>

On 07/01/13 21:06, Zhong Yu wrote:

> Just a trivial naming suggestion:
>
>      whenComplete(BiConsumer<T,Throwable>)
> =>
>      finallyAccept(BiConsumer<T,Throwable>)
>
>
>      handle(BiFunction<T,Throwable,U>)
> =>
>      finallyApply(BiFunction<T,Throwable,U>)
>
>

Thanks. I didn't immediately reply since it is worth
considering making these two method names more
uniform with themselves and others. But "finally"
is not a good choice because people could be confused
into thinking that if has try-finally-like properties
wrt to its calling context, not the completion flow.
Running through various other possibilities,
and recalling how hard it was just to settle on
name "handle" leads me to think that the current names
are better than most other choices.

Thanks to everyone else who replied on and off list.
It seems that no one disagrees that adding CompletionStage
is a helpful change.

-Doug




From zhong.j.yu at gmail.com  Thu Jul  4 13:07:41 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 4 Jul 2013 12:07:41 -0500
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <51D5691C.1040701@cs.oswego.edu>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
Message-ID: <CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>

Doug, I am not sure whether the interface in its current form is the
best for the community. It seems being rushed out. There are
unresolved objections, e.g. regarding cancellation. I wish you could
allocate more time for discussions.

Zhong Yu

On Thu, Jul 4, 2013 at 7:22 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 07/01/13 21:06, Zhong Yu wrote:
>
>> Just a trivial naming suggestion:
>>
>>      whenComplete(BiConsumer<T,Throwable>)
>> =>
>>      finallyAccept(BiConsumer<T,Throwable>)
>>
>>
>>      handle(BiFunction<T,Throwable,U>)
>> =>
>>      finallyApply(BiFunction<T,Throwable,U>)
>>
>>
>
> Thanks. I didn't immediately reply since it is worth
> considering making these two method names more
> uniform with themselves and others. But "finally"
> is not a good choice because people could be confused
> into thinking that if has try-finally-like properties
> wrt to its calling context, not the completion flow.
> Running through various other possibilities,
> and recalling how hard it was just to settle on
> name "handle" leads me to think that the current names
> are better than most other choices.
>
> Thanks to everyone else who replied on and off list.
> It seems that no one disagrees that adding CompletionStage
> is a helpful change.
>
> -Doug
>
>
>

From viktor.klang at gmail.com  Thu Jul  4 13:21:38 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 4 Jul 2013 19:21:38 +0200
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
	<CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
Message-ID: <CANPzfU8MUyp1bVZ5RC+0tw=a8=RLuNVmO2xh38swod+XxDDBkw@mail.gmail.com>

I think it is very easy, if you want cancellation, use CompletableFuture.

Cheers,
V
On Jul 4, 2013 7:15 PM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> Doug, I am not sure whether the interface in its current form is the
> best for the community. It seems being rushed out. There are
> unresolved objections, e.g. regarding cancellation. I wish you could
> allocate more time for discussions.
>
> Zhong Yu
>
> On Thu, Jul 4, 2013 at 7:22 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> > On 07/01/13 21:06, Zhong Yu wrote:
> >
> >> Just a trivial naming suggestion:
> >>
> >>      whenComplete(BiConsumer<T,Throwable>)
> >> =>
> >>      finallyAccept(BiConsumer<T,Throwable>)
> >>
> >>
> >>      handle(BiFunction<T,Throwable,U>)
> >> =>
> >>      finallyApply(BiFunction<T,Throwable,U>)
> >>
> >>
> >
> > Thanks. I didn't immediately reply since it is worth
> > considering making these two method names more
> > uniform with themselves and others. But "finally"
> > is not a good choice because people could be confused
> > into thinking that if has try-finally-like properties
> > wrt to its calling context, not the completion flow.
> > Running through various other possibilities,
> > and recalling how hard it was just to settle on
> > name "handle" leads me to think that the current names
> > are better than most other choices.
> >
> > Thanks to everyone else who replied on and off list.
> > It seems that no one disagrees that adding CompletionStage
> > is a helpful change.
> >
> > -Doug
> >
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/02affe92/attachment.html>

From zhong.j.yu at gmail.com  Thu Jul  4 14:48:59 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 4 Jul 2013 13:48:59 -0500
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <51D5691C.1040701@cs.oswego.edu>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
Message-ID: <CACuKZqG1u5pkksL5DhzSjp055UtMHzFC1j+MZucZuzWYZ08-LQ@mail.gmail.com>

On Thu, Jul 4, 2013 at 7:22 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 07/01/13 21:06, Zhong Yu wrote:
>
>> Just a trivial naming suggestion:
>>
>>      whenComplete(BiConsumer<T,Throwable>)
>> =>
>>      finallyAccept(BiConsumer<T,Throwable>)
>>
>>
>>      handle(BiFunction<T,Throwable,U>)
>> =>
>>      finallyApply(BiFunction<T,Throwable,U>)
>>
>>
>
> Thanks. I didn't immediately reply since it is worth
> considering making these two method names more
> uniform with themselves and others. But "finally"
> is not a good choice because people could be confused
> into thinking that if has try-finally-like properties
> wrt to its calling context, not the completion flow.
> Running through various other possibilities,
> and recalling how hard it was just to settle on
> name "handle" leads me to think that the current names
> are better than most other choices.

Hi Doug, have you considered overloading with different arities?

    thenAccept(Consumer<T>)
    thenAccept(BiConsumer<T,Throwable>)

    stage.thenAccept( result -> {...} );
    stage.thenAccept( (result, error) -> {...} );

Such overloading causes no problems for either compiler or human readers.

Zhong Yu

>
> Thanks to everyone else who replied on and off list.
> It seems that no one disagrees that adding CompletionStage
> is a helpful change.
>
> -Doug
>
>
>

From zhong.j.yu at gmail.com  Thu Jul  4 15:00:07 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 4 Jul 2013 14:00:07 -0500
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CANPzfU8MUyp1bVZ5RC+0tw=a8=RLuNVmO2xh38swod+XxDDBkw@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
	<CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
	<CANPzfU8MUyp1bVZ5RC+0tw=a8=RLuNVmO2xh38swod+XxDDBkw@mail.gmail.com>
Message-ID: <CACuKZqG8ThRMK=kTzbhd_f1TWWazUROZSCCMcDCpon5UCZkDUQ@mail.gmail.com>

Viktor, what I meant is

1. whether cancel() should send a cancellation request that propagates upwards.
2. whether to include cancel() on the "read-only" interface.

You and Sam Pullara have raised these issues to Doug, and Doug's
responses are not fully persuasive it appears to me. Maybe I just
don't quite get the exact role this CompletionStage interface is
supposed to play.

Zhong Yu

On Thu, Jul 4, 2013 at 12:21 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:
> I think it is very easy, if you want cancellation, use CompletableFuture.
>
> Cheers,
> V
>
> On Jul 4, 2013 7:15 PM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
>>
>> Doug, I am not sure whether the interface in its current form is the
>> best for the community. It seems being rushed out. There are
>> unresolved objections, e.g. regarding cancellation. I wish you could
>> allocate more time for discussions.
>>
>> Zhong Yu
>>
>> On Thu, Jul 4, 2013 at 7:22 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>> > On 07/01/13 21:06, Zhong Yu wrote:
>> >
>> >> Just a trivial naming suggestion:
>> >>
>> >>      whenComplete(BiConsumer<T,Throwable>)
>> >> =>
>> >>      finallyAccept(BiConsumer<T,Throwable>)
>> >>
>> >>
>> >>      handle(BiFunction<T,Throwable,U>)
>> >> =>
>> >>      finallyApply(BiFunction<T,Throwable,U>)
>> >>
>> >>
>> >
>> > Thanks. I didn't immediately reply since it is worth
>> > considering making these two method names more
>> > uniform with themselves and others. But "finally"
>> > is not a good choice because people could be confused
>> > into thinking that if has try-finally-like properties
>> > wrt to its calling context, not the completion flow.
>> > Running through various other possibilities,
>> > and recalling how hard it was just to settle on
>> > name "handle" leads me to think that the current names
>> > are better than most other choices.
>> >
>> > Thanks to everyone else who replied on and off list.
>> > It seems that no one disagrees that adding CompletionStage
>> > is a helpful change.
>> >
>> > -Doug
>> >
>> >
>> >
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Thu Jul  4 15:11:08 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 4 Jul 2013 21:11:08 +0200
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CACuKZqG8ThRMK=kTzbhd_f1TWWazUROZSCCMcDCpon5UCZkDUQ@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
	<CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
	<CANPzfU8MUyp1bVZ5RC+0tw=a8=RLuNVmO2xh38swod+XxDDBkw@mail.gmail.com>
	<CACuKZqG8ThRMK=kTzbhd_f1TWWazUROZSCCMcDCpon5UCZkDUQ@mail.gmail.com>
Message-ID: <CANPzfU8OoQ-A+miGgyHsqJF2=0vmMk7Tp7KEtUEP3KN3DJcG5w@mail.gmail.com>

Hi Zhong,

"cancel" violates read-only so it cannot be on the interface. (One reader
uninterested in the result doesn't mean all readers are uninterested.)

Therefor whether cancel sends an upstream message or not is irrelevant for
CompletionStage. It's a discussion for CompletableFuture.

So my conclusion is: if you want cancellation, use CompletableFuture.

Cheers,
V
On Jul 4, 2013 9:00 PM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:

> Viktor, what I meant is
>
> 1. whether cancel() should send a cancellation request that propagates
> upwards.
> 2. whether to include cancel() on the "read-only" interface.
>
> You and Sam Pullara have raised these issues to Doug, and Doug's
> responses are not fully persuasive it appears to me. Maybe I just
> don't quite get the exact role this CompletionStage interface is
> supposed to play.
>
> Zhong Yu
>
> On Thu, Jul 4, 2013 at 12:21 PM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
> > I think it is very easy, if you want cancellation, use CompletableFuture.
> >
> > Cheers,
> > V
> >
> > On Jul 4, 2013 7:15 PM, "Zhong Yu" <zhong.j.yu at gmail.com> wrote:
> >>
> >> Doug, I am not sure whether the interface in its current form is the
> >> best for the community. It seems being rushed out. There are
> >> unresolved objections, e.g. regarding cancellation. I wish you could
> >> allocate more time for discussions.
> >>
> >> Zhong Yu
> >>
> >> On Thu, Jul 4, 2013 at 7:22 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> >> > On 07/01/13 21:06, Zhong Yu wrote:
> >> >
> >> >> Just a trivial naming suggestion:
> >> >>
> >> >>      whenComplete(BiConsumer<T,Throwable>)
> >> >> =>
> >> >>      finallyAccept(BiConsumer<T,Throwable>)
> >> >>
> >> >>
> >> >>      handle(BiFunction<T,Throwable,U>)
> >> >> =>
> >> >>      finallyApply(BiFunction<T,Throwable,U>)
> >> >>
> >> >>
> >> >
> >> > Thanks. I didn't immediately reply since it is worth
> >> > considering making these two method names more
> >> > uniform with themselves and others. But "finally"
> >> > is not a good choice because people could be confused
> >> > into thinking that if has try-finally-like properties
> >> > wrt to its calling context, not the completion flow.
> >> > Running through various other possibilities,
> >> > and recalling how hard it was just to settle on
> >> > name "handle" leads me to think that the current names
> >> > are better than most other choices.
> >> >
> >> > Thanks to everyone else who replied on and off list.
> >> > It seems that no one disagrees that adding CompletionStage
> >> > is a helpful change.
> >> >
> >> > -Doug
> >> >
> >> >
> >> >
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130704/0dfd789a/attachment.html>

From dl at cs.oswego.edu  Fri Jul  5 06:19:42 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 05 Jul 2013 06:19:42 -0400
Subject: [concurrency-interest] Interface CompletionStage
In-Reply-To: <CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
References: <51D1E60A.2090606@cs.oswego.edu>
	<CACuKZqFxO1Ps2UM4c-2pT1S5teP3SfAFyLHWF9K29BjLJCJdFg@mail.gmail.com>
	<51D5691C.1040701@cs.oswego.edu>
	<CACuKZqFqP9z0ho=8gME+DdX8kReVRoHzaYkKpfiBS1M8ezEyeQ@mail.gmail.com>
Message-ID: <51D69DBE.5000204@cs.oswego.edu>

On 07/04/13 13:07, Zhong Yu wrote:
> Doug, I am not sure whether the interface in its current form is the
> best for the community. It seems being rushed out. There are
> unresolved objections, e.g. regarding cancellation. I wish you could
> allocate more time for discussions.

Sorry, I must not have explained the goals carefully enough.
One more time:

1. It was (and remains) clear that different frameworks
and usages would like to impose different policies surrounding
methods for forcibly completing, accessing status or results,
blocking, etc.

2. Some of these different policies are clearly mutually
incompatible, so no amount of discussion would resolve them.

3. CompletableFuture implements mechanics leading to
policies compatible with those of Future.

4. Without CompletionStage, our implicit response to
people wanting other policies was: Tough luck,  write your own.
But with it, the response is: Here's an easy recipe for
creating your own customized implementation that works in
a large number of cases; and for others, you can write
something completely different and still interoperate
with others.

This seemed uncontroversial enough that I would have been
shocked to hear any serious objection.

-Doug



From dl at cs.oswego.edu  Fri Jul  5 08:09:04 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 05 Jul 2013 08:09:04 -0400
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <51D55577.6050706@cs.oswego.edu>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu>
Message-ID: <51D6B760.3090101@cs.oswego.edu>

On 07/04/13 06:59, Doug Lea wrote:
> On 07/03/13 21:41, Martin Buchholz wrote:
>> This is not a known problem.  Can you identify whether your problem is specific
>> to the backport?
>>
>
> I can't replicate this on the current CHMV8 (which does include
> an update since my "refresh" post), but I don't have stock
> JDK6 handy to test on.
> Martin and I will take this off-list to try to further investigate.
>

Back on to list: A fix was committed that is just barely worth
mentioning as a programming note: I had refactored some
messy loops to use slightly less messy tail-recursion.
But: one of those loops was only guaranteed to terminate
eventually as a reader catches up, and tail recursion is
not guaranteed to be loopified, so could cause stack-overflow.
This and a related case are now manually re-loopified.

-Doug



From jed at atlassian.com  Fri Jul  5 19:20:56 2013
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Sat, 6 Jul 2013 09:20:56 +1000
Subject: [concurrency-interest] java.lang.StackOverflowError on
	ConcurrentHashMap V8
In-Reply-To: <51D6B760.3090101@cs.oswego.edu>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu> <51D6B760.3090101@cs.oswego.edu>
Message-ID: <6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>

if only the JVM did tail-call elimination? :-)

cheers,
jed.

On 05/07/2013, at 10:09 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 07/04/13 06:59, Doug Lea wrote:
>> On 07/03/13 21:41, Martin Buchholz wrote:
>>> This is not a known problem.  Can you identify whether your problem is specific
>>> to the backport?
>> 
>> I can't replicate this on the current CHMV8 (which does include
>> an update since my "refresh" post), but I don't have stock
>> JDK6 handy to test on.
>> Martin and I will take this off-list to try to further investigate.
> 
> Back on to list: A fix was committed that is just barely worth
> mentioning as a programming note: I had refactored some
> messy loops to use slightly less messy tail-recursion.
> But: one of those loops was only guaranteed to terminate
> eventually as a reader catches up, and tail recursion is
> not guaranteed to be loopified, so could cause stack-overflow.
> This and a related case are now manually re-loopified.
> 
> -Doug
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Sat Jul  6 05:10:47 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 6 Jul 2013 12:10:47 +0300
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu> <51D6B760.3090101@cs.oswego.edu>
	<6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>
Message-ID: <CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>

On Sat, Jul 6, 2013 at 2:20 AM, Jed Wesley-Smith <jed at atlassian.com> wrote:

> if only the JVM did tail-call elimination? :-)
>
> There won't be proper stack traces, unfortunately. Still unsure if the
issue can be mended somehow by keeping local metadata, so whoever walks the
stack would be aware.
Also reminds me - almost anytime I did loop->recursion optimization I'd got
bitten in similar fashion.

Stanimir

cheers,
> jed.
>
> On 05/07/2013, at 10:09 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> > On 07/04/13 06:59, Doug Lea wrote:
> >> On 07/03/13 21:41, Martin Buchholz wrote:
> >>> This is not a known problem.  Can you identify whether your problem is
> specific
> >>> to the backport?
> >>
> >> I can't replicate this on the current CHMV8 (which does include
> >> an update since my "refresh" post), but I don't have stock
> >> JDK6 handy to test on.
> >> Martin and I will take this off-list to try to further investigate.
> >
> > Back on to list: A fix was committed that is just barely worth
> > mentioning as a programming note: I had refactored some
> > messy loops to use slightly less messy tail-recursion.
> > But: one of those loops was only guaranteed to terminate
> > eventually as a reader catches up, and tail recursion is
> > not guaranteed to be loopified, so could cause stack-overflow.
> > This and a related case are now manually re-loopified.
> >
> > -Doug
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130706/fc601b3f/attachment.html>

From viktor.klang at gmail.com  Sat Jul  6 07:59:45 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 6 Jul 2013 13:59:45 +0200
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu> <51D6B760.3090101@cs.oswego.edu>
	<6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>
	<CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>
Message-ID: <CANPzfU8uq_c_UUpdifREDLOU1x6JnXmMzKEfUuKmk9fom1Zm0Q@mail.gmail.com>

I thought I'd chime in :)

I do tailrecursion in Scala more frequently than loops and I've had 0
issues debugging it since you don't need to deal with mutable state.

With async programming becoming more and more popular, the value of stack
traces become less useful, having a JDK Tracing lib ? la Dapper would be
very useful.

Cheers,
V
On Jul 6, 2013 11:17 AM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

>
>
> On Sat, Jul 6, 2013 at 2:20 AM, Jed Wesley-Smith <jed at atlassian.com>wrote:
>
>> if only the JVM did tail-call elimination? :-)
>>
>> There won't be proper stack traces, unfortunately. Still unsure if the
> issue can be mended somehow by keeping local metadata, so whoever walks the
> stack would be aware.
> Also reminds me - almost anytime I did loop->recursion optimization I'd
> got bitten in similar fashion.
>
> Stanimir
>
> cheers,
>> jed.
>>
>> On 05/07/2013, at 10:09 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> > On 07/04/13 06:59, Doug Lea wrote:
>> >> On 07/03/13 21:41, Martin Buchholz wrote:
>> >>> This is not a known problem.  Can you identify whether your problem
>> is specific
>> >>> to the backport?
>> >>
>> >> I can't replicate this on the current CHMV8 (which does include
>> >> an update since my "refresh" post), but I don't have stock
>> >> JDK6 handy to test on.
>> >> Martin and I will take this off-list to try to further investigate.
>> >
>> > Back on to list: A fix was committed that is just barely worth
>> > mentioning as a programming note: I had refactored some
>> > messy loops to use slightly less messy tail-recursion.
>> > But: one of those loops was only guaranteed to terminate
>> > eventually as a reader catches up, and tail recursion is
>> > not guaranteed to be loopified, so could cause stack-overflow.
>> > This and a related case are now manually re-loopified.
>> >
>> > -Doug
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130706/add8bdb9/attachment.html>

From zhong.j.yu at gmail.com  Sat Jul  6 08:36:21 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Sat, 6 Jul 2013 07:36:21 -0500
Subject: [concurrency-interest] java.lang.StackOverflowError on
 ConcurrentHashMap V8
In-Reply-To: <CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu> <51D6B760.3090101@cs.oswego.edu>
	<6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>
	<CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>
Message-ID: <CACuKZqGfo-4H6Xhnxr4PV2t28jihJRkRjLxtAi=jEg2bBHA8SQ@mail.gmail.com>

On Sat, Jul 6, 2013 at 4:10 AM, Stanimir Simeonoff <stanimir at riflexo.com> wrote:
>
>
> On Sat, Jul 6, 2013 at 2:20 AM, Jed Wesley-Smith <jed at atlassian.com> wrote:
>>
>> if only the JVM did tail-call elimination? :-)
>>
> There won't be proper stack traces, unfortunately. Still unsure if the issue
> can be mended somehow by keeping local metadata, so whoever walks the stack
> would be aware.

The nested stacktrace could be an artifact that doesn't really concern
the user, for example

    void buyFood()
    {
        ...
        eatFood();
    }

eatFood is not a sub action of buyFood, its a follow up action. We do
not need to show in stacktrace that buyFood occurred earlier than
eatFood.

I think the language could have a keyword for tailcall so the
programmer can express the intention explicitly. Since "goto" is a
reserved keyword...

    void buyFood()
    {
        ...
        goto eatFood();
    }

the loss of stacktrace is then granted by the programmer.

Zhong Yu


> Also reminds me - almost anytime I did loop->recursion optimization I'd got
> bitten in similar fashion.
>
> Stanimir
>
>> cheers,
>> jed.
>>
>> On 05/07/2013, at 10:09 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> > On 07/04/13 06:59, Doug Lea wrote:
>> >> On 07/03/13 21:41, Martin Buchholz wrote:
>> >>> This is not a known problem.  Can you identify whether your problem is
>> >>> specific
>> >>> to the backport?
>> >>
>> >> I can't replicate this on the current CHMV8 (which does include
>> >> an update since my "refresh" post), but I don't have stock
>> >> JDK6 handy to test on.
>> >> Martin and I will take this off-list to try to further investigate.
>> >
>> > Back on to list: A fix was committed that is just barely worth
>> > mentioning as a programming note: I had refactored some
>> > messy loops to use slightly less messy tail-recursion.
>> > But: one of those loops was only guaranteed to terminate
>> > eventually as a reader catches up, and tail recursion is
>> > not guaranteed to be loopified, so could cause stack-overflow.
>> > This and a related case are now manually re-loopified.
>> >
>> > -Doug
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From thurston at nomagicsoftware.com  Sat Jul  6 17:48:57 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 6 Jul 2013 14:48:57 -0700 (PDT)
Subject: [concurrency-interest] Most efficient way to 'signal' a Thread
Message-ID: <1373147337193-9863.post@n7.nabble.com>

The scenario I was working on was a classic multi-writer (producer), single
reader (consumer), placing items (the writers) on a concurrent queue and
then processing each item (the reader)

I tried 3 different approaches to the issue of having the writers 'wakeup'
the reader thread just after placing an item on the queue (brief
description/pseudo-code):

1.  Wait/Notify on an object
     writers' code:
     synchronized (lock)
         lock.notify()
     reader code:
          lock.wait(2_000L)
2.  ReentrantLock/Condition
     writers' code:
     if (rl.tryLock)
         condition.signal()
     reader code:
          condition.await(2_000L)
3.  Interrupt
     writers' code:
     readerThread.interrupt()
     reader code:
     Thread.sleep(2_000L)


Instead of just reporting results, I would like to appeal to your
understanding/intuition.  Which would you expect to 'perform'* better?
Here's mine:
I certainly expected that #2 would outperform #1 for the simple reason that
(many/most?) writers could avoid acquiring the lock entirely (tryLock would
fail immediately)
I was uncertain about #3 - it has the advantage that no locks/mutexes be
used at all but I thought the overhead of constructing the
InterruptedException would be a detriment (plus there's something
aesthetically unappealing about it fwiw)

So what would you expect?

*perform means the time until all of the writer theads had finished their
work (add item; wakeup reader); I was not interested in when the reader had
finished processing all items on the queue.  I used a CountDownLatch to
eliminate any Thread creation/start overhead vagaries - all of the writer
Threads were 'banging' against the latch before I started the benchmark





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Most-efficient-way-to-signal-a-Thread-tp9863.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From gustav.r.akesson at gmail.com  Sun Jul  7 15:45:26 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Sun, 7 Jul 2013 21:45:26 +0200
Subject: [concurrency-interest]  Spinning locks
Message-ID: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>

Hi,

For fun I constructed a micro benchmark to test ArrayBlockingQueue using
regular locking and a modified version using spinlocking similar to the
behavior of LinkedTransferQueue. I noticed an increased (30-40%) throughput
when using optimistic spinning and then lock. The test had 50-50
consumers/producers and as many CPUs as my machine. And it was only pure
producer-consumer behavior, i.e. none did more than enqueing/dequeing.

I take that ReentrantLock doesn't currently support any spinning behing the
scenes, but is there any thought on introducing this? Is it too risky that
the cost of spinning could outweight the benefits? Is there are thoughts on
adding spinning mechanism to other concurrent collections similar to
LinkedTransferQueue?


Best Regards,

Gustav ?kesson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130707/b9b61fc9/attachment.html>

From dl at cs.oswego.edu  Sun Jul  7 17:31:01 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 07 Jul 2013 17:31:01 -0400
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
References: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
Message-ID: <51D9DE15.7070302@cs.oswego.edu>

On 07/07/13 15:45, Gustav ?kesson wrote:

> I take that ReentrantLock doesn't currently support any spinning behing the
> scenes, but is there any thought on introducing this? Is it too risky that the
> cost of spinning could outweight the benefits? Is there are thoughts on adding
> spinning mechanism to other concurrent collections similar to LinkedTransferQueue?
>

Adaptive spinning at the level of ReentrantLock or AbstractQueuedSynchronizer
is a little tricky to add in a way that would almost always be helpful,
so there was a tentative plan to rely on VM support one layer down.
See JEP 143: http://openjdk.java.net/jeps/143
But Oracle decided not to commit to this for JDK8.

So at the moment there's not a tentative plan, except to continue
to tell people that it they can still add simple forms of bounded spinning
using tryLock() and related constructions.

-Doug






From cheremin at gmail.com  Mon Jul  8 01:01:11 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Mon, 8 Jul 2013 09:01:11 +0400
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <51D9DE15.7070302@cs.oswego.edu>
References: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
	<51D9DE15.7070302@cs.oswego.edu>
Message-ID: <CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>

Doug, could you make it a bit cleaner -- what makes adaptive spinning in RL
tricky? AFAIK, there is (adaptive?) spinning behind synchronized(), why
couldn't it be just copied inside RL?  What is the key thing which could be
done inside VM, but not on java level?


2013/7/8 Doug Lea <dl at cs.oswego.edu>

> On 07/07/13 15:45, Gustav ?kesson wrote:
>
>  I take that ReentrantLock doesn't currently support any spinning behing
>> the
>> scenes, but is there any thought on introducing this? Is it too risky
>> that the
>> cost of spinning could outweight the benefits? Is there are thoughts on
>> adding
>> spinning mechanism to other concurrent collections similar to
>> LinkedTransferQueue?
>>
>>
> Adaptive spinning at the level of ReentrantLock or
> AbstractQueuedSynchronizer
> is a little tricky to add in a way that would almost always be helpful,
> so there was a tentative plan to rely on VM support one layer down.
> See JEP 143: http://openjdk.java.net/jeps/**143<http://openjdk.java.net/jeps/143>
> But Oracle decided not to commit to this for JDK8.
>
> So at the moment there's not a tentative plan, except to continue
> to tell people that it they can still add simple forms of bounded spinning
> using tryLock() and related constructions.
>
> -Doug
>
>
>
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/78f87a9a/attachment.html>

From maasgar2011 at gmail.com  Mon Jul  8 03:21:38 2013
From: maasgar2011 at gmail.com (Mohammad Ali Asgar)
Date: Mon, 8 Jul 2013 13:21:38 +0600
Subject: [concurrency-interest] Mapping between OS threads and Java threads
Message-ID: <CAERq_s23Do8JEfLz=QzZy=LOwnHXYB4MtmXcbwT_FQc3+kEUKQ@mail.gmail.com>

Hello,

The API documentation of Java on Thread.State says:
"A thread can be in only one state at a given point in time. These states
are virtual machine states which do not reflect any operating system thread
states."

The specific states mentioned are:
- NEW (A thread that has not yet started is in this state.)
- RUNNABLE (A thread executing in the Java virtual machine is in this
state.)
- BLOCKED (A thread that is blocked waiting for a monitor lock is in this
state.)
- WAITING (A thread that is waiting indefinitely for another thread to
perform a particular action is in this state.)
- TIMED_WAITING (A thread that is waiting for another thread to perform an
action for up to a specified waiting time is in this state.)
- TERMINATED (A thread that has exited is in this state.)

Is there any good reference that could help one learn the mapping between
OS level threads and application threads hosted by JVM? I am trying to
picture how the OS level threads hold up a JVM which, in turn, holds up an
application containing one or more threads in it.

Thank you.

Best Regards,
Asgar
Ericsson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/eb4575a3/attachment.html>

From gustav.r.akesson at gmail.com  Mon Jul  8 03:32:19 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Mon, 8 Jul 2013 09:32:19 +0200
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>
References: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
	<51D9DE15.7070302@cs.oswego.edu>
	<CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>
Message-ID: <CAKEw5+7ZBd=L8DwG+ubxEK7G2uPQABURnLbxPq-CkUyX4KDPwQ@mail.gmail.com>

Hi,

Interesting input, Ruslan.

Also, Doug, I understand it's perhaps not always suitable to spin on a
ReentrantLock (for instance, if a lot is performed within the critical
section), but if we look at ArrayBlockingQueue - take/put has a finite set
of instructions, isn't it possible to find a suitable spin/yield/wait
formula for that kind of datastructure? I.e. in the case t hat we know that
a blocking/unblocking actually costs more than to simply spin for the lock.


Best Regards,

Gustav ?kesson


On Mon, Jul 8, 2013 at 7:01 AM, Ruslan Cheremin <cheremin at gmail.com> wrote:

> Doug, could you make it a bit cleaner -- what makes adaptive spinning in
> RL tricky? AFAIK, there is (adaptive?) spinning behind synchronized(), why
> couldn't it be just copied inside RL?  What is the key thing which could be
> done inside VM, but not on java level?
>
>
> 2013/7/8 Doug Lea <dl at cs.oswego.edu>
>
>> On 07/07/13 15:45, Gustav ?kesson wrote:
>>
>>  I take that ReentrantLock doesn't currently support any spinning behing
>>> the
>>> scenes, but is there any thought on introducing this? Is it too risky
>>> that the
>>> cost of spinning could outweight the benefits? Is there are thoughts on
>>> adding
>>> spinning mechanism to other concurrent collections similar to
>>> LinkedTransferQueue?
>>>
>>>
>> Adaptive spinning at the level of ReentrantLock or
>> AbstractQueuedSynchronizer
>> is a little tricky to add in a way that would almost always be helpful,
>> so there was a tentative plan to rely on VM support one layer down.
>> See JEP 143: http://openjdk.java.net/jeps/**143<http://openjdk.java.net/jeps/143>
>> But Oracle decided not to commit to this for JDK8.
>>
>> So at the moment there's not a tentative plan, except to continue
>> to tell people that it they can still add simple forms of bounded spinning
>> using tryLock() and related constructions.
>>
>> -Doug
>>
>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/da9d91fb/attachment.html>

From davidcholmes at aapt.net.au  Mon Jul  8 04:14:10 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 8 Jul 2013 18:14:10 +1000
Subject: [concurrency-interest] Mapping between OS threads and Java
	threads
In-Reply-To: <CAERq_s23Do8JEfLz=QzZy=LOwnHXYB4MtmXcbwT_FQc3+kEUKQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAELMJOAA.davidcholmes@aapt.net.au>

Hi,

In hotspot, quite simply, each Java thread is associated directly with a
native OS thread.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohammad Ali
Asgar
  Sent: Monday, 8 July 2013 5:22 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Mapping between OS threads and Java
threads


  Hello,


  The API documentation of Java on Thread.State says:
  "A thread can be in only one state at a given point in time. These states
are virtual machine states which do not reflect any operating system thread
states."


  The specific states mentioned are:
  - NEW (A thread that has not yet started is in this state.)
  - RUNNABLE (A thread executing in the Java virtual machine is in this
state.)
  - BLOCKED (A thread that is blocked waiting for a monitor lock is in this
state.)
  - WAITING (A thread that is waiting indefinitely for another thread to
perform a particular action is in this state.)
  - TIMED_WAITING (A thread that is waiting for another thread to perform an
action for up to a specified waiting time is in this state.)
  - TERMINATED (A thread that has exited is in this state.)


  Is there any good reference that could help one learn the mapping between
OS level threads and application threads hosted by JVM? I am trying to
picture how the OS level threads hold up a JVM which, in turn, holds up an
application containing one or more threads in it.


  Thank you.


  Best Regards,
  Asgar
  Ericsson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/be96ff25/attachment-0001.html>

From aph at redhat.com  Mon Jul  8 04:57:51 2013
From: aph at redhat.com (Andrew Haley)
Date: Mon, 08 Jul 2013 09:57:51 +0100
Subject: [concurrency-interest] Mapping between OS threads and Java
	threads
In-Reply-To: <CAERq_s23Do8JEfLz=QzZy=LOwnHXYB4MtmXcbwT_FQc3+kEUKQ@mail.gmail.com>
References: <CAERq_s23Do8JEfLz=QzZy=LOwnHXYB4MtmXcbwT_FQc3+kEUKQ@mail.gmail.com>
Message-ID: <51DA7F0F.3000300@redhat.com>

On 07/08/2013 08:21 AM, Mohammad Ali Asgar wrote:
> Is there any good reference that could help one learn the mapping between
> OS level threads and application threads hosted by JVM? I am trying to
> picture how the OS level threads hold up a JVM which, in turn, holds up an
> application containing one or more threads in it.

How could there be?  We can talk about, say, HotSpot on Linux, but
that's all.  Every OS and VM is different.

Andrew.


From dl at cs.oswego.edu  Mon Jul  8 05:41:00 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Jul 2013 05:41:00 -0400
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>
References: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
	<51D9DE15.7070302@cs.oswego.edu>
	<CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>
Message-ID: <51DA892C.6040705@cs.oswego.edu>

On 07/08/13 01:01, Ruslan Cheremin wrote:
> Doug, could you make it a bit cleaner -- what makes adaptive spinning in RL
> tricky? AFAIK, there is (adaptive?) spinning behind synchronized(), why couldn't
> it be just copied inside RL?  What is the key thing which could be done inside
> VM, but not on java level?
>
>

When we know something about the structure of thread interactions
in a j.u.c class we can and usually do write custom spin/help/block
code. But for arbitrary usages of arbitrary exclusive locks, we don't
know much, and the only guidance about spin/block we could use is at
the OS/VM level: Are there a lot of available cores? Are there
a lot of blocked threads? Etc. Depending on the state of the machine,
we'd want to spin at least a little. The idea for the JEP 143 work
is that these kinds of decisions/actions could be done inside
the VM implementation of LockSupport.park itself, since they
don't rely on context. They could then equally apply to j.u.c
sync and the similar park-like calls underlying synchronized blocks.
It was a pretty good idea. Hopefully someday.

-Doug



> 2013/7/8 Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu>>
>
>     On 07/07/13 15:45, Gustav ?kesson wrote:
>
>         I take that ReentrantLock doesn't currently support any spinning behing the
>         scenes, but is there any thought on introducing this? Is it too risky
>         that the
>         cost of spinning could outweight the benefits? Is there are thoughts on
>         adding
>         spinning mechanism to other concurrent collections similar to
>         LinkedTransferQueue?
>
>
>     Adaptive spinning at the level of ReentrantLock or AbstractQueuedSynchronizer
>     is a little tricky to add in a way that would almost always be helpful,
>     so there was a tentative plan to rely on VM support one layer down.
>     See JEP 143: http://openjdk.java.net/jeps/__143
>     <http://openjdk.java.net/jeps/143>
>     But Oracle decided not to commit to this for JDK8.
>
>     So at the moment there's not a tentative plan, except to continue
>     to tell people that it they can still add simple forms of bounded spinning
>     using tryLock() and related constructions.
>
>     -Doug
>
>
>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>




From dl at cs.oswego.edu  Mon Jul  8 06:00:14 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Jul 2013 06:00:14 -0400
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <CAKEw5+7ZBd=L8DwG+ubxEK7G2uPQABURnLbxPq-CkUyX4KDPwQ@mail.gmail.com>
References: <CAKEw5+5Fc6P9Qp+xdCixncdR_UyuR2FAZ9uD4MzxXR=bRY8kuQ@mail.gmail.com>
	<51D9DE15.7070302@cs.oswego.edu>
	<CAOwENiJ4wvomWeXhYuPFAVwMo+jVww9+i+JmELCEZvPq5T0rAQ@mail.gmail.com>
	<CAKEw5+7ZBd=L8DwG+ubxEK7G2uPQABURnLbxPq-CkUyX4KDPwQ@mail.gmail.com>
Message-ID: <51DA8DAE.7080309@cs.oswego.edu>

On 07/08/13 03:32, Gustav ?kesson wrote:

> but if we look at ArrayBlockingQueue - take/put has a finite set of
> instructions, isn't it possible to find a suitable spin/yield/wait formula for
> that kind of datastructure?

Yes. Rethinking ArrayBlockingQueue to explore these and
other improvements has been a low-priority to-do list item for
too long.  Hopefully soon. Probably not all that soon though...

-Doug






From peter.levart at gmail.com  Mon Jul  8 06:57:08 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 08 Jul 2013 12:57:08 +0200
Subject: [concurrency-interest] RRWL with 'bad' Thread.getId()
	implementations
In-Reply-To: <51CA0C7E.9020605@oracle.com>
References: <CDEF737F.2F3C6%chris.w.dennis@gmail.com>
	<51C9FF80.8090709@oracle.com>
	<CAEJX8oquvXT6dLWyWKjJjbonW8O_HFXLvw+0gibdtuWhOjAaZA@mail.gmail.com>
	<CA+kOe0_gE7Y49=YJ7msnrg4iQWTDoZUeMDqdFtNUGXPiRHT-aw@mail.gmail.com>
	<51CA08B2.9080606@oracle.com>
	<CA+kOe08+GjjTWz9gO8-rziMy7MCzd6vhrYxqDQ46tOc489a9MQ@mail.gmail.com>
	<51CA0C7E.9020605@oracle.com>
Message-ID: <51DA9B04.5060402@gmail.com>

On 06/25/2013 11:32 PM, Aleksey Shipilev wrote:
> On 06/26/2013 01:20 AM, Martin Buchholz wrote:
>>
>>
>> On Tue, Jun 25, 2013 at 2:16 PM, Aleksey Shipilev
>> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>>      On 06/26/2013 01:13 AM, Martin Buchholz wrote:
>>      On the serious note, we need to frame this discussion a bit. Do you want
>>      the bug against RRWL with the testcase Chris had came up with? (That is,
>>      do you agree this is the bug in RRWL?)
>>
>>
>> I think I agree this is a small bug in RRWL, but I suspect any attempt
>> to cure it would be worse than the disease.
> I think the same. Nevertheless, the issue is recorded as CR 8017739.
> Will be publicly available soon as:
>   http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=8017739
>
> Thanks everyone!
>
> -Aleksey.
>

Hi,

So not holding a strong reference to Thread instance in HoldCounter is 
meant to avoid producing memory leak when the thread in question is not 
releasing the lock properly, right? Because when the count in 
HoldCounter drops to 0, the ThreadLocal entry is removed anyway and if 
'cachedHoldCounter' would be set to 'null' at the same time (when it 
happens to be the same as current thread's) then any proper use of RRWL 
(try/finally) would not suffer from memory leak.

Anyway, using WeakReference<Thread> (as a superclass of HoldCounter to 
avoid another object dereference) can be a quick and easy fix for 
Thread.getId() problem:

--- 
jdk/src/share/classes/java/util/concurrent/locks/ReentrantReadWriteLock.java
+++ 
jdk/src/share/classes/java/util/concurrent/locks/ReentrantReadWriteLock.java
@@ -34,6 +34,7 @@
   */

  package java.util.concurrent.locks;
+import java.lang.ref.WeakReference;
  import java.util.concurrent.TimeUnit;
  import java.util.Collection;

@@ -274,11 +275,12 @@
           * A counter for per-thread read hold counts.
           * Maintained as a ThreadLocal; cached in cachedHoldCounter
           */
-        static final class HoldCounter {
+        static final class HoldCounter extends WeakReference<Thread> {
              int count = 0;
-            // Use id, not reference, to avoid garbage retention
-            final long tid = Thread.currentThread().getId();
+            HoldCounter() {
+                super(Thread.currentThread());
-        }
+            }
+        }

          /**
           * ThreadLocal subclass. Easiest to explicitly define for sake
@@ -420,7 +422,7 @@
                      firstReaderHoldCount--;
              } else {
                  HoldCounter rh = cachedHoldCounter;
-                if (rh == null || rh.tid != current.getId())
+                if (rh == null || rh.get() != current)
                      rh = readHolds.get();
                  int count = rh.count;
                  if (count <= 1) {
@@ -478,7 +480,7 @@
                      firstReaderHoldCount++;
                  } else {
                      HoldCounter rh = cachedHoldCounter;
-                    if (rh == null || rh.tid != current.getId())
+                    if (rh == null || rh.get() != current)
                          cachedHoldCounter = rh = readHolds.get();
                      else if (rh.count == 0)
                          readHolds.set(rh);
@@ -515,7 +517,7 @@
                      } else {
                          if (rh == null) {
                              rh = cachedHoldCounter;
-                            if (rh == null || rh.tid != current.getId()) {
+                            if (rh == null || rh.get() != current) {
                                  rh = readHolds.get();
                                  if (rh.count == 0)
                                      readHolds.remove();
@@ -536,7 +538,7 @@
                      } else {
                          if (rh == null)
                              rh = cachedHoldCounter;
-                        if (rh == null || rh.tid != current.getId())
+                        if (rh == null || rh.get() != current)
                              rh = readHolds.get();
                          else if (rh.count == 0)
                              readHolds.set(rh);
@@ -592,7 +594,7 @@
                          firstReaderHoldCount++;
                      } else {
                          HoldCounter rh = cachedHoldCounter;
-                        if (rh == null || rh.tid != current.getId())
+                        if (rh == null || rh.get() != current)
                              cachedHoldCounter = rh = readHolds.get();
                          else if (rh.count == 0)
                              readHolds.set(rh);
@@ -643,7 +645,7 @@
                  return firstReaderHoldCount;

              HoldCounter rh = cachedHoldCounter;
-            if (rh != null && rh.tid == current.getId())
+            if (rh != null && rh.get() == current)
                  return rh.count;

              int count = readHolds.get().count;



There is another potential memory leak in RRWL when not used properly: 
the 'firstReader' points to Thread object of the 1st thread that 
obtained read lock. If the thread does not release the lock, the field 
is not reset to 'null'...

So the question is whether the memory leakage should be prevented for 
proper use of RRWL only or for any improper use also...


Regards, Peter


From peter.levart at gmail.com  Mon Jul  8 07:11:03 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 08 Jul 2013 13:11:03 +0200
Subject: [concurrency-interest] RRWL with 'bad' Thread.getId()
	implementations
In-Reply-To: <51CA21B5.2010907@cs.oswego.edu>
References: <CDEF4299.2F31E%chris.w.dennis@gmail.com>
	<51CA21B5.2010907@cs.oswego.edu>
Message-ID: <51DA9E47.4050203@gmail.com>

On 06/26/2013 01:03 AM, Doug Lea wrote:
> The alternative of using a weak ref is not a good option.
> The reason we use id's to begin with is that if you store
> a Thread ref in a ThreadLocal, even with a weak rewf wrapper,
> there several common scenarios where it cannot be GCed.
> (Some of you might recall discussions about Ephemerons
> motivated by this issue. But these never arrived.) 

Hi Doug (or anyone),

I'm interested in the scenarios when an otherwise unreachable Thread 
object of a stopped thread could not be GC-ed because the WeakReference 
pointing to that Thread was referenced from the ThreadLocal map of the 
same thread.

Regards, Peter


From davidcholmes at aapt.net.au  Mon Jul  8 07:15:32 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 8 Jul 2013 21:15:32 +1000
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <51DA892C.6040705@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCEELNJOAA.davidcholmes@aapt.net.au>

Though I'll add that spinning for Lock acquisition may be a good thing, but
park doesn't know why you are parking. In the context of
AbstractQueuedSynchronizer there is also no knowledge of why you need to
block.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Monday, 8 July 2013 7:41 PM
> To: Ruslan Cheremin
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Spinning locks
>
>
> On 07/08/13 01:01, Ruslan Cheremin wrote:
> > Doug, could you make it a bit cleaner -- what makes adaptive
> spinning in RL
> > tricky? AFAIK, there is (adaptive?) spinning behind
> synchronized(), why couldn't
> > it be just copied inside RL?  What is the key thing which could
> be done inside
> > VM, but not on java level?
> >
> >
>
> When we know something about the structure of thread interactions
> in a j.u.c class we can and usually do write custom spin/help/block
> code. But for arbitrary usages of arbitrary exclusive locks, we don't
> know much, and the only guidance about spin/block we could use is at
> the OS/VM level: Are there a lot of available cores? Are there
> a lot of blocked threads? Etc. Depending on the state of the machine,
> we'd want to spin at least a little. The idea for the JEP 143 work
> is that these kinds of decisions/actions could be done inside
> the VM implementation of LockSupport.park itself, since they
> don't rely on context. They could then equally apply to j.u.c
> sync and the similar park-like calls underlying synchronized blocks.
> It was a pretty good idea. Hopefully someday.
>
> -Doug
>
>
>
> > 2013/7/8 Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu>>
> >
> >     On 07/07/13 15:45, Gustav ?kesson wrote:
> >
> >         I take that ReentrantLock doesn't currently support any
> spinning behing the
> >         scenes, but is there any thought on introducing this?
> Is it too risky
> >         that the
> >         cost of spinning could outweight the benefits? Is there
> are thoughts on
> >         adding
> >         spinning mechanism to other concurrent collections similar to
> >         LinkedTransferQueue?
> >
> >
> >     Adaptive spinning at the level of ReentrantLock or
> AbstractQueuedSynchronizer
> >     is a little tricky to add in a way that would almost always
> be helpful,
> >     so there was a tentative plan to rely on VM support one layer down.
> >     See JEP 143: http://openjdk.java.net/jeps/__143
> >     <http://openjdk.java.net/jeps/143>
> >     But Oracle decided not to commit to this for JDK8.
> >
> >     So at the moment there's not a tentative plan, except to continue
> >     to tell people that it they can still add simple forms of
> bounded spinning
> >     using tryLock() and related constructions.
> >
> >     -Doug
> >
> >
> >
> >
> >
> >     _________________________________________________
> >     Concurrency-interest mailing list
> >     Concurrency-interest at cs.__oswego.edu
> <mailto:Concurrency-interest at cs.oswego.edu>
> >     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
> >     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> >
> >
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> -----
> No virus found in this message.
> Checked by AVG - www.avg.com
> Version: 2013.0.3345 / Virus Database: 3204/6472 - Release Date: 07/07/13
>
>


From davidcholmes at aapt.net.au  Mon Jul  8 07:17:40 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 8 Jul 2013 21:17:40 +1000
Subject: [concurrency-interest] Mapping between OS threads and Java
	threads
In-Reply-To: <CAMjJYBn8c_xaNZXsoDN-8ju2G5Rh3gs+mShui0oJMGHctOzaAA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKELNJOAA.davidcholmes@aapt.net.au>

On all platforms, and all architectures currently supported by OpenJDK, the
Hotspot VM uses a single native thread for each Java thread.

If you need to know about other VMs then you need to ask the VM vendors.

David
  -----Original Message-----
  From: Nader Aeinehchi [mailto:nader.aeinehchi at gmail.com]
  Sent: Monday, 8 July 2013 8:07 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Mapping between OS threads and Java
threads


  Hi

  "In hotspot, quite simply, each Java thread is associated directly with a
native OS thread."

  Does the above statement apply for the following?

  - different JVM's: Oracle JVM, IBM JVM etc
  - different operating systems: Linux, Windows, z/OS etc
  - differnt hardware architectures: X86, RISC, mainframe

  Best regards
  Nader Aeinehchi



  On Mon, Jul 8, 2013 at 10:14 AM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Hi,

    In hotspot, quite simply, each Java thread is associated directly with a
native OS thread.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohammad Ali
Asgar
      Sent: Monday, 8 July 2013 5:22 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Mapping between OS threads and Java
threads


      Hello,


      The API documentation of Java on Thread.State says:
      "A thread can be in only one state at a given point in time. These
states are virtual machine states which do not reflect any operating system
thread states."


      The specific states mentioned are:
      - NEW (A thread that has not yet started is in this state.)
      - RUNNABLE (A thread executing in the Java virtual machine is in this
state.)
      - BLOCKED (A thread that is blocked waiting for a monitor lock is in
this state.)
      - WAITING (A thread that is waiting indefinitely for another thread to
perform a particular action is in this state.)
      - TIMED_WAITING (A thread that is waiting for another thread to
perform an action for up to a specified waiting time is in this state.)
      - TERMINATED (A thread that has exited is in this state.)


      Is there any good reference that could help one learn the mapping
between OS level threads and application threads hosted by JVM? I am trying
to picture how the OS level threads hold up a JVM which, in turn, holds up
an application containing one or more threads in it.


      Thank you.


      Best Regards,
      Asgar
      Ericsson

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest




  No virus found in this message.
  Checked by AVG - www.avg.com
  Version: 2013.0.3345 / Virus Database: 3204/6472 - Release Date: 07/07/13
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/5d2d146f/attachment-0001.html>

From dl at cs.oswego.edu  Mon Jul  8 07:25:15 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Jul 2013 07:25:15 -0400
Subject: [concurrency-interest] RRWL with 'bad' Thread.getId()
	implementations
In-Reply-To: <51DA9E47.4050203@gmail.com>
References: <CDEF4299.2F31E%chris.w.dennis@gmail.com>
	<51CA21B5.2010907@cs.oswego.edu> <51DA9E47.4050203@gmail.com>
Message-ID: <51DAA19B.8000509@cs.oswego.edu>

On 07/08/13 07:11, Peter Levart wrote:
> On 06/26/2013 01:03 AM, Doug Lea wrote:
>> The alternative of using a weak ref is not a good option.

(Which still makes me hesitant to consider your suggestion
in your previous post.)

>> The reason we use id's to begin with is that if you store
>> a Thread ref in a ThreadLocal, even with a weak rewf wrapper,
>> there several common scenarios where it cannot be GCed.
>> (Some of you might recall discussions about Ephemerons
>> motivated by this issue. But these never arrived.)
>
> Hi Doug (or anyone),
>
> I'm interested in the scenarios when an otherwise unreachable Thread object of a
> stopped thread could not be GC-ed because the WeakReference pointing to that
> Thread was referenced from the ThreadLocal map of the same thread.
>


We first discovered these issues in web frameworks that
cleverly session-ize Java code by transforming (via
byte-code tools or annotations) essentially all objects
to ThreadLocals. In which case scenarios that otherwise
seem too crazy to contemplate become commonplace.

-Doug



From nathan.reynolds at oracle.com  Mon Jul  8 11:46:07 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 08 Jul 2013 08:46:07 -0700
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEELNJOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEELNJOAA.davidcholmes@aapt.net.au>
Message-ID: <51DADEBF.60009@oracle.com>

If the physical cores aren't at 100%, would it be a good idea to let 
threads spin?  For example, if I have a 16-core machine with 
hyper-threading, then I could run 32 threads concurrently.  Hence, I 
would only allow spinning if the CPU usage is ? 50% (i.e. ? 16 
threads).  This will ensure that spinning threads don't take resources 
from threads doing actual work.  If say 5 threads are doing real work, 
then I would only allow for 11 threads to spin.

The easy way to implement this is to have the spinning threads monitor 
the CPU usage.  If the usage goes above 50%, then the spinning thread(s) 
block.  Coordinating which threads will block could be tricky.

Of course, if the lock requires FIFO ordering of the acquiring threads, 
then spinning isn't going to help and the threads should just block.

Note: To make this discussion easier, let's ignore power consumption and 
virtualization.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/8/2013 4:15 AM, David Holmes wrote:
> Though I'll add that spinning for Lock acquisition may be a good thing, but
> park doesn't know why you are parking. In the context of
> AbstractQueuedSynchronizer there is also no knowledge of why you need to
> block.
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
>> Sent: Monday, 8 July 2013 7:41 PM
>> To: Ruslan Cheremin
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Spinning locks
>>
>>
>> On 07/08/13 01:01, Ruslan Cheremin wrote:
>>> Doug, could you make it a bit cleaner -- what makes adaptive
>> spinning in RL
>>> tricky? AFAIK, there is (adaptive?) spinning behind
>> synchronized(), why couldn't
>>> it be just copied inside RL?  What is the key thing which could
>> be done inside
>>> VM, but not on java level?
>>>
>>>
>> When we know something about the structure of thread interactions
>> in a j.u.c class we can and usually do write custom spin/help/block
>> code. But for arbitrary usages of arbitrary exclusive locks, we don't
>> know much, and the only guidance about spin/block we could use is at
>> the OS/VM level: Are there a lot of available cores? Are there
>> a lot of blocked threads? Etc. Depending on the state of the machine,
>> we'd want to spin at least a little. The idea for the JEP 143 work
>> is that these kinds of decisions/actions could be done inside
>> the VM implementation of LockSupport.park itself, since they
>> don't rely on context. They could then equally apply to j.u.c
>> sync and the similar park-like calls underlying synchronized blocks.
>> It was a pretty good idea. Hopefully someday.
>>
>> -Doug
>>
>>
>>
>>> 2013/7/8 Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu>>
>>>
>>>      On 07/07/13 15:45, Gustav ?kesson wrote:
>>>
>>>          I take that ReentrantLock doesn't currently support any
>> spinning behing the
>>>          scenes, but is there any thought on introducing this?
>> Is it too risky
>>>          that the
>>>          cost of spinning could outweight the benefits? Is there
>> are thoughts on
>>>          adding
>>>          spinning mechanism to other concurrent collections similar to
>>>          LinkedTransferQueue?
>>>
>>>
>>>      Adaptive spinning at the level of ReentrantLock or
>> AbstractQueuedSynchronizer
>>>      is a little tricky to add in a way that would almost always
>> be helpful,
>>>      so there was a tentative plan to rely on VM support one layer down.
>>>      See JEP 143: http://openjdk.java.net/jeps/__143
>>>      <http://openjdk.java.net/jeps/143>
>>>      But Oracle decided not to commit to this for JDK8.
>>>
>>>      So at the moment there's not a tentative plan, except to continue
>>>      to tell people that it they can still add simple forms of
>> bounded spinning
>>>      using tryLock() and related constructions.
>>>
>>>      -Doug
>>>
>>>
>>>
>>>
>>>
>>>      _________________________________________________
>>>      Concurrency-interest mailing list
>>>      Concurrency-interest at cs.__oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>      http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>>      <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> -----
>> No virus found in this message.
>> Checked by AVG - www.avg.com
>> Version: 2013.0.3345 / Virus Database: 3204/6472 - Release Date: 07/07/13
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/60118405/attachment.html>

From gustav.r.akesson at gmail.com  Mon Jul  8 12:09:04 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Mon, 8 Jul 2013 18:09:04 +0200
Subject: [concurrency-interest] Spinning locks
In-Reply-To: <51DADEBF.60009@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEELNJOAA.davidcholmes@aapt.net.au>
	<51DADEBF.60009@oracle.com>
Message-ID: <CAKEw5+6=_yUQjLe-PKth3oaVv+ewGN4dnH640+qm0yoXmxo6+A@mail.gmail.com>

Perhaps this would work as an algorithm, but implementation wise it ought
to be very expensive to poll CPU usage at such rate which this algorithm
requires. I'm thinking several thousands polls per seconds. And if it isn't
expensive I suppose that the value is too coarse-grained to serve this
purpose (as with using timer in spinning).

Besy Regards,

Gustav ?kesson
Den 8 jul 2013 17:55 skrev "Nathan Reynolds" <nathan.reynolds at oracle.com>:

>  If the physical cores aren't at 100%, would it be a good idea to let
> threads spin?  For example, if I have a 16-core machine with
> hyper-threading, then I could run 32 threads concurrently.  Hence, I would
> only allow spinning if the CPU usage is ? 50% (i.e. ? 16 threads).  This
> will ensure that spinning threads don't take resources from threads doing
> actual work.  If say 5 threads are doing real work, then I would only allow
> for 11 threads to spin.
>
> The easy way to implement this is to have the spinning threads monitor the
> CPU usage.  If the usage goes above 50%, then the spinning thread(s)
> block.  Coordinating which threads will block could be tricky.
>
> Of course, if the lock requires FIFO ordering of the acquiring threads,
> then spinning isn't going to help and the threads should just block.
>
> Note: To make this discussion easier, let's ignore power consumption and
> virtualization.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 7/8/2013 4:15 AM, David Holmes wrote:
>
> Though I'll add that spinning for Lock acquisition may be a good thing, but
> park doesn't know why you are parking. In the context of
> AbstractQueuedSynchronizer there is also no knowledge of why you need to
> block.
>
> David
>
>
>  -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu <concurrency-interest-bounces at cs.oswego.edu>]On Behalf Of Doug Lea
> Sent: Monday, 8 July 2013 7:41 PM
> To: Ruslan Cheremin
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Spinning locks
>
>
> On 07/08/13 01:01, Ruslan Cheremin wrote:
>
>  Doug, could you make it a bit cleaner -- what makes adaptive
>
>  spinning in RL
>
>  tricky? AFAIK, there is (adaptive?) spinning behind
>
>  synchronized(), why couldn't
>
>  it be just copied inside RL?  What is the key thing which could
>
>  be done inside
>
>  VM, but not on java level?
>
>
>
>
> When we know something about the structure of thread interactions
> in a j.u.c class we can and usually do write custom spin/help/block
> code. But for arbitrary usages of arbitrary exclusive locks, we don't
> know much, and the only guidance about spin/block we could use is at
> the OS/VM level: Are there a lot of available cores? Are there
> a lot of blocked threads? Etc. Depending on the state of the machine,
> we'd want to spin at least a little. The idea for the JEP 143 work
> is that these kinds of decisions/actions could be done inside
> the VM implementation of LockSupport.park itself, since they
> don't rely on context. They could then equally apply to j.u.c
> sync and the similar park-like calls underlying synchronized blocks.
> It was a pretty good idea. Hopefully someday.
>
> -Doug
>
>
>
>
>  2013/7/8 Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu> <dl at cs.oswego.edu>>
>
>     On 07/07/13 15:45, Gustav ?kesson wrote:
>
>         I take that ReentrantLock doesn't currently support any
>
>  spinning behing the
>
>          scenes, but is there any thought on introducing this?
>
>  Is it too risky
>
>          that the
>         cost of spinning could outweight the benefits? Is there
>
>  are thoughts on
>
>          adding
>         spinning mechanism to other concurrent collections similar to
>         LinkedTransferQueue?
>
>
>     Adaptive spinning at the level of ReentrantLock or
>
>  AbstractQueuedSynchronizer
>
>      is a little tricky to add in a way that would almost always
>
>  be helpful,
>
>      so there was a tentative plan to rely on VM support one layer down.
>     See JEP 143: http://openjdk.java.net/jeps/__143
>     <http://openjdk.java.net/jeps/143> <http://openjdk.java.net/jeps/143>
>     But Oracle decided not to commit to this for JDK8.
>
>     So at the moment there's not a tentative plan, except to continue
>     to tell people that it they can still add simple forms of
>
>  bounded spinning
>
>      using tryLock() and related constructions.
>
>     -Doug
>
>
>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu
>
>  <mailto:Concurrency-interest at cs.oswego.edu> <Concurrency-interest at cs.oswego.edu>
>
>      http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest> <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> -----
> No virus found in this message.
> Checked by AVG - www.avg.com
> Version: 2013.0.3345 / Virus Database: 3204/6472 - Release Date: 07/07/13
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/462ac1c9/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon Jul  8 12:42:37 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 8 Jul 2013 20:42:37 +0400
Subject: [concurrency-interest] RRWL with 'bad' Thread.getId()
	implementations
Message-ID: <CAO-wXwKpU32SQjCr0pW9TH5_w_tTvmeONUfS0Su3kBS0crZW=Q@mail.gmail.com>

Peter Levart wrote:
>>I'm interested in the scenarios when an otherwise unreachable Thread
>>object of a stopped thread could not be GC-ed because the WeakReference
>>pointing to that Thread was referenced from the ThreadLocal map of the
>>same thread.

Doug Lea wrote:
>>in web frameworks that
>>cleverly session-ize Java code by transforming (via
>>byte-code tools or annotations) essentially all objects
>>to ThreadLocals

Could someone please explain to me why/how is that even possible that
weakly reachable object can't be GC'ed? I see that Doug Lea provided an
example of scenario, but it doesn't explains why weakly reachable object
can't be GC'ed in that scenario.

--
Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/ef06bba1/attachment.html>

From thurston at nomagicsoftware.com  Mon Jul  8 14:43:57 2013
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 8 Jul 2013 11:43:57 -0700 (PDT)
Subject: [concurrency-interest] java.lang.StackOverflowError on
	ConcurrentHashMap V8
In-Reply-To: <CANPzfU8uq_c_UUpdifREDLOU1x6JnXmMzKEfUuKmk9fom1Zm0Q@mail.gmail.com>
References: <6E2CD8AC-A125-4A99-B225-2085656488EA@tmax.co.kr>
	<CA+kOe08igLhGRfscasiXc5+i0f2_SQm9ipJX3E=TWXgS1apc5w@mail.gmail.com>
	<51D55577.6050706@cs.oswego.edu> <51D6B760.3090101@cs.oswego.edu>
	<6785144F-71F7-439F-BF69-DD429947F722@atlassian.com>
	<CAEJX8orexZhC90dV3mcBzW8mBoeMwmUS0cMi=tWe-g0p=9dmAQ@mail.gmail.com>
	<CANPzfU8uq_c_UUpdifREDLOU1x6JnXmMzKEfUuKmk9fom1Zm0Q@mail.gmail.com>
Message-ID: <1373309037748-9881.post@n7.nabble.com>

I'm assuming that Scala implements tail recursion by 'loopifying', no?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/java-lang-StackOverflowError-on-ConcurrentHashMap-V8-tp9834p9881.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From die9823 at gmail.com  Mon Jul  8 14:58:33 2013
From: die9823 at gmail.com (Trallan Die Tralla)
Date: Mon, 8 Jul 2013 19:58:33 +0100
Subject: [concurrency-interest] effective field access in shared class
Message-ID: <1188796159.20130708195833@gmail.com>


Hi,

I have a class (SharedData) that has some fields that are modified and read by
Thread-A but only read by Thread-B.

Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
the fields volatile, but wondering if there is a better way that can help Thread-A to
run more effectively during the periods while Thread-B sleeps.

Basically I'd like to have a "magic method" that can issue a memory sync when
Thread-B needs to see the fields.

Thread-B might not need to see the latest status but it should not see partial
updates to the fields (even on 32 bit JVM).

I was wondering removing the volatile, but not sure if it can cause issues.

Example class:

class SharedData {

      private volatile field1;
      private volatile field2;

      // setters / getters
}

Thanks for any ideas...

Thanks,
Trallan 


From igor.fedan at gmail.com  Mon Jul  8 15:09:33 2013
From: igor.fedan at gmail.com (Igor Fedan)
Date: Mon, 8 Jul 2013 15:09:33 -0400
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <1188796159.20130708195833@gmail.com>
References: <1188796159.20130708195833@gmail.com>
Message-ID: <E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>

Hi,

You need a memory barrier, before Thread B starts reading the data.

Best regards, Igor Fedan.

On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com> wrote:

> 
> Hi,
> 
> I have a class (SharedData) that has some fields that are modified and read by
> Thread-A but only read by Thread-B.
> 
> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
> the fields volatile, but wondering if there is a better way that can help Thread-A to
> run more effectively during the periods while Thread-B sleeps.
> 
> Basically I'd like to have a "magic method" that can issue a memory sync when
> Thread-B needs to see the fields.
> 
> Thread-B might not need to see the latest status but it should not see partial
> updates to the fields (even on 32 bit JVM).
> 
> I was wondering removing the volatile, but not sure if it can cause issues.
> 
> Example class:
> 
> class SharedData {
> 
>      private volatile field1;
>      private volatile field2;
> 
>      // setters / getters
> }
> 
> Thanks for any ideas...
> 
> Thanks,
> Trallan 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Mon Jul  8 15:29:05 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 08 Jul 2013 12:29:05 -0700
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <1188796159.20130708195833@gmail.com>
References: <1188796159.20130708195833@gmail.com>
Message-ID: <51DB1301.2070300@oracle.com>

Why not use Atomic___FieldUpdater.lazySet()?  If I remember right, this 
has a store-store fence (which becomes a no-op on x86).  This is much 
cheaper than the store-load fence which is required by volatile writes.

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/atomic/AtomicReferenceFieldUpdater.html#lazySet%28T,%20V%29

-Nathan

On 7/8/2013 11:58 AM, Trallan Die Tralla wrote:
> Hi,
>
> I have a class (SharedData) that has some fields that are modified and read by
> Thread-A but only read by Thread-B.
>
> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
> the fields volatile, but wondering if there is a better way that can help Thread-A to
> run more effectively during the periods while Thread-B sleeps.
>
> Basically I'd like to have a "magic method" that can issue a memory sync when
> Thread-B needs to see the fields.
>
> Thread-B might not need to see the latest status but it should not see partial
> updates to the fields (even on 32 bit JVM).
>
> I was wondering removing the volatile, but not sure if it can cause issues.
>
> Example class:
>
> class SharedData {
>
>        private volatile field1;
>        private volatile field2;
>
>        // setters / getters
> }
>
> Thanks for any ideas...
>
> Thanks,
> Trallan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/1bbe101f/attachment.html>

From dahankzter at gmail.com  Mon Jul  8 15:30:57 2013
From: dahankzter at gmail.com (Henrik Johansson)
Date: Mon, 8 Jul 2013 21:30:57 +0200
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <1188796159.20130708195833@gmail.com>
References: <1188796159.20130708195833@gmail.com>
Message-ID: <CAKOF6965GGJ5kae-XYM4yzZTpL3jpxb3d0AwaHKy9hOFS1RUDA@mail.gmail.com>

You could perhaps use higher levels of abstractions such as message passing
to have the recurring process ask for the current state. The producer can
then go about its business without the need for the globally visioner
shared state.
On Jul 8, 2013 9:04 PM, "Trallan Die Tralla" <die9823 at gmail.com> wrote:

>
> Hi,
>
> I have a class (SharedData) that has some fields that are modified and
> read by
> Thread-A but only read by Thread-B.
>
> Thread-A runs constantly, but Thread-B is scheduled to run every 5
> minutes. I declared
> the fields volatile, but wondering if there is a better way that can help
> Thread-A to
> run more effectively during the periods while Thread-B sleeps.
>
> Basically I'd like to have a "magic method" that can issue a memory sync
> when
> Thread-B needs to see the fields.
>
> Thread-B might not need to see the latest status but it should not see
> partial
> updates to the fields (even on 32 bit JVM).
>
> I was wondering removing the volatile, but not sure if it can cause issues.
>
> Example class:
>
> class SharedData {
>
>       private volatile field1;
>       private volatile field2;
>
>       // setters / getters
> }
>
> Thanks for any ideas...
>
> Thanks,
> Trallan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/b5b8efdb/attachment.html>

From nathan.reynolds at oracle.com  Mon Jul  8 15:33:20 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 08 Jul 2013 12:33:20 -0700
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
Message-ID: <51DB1400.5030300@oracle.com>

If Thread A keeps the field values in registers, then there isn't 
anything Thread B can do to see the values.  So, you need Thread A to 
have some sort of memory barrier so that JIT can't leave the field 
values in registers.

-Nathan

On 7/8/2013 12:09 PM, Igor Fedan wrote:
> Hi,
>
> You need a memory barrier, before Thread B starts reading the data.
>
> Best regards, Igor Fedan.
>
> On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com> wrote:
>
>> Hi,
>>
>> I have a class (SharedData) that has some fields that are modified and read by
>> Thread-A but only read by Thread-B.
>>
>> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
>> the fields volatile, but wondering if there is a better way that can help Thread-A to
>> run more effectively during the periods while Thread-B sleeps.
>>
>> Basically I'd like to have a "magic method" that can issue a memory sync when
>> Thread-B needs to see the fields.
>>
>> Thread-B might not need to see the latest status but it should not see partial
>> updates to the fields (even on 32 bit JVM).
>>
>> I was wondering removing the volatile, but not sure if it can cause issues.
>>
>> Example class:
>>
>> class SharedData {
>>
>>       private volatile field1;
>>       private volatile field2;
>>
>>       // setters / getters
>> }
>>
>> Thanks for any ideas...
>>
>> Thanks,
>> Trallan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/bd9b619b/attachment.html>

From yankee.sierra at gmail.com  Mon Jul  8 15:36:41 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Mon, 8 Jul 2013 15:36:41 -0400
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
Message-ID: <CAE+h5-DGT4BU5snO1axcy18Pdha24+cCNp_HRYXmqhor3ETrCg@mail.gmail.com>

What about a cooperative approach, in which Thread-B asks for an update
from Thread-A?

You could use a concurrent queues of CountDownLatches. As Thread-A does its
thing, it periodically polls the queue for a latch. If it doesn't find one
(it gets a null), it merrily goes on its way. Otherwise, it invokes
countDown() on the latch and then goes merrily on its way.

On the Thread-B side, you create a CountDownLatch(1), put it into the queue
and then await() on it. When await() finishes, that establishes a
happens-before with Thread-A counting down on that same latch.


On Mon, Jul 8, 2013 at 3:09 PM, Igor Fedan <igor.fedan at gmail.com> wrote:

> Hi,
>
> You need a memory barrier, before Thread B starts reading the data.
>
> Best regards, Igor Fedan.
>
> On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com> wrote:
>
> >
> > Hi,
> >
> > I have a class (SharedData) that has some fields that are modified and
> read by
> > Thread-A but only read by Thread-B.
> >
> > Thread-A runs constantly, but Thread-B is scheduled to run every 5
> minutes. I declared
> > the fields volatile, but wondering if there is a better way that can
> help Thread-A to
> > run more effectively during the periods while Thread-B sleeps.
> >
> > Basically I'd like to have a "magic method" that can issue a memory sync
> when
> > Thread-B needs to see the fields.
> >
> > Thread-B might not need to see the latest status but it should not see
> partial
> > updates to the fields (even on 32 bit JVM).
> >
> > I was wondering removing the volatile, but not sure if it can cause
> issues.
> >
> > Example class:
> >
> > class SharedData {
> >
> >      private volatile field1;
> >      private volatile field2;
> >
> >      // setters / getters
> > }
> >
> > Thanks for any ideas...
> >
> > Thanks,
> > Trallan
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/6018a980/attachment.html>

From yankee.sierra at gmail.com  Mon Jul  8 15:52:58 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Mon, 8 Jul 2013 15:52:58 -0400
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <51DB1400.5030300@oracle.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
	<51DB1400.5030300@oracle.com>
Message-ID: <CAE+h5-Cddku8gOp4nCXdCG2DKX0PYUhP8xuyDA==g+3=2pwAZg@mail.gmail.com>

You also need to "prevent" Thread-A from partially publishing results to
Thread-B, which is the trickier bit since Java doesn't have that kind of
semantics. If I read the original question correctly, it's okay if separate
fields aren't updated in lock-step; but this publish-prevention is needed
for longs and doubles. You could make those fields volatile, but as I
understand it, that would mean extra memory syncs for the writes in
practice (at least on x86). Is the goal to reduce those?

If so, I think the thing to do would be for Thread-A to create a new object
containing the state of the world on demand, as Henrik suggested.


On Mon, Jul 8, 2013 at 3:33 PM, Nathan Reynolds
<nathan.reynolds at oracle.com>wrote:

>  If Thread A keeps the field values in registers, then there isn't
> anything Thread B can do to see the values.  So, you need Thread A to have
> some sort of memory barrier so that JIT can't leave the field values in
> registers.
>
> -Nathan
>
> On 7/8/2013 12:09 PM, Igor Fedan wrote:
>
> Hi,
>
> You need a memory barrier, before Thread B starts reading the data.
>
> Best regards, Igor Fedan.
>
> On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com> <die9823 at gmail.com> wrote:
>
>
>  Hi,
>
> I have a class (SharedData) that has some fields that are modified and read by
> Thread-A but only read by Thread-B.
>
> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
> the fields volatile, but wondering if there is a better way that can help Thread-A to
> run more effectively during the periods while Thread-B sleeps.
>
> Basically I'd like to have a "magic method" that can issue a memory sync when
> Thread-B needs to see the fields.
>
> Thread-B might not need to see the latest status but it should not see partial
> updates to the fields (even on 32 bit JVM).
>
> I was wondering removing the volatile, but not sure if it can cause issues.
>
> Example class:
>
> class SharedData {
>
>      private volatile field1;
>      private volatile field2;
>
>      // setters / getters
> }
>
> Thanks for any ideas...
>
> Thanks,
> Trallan
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/59b4ee34/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon Jul  8 15:59:14 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 08 Jul 2013 12:59:14 -0700
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <CAE+h5-DGT4BU5snO1axcy18Pdha24+cCNp_HRYXmqhor3ETrCg@mail.gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
	<CAE+h5-DGT4BU5snO1axcy18Pdha24+cCNp_HRYXmqhor3ETrCg@mail.gmail.com>
Message-ID: <51DB1A12.4010903@oracle.com>

This sparked an idea.

What if Thread A had the following in its loop?  m_report is a volatile 
boolean.  When set, Thread A will update the volatile fields with data 
and then set m_report to false.  When Thread B wants some data, it sets 
m_report to true and sleep-waits for Thread A to set it to false.  Once 
it is false, we are guaranteed to have consistent and up to date data 
for Thread B.  The cost to Thread A is that it has to read a field from 
L1 cache.  This will cost 3 cycles (x86) on Thread A and a JIT memory 
barrier which might prevent JIT from doing some optimizations.

{
    ...

    if (m_report)
    {
       // Update volatile fields with data
       m_report = false;
    }
}

-Nathan

On 7/8/2013 12:36 PM, Yuval Shavit wrote:
> What about a cooperative approach, in which Thread-B asks for an 
> update from Thread-A?
>
> You could use a concurrent queues of CountDownLatches. As Thread-A 
> does its thing, it periodically polls the queue for a latch. If it 
> doesn't find one (it gets a null), it merrily goes on its way. 
> Otherwise, it invokes countDown() on the latch and then goes merrily 
> on its way.
>
> On the Thread-B side, you create a CountDownLatch(1), put it into the 
> queue and then await() on it. When await() finishes, that establishes 
> a happens-before with Thread-A counting down on that same latch.
>
>
> On Mon, Jul 8, 2013 at 3:09 PM, Igor Fedan <igor.fedan at gmail.com 
> <mailto:igor.fedan at gmail.com>> wrote:
>
>     Hi,
>
>     You need a memory barrier, before Thread B starts reading the data.
>
>     Best regards, Igor Fedan.
>
>     On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com
>     <mailto:die9823 at gmail.com>> wrote:
>
>     >
>     > Hi,
>     >
>     > I have a class (SharedData) that has some fields that are
>     modified and read by
>     > Thread-A but only read by Thread-B.
>     >
>     > Thread-A runs constantly, but Thread-B is scheduled to run every
>     5 minutes. I declared
>     > the fields volatile, but wondering if there is a better way that
>     can help Thread-A to
>     > run more effectively during the periods while Thread-B sleeps.
>     >
>     > Basically I'd like to have a "magic method" that can issue a
>     memory sync when
>     > Thread-B needs to see the fields.
>     >
>     > Thread-B might not need to see the latest status but it should
>     not see partial
>     > updates to the fields (even on 32 bit JVM).
>     >
>     > I was wondering removing the volatile, but not sure if it can
>     cause issues.
>     >
>     > Example class:
>     >
>     > class SharedData {
>     >
>     >      private volatile field1;
>     >      private volatile field2;
>     >
>     >      // setters / getters
>     > }
>     >
>     > Thanks for any ideas...
>     >
>     > Thanks,
>     > Trallan
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/755049b6/attachment.html>

From nitsanw at yahoo.com  Mon Jul  8 16:48:06 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Mon, 8 Jul 2013 13:48:06 -0700 (PDT)
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <CAE+h5-Cddku8gOp4nCXdCG2DKX0PYUhP8xuyDA==g+3=2pwAZg@mail.gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
	<51DB1400.5030300@oracle.com>
	<CAE+h5-Cddku8gOp4nCXdCG2DKX0PYUhP8xuyDA==g+3=2pwAZg@mail.gmail.com>
Message-ID: <1373316486.21090.YahooMailNeo@web120704.mail.ne1.yahoo.com>

If you need to notify Thread A and be guaranteed an immediate/near immediate response then you will in all probability make the implementation of Thread A more complex, and perhaps slower than intended. If the response time is not important then setting some flag from B to trigger a publication from A might be acceptable. Note that at a minimum this will require a LOADLOAD barrier, which while being a noop (in the sense no special instruction is required) is not free as it inhibits optimisation. See here for a benchmark/demonstration of that point:?http://brooker.co.za/blog/2012/09/10/volatile.html
If you are willing to lose precision then update the outgoing value periodically (i.e. every 50/100/1000 updates) from a plain counter. Getting a time read and comparing may end up more expensive than a lazySet, so my guess would be that timer based implementations are not your best choice.
Ultimately I think you should listen to Mr Reynolds and either use Atomic*.lazySet or an AtomicFieldUpdater. Note that these are only appropriate if you have a single thread writing to the variable. LazySet is not exactly free (as suggested below), but it's pretty cheap. It looks like a plain mov (and it is) but it prevents the compiler from re-ordering around it. I wrote a blog post about lazySet a while back, tracing some of the history and definitions related to it:?http://psy-lob-saw.blogspot.com/2012/12/atomiclazyset-is-performance-win-for.html
There's a more recent post dedicated to concurrent counters which might be what you're looking for:?http://psy-lob-saw.blogspot.com/2013/06/java-concurrent-counters-by-numbers.html
Using a plain store(i.e not volatile or lazySet/putOrdered) can still work, but is not good practice as mentioned below.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/8f997ada/attachment.html>

From zhong.j.yu at gmail.com  Mon Jul  8 17:04:37 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 8 Jul 2013 16:04:37 -0500
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <1188796159.20130708195833@gmail.com>
References: <1188796159.20130708195833@gmail.com>
Message-ID: <CACuKZqEeJL_qYqdSiG1jUyJWJT8E+j_Zw-6A4H7xhE6Tke6wKA@mail.gmail.com>

Maybe vanilla synchronized{} is the fastest, if VM does biased locking.

On Mon, Jul 8, 2013 at 1:58 PM, Trallan Die Tralla <die9823 at gmail.com> wrote:
>
> Hi,
>
> I have a class (SharedData) that has some fields that are modified and read by
> Thread-A but only read by Thread-B.
>
> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
> the fields volatile, but wondering if there is a better way that can help Thread-A to
> run more effectively during the periods while Thread-B sleeps.
>
> Basically I'd like to have a "magic method" that can issue a memory sync when
> Thread-B needs to see the fields.
>
> Thread-B might not need to see the latest status but it should not see partial
> updates to the fields (even on 32 bit JVM).
>
> I was wondering removing the volatile, but not sure if it can cause issues.
>
> Example class:
>
> class SharedData {
>
>       private volatile field1;
>       private volatile field2;
>
>       // setters / getters
> }
>
> Thanks for any ideas...
>
> Thanks,
> Trallan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From igor.fedan at gmail.com  Mon Jul  8 17:51:09 2013
From: igor.fedan at gmail.com (Igor Fedan)
Date: Mon, 8 Jul 2013 17:51:09 -0400
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <CACuKZqEeJL_qYqdSiG1jUyJWJT8E+j_Zw-6A4H7xhE6Tke6wKA@mail.gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<CACuKZqEeJL_qYqdSiG1jUyJWJT8E+j_Zw-6A4H7xhE6Tke6wKA@mail.gmail.com>
Message-ID: <C779D6A5-7BAB-4E64-B7F5-868C60B35AEA@gmail.com>


>> class SharedData {
>> 
>>      private field1;
>>      private field2;
>> 
>>      // setters / getters

Thread A (write, read without volatile) - you don't need to see changes in other threads (Thread B is read-only)

Thread B (read volatile through Unsafe.getXXXVolatile)  - you will get the latest data through memory barrier


Best regards, Igor Fedan.

On Jul 8, 2013, at 17:04, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> Maybe vanilla synchronized{} is the fastest, if VM does biased locking.
> 
> On Mon, Jul 8, 2013 at 1:58 PM, Trallan Die Tralla <die9823 at gmail.com> wrote:
>> 
>> Hi,
>> 
>> I have a class (SharedData) that has some fields that are modified and read by
>> Thread-A but only read by Thread-B.
>> 
>> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
>> the fields volatile, but wondering if there is a better way that can help Thread-A to
>> run more effectively during the periods while Thread-B sleeps.
>> 
>> Basically I'd like to have a "magic method" that can issue a memory sync when
>> Thread-B needs to see the fields.
>> 
>> Thread-B might not need to see the latest status but it should not see partial
>> updates to the fields (even on 32 bit JVM).
>> 
>> I was wondering removing the volatile, but not sure if it can cause issues.
>> 
>> Example class:
>> 
>> class SharedData {
>> 
>>      private volatile field1;
>>      private volatile field2;
>> 
>>      // setters / getters
>> }
>> 
>> Thanks for any ideas...
>> 
>> Thanks,
>> Trallan
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Mon Jul  8 18:36:31 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 08 Jul 2013 15:36:31 -0700
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <C779D6A5-7BAB-4E64-B7F5-868C60B35AEA@gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<CACuKZqEeJL_qYqdSiG1jUyJWJT8E+j_Zw-6A4H7xhE6Tke6wKA@mail.gmail.com>
	<C779D6A5-7BAB-4E64-B7F5-868C60B35AEA@gmail.com>
Message-ID: <51DB3EEF.8010501@oracle.com>

If Thread A doesn't write to a volatile, what guarantee is there that 
field1 and field2 will be written to?  JIT could decide to cache these 
fields' values in registers.  For example, the following loop could be 
optimized by JIT to do the increments in registers.  Thus, if Thread A 
were executing this loop, then there isn't anything Thread B can do to 
get the values that Thread A has computed.  The values don't exist in 
L1, L2, L3 or L4 cache or main memory.  An even worse situation is that 
JIT could get rid of "field1++" and "field2++" so that the loop is 
completely empty!  Then for sure, there is nothing Thread B or even 
Thread A can do to get the values!

while (true)
{
     field1++;
     field2++;
}

I thought that in order to guarantee data to be passed from Thread A to 
Thread B, then you need a happens-before relationship between the two 
threads.  Can data be passed without that guarantee?  Yes, but you are 
at the mercy of JIT.

-Nathan

On 7/8/2013 2:51 PM, Igor Fedan wrote:
>>> class SharedData {
>>>
>>>       private field1;
>>>       private field2;
>>>
>>>       // setters / getters
> Thread A (write, read without volatile) - you don't need to see changes in other threads (Thread B is read-only)
>
> Thread B (read volatile through Unsafe.getXXXVolatile)  - you will get the latest data through memory barrier
>
>
> Best regards, Igor Fedan.
>
> On Jul 8, 2013, at 17:04, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Maybe vanilla synchronized{} is the fastest, if VM does biased locking.
>>
>> On Mon, Jul 8, 2013 at 1:58 PM, Trallan Die Tralla <die9823 at gmail.com> wrote:
>>> Hi,
>>>
>>> I have a class (SharedData) that has some fields that are modified and read by
>>> Thread-A but only read by Thread-B.
>>>
>>> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
>>> the fields volatile, but wondering if there is a better way that can help Thread-A to
>>> run more effectively during the periods while Thread-B sleeps.
>>>
>>> Basically I'd like to have a "magic method" that can issue a memory sync when
>>> Thread-B needs to see the fields.
>>>
>>> Thread-B might not need to see the latest status but it should not see partial
>>> updates to the fields (even on 32 bit JVM).
>>>
>>> I was wondering removing the volatile, but not sure if it can cause issues.
>>>
>>> Example class:
>>>
>>> class SharedData {
>>>
>>>       private volatile field1;
>>>       private volatile field2;
>>>
>>>       // setters / getters
>>> }
>>>
>>> Thanks for any ideas...
>>>
>>> Thanks,
>>> Trallan
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130708/b00c7da7/attachment.html>

From aaron.grunthal at infinite-source.de  Mon Jul  8 21:37:02 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 09 Jul 2013 03:37:02 +0200
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <C779D6A5-7BAB-4E64-B7F5-868C60B35AEA@gmail.com>
References: <1188796159.20130708195833@gmail.com>
	<CACuKZqEeJL_qYqdSiG1jUyJWJT8E+j_Zw-6A4H7xhE6Tke6wKA@mail.gmail.com>
	<C779D6A5-7BAB-4E64-B7F5-868C60B35AEA@gmail.com>
Message-ID: <51DB693E.80004@infinite-source.de>

That may work, but it may also fail spectacularly. If the writes to the 
private files are in a tight loop they might never (or almost never) get 
flushed to the main memory, and thus will never become visible, even 
with a volatile read.

Volatile accesses always takes two players. A volatile read basically 
says "make an effort to get the latest data from main memory [or shared 
cache lines]". But that's less than half the effort. At least on x86 
most of the heroics are performed by the volatile write, actually 
flushing the write buffers, acquiring exclusive ownership on caches or 
whatever the cache coherence protocols require (I'm a bit fuzzy on 
that). If you don't do that then there's possibly nothing for the read 
to see, ever.

I think if you read the lazySet API even an ordered put doesn't 
guarantee the write becoming visible to other threads. It only provides 
ordering guarantees, not visibility. And a plain write obviously is 
weaker than an ordered put.

In practice there'll usually be something that'll flush the variables to 
memory eventually. A context switch, some system call, a synchronized 
block, simple register pressure, etc. But there is *no guarantee* for that.

It's basically asking for heisenbugs which depend on the JVM, optimizer 
ergonomics (i.e. their mood of the day), the program's state and the 
constellation of planets.

On 08.07.2013 23:51, Igor Fedan wrote:
>
>>> class SharedData {
>>>
>>>       private field1;
>>>       private field2;
>>>
>>>       // setters / getters
>
> Thread A (write, read without volatile) - you don't need to see changes in other threads (Thread B is read-only)
>
> Thread B (read volatile through Unsafe.getXXXVolatile)  - you will get the latest data through memory barrier
>
>
> Best regards, Igor Fedan.
>
> On Jul 8, 2013, at 17:04, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Maybe vanilla synchronized{} is the fastest, if VM does biased locking.
>>
>> On Mon, Jul 8, 2013 at 1:58 PM, Trallan Die Tralla <die9823 at gmail.com> wrote:
>>>
>>> Hi,
>>>
>>> I have a class (SharedData) that has some fields that are modified and read by
>>> Thread-A but only read by Thread-B.
>>>
>>> Thread-A runs constantly, but Thread-B is scheduled to run every 5 minutes. I declared
>>> the fields volatile, but wondering if there is a better way that can help Thread-A to
>>> run more effectively during the periods while Thread-B sleeps.
>>>
>>> Basically I'd like to have a "magic method" that can issue a memory sync when
>>> Thread-B needs to see the fields.
>>>
>>> Thread-B might not need to see the latest status but it should not see partial
>>> updates to the fields (even on 32 bit JVM).
>>>
>>> I was wondering removing the volatile, but not sure if it can cause issues.
>>>
>>> Example class:
>>>
>>> class SharedData {
>>>
>>>       private volatile field1;
>>>       private volatile field2;
>>>
>>>       // setters / getters
>>> }
>>>
>>> Thanks for any ideas...
>>>
>>> Thanks,
>>> Trallan
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From peter.levart at gmail.com  Tue Jul  9 05:53:05 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 09 Jul 2013 11:53:05 +0200
Subject: [concurrency-interest] effective field access in shared class
In-Reply-To: <51DB1A12.4010903@oracle.com>
References: <1188796159.20130708195833@gmail.com>
	<E9188F2F-40CE-4591-AC18-FF2FF715ACB4@gmail.com>
	<CAE+h5-DGT4BU5snO1axcy18Pdha24+cCNp_HRYXmqhor3ETrCg@mail.gmail.com>
	<51DB1A12.4010903@oracle.com>
Message-ID: <51DBDD81.7070206@gmail.com>

Hi,

The variation to the below idea would be to have a 3rd thread (Thread-C) 
which would periodically set m_report to true (with a constant sleep 
between the sets). That will make Thread-A update the volatile fields 
periodically without asking for system time to frequently. Thread-B 
would eventually wake-up and just read the fields, being aware that 
their state is at most as old as the delay between Thread-C's recurrent 
strokes.

Regards, Peter

On 07/08/2013 09:59 PM, Nathan Reynolds wrote:
> This sparked an idea.
>
> What if Thread A had the following in its loop?  m_report is a 
> volatile boolean.  When set, Thread A will update the volatile fields 
> with data and then set m_report to false.  When Thread B wants some 
> data, it sets m_report to true and sleep-waits for Thread A to set it 
> to false.  Once it is false, we are guaranteed to have consistent and 
> up to date data for Thread B. The cost to Thread A is that it has to 
> read a field from L1 cache.  This will cost 3 cycles (x86) on Thread A 
> and a JIT memory barrier which might prevent JIT from doing some 
> optimizations.
>
> {
>    ...
>
>    if (m_report)
>    {
>       // Update volatile fields with data
>       m_report = false;
>    }
> }
>
> -Nathan
> On 7/8/2013 12:36 PM, Yuval Shavit wrote:
>> What about a cooperative approach, in which Thread-B asks for an 
>> update from Thread-A?
>>
>> You could use a concurrent queues of CountDownLatches. As Thread-A 
>> does its thing, it periodically polls the queue for a latch. If it 
>> doesn't find one (it gets a null), it merrily goes on its way. 
>> Otherwise, it invokes countDown() on the latch and then goes merrily 
>> on its way.
>>
>> On the Thread-B side, you create a CountDownLatch(1), put it into the 
>> queue and then await() on it. When await() finishes, that establishes 
>> a happens-before with Thread-A counting down on that same latch.
>>
>>
>> On Mon, Jul 8, 2013 at 3:09 PM, Igor Fedan <igor.fedan at gmail.com 
>> <mailto:igor.fedan at gmail.com>> wrote:
>>
>>     Hi,
>>
>>     You need a memory barrier, before Thread B starts reading the data.
>>
>>     Best regards, Igor Fedan.
>>
>>     On Jul 8, 2013, at 14:58, Trallan Die Tralla <die9823 at gmail.com
>>     <mailto:die9823 at gmail.com>> wrote:
>>
>>     >
>>     > Hi,
>>     >
>>     > I have a class (SharedData) that has some fields that are
>>     modified and read by
>>     > Thread-A but only read by Thread-B.
>>     >
>>     > Thread-A runs constantly, but Thread-B is scheduled to run
>>     every 5 minutes. I declared
>>     > the fields volatile, but wondering if there is a better way
>>     that can help Thread-A to
>>     > run more effectively during the periods while Thread-B sleeps.
>>     >
>>     > Basically I'd like to have a "magic method" that can issue a
>>     memory sync when
>>     > Thread-B needs to see the fields.
>>     >
>>     > Thread-B might not need to see the latest status but it should
>>     not see partial
>>     > updates to the fields (even on 32 bit JVM).
>>     >
>>     > I was wondering removing the volatile, but not sure if it can
>>     cause issues.
>>     >
>>     > Example class:
>>     >
>>     > class SharedData {
>>     >
>>     >      private volatile field1;
>>     >      private volatile field2;
>>     >
>>     >      // setters / getters
>>     > }
>>     >
>>     > Thanks for any ideas...
>>     >
>>     > Thanks,
>>     > Trallan
>>     >
>>     > _______________________________________________
>>     > Concurrency-interest mailing list
>>     > Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130709/0f520362/attachment-0001.html>

From dl at cs.oswego.edu  Wed Jul 10 15:13:22 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 10 Jul 2013 15:13:22 -0400
Subject: [concurrency-interest] class SplittableRandom
Message-ID: <51DDB252.1070306@cs.oswego.edu>

[Note: I'm also posting this on the openjdk core-libs-dev list.]


We expect that using random numbers in parallel Stream computations
will be common. (We know it is common in parallel computing in
general.) But we had left support for it in an unsatisfactory state.
If you want to create a stream of random numbers to drive a parallel
computation, you'd choose among two options, neither of them providing
what you probably want: (1) Use a stream based on a single shared
java.util.Random object, in which case your program will encounter
stunning slowdowns when run with many cores; or (2) Use a stream based
on ThreadLocalRandom, which avoids contention, but gives you no
control over the use or properties of the per-thread singleton Random
object. While the ThreadLocalRandom option is great for many purposes,
you wouldn't want to use it in, say, a high-quality Monte Carlo
simulation.

Enter Guy Steele. Guy has been working on an algorithm that addresses
exactly the substantial range of uses not otherwise supported: It is,
in essence, the Random number generator analog of a Spliterator. Class
SplittableRandom supports method split() that creates a sub-generator
that when used in parallel with the original, maintains its
statistical properties.

When Brian Goetz and I heard that this was nearing completion, we
entered drop-everything mode to explore whether it could be added now
in time for JDK8. We conclude that it should. We've been helping with
JDK-ifying the basic algorithm, integrating java.util.Stream support,
etc, to enable addition as class java.util.SplittableRandom.

Just to be on the cautious side though, we are for the moment treating
this in the same way we treat jsr166<n> candidates for potential
OpenJDK integration. The initial version is available at
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/SplittableRandom.java?view=log
With API docs at:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/SplittableRandom.html

This post serves as a request for comment, with shorter than usual
turnaround (a couple of days) before considering a request to
integrate into OpenJDK 8. So, please take a look.

Here are answers to some likely questions:

Q: How much faster is it than java.util.Random?

A: In sequential usages, usually at least twice as fast for long and
double methods; usually only slightly faster for int methods.  In
parallel usages, SplittableRandom is almost arbitrarily faster. The
very first simple parallel Stream program I wrote (to generate and sum
nextLong()'s) ran 2900 times faster than the java.util.Random
equivalent on a 32-way machine.

Q: When can/should I use it instead of java.util.Random?

A: Whenever you are not sharing one across Threads. Instances of
SplittableRandom are not thread-safe. They are designed to be split,
not shared, across threads.  When class SplittableRandom applies (or
you can rework your program to make it apply), it is usually a better
choice.  Not only is it usually faster, it also has better statistical
independence and uniformity properties.

Q: When can/should I use it instead of java.util.concurrent.ThreadLocalRandom?

A: When you are doing structured fork/join computations, so you can
explicitly split one rather than relying on the per-thread singleton
instance.

Q: Why is this in java.util, not java.util.concurrent?

A: Because, like java.util.Spliterator, SplittableRandom is a tool for
arranging isolated parallel computations that don't entail any
concurrency control themselves.

Q: Why isn't SplittableRandom a subclass of Random?

A: Class Random requires thread-safety in its spec. It would be
nonsensical for SplittableRandom to comply.

Q: Why don't you at least come up with a new interface that
defines methods shared with java.util.Random?

A: We spent a couple of days exploring this. We think it could and
probably should be done, but not now. Method names and specs of
SplittableRandom are chosen to make it possible. But we encountered
enough short-term obstacles to conclude that this is an unwise move
for JDK8. Among the issues are that we'd need to adjust some specs and
possibly some code in java.util.Random, and that we are at a loss
about whether or how to generalize SplittableRandom's added Stream
methods. In the mean time, it would be more than acceptable for
SplittableRandom to be used primarily in new code (or new adaptions of
old code) that wouldn't need or want to be interoperable with code
using java.util.Random.

Q: Are we going to revisit with SplittableRandom all those memory
contention issues we saw with ThreadLocalRandom?

A: Most likely not. Most of the memory contention issues surrounding
ThreadLocalRandom arise because they are long-lived. SplittableRandoms
will tend to be short-lived. In any case, now that we have the tools
to cope (@Contended), we can evaluate and adjust if more detailed
empirical analysis warrants.


From nathan.reynolds at oracle.com  Wed Jul 10 16:04:04 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Jul 2013 13:04:04 -0700
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDB252.1070306@cs.oswego.edu>
References: <51DDB252.1070306@cs.oswego.edu>
Message-ID: <51DDBE34.8020108@oracle.com>

Intel's Ivy Bridge processor has a RDRAND instruction which provides 
cryptographically secure random data.  RDRAND has a latency of many 
cycles due to sending a message over the processor's internal bus.  If I 
understand correctly, RDRAND should be able to scale up with the cores. 
Would this be a suitable solution for workloads running on Ivy Bridge?  
Or am I missing some requirement?

-Nathan

On 7/10/2013 12:13 PM, Doug Lea wrote:
> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>
>
> We expect that using random numbers in parallel Stream computations
> will be common. (We know it is common in parallel computing in
> general.) But we had left support for it in an unsatisfactory state.
> If you want to create a stream of random numbers to drive a parallel
> computation, you'd choose among two options, neither of them providing
> what you probably want: (1) Use a stream based on a single shared
> java.util.Random object, in which case your program will encounter
> stunning slowdowns when run with many cores; or (2) Use a stream based
> on ThreadLocalRandom, which avoids contention, but gives you no
> control over the use or properties of the per-thread singleton Random
> object. While the ThreadLocalRandom option is great for many purposes,
> you wouldn't want to use it in, say, a high-quality Monte Carlo
> simulation.
>
> Enter Guy Steele. Guy has been working on an algorithm that addresses
> exactly the substantial range of uses not otherwise supported: It is,
> in essence, the Random number generator analog of a Spliterator. Class
> SplittableRandom supports method split() that creates a sub-generator
> that when used in parallel with the original, maintains its
> statistical properties.
>
> When Brian Goetz and I heard that this was nearing completion, we
> entered drop-everything mode to explore whether it could be added now
> in time for JDK8. We conclude that it should. We've been helping with
> JDK-ifying the basic algorithm, integrating java.util.Stream support,
> etc, to enable addition as class java.util.SplittableRandom.
>
> Just to be on the cautious side though, we are for the moment treating
> this in the same way we treat jsr166<n> candidates for potential
> OpenJDK integration. The initial version is available at
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/SplittableRandom.java?view=log 
>
> With API docs at:
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/SplittableRandom.html 
>
>
> This post serves as a request for comment, with shorter than usual
> turnaround (a couple of days) before considering a request to
> integrate into OpenJDK 8. So, please take a look.
>
> Here are answers to some likely questions:
>
> Q: How much faster is it than java.util.Random?
>
> A: In sequential usages, usually at least twice as fast for long and
> double methods; usually only slightly faster for int methods.  In
> parallel usages, SplittableRandom is almost arbitrarily faster. The
> very first simple parallel Stream program I wrote (to generate and sum
> nextLong()'s) ran 2900 times faster than the java.util.Random
> equivalent on a 32-way machine.
>
> Q: When can/should I use it instead of java.util.Random?
>
> A: Whenever you are not sharing one across Threads. Instances of
> SplittableRandom are not thread-safe. They are designed to be split,
> not shared, across threads.  When class SplittableRandom applies (or
> you can rework your program to make it apply), it is usually a better
> choice.  Not only is it usually faster, it also has better statistical
> independence and uniformity properties.
>
> Q: When can/should I use it instead of 
> java.util.concurrent.ThreadLocalRandom?
>
> A: When you are doing structured fork/join computations, so you can
> explicitly split one rather than relying on the per-thread singleton
> instance.
>
> Q: Why is this in java.util, not java.util.concurrent?
>
> A: Because, like java.util.Spliterator, SplittableRandom is a tool for
> arranging isolated parallel computations that don't entail any
> concurrency control themselves.
>
> Q: Why isn't SplittableRandom a subclass of Random?
>
> A: Class Random requires thread-safety in its spec. It would be
> nonsensical for SplittableRandom to comply.
>
> Q: Why don't you at least come up with a new interface that
> defines methods shared with java.util.Random?
>
> A: We spent a couple of days exploring this. We think it could and
> probably should be done, but not now. Method names and specs of
> SplittableRandom are chosen to make it possible. But we encountered
> enough short-term obstacles to conclude that this is an unwise move
> for JDK8. Among the issues are that we'd need to adjust some specs and
> possibly some code in java.util.Random, and that we are at a loss
> about whether or how to generalize SplittableRandom's added Stream
> methods. In the mean time, it would be more than acceptable for
> SplittableRandom to be used primarily in new code (or new adaptions of
> old code) that wouldn't need or want to be interoperable with code
> using java.util.Random.
>
> Q: Are we going to revisit with SplittableRandom all those memory
> contention issues we saw with ThreadLocalRandom?
>
> A: Most likely not. Most of the memory contention issues surrounding
> ThreadLocalRandom arise because they are long-lived. SplittableRandoms
> will tend to be short-lived. In any case, now that we have the tools
> to cope (@Contended), we can evaluate and adjust if more detailed
> empirical analysis warrants.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130710/38e62b5b/attachment.html>

From aaron.grunthal at infinite-source.de  Wed Jul 10 19:14:24 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Thu, 11 Jul 2013 01:14:24 +0200
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDB252.1070306@cs.oswego.edu>
References: <51DDB252.1070306@cs.oswego.edu>
Message-ID: <51DDEAD0.6030701@infinite-source.de>

Would this be of any use in a case like this?

List<List<Int>> a;

a.parallelStream().filter(...).map(e->e.get(random.nextInt(e.size()))).reduce(...)

i.e. a filter, random sample, reduce chain?

I don't see a way to basically use two (or N) streams (the source and 
the random numbers) and merge them in lockstep at one stage.

On 10.07.2013 21:13, Doug Lea wrote:
> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>
>
> We expect that using random numbers in parallel Stream computations
> will be common. (We know it is common in parallel computing in
> general.) But we had left support for it in an unsatisfactory state.
> If you want to create a stream of random numbers to drive a parallel
> computation, you'd choose among two options, neither of them providing
> what you probably want: (1) Use a stream based on a single shared
> java.util.Random object, in which case your program will encounter
> stunning slowdowns when run with many cores; or (2) Use a stream based
> on ThreadLocalRandom, which avoids contention, but gives you no
> control over the use or properties of the per-thread singleton Random
> object. While the ThreadLocalRandom option is great for many purposes,
> you wouldn't want to use it in, say, a high-quality Monte Carlo
> simulation.



From dl at cs.oswego.edu  Wed Jul 10 19:17:23 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 10 Jul 2013 19:17:23 -0400
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDBE34.8020108@oracle.com>
References: <51DDB252.1070306@cs.oswego.edu> <51DDBE34.8020108@oracle.com>
Message-ID: <51DDEB83.3060409@cs.oswego.edu>

On 07/10/13 16:04, Nathan Reynolds wrote:
> Intel's Ivy Bridge processor has a RDRAND instruction which provides
> cryptographically secure random data.  RDRAND has a latency of many cycles due
> to sending a message over the processor's internal bus.  If I understand
> correctly, RDRAND should be able to scale up with the cores. Would this be a
> suitable solution for workloads running on Ivy Bridge?  Or am I missing some
> requirement?
>

Umm, how about: write once run anywhere :-)

I do think that there are some reasonable VM-support level
paths for improving random number generation in general.
There's nothing in SplittableRandom specs saying that
it cannot use somehow use them internally. Having
learned our lessons many times, we are pretty careful
not to overconstrain ourselves out from making possible
future improvements.

-Doug


From nathan.reynolds at oracle.com  Wed Jul 10 20:17:10 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 10 Jul 2013 17:17:10 -0700
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDEB83.3060409@cs.oswego.edu>
References: <51DDB252.1070306@cs.oswego.edu> <51DDBE34.8020108@oracle.com>
	<51DDEB83.3060409@cs.oswego.edu>
Message-ID: <51DDF986.9080106@oracle.com>

 > Umm, how about: write once run anywhere :-)

When did that become a rule?  ;)

Let me ask a different way.  Let's say a program is running on Ivy 
Bridge.  Let's say the JVM intrinsified the random number generation 
method(s) to use RDRAND.  Would this solve this problem?  Would this 
make SplittableRandom and ThreadLocalRandom moot on Ivy Bridge?  Or 
would RDRAND be too costly in terms of latency or not fit some of the 
requirements?

I realize that all other platforms will need SplittableRandom and 
ThreadLocalRandom in order to perform well.  These are good 
improvements.  I am just wondering if RDRAND is useful in this situation.

-Nathan

On 7/10/2013 4:17 PM, Doug Lea wrote:
> On 07/10/13 16:04, Nathan Reynolds wrote:
>> Intel's Ivy Bridge processor has a RDRAND instruction which provides
>> cryptographically secure random data.  RDRAND has a latency of many 
>> cycles due
>> to sending a message over the processor's internal bus.  If I understand
>> correctly, RDRAND should be able to scale up with the cores. Would 
>> this be a
>> suitable solution for workloads running on Ivy Bridge?  Or am I 
>> missing some
>> requirement?
>>
>
> Umm, how about: write once run anywhere :-)
>
> I do think that there are some reasonable VM-support level
> paths for improving random number generation in general.
> There's nothing in SplittableRandom specs saying that
> it cannot use somehow use them internally. Having
> learned our lessons many times, we are pretty careful
> not to overconstrain ourselves out from making possible
> future improvements.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130710/f7de789d/attachment-0001.html>

From dl at cs.oswego.edu  Thu Jul 11 10:02:40 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 11 Jul 2013 10:02:40 -0400
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDF986.9080106@oracle.com>
References: <51DDB252.1070306@cs.oswego.edu> <51DDBE34.8020108@oracle.com>
	<51DDEB83.3060409@cs.oswego.edu> <51DDF986.9080106@oracle.com>
Message-ID: <51DEBB00.60104@cs.oswego.edu>

On 07/10/13 20:17, Nathan Reynolds wrote:

> Let me ask a different way.  Let's say a program is running on Ivy Bridge.
> Let's say the JVM intrinsified the random number generation method(s) to use
> RDRAND.  Would this solve this problem?

Still basically the same answer:
At least these days, we write specs that say
as little as possible about implementations,
and focus only on required properties.
(This allows us to swap in different algorithms etc, which
we have at some point done with many j.u.c classes.)
So if a hardware generator has the required properties,
then there is no reason not to use it within
SplittableRandom or ThreadLocalRandom.

I think you'd need to be a bit more conservative
about java.util.Random though, since there are
some known dependencies on the particular algorithms.
Which is not too surprising because the class docs
spell out the implementations.

-Doug



From peter.levart at gmail.com  Thu Jul 11 10:29:02 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 11 Jul 2013 16:29:02 +0200
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDEAD0.6030701@infinite-source.de>
References: <51DDB252.1070306@cs.oswego.edu>
	<51DDEAD0.6030701@infinite-source.de>
Message-ID: <51DEC12E.2020902@gmail.com>

On 07/11/2013 01:14 AM, Aaron Grunthal wrote:
> Would this be of any use in a case like this?
>
> List<List<Int>> a;
>
> a.parallelStream().filter(...).map(e->e.get(random.nextInt(e.size()))).reduce(...) 
>
>
> i.e. a filter, random sample, reduce chain?
>
> I don't see a way to basically use two (or N) streams (the source and 
> the random numbers) and merge them in lockstep at one stage.
>

In the absence of Stream.join(), we would need something like the 
following in the Stream API:

Stream<T> {

         <R, S> Stream<R> splitMap(S seed,
                                   UnaryOperator<S> splitter,
                                   BiFunction<? super T, ? super S, ? 
extends R> mapper);


Your example would then read:

List<List<Int>> a = ...;

a.parallelStream()
.filter(...)
.splitMap(
new SplittableRandom(),
         sr -> sr.split(),
(list, sr) -> list.get(sr.nextInt(list.size()))
)
.reduce(...)


But I don't know if such API is usable in any other scenarios though.

Regards, Peter


> On 10.07.2013 21:13, Doug Lea wrote:
>> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>>
>>
>> We expect that using random numbers in parallel Stream computations
>> will be common. (We know it is common in parallel computing in
>> general.) But we had left support for it in an unsatisfactory state.
>> If you want to create a stream of random numbers to drive a parallel
>> computation, you'd choose among two options, neither of them providing
>> what you probably want: (1) Use a stream based on a single shared
>> java.util.Random object, in which case your program will encounter
>> stunning slowdowns when run with many cores; or (2) Use a stream based
>> on ThreadLocalRandom, which avoids contention, but gives you no
>> control over the use or properties of the per-thread singleton Random
>> object. While the ThreadLocalRandom option is great for many purposes,
>> you wouldn't want to use it in, say, a high-quality Monte Carlo
>> simulation.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gustav.r.akesson at gmail.com  Thu Jul 11 15:54:43 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Thu, 11 Jul 2013 21:54:43 +0200
Subject: [concurrency-interest]  StripedBlockingQueue
Message-ID: <CAKEw5+7qD9icpN4hpvNVnRV+-qeGULb_1cB2fKfZMCmhYRMAUg@mail.gmail.com>

Hi fellow concurrency-addicts,

I'm calling to you to aid my implementation of the StripedBlockingQueue.
Long story short: in the application I'm currently developing we have the
need to give different tasks in a threadpool different priorities, but
without the risk of task starvation. Let's say we have two clients, then we
want that during load 70% are spent on client 1 and 30% on client 2. And if
only requests from client 1 is coming in then 100% should be spent there.

We've tried the PriorityBlockingQueue but during load that actually starved
out client 1...

For more detailed description, please read the JavaDoc.

Whatever code view you prefer, but here it is - StripedBlockingQueue.java
https://github.com/gakesson/ConcurrencyUtils/tree/1819f74285f0545322cfb82daaf70ce974ec16a5

*Doug Lea and others,* I would really, really appreciate if you could have
a look at the StripedBlockingQueue.java. The reason I'm calling for your
aid is that this implementation is intended to sit at the heart of a
performance-critical server system with high uptime requirements and with
more than one billion users world-wide. So I better get it right unless I
want to end up on the street. :-)

Thanks in advance, and don't hesitate to ask me if something would ease
your review/input.


Best Regards,

Gustav ?kesson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130711/317d8f29/attachment.html>

From aaron.grunthal at infinite-source.de  Thu Jul 11 20:56:29 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Fri, 12 Jul 2013 02:56:29 +0200
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DEC12E.2020902@gmail.com>
References: <51DDB252.1070306@cs.oswego.edu>
	<51DDEAD0.6030701@infinite-source.de> <51DEC12E.2020902@gmail.com>
Message-ID: <51DF543D.9080005@infinite-source.de>

On 11.07.2013 16:29, Peter Levart wrote:
> On 07/11/2013 01:14 AM, Aaron Grunthal wrote:
>> Would this be of any use in a case like this?
>>
>> List<List<Int>> a;
>>
>> a.parallelStream().filter(...).map(e->e.get(random.nextInt(e.size()))).reduce(...)
>>
>>
>> i.e. a filter, random sample, reduce chain?
>>
>> I don't see a way to basically use two (or N) streams (the source and
>> the random numbers) and merge them in lockstep at one stage.
>>
>
> In the absence of Stream.join(), we would need something like the
> following in the Stream API:

Thinking about about .join() and parallel processing: this would only 
seem possible [assuming it's order-preserving] with an trySplit(int) 
aiming for specific sizes that match the splitting of the other stream. 
I.e. it would require sources that allow arbitrary seeks.

That made me think about seekable PRNGs, such as salsa20. Wouldn't it be 
better to have a seekable random and perform splits like one would on an 
ordered, finite-sized, random-access list? An infinite version without 
seek offset wraparound would still be obtainable from that with a 
per-split reseed.

As I understand it SplittableRandom - when run as parallel stream - 
doesn't offer the same sequence as the sequential version. And even the 
splits would depend on the thread pool size, so it'll be difficult to 
re-run some stream with the same pseudorandom sequence, which sometimes 
is necessary, e.g. to debug things.





>
> Stream<T> {
>
>          <R, S> Stream<R> splitMap(S seed,
>                                    UnaryOperator<S> splitter,
>                                    BiFunction<? super T, ? super S, ?
> extends R> mapper);
>
>
> Your example would then read:
>
> List<List<Int>> a = ...;
>
> a.parallelStream()
> .filter(...)
> .splitMap(
> new SplittableRandom(),
>          sr -> sr.split(),
> (list, sr) -> list.get(sr.nextInt(list.size()))
> )
> .reduce(...)
>
>
> But I don't know if such API is usable in any other scenarios though.
>
> Regards, Peter
>
>
>> On 10.07.2013 21:13, Doug Lea wrote:
>>> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>>>
>>>
>>> We expect that using random numbers in parallel Stream computations
>>> will be common. (We know it is common in parallel computing in
>>> general.) But we had left support for it in an unsatisfactory state.
>>> If you want to create a stream of random numbers to drive a parallel
>>> computation, you'd choose among two options, neither of them providing
>>> what you probably want: (1) Use a stream based on a single shared
>>> java.util.Random object, in which case your program will encounter
>>> stunning slowdowns when run with many cores; or (2) Use a stream based
>>> on ThreadLocalRandom, which avoids contention, but gives you no
>>> control over the use or properties of the per-thread singleton Random
>>> object. While the ThreadLocalRandom option is great for many purposes,
>>> you wouldn't want to use it in, say, a high-quality Monte Carlo
>>> simulation.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From peter.levart at gmail.com  Fri Jul 12 06:59:32 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 12 Jul 2013 12:59:32 +0200
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DF543D.9080005@infinite-source.de>
References: <51DDB252.1070306@cs.oswego.edu>
	<51DDEAD0.6030701@infinite-source.de> <51DEC12E.2020902@gmail.com>
	<51DF543D.9080005@infinite-source.de>
Message-ID: <51DFE194.80701@gmail.com>

On 07/12/2013 02:56 AM, Aaron Grunthal wrote:
> On 11.07.2013 16:29, Peter Levart wrote:
>> On 07/11/2013 01:14 AM, Aaron Grunthal wrote:
>>> Would this be of any use in a case like this?
>>>
>>> List<List<Int>> a;
>>>
>>> a.parallelStream().filter(...).map(e->e.get(random.nextInt(e.size()))).reduce(...) 
>>>
>>>
>>>
>>> i.e. a filter, random sample, reduce chain?
>>>
>>> I don't see a way to basically use two (or N) streams (the source and
>>> the random numbers) and merge them in lockstep at one stage.
>>>
>>
>> In the absence of Stream.join(), we would need something like the
>> following in the Stream API:
>
> Thinking about about .join() and parallel processing: this would only 
> seem possible [assuming it's order-preserving] with an trySplit(int) 
> aiming for specific sizes that match the splitting of the other 
> stream. I.e. it would require sources that allow arbitrary seeks.
>
> That made me think about seekable PRNGs, such as salsa20. Wouldn't it 
> be better to have a seekable random and perform splits like one would 
> on an ordered, finite-sized, random-access list? An infinite version 
> without seek offset wraparound would still be obtainable from that 
> with a per-split reseed.

The following library:

http://www.iro.umontreal.ca/~simardr/ssj/doc/html/index.html?umontreal/iro/lecuyer/rng/RandomStream.html

Takes a different approach. The API allows for the full period of the 
PRNG to be split into adjacent streams and each such stream to be 
further split into sub-streams (the later can be rewind/skipped). The 
lengths and numbers of streams are usually very large so there's no 
danger of wraparound or overlap. Each separate generator instance 
generates numbers from a separate stream. But there is some central 
point where contention can occur when creating individual instances. The 
SplittableRandom only has contention for constructing root instances, 
it's split() on the other hand has no contention.

>
> As I understand it SplittableRandom - when run as parallel stream - 
> doesn't offer the same sequence as the sequential version. And even 
> the splits would depend on the thread pool size, so it'll be difficult 
> to re-run some stream with the same pseudorandom sequence, which 
> sometimes is necessary, e.g. to debug things.

This could be "fixed" by providing another constructor that could set 
the initial "gamma" value for the root instance in addition to seed. The 
beauty of fork-join is that although individual tasks can run at 
different speeds/times each time, the decomposition is typically data 
driven and stable. And if SplittableRandom.split()s follow Task.fork()s, 
so are the random sequences consumed in each task.

Regards, Peter

>
>
>
>
>
>>
>> Stream<T> {
>>
>>          <R, S> Stream<R> splitMap(S seed,
>>                                    UnaryOperator<S> splitter,
>>                                    BiFunction<? super T, ? super S, ?
>> extends R> mapper);
>>
>>
>> Your example would then read:
>>
>> List<List<Int>> a = ...;
>>
>> a.parallelStream()
>> .filter(...)
>> .splitMap(
>> new SplittableRandom(),
>>          sr -> sr.split(),
>> (list, sr) -> list.get(sr.nextInt(list.size()))
>> )
>> .reduce(...)
>>
>>
>> But I don't know if such API is usable in any other scenarios though.
>>
>> Regards, Peter
>>
>>
>>> On 10.07.2013 21:13, Doug Lea wrote:
>>>> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>>>>
>>>>
>>>> We expect that using random numbers in parallel Stream computations
>>>> will be common. (We know it is common in parallel computing in
>>>> general.) But we had left support for it in an unsatisfactory state.
>>>> If you want to create a stream of random numbers to drive a parallel
>>>> computation, you'd choose among two options, neither of them providing
>>>> what you probably want: (1) Use a stream based on a single shared
>>>> java.util.Random object, in which case your program will encounter
>>>> stunning slowdowns when run with many cores; or (2) Use a stream based
>>>> on ThreadLocalRandom, which avoids contention, but gives you no
>>>> control over the use or properties of the per-thread singleton Random
>>>> object. While the ThreadLocalRandom option is great for many purposes,
>>>> you wouldn't want to use it in, say, a high-quality Monte Carlo
>>>> simulation.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Fri Jul 12 08:00:36 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 12 Jul 2013 13:00:36 +0100
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DEC12E.2020902@gmail.com>
References: <51DDB252.1070306@cs.oswego.edu>
	<51DDEAD0.6030701@infinite-source.de> <51DEC12E.2020902@gmail.com>
Message-ID: <51DFEFE4.3010308@oracle.com>

You've just re-invented applicative functor.

Put simply, given a lambda and a list of values produce a list of 
lambdas. Then given a list of lambdas and a list of values, pair up the 
elements in matching positions, and apply the lambdas to the matching 
elements to produce a list of values. If those values are lambdas, you 
can "zip" another list of values, etc. This way you can apply functions 
with multiple arguments to lists of arguments.

Think of scalar vector product, where you first need to apply 
multiplication (function with two arguments) to two vectors (two lists 
of arguments for the function) to produce products of corresponding 
coordinates.

The uses are many.

Alex

On 11/07/2013 15:29, Peter Levart wrote:
> On 07/11/2013 01:14 AM, Aaron Grunthal wrote:
>> Would this be of any use in a case like this?
>>
>> List<List<Int>> a;
>>
>> a.parallelStream().filter(...).map(e->e.get(random.nextInt(e.size()))).reduce(...) 
>>
>>
>> i.e. a filter, random sample, reduce chain?
>>
>> I don't see a way to basically use two (or N) streams (the source and 
>> the random numbers) and merge them in lockstep at one stage.
>>
>
> In the absence of Stream.join(), we would need something like the 
> following in the Stream API:
>
> Stream<T> {
>
>         <R, S> Stream<R> splitMap(S seed,
>                                   UnaryOperator<S> splitter,
>                                   BiFunction<? super T, ? super S, ? 
> extends R> mapper);
>
>
> Your example would then read:
>
> List<List<Int>> a = ...;
>
> a.parallelStream()
> .filter(...)
> .splitMap(
> new SplittableRandom(),
>         sr -> sr.split(),
> (list, sr) -> list.get(sr.nextInt(list.size()))
> )
> .reduce(...)
>
>
> But I don't know if such API is usable in any other scenarios though.
>
> Regards, Peter
>
>
>> On 10.07.2013 21:13, Doug Lea wrote:
>>> [Note: I'm also posting this on the openjdk core-libs-dev list.]
>>>
>>>
>>> We expect that using random numbers in parallel Stream computations
>>> will be common. (We know it is common in parallel computing in
>>> general.) But we had left support for it in an unsatisfactory state.
>>> If you want to create a stream of random numbers to drive a parallel
>>> computation, you'd choose among two options, neither of them providing
>>> what you probably want: (1) Use a stream based on a single shared
>>> java.util.Random object, in which case your program will encounter
>>> stunning slowdowns when run with many cores; or (2) Use a stream based
>>> on ThreadLocalRandom, which avoids contention, but gives you no
>>> control over the use or properties of the per-thread singleton Random
>>> object. While the ThreadLocalRandom option is great for many purposes,
>>> you wouldn't want to use it in, say, a high-quality Monte Carlo
>>> simulation.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Fri Jul 12 13:09:00 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 12 Jul 2013 13:09:00 -0400
Subject: [concurrency-interest] class SplittableRandom
In-Reply-To: <51DDB252.1070306@cs.oswego.edu>
References: <51DDB252.1070306@cs.oswego.edu>
Message-ID: <51E0382C.10507@cs.oswego.edu>

[Also posted on core-libs-dev.]

Thanks to all those sending quick feedback, both on and off lists!
We're increasingly confident about the value of adding
SplittableRandom, so will proceed with integration request. I'm sure
that we'll introduce incremental improvements during OpenJDK8 bake-in
phase, and apply some of the lessons learned to other code. But we
wanted to ensure that this is at least considered for inclusion along
with last of the other jsr166-related updates (that are all currently
in-process).

Paul Sandoz will follow up on core-libs-dev with the CR details, on
his way out to a well-deserved vacation. So I may request help from
others for any Oracle process issues that might arise.  The CR is far
from the end of this process, so I also plan to post follow-ups as it
progresses.

-Doug


From ryan.lubke at oracle.com  Fri Jul 12 13:26:21 2013
From: ryan.lubke at oracle.com (Ryan Lubke)
Date: Fri, 12 Jul 2013 10:26:21 -0700
Subject: [concurrency-interest] 6663476 : FutureTask.get() may return null
 if set() is not called from run()
Message-ID: <51E03C3D.4050907@oracle.com>

Hey Folks,

I believe I've run into this in my project which uses a custom Future 
that exposes set() and setException().

I have a couple of questions on the evaluation.

1.  I've looked at the code in FutureTask (7u25) and I'm not seeing what 
innerRun() offers in ensuring the
      proper value is published over invoking set()/setException() directly.

2. The last entry in the issue states:

"The submitter is correct that there is a serious
      concurrency bug here.  In this fragment from
      FutureTask.java:innerSet,

                 if (compareAndSetState(s, RAN)) {
                     result = v;
                     releaseShared(0);
                     done();
                     return;
                 }

      if another thread calls get after the compareAndSet succeeds,
      but before result is assigned, get will return null instead of the 
correct value."

      What I'm unclear on is how any threads parked at the get() call 
would run
      after the state change?  Seems to me, they are only notified when 
releaseShared()
      is invoked which would mean the proper result would be visible, yes?

So for 1 and 2, what am I missing here?

Thanks for helping me fill in the blanks.

-rl


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130712/2f4e97c5/attachment.html>

From martinrb at google.com  Fri Jul 12 18:12:48 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 12 Jul 2013 15:12:48 -0700
Subject: [concurrency-interest] 6663476 : FutureTask.get() may return
 null if set() is not called from run()
In-Reply-To: <51E03C3D.4050907@oracle.com>
References: <51E03C3D.4050907@oracle.com>
Message-ID: <CA+kOe0-0Tc8tNQjtWmSefH3BpSJa7TUEfw4368sv_3FwMZEHQA@mail.gmail.com>

Latest FutureTask sources are very different, and will be in jdk8.

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/FutureTask.java?view=markup


On Fri, Jul 12, 2013 at 10:26 AM, Ryan Lubke <ryan.lubke at oracle.com> wrote:

>  Hey Folks,
>
> I believe I've run into this in my project which uses a custom Future that
> exposes set() and setException().
>
> I have a couple of questions on the evaluation.
>
> 1.  I've looked at the code in FutureTask (7u25) and I'm not seeing what
> innerRun() offers in ensuring the
>      proper value is published over invoking set()/setException() directly.
>
> 2. The last entry in the issue states:
>
>      "The submitter is correct that there is a serious
>      concurrency bug here.  In this fragment from
>      FutureTask.java:innerSet,
>
>                 if (compareAndSetState(s, RAN)) {
>                     result = v;
>                     releaseShared(0);
>                     done();
>                     return;
>                 }
>
>      if another thread calls get after the compareAndSet succeeds,
>      but before result is assigned, get will return null instead of the
> correct value."
>
>      What I'm unclear on is how any threads parked at the get() call would
> run
>      after the state change?  Seems to me, they are only notified when
> releaseShared()
>      is invoked which would mean the proper result would be visible, yes?
>
> So for 1 and 2, what am I missing here?
>
> Thanks for helping me fill in the blanks.
>
> -rl
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130712/48834cac/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jul 12 18:31:45 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 13 Jul 2013 02:31:45 +0400
Subject: [concurrency-interest] 6663476 : FutureTask.get() may return
 null if set() is not called from run()
In-Reply-To: <51E03C3D.4050907@oracle.com>
References: <51E03C3D.4050907@oracle.com>
Message-ID: <51E083D1.9040607@oracle.com>

On 07/12/2013 09:26 PM, Ryan Lubke wrote:
> 1.  I've looked at the code in FutureTask (7u25) and I'm not seeing what
> innerRun() offers in ensuring the
>      proper value is published over invoking set()/setException() directly.

Yes, 7u25 still misses the fix. We had backported the fix for this in
upcoming JDK 7u40.

There is also the jcstress test case for exactly this thing:
 http://hg.openjdk.java.net/code-tools/jcstress/file/tip/tests-custom/src/main/java/org/openjdk/jcstress/tests/future/FutureTaskSetTest.java

> 2. The last entry in the issue states:
> 
>      "The submitter is correct that there is a serious
>      concurrency bug here.  In this fragment from
>      FutureTask.java:innerSet,
> 
>                 if (compareAndSetState(s, RAN)) {
>                     result = v;
>                     releaseShared(0);
>                     done();
>                     return;
>                 }
> 
>      if another thread calls get after the compareAndSet succeeds,
>      but before result is assigned, get will return null instead of the
> correct value."
> 
> What I'm unclear on is how any threads parked at the get() call would
> run after the state change?  Seems to me, they are only notified
> when releaseShared() is invoked which would mean the proper result
> would be visible, yes?

The key here is "another" thread, meaning the stray thread not yet
blocked on get(), but just arriving at get() to check the state. It
might very well happen it will detect the state = RAN, which means it
can read the result, but result is not yet set!

-Aleksey.


From gustav.r.akesson at gmail.com  Sun Jul 14 16:04:39 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Sun, 14 Jul 2013 22:04:39 +0200
Subject: [concurrency-interest]  Semaphore::reducePermits(n)
Message-ID: <CAKEw5+6Od5gBbXFrqxZ4tzjaVe=mtvPxcd1Wu0rmTyJZRVVbHg@mail.gmail.com>

Hi guys,

I have use-case where I have X resources which I want to protect with a
Y-initialised Semaphore. X is always greater than Y, and I need to have the
behavior of that certain high-prio threads should always be allowed to get
a resource regardless of whether the Semaphore is 0 or not. In order to
simply the returning of permit when no more work on the resource is needed
I had the below in mind. Basically, the behavior is that the high-prio
thread tries to acquire the permit, and if unsuccessful it decrements to
below zero in order to throttle others (non-prio) threads to grab a permit.

Is this considered to be safe? Looking at the Semaphore implementation I
can't see any obvious issues, but am I missing something?


....
boolean acquired = false;

if (is high-prio thread)
{
    acquired = adaptiveSemaphore.tryAcquire();

    if (!acquired)
    {
        adaptiveSemaphore.decrementPermit();
        acquired = true;
    }
}
else
{
     adaptiveSemaphore.acquire();
     acquired = true;
}
....


    private static class AdaptiveSemaphore extends Semaphore
    {

        AdaptiveSemaphore(int permits, boolean fair)
        {
            super(permits, fair);
        }

        /**
         * Decrements the permits by one(1). This might have the affect
that the current permits go below zero.
         *
         */
        void decrementPermit()
        {
            reducePermits(1);
        }
    }



Best Regards,
Gustav ?kesson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130714/8481f70e/attachment.html>

From ariel at weisberg.ws  Mon Jul 15 16:49:13 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Mon, 15 Jul 2013 16:49:13 -0400
Subject: [concurrency-interest] Unexpected memory usage in
	NonBlockingHashHashMap
Message-ID: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>

Hi all,

I was trying to make use of Cliff Click's NonBlockingHashSet and
NonBlockingHashMap
(1.1.2) and found unexpected heap memory usage. When I did a heap dump
of live objects I found that a lot of resources were being used even
though size for the data structures reports 0 or some reasonable number
of entries.

Attached is a quick demonstration app that creates a set of longs and
churns through through the set adding and removing entries, occasionally
replacing most of the set. It seems like removing and replacing most of
the entries triggers the behavior that leaks the most memory.

I am using
java version "1.6.0_27"
OpenJDK Runtime Environment (IcedTea6 1.12.5)
(6b27-1.12.5-0ubuntu0.12.04.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

It also reproduced with Sun JDK 1.6.0_39

You can swap in ConcurrentHashMap and it works as expected and produces
modest 9 megabyte heap dumps.

Is this an expected behavior for this use case? Is there some caveat I
should know?

Thanks,
Ariel

From ariel at weisberg.ws  Mon Jul 15 16:49:54 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Mon, 15 Jul 2013 16:49:54 -0400
Subject: [concurrency-interest] Unexpected memory usage in
	NonBlockingHashHashMap
In-Reply-To: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>
References: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>
Message-ID: <1373921394.5212.140661255997133.6BE40980@webmail.messagingengine.com>

Hi,

Apologies, the example app follows.

Ariel

On Mon, Jul 15, 2013, at 04:49 PM, Ariel Weisberg wrote:
> Hi all,
> 
> I was trying to make use of Cliff Click's NonBlockingHashSet and
> NonBlockingHashMap
> (1.1.2) and found unexpected heap memory usage. When I did a heap dump
> of live objects I found that a lot of resources were being used even
> though size for the data structures reports 0 or some reasonable number
> of entries.
> 
> Attached is a quick demonstration app that creates a set of longs and
> churns through through the set adding and removing entries, occasionally
> replacing most of the set. It seems like removing and replacing most of
> the entries triggers the behavior that leaks the most memory.
> 
> I am using
> java version "1.6.0_27"
> OpenJDK Runtime Environment (IcedTea6 1.12.5)
> (6b27-1.12.5-0ubuntu0.12.04.1)
> OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
> 
> It also reproduced with Sun JDK 1.6.0_39
> 
> You can swap in ConcurrentHashMap and it works as expected and produces
> modest 9 megabyte heap dumps.
> 
> Is this an expected behavior for this use case? Is there some caveat I
> should know?
> 
> Thanks,
> Ariel
-------------- next part --------------
A non-text attachment was scrubbed...
Name: HammerSet.java
Type: text/x-java
Size: 1907 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130715/5a78cb4f/attachment.bin>

From aleksey.shipilev at oracle.com  Tue Jul 16 01:43:51 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 16 Jul 2013 09:43:51 +0400
Subject: [concurrency-interest] Unexpected memory usage in
	NonBlockingHashHashMap
In-Reply-To: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>
References: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>
Message-ID: <51E4DD97.7030808@oracle.com>

Hi Ariel,

On 07/16/2013 12:49 AM, Ariel Weisberg wrote:
> I was trying to make use of Cliff Click's NonBlockingHashSet and 
> NonBlockingHashMap (1.1.2) and found unexpected heap memory usage.
> When I did a heap dump of live objects I found that a lot of
> resources were being used even though size for the data structures
> reports 0 or some reasonable number of entries.

Can you be a little more specific? Sample heap dump report would be
useful here without pushing people to actually set up the experiment to
understand the question :)

> Is this an expected behavior for this use case? Is there some caveat
> I should know?

First thing that comes to mind is that heap dumps are notoriously bad in
capturing the actual instance sizes. Current HPROF format merely lists
the fields, thinking the sum over the field sizes is the instance size,
which is not, due to alignment, padding, etc. Hence, even if your tool
shows the instance size of zero, it does not mean the instance has zero
size.

Second is, in hardcore concurrency you sometimes need to overprovision
stuff to get saner, more error-prone, and also more-performant code. It
would help if you do a little additional step and cross-reference what
you see with the NBHHM sources.

-Aleksey.

From mr.chrisvest at gmail.com  Tue Jul 16 03:44:05 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Tue, 16 Jul 2013 09:44:05 +0200
Subject: [concurrency-interest] Unexpected memory usage in
	NonBlockingHashHashMap
In-Reply-To: <51E4DD97.7030808@oracle.com>
References: <1373921353.5093.140661255996697.21E5240D@webmail.messagingengine.com>
	<51E4DD97.7030808@oracle.com>
Message-ID: <CAHXi_0fys=0tuOV2T2iBsZA6hfCOuJsHhH9f=L5LaHYZbxs58w@mail.gmail.com>

If I remember correctly, NBHM is a big array where the elements are
alternating between keys and associated values.
Collisions is handled by reprobing ahead for available slots, and deletion
works by writing tombstones.
If and operation does too many reprobes, then a resize is triggered which
will create a new array and cooperatively copy stuff over.
Depending on heuristics, the new array will either have the same size as
the old one, or double or quadruple the size of the old array.
The tombstones are cleaned up by the resize. During the copying, both
arrays are kept around, obviously. Resize will never shrink the array,
however.

I'm thinking that your program (which I admittedly have not run) might
tickle it in a way, that cause it to balloon up to big array sizes.
When you then churn the keys with adds and removals, you will cause further
resizes, where the big array is kept around twice.

This is just a hypothesis, however, based on what little I know. You can
check it by seeing if you have one or two large arrays in your memory dumps.

Cheers,
Chris


On 16 July 2013 07:43, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:

> Hi Ariel,
>
> On 07/16/2013 12:49 AM, Ariel Weisberg wrote:
> > I was trying to make use of Cliff Click's NonBlockingHashSet and
> > NonBlockingHashMap (1.1.2) and found unexpected heap memory usage.
> > When I did a heap dump of live objects I found that a lot of
> > resources were being used even though size for the data structures
> > reports 0 or some reasonable number of entries.
>
> Can you be a little more specific? Sample heap dump report would be
> useful here without pushing people to actually set up the experiment to
> understand the question :)
>
> > Is this an expected behavior for this use case? Is there some caveat
> > I should know?
>
> First thing that comes to mind is that heap dumps are notoriously bad in
> capturing the actual instance sizes. Current HPROF format merely lists
> the fields, thinking the sum over the field sizes is the instance size,
> which is not, due to alignment, padding, etc. Hence, even if your tool
> shows the instance size of zero, it does not mean the instance has zero
> size.
>
> Second is, in hardcore concurrency you sometimes need to overprovision
> stuff to get saner, more error-prone, and also more-performant code. It
> would help if you do a little additional step and cross-reference what
> you see with the NBHHM sources.
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/e24904df/attachment.html>

From galder at redhat.com  Tue Jul 16 05:14:07 2013
From: galder at redhat.com (=?iso-8859-1?Q?Galder_Zamarre=F1o?=)
Date: Tue, 16 Jul 2013 11:14:07 +0200
Subject: [concurrency-interest] A variant of the jsr166e.CHMv8.TreeBin bug
	is back?
Message-ID: <561232CE-2488-4E15-A87A-28642F23E9D4@redhat.com>

Hi,

A few weeks back, Alex Snaps sent an email wrt to a possible treebin bug in jsr166e.CHMv8.TreeBin [1].

With revision 1.100 and applying the patches in [2], the test passed.

However, running the same test that Alex submitted with revision 1.110 of jsr166e.CHMv8 [3] (no patches) fails with:

java.lang.AssertionError: EvilComparableKey ( "383" )

There are multiple keys that are lost, not just this one. I'm wondering if this is already a known issue...

Cheers,

[1] http://cs.oswego.edu/pipermail/concurrency-interest/2013-June/011393.html 
[2] http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ConcurrentHashMap.java?r1=1.216&r2=1.217
--
Galder Zamarre?o
galder at redhat.com
twitter.com/galderz

Project Lead, Escalante
http://escalante.io

Engineer, Infinispan
http://infinispan.org



From dl at cs.oswego.edu  Tue Jul 16 07:21:02 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 16 Jul 2013 07:21:02 -0400
Subject: [concurrency-interest] A variant of the jsr166e.CHMv8.TreeBin
 bug is back?
In-Reply-To: <561232CE-2488-4E15-A87A-28642F23E9D4@redhat.com>
References: <561232CE-2488-4E15-A87A-28642F23E9D4@redhat.com>
Message-ID: <51E52C9E.6060008@cs.oswego.edu>

On 07/16/13 05:14, Galder Zamarre?o wrote:
> Hi,
>
> A few weeks back, Alex Snaps sent an email wrt to a possible treebin bug in
> jsr166e.CHMv8.TreeBin [1].
>
> With revision 1.100 and applying the patches in [2], the test passed.
>
> However, running the same test that Alex submitted with revision 1.110 of
> jsr166e.CHMv8 [3] (no patches) fails
>
> I'm wondering if this is already a known issue...
>

Thanks! It wasn't. Almost surely a consequence of having
too many versions of CHM in flux. I will track it down.

-Doug




From mjpt777 at gmail.com  Tue Jul 16 12:13:54 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Tue, 16 Jul 2013 17:13:54 +0100
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
	Issue 29
In-Reply-To: <mailman.1.1373990400.13650.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1373990400.13650.concurrency-interest@cs.oswego.edu>
Message-ID: <CAChYfd_Lak6ULUNAD41Y9oJvdTzTR0dhGDu3hGsDjaUr=gzBFw@mail.gmail.com>

It is best to get a heap dump to determine what is using the memory.

However a quick guess would be that NonBlockingHashMap uses uses
open-addressing rather than bucket and chain design so you'll have some
large arrays for the keys and values.  If you put a lot of items in the set
then remove them the backing arrays will still be large having had to grow
to accommodate the entries.

Regards,
Martin...

On 16 July 2013 17:00, <concurrency-interest-request at cs.oswego.edu> wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu
> Date: Mon, 15 Jul 2013 16:49:13 -0400
> From: Ariel Weisberg <ariel at weisberg.ws>
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Unexpected memory usage in
>         NonBlockingHashHashMap
> Message-ID:
>         <
> 1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>
> Content-Type: text/plain
>
> Hi all,
>
> I was trying to make use of Cliff Click's NonBlockingHashSet and
> NonBlockingHashMap
> (1.1.2) and found unexpected heap memory usage. When I did a heap dump
> of live objects I found that a lot of resources were being used even
> though size for the data structures reports 0 or some reasonable number
> of entries.
>
> Attached is a quick demonstration app that creates a set of longs and
> churns through through the set adding and removing entries, occasionally
> replacing most of the set. It seems like removing and replacing most of
> the entries triggers the behavior that leaks the most memory.
>
> I am using
> java version "1.6.0_27"
> OpenJDK Runtime Environment (IcedTea6 1.12.5)
> (6b27-1.12.5-0ubuntu0.12.04.1)
> OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
>
> It also reproduced with Sun JDK 1.6.0_39
>
> You can swap in ConcurrentHashMap and it works as expected and produces
> modest 9 megabyte heap dumps.
>
> Is this an expected behavior for this use case? Is there some caveat I
> should know?
>
> Thanks,
> Ariel
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/b4a8daec/attachment.html>

From alexandr.berdnik at gmail.com  Tue Jul 16 13:31:30 2013
From: alexandr.berdnik at gmail.com (Alexandr Berdnik)
Date: Tue, 16 Jul 2013 18:31:30 +0100
Subject: [concurrency-interest] volatile read reorderings
Message-ID: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>

Hi all,
just a quick question here.

volatile int a = 0;
volatile int b = 0;

// thread 1
a = 1;
b = 1;

// thread 2
int aa = a;
int bb = b;

The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
reordered? I have not found any guarantees for that in JMM.

Thanks,
Alex.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/5c42fb9d/attachment.html>

From dl at cs.oswego.edu  Tue Jul 16 13:41:05 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 16 Jul 2013 13:41:05 -0400
Subject: [concurrency-interest] Outage Wednesday July 17
Message-ID: <51E585B1.4070509@cs.oswego.edu>


The outage I mentioned a few weeks ago got postponed a few times,
but this this time they say they really mean it. So this list,
CVS, etc, will all be unavailable for hopefully less than a day
as our department moves equipment across the street to a new
building.

-Doug

From vitalyd at gmail.com  Tue Jul 16 13:42:42 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 16 Jul 2013 13:42:42 -0400
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
Message-ID: <CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>

No, that's not a possible outcome.  Volatile reads can't reorder with each
other and volatile stores cannot reorder with each other.  I think your
question was mostly about volatile writes though - if you read bb=1 then it
must be that b=1 executed and thus a=1 as well.

Sent from my phone
On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" <alexandr.berdnik at gmail.com>
wrote:

> Hi all,
> just a quick question here.
>
> volatile int a = 0;
> volatile int b = 0;
>
> // thread 1
> a = 1;
> b = 1;
>
> // thread 2
> int aa = a;
> int bb = b;
>
> The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
> reordered? I have not found any guarantees for that in JMM.
>
> Thanks,
> Alex.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/14ef47fa/attachment.html>

From alexandr.berdnik at gmail.com  Tue Jul 16 13:46:13 2013
From: alexandr.berdnik at gmail.com (Alexandr Berdnik)
Date: Tue, 16 Jul 2013 18:46:13 +0100
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
	<CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
Message-ID: <CAG6hYpYcH_jRRvzEeUF-TQnQdwmjOMBUoU_xzjL-eEArv+OJLA@mail.gmail.com>

Got it, thanks.


On Tue, Jul 16, 2013 at 6:42 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> No, that's not a possible outcome.  Volatile reads can't reorder with each
> other and volatile stores cannot reorder with each other.  I think your
> question was mostly about volatile writes though - if you read bb=1 then it
> must be that b=1 executed and thus a=1 as well.
>
> Sent from my phone
> On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" <alexandr.berdnik at gmail.com>
> wrote:
>
>> Hi all,
>> just a quick question here.
>>
>> volatile int a = 0;
>> volatile int b = 0;
>>
>> // thread 1
>> a = 1;
>> b = 1;
>>
>> // thread 2
>> int aa = a;
>> int bb = b;
>>
>> The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
>> reordered? I have not found any guarantees for that in JMM.
>>
>> Thanks,
>> Alex.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/2e1ff329/attachment.html>

From mr.chrisvest at gmail.com  Tue Jul 16 13:55:38 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Tue, 16 Jul 2013 19:55:38 +0200
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
Message-ID: <CAHXi_0eTj63nN_MqFyyPqrj7hDj8oj7QYJm2jg58Y_HHBx3pDQ@mail.gmail.com>

You can have this interleaving (given a = 0, b = 0):

Thread 1    Thread 2
             aa = a
 a = 1
 b = 1
             bb = b

Causing aa == 0 and bb == 1.

Reordering within each thread won't happen, though.



On 16 July 2013 19:31, Alexandr Berdnik <alexandr.berdnik at gmail.com> wrote:

> Hi all,
> just a quick question here.
>
> volatile int a = 0;
> volatile int b = 0;
>
> // thread 1
> a = 1;
> b = 1;
>
> // thread 2
> int aa = a;
> int bb = b;
>
> The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
> reordered? I have not found any guarantees for that in JMM.
>
> Thanks,
> Alex.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/b02e0970/attachment.html>

From oleksandr.otenko at oracle.com  Tue Jul 16 14:36:25 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 16 Jul 2013 19:36:25 +0100
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
	<CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
Message-ID: <51E592A9.3070101@oracle.com>

yes, but b==1 implies a==1, not bb==1 implies aa==1.

You need to reason from:

Rb1 implies Wb1 by meaning of read and write operations
Wb1 implies Wa1 by program order and volatile semantics

hence Rb1 implies Wa1.

A true statement is this:

Ra1 implies Rb1 by program order and volatile semantics
Rb1 implies Wb1 by meaning of read and write operations
Wb1 implies Wa1 by program order and volatile semantics

Which is a subset of the outcomes of the following (means the right 
order is the following):
bb=b;
aa=a;
assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1


Alex

On 16/07/2013 18:42, Vitaly Davidovich wrote:
>
> No, that's not a possible outcome.  Volatile reads can't reorder with 
> each other and volatile stores cannot reorder with each other.  I 
> think your question was mostly about volatile writes though - if you 
> read bb=1 then it must be that b=1 executed and thus a=1 as well.
>
> Sent from my phone
>
> On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" 
> <alexandr.berdnik at gmail.com <mailto:alexandr.berdnik at gmail.com>> wrote:
>
>     Hi all,
>     just a quick question here.
>
>     volatile int a = 0;
>     volatile int b = 0;
>
>     // thread 1
>     a = 1;
>     b = 1;
>
>     // thread 2
>     int aa = a;
>     int bb = b;
>
>     The question is: if aa=0, bb=1 is acceptable? I.e. can volatile
>     reads be reordered? I have not found any guarantees for that in JMM.
>
>     Thanks,
>     Alex.
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/1590cc28/attachment-0001.html>

From chris.w.dennis at gmail.com  Tue Jul 16 14:36:31 2013
From: chris.w.dennis at gmail.com (Chris Dennis)
Date: Tue, 16 Jul 2013 14:36:31 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
 Issue 29
In-Reply-To: <CAChYfd_Lak6ULUNAD41Y9oJvdTzTR0dhGDu3hGsDjaUr=gzBFw@mail.gmail.com>
Message-ID: <CE0B0A30.30D86%chris.w.dennis@gmail.com>

>From my memory of the code I think you're right.  NonBlockingHashMap makes
no attempt to shrink its table when the occupancy drops.  Of course CHM
doesn't either, it's just that the effect of this is more pronounced with
open addressing.  Does anyone know if the new CHMV8 ever attempts to
rehash in to a smaller table? Has anyone ever looked in to the possibility
of doing this?

Chris

On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:

>It is best to get a heap dump to determine what is using the memory.
>However a quick guess would be that NonBlockingHashMap uses uses
>open-addressing rather than bucket and chain design so you'll have some
>large arrays for the keys and values.  If you put a lot of items in the
>set then remove them the backing arrays will still be large having had to
>grow to accommodate the entries.
>
>Regards,
>Martin...
>
>On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>
>wrote:
>
>Send Concurrency-interest mailing list submissions to
>        concurrency-interest at cs.oswego.edu
>Date: Mon, 15 Jul 2013 16:49:13 -0400
>From: Ariel Weisberg <ariel at weisberg.ws>
>To: concurrency-interest at cs.oswego.edu
>Subject: [concurrency-interest] Unexpected memory usage in
>        NonBlockingHashHashMap
>Message-ID:
>        
><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>
>Content-Type: text/plain
>
>Hi all,
>
>I was trying to make use of Cliff Click's NonBlockingHashSet and
>NonBlockingHashMap
>(1.1.2) and found unexpected heap memory usage. When I did a heap dump
>of live objects I found that a lot of resources were being used even
>though size for the data structures reports 0 or some reasonable number
>of entries.
>
>Attached is a quick demonstration app that creates a set of longs and
>churns through through the set adding and removing entries, occasionally
>replacing most of the set. It seems like removing and replacing most of
>the entries triggers the behavior that leaks the most memory.
>
>I am using
>java version "1.6.0_27"
>OpenJDK Runtime Environment (IcedTea6 1.12.5)
>(6b27-1.12.5-0ubuntu0.12.04.1)
>OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
>
>It also reproduced with Sun JDK 1.6.0_39
>
>You can swap in ConcurrentHashMap and it works as expected and produces
>modest 9 megabyte heap dumps.
>
>Is this an expected behavior for this use case? Is there some caveat I
>should know?
>
>Thanks,
>Ariel
>
>
>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From vitalyd at gmail.com  Tue Jul 16 14:46:43 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 16 Jul 2013 14:46:43 -0400
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <51E592A9.3070101@oracle.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
	<CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
	<51E592A9.3070101@oracle.com>
Message-ID: <CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA@mail.gmail.com>

Right, my response was only in context of JMM as that's what the question
was framed on, I should've made that clearer - as Chris pointed out,
scheduling can yield the said outcome.

Sent from my phone
On Jul 16, 2013 2:36 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  yes, but b==1 implies a==1, not bb==1 implies aa==1.
>
> You need to reason from:
>
> Rb1 implies Wb1 by meaning of read and write operations
> Wb1 implies Wa1 by program order and volatile semantics
>
> hence Rb1 implies Wa1.
>
> A true statement is this:
>
> Ra1 implies Rb1 by program order and volatile semantics
> Rb1 implies Wb1 by meaning of read and write operations
> Wb1 implies Wa1 by program order and volatile semantics
>
> Which is a subset of the outcomes of the following (means the right order
> is the following):
> bb=b;
> aa=a;
> assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1
>
>
> Alex
>
> On 16/07/2013 18:42, Vitaly Davidovich wrote:
>
> No, that's not a possible outcome.  Volatile reads can't reorder with each
> other and volatile stores cannot reorder with each other.  I think your
> question was mostly about volatile writes though - if you read bb=1 then it
> must be that b=1 executed and thus a=1 as well.
>
> Sent from my phone
> On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" <alexandr.berdnik at gmail.com>
> wrote:
>
>>       Hi all,
>> just a quick question here.
>>
>>  volatile int a = 0;
>>  volatile int b = 0;
>>
>>  // thread 1
>>  a = 1;
>>  b = 1;
>>
>>  // thread 2
>>  int aa = a;
>>  int bb = b;
>>
>>  The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
>> reordered? I have not found any guarantees for that in JMM.
>>
>>  Thanks,
>>  Alex.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/77af3dee/attachment.html>

From ariel at weisberg.ws  Tue Jul 16 15:03:04 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Tue, 16 Jul 2013 15:03:04 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
 Issue 29
In-Reply-To: <CE0B0A30.30D86%chris.w.dennis@gmail.com>
References: <CE0B0A30.30D86%chris.w.dennis@gmail.com>
Message-ID: <1374001384.21088.140661256390085.573194C5@webmail.messagingengine.com>

Hi,

With at most 54k entries in the table at any time does it really need to
allocate millions of slots? It also doesn't help that it keeps
references to removed entries. You can empty the map, take a heap dump,
and still see references to objects that were stored in the map that are
reachable from the map.

I get that it is a pathological behavior, but it occurs without a lot of
work. I got hit by it using 3 NBHMs to track connection specific state
with an app that maintains 50-150k connections. I started 3 clients apps
each creating 50k connections, and 1k connections that cycle and it will
slowly take up more space and if I restart a client it seems to use more
memory immediately as it drops and adds 50k connection entries. I can do
it with just one client.

I think the space utilization is odd period. If I put in a 100k random
numbers it allocates 1 million slots (although I guess you / 2 since
slots are used for keys as well). If I completely remove all values and
then add back 100k more and repeat the process the number of slots
allocated increases indefinitely! And this is from a single thread. It
also pauses for a long time, and if you drop in CHM it works as
expected.

See attached single threaded app grows the map indefinitely as
described. You don't have to run long to watch it choke. You can see
that occasionally it pauses for a long time to grow the number of slots,
runs fast for a while, and then pauses to rehash. I am removing all
entries every 100k entries.

Thanks,
Ariel

On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:
> From my memory of the code I think you're right.  NonBlockingHashMap
> makes
> no attempt to shrink its table when the occupancy drops.  Of course CHM
> doesn't either, it's just that the effect of this is more pronounced with
> open addressing.  Does anyone know if the new CHMV8 ever attempts to
> rehash in to a smaller table? Has anyone ever looked in to the
> possibility
> of doing this?
> 
> Chris
> 
> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:
> 
> >It is best to get a heap dump to determine what is using the memory.
> >However a quick guess would be that NonBlockingHashMap uses uses
> >open-addressing rather than bucket and chain design so you'll have some
> >large arrays for the keys and values.  If you put a lot of items in the
> >set then remove them the backing arrays will still be large having had to
> >grow to accommodate the entries.
> >
> >Regards,
> >Martin...
> >
> >On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>
> >wrote:
> >
> >Send Concurrency-interest mailing list submissions to
> >        concurrency-interest at cs.oswego.edu
> >Date: Mon, 15 Jul 2013 16:49:13 -0400
> >From: Ariel Weisberg <ariel at weisberg.ws>
> >To: concurrency-interest at cs.oswego.edu
> >Subject: [concurrency-interest] Unexpected memory usage in
> >        NonBlockingHashHashMap
> >Message-ID:
> >        
> ><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>
> >Content-Type: text/plain
> >
> >Hi all,
> >
> >I was trying to make use of Cliff Click's NonBlockingHashSet and
> >NonBlockingHashMap
> >(1.1.2) and found unexpected heap memory usage. When I did a heap dump
> >of live objects I found that a lot of resources were being used even
> >though size for the data structures reports 0 or some reasonable number
> >of entries.
> >
> >Attached is a quick demonstration app that creates a set of longs and
> >churns through through the set adding and removing entries, occasionally
> >replacing most of the set. It seems like removing and replacing most of
> >the entries triggers the behavior that leaks the most memory.
> >
> >I am using
> >java version "1.6.0_27"
> >OpenJDK Runtime Environment (IcedTea6 1.12.5)
> >(6b27-1.12.5-0ubuntu0.12.04.1)
> >OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
> >
> >It also reproduced with Sun JDK 1.6.0_39
> >
> >You can swap in ConcurrentHashMap and it works as expected and produces
> >modest 9 megabyte heap dumps.
> >
> >Is this an expected behavior for this use case? Is there some caveat I
> >should know?
> >
> >Thanks,
> >Ariel
> >
> >
> >
> >
> >_______________________________________________
> >Concurrency-interest mailing list
> >Concurrency-interest at cs.oswego.edu
> >http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: HammerSet.java
Type: text/x-java
Size: 1007 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/e659aa19/attachment.bin>

From valentin.male.kovalenko at gmail.com  Tue Jul 16 16:20:19 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Wed, 17 Jul 2013 00:20:19 +0400
Subject: [concurrency-interest] volatile read reorderings
Message-ID: <CAO-wXw+eqkVURBP9u+4cjr_OQBhTswooXUknnfYpC0DqWCb51g@mail.gmail.com>

>>can volatile reads be reordered?
>>I have not found any guarantees for that in JMM.
Check this:
http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5
there is a statement "A write to a volatile field happens-before every
subsequent read of that field."

>>The question is: if aa=0, bb=1 is acceptable?
No, it's impossible because of happens-before relation noted above.

>>can volatile reads be reordered?
You do_not_need_to_care_about_reorderings if you are just a Java developer.
All you need is to be sure that there are no data races (conflicting
accesses that are not ordered by a happens-before relationship) in your
program. This is no just my opinion, JLS states this in "17.4.5.
Happens-before Order paragraph" (see "A program is correctly synchronized
if and only if all sequentially consistent executions are free of data
races..." and and further)

--
Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/eec162c2/attachment.html>

From daniel.norberg at gmail.com  Tue Jul 16 18:56:06 2013
From: daniel.norberg at gmail.com (Daniel Norberg)
Date: Wed, 17 Jul 2013 00:56:06 +0200
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
	Issue 29
In-Reply-To: <1374001384.21088.140661256390085.573194C5@webmail.messagingengine.com>
References: <CE0B0A30.30D86%chris.w.dennis@gmail.com>
	<1374001384.21088.140661256390085.573194C5@webmail.messagingengine.com>
Message-ID: <5FAF8E27-40B8-4207-B619-2ED4F75F1507@gmail.com>

Hi,

I encountered this issue with NonBlockingHashMap last year as well. It indeed seems to be due to how NBHM keeps tombstones for removed entries and keeps doubling the size of the backing array when it becomes full, even if it is just contains tombstones.

http://sourceforge.net/p/high-scale-lib/bugs/14/

NBHM seems to work well when the set of observed keys is bounded but explodes in the face of a high churn of unique keys. I recommend using CHM instead.

Cheers,
Daniel

On 16 jul 2013, at 21:03, Ariel Weisberg <ariel at weisberg.ws> wrote:

> Hi,
> 
> With at most 54k entries in the table at any time does it really need to
> allocate millions of slots? It also doesn't help that it keeps
> references to removed entries. You can empty the map, take a heap dump,
> and still see references to objects that were stored in the map that are
> reachable from the map.
> 
> I get that it is a pathological behavior, but it occurs without a lot of
> work. I got hit by it using 3 NBHMs to track connection specific state
> with an app that maintains 50-150k connections. I started 3 clients apps
> each creating 50k connections, and 1k connections that cycle and it will
> slowly take up more space and if I restart a client it seems to use more
> memory immediately as it drops and adds 50k connection entries. I can do
> it with just one client.
> 
> I think the space utilization is odd period. If I put in a 100k random
> numbers it allocates 1 million slots (although I guess you / 2 since
> slots are used for keys as well). If I completely remove all values and
> then add back 100k more and repeat the process the number of slots
> allocated increases indefinitely! And this is from a single thread. It
> also pauses for a long time, and if you drop in CHM it works as
> expected.
> 
> See attached single threaded app grows the map indefinitely as
> described. You don't have to run long to watch it choke. You can see
> that occasionally it pauses for a long time to grow the number of slots,
> runs fast for a while, and then pauses to rehash. I am removing all
> entries every 100k entries.
> 
> Thanks,
> Ariel
> 
> On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:
>> From my memory of the code I think you're right.  NonBlockingHashMap
>> makes
>> no attempt to shrink its table when the occupancy drops.  Of course CHM
>> doesn't either, it's just that the effect of this is more pronounced with
>> open addressing.  Does anyone know if the new CHMV8 ever attempts to
>> rehash in to a smaller table? Has anyone ever looked in to the
>> possibility
>> of doing this?
>> 
>> Chris
>> 
>> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:
>> 
>>> It is best to get a heap dump to determine what is using the memory.
>>> However a quick guess would be that NonBlockingHashMap uses uses
>>> open-addressing rather than bucket and chain design so you'll have some
>>> large arrays for the keys and values.  If you put a lot of items in the
>>> set then remove them the backing arrays will still be large having had to
>>> grow to accommodate the entries.
>>> 
>>> Regards,
>>> Martin...
>>> 
>>> On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>
>>> wrote:
>>> 
>>> Send Concurrency-interest mailing list submissions to
>>>       concurrency-interest at cs.oswego.edu
>>> Date: Mon, 15 Jul 2013 16:49:13 -0400
>>> From: Ariel Weisberg <ariel at weisberg.ws>
>>> To: concurrency-interest at cs.oswego.edu
>>> Subject: [concurrency-interest] Unexpected memory usage in
>>>       NonBlockingHashHashMap
>>> Message-ID:
>>> 
>>> <1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>
>>> Content-Type: text/plain
>>> 
>>> Hi all,
>>> 
>>> I was trying to make use of Cliff Click's NonBlockingHashSet and
>>> NonBlockingHashMap
>>> (1.1.2) and found unexpected heap memory usage. When I did a heap dump
>>> of live objects I found that a lot of resources were being used even
>>> though size for the data structures reports 0 or some reasonable number
>>> of entries.
>>> 
>>> Attached is a quick demonstration app that creates a set of longs and
>>> churns through through the set adding and removing entries, occasionally
>>> replacing most of the set. It seems like removing and replacing most of
>>> the entries triggers the behavior that leaks the most memory.
>>> 
>>> I am using
>>> java version "1.6.0_27"
>>> OpenJDK Runtime Environment (IcedTea6 1.12.5)
>>> (6b27-1.12.5-0ubuntu0.12.04.1)
>>> OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
>>> 
>>> It also reproduced with Sun JDK 1.6.0_39
>>> 
>>> You can swap in ConcurrentHashMap and it works as expected and produces
>>> modest 9 megabyte heap dumps.
>>> 
>>> Is this an expected behavior for this use case? Is there some caveat I
>>> should know?
>>> 
>>> Thanks,
>>> Ariel
>>> 
>>> 
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> <HammerSet.java>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/c34ebf47/attachment-0001.html>

From fedos.serg at gmail.com  Wed Jul 17 03:02:12 2013
From: fedos.serg at gmail.com (Sergey Fedoseenkov)
Date: Wed, 17 Jul 2013 11:02:12 +0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
 Issue 29
Message-ID: <CAB8u1202hV_nAQdCr06kA=vJ_goUP3G_3_oJK8bRY_Z+oUsJBw@mail.gmail.com>

Hi, didnt't you consider using some data structure for keeping track of
your NBHMs with connections for each user separately so when some user
drops off you are able to remove the reference to NBHM from it and it will
be GCed.

It looks like from the description that new users don't join frequently so
this data structure may be CHM.

--
Sergey

On Wednesday, July 17, 2013, wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu <javascript:;>
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu <javascript:;>
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu <javascript:;>
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Re: Concurrency-interest Digest, Vol 102, Issue 29 (Chris Dennis)
>    2. Re: volatile read reorderings (Vitaly Davidovich)
>    3. Re: Concurrency-interest Digest, Vol 102, Issue 29
>       (Ariel Weisberg)
>    4. Re: volatile read reorderings (Valentin Kovalenko)
>    5. Re: Concurrency-interest Digest, Vol 102, Issue 29
>       (Daniel Norberg)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 16 Jul 2013 14:36:31 -0400
> From: Chris Dennis <chris.w.dennis at gmail.com <javascript:;>>
> To: Martin Thompson <mjpt777 at gmail.com <javascript:;>>, Ariel Weisberg
>         <ariel at weisberg.ws <javascript:;>>,     <
> concurrency-interest at cs.oswego.edu <javascript:;>>
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>         102, Issue 29
> Message-ID: <CE0B0A30.30D86%chris.w.dennis at gmail.com <javascript:;>>
> Content-Type: text/plain;       charset="UTF-8"
>
> >From my memory of the code I think you're right.  NonBlockingHashMap makes
> no attempt to shrink its table when the occupancy drops.  Of course CHM
> doesn't either, it's just that the effect of this is more pronounced with
> open addressing.  Does anyone know if the new CHMV8 ever attempts to
> rehash in to a smaller table? Has anyone ever looked in to the possibility
> of doing this?
>
> Chris
>
> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com <javascript:;>>
> wrote:
>
> >It is best to get a heap dump to determine what is using the memory.
> >However a quick guess would be that NonBlockingHashMap uses uses
> >open-addressing rather than bucket and chain design so you'll have some
> >large arrays for the keys and values.  If you put a lot of items in the
> >set then remove them the backing arrays will still be large having had to
> >grow to accommodate the entries.
> >
> >Regards,
> >Martin...
> >
> >On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu<javascript:;>
> >
> >wrote:
> >
> >Send Concurrency-interest mailing list submissions to
> >        concurrency-interest at cs.oswego.edu <javascript:;>
> >Date: Mon, 15 Jul 2013 16:49:13 -0400
> >From: Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> >To: concurrency-interest at cs.oswego.edu <javascript:;>
> >Subject: [concurrency-interest] Unexpected memory usage in
> >        NonBlockingHashHashMap
> >Message-ID:
> >
> ><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com<javascript:;>
> >
> >Content-Type: text/plain
> >
> >Hi all,
> >
> >I was trying to make use of Cliff Click's NonBlockingHashSet and
> >NonBlockingHashMap
> >(1.1.2) and found unexpected heap memory usage. When I did a heap dump
> >of live objects I found that a lot of resources were being used even
> >though size for the data structures reports 0 or some reasonable number
> >of entries.
> >
> >Attached is a quick demonstration app that creates a set of longs and
> >churns through through the set adding and removing entries, occasionally
> >replacing most of the set. It seems like removing and replacing most of
> >the entries triggers the behavior that leaks the most memory.
> >
> >I am using
> >java version "1.6.0_27"
> >OpenJDK Runtime Environment (IcedTea6 1.12.5)
> >(6b27-1.12.5-0ubuntu0.12.04.1)
> >OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
> >
> >It also reproduced with Sun JDK 1.6.0_39
> >
> >You can swap in ConcurrentHashMap and it works as expected and produces
> >modest 9 megabyte heap dumps.
> >
> >Is this an expected behavior for this use case? Is there some caveat I
> >should know?
> >
> >Thanks,
> >Ariel
> >
> >
> >
> >
> >_______________________________________________
> >Concurrency-interest mailing list
> >Concurrency-interest at cs.oswego.edu <javascript:;>
> >http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 16 Jul 2013 14:46:43 -0400
> From: Vitaly Davidovich <vitalyd at gmail.com <javascript:;>>
> To: oleksandr otenko <oleksandr.otenko at oracle.com <javascript:;>>
> Cc: concurrency-interest at cs.oswego.edu <javascript:;>,   Alexandr Berdnik
>         <alexandr.berdnik at gmail.com <javascript:;>>
> Subject: Re: [concurrency-interest] volatile read reorderings
> Message-ID:
>         <
> CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA at mail.gmail.com<javascript:;>
> >
> Content-Type: text/plain; charset="iso-8859-1"
>
> Right, my response was only in context of JMM as that's what the question
> was framed on, I should've made that clearer - as Chris pointed out,
> scheduling can yield the said outcome.
>
> Sent from my phone
> On Jul 16, 2013 2:36 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com<javascript:;>
> >
> wrote:
>
> >  yes, but b==1 implies a==1, not bb==1 implies aa==1.
> >
> > You need to reason from:
> >
> > Rb1 implies Wb1 by meaning of read and write operations
> > Wb1 implies Wa1 by program order and volatile semantics
> >
> > hence Rb1 implies Wa1.
> >
> > A true statement is this:
> >
> > Ra1 implies Rb1 by program order and volatile semantics
> > Rb1 implies Wb1 by meaning of read and write operations
> > Wb1 implies Wa1 by program order and volatile semantics
> >
> > Which is a subset of the outcomes of the following (means the right order
> > is the following):
> > bb=b;
> > aa=a;
> > assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1
> >
> >
> > Alex
> >
> > On 16/07/2013 18:42, Vitaly Davidovich wrote:
> >
> > No, that's not a possible outcome.  Volatile reads can't reorder with
> each
> > other and volatile stores cannot reorder with each other.  I think your
> > question was mostly about volatile writes though - if you read bb=1 then
> it
> > must be that b=1 executed and thus a=1 as well.
> >
> > Sent from my phone
> > On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" <alexandr.berdnik at gmail.com<javascript:;>
> >
> > wrote:
> >
> >>       Hi all,
> >> just a quick question here.
> >>
> >>  volatile int a = 0;
> >>  volatile int b = 0;
> >>
> >>  // thread 1
> >>  a = 1;
> >>  b = 1;
> >>
> >>  // thread 2
> >>  int aa = a;
> >>  int bb = b;
> >>
> >>  The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads
> be
> >> reordered? I have not found any guarantees for that in JMM.
> >>
> >>  Thanks,
> >>  Alex.
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu <javascript:;>
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing listConcurrency-interest
> @cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/77af3dee/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Tue, 16 Jul 2013 15:03:04 -0400
> From: Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> To: Chris Dennis <chris.w.dennis at gmail.com <javascript:;>>,    Martin
> Thompson
>         <mjpt777 at gmail.com <javascript:;>>,
> concurrency-interest at cs.oswego.edu <javascript:;>
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>         102, Issue 29
> Message-ID:
>         <
> 1374001384.21088.140661256390085.573194C5 at webmail.messagingengine.com<javascript:;>
> >
>
> Content-Type: text/plain; charset="us-ascii"
>
> Hi,
>
> With at most 54k entries in the table at any time does it really need to
> allocate millions of slots? It also doesn't help that it keeps
> references to removed entries. You can empty the map, take a heap dump,
> and still see references to objects that were stored in the map that are
> reachable from the map.
>
> I get that it is a pathological behavior, but it occurs without a lot of
> work. I got hit by it using 3 NBHMs to track connection specific state
> with an app that maintains 50-150k connections. I started 3 clients apps
> each creating 50k connections, and 1k connections that cycle and it will
> slowly take up more space and if I restart a client it seems to use more
> memory immediately as it drops and adds 50k connection entries. I can do
> it with just one client.
>
> I think the space utilization is odd period. If I put in a 100k random
> numbers it allocates 1 million slots (although I guess you / 2 since
> slots are used for keys as well). If I completely remove all values and
> then add back 100k more and repeat the process the number of slots
> allocated increases indefinitely! And this is from a single thread. It
> also pauses for a long time, and if you drop in CHM it works as
> expected.
>
> See attached single threaded app grows the map indefinitely as
> described. You don't have to run long to watch it choke. You can see
> that occasionally it pauses for a long time to grow the number of slots,
> runs fast for a while, and then pauses to rehash. I am removing all
> entries every 100k entries.
>
> Thanks,
> Ariel
>
> On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:
> > From my memory of the code I think you're right.  NonBlockingHashMap
> > makes
> > no attempt to shrink its table when the occupancy drops.  Of course CHM
> > doesn't either, it's just that the effect of this is more pronounced with
> > open addressing.  Does anyone know if the new CHMV8 ever attempts to
> > rehash in to a smaller table? Has anyone ever looked in to the
> > possibility
> > of doing this?
> >
> > Chris
> >
> > On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com <javascript:;>>
> wrote:
> >
> > >It is best to get a heap dump to determine what is using the memory.
> > >However a quick guess would be that NonBlockingHashMap uses uses
> > >open-addressing rather than bucket and chain design so you'll have some
> > >large arrays for the keys and values.  If you put a lot of items in the
> > >set then remove them the backing arrays will still be large having had
> to
> > >grow to accommodate the entries.
> > >
> > >Regards,
> > >Martin...
> > >
> > >On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu<javascript:;>
> >
> > >wrote:
> > >
> > >Send Concurrency-interest mailing list submissions to
> > >        concurrency-interest at cs.oswego.edu <javascript:;>
> > >Date: Mon, 15 Jul 2013 16:49:13 -0400
> > >From: Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> > >To: concurrency-interest at cs.oswego.edu <javascript:;>
> > >Subject: [concurrency-interest] Unexpected memory usage in
> > >        NonBlockingHashHashMap
> > >Message-ID:
> > >
> > ><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com<javascript:;>
> >
> > >Content-Type: text/plain
> > >
> > >Hi all,
> > >
> > >I was trying to make use of Cliff Click's NonBlockingHashSet and
> > >NonBlockingHashMap
> > >(1.1.2) and found unexpected heap memory usage. When I did a heap dump
> > >of live objects I found that a lot of resources were being used even
> > >though size for the data structures reports 0 or some reasonable number
> > >of entries.
> > >
> > >Attached is a quick demonstration app that creates a set of longs and
> > >churns through through the set adding and removing entries, occasionally
> > >replacing most of the set. It seems like removing and replacing most of
> > >the entries triggers the behavior that leaks the most memory.
> > >
> > >I am using
> > >java version "1.6.0_27"
> > >OpenJDK Runtime Environment (IcedTea6 1.12.5)
> > >(6b27-1.12.5-0ubuntu0.12.04.1)
> > >OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
> > >
> > >It also reproduced with Sun JDK 1.6.0_39
> > >
> > >You can swap in ConcurrentHashMap and it works as expected and produces
> > >modest 9 megabyte heap dumps.
> > >
> > >Is this an expected behavior for this use case? Is there some caveat I
> > >should know?
> > >
> > >Thanks,
> > >Ariel
> > >
> > >
> > >
> > >
> > >_______________________________________________
> > >Concurrency-interest mailing list
> > >Concurrency-interest at cs.oswego.edu <javascript:;>
> > >http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> -------------- next part --------------
> A non-text attachment was scrubbed...
> Name: HammerSet.java
> Type: text/x-java
> Size: 1007 bytes
> Desc: not available
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/e659aa19/attachment-0001.bin
> >
>
> ------------------------------
>
> Message: 4
> Date: Wed, 17 Jul 2013 00:20:19 +0400
> From: Valentin Kovalenko <valentin.male.kovalenko at gmail.com <javascript:;>
> >
> To: concurrency-interest at cs.oswego.edu <javascript:;>
> Subject: Re: [concurrency-interest] volatile read reorderings
> Message-ID:
>         <
> CAO-wXw+eqkVURBP9u+4cjr_OQBhTswooXUknnfYpC0DqWCb51g at mail.gmail.com<javascript:;>
> >
> Content-Type: text/plain; charset="utf-8"
>
> >>can volatile reads be reordered?
> >>I have not found any guarantees for that in JMM.
> Check this:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5
> there is a statement "A write to a volatile field happens-before every
> subsequent read of that field."
>
> >>The question is: if aa=0, bb=1 is acceptable?
> No, it's impossible because of happens-before relation noted above.
>
> >>can volatile reads be reordered?
> You do_not_need_to_care_about_reorderings if you are just a Java developer.
> All you need is to be sure that there are no data races (conflicting
> accesses that are not ordered by a happens-before relationship) in your
> program. This is no just my opinion, JLS states this in "17.4.5.
> Happens-before Order paragraph" (see "A program is correctly synchronized
> if and only if all sequentially consistent executions are free of data
> races..." and and further)
>
> --
> Valentin
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/eec162c2/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 5
> Date: Wed, 17 Jul 2013 00:56:06 +0200
> From: Daniel Norberg <daniel.norberg at gmail.com <javascript:;>>
> To: Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> Cc: Martin Thompson <mjpt777 at gmail.com <javascript:;>>,
>         "concurrency-interest at cs.oswego.edu <javascript:;>"
>         <concurrency-interest at cs.oswego.edu <javascript:;>>
> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>         102,    Issue 29
> Message-ID: <5FAF8E27-40B8-4207-B619-2ED4F75F1507 at gmail.com <javascript:;>
> >
> Content-Type: text/plain; charset="us-ascii"
>
> Hi,
>
> I encountered this issue with NonBlockingHashMap last year as well. It
> indeed seems to be due to how NBHM keeps tombstones for removed entries and
> keeps doubling the size of the backing array when it becomes full, even if
> it is just contains tombstones.
>
> http://sourceforge.net/p/high-scale-lib/bugs/14/
>
> NBHM seems to work well when the set of observed keys is bounded but
> explodes in the face of a high churn of unique keys. I recommend using CHM
> instead.
>
> Cheers,
> Daniel
>
> On 16 jul 2013, at 21:03, Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> wrote:
>
> > Hi,
> >
> > With at most 54k entries in the table at any time does it really need to
> > allocate millions of slots? It also doesn't help that it keeps
> > references to removed entries. You can empty the map, take a heap dump,
> > and still see references to objects that were stored in the map that are
> > reachable from the map.
> >
> > I get that it is a pathological behavior, but it occurs without a lot of
> > work. I got hit by it using 3 NBHMs to track connection specific state
> > with an app that maintains 50-150k connections. I started 3 clients apps
> > each creating 50k connections, and 1k connections that cycle and it will
> > slowly take up more space and if I restart a client it seems to use more
> > memory immediately as it drops and adds 50k connection entries. I can do
> > it with just one client.
> >
> > I think the space utilization is odd period. If I put in a 100k random
> > numbers it allocates 1 million slots (although I guess you / 2 since
> > slots are used for keys as well). If I completely remove all values and
> > then add back 100k more and repeat the process the number of slots
> > allocated increases indefinitely! And this is from a single thread. It
> > also pauses for a long time, and if you drop in CHM it works as
> > expected.
> >
> > See attached single threaded app grows the map indefinitely as
> > described. You don't have to run long to watch it choke. You can see
> > that occasionally it pauses for a long time to grow the number of slots,
> > runs fast for a while, and then pauses to rehash. I am removing all
> > entries every 100k entries.
> >
> > Thanks,
> > Ariel
> >
> > On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:
> >> From my memory of the code I think you're right.  NonBlockingHashMap
> >> makes
> >> no attempt to shrink its table when the occupancy drops.  Of course CHM
> >> doesn't either, it's just that the effect of this is more pronounced
> with
> >> open addressing.  Does anyone know if the new CHMV8 ever attempts to
> >> rehash in to a smaller table? Has anyone ever looked in to the
> >> possibility
> >> of doing this?
> >>
> >> Chris
> >>
> >> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com<javascript:;>>
> wrote:
> >>
> >>> It is best to get a heap dump to determine what is using the memory.
> >>> However a quick guess would be that NonBlockingHashMap uses uses
> >>> open-addressing rather than bucket and chain design so you'll have some
> >>> large arrays for the keys and values.  If you put a lot of items in the
> >>> set then remove them the backing arrays will still be large having had
> to
> >>> grow to accommodate the entries.
> >>>
> >>> Regards,
> >>> Martin...
> >>>
> >>> On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu<javascript:;>
> >
> >>> wrote:
> >>>
> >>> Send Concurrency-interest mailing list submissions to
> >>>       concurrency-interest at cs.oswego.edu <javascript:;>
> >>> Date: Mon, 15 Jul 2013 16:49:13 -0400
> >>> From: Ariel Weisberg <ariel at weisberg.ws <javascript:;>>
> >>> To: concurrency-interest at cs.oswego.edu <javascript:;>
> >>> Subject: [concurrency-interest] Unexpected memory usage in
> >>>       NonBlockingHashHashMap
> >>> Message-ID:
> >>>
> >>> <1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com<javascript:;>
> >
> >>> Content-Type: text/plain
> >>>
> >>> Hi all,
> >>>
> >>> I was trying to make use of Cliff Click's NonBlockingHashSet and
> >>> NonBlockingHashMap
> >>> (1.1.2) and found unexpected heap memory usage. When I did a heap dump
> >>> of live objects I found that a lot of resources were being used even
> >>> though size for the data structures reports 0 or some reasonable number
> >>> of entries.
> >>>
> >>> Attached is a quick demonstration app that creates a set of longs and
> >>> churns through through the set adding and removing entries,
> occasionally
> >>> replacing most of the set. It seems like removing and replacing most of
> >>> the entries triggers the behavior that leaks the most memory.
> >>>
> >>> I am using
> >>> java version "1.6.0_27"
> >>> OpenJDK Runtime Environment (IcedTea6 1.12.5)
> >>> (6b27-1.12.5-0ubuntu0.12.04.1)
> >>> OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
> >>>
> >>> It also reproduced with Sun JDK 1.6.0_39
> >>>
> >>> You can swap in ConcurrentHashMap and it works as expected and produces
> >>> modest 9 megabyte heap dumps.
> >>>
> >>> Is this an expected behavior for this use case? Is there some caveat I
> >>> should know?
> >>>
> >>> Thanks,
> >>> Ariel
> >>>
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu <javascript:;>
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > <HammerSet.java>
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu <javascript:;>
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/c34ebf47/attachment.html
> >
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <javascript:;>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 102, Issue 31
> *****************************************************
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/6b43985c/attachment-0001.html>

From nitsanw at yahoo.com  Wed Jul 17 05:01:34 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 17 Jul 2013 02:01:34 -0700 (PDT)
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAO-wXw+eqkVURBP9u+4cjr_OQBhTswooXUknnfYpC0DqWCb51g@mail.gmail.com>
References: <CAO-wXw+eqkVURBP9u+4cjr_OQBhTswooXUknnfYpC0DqWCb51g@mail.gmail.com>
Message-ID: <1374051694.18173.YahooMailNeo@web120705.mail.ne1.yahoo.com>

>>The question is: if aa=0, bb=1 is acceptable?
>No, it's impossible because of happens-before relation noted above.
Wrong.
To repeat Mr. Vest:
>You can have this interleaving (given a = 0, b = 0):
>
>
>Thread 1 ? ?Thread 2
>? ? ? ? ? ? ?aa = a
>?a = 1
>?b = 1
>? ? ? ? ? ? ?bb = b
>
>
>Causing aa == 0 and bb == 1.
>
>
>Reordering within each thread won't happen, though.
bb == 1 --> a == 1, that is the happens before relationship. ?bb == 0 --> aa == 0 is also part of the happens before relationship. But nothing stops the JVM from pausing either thread between statements.

>there is a statement "A write to a volatile field happens-before every subsequent read of that field."
This is an unfortunate wording, there is no way to force arbitrary reads to happen concurrently. It means that once the write has been OBSERVED, all other changes which happened before it will be OBSERVED to have happened before it, weather they are volatile or not. You can write some code to demonstrate the above, or have a look at the jcstress/torture-suite (http://openjdk.java.net/projects/code-tools/jcstress/) for examples of this behaviour?and others.
.


From oleksandr.otenko at oracle.com  Wed Jul 17 05:38:40 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 17 Jul 2013 10:38:40 +0100
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
	<CAHjP37HQrphrSL+BsRW-W19jK-zbv1ZfKoq+Ph5gTxnSZzuXRA@mail.gmail.com>
	<51E592A9.3070101@oracle.com>
	<CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA@mail.gmail.com>
Message-ID: <51E66620.9020703@oracle.com>

I am not criticizing you. I am taking this opportunity to demonstrate 
the question must be positioned the other way around.

Instead of asking "is it possible that a bad outcome can happen", the 
engineer should ask "is this statement provable / can we rely on / can 
we design the algorithm to take advantage of the proposition that aa==1, 
if we prove bb==1"

This is the same as constructively proving

(Rb1?Wb1)?(Ra1?Wa1)

(given evidence of Rb1?Wb1, eg bb==1, prove Ra1?Wa1)

This is possible by transitivity of Ra1?Rb1, Rb1?Wb1, Wb1?Wa1, but 
requires a order of reads different from the original code.


Alex


On 16/07/2013 19:46, Vitaly Davidovich wrote:
>
> Right, my response was only in context of JMM as that's what the 
> question was framed on, I should've made that clearer - as Chris 
> pointed out, scheduling can yield the said outcome.
>
> Sent from my phone
>
> On Jul 16, 2013 2:36 PM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     yes, but b==1 implies a==1, not bb==1 implies aa==1.
>
>     You need to reason from:
>
>     Rb1 implies Wb1 by meaning of read and write operations
>     Wb1 implies Wa1 by program order and volatile semantics
>
>     hence Rb1 implies Wa1.
>
>     A true statement is this:
>
>     Ra1 implies Rb1 by program order and volatile semantics
>     Rb1 implies Wb1 by meaning of read and write operations
>     Wb1 implies Wa1 by program order and volatile semantics
>
>     Which is a subset of the outcomes of the following (means the
>     right order is the following):
>     bb=b;
>     aa=a;
>     assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1
>
>
>     Alex
>
>     On 16/07/2013 18:42, Vitaly Davidovich wrote:
>>
>>     No, that's not a possible outcome.  Volatile reads can't reorder
>>     with each other and volatile stores cannot reorder with each
>>     other.  I think your question was mostly about volatile writes
>>     though - if you read bb=1 then it must be that b=1 executed and
>>     thus a=1 as well.
>>
>>     Sent from my phone
>>
>>     On Jul 16, 2013 1:36 PM, "Alexandr Berdnik"
>>     <alexandr.berdnik at gmail.com <mailto:alexandr.berdnik at gmail.com>>
>>     wrote:
>>
>>         Hi all,
>>         just a quick question here.
>>
>>         volatile int a = 0;
>>         volatile int b = 0;
>>
>>         // thread 1
>>         a = 1;
>>         b = 1;
>>
>>         // thread 2
>>         int aa = a;
>>         int bb = b;
>>
>>         The question is: if aa=0, bb=1 is acceptable? I.e. can
>>         volatile reads be reordered? I have not found any guarantees
>>         for that in JMM.
>>
>>         Thanks,
>>         Alex.
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/3fd54c30/attachment.html>

From valentin.male.kovalenko at gmail.com  Wed Jul 17 05:58:41 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Wed, 17 Jul 2013 13:58:41 +0400
Subject: [concurrency-interest] volatile read reorderings
Message-ID: <CAO-wXw+1SCzO3OCZCGemWYZsAfEimCCK-PkmFZByuM_qMApT0g@mail.gmail.com>

Nitsan,
>>Valentin Kovalenko wrote>> The question is: if aa=0, bb=1 is acceptable?
>>Valentin Kovalenko wrote>> No, it's impossible because of happens-before
relation noted above.
>>Nitsan Wakart wrote>> Wrong.
you are right, that was my huge fail:) Ofcourse it's possible, I've just
incorrectly interpreted the question, while it was quite clearly formulated.
Thanks for the fix!

--
Valentin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/be17cb69/attachment.html>

From zhong.j.yu at gmail.com  Wed Jul 17 11:51:03 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 17 Jul 2013 10:51:03 -0500
Subject: [concurrency-interest] volatile read reorderings
In-Reply-To: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
References: <CAG6hYpbNDAPjkEac2fsUu+4Zk-yqfaXrxK5SLR7Q8swHHw=QhQ@mail.gmail.com>
Message-ID: <CACuKZqH2jGkmN8uptyTuyHAndLK+Ou3Pw1pUOEbYexwbeJXz0Q@mail.gmail.com>

Among volatile actions, all executions are sequentially consistent,
there's no chance to observe any reordering.

On Tue, Jul 16, 2013 at 12:31 PM, Alexandr Berdnik
<alexandr.berdnik at gmail.com> wrote:
> Hi all,
> just a quick question here.
>
> volatile int a = 0;
> volatile int b = 0;
>
> // thread 1
> a = 1;
> b = 1;
>
> // thread 2
> int aa = a;
> int bb = b;
>
> The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
> reordered? I have not found any guarantees for that in JMM.
>
> Thanks,
> Alex.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From chris.w.dennis at gmail.com  Wed Jul 17 12:54:42 2013
From: chris.w.dennis at gmail.com (Chris Dennis)
Date: Wed, 17 Jul 2013 12:54:42 -0400
Subject: [concurrency-interest] A variant of the jsr166e.CHMv8.TreeBin
 bug is back?
In-Reply-To: <51E52C9E.6060008@cs.oswego.edu>
Message-ID: <CE0C440C.30F34%chris.w.dennis@gmail.com>

Would it not make sense to have a test like Alex's that exercises the
various aspects of the tree logic in CHMV8 included in the OpenJDK jtreg
suite?

On 7/16/13 7:21 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

>On 07/16/13 05:14, Galder Zamarre?o wrote:
>> Hi,
>>
>> A few weeks back, Alex Snaps sent an email wrt to a possible treebin
>>bug in
>> jsr166e.CHMv8.TreeBin [1].
>>
>> With revision 1.100 and applying the patches in [2], the test passed.
>>
>> However, running the same test that Alex submitted with revision 1.110
>>of
>> jsr166e.CHMv8 [3] (no patches) fails
>>
>> I'm wondering if this is already a known issue...
>>
>
>Thanks! It wasn't. Almost surely a consequence of having
>too many versions of CHM in flux. I will track it down.
>
>-Doug
>
>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From ariel at weisberg.ws  Thu Jul 18 10:58:52 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Thu, 18 Jul 2013 10:58:52 -0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
 Issue 29
In-Reply-To: <CAB8u1202hV_nAQdCr06kA=vJ_goUP3G_3_oJK8bRY_Z+oUsJBw@mail.gmail.com>
References: <CAB8u1202hV_nAQdCr06kA=vJ_goUP3G_3_oJK8bRY_Z+oUsJBw@mail.gmail.com>
Message-ID: <1374159532.31313.140661257165889.0147FDD8@webmail.messagingengine.com>

Hi,



I can't maintain per connection data structures for most of these use
cases because they are being used to find the connection when a remote
response comes back or to iterate over the entire set of connections
concurrently with connections being concurrently added/removed.



What am I getting from this is that NBHM is not a drop in replacement
for synchronized hash map or CHM. You need to know something about the
keys to safely use it.



Ariel



On Wed, Jul 17, 2013, at 03:02 AM, Sergey Fedoseenkov wrote:

  Hi, didnt't you consider using some data structure for keeping track
  of your NBHMs with connections for each user separately so when some
  user drops off you are able to remove the reference to NBHM from it
  and it will be GCed.



It looks like from the description that new users don't join frequently
so this data structure may be CHM.



--

Sergey

On Wednesday, July 17, 2013, wrote:

Send Concurrency-interest mailing list submissions to

        concurrency-interest at cs.oswego.edu



To subscribe or unsubscribe via the World Wide Web, visit

        [1]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

or, via email, send a message with subject or body 'help' to

        concurrency-interest-request at cs.oswego.edu



You can reach the person managing the list at

        concurrency-interest-owner at cs.oswego.edu



When replying, please edit your Subject line so it is more specific

than "Re: Contents of Concurrency-interest digest..."





Today's Topics:



   1. Re: Concurrency-interest Digest, Vol 102, Issue 29 (Chris Dennis)

   2. Re: volatile read reorderings (Vitaly Davidovich)

   3. Re: Concurrency-interest Digest, Vol 102, Issue 29

      (Ariel Weisberg)

   4. Re: volatile read reorderings (Valentin Kovalenko)

   5. Re: Concurrency-interest Digest, Vol 102, Issue 29

      (Daniel Norberg)





----------------------------------------------------------------------



Message: 1

Date: Tue, 16 Jul 2013 14:36:31 -0400

From: Chris Dennis <chris.w.dennis at gmail.com>

To: Martin Thompson <mjpt777 at gmail.com>, Ariel Weisberg

        <ariel at weisberg.ws>,     <concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol

        102, Issue 29

Message-ID: <CE0B0A30.30D86%chris.w.dennis at gmail.com>

Content-Type: text/plain;       charset="UTF-8"



>From my memory of the code I think you're right.  NonBlockingHashMap
makes

no attempt to shrink its table when the occupancy drops.  Of course CHM

doesn't either, it's just that the effect of this is more pronounced
with

open addressing.  Does anyone know if the new CHMV8 ever attempts to

rehash in to a smaller table? Has anyone ever looked in to the
possibility

of doing this?



Chris



On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:



>It is best to get a heap dump to determine what is using the memory.

>However a quick guess would be that NonBlockingHashMap uses uses

>open-addressing rather than bucket and chain design so you'll have
some

>large arrays for the keys and values.  If you put a lot of items in
the

>set then remove them the backing arrays will still be large having had
to

>grow to accommodate the entries.

>

>Regards,

>Martin...

>

>On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>

>wrote:

>

>Send Concurrency-interest mailing list submissions to

>        concurrency-interest at cs.oswego.edu

>Date: Mon, 15 Jul 2013 16:49:13 -0400

>From: Ariel Weisberg <ariel at weisberg.ws>

>To: concurrency-interest at cs.oswego.edu

>Subject: [concurrency-interest] Unexpected memory usage in

>        NonBlockingHashHashMap

>Message-ID:

>

><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>

>Content-Type: text/plain

>

>Hi all,

>

>I was trying to make use of Cliff Click's NonBlockingHashSet and

>NonBlockingHashMap

>(1.1.2) and found unexpected heap memory usage. When I did a heap dump

>of live objects I found that a lot of resources were being used even

>though size for the data structures reports 0 or some reasonable
number

>of entries.

>

>Attached is a quick demonstration app that creates a set of longs and

>churns through through the set adding and removing entries,
occasionally

>replacing most of the set. It seems like removing and replacing most
of

>the entries triggers the behavior that leaks the most memory.

>

>I am using

>java version "1.6.0_27"

>OpenJDK Runtime Environment (IcedTea6 1.12.5)

>(6b27-1.12.5-0ubuntu0.12.04.1)

>OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

>

>It also reproduced with Sun JDK 1.6.0_39

>

>You can swap in ConcurrentHashMap and it works as expected and
produces

>modest 9 megabyte heap dumps.

>

>Is this an expected behavior for this use case? Is there some caveat I

>should know?

>

>Thanks,

>Ariel

>

>

>

>

>_______________________________________________

>Concurrency-interest mailing list

>Concurrency-interest at cs.oswego.edu

>[2]http://cs.oswego.edu/mailman/listinfo/concurrency-interest









------------------------------



Message: 2

Date: Tue, 16 Jul 2013 14:46:43 -0400

From: Vitaly Davidovich <vitalyd at gmail.com>

To: oleksandr otenko <oleksandr.otenko at oracle.com>

Cc: concurrency-interest at cs.oswego.edu,   Alexandr Berdnik

        <alexandr.berdnik at gmail.com>

Subject: Re: [concurrency-interest] volatile read reorderings

Message-ID:


<CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA at mail.gmail.com>

Content-Type: text/plain; charset="iso-8859-1"



Right, my response was only in context of JMM as that's what the
question

was framed on, I should've made that clearer - as Chris pointed out,

scheduling can yield the said outcome.



Sent from my phone

On Jul 16, 2013 2:36 PM, "Oleksandr Otenko"
<oleksandr.otenko at oracle.com>

wrote:



>  yes, but b==1 implies a==1, not bb==1 implies aa==1.

>

> You need to reason from:

>

> Rb1 implies Wb1 by meaning of read and write operations

> Wb1 implies Wa1 by program order and volatile semantics

>

> hence Rb1 implies Wa1.

>

> A true statement is this:

>

> Ra1 implies Rb1 by program order and volatile semantics

> Rb1 implies Wb1 by meaning of read and write operations

> Wb1 implies Wa1 by program order and volatile semantics

>

> Which is a subset of the outcomes of the following (means the right
order

> is the following):

> bb=b;

> aa=a;

> assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1

>

>

> Alex

>

> On 16/07/2013 18:42, Vitaly Davidovich wrote:

>

> No, that's not a possible outcome.  Volatile reads can't reorder with
each

> other and volatile stores cannot reorder with each other.  I think
your

> question was mostly about volatile writes though - if you read bb=1
then it

> must be that b=1 executed and thus a=1 as well.

>

> Sent from my phone

> On Jul 16, 2013 1:36 PM, "Alexandr Berdnik"
<alexandr.berdnik at gmail.com>

> wrote:

>

>>       Hi all,

>> just a quick question here.

>>

>>  volatile int a = 0;

>>  volatile int b = 0;

>>

>>  // thread 1

>>  a = 1;

>>  b = 1;

>>

>>  // thread 2

>>  int aa = a;

>>  int bb = b;

>>

>>  The question is: if aa=0, bb=1 is acceptable? I.e. can volatile
reads be

>> reordered? I have not found any guarantees for that in JMM.

>>

>>  Thanks,

>>  Alex.

>>

>> _______________________________________________

>> Concurrency-interest mailing list

>> Concurrency-interest at cs.oswego.edu

>> [3]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

>>

>>

>

> _______________________________________________

> Concurrency-interest mailing
listConcurrency-interest at cs.oswego.eduhttp://[4]cs.oswego.edu/mailman/l
istinfo/concurrency-interest

>

>

>

-------------- next part --------------

An HTML attachment was scrubbed...

URL:
<[5]http://cs.oswego.edu/pipermail/concurrency-interest/attachments/201
30716/77af3dee/attachment-0001.html>



------------------------------



Message: 3

Date: Tue, 16 Jul 2013 15:03:04 -0400

From: Ariel Weisberg <ariel at weisberg.ws>

To: Chris Dennis <chris.w.dennis at gmail.com>,    Martin Thompson

        <mjpt777 at gmail.com>, concurrency-interest at cs.oswego.edu

Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol

        102, Issue 29

Message-ID:


<1374001384.21088.140661256390085.573194C5 at webmail.messagingengine.com>



Content-Type: text/plain; charset="us-ascii"



Hi,



With at most 54k entries in the table at any time does it really need
to

allocate millions of slots? It also doesn't help that it keeps

references to removed entries. You can empty the map, take a heap dump,

and still see references to objects that were stored in the map that
are

reachable from the map.



I get that it is a pathological behavior, but it occurs without a lot
of

work. I got hit by it using 3 NBHMs to track connection specific state

with an app that maintains 50-150k connections. I started 3 clients
apps

each creating 50k connections, and 1k connections that cycle and it
will

slowly take up more space and if I restart a client it seems to use
more

memory immediately as it drops and adds 50k connection entries. I can
do

it with just one client.



I think the space utilization is odd period. If I put in a 100k random

numbers it allocates 1 million slots (although I guess you / 2 since

slots are used for keys as well). If I completely remove all values and

then add back 100k more and repeat the process the number of slots

allocated increases indefinitely! And this is from a single thread. It

also pauses for a long time, and if you drop in CHM it works as

expected.



See attached single threaded app grows the map indefinitely as

described. You don't have to run long to watch it choke. You can see

that occasionally it pauses for a long time to grow the number of
slots,

runs fast for a while, and then pauses to rehash. I am removing all

entries every 100k entries.



Thanks,

Ariel



On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:

> From my memory of the code I think you're right.  NonBlockingHashMap

> makes

> no attempt to shrink its table when the occupancy drops.  Of course
CHM

> doesn't either, it's just that the effect of this is more pronounced
with

> open addressing.  Does anyone know if the new CHMV8 ever attempts to

> rehash in to a smaller table? Has anyone ever looked in to the

> possibility

> of doing this?

>

> Chris

>

> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:

>

> >It is best to get a heap dump to determine what is using the memory.

> >However a quick guess would be that NonBlockingHashMap uses uses

> >open-addressing rather than bucket and chain design so you'll have
some

> >large arrays for the keys and values.  If you put a lot of items in
the

> >set then remove them the backing arrays will still be large having
had to

> >grow to accommodate the entries.

> >

> >Regards,

> >Martin...

> >

> >On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>

> >wrote:

> >

> >Send Concurrency-interest mailing list submissions to

> >        concurrency-interest at cs.oswego.edu

> >Date: Mon, 15 Jul 2013 16:49:13 -0400

> >From: Ariel Weisberg <ariel at weisberg.ws>

> >To: concurrency-interest at cs.oswego.edu

> >Subject: [concurrency-interest] Unexpected memory usage in

> >        NonBlockingHashHashMap

> >Message-ID:

> >

>
><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>

> >Content-Type: text/plain

> >

> >Hi all,

> >

> >I was trying to make use of Cliff Click's NonBlockingHashSet and

> >NonBlockingHashMap

> >(1.1.2) and found unexpected heap memory usage. When I did a heap
dump

> >of live objects I found that a lot of resources were being used even

> >though size for the data structures reports 0 or some reasonable
number

> >of entries.

> >

> >Attached is a quick demonstration app that creates a set of longs
and

> >churns through through the set adding and removing entries,
occasionally

> >replacing most of the set. It seems like removing and replacing most
of

> >the entries triggers the behavior that leaks the most memory.

> >

> >I am using

> >java version "1.6.0_27"

> >OpenJDK Runtime Environment (IcedTea6 1.12.5)

> >(6b27-1.12.5-0ubuntu0.12.04.1)

> >OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

> >

> >It also reproduced with Sun JDK 1.6.0_39

> >

> >You can swap in ConcurrentHashMap and it works as expected and
produces

> >modest 9 megabyte heap dumps.

> >

> >Is this an expected behavior for this use case? Is there some caveat
I

> >should know?

> >

> >Thanks,

> >Ariel

> >

> >

> >

> >

> >_______________________________________________

> >Concurrency-interest mailing list

> >Concurrency-interest at cs.oswego.edu

> >[6]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

>

>

-------------- next part --------------

A non-text attachment was scrubbed...

Name: HammerSet.java

Type: text/x-java

Size: 1007 bytes

Desc: not available

URL:
<[7]http://cs.oswego.edu/pipermail/concurrency-interest/attachments/201
30716/e659aa19/attachment-0001.bin>



------------------------------



Message: 4

Date: Wed, 17 Jul 2013 00:20:19 +0400

From: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>

To: concurrency-interest at cs.oswego.edu

Subject: Re: [concurrency-interest] volatile read reorderings

Message-ID:


<CAO-wXw+eqkVURBP9u+4cjr_OQBhTswooXUknnfYpC0DqWCb51g at mail.gmail.com>

Content-Type: text/plain; charset="utf-8"



>>can volatile reads be reordered?

>>I have not found any guarantees for that in JMM.

Check this:

[8]http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.
4.5

there is a statement "A write to a volatile field happens-before every

subsequent read of that field."



>>The question is: if aa=0, bb=1 is acceptable?

No, it's impossible because of happens-before relation noted above.



>>can volatile reads be reordered?

You do_not_need_to_care_about_reorderings if you are just a Java
developer.

All you need is to be sure that there are no data races (conflicting

accesses that are not ordered by a happens-before relationship) in your

program. This is no just my opinion, JLS states this in "17.4.5.

Happens-before Order paragraph" (see "A program is correctly
synchronized

if and only if all sequentially consistent executions are free of data

races..." and and further)



--

Valentin

-------------- next part --------------

An HTML attachment was scrubbed...

URL:
<[9]http://cs.oswego.edu/pipermail/concurrency-interest/attachments/201
30717/eec162c2/attachment-0001.html>



------------------------------



Message: 5

Date: Wed, 17 Jul 2013 00:56:06 +0200

From: Daniel Norberg <daniel.norberg at gmail.com>

To: Ariel Weisberg <ariel at weisberg.ws>

Cc: Martin Thompson <mjpt777 at gmail.com>,

        "concurrency-interest at cs.oswego.edu"

        <concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol

        102,    Issue 29

Message-ID: <5FAF8E27-40B8-4207-B619-2ED4F75F1507 at gmail.com>

Content-Type: text/plain; charset="us-ascii"



Hi,



I encountered this issue with NonBlockingHashMap last year as well. It
indeed seems to be due to how NBHM keeps tombstones for removed entries
and keeps doubling the size of the backing array when it becomes full,
even if it is just contains tombstones.



[10]http://sourceforge.net/p/high-scale-lib/bugs/14/



NBHM seems to work well when the set of observed keys is bounded but
explodes in the face of a high churn of unique keys. I recommend using
CHM instead.



Cheers,

Daniel



On 16 jul 2013, at 21:03, Ariel Weisberg <ariel at weisberg.ws> wrote:



> Hi,

>

> With at most 54k entries in the table at any time does it really need
to

> allocate millions of slots? It also doesn't help that it keeps

> references to removed entries. You can empty the map, take a heap
dump,

> and still see references to objects that were stored in the map that
are

> reachable from the map.

>

> I get that it is a pathological behavior, but it occurs without a lot
of

> work. I got hit by it using 3 NBHMs to track connection specific
state

> with an app that maintains 50-150k connections. I started 3 clients
apps

> each creating 50k connections, and 1k connections that cycle and it
will

> slowly take up more space and if I restart a client it seems to use
more

> memory immediately as it drops and adds 50k connection entries. I can
do

> it with just one client.

>

> I think the space utilization is odd period. If I put in a 100k
random

> numbers it allocates 1 million slots (although I guess you / 2 since

> slots are used for keys as well). If I completely remove all values
and

> then add back 100k more and repeat the process the number of slots

> allocated increases indefinitely! And this is from a single thread.
It

> also pauses for a long time, and if you drop in CHM it works as

> expected.

>

> See attached single threaded app grows the map indefinitely as

> described. You don't have to run long to watch it choke. You can see

> that occasionally it pauses for a long time to grow the number of
slots,

> runs fast for a while, and then pauses to rehash. I am removing all

> entries every 100k entries.

>

> Thanks,

> Ariel

>

> On Tue, Jul 16, 2013, at 02:36 PM, Chris Dennis wrote:

>> From my memory of the code I think you're right.  NonBlockingHashMap

>> makes

>> no attempt to shrink its table when the occupancy drops.  Of course
CHM

>> doesn't either, it's just that the effect of this is more pronounced
with

>> open addressing.  Does anyone know if the new CHMV8 ever attempts to

>> rehash in to a smaller table? Has anyone ever looked in to the

>> possibility

>> of doing this?

>>

>> Chris

>>

>> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:

>>

>>> It is best to get a heap dump to determine what is using the
memory.

>>> However a quick guess would be that NonBlockingHashMap uses uses

>>> open-addressing rather than bucket and chain design so you'll have
some

>>> large arrays for the keys and values.  If you put a lot of items in
the

>>> set then remove them the backing arrays will still be large having
had to

>>> grow to accommodate the entries.

>>>

>>> Regards,

>>> Martin...

>>>

>>> On 16 July 2013 17:00,
<concurrency-interest-request at cs.oswego.edu>

>>> wrote:

>>>

>>> Send Concurrency-interest mailing list submissions to

>>>       concurrency-interest at cs.oswego.edu

>>> Date: Mon, 15 Jul 2013 16:49:13 -0400

>>> From: Ariel Weisberg <ariel at weisberg.ws>

>>> To: concurrency-interest at cs.oswego.edu

>>> Subject: [concurrency-interest] Unexpected memory usage in

>>>       NonBlockingHashHashMap

>>> Message-ID:

>>>

>>>
<1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>

>>> Content-Type: text/plain

>>>

>>> Hi all,

>>>

>>> I was trying to make use of Cliff Click's NonBlockingHashSet and

>>> NonBlockingHashMap

>>> (1.1.2) and found unexpected heap memory usage. When I did a heap
dump

>>> of live objects I found that a lot of resources were being used
even

>>> though size for the data structures reports 0 or some reasonable
number

>>> of entries.

>>>

>>> Attached is a quick demonstration app that creates a set of longs
and

>>> churns through through the set adding and removing entries,
occasionally

>>> replacing most of the set. It seems like removing and replacing
most of

>>> the entries triggers the behavior that leaks the most memory.

>>>

>>> I am using

>>> java version "1.6.0_27"

>>> OpenJDK Runtime Environment (IcedTea6 1.12.5)

>>> (6b27-1.12.5-0ubuntu0.12.04.1)

>>> OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

>>>

>>> It also reproduced with Sun JDK 1.6.0_39

>>>

>>> You can swap in ConcurrentHashMap and it works as expected and
produces

>>> modest 9 megabyte heap dumps.

>>>

>>> Is this an expected behavior for this use case? Is there some
caveat I

>>> should know?

>>>

>>> Thanks,

>>> Ariel

>>>

>>>

>>>

>>>

>>> _______________________________________________

>>> Concurrency-interest mailing list

>>> Concurrency-interest at cs.oswego.edu

>>> [11]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

> <HammerSet.java>

> _______________________________________________

> Concurrency-interest mailing list

> Concurrency-interest at cs.oswego.edu

> [12]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------

An HTML attachment was scrubbed...

URL:
<[13]http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20
130717/c34ebf47/attachment.html>



------------------------------



_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu

[14]http://cs.oswego.edu/mailman/listinfo/concurrency-interest





End of Concurrency-interest Digest, Vol 102, Issue 31

*****************************************************



_______________________________________________

Concurrency-interest mailing list

[15]Concurrency-interest at cs.oswego.edu

[16]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
2. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
3. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
4. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
5. http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/77af3dee/attachment-0001.html
6. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
7. http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/e659aa19/attachment-0001.bin
8. http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5
9. http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/eec162c2/attachment-0001.html
  10. http://sourceforge.net/p/high-scale-lib/bugs/14/
  11. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  12. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  13. http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130717/c34ebf47/attachment.html
  14. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
  15. mailto:Concurrency-interest at cs.oswego.edu
  16. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130718/fc0c4819/attachment-0001.html>

From mr.chrisvest at gmail.com  Thu Jul 18 12:38:48 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Thu, 18 Jul 2013 18:38:48 +0200
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 102,
	Issue 29
In-Reply-To: <1374159532.31313.140661257165889.0147FDD8@webmail.messagingengine.com>
References: <CAB8u1202hV_nAQdCr06kA=vJ_goUP3G_3_oJK8bRY_Z+oUsJBw@mail.gmail.com>
	<1374159532.31313.140661257165889.0147FDD8@webmail.messagingengine.com>
Message-ID: <E1EB00BE-2A9D-496A-B7F2-670373CD9521@gmail.com>

Have you tried out the Javolution FastMap?

I haven't myself, but I'd be interested in hearing about people's experience with it.

Chris

On 18/07/2013, at 16.58, Ariel Weisberg <ariel at weisberg.ws> wrote:

> Hi,
>  
> I can't maintain per connection data structures for most of these use cases because they are being used to find the connection when a remote response comes back or to iterate over the entire set of connections concurrently with connections being concurrently added/removed.
>  
> What am I getting from this is that NBHM is not a drop in replacement for synchronized hash map or CHM. You need to know something about the keys to safely use it.
>  
> Ariel
>  
> On Wed, Jul 17, 2013, at 03:02 AM, Sergey Fedoseenkov wrote:
>> Hi, didnt't you consider using some data structure for keeping track of your NBHMs with connections for each user separately so when some user drops off you are able to remove the reference to NBHM from it and it will be GCed.
>>  
>> It looks like from the description that new users don't join frequently so this data structure may be CHM.
>>  
>> --
>> Sergey
>>  
>> On Wednesday, July 17, 2013, wrote:
>>  
>> Send Concurrency-interest mailing list submissions to
>>         concurrency-interest at cs.oswego.edu
>>  
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> or, via email, send a message with subject or body 'help' to
>>         concurrency-interest-request at cs.oswego.edu
>>  
>> You can reach the person managing the list at
>>         concurrency-interest-owner at cs.oswego.edu
>>  
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of Concurrency-interest digest..."
>>  
>>  
>> Today's Topics:
>>  
>>    1. Re: Concurrency-interest Digest, Vol 102, Issue 29 (Chris Dennis)
>>    2. Re: volatile read reorderings (Vitaly Davidovich)
>>    3. Re: Concurrency-interest Digest, Vol 102, Issue 29
>>       (Ariel Weisberg)
>>    4. Re: volatile read reorderings (Valentin Kovalenko)
>>    5. Re: Concurrency-interest Digest, Vol 102, Issue 29
>>       (Daniel Norberg)
>>  
>>  
>> ----------------------------------------------------------------------
>>  
>> Message: 1
>> Date: Tue, 16 Jul 2013 14:36:31 -0400
>> From: Chris Dennis <chris.w.dennis at gmail.com>
>> To: Martin Thompson <mjpt777 at gmail.com>, Ariel Weisberg
>>         <ariel at weisberg.ws>,     <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>>         102, Issue 29
>> Message-ID: <CE0B0A30.30D86%chris.w.dennis at gmail.com>
>> Content-Type: text/plain;       charset="UTF-8"
>>  
>> >From my memory of the code I think you're right.  NonBlockingHashMap makes
>> no attempt to shrink its table when the occupancy drops.  Of course CHM
>> doesn't either, it's just that the effect of this is more pronounced with
>> open addressing.  Does anyone know if the new CHMV8 ever attempts to
>> rehash in to a smaller table? Has anyone ever looked in to the possibility
>> of doing this?
>>  
>> Chris
>>  
>> On 7/16/13 12:13 PM, "Martin Thompson" <mjpt777 at gmail.com> wrote:
>>  
>> >It is best to get a heap dump to determine what is using the memory.
>> >However a quick guess would be that NonBlockingHashMap uses uses
>> >open-addressing rather than bucket and chain design so you'll have some
>> >large arrays for the keys and values.  If you put a lot of items in the
>> >set then remove them the backing arrays will still be large having had to
>> >grow to accommodate the entries.
>> >
>> >Regards,
>> >Martin...
>> >
>> >On 16 July 2013 17:00,  <concurrency-interest-request at cs.oswego.edu>
>> >wrote:
>> >
>> >Send Concurrency-interest mailing list submissions to
>> >        concurrency-interest at cs.oswego.edu
>> >Date: Mon, 15 Jul 2013 16:49:13 -0400
>> >From: Ariel Weisberg <ariel at weisberg.ws>
>> >To: concurrency-interest at cs.oswego.edu
>> >Subject: [concurrency-interest] Unexpected memory usage in
>> >        NonBlockingHashHashMap
>> >Message-ID:
>> >
>> ><1373921353.5093.140661255996697.21E5240D at webmail.messagingengine.com>
>> >Content-Type: text/plain
>> >
>> >Hi all,
>> >
>> >I was trying to make use of Cliff Click's NonBlockingHashSet and
>> >NonBlockingHashMap
>> >(1.1.2) and found unexpected heap memory usage. When I did a heap dump
>> >of live objects I found that a lot of resources were being used even
>> >though size for the data structures reports 0 or some reasonable number
>> >of entries.
>> >
>> >Attached is a quick demonstration app that creates a set of longs and
>> >churns through through the set adding and removing entries, occasionally
>> >replacing most of the set. It seems like removing and replacing most of
>> >the entries triggers the behavior that leaks the most memory.
>> >
>> >I am using
>> >java version "1.6.0_27"
>> >OpenJDK Runtime Environment (IcedTea6 1.12.5)
>> >(6b27-1.12.5-0ubuntu0.12.04.1)
>> >OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
>> >
>> >It also reproduced with Sun JDK 1.6.0_39
>> >
>> >You can swap in ConcurrentHashMap and it works as expected and produces
>> >modest 9 megabyte heap dumps.
>> >
>> >Is this an expected behavior for this use case? Is there some caveat I
>> >should know?
>> >
>> >Thanks,
>> >Ariel
>> >
>> >
>> >
>> >
>> >_______________________________________________
>> >Concurrency-interest mailing list
>> >Concurrency-interest at cs.oswego.edu
>> >http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>  
>>  
>>  
>>  
>> ------------------------------
>>  
>> Message: 2
>> Date: Tue, 16 Jul 2013 14:46:43 -0400
>> From: Vitaly Davidovich <vitalyd at gmail.com>
>> To: oleksandr otenko <oleksandr.otenko at oracle.com>
>> Cc: concurrency-interest at cs.oswego.edu,   Alexandr Berdnik
>>         <alexandr.berdnik at gmail.com>
>> Subject: Re: [concurrency-interest] volatile read reorderings
>> Message-ID:
>>         <CAHjP37FScqWZJKDbtX7vy099G7stiJQ+5HYa-2nm654h-TBqDA at mail.gmail.com>
>> Content-Type: text/plain; charset="iso-8859-1"
>>  
>> Right, my response was only in context of JMM as that's what the question
>> was framed on, I should've made that clearer - as Chris pointed out,
>> scheduling can yield the said outcome.
>>  
>> Sent from my phone
>> On Jul 16, 2013 2:36 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>  
>> >  yes, but b==1 implies a==1, not bb==1 implies aa==1.
>> >
>> > You need to reason from:
>> >
>> > Rb1 implies Wb1 by meaning of read and write operations
>> > Wb1 implies Wa1 by program order and volatile semantics
>> >
>> > hence Rb1 implies Wa1.
>> >
>> > A true statement is this:
>> >
>> > Ra1 implies Rb1 by program order and volatile semantics
>> > Rb1 implies Wb1 by meaning of read and write operations
>> > Wb1 implies Wa1 by program order and volatile semantics
>> >
>> > Which is a subset of the outcomes of the following (means the right order
>> > is the following):
>> > bb=b;
>> > aa=a;
>> > assert aa==1 || bb!=1; // which is the same as bb==1 implies aa==1
>> >
>> >
>> > Alex
>> >
>> > On 16/07/2013 18:42, Vitaly Davidovich wrote:
>> >
>> > No, that's not a possible outcome.  Volatile reads can't reorder with each
>> > other and volatile stores cannot reorder with each other.  I think your
>> > question was mostly about volatile writes though - if you read bb=1 then it
>> > must be that b=1 executed and thus a=1 as well.
>> >
>> > Sent from my phone
>> > On Jul 16, 2013 1:36 PM, "Alexandr Berdnik" <alexandr.berdnik at gmail.com>
>> > wrote:
>> >
>> >>       Hi all,
>> >> just a quick question here.
>> >>
>> >>  volatile int a = 0;
>> >>  volatile int b = 0;
>> >>
>> >>  // thread 1
>> >>  a = 1;
>> >>  b = 1;
>> >>
>> >>  // thread 2
>> >>  int aa = a;
>> >>  int bb = b;
>> >>
>> >>  The question is: if aa=0, bb=1 is acceptable? I.e. can volatile reads be
>> >> reordered? I have not found any guarantees for that in JMM.
>> >>
>> >>  Thanks,
>> >>  Alex.
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> >
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130716/77af3dee/attachment-0001.html>
>>  
>> ------------------------------
>>  
>> Message: 3
>> Date: Tue, 16 Jul 2013 15:03:04 -0400
>> From: Ariel Weisberg <ariel at weisberg.ws>
>> To: Chris Dennis <chris.w.dennis at gmail.com>,    Martin Thompson
>>         <mjpt777 at gmail.com>, concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Concurrency-interest Digest, Vol
>>         102, Issue 29
>> Message-ID:
>>         <1374001384.21088.140661256390085.573194C5 at webmail.messagingengine.com>
>>  
>> Content-Type: text/plain; charset="us-ascii"
>>  
>> Hi,
>>  
>> With at most 54k entries in the table at any time does it really need to
>> allocate millions of slots? It also doesn't help that it keeps
>> references to removed entries. You can empty the map, take a heap dump,
>> and still see references to objects that were stored in the map that are
>> reachable from the map.
>>  
>> I get that it is a pathological behavior, but it occurs without a lot of
>> work. I got hit by it using 3 NBHMs to track connection specific state
>> with an app that maintains 50-150k connections. I started 3 clients apps
>> each creating 50k connections, and 1k connections that cycle and it will
>> slowly take up more space and if I restart a client it seems to use more
>> memory immediately as it drops and adds 50k connection entries. I can do
>> it with just one client.
>>  
>> I think the space utilization is odd period. If I put in a 100k random
>> numbers it allocates 1 million slots (although I guess you / 2 since
>> slots are used for keys as well). If I completely remove all values and
>> then add back 100k more and repeat the process the number of slots
>> allocated increases indefinitely! And this is from a single thread. It
>> also pauses for a long time, and if you drop in CHM it works as
>> expected.
>>  
>> See attached single threaded app grows the map indefinitely as
>> described. You don't have to run long to watch it choke. You can see
>> that occasionally it pauses for a long time to grow the number of slots,
>> runs fast for a while, and then pauses to rehash. I am removing all
>> e
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130718/917522d4/attachment-0001.html>

From niall at npgall.com  Fri Jul 19 07:21:43 2013
From: niall at npgall.com (Niall Gallagher)
Date: Fri, 19 Jul 2013 12:21:43 +0100
Subject: [concurrency-interest] Read-write-update lock
Message-ID: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>

Hi,

I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.

Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.

I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].

I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?

The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?

Some differences between my implementation and the Linux kernel mailing list discussion: 
- My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
- I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).

There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).

For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?

Best regards,

Niall Gallagher
www.npgall.com

[1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
    http://code.google.com/p/concurrent-locks/

[2] Linux kernel mailing list: Read/Write locks that can be changed into each other
    http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html

[3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
    http://code.google.com/p/concurrent-trees/



From mjpt777 at gmail.com  Fri Jul 19 08:39:01 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Fri, 19 Jul 2013 13:39:01 +0100
Subject: [concurrency-interest] Atomics for ByteBuffer
Message-ID: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>

One of the most common uses I see for Unsafe is building IPC
implementations.  If we want folk to stop using Unsafe then we need to give
them an alternative.

Is there any reason why we cannot add atomic methods like putOrderedX,
putVolatileX, getVolatileX, getAndAddX, compareAndSetX to ByteBuffer?

With these new methods an IPC implementation can be built using memory
mapped files without resorting to using Unsafe.

Regards,
Martin...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/d41bad7b/attachment.html>

From mr.chrisvest at gmail.com  Fri Jul 19 08:43:05 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Fri, 19 Jul 2013 14:43:05 +0200
Subject: [concurrency-interest] Monotonic time
Message-ID: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>

Hi,

Is System.nanoTime safe to use across threads/cores? Say, if I do a read on
one core, does it preserve its monotonic properties and a
reasonable accuracy when compared with a read on another core?

I'm working on an object pool that by default expires objects after they
have been live for about 10 minutes.

To implement this, I have a timestamp for when I created them, and then
regularly (every time I want to borrow an object from the pool) measure the
current time and calculate their age.

I currently use System.currentTimeMillis to measure time. This is a
problem, however, because the system clock can be adjusted backwards: in
places that observe daylight savings time, for instance, objects might
suddenly find themselves with a creation timestamp one hour in the future.

Thinking about how to fix this, I am lead to my opening question.

Cheers,
Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/21edbc6f/attachment.html>

From vitalyd at gmail.com  Fri Jul 19 08:56:43 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 19 Jul 2013 08:56:43 -0400
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
References: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
Message-ID: <CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>

Hi Chris,

System.currentTimeMillis() can be adjusted (e.g.NTP, system clock) but not
for daylight savings time - it's # of millis in UTC which doesn't have
DST.  If you were to get wall clock time in some timezone that does have
DST, you'd see adjustments but the millis would stay the same.  For your 10
min granularity, currentTimeMillis is fine, IMHO.

System.nanoTime tries to use the high precision time source on the system,
and it's monotonic.  As for whether it's monotonic across cores I think
depends on the underlying OS and CPU facilities.  Typically you use it for
high precision timing on single core though (at least in my experience).

Sent from my phone
On Jul 19, 2013 8:48 AM, "Chris Vest" <mr.chrisvest at gmail.com> wrote:

> Hi,
>
> Is System.nanoTime safe to use across threads/cores? Say, if I do a read
> on one core, does it preserve its monotonic properties and a
> reasonable accuracy when compared with a read on another core?
>
> I'm working on an object pool that by default expires objects after they
> have been live for about 10 minutes.
>
> To implement this, I have a timestamp for when I created them, and then
> regularly (every time I want to borrow an object from the pool) measure the
> current time and calculate their age.
>
> I currently use System.currentTimeMillis to measure time. This is a
> problem, however, because the system clock can be adjusted backwards: in
> places that observe daylight savings time, for instance, objects might
> suddenly find themselves with a creation timestamp one hour in the future.
>
> Thinking about how to fix this, I am lead to my opening question.
>
> Cheers,
> Chris
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/df48af3e/attachment.html>

From vitalyd at gmail.com  Fri Jul 19 09:03:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 19 Jul 2013 09:03:13 -0400
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
References: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
Message-ID: <CAHjP37HkFcpN+8dBZJdK6jVsUw4u78o0-=uj53gz2PvgN3qZpQ@mail.gmail.com>

ByteBuffer is not spec'd to be threadsafe, IIRC.  Adding these methods
there would lead to confusion, I think, if they remain spec'd as such.
Maybe a separate impl can be created to support atomics?

Sent from my phone
On Jul 19, 2013 8:46 AM, "Martin Thompson" <mjpt777 at gmail.com> wrote:

> One of the most common uses I see for Unsafe is building IPC
> implementations.  If we want folk to stop using Unsafe then we need to give
> them an alternative.
>
> Is there any reason why we cannot add atomic methods like putOrderedX,
> putVolatileX, getVolatileX, getAndAddX, compareAndSetX to ByteBuffer?
>
> With these new methods an IPC implementation can be built using memory
> mapped files without resorting to using Unsafe.
>
> Regards,
> Martin...
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/c2cf0490/attachment.html>

From mr.chrisvest at gmail.com  Fri Jul 19 10:19:52 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Fri, 19 Jul 2013 16:19:52 +0200
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>
References: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
	<CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>
Message-ID: <CAHXi_0cBu48LqX7u1Wg-Q7sAc7-1FC8rL9ThaYug37r+wUchtg@mail.gmail.com>

Hi Vitaly,

DST was my main concern with its big jumps. Good to know it's not a
problem. I expect NTP adjustments to be very gentle compared to the time
scales most typical for this use case, and I can write off manual
adjustments with a bit of documentation.

Thanks.


On 19 July 2013 14:56, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Hi Chris,
>
> System.currentTimeMillis() can be adjusted (e.g.NTP, system clock) but not
> for daylight savings time - it's # of millis in UTC which doesn't have
> DST.  If you were to get wall clock time in some timezone that does have
> DST, you'd see adjustments but the millis would stay the same.  For your 10
> min granularity, currentTimeMillis is fine, IMHO.
>
> System.nanoTime tries to use the high precision time source on the system,
> and it's monotonic.  As for whether it's monotonic across cores I think
> depends on the underlying OS and CPU facilities.  Typically you use it for
> high precision timing on single core though (at least in my experience).
>
> Sent from my phone
> On Jul 19, 2013 8:48 AM, "Chris Vest" <mr.chrisvest at gmail.com> wrote:
>
>> Hi,
>>
>> Is System.nanoTime safe to use across threads/cores? Say, if I do a read
>> on one core, does it preserve its monotonic properties and a
>> reasonable accuracy when compared with a read on another core?
>>
>> I'm working on an object pool that by default expires objects after they
>> have been live for about 10 minutes.
>>
>> To implement this, I have a timestamp for when I created them, and then
>> regularly (every time I want to borrow an object from the pool) measure the
>> current time and calculate their age.
>>
>> I currently use System.currentTimeMillis to measure time. This is a
>> problem, however, because the system clock can be adjusted backwards: in
>> places that observe daylight savings time, for instance, objects might
>> suddenly find themselves with a creation timestamp one hour in the future.
>>
>> Thinking about how to fix this, I am lead to my opening question.
>>
>> Cheers,
>> Chris
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/3292ff3c/attachment.html>

From ariel at weisberg.ws  Fri Jul 19 11:32:57 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Fri, 19 Jul 2013 11:32:57 -0400
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>
References: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
	<CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>
Message-ID: <1374247977.8532.140661257585313.06D2D9B5@webmail.messagingengine.com>

Hi,



In my experience System.nanoTime is not always monotonic. I believe I
saw it on AMD. This has come up before. Pretty much any time source
needs to be checked for negative movement and the error handled. For
things that just need time to move forward add an offset to make up for
the backwards movement. For performance measurements I drop the sample
or assume it took the average amount of time.



Ariel

On Fri, Jul 19, 2013, at 08:56 AM, Vitaly Davidovich wrote:

  Hi Chris,

  System.currentTimeMillis() can be adjusted (e.g.NTP, system clock)
  but not for daylight savings time - it's # of millis in UTC which
  doesn't have DST.  If you were to get wall clock time in some
  timezone that does have DST, you'd see adjustments but the millis
  would stay the same.  For your 10 min granularity, currentTimeMillis
  is fine, IMHO.

  System.nanoTime tries to use the high precision time source on the
  system, and it's monotonic.  As for whether it's monotonic across
  cores I think depends on the underlying OS and CPU facilities.
  Typically you use it for high precision timing on single core though
  (at least in my experience).

  Sent from my phone

On Jul 19, 2013 8:48 AM, "Chris Vest" <[1]mr.chrisvest at gmail.com>
wrote:

Hi,

Is System.nanoTime safe to use across threads/cores? Say, if I do a
read on one core, does it preserve its monotonic properties and a
reasonable accuracy when compared with a read on another core?


I'm working on an object pool that by default expires objects after
they have been live for about 10 minutes.

To implement this, I have a timestamp for when I created them, and then
regularly (every time I want to borrow an object from the pool) measure
the current time and calculate their age.

I currently use System.currentTimeMillis to measure time. This is a
problem, however, because the system clock can be adjusted backwards:
in places that observe daylight savings time, for instance, objects
might suddenly find themselves with a creation timestamp one hour in
the future.

Thinking about how to fix this, I am lead to my opening question.

Cheers,
Chris



_______________________________________________

Concurrency-interest mailing list

[2]Concurrency-interest at cs.oswego.edu

[3]http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________

Concurrency-interest mailing list

[4]Concurrency-interest at cs.oswego.edu

[5]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. mailto:mr.chrisvest at gmail.com
2. mailto:Concurrency-interest at cs.oswego.edu
3. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
4. mailto:Concurrency-interest at cs.oswego.edu
5. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/697b91e0/attachment.html>

From nathan.reynolds at oracle.com  Fri Jul 19 13:27:46 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 19 Jul 2013 10:27:46 -0700
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
Message-ID: <51E97712.2010306@oracle.com>

I ran into upgrading a lock problem a while back.  The solution I took 
is first have the thread read acquire the lock.  Then if the thread 
detects that it needs the write lock, it releases the read lock and 
starts over by acquiring the write lock.  Of course, all of the data the 
thread collected in the read lock is considered invalid.  This greatly 
improved scalability and the "upgrade" didn't hurt performance.  This is 
because  writes mostly happen at start up and are rare otherwise.

The above logic works great and there hasn't been a need for a 
read-write-update lock.  The problem in my situation is that I can't 
predict at the time of acquiring the lock if I will need to do any 
writes.  The thread has to check the protected state and then decide if 
any writes are necessary.  (I would guess this scenario is true of most 
caches.)  If all threads do update acquires, then I am back to the same 
contention I had with all threads doing write acquires.

I haven't needed this kind of lock.  Every time I run into lock 
contention, I always solve the problem without it.  If I had such a lock 
in my toolbox, maybe I would select it.

Can you give a good example of where update acquires can actually help?  
A toy example is one method which always needs to do reads and another 
method which has to do a lot of reads with a very small portion to do 
writes.  The first method is called very frequently relative to the 
second method.  Is there such a situation in software?

-Nathan

On 7/19/2013 4:21 AM, Niall Gallagher wrote:
> Hi,
>
> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>
> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>
> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>
> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>
> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>
> Some differences between my implementation and the Linux kernel mailing list discussion:
> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>
> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>
> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>
> Best regards,
>
> Niall Gallagher
> www.npgall.com
>
> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>      http://code.google.com/p/concurrent-locks/
>
> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>      http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>
> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>      http://code.google.com/p/concurrent-trees/
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/a4092135/attachment-0001.html>

From nathan.reynolds at oracle.com  Fri Jul 19 13:36:52 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 19 Jul 2013 10:36:52 -0700
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <CAHjP37HkFcpN+8dBZJdK6jVsUw4u78o0-=uj53gz2PvgN3qZpQ@mail.gmail.com>
References: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
	<CAHjP37HkFcpN+8dBZJdK6jVsUw4u78o0-=uj53gz2PvgN3qZpQ@mail.gmail.com>
Message-ID: <51E97934.40709@oracle.com>

I see a need for ByteBuffer with atomics (say AtomicByteBuffer).  
Infiniband supports atomic operations over the network.  If Infiniband 
atomics were linked to processor atomics and the program uses 
AtomicByteBuffer, then 2 processes on different machines could execute 
atomic operations on the same piece of memory.

One problem is that the atomic operations can't straddle 2 cache lines.  
If alignment is required, that problem would be solved.

-Nathan

On 7/19/2013 6:03 AM, Vitaly Davidovich wrote:
>
> ByteBuffer is not spec'd to be threadsafe, IIRC. Adding these methods 
> there would lead to confusion, I think, if they remain spec'd as 
> such.  Maybe a separate impl can be created to support atomics?
>
> Sent from my phone
>
> On Jul 19, 2013 8:46 AM, "Martin Thompson" <mjpt777 at gmail.com 
> <mailto:mjpt777 at gmail.com>> wrote:
>
>     One of the most common uses I see for Unsafe is building IPC
>     implementations.  If we want folk to stop using Unsafe then we
>     need to give them an alternative.
>
>     Is there any reason why we cannot add atomic methods like
>     putOrderedX, putVolatileX, getVolatileX, getAndAddX,
>     compareAndSetX to ByteBuffer?
>
>     With these new methods an IPC implementation can be built using
>     memory mapped files without resorting to using Unsafe.
>
>     Regards,
>     Martin...
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/5760238a/attachment.html>

From kirk at kodewerk.com  Fri Jul 19 13:40:48 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Fri, 19 Jul 2013 19:40:48 +0200
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
References: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
Message-ID: <E9C1C2C7-F169-489E-820F-248DA8A36229@kodewerk.com>


On 2013-07-19, at 2:39 PM, Martin Thompson <mjpt777 at gmail.com> wrote:

> One of the most common uses I see for Unsafe is building IPC implementations.  If we want folk to stop using Unsafe then we need to give them an alternative.
> 
> Is there any reason why we cannot add atomic methods like putOrderedX, putVolatileX, getVolatileX, getAndAddX, compareAndSetX to ByteBuffer?

No

> 
> With these new methods an IPC implementation can be built using memory mapped files without resorting to using Unsafe.

+1

Kirk



From nathan.reynolds at oracle.com  Fri Jul 19 13:42:31 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 19 Jul 2013 10:42:31 -0700
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <1374247977.8532.140661257585313.06D2D9B5@webmail.messagingengine.com>
References: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
	<CAHjP37FAyTdnG4jXn1++Pmi0X4JKOBSXNtmfV4vze6Odafew9Q@mail.gmail.com>
	<1374247977.8532.140661257585313.06D2D9B5@webmail.messagingengine.com>
Message-ID: <51E97A87.1040709@oracle.com>

You might want to check out these two blog posts.  It is amazing 
software can do anything with time. Good luck.

http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time
http://infiniteundo.com/post/25509354022/more-falsehoods-programmers-believe-about-time-wisdom

-Nathan

On 7/19/2013 8:32 AM, Ariel Weisberg wrote:
> Hi,
> In my experience System.nanoTime is not always monotonic. I believe I 
> saw it on AMD. This has come up before. Pretty much any time source 
> needs to be checked for negative movement and the error handled. For 
> things that just need time to move forward add an offset to make up 
> for the backwards movement. For performance measurements I drop the 
> sample or assume it took the average amount of time.
> Ariel
> On Fri, Jul 19, 2013, at 08:56 AM, Vitaly Davidovich wrote:
>>
>> Hi Chris,
>>
>> System.currentTimeMillis() can be adjusted (e.g.NTP, system clock) 
>> but not for daylight savings time - it's # of millis in UTC which 
>> doesn't have DST.  If you were to get wall clock time in some 
>> timezone that does have DST, you'd see adjustments but the millis 
>> would stay the same.  For your 10 min granularity, currentTimeMillis 
>> is fine, IMHO.
>>
>> System.nanoTime tries to use the high precision time source on the 
>> system, and it's monotonic.  As for whether it's monotonic across 
>> cores I think depends on the underlying OS and CPU facilities.  
>> Typically you use it for high precision timing on single core though 
>> (at least in my experience).
>>
>> Sent from my phone
>>
>> On Jul 19, 2013 8:48 AM, "Chris Vest" <mr.chrisvest at gmail.com 
>> <mailto:mr.chrisvest at gmail.com>> wrote:
>>
>>     Hi,
>>     Is System.nanoTime safe to use across threads/cores? Say, if I do
>>     a read on one core, does it preserve its monotonic properties and
>>     a reasonable accuracy when compared with a read on another core?
>>     I'm working on an object pool that by default expires objects
>>     after they have been live for about 10 minutes.
>>     To implement this, I have a timestamp for when I created them,
>>     and then regularly (every time I want to borrow an object from
>>     the pool) measure the current time and calculate their age.
>>     I currently use System.currentTimeMillis to measure time. This is
>>     a problem, however, because the system clock can be adjusted
>>     backwards: in places that observe daylight savings time, for
>>     instance, objects might suddenly find themselves with a creation
>>     timestamp one hour in the future.
>>     Thinking about how to fix this, I am lead to my opening question.
>>     Cheers,
>>     Chris
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _________________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130719/ea902141/attachment.html>

From tmont at nard.net  Fri Jul 19 14:45:38 2013
From: tmont at nard.net (Todd L. Montgomery)
Date: Fri, 19 Jul 2013 11:45:38 -0700
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
References: <CAChYfd8PdCyRoS8PybzEf8WS1FF=o9JgVJCbWQUVXw7OJS56eQ@mail.gmail.com>
Message-ID: <341BF82C-8AF6-43BB-A367-F134FE0D5141@nard.net>

This would be awesome!

-- Todd L. Montgomery
Sent from my iPad

On Jul 19, 2013, at 5:39, Martin Thompson <mjpt777 at gmail.com> wrote:

> One of the most common uses I see for Unsafe is building IPC implementations.  If we want folk to stop using Unsafe then we need to give them an alternative.
> 
> Is there any reason why we cannot add atomic methods like putOrderedX, putVolatileX, getVolatileX, getAndAddX, compareAndSetX to ByteBuffer?
> 
> With these new methods an IPC implementation can be built using memory mapped files without resorting to using Unsafe.
> 
> Regards,
> Martin...
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Fri Jul 19 17:47:46 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 20 Jul 2013 07:47:46 +1000
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCECJJPAA.davidcholmes@aapt.net.au>

nanoTime() tries to be monotonic but it is dependent on the high resolution time facilities of the OS. If the OS is configured so that it's "monotonic" time source is not actually monotonic then nanoTime is broken too. This can be caused by a number of issues most of which involve improper use of the TSC as the timesource (something you can select yourself depending on the OS). There can also be configuration issues between virtualization environments and the guest OS that break time properties.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Chris Vest
  Sent: Friday, 19 July 2013 10:43 PM
  To: concurrency-interest
  Subject: [concurrency-interest] Monotonic time


  Hi,


  Is System.nanoTime safe to use across threads/cores? Say, if I do a read on one core, does it preserve its monotonic properties and a reasonable accuracy when compared with a read on another core?



  I'm working on an object pool that by default expires objects after they have been live for about 10 minutes.


  To implement this, I have a timestamp for when I created them, and then regularly (every time I want to borrow an object from the pool) measure the current time and calculate their age.


  I currently use System.currentTimeMillis to measure time. This is a problem, however, because the system clock can be adjusted backwards: in places that observe daylight savings time, for instance, objects might suddenly find themselves with a creation timestamp one hour in the future.


  Thinking about how to fix this, I am lead to my opening question.


  Cheers,
  Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130720/8e635cef/attachment.html>

From mr.chrisvest at gmail.com  Sat Jul 20 10:06:02 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Sat, 20 Jul 2013 16:06:02 +0200
Subject: [concurrency-interest] Monotonic time
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCECJJPAA.davidcholmes@aapt.net.au>
References: <CAHXi_0di1X3=1HNsFvfcvKK9PtzRS4z65VtYHBhA=LLW8wSxKg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCECJJPAA.davidcholmes@aapt.net.au>
Message-ID: <CAHXi_0dyPBMJz3S1W2ROiPjnty7BW55k2y_D-3+HioL-pMPNmw@mail.gmail.com>

It sounds like there's no right answer, though currentTimeMillis appear to
come with marginally fewer evils. (Thanks for those links, Nathan.)

A timestamp for when they were created, and a count of how many times they
have been borrowed, are unfortunately the only things I can base a generic
default expiration policy on. Good thing I made it possible for people to
plug in their own logic, with awareness of what it means for their
particular objects to be valid.

Cheers,
Chris



On 19 July 2013 23:47, David Holmes <davidcholmes at aapt.net.au> wrote:

> **
> nanoTime() tries to be monotonic but it is dependent on the high
> resolution time facilities of the OS. If the OS is configured so that it's
> "monotonic" time source is not actually monotonic then nanoTime is broken
> too. This can be caused by a number of issues most of which involve
> improper use of the TSC as the timesource (something you can select
> yourself depending on the OS). There can also be configuration issues
> between virtualization environments and the guest OS that break time
> properties.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Chris Vest
> *Sent:* Friday, 19 July 2013 10:43 PM
> *To:* concurrency-interest
> *Subject:* [concurrency-interest] Monotonic time
>
> Hi,
>
> Is System.nanoTime safe to use across threads/cores? Say, if I do a read
> on one core, does it preserve its monotonic properties and a
> reasonable accuracy when compared with a read on another core?
>
> I'm working on an object pool that by default expires objects after they
> have been live for about 10 minutes.
>
> To implement this, I have a timestamp for when I created them, and then
> regularly (every time I want to borrow an object from the pool) measure the
> current time and calculate their age.
>
> I currently use System.currentTimeMillis to measure time. This is a
> problem, however, because the system clock can be adjusted backwards: in
> places that observe daylight savings time, for instance, objects might
> suddenly find themselves with a creation timestamp one hour in the future.
>
> Thinking about how to fix this, I am lead to my opening question.
>
> Cheers,
> Chris
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130720/cf7fab09/attachment.html>

From mike at uark.edu  Sat Jul 20 12:28:53 2013
From: mike at uark.edu (Michael Akerman)
Date: Sat, 20 Jul 2013 16:28:53 +0000
Subject: [concurrency-interest] Design of Thread Safe Iterator Proxy
In-Reply-To: <CAA5REoU+3sMAHa7Z2=k3pdETeZck1z5WGDe8P9v18_fGQUTQmA@mail.gmail.com>
References: <CAA5REoU+3sMAHa7Z2=k3pdETeZck1z5WGDe8P9v18_fGQUTQmA@mail.gmail.com>
Message-ID: <8821AE6678928F4EA5A3D4CA8417165F4CD1E6E4@ex-mbx1b.uark.edu>

Piyush,

You need to unlock the ReentrantLock if the wrapped Iterator's hasNext return's false.  Otherwise everyone gets stuck at the end of the list.

For example:

    public boolean hasNext() {
        lock.lock();
        boolean hasNext = false;
        try {
            hasNext = iterator.hasNext();
    if ( !hasNext ) lock.unlock();  //the line I added
        } catch (Exception e) {
            lock.unlock();
        }
        return hasNext;
    }


Michael Akerman
Systems Specialist
IT Services
________________________________
From: concurrency-interest-bounces at cs.oswego.edu [concurrency-interest-bounces at cs.oswego.edu] on behalf of piyush katariya [corporate.piyush at gmail.com]
Sent: Monday, June 10, 2013 1:37 AM
To: Concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Design of Thread Safe Iterator Proxy

Hi,

       so i was in the need of ThreadSafe iterator, so that multiple threads can access over it concurrently without  throwing "ConcurrentModificationException".

i came with solution attached herewith, but for some reason..multiple threads from thread pool after iterating over, stucks...

can somebody help me with it..


Regards,
Piyush Katariya
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130720/5c2da2f6/attachment.html>

From dl at cs.oswego.edu  Sat Jul 20 13:03:45 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Jul 2013 13:03:45 -0400
Subject: [concurrency-interest] A variant of the jsr166e.CHMv8.TreeBin
 bug is back?
In-Reply-To: <51E52C9E.6060008@cs.oswego.edu>
References: <561232CE-2488-4E15-A87A-28642F23E9D4@redhat.com>
	<51E52C9E.6060008@cs.oswego.edu>
Message-ID: <51EAC2F1.8050402@cs.oswego.edu>

On 07/16/13 07:21, Doug Lea wrote:
> On 07/16/13 05:14, Galder Zamarre?o wrote:
>> Hi,
>>
>> A few weeks back, Alex Snaps sent an email wrt to a possible treebin bug in
>> jsr166e.CHMv8.TreeBin [1].
>>
>> With revision 1.100 and applying the patches in [2], the test passed.
>>
>> However, running the same test that Alex submitted with revision 1.110 of
>> jsr166e.CHMv8 [3] (no patches) fails
>>

Sorry for problems. Now re-fixed. (The test case was a near-duplicate
of one in existing tests. But not exactly the same, which embarrassingly
led fix to be dropped in updates. Now added to the usual test cases.)

-Doug





From viktor.klang at gmail.com  Sat Jul 20 14:16:52 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 20 Jul 2013 20:16:52 +0200
Subject: [concurrency-interest] Design of Thread Safe Iterator Proxy
In-Reply-To: <8821AE6678928F4EA5A3D4CA8417165F4CD1E6E4@ex-mbx1b.uark.edu>
References: <CAA5REoU+3sMAHa7Z2=k3pdETeZck1z5WGDe8P9v18_fGQUTQmA@mail.gmail.com>
	<8821AE6678928F4EA5A3D4CA8417165F4CD1E6E4@ex-mbx1b.uark.edu>
Message-ID: <CANPzfU_WR8SSONjPrDwbzSaY5wfPa4+pr18Hp4O9+uZu7CLacA@mail.gmail.com>

public boolean hasNext() {
        lock.lock();
        boolean hasNext = false;
        try {
            hasNext = iterator.hasNext();
    if ( !hasNext ) lock.unlock();  //the line I added
        } catch (Exception e) {
            lock.unlock();

Swallowing exceptions?

        }
        return hasNext;
    }


On Sat, Jul 20, 2013 at 6:28 PM, Michael Akerman <mike at uark.edu> wrote:

>  Piyush,
>
>  You need to unlock the ReentrantLock if the wrapped Iterator's hasNext
> return's false.  Otherwise everyone gets stuck at the end of the list.
>
>  For example:
>
>      public boolean hasNext() {
>         lock.lock();
>         boolean hasNext = false;
>         try {
>             hasNext = iterator.hasNext();
>     if ( !hasNext ) lock.unlock();  //the line I added
>         } catch (Exception e) {
>             lock.unlock();
>         }
>         return hasNext;
>     }
>
>
>   *Michael Akerman*
> Systems Specialist
> IT Services
>   ------------------------------
> *From:* concurrency-interest-bounces at cs.oswego.edu [
> concurrency-interest-bounces at cs.oswego.edu] on behalf of piyush katariya [
> corporate.piyush at gmail.com]
> *Sent:* Monday, June 10, 2013 1:37 AM
> *To:* Concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Design of Thread Safe Iterator Proxy
>
>   Hi,
>
>         so i was in the need of ThreadSafe iterator, so that multiple
> threads can access over it concurrently without  throwing
> "ConcurrentModificationException".
>
>  i came with solution attached herewith, but for some reason..multiple
> threads from thread pool after iterating over, stucks...
>
>  can somebody help me with it..
>
>
>  Regards,
> Piyush Katariya
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130720/92dd3008/attachment-0001.html>

From dl at cs.oswego.edu  Sat Jul 20 15:27:46 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 20 Jul 2013 15:27:46 -0400
Subject: [concurrency-interest] Outage Wednesday July 17
In-Reply-To: <51E585B1.4070509@cs.oswego.edu>
References: <51E585B1.4070509@cs.oswego.edu>
Message-ID: <51EAE4B2.4070701@cs.oswego.edu>

On 07/16/13 13:41, Doug Lea wrote:
>
> The outage I mentioned a few weeks ago got postponed a few times,
> but this this time they say they really mean it. So this list,
> CVS, etc, will all be unavailable for hopefully less than a day
> as our department moves equipment across the street to a new
> building.
>

As a few people noticed, "available" didn't mean "continuously
available" over the past few days. Please bear with us for probably
a few more sporadic short-term outages as things settle here.

-Doug



From yankee.sierra at gmail.com  Sat Jul 20 21:52:29 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Sat, 20 Jul 2013 21:52:29 -0400
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <51E97712.2010306@oracle.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
Message-ID: <CAE+h5-Cyk4TwL2UVY3-TJa5Tacour7QTWagcYLVGGCYh2fuGQg@mail.gmail.com>

Here's a possible example: a data store that uses tombstones (or some other
MVCC-type scheme) and would like to proactively delete them during reads.
That is, it's scanning a page for a read operation, and notices that a
value has several versions which can be compacted. It would like to upgrade
its lock to a write, perform the compaction, and then downgrade back to the
read-only lock.

I think this case is also interesting because if the thread can't perform
an upgrade immediately, it probably wants to just move on without retrying
(since cleanup is not critical to the scan). Furthermore, since scans may
take a while (and you don't want the write lock for all of that time), and
a lot of data may have already been scanned (and possibly already sent over
the network, in a streaming mode), it's not really plausible to back off
and acquire a write lock for the full duration of a new scan.


On Fri, Jul 19, 2013 at 1:27 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  I ran into upgrading a lock problem a while back.  The solution I took
> is first have the thread read acquire the lock.  Then if the thread detects
> that it needs the write lock, it releases the read lock and starts over by
> acquiring the write lock.  Of course, all of the data the thread collected
> in the read lock is considered invalid.  This greatly improved scalability
> and the "upgrade" didn't hurt performance.  This is because  writes mostly
> happen at start up and are rare otherwise.
>
> The above logic works great and there hasn't been a need for a
> read-write-update lock.  The problem in my situation is that I can't
> predict at the time of acquiring the lock if I will need to do any writes.
> The thread has to check the protected state and then decide if any writes
> are necessary.  (I would guess this scenario is true of most caches.)  If
> all threads do update acquires, then I am back to the same contention I had
> with all threads doing write acquires.
>
> I haven't needed this kind of lock.  Every time I run into lock
> contention, I always solve the problem without it.  If I had such a lock in
> my toolbox, maybe I would select it.
>
> Can you give a good example of where update acquires can actually help?  A
> toy example is one method which always needs to do reads and another method
> which has to do a lot of reads with a very small portion to do writes.  The
> first method is called very frequently relative to the second method.  Is
> there such a situation in software?
>
> -Nathan
>
> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>
> Hi,
>
> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>
> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>
> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>
> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>
> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>
> Some differences between my implementation and the Linux kernel mailing list discussion:
> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>
> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>
> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>
> Best regards,
>
> Niall Gallagherwww.npgall.com
>
> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>     http://code.google.com/p/concurrent-locks/
>
> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>     http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>
> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>     http://code.google.com/p/concurrent-trees/
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130720/1fb3ebf6/attachment.html>

From peter.kitt.reilly at gmail.com  Sun Jul 21 03:38:46 2013
From: peter.kitt.reilly at gmail.com (Peter Reilly)
Date: Sun, 21 Jul 2013 01:38:46 -0600
Subject: [concurrency-interest] peter.kitt.reilly
Message-ID: <CAPohPdnVXxDjrOwxr1vPdo+NZt-=BrFNcnR50uAO3B5uUCcokw@mail.gmail.com>

http://international.aron-online.be/ynrb/ctjtoh.plhjrewcnu

From mjpt777 at gmail.com  Sun Jul 21 06:54:37 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Sun, 21 Jul 2013 11:54:37 +0100
Subject: [concurrency-interest] Atomics for ByteBuffer
Message-ID: <CAChYfd_KAXaVFkrj+0sO5ZCaQZOZq1hHVFVqFbMmnaHVZKN7gQ@mail.gmail.com>

How would we progress such a change to have an AtomicByteBuffer?

Martin...

> I see a need for ByteBuffer with atomics (say AtomicByteBuffer).
> Infiniband supports atomic operations over the network.  If Infiniband
> atomics were linked to processor atomics and the program uses
> AtomicByteBuffer, then 2 processes on different machines could execute
> atomic operations on the same piece of memory.
>
> One problem is that the atomic operations can't straddle 2 cache lines.
> If alignment is required, that problem would be solved.
>
> -Nathan
>
> On 7/19/2013 6:03 AM, Vitaly Davidovich wrote:
>
> ByteBuffer is not spec'd to be threadsafe, IIRC. Adding these methods
> there would lead to confusion, I think, if they remain spec'd as
> such.  Maybe a separate impl can be created to support atomics?
> > Sent from my phone
> > On Jul 19, 2013 8:46 AM, "Martin Thompson" <mjpt777 at gmail.com
> <mailto:mjpt777 at gmail.com>> wrote:
>
>     One of the most common uses I see for Unsafe is building IPC
>     implementations.  If we want folk to stop using Unsafe then we
>     need to give them an alternative.
>
>     Is there any reason why we cannot add atomic methods like
>     putOrderedX, putVolatileX, getVolatileX, getAndAddX,
>     compareAndSetX to ByteBuffer?
>
>     With these new methods an IPC implementation can be built using
>     memory mapped files without resorting to using Unsafe.
>
>     Regards,
>     Martin...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130721/cf07f4d6/attachment.html>

From corporate.piyush at gmail.com  Mon Jul 22 01:25:39 2013
From: corporate.piyush at gmail.com (piyush katariya)
Date: Mon, 22 Jul 2013 06:25:39 +0100
Subject: [concurrency-interest] Design of Thread Safe Iterator Proxy
In-Reply-To: <8821AE6678928F4EA5A3D4CA8417165F4CD1E6E4@ex-mbx1b.uark.edu>
References: <CAA5REoU+3sMAHa7Z2=k3pdETeZck1z5WGDe8P9v18_fGQUTQmA@mail.gmail.com>
	<8821AE6678928F4EA5A3D4CA8417165F4CD1E6E4@ex-mbx1b.uark.edu>
Message-ID: <CAA5REoVOr-9pVi7sSC0pp+P6D1i_pB3Oa8Z4kfX1LYPVx4mMJQ@mail.gmail.com>

Thanks amigos :)


On Saturday, July 20, 2013, Michael Akerman wrote:

>  Piyush,
>
>  You need to unlock the ReentrantLock if the wrapped Iterator's hasNext
> return's false.  Otherwise everyone gets stuck at the end of the list.
>
>  For example:
>
>      public boolean hasNext() {
>         lock.lock();
>         boolean hasNext = false;
>         try {
>             hasNext = iterator.hasNext();
>     if ( !hasNext ) lock.unlock();  //the line I added
>         } catch (Exception e) {
>             lock.unlock();
>         }
>         return hasNext;
>     }
>
>
>   *Michael Akerman*
> Systems Specialist
> IT Services
>   ------------------------------
> *From:* concurrency-interest-bounces at cs.oswego.edu <javascript:_e({},
> 'cvml', 'concurrency-interest-bounces at cs.oswego.edu');> [
> concurrency-interest-bounces at cs.oswego.edu <javascript:_e({}, 'cvml',
> 'concurrency-interest-bounces at cs.oswego.edu');>] on behalf of piyush
> katariya [corporate.piyush at gmail.com <javascript:_e({}, 'cvml',
> 'corporate.piyush at gmail.com');>]
> *Sent:* Monday, June 10, 2013 1:37 AM
> *To:* Concurrency-interest at cs.oswego.edu <javascript:_e({}, 'cvml',
> 'Concurrency-interest at cs.oswego.edu');>
> *Subject:* [concurrency-interest] Design of Thread Safe Iterator Proxy
>
>   Hi,
>
>         so i was in the need of ThreadSafe iterator, so that multiple
> threads can access over it concurrently without  throwing
> "ConcurrentModificationException".
>
>  i came with solution attached herewith, but for some reason..multiple
> threads from thread pool after iterating over, stucks...
>
>  can somebody help me with it..
>
>
>  Regards,
> Piyush Katariya
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130722/77a31588/attachment.html>

From nathan.reynolds at oracle.com  Mon Jul 22 14:07:28 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 22 Jul 2013 11:07:28 -0700
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <CAE+h5-Cyk4TwL2UVY3-TJa5Tacour7QTWagcYLVGGCYh2fuGQg@mail.gmail.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<CAE+h5-Cyk4TwL2UVY3-TJa5Tacour7QTWagcYLVGGCYh2fuGQg@mail.gmail.com>
Message-ID: <51ED74E0.9010801@oracle.com>

You will need to try-update for each read.  If that succeeds, then the 
thread can upgrade during its read operation to do compaction. If that 
fails, then the thread will simply read.  Note: Only 1 reader thread 
will hold the lock in update-able mode at a time.  All other threads 
will have to only do read locks.

The thread with update ability will block and wait for all readers to 
release the lock before the thread can proceed.  This will slow down the 
update thread.

Also, all threads wishing to do write acquires will block and wait for 
the thread with the update ability.  In other words, writers will slow down.

A scenario is that 1 updater and several readers are holding the lock.  
A writer comes along and blocks.  No new readers are allowed to acquire 
the lock.  They block too.  The updater wishes to upgrade the lock and 
it blocks until all of the readers drain.  Then the updater continues 
and finally releases the lock entirely.  Then the writer proceeds.  
Finally all of the readers proceed and one of them acquires the update 
ability.  Rinse and repeat.

I guess my basic message is this: Watch out for scalability problems!

-Nathan

On 7/20/2013 6:52 PM, Yuval Shavit wrote:
> Here's a possible example: a data store that uses tombstones (or some 
> other MVCC-type scheme) and would like to proactively delete them 
> during reads. That is, it's scanning a page for a read operation, and 
> notices that a value has several versions which can be compacted. It 
> would like to upgrade its lock to a write, perform the compaction, and 
> then downgrade back to the read-only lock.
>
> I think this case is also interesting because if the thread can't 
> perform an upgrade immediately, it probably wants to just move on 
> without retrying (since cleanup is not critical to the scan). 
> Furthermore, since scans may take a while (and you don't want the 
> write lock for all of that time), and a lot of data may have already 
> been scanned (and possibly already sent over the network, in a 
> streaming mode), it's not really plausible to back off and acquire a 
> write lock for the full duration of a new scan.
>
>
> On Fri, Jul 19, 2013 at 1:27 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     I ran into upgrading a lock problem a while back. The solution I
>     took is first have the thread read acquire the lock.  Then if the
>     thread detects that it needs the write lock, it releases the read
>     lock and starts over by acquiring the write lock.  Of course, all
>     of the data the thread collected in the read lock is considered
>     invalid.  This greatly improved scalability and the "upgrade"
>     didn't hurt performance.  This is because  writes mostly happen at
>     start up and are rare otherwise.
>
>     The above logic works great and there hasn't been a need for a
>     read-write-update lock.  The problem in my situation is that I
>     can't predict at the time of acquiring the lock if I will need to
>     do any writes.  The thread has to check the protected state and
>     then decide if any writes are necessary.  (I would guess this
>     scenario is true of most caches.)  If all threads do update
>     acquires, then I am back to the same contention I had with all
>     threads doing write acquires.
>
>     I haven't needed this kind of lock.  Every time I run into lock
>     contention, I always solve the problem without it.  If I had such
>     a lock in my toolbox, maybe I would select it.
>
>     Can you give a good example of where update acquires can actually
>     help?  A toy example is one method which always needs to do reads
>     and another method which has to do a lot of reads with a very
>     small portion to do writes. The first method is called very
>     frequently relative to the second method.  Is there such a
>     situation in software?
>
>     -Nathan
>
>     On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>     Hi,
>>
>>     I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>
>>     Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>
>>     I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>
>>     I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>
>>     The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>
>>     Some differences between my implementation and the Linux kernel mailing list discussion:
>>     - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>     - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>
>>     There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>
>>     For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>
>>     Best regards,
>>
>>     Niall Gallagher
>>     www.npgall.com  <http://www.npgall.com>
>>
>>     [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>          http://code.google.com/p/concurrent-locks/
>>
>>     [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>          http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>
>>     [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>          http://code.google.com/p/concurrent-trees/
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130722/7f4aeefe/attachment.html>

From nathan.reynolds at oracle.com  Mon Jul 22 14:08:43 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 22 Jul 2013 11:08:43 -0700
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <CAChYfd_KAXaVFkrj+0sO5ZCaQZOZq1hHVFVqFbMmnaHVZKN7gQ@mail.gmail.com>
References: <CAChYfd_KAXaVFkrj+0sO5ZCaQZOZq1hHVFVqFbMmnaHVZKN7gQ@mail.gmail.com>
Message-ID: <51ED752B.9030004@oracle.com>

I am not sure.  I suppose you would need to create a JSR or JEP or 
something of that nature.

-Nathan

On 7/21/2013 3:54 AM, Martin Thompson wrote:
> How would we progress such a change to have an AtomicByteBuffer?
>
> Martin...
>
> > I see a need for ByteBuffer with atomics (say AtomicByteBuffer).
> > Infiniband supports atomic operations over the network.  If Infiniband
> > atomics were linked to processor atomics and the program uses
> > AtomicByteBuffer, then 2 processes on different machines could execute
> > atomic operations on the same piece of memory.
> >
> > One problem is that the atomic operations can't straddle 2 cache lines.
> > If alignment is required, that problem would be solved.
> >
> > -Nathan
> >
> > On 7/19/2013 6:03 AM, Vitaly Davidovich wrote:
> >
> > ByteBuffer is not spec'd to be threadsafe, IIRC. Adding these methods
> > there would lead to confusion, I think, if they remain spec'd as
> > such.  Maybe a separate impl can be created to support atomics?
> > > Sent from my phone
> > > On Jul 19, 2013 8:46 AM, "Martin Thompson" <mjpt777 at gmail.com 
> <http://gmail.com>
> > <mailto:mjpt777 <mailto:mjpt777> at gmail.com <http://gmail.com>>> 
> wrote:
> >
> >     One of the most common uses I see for Unsafe is building IPC
> >     implementations.  If we want folk to stop using Unsafe then we
> >     need to give them an alternative.
> >
> >     Is there any reason why we cannot add atomic methods like
> >     putOrderedX, putVolatileX, getVolatileX, getAndAddX,
> >     compareAndSetX to ByteBuffer?
> >
> >     With these new methods an IPC implementation can be built using
> >     memory mapped files without resorting to using Unsafe.
> >
> >     Regards,
> >     Martin...
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130722/6f917700/attachment.html>

From davidcholmes at aapt.net.au  Mon Jul 22 19:36:11 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 23 Jul 2013 09:36:11 +1000
Subject: [concurrency-interest] Atomics for ByteBuffer
In-Reply-To: <51ED752B.9030004@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEDNJPAA.davidcholmes@aapt.net.au>

Indeed this needs to follow the JEP process in general:

http://openjdk.java.net/jeps/1/

which may, or may not, lead to a JSR.

Discussion needs to be initiated on the appropriate openjdk mail alias -
probably nio-dev in this case.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Nathan
Reynolds
  Sent: Tuesday, 23 July 2013 4:09 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Atomics for ByteBuffer


  I am not sure.  I suppose you would need to create a JSR or JEP or
something of that nature.

-NathanOn 7/21/2013 3:54 AM, Martin Thompson wrote:

    How would we progress such a change to have an AtomicByteBuffer?


    Martin...


    > I see a need for ByteBuffer with atomics (say AtomicByteBuffer).
    > Infiniband supports atomic operations over the network.  If Infiniband
    > atomics were linked to processor atomics and the program uses
    > AtomicByteBuffer, then 2 processes on different machines could execute
    > atomic operations on the same piece of memory.
    >
    > One problem is that the atomic operations can't straddle 2 cache
lines.
    > If alignment is required, that problem would be solved.
    >
    > -Nathan
    >
    > On 7/19/2013 6:03 AM, Vitaly Davidovich wrote:
    >
    > ByteBuffer is not spec'd to be threadsafe, IIRC. Adding these methods
    > there would lead to confusion, I think, if they remain spec'd as
    > such.  Maybe a separate impl can be created to support atomics?
    > > Sent from my phone
    > > On Jul 19, 2013 8:46 AM, "Martin Thompson" <mjpt777 at gmail.com
    > <mailto:mjpt777 at gmail.com>> wrote:
    >
    >     One of the most common uses I see for Unsafe is building IPC
    >     implementations.  If we want folk to stop using Unsafe then we
    >     need to give them an alternative.
    >
    >     Is there any reason why we cannot add atomic methods like
    >     putOrderedX, putVolatileX, getVolatileX, getAndAddX,
    >     compareAndSetX to ByteBuffer?
    >
    >     With these new methods an IPC implementation can be built using
    >     memory mapped files without resorting to using Unsafe.
    >
    >     Regards,
    >     Martin...



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130723/d4c9ae16/attachment-0001.html>

From niall at npgall.com  Mon Jul 22 19:43:02 2013
From: niall at npgall.com (Niall Gallagher)
Date: Tue, 23 Jul 2013 00:43:02 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <51E97712.2010306@oracle.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
Message-ID: <C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>

The motivation for this read-write-update lock was actually a distributed systems problem.

The crux of the problem was that it was not feasible to "re-play" requests.

Broadly, requests entering a front-end system could be classified as being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST). On receiving a request, the front-end system would read from a local data store and then forward the request to back-end systems. The front-end system would gather responses from the back-end systems and from those determine if its local data store needed to be updated.

In a system like that, it's not possible for the front-end system to "try" the request using a read lock, and then if it later detects that it needs to write, drop the read lock and re-play the request using a write lock. First, because dropping the read lock would allow another thread to steal the write lock, causing lost updates, and second because re-playing the request over multiple back-end systems would be expensive. The only solution would be to acquire the write lock on every HTTP POST.

So I started to view the read-write lock as somewhat limiting.

There is also the performance aspect. For simplicity say 50% of requests are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the write lock which would (1) block all reading threads even if only a small fraction of POSTs would require writes, and (2) would block all reading threads for the entire latency of interactions with back-end servers.

A read-write-update lock on the other hand, (1) does not block reading threads unless a write actually is performed, and (2) only blocks reading threads from the point at which the lock is upgraded, which in the case above is after responses from back-end systems have been gathered and so is for only a fraction of the overall processing time.

I think the example scales-down to simpler situations too. The key advantage of a read-write-update lock is in read-before-write access patterns.

Read-before-write is common. Many applications do it in three steps: (1) read data, (2) do some computations on the data, (3) write out the resulting data. A conventional read-write lock does not support this use case well.

At the very least, a read-write-update lock would increase concurrency by not blocking reading threads until a writing thread reaches step 3; concurrent reads would be allowed during steps 1 & 2. A conventional read-write lock would block reads for all three steps.

I think also in some systems it's basically tricky to implement re-play functionality, so a read-write-update lock[1] seems like a nice proposal. YMMV!

Niall Gallagher
www.npgall.com

[1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
    http://code.google.com/p/concurrent-locks/


On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com> wrote:

> I ran into upgrading a lock problem a while back.  The solution I took is first have the thread read acquire the lock.  Then if the thread detects that it needs the write lock, it releases the read lock and starts over by acquiring the write lock.  Of course, all of the data the thread collected in the read lock is considered invalid.  This greatly improved scalability and the "upgrade" didn't hurt performance.  This is because  writes mostly happen at start up and are rare otherwise.
> 
> The above logic works great and there hasn't been a need for a read-write-update lock.  The problem in my situation is that I can't predict at the time of acquiring the lock if I will need to do any writes.  The thread has to check the protected state and then decide if any writes are necessary.  (I would guess this scenario is true of most caches.)  If all threads do update acquires, then I am back to the same contention I had with all threads doing write acquires.
> 
> I haven't needed this kind of lock.  Every time I run into lock contention, I always solve the problem without it.  If I had such a lock in my toolbox, maybe I would select it.
> 
> Can you give a good example of where update acquires can actually help?  A toy example is one method which always needs to do reads and another method which has to do a lot of reads with a very small portion to do writes.  The first method is called very frequently relative to the second method.  Is there such a situation in software?
> -Nathan
> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>> Hi,
>> 
>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>> 
>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>> 
>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>> 
>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>> 
>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>> 
>> Some differences between my implementation and the Linux kernel mailing list discussion: 
>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>> 
>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>> 
>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>> 
>> Best regards,
>> 
>> Niall Gallagher
>> www.npgall.com
>> 
>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>     http://code.google.com/p/concurrent-locks/
>> 
>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>     http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>> 
>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>     http://code.google.com/p/concurrent-trees/
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130723/b87a28c4/attachment.html>

From aaron.grunthal at infinite-source.de  Mon Jul 22 21:57:05 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 23 Jul 2013 03:57:05 +0200
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
Message-ID: <51EDE2F1.40900@infinite-source.de>

The problem with a read-update is that the lock can never guarantee that 
the update is going to succeed. A write lock is generally considered an 
exclusive lock, think of transaction semantics. If two threads hold a 
read lock and both of them try to upgrade to a write lock neither can 
upgrade since an exclusive write lock would violate the shared semantics 
of a read lock. So at least one of them would have to stay in read mode, 
finished their logic in read-only mode and then relinquish the read lock 
before the other thread can even acquire the write lock.

So read-update will always have to support fail-and-retry or the logic 
would have to ensure that only one reader thread would try to upgrade to 
write at any given time and get priority over threads that try to 
acquire a write lock directly.

Of course there are special scenarios where you actually have "read up 
to X in read mode, acquire write mode for X+1" where the write lock 
would not violate violate the semantics of the earlier read chunks, but 
those things are different from a simple read-write lock, they're sets 
of range-locks, such as used by database engines.

So as far as I can see the "non-replayable request" issue wouldn't be 
solved by a generic read-tryUpgrade lock.


On 23.07.2013 01:43, Niall Gallagher wrote:
> The motivation for this read-write-update lock was actually a
> distributed systems problem.
>
> The crux of the problem was that it was not feasible to "re-play" requests.
>
> Broadly, requests entering a front-end system could be classified as
> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
> On receiving a request, the front-end system would read from a local
> data store and then forward the request to back-end systems. The
> front-end system would gather responses from the back-end systems and
> from those determine if its local data store needed to be updated.
>
> In a system like that, it's not possible for the front-end system to
> "try" the request using a read lock, and then if it later detects that
> it needs to write, drop the read lock and re-play the request using a
> write lock. First, because dropping the read lock would allow another
> thread to steal the write lock, causing lost updates, and second because
> re-playing the request over multiple back-end systems would be
> expensive. The only solution would be to acquire the write lock on every
> HTTP POST.
>
> So I started to view the read-write lock as somewhat limiting.
>
> There is also the performance aspect. For simplicity say 50% of requests
> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
> write lock which would (1) block all reading threads even if only a
> small fraction of POSTs would require writes, and (2) would block all
> reading threads for the entire latency of interactions with back-end
> servers.
>
> A read-write-update lock on the other hand, (1) does not block reading
> threads unless a write actually is performed, and (2) only blocks
> reading threads from the point at which the lock is upgraded, which in
> the case above is after responses from back-end systems have been
> gathered and so is for only a fraction of the overall processing time.
>
> I think the example scales-down to simpler situations too. The key
> advantage of a read-write-update lock is in read-before-write access
> patterns.
>
> Read-before-write is common. Many applications do it in three steps: (1)
> read data, (2) do some computations on the data, (3) write out the
> resulting data. A conventional read-write lock does not support this use
> case well.
>
> At the very least, a read-write-update lock would increase concurrency
> by not blocking reading threads until a writing thread reaches step 3;
> concurrent reads would be allowed during steps 1 & 2. A conventional
> read-write lock would block reads for all three steps.
>
> I think also in some systems it's basically tricky to implement re-play
> functionality, so a read-write-update lock[1] seems like a nice
> proposal. YMMV!
>
> Niall Gallagher
> www.npgall.com <http://www.npgall.com>
>
> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
> for Java
> http://code.google.com/p/concurrent-locks/
>
>
> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>> I ran into upgrading a lock problem a while back.  The solution I took
>> is first have the thread read acquire the lock.  Then if the thread
>> detects that it needs the write lock, it releases the read lock and
>> starts over by acquiring the write lock.  Of course, all of the data
>> the thread collected in the read lock is considered invalid.  This
>> greatly improved scalability and the "upgrade" didn't hurt
>> performance.  This is because  writes mostly happen at start up and
>> are rare otherwise.
>>
>> The above logic works great and there hasn't been a need for a
>> read-write-update lock.  The problem in my situation is that I can't
>> predict at the time of acquiring the lock if I will need to do any
>> writes.  The thread has to check the protected state and then decide
>> if any writes are necessary.  (I would guess this scenario is true of
>> most caches.)  If all threads do update acquires, then I am back to
>> the same contention I had with all threads doing write acquires.
>>
>> I haven't needed this kind of lock.  Every time I run into lock
>> contention, I always solve the problem without it.  If I had such a
>> lock in my toolbox, maybe I would select it.
>>
>> Can you give a good example of where update acquires can actually
>> help?  A toy example is one method which always needs to do reads and
>> another method which has to do a lot of reads with a very small
>> portion to do writes.  The first method is called very frequently
>> relative to the second method.  Is there such a situation in software?
>> -Nathan
>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>> Hi,
>>>
>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>
>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>
>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>
>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>
>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>
>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>
>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>
>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>
>>> Best regards,
>>>
>>> Niall Gallagher
>>> www.npgall.com
>>>
>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>      http://code.google.com/p/concurrent-locks/
>>>
>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>      http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>
>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>      http://code.google.com/p/concurrent-trees/
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From niall at npgall.com  Mon Jul 22 23:48:07 2013
From: niall at npgall.com (Niall Gallagher)
Date: Tue, 23 Jul 2013 04:48:07 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <51EDE2F1.40900@infinite-source.de>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
Message-ID: <27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>

Actually the read-write-update lock solves that deadlock problem.

The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.

[1] http://code.google.com/p/concurrent-locks/


On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:

> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
> 
> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
> 
> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
> 
> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
> 
> 
> On 23.07.2013 01:43, Niall Gallagher wrote:
>> The motivation for this read-write-update lock was actually a
>> distributed systems problem.
>> 
>> The crux of the problem was that it was not feasible to "re-play" requests.
>> 
>> Broadly, requests entering a front-end system could be classified as
>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>> On receiving a request, the front-end system would read from a local
>> data store and then forward the request to back-end systems. The
>> front-end system would gather responses from the back-end systems and
>> from those determine if its local data store needed to be updated.
>> 
>> In a system like that, it's not possible for the front-end system to
>> "try" the request using a read lock, and then if it later detects that
>> it needs to write, drop the read lock and re-play the request using a
>> write lock. First, because dropping the read lock would allow another
>> thread to steal the write lock, causing lost updates, and second because
>> re-playing the request over multiple back-end systems would be
>> expensive. The only solution would be to acquire the write lock on every
>> HTTP POST.
>> 
>> So I started to view the read-write lock as somewhat limiting.
>> 
>> There is also the performance aspect. For simplicity say 50% of requests
>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>> write lock which would (1) block all reading threads even if only a
>> small fraction of POSTs would require writes, and (2) would block all
>> reading threads for the entire latency of interactions with back-end
>> servers.
>> 
>> A read-write-update lock on the other hand, (1) does not block reading
>> threads unless a write actually is performed, and (2) only blocks
>> reading threads from the point at which the lock is upgraded, which in
>> the case above is after responses from back-end systems have been
>> gathered and so is for only a fraction of the overall processing time.
>> 
>> I think the example scales-down to simpler situations too. The key
>> advantage of a read-write-update lock is in read-before-write access
>> patterns.
>> 
>> Read-before-write is common. Many applications do it in three steps: (1)
>> read data, (2) do some computations on the data, (3) write out the
>> resulting data. A conventional read-write lock does not support this use
>> case well.
>> 
>> At the very least, a read-write-update lock would increase concurrency
>> by not blocking reading threads until a writing thread reaches step 3;
>> concurrent reads would be allowed during steps 1 & 2. A conventional
>> read-write lock would block reads for all three steps.
>> 
>> I think also in some systems it's basically tricky to implement re-play
>> functionality, so a read-write-update lock[1] seems like a nice
>> proposal. YMMV!
>> 
>> Niall Gallagher
>> www.npgall.com <http://www.npgall.com>
>> 
>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>> for Java
>> http://code.google.com/p/concurrent-locks/
>> 
>> 
>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>> <mailto:nathan.reynolds at oracle.com>> wrote:
>> 
>>> I ran into upgrading a lock problem a while back.  The solution I took
>>> is first have the thread read acquire the lock.  Then if the thread
>>> detects that it needs the write lock, it releases the read lock and
>>> starts over by acquiring the write lock.  Of course, all of the data
>>> the thread collected in the read lock is considered invalid.  This
>>> greatly improved scalability and the "upgrade" didn't hurt
>>> performance.  This is because  writes mostly happen at start up and
>>> are rare otherwise.
>>> 
>>> The above logic works great and there hasn't been a need for a
>>> read-write-update lock.  The problem in my situation is that I can't
>>> predict at the time of acquiring the lock if I will need to do any
>>> writes.  The thread has to check the protected state and then decide
>>> if any writes are necessary.  (I would guess this scenario is true of
>>> most caches.)  If all threads do update acquires, then I am back to
>>> the same contention I had with all threads doing write acquires.
>>> 
>>> I haven't needed this kind of lock.  Every time I run into lock
>>> contention, I always solve the problem without it.  If I had such a
>>> lock in my toolbox, maybe I would select it.
>>> 
>>> Can you give a good example of where update acquires can actually
>>> help?  A toy example is one method which always needs to do reads and
>>> another method which has to do a lot of reads with a very small
>>> portion to do writes.  The first method is called very frequently
>>> relative to the second method.  Is there such a situation in software?
>>> -Nathan
>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>> Hi,
>>>> 
>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>> 
>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>> 
>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>> 
>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>> 
>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>> 
>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>> 
>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>> 
>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>> 
>>>> Best regards,
>>>> 
>>>> Niall Gallagher
>>>> www.npgall.com
>>>> 
>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>     http://code.google.com/p/concurrent-locks/
>>>> 
>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>     http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>> 
>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>     http://code.google.com/p/concurrent-trees/
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From zhong.j.yu at gmail.com  Tue Jul 23 00:29:53 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Mon, 22 Jul 2013 23:29:53 -0500
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
Message-ID: <CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>

Hi Niall, my understanding is that you are trying to implement
something like this

Lock readLock = ...;
Lock writeLock = ...;
Lock updateLock = ...;

GET:

    readLock.lock();
    ...
    readLock.unlock();


POST:

    updateLock.lock();  // exclusive
    readLock.lock();
   ...
    if(...)  // occasionally
        writeLock.lock();  // upgrade
        ...
        writeLock.unlock();
    ...
    readLock.unlock();
    updateLock.unlock();


except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
you need to roll your own. Is that correct?

Zhong Yu




On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
> Actually the read-write-update lock solves that deadlock problem.
>
> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>
> [1] http://code.google.com/p/concurrent-locks/
>
>
> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>
>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>
>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>
>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>
>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>
>>
>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>> The motivation for this read-write-update lock was actually a
>>> distributed systems problem.
>>>
>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>
>>> Broadly, requests entering a front-end system could be classified as
>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>> On receiving a request, the front-end system would read from a local
>>> data store and then forward the request to back-end systems. The
>>> front-end system would gather responses from the back-end systems and
>>> from those determine if its local data store needed to be updated.
>>>
>>> In a system like that, it's not possible for the front-end system to
>>> "try" the request using a read lock, and then if it later detects that
>>> it needs to write, drop the read lock and re-play the request using a
>>> write lock. First, because dropping the read lock would allow another
>>> thread to steal the write lock, causing lost updates, and second because
>>> re-playing the request over multiple back-end systems would be
>>> expensive. The only solution would be to acquire the write lock on every
>>> HTTP POST.
>>>
>>> So I started to view the read-write lock as somewhat limiting.
>>>
>>> There is also the performance aspect. For simplicity say 50% of requests
>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>> write lock which would (1) block all reading threads even if only a
>>> small fraction of POSTs would require writes, and (2) would block all
>>> reading threads for the entire latency of interactions with back-end
>>> servers.
>>>
>>> A read-write-update lock on the other hand, (1) does not block reading
>>> threads unless a write actually is performed, and (2) only blocks
>>> reading threads from the point at which the lock is upgraded, which in
>>> the case above is after responses from back-end systems have been
>>> gathered and so is for only a fraction of the overall processing time.
>>>
>>> I think the example scales-down to simpler situations too. The key
>>> advantage of a read-write-update lock is in read-before-write access
>>> patterns.
>>>
>>> Read-before-write is common. Many applications do it in three steps: (1)
>>> read data, (2) do some computations on the data, (3) write out the
>>> resulting data. A conventional read-write lock does not support this use
>>> case well.
>>>
>>> At the very least, a read-write-update lock would increase concurrency
>>> by not blocking reading threads until a writing thread reaches step 3;
>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>> read-write lock would block reads for all three steps.
>>>
>>> I think also in some systems it's basically tricky to implement re-play
>>> functionality, so a read-write-update lock[1] seems like a nice
>>> proposal. YMMV!
>>>
>>> Niall Gallagher
>>> www.npgall.com <http://www.npgall.com>
>>>
>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>> for Java
>>> http://code.google.com/p/concurrent-locks/
>>>
>>>
>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>
>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>> is first have the thread read acquire the lock.  Then if the thread
>>>> detects that it needs the write lock, it releases the read lock and
>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>> the thread collected in the read lock is considered invalid.  This
>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>> performance.  This is because  writes mostly happen at start up and
>>>> are rare otherwise.
>>>>
>>>> The above logic works great and there hasn't been a need for a
>>>> read-write-update lock.  The problem in my situation is that I can't
>>>> predict at the time of acquiring the lock if I will need to do any
>>>> writes.  The thread has to check the protected state and then decide
>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>> most caches.)  If all threads do update acquires, then I am back to
>>>> the same contention I had with all threads doing write acquires.
>>>>
>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>> contention, I always solve the problem without it.  If I had such a
>>>> lock in my toolbox, maybe I would select it.
>>>>
>>>> Can you give a good example of where update acquires can actually
>>>> help?  A toy example is one method which always needs to do reads and
>>>> another method which has to do a lot of reads with a very small
>>>> portion to do writes.  The first method is called very frequently
>>>> relative to the second method.  Is there such a situation in software?
>>>> -Nathan
>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>> Hi,
>>>>>
>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>
>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>
>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>
>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>
>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>
>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>
>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>
>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>
>>>>> Best regards,
>>>>>
>>>>> Niall Gallagher
>>>>> www.npgall.com
>>>>>
>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>     http://code.google.com/p/concurrent-locks/
>>>>>
>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>     http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>
>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>     http://code.google.com/p/concurrent-trees/
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From niall at npgall.com  Tue Jul 23 01:28:59 2013
From: niall at npgall.com (Niall Gallagher)
Date: Tue, 23 Jul 2013 06:28:59 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
Message-ID: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>

Hi Zhong,

Not exactly, but that is very close indeed. The only difference is in POST it does not acquire the read lock, so it's simply like this:

   updateLock.lock();  // exclusive
  ...
   if(...)  // occasionally
       writeLock.lock();  // upgrade
       ...
       writeLock.unlock();
   ...
   updateLock.unlock();


I initially did think that I'd have to roll my own ReentrantReadWriteLock for the reason you mentioned, but then I found a way around it. My reasoning is, it's not necessary to acquire an actual read lock from ReentrantReadWriteLock after the update lock has been acquired, because here we basically invent a new type of lock anyway (the update lock) so we just say in the documentation that the update lock gives the application "permission" to read but not write. So the number of threads with read "permission" at any one time can be <number of threads holding regular read locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update lock>.

The write lock is only granted when two conditions hold: the requesting thread must hold the update lock (enforced in the logic in RWULock), and other threads holding regular read locks must release those read locks (enforced by ReentrantReadWriteLock itself which RWULock delegates to internally). So this way it re-uses the ReentrantReadWriteLock of the JDK, no need for a custom implementation.

Best regards,
Niall Gallagher

On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> Hi Niall, my understanding is that you are trying to implement
> something like this
> 
> Lock readLock = ...;
> Lock writeLock = ...;
> Lock updateLock = ...;
> 
> GET:
> 
>    readLock.lock();
>    ...
>    readLock.unlock();
> 
> 
> POST:
> 
>    updateLock.lock();  // exclusive
>    readLock.lock();
>   ...
>    if(...)  // occasionally
>        writeLock.lock();  // upgrade
>        ...
>        writeLock.unlock();
>    ...
>    readLock.unlock();
>    updateLock.unlock();
> 
> 
> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
> you need to roll your own. Is that correct?
> 
> Zhong Yu
> 
> 
> 
> 
> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
>> Actually the read-write-update lock solves that deadlock problem.
>> 
>> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>> 
>> [1] http://code.google.com/p/concurrent-locks/
>> 
>> 
>> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>> 
>>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>> 
>>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>> 
>>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>> 
>>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>> 
>>> 
>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>> The motivation for this read-write-update lock was actually a
>>>> distributed systems problem.
>>>> 
>>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>> 
>>>> Broadly, requests entering a front-end system could be classified as
>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>>> On receiving a request, the front-end system would read from a local
>>>> data store and then forward the request to back-end systems. The
>>>> front-end system would gather responses from the back-end systems and
>>>> from those determine if its local data store needed to be updated.
>>>> 
>>>> In a system like that, it's not possible for the front-end system to
>>>> "try" the request using a read lock, and then if it later detects that
>>>> it needs to write, drop the read lock and re-play the request using a
>>>> write lock. First, because dropping the read lock would allow another
>>>> thread to steal the write lock, causing lost updates, and second because
>>>> re-playing the request over multiple back-end systems would be
>>>> expensive. The only solution would be to acquire the write lock on every
>>>> HTTP POST.
>>>> 
>>>> So I started to view the read-write lock as somewhat limiting.
>>>> 
>>>> There is also the performance aspect. For simplicity say 50% of requests
>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>> write lock which would (1) block all reading threads even if only a
>>>> small fraction of POSTs would require writes, and (2) would block all
>>>> reading threads for the entire latency of interactions with back-end
>>>> servers.
>>>> 
>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>> threads unless a write actually is performed, and (2) only blocks
>>>> reading threads from the point at which the lock is upgraded, which in
>>>> the case above is after responses from back-end systems have been
>>>> gathered and so is for only a fraction of the overall processing time.
>>>> 
>>>> I think the example scales-down to simpler situations too. The key
>>>> advantage of a read-write-update lock is in read-before-write access
>>>> patterns.
>>>> 
>>>> Read-before-write is common. Many applications do it in three steps: (1)
>>>> read data, (2) do some computations on the data, (3) write out the
>>>> resulting data. A conventional read-write lock does not support this use
>>>> case well.
>>>> 
>>>> At the very least, a read-write-update lock would increase concurrency
>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>> read-write lock would block reads for all three steps.
>>>> 
>>>> I think also in some systems it's basically tricky to implement re-play
>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>> proposal. YMMV!
>>>> 
>>>> Niall Gallagher
>>>> www.npgall.com <http://www.npgall.com>
>>>> 
>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>> for Java
>>>> http://code.google.com/p/concurrent-locks/
>>>> 
>>>> 
>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>> 
>>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>> detects that it needs the write lock, it releases the read lock and
>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>> the thread collected in the read lock is considered invalid.  This
>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>> performance.  This is because  writes mostly happen at start up and
>>>>> are rare otherwise.
>>>>> 
>>>>> The above logic works great and there hasn't been a need for a
>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>> writes.  The thread has to check the protected state and then decide
>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>> the same contention I had with all threads doing write acquires.
>>>>> 
>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>> contention, I always solve the problem without it.  If I had such a
>>>>> lock in my toolbox, maybe I would select it.
>>>>> 
>>>>> Can you give a good example of where update acquires can actually
>>>>> help?  A toy example is one method which always needs to do reads and
>>>>> another method which has to do a lot of reads with a very small
>>>>> portion to do writes.  The first method is called very frequently
>>>>> relative to the second method.  Is there such a situation in software?
>>>>> -Nathan
>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>> Hi,
>>>>>> 
>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>> 
>>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>> 
>>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>> 
>>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>> 
>>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>> 
>>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>> 
>>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>> 
>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>> 
>>>>>> Best regards,
>>>>>> 
>>>>>> Niall Gallagher
>>>>>> www.npgall.com
>>>>>> 
>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>>    http://code.google.com/p/concurrent-locks/
>>>>>> 
>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>>    http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>> 
>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>>    http://code.google.com/p/concurrent-trees/
>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From davidcholmes at aapt.net.au  Tue Jul 23 01:41:58 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 23 Jul 2013 15:41:58 +1000
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEECJPAA.davidcholmes@aapt.net.au>

Niall,

Under the Java Memory Model a thread that acquires the updateLock is not
guaranteed to see any updates performed under the WriteLock. You would need
to actually acquire the readLock to be strictly correct.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Niall
> Gallagher
> Sent: Tuesday, 23 July 2013 3:29 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Read-write-update lock
>
>
> Hi Zhong,
>
> Not exactly, but that is very close indeed. The only difference
> is in POST it does not acquire the read lock, so it's simply like this:
>
>    updateLock.lock();  // exclusive
>   ...
>    if(...)  // occasionally
>        writeLock.lock();  // upgrade
>        ...
>        writeLock.unlock();
>    ...
>    updateLock.unlock();
>
>
> I initially did think that I'd have to roll my own
> ReentrantReadWriteLock for the reason you mentioned, but then I
> found a way around it. My reasoning is, it's not necessary to
> acquire an actual read lock from ReentrantReadWriteLock after the
> update lock has been acquired, because here we basically invent a
> new type of lock anyway (the update lock) so we just say in the
> documentation that the update lock gives the application
> "permission" to read but not write. So the number of threads with
> read "permission" at any one time can be <number of threads
> holding regular read locks from ReentrantReadWriteLock> + <0 or 1
> threads holding the update lock>.
>
> The write lock is only granted when two conditions hold: the
> requesting thread must hold the update lock (enforced in the
> logic in RWULock), and other threads holding regular read locks
> must release those read locks (enforced by ReentrantReadWriteLock
> itself which RWULock delegates to internally). So this way it
> re-uses the ReentrantReadWriteLock of the JDK, no need for a
> custom implementation.
>
> Best regards,
> Niall Gallagher
>
> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
> > Hi Niall, my understanding is that you are trying to implement
> > something like this
> >
> > Lock readLock = ...;
> > Lock writeLock = ...;
> > Lock updateLock = ...;
> >
> > GET:
> >
> >    readLock.lock();
> >    ...
> >    readLock.unlock();
> >
> >
> > POST:
> >
> >    updateLock.lock();  // exclusive
> >    readLock.lock();
> >   ...
> >    if(...)  // occasionally
> >        writeLock.lock();  // upgrade
> >        ...
> >        writeLock.unlock();
> >    ...
> >    readLock.unlock();
> >    updateLock.unlock();
> >
> >
> > except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
> > you need to roll your own. Is that correct?
> >
> > Zhong Yu
> >
> >
> >
> >
> > On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher
> <niall at npgall.com> wrote:
> >> Actually the read-write-update lock solves that deadlock problem.
> >>
> >> The update lock is not the same as the read lock[1]. Only one
> thread can acquire the update lock at a time. Only the thread
> holding the update lock can acquire the write lock. Threads which
> might need to write, should not acquire read locks but should
> acquire the update lock instead, and the implementation will
> enforce that correct usage with exceptions. So there is not a
> situation where two threads could mutually hold a lock that the
> other is waiting for, which prevents deadlock.
> >>
> >> [1] http://code.google.com/p/concurrent-locks/
> >>
> >>
> >> On 23 Jul 2013, at 02:57, Aaron Grunthal
> <aaron.grunthal at infinite-source.de> wrote:
> >>
> >>> The problem with a read-update is that the lock can never
> guarantee that the update is going to succeed. A write lock is
> generally considered an exclusive lock, think of transaction
> semantics. If two threads hold a read lock and both of them try
> to upgrade to a write lock neither can upgrade since an exclusive
> write lock would violate the shared semantics of a read lock. So
> at least one of them would have to stay in read mode, finished
> their logic in read-only mode and then relinquish the read lock
> before the other thread can even acquire the write lock.
> >>>
> >>> So read-update will always have to support fail-and-retry or
> the logic would have to ensure that only one reader thread would
> try to upgrade to write at any given time and get priority over
> threads that try to acquire a write lock directly.
> >>>
> >>> Of course there are special scenarios where you actually have
> "read up to X in read mode, acquire write mode for X+1" where the
> write lock would not violate violate the semantics of the earlier
> read chunks, but those things are different from a simple
> read-write lock, they're sets of range-locks, such as used by
> database engines.
> >>>
> >>> So as far as I can see the "non-replayable request" issue
> wouldn't be solved by a generic read-tryUpgrade lock.
> >>>
> >>>
> >>> On 23.07.2013 01:43, Niall Gallagher wrote:
> >>>> The motivation for this read-write-update lock was actually a
> >>>> distributed systems problem.
> >>>>
> >>>> The crux of the problem was that it was not feasible to
> "re-play" requests.
> >>>>
> >>>> Broadly, requests entering a front-end system could be classified as
> >>>> being "read-only" (HTTP GET) or "possibly requiring write"
> (HTTP POST).
> >>>> On receiving a request, the front-end system would read from a local
> >>>> data store and then forward the request to back-end systems. The
> >>>> front-end system would gather responses from the back-end systems and
> >>>> from those determine if its local data store needed to be updated.
> >>>>
> >>>> In a system like that, it's not possible for the front-end system to
> >>>> "try" the request using a read lock, and then if it later
> detects that
> >>>> it needs to write, drop the read lock and re-play the request using a
> >>>> write lock. First, because dropping the read lock would allow another
> >>>> thread to steal the write lock, causing lost updates, and
> second because
> >>>> re-playing the request over multiple back-end systems would be
> >>>> expensive. The only solution would be to acquire the write
> lock on every
> >>>> HTTP POST.
> >>>>
> >>>> So I started to view the read-write lock as somewhat limiting.
> >>>>
> >>>> There is also the performance aspect. For simplicity say 50%
> of requests
> >>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
> >>>> write lock which would (1) block all reading threads even if only a
> >>>> small fraction of POSTs would require writes, and (2) would block all
> >>>> reading threads for the entire latency of interactions with back-end
> >>>> servers.
> >>>>
> >>>> A read-write-update lock on the other hand, (1) does not
> block reading
> >>>> threads unless a write actually is performed, and (2) only blocks
> >>>> reading threads from the point at which the lock is
> upgraded, which in
> >>>> the case above is after responses from back-end systems have been
> >>>> gathered and so is for only a fraction of the overall
> processing time.
> >>>>
> >>>> I think the example scales-down to simpler situations too. The key
> >>>> advantage of a read-write-update lock is in read-before-write access
> >>>> patterns.
> >>>>
> >>>> Read-before-write is common. Many applications do it in
> three steps: (1)
> >>>> read data, (2) do some computations on the data, (3) write out the
> >>>> resulting data. A conventional read-write lock does not
> support this use
> >>>> case well.
> >>>>
> >>>> At the very least, a read-write-update lock would increase
> concurrency
> >>>> by not blocking reading threads until a writing thread
> reaches step 3;
> >>>> concurrent reads would be allowed during steps 1 & 2. A conventional
> >>>> read-write lock would block reads for all three steps.
> >>>>
> >>>> I think also in some systems it's basically tricky to
> implement re-play
> >>>> functionality, so a read-write-update lock[1] seems like a nice
> >>>> proposal. YMMV!
> >>>>
> >>>> Niall Gallagher
> >>>> www.npgall.com <http://www.npgall.com>
> >>>>
> >>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
> >>>> for Java
> >>>> http://code.google.com/p/concurrent-locks/
> >>>>
> >>>>
> >>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
> >>>> <mailto:nathan.reynolds at oracle.com>> wrote:
> >>>>
> >>>>> I ran into upgrading a lock problem a while back.  The
> solution I took
> >>>>> is first have the thread read acquire the lock.  Then if the thread
> >>>>> detects that it needs the write lock, it releases the read lock and
> >>>>> starts over by acquiring the write lock.  Of course, all of the data
> >>>>> the thread collected in the read lock is considered invalid.  This
> >>>>> greatly improved scalability and the "upgrade" didn't hurt
> >>>>> performance.  This is because  writes mostly happen at start up and
> >>>>> are rare otherwise.
> >>>>>
> >>>>> The above logic works great and there hasn't been a need for a
> >>>>> read-write-update lock.  The problem in my situation is that I can't
> >>>>> predict at the time of acquiring the lock if I will need to do any
> >>>>> writes.  The thread has to check the protected state and then decide
> >>>>> if any writes are necessary.  (I would guess this scenario
> is true of
> >>>>> most caches.)  If all threads do update acquires, then I am back to
> >>>>> the same contention I had with all threads doing write acquires.
> >>>>>
> >>>>> I haven't needed this kind of lock.  Every time I run into lock
> >>>>> contention, I always solve the problem without it.  If I had such a
> >>>>> lock in my toolbox, maybe I would select it.
> >>>>>
> >>>>> Can you give a good example of where update acquires can actually
> >>>>> help?  A toy example is one method which always needs to do
> reads and
> >>>>> another method which has to do a lot of reads with a very small
> >>>>> portion to do writes.  The first method is called very frequently
> >>>>> relative to the second method.  Is there such a situation
> in software?
> >>>>> -Nathan
> >>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
> >>>>>> Hi,
> >>>>>>
> >>>>>> I guess it must have been discussed when
> ReentrantReadWriteLock was being written, the limitation that the
> read lock cannot be upgraded to a write lock.
> >>>>>>
> >>>>>> Two threads hold a read lock, both then try to acquire the
> write lock -> deadlock. So if a read-mostly thread might ever
> need write access, it must either hold a write lock for the whole
> time (blocking other concurrent readers for its whole duration),
> or it must drop the read lock before acquiring the write lock,
> with the risk that another thread might steal the write lock from
> it rendering any data it read as stale.
> >>>>>>
> >>>>>> I've written an extension to the basic readers-writer lock
> concept: a read-write-update lock for Java:
> ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
> >>>>>>
> >>>>>> I was wondering if people on this mailing list would like
> to code review it, or get involved in the project, before I make
> a 1.0 release to Maven Central?
> >>>>>>
> >>>>>> The idea for a read-write-update lock is not new. I see a
> reference to it on the Linux kernel mailing list from circa 2000
> here[2]. Maybe the concept goes by other names also?
> >>>>>>
> >>>>>> Some differences between my implementation and the Linux
> kernel mailing list discussion:
> >>>>>> - My implementation is reentrant, which means you actually
> can acquire the Mutex again if you already hold a Write lock
> >>>>>> - I didn't see any reason for Nothing -> Update to acquire
> both the Mutex and Read locks, so my implementation only acquires
> Mutex (which is enough to prevent other threads acquiring the
> Write lock anyway). In fact if it did acquire both, deadlock
> would occur in reentrant scenario Nothing -> Update (lock Mutex,
> lock Read), Update -> Update (lock Mutex again, lock Read again)
> followed by Update -> Write (drop Read, but hold count remains at
> 1, lock write = deadlock).
> >>>>>>
> >>>>>> There is also an implementation of CompositeLock in the
> project for group-locking and group-unlocking collections of
> locks with rollback support, which I plan to use for locking
> nodes in hierarchical structures such as trees (I'm the author of
> Concurrent-Trees[3]).
> >>>>>>
> >>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK
> ReentrantReadWriteLock as much as possible, so you'll see the
> code is quite simple. So another question is do you think this
> kind of R-W-U-Lock might ever make it into the JDK, or is it
> still too esoteric?
> >>>>>>
> >>>>>> Best regards,
> >>>>>>
> >>>>>> Niall Gallagher
> >>>>>> www.npgall.com
> >>>>>>
> >>>>>> [1] Concurrent-Locks: Read-write-update / upgradable
> read-write locks for Java
> >>>>>>    http://code.google.com/p/concurrent-locks/
> >>>>>>
> >>>>>> [2] Linux kernel mailing list: Read/Write locks that can
> be changed into each other
> >>>>>>    http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
> >>>>>>
> >>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent
> Suffix Trees for Java
> >>>>>>    http://code.google.com/p/concurrent-trees/
> >>>>>>
> >>>>>>
> >>>>>> _______________________________________________
> >>>>>> Concurrency-interest mailing list
> >>>>>> Concurrency-interest at cs.oswego.edu
> >>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>>>
> >>>>>
> >>>>> _______________________________________________
> >>>>> Concurrency-interest mailing list
> >>>>> Concurrency-interest at cs.oswego.edu
> >>>>> <mailto:Concurrency-interest at cs.oswego.edu>
> >>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> Concurrency-interest mailing list
> >>>> Concurrency-interest at cs.oswego.edu
> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> -----
> No virus found in this message.
> Checked by AVG - www.avg.com
> Version: 2013.0.3349 / Virus Database: 3204/6509 - Release Date: 07/21/13
>


From heinz at javaspecialists.eu  Tue Jul 23 02:07:46 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 23 Jul 2013 09:07:46 +0300
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
	<0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
Message-ID: <CACLL95qYWkQtohzCy6S4bTvr463GwxKqEG-+FSv27RAcx_Pe8g@mail.gmail.com>

Java 8 StampedLock allows you to try to upgrade from a read lock to a
write lock.  This might be your solution, instead of rolling your own.

Heinz

On 23/07/2013, Niall Gallagher <niall at npgall.com> wrote:
> Hi Zhong,
>
> Not exactly, but that is very close indeed. The only difference is in POST
> it does not acquire the read lock, so it's simply like this:
>
>    updateLock.lock();  // exclusive
>   ...
>    if(...)  // occasionally
>        writeLock.lock();  // upgrade
>        ...
>        writeLock.unlock();
>    ...
>    updateLock.unlock();
>
>
> I initially did think that I'd have to roll my own ReentrantReadWriteLock
> for the reason you mentioned, but then I found a way around it. My reasoning
> is, it's not necessary to acquire an actual read lock from
> ReentrantReadWriteLock after the update lock has been acquired, because here
> we basically invent a new type of lock anyway (the update lock) so we just
> say in the documentation that the update lock gives the application
> "permission" to read but not write. So the number of threads with read
> "permission" at any one time can be <number of threads holding regular read
> locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update
> lock>.
>
> The write lock is only granted when two conditions hold: the requesting
> thread must hold the update lock (enforced in the logic in RWULock), and
> other threads holding regular read locks must release those read locks
> (enforced by ReentrantReadWriteLock itself which RWULock delegates to
> internally). So this way it re-uses the ReentrantReadWriteLock of the JDK,
> no need for a custom implementation.
>
> Best regards,
> Niall Gallagher
>
> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Hi Niall, my understanding is that you are trying to implement
>> something like this
>>
>> Lock readLock = ...;
>> Lock writeLock = ...;
>> Lock updateLock = ...;
>>
>> GET:
>>
>>    readLock.lock();
>>    ...
>>    readLock.unlock();
>>
>>
>> POST:
>>
>>    updateLock.lock();  // exclusive
>>    readLock.lock();
>>   ...
>>    if(...)  // occasionally
>>        writeLock.lock();  // upgrade
>>        ...
>>        writeLock.unlock();
>>    ...
>>    readLock.unlock();
>>    updateLock.unlock();
>>
>>
>> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
>> you need to roll your own. Is that correct?
>>
>> Zhong Yu
>>
>>
>>
>>
>> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com>
>> wrote:
>>> Actually the read-write-update lock solves that deadlock problem.
>>>
>>> The update lock is not the same as the read lock[1]. Only one thread can
>>> acquire the update lock at a time. Only the thread holding the update
>>> lock can acquire the write lock. Threads which might need to write,
>>> should not acquire read locks but should acquire the update lock instead,
>>> and the implementation will enforce that correct usage with exceptions.
>>> So there is not a situation where two threads could mutually hold a lock
>>> that the other is waiting for, which prevents deadlock.
>>>
>>> [1] http://code.google.com/p/concurrent-locks/
>>>
>>>
>>> On 23 Jul 2013, at 02:57, Aaron Grunthal
>>> <aaron.grunthal at infinite-source.de> wrote:
>>>
>>>> The problem with a read-update is that the lock can never guarantee that
>>>> the update is going to succeed. A write lock is generally considered an
>>>> exclusive lock, think of transaction semantics. If two threads hold a
>>>> read lock and both of them try to upgrade to a write lock neither can
>>>> upgrade since an exclusive write lock would violate the shared semantics
>>>> of a read lock. So at least one of them would have to stay in read mode,
>>>> finished their logic in read-only mode and then relinquish the read lock
>>>> before the other thread can even acquire the write lock.
>>>>
>>>> So read-update will always have to support fail-and-retry or the logic
>>>> would have to ensure that only one reader thread would try to upgrade to
>>>> write at any given time and get priority over threads that try to
>>>> acquire a write lock directly.
>>>>
>>>> Of course there are special scenarios where you actually have "read up
>>>> to X in read mode, acquire write mode for X+1" where the write lock
>>>> would not violate violate the semantics of the earlier read chunks, but
>>>> those things are different from a simple read-write lock, they're sets
>>>> of range-locks, such as used by database engines.
>>>>
>>>> So as far as I can see the "non-replayable request" issue wouldn't be
>>>> solved by a generic read-tryUpgrade lock.
>>>>
>>>>
>>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>>> The motivation for this read-write-update lock was actually a
>>>>> distributed systems problem.
>>>>>
>>>>> The crux of the problem was that it was not feasible to "re-play"
>>>>> requests.
>>>>>
>>>>> Broadly, requests entering a front-end system could be classified as
>>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP
>>>>> POST).
>>>>> On receiving a request, the front-end system would read from a local
>>>>> data store and then forward the request to back-end systems. The
>>>>> front-end system would gather responses from the back-end systems and
>>>>> from those determine if its local data store needed to be updated.
>>>>>
>>>>> In a system like that, it's not possible for the front-end system to
>>>>> "try" the request using a read lock, and then if it later detects that
>>>>> it needs to write, drop the read lock and re-play the request using a
>>>>> write lock. First, because dropping the read lock would allow another
>>>>> thread to steal the write lock, causing lost updates, and second
>>>>> because
>>>>> re-playing the request over multiple back-end systems would be
>>>>> expensive. The only solution would be to acquire the write lock on
>>>>> every
>>>>> HTTP POST.
>>>>>
>>>>> So I started to view the read-write lock as somewhat limiting.
>>>>>
>>>>> There is also the performance aspect. For simplicity say 50% of
>>>>> requests
>>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>>> write lock which would (1) block all reading threads even if only a
>>>>> small fraction of POSTs would require writes, and (2) would block all
>>>>> reading threads for the entire latency of interactions with back-end
>>>>> servers.
>>>>>
>>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>>> threads unless a write actually is performed, and (2) only blocks
>>>>> reading threads from the point at which the lock is upgraded, which in
>>>>> the case above is after responses from back-end systems have been
>>>>> gathered and so is for only a fraction of the overall processing time.
>>>>>
>>>>> I think the example scales-down to simpler situations too. The key
>>>>> advantage of a read-write-update lock is in read-before-write access
>>>>> patterns.
>>>>>
>>>>> Read-before-write is common. Many applications do it in three steps:
>>>>> (1)
>>>>> read data, (2) do some computations on the data, (3) write out the
>>>>> resulting data. A conventional read-write lock does not support this
>>>>> use
>>>>> case well.
>>>>>
>>>>> At the very least, a read-write-update lock would increase concurrency
>>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>>> read-write lock would block reads for all three steps.
>>>>>
>>>>> I think also in some systems it's basically tricky to implement
>>>>> re-play
>>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>>> proposal. YMMV!
>>>>>
>>>>> Niall Gallagher
>>>>> www.npgall.com <http://www.npgall.com>
>>>>>
>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>> for Java
>>>>> http://code.google.com/p/concurrent-locks/
>>>>>
>>>>>
>>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>
>>>>>> I ran into upgrading a lock problem a while back.  The solution I
>>>>>> took
>>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>>> detects that it needs the write lock, it releases the read lock and
>>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>>> the thread collected in the read lock is considered invalid.  This
>>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>>> performance.  This is because  writes mostly happen at start up and
>>>>>> are rare otherwise.
>>>>>>
>>>>>> The above logic works great and there hasn't been a need for a
>>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>>> writes.  The thread has to check the protected state and then decide
>>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>>> the same contention I had with all threads doing write acquires.
>>>>>>
>>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>>> contention, I always solve the problem without it.  If I had such a
>>>>>> lock in my toolbox, maybe I would select it.
>>>>>>
>>>>>> Can you give a good example of where update acquires can actually
>>>>>> help?  A toy example is one method which always needs to do reads and
>>>>>> another method which has to do a lot of reads with a very small
>>>>>> portion to do writes.  The first method is called very frequently
>>>>>> relative to the second method.  Is there such a situation in
>>>>>> software?
>>>>>> -Nathan
>>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>>> Hi,
>>>>>>>
>>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was
>>>>>>> being written, the limitation that the read lock cannot be upgraded
>>>>>>> to a write lock.
>>>>>>>
>>>>>>> Two threads hold a read lock, both then try to acquire the write lock
>>>>>>> -> deadlock. So if a read-mostly thread might ever need write access,
>>>>>>> it must either hold a write lock for the whole time (blocking other
>>>>>>> concurrent readers for its whole duration), or it must drop the read
>>>>>>> lock before acquiring the write lock, with the risk that another
>>>>>>> thread might steal the write lock from it rendering any data it read
>>>>>>> as stale.
>>>>>>>
>>>>>>> I've written an extension to the basic readers-writer lock concept: a
>>>>>>> read-write-update lock for Java: ReentrantReadWriteUpdateLock in
>>>>>>> Concurrent-Locks on Google Code[1].
>>>>>>>
>>>>>>> I was wondering if people on this mailing list would like to code
>>>>>>> review it, or get involved in the project, before I make a 1.0
>>>>>>> release to Maven Central?
>>>>>>>
>>>>>>> The idea for a read-write-update lock is not new. I see a reference
>>>>>>> to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe
>>>>>>> the concept goes by other names also?
>>>>>>>
>>>>>>> Some differences between my implementation and the Linux kernel
>>>>>>> mailing list discussion:
>>>>>>> - My implementation is reentrant, which means you actually can
>>>>>>> acquire the Mutex again if you already hold a Write lock
>>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the
>>>>>>> Mutex and Read locks, so my implementation only acquires Mutex (which
>>>>>>> is enough to prevent other threads acquiring the Write lock anyway).
>>>>>>> In fact if it did acquire both, deadlock would occur in reentrant
>>>>>>> scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update
>>>>>>> (lock Mutex again, lock Read again) followed by Update -> Write (drop
>>>>>>> Read, but hold count remains at 1, lock write = deadlock).
>>>>>>>
>>>>>>> There is also an implementation of CompositeLock in the project for
>>>>>>> group-locking and group-unlocking collections of locks with rollback
>>>>>>> support, which I plan to use for locking nodes in hierarchical
>>>>>>> structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>>>
>>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK
>>>>>>> ReentrantReadWriteLock as much as possible, so you'll see the code is
>>>>>>> quite simple. So another question is do you think this kind of
>>>>>>> R-W-U-Lock might ever make it into the JDK, or is it still too
>>>>>>> esoteric?
>>>>>>>
>>>>>>> Best regards,
>>>>>>>
>>>>>>> Niall Gallagher
>>>>>>> www.npgall.com
>>>>>>>
>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>>>> for Java
>>>>>>>    http://code.google.com/p/concurrent-locks/
>>>>>>>
>>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed
>>>>>>> into each other
>>>>>>>    http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>>>
>>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees
>>>>>>> for Java
>>>>>>>    http://code.google.com/p/concurrent-trees/
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion 2005-2013
JavaOne Rockstar Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz

From martinrb at google.com  Tue Jul 23 02:29:44 2013
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 22 Jul 2013 23:29:44 -0700
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEECJPAA.davidcholmes@aapt.net.au>
References: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
	<NFBBKALFDCPFIDBNKAPCMEECJPAA.davidcholmes@aapt.net.au>
Message-ID: <CA+kOe08YJqk4VrFnfC4NcaXGYLfsynhLkwx=rPsWduPDxJ_JpA@mail.gmail.com>

On Mon, Jul 22, 2013 at 10:41 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> Niall,
>
> Under the Java Memory Model a thread that acquires the updateLock is not
> guaranteed to see any updates performed under the WriteLock. You would need
> to actually acquire the readLock to be strictly correct.
>

I don't see this.  After doing updates under the WriteLock, both the
WriteLock and the UpdateLock are unlocked, providing visibility to changes
to readers acquiring either the ReadLock or the UpdateLock.  Assuming no
one acquires the WriteLock without first acquiring UpdateLock.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130722/b48861a7/attachment.html>

From davidcholmes at aapt.net.au  Tue Jul 23 02:35:29 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 23 Jul 2013 16:35:29 +1000
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <CA+kOe08YJqk4VrFnfC4NcaXGYLfsynhLkwx=rPsWduPDxJ_JpA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEEEJPAA.davidcholmes@aapt.net.au>

Martin,

"Assuming no one acquires the WriteLock without first acquiring UpdateLock."

I don't recall seeing anything that indicates you can't just ask for the
WriteLock.

David

 -----Original Message-----
From: Martin Buchholz [mailto:martinrb at google.com]
Sent: Tuesday, 23 July 2013 4:30 PM
To: David Holmes
Cc: Niall Gallagher; concurrency-interest
Subject: Re: [concurrency-interest] Read-write-update lock







  On Mon, Jul 22, 2013 at 10:41 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Niall,

    Under the Java Memory Model a thread that acquires the updateLock is not
    guaranteed to see any updates performed under the WriteLock. You would
need
    to actually acquire the readLock to be strictly correct.



  I don't see this.  After doing updates under the WriteLock, both the
WriteLock and the UpdateLock are unlocked, providing visibility to changes
to readers acquiring either the ReadLock or the UpdateLock.  Assuming no one
acquires the WriteLock without first acquiring UpdateLock.
  No virus found in this message.
  Checked by AVG - www.avg.com
  Version: 2013.0.3349 / Virus Database: 3204/6509 - Release Date: 07/21/13
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130723/78c97b5e/attachment.html>

From oleksandr.otenko at oracle.com  Tue Jul 23 07:50:51 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 23 Jul 2013 12:50:51 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
	<0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
Message-ID: <51EE6E1B.3080000@oracle.com>

Exclusive decision separate from exclusive update. Neat!

(I wouldn't call the exclusive decision part "update")

Alex

On 23/07/2013 06:28, Niall Gallagher wrote:
> Hi Zhong,
>
> Not exactly, but that is very close indeed. The only difference is in POST it does not acquire the read lock, so it's simply like this:
>
>     updateLock.lock();  // exclusive
>    ...
>     if(...)  // occasionally
>         writeLock.lock();  // upgrade
>         ...
>         writeLock.unlock();
>     ...
>     updateLock.unlock();
>
>
> I initially did think that I'd have to roll my own ReentrantReadWriteLock for the reason you mentioned, but then I found a way around it. My reasoning is, it's not necessary to acquire an actual read lock from ReentrantReadWriteLock after the update lock has been acquired, because here we basically invent a new type of lock anyway (the update lock) so we just say in the documentation that the update lock gives the application "permission" to read but not write. So the number of threads with read "permission" at any one time can be <number of threads holding regular read locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update lock>.
>
> The write lock is only granted when two conditions hold: the requesting thread must hold the update lock (enforced in the logic in RWULock), and other threads holding regular read locks must release those read locks (enforced by ReentrantReadWriteLock itself which RWULock delegates to internally). So this way it re-uses the ReentrantReadWriteLock of the JDK, no need for a custom implementation.
>
> Best regards,
> Niall Gallagher
>
> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Hi Niall, my understanding is that you are trying to implement
>> something like this
>>
>> Lock readLock = ...;
>> Lock writeLock = ...;
>> Lock updateLock = ...;
>>
>> GET:
>>
>>     readLock.lock();
>>     ...
>>     readLock.unlock();
>>
>>
>> POST:
>>
>>     updateLock.lock();  // exclusive
>>     readLock.lock();
>>    ...
>>     if(...)  // occasionally
>>         writeLock.lock();  // upgrade
>>         ...
>>         writeLock.unlock();
>>     ...
>>     readLock.unlock();
>>     updateLock.unlock();
>>
>>
>> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
>> you need to roll your own. Is that correct?
>>
>> Zhong Yu
>>
>>
>>
>>
>> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
>>> Actually the read-write-update lock solves that deadlock problem.
>>>
>>> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>>>
>>> [1] http://code.google.com/p/concurrent-locks/
>>>
>>>
>>> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>>>
>>>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>>>
>>>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>>>
>>>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>>>
>>>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>>>
>>>>
>>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>>> The motivation for this read-write-update lock was actually a
>>>>> distributed systems problem.
>>>>>
>>>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>>>
>>>>> Broadly, requests entering a front-end system could be classified as
>>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>>>> On receiving a request, the front-end system would read from a local
>>>>> data store and then forward the request to back-end systems. The
>>>>> front-end system would gather responses from the back-end systems and
>>>>> from those determine if its local data store needed to be updated.
>>>>>
>>>>> In a system like that, it's not possible for the front-end system to
>>>>> "try" the request using a read lock, and then if it later detects that
>>>>> it needs to write, drop the read lock and re-play the request using a
>>>>> write lock. First, because dropping the read lock would allow another
>>>>> thread to steal the write lock, causing lost updates, and second because
>>>>> re-playing the request over multiple back-end systems would be
>>>>> expensive. The only solution would be to acquire the write lock on every
>>>>> HTTP POST.
>>>>>
>>>>> So I started to view the read-write lock as somewhat limiting.
>>>>>
>>>>> There is also the performance aspect. For simplicity say 50% of requests
>>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>>> write lock which would (1) block all reading threads even if only a
>>>>> small fraction of POSTs would require writes, and (2) would block all
>>>>> reading threads for the entire latency of interactions with back-end
>>>>> servers.
>>>>>
>>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>>> threads unless a write actually is performed, and (2) only blocks
>>>>> reading threads from the point at which the lock is upgraded, which in
>>>>> the case above is after responses from back-end systems have been
>>>>> gathered and so is for only a fraction of the overall processing time.
>>>>>
>>>>> I think the example scales-down to simpler situations too. The key
>>>>> advantage of a read-write-update lock is in read-before-write access
>>>>> patterns.
>>>>>
>>>>> Read-before-write is common. Many applications do it in three steps: (1)
>>>>> read data, (2) do some computations on the data, (3) write out the
>>>>> resulting data. A conventional read-write lock does not support this use
>>>>> case well.
>>>>>
>>>>> At the very least, a read-write-update lock would increase concurrency
>>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>>> read-write lock would block reads for all three steps.
>>>>>
>>>>> I think also in some systems it's basically tricky to implement re-play
>>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>>> proposal. YMMV!
>>>>>
>>>>> Niall Gallagher
>>>>> www.npgall.com <http://www.npgall.com>
>>>>>
>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>> for Java
>>>>> http://code.google.com/p/concurrent-locks/
>>>>>
>>>>>
>>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>
>>>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>>> detects that it needs the write lock, it releases the read lock and
>>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>>> the thread collected in the read lock is considered invalid.  This
>>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>>> performance.  This is because  writes mostly happen at start up and
>>>>>> are rare otherwise.
>>>>>>
>>>>>> The above logic works great and there hasn't been a need for a
>>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>>> writes.  The thread has to check the protected state and then decide
>>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>>> the same contention I had with all threads doing write acquires.
>>>>>>
>>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>>> contention, I always solve the problem without it.  If I had such a
>>>>>> lock in my toolbox, maybe I would select it.
>>>>>>
>>>>>> Can you give a good example of where update acquires can actually
>>>>>> help?  A toy example is one method which always needs to do reads and
>>>>>> another method which has to do a lot of reads with a very small
>>>>>> portion to do writes.  The first method is called very frequently
>>>>>> relative to the second method.  Is there such a situation in software?
>>>>>> -Nathan
>>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>>> Hi,
>>>>>>>
>>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>>>
>>>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>>>
>>>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>>>
>>>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>>>
>>>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>>>
>>>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>>>
>>>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>>>
>>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>>>
>>>>>>> Best regards,
>>>>>>>
>>>>>>> Niall Gallagher
>>>>>>> www.npgall.com
>>>>>>>
>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>>>     http://code.google.com/p/concurrent-locks/
>>>>>>>
>>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>>>     http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>>>
>>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>>>     http://code.google.com/p/concurrent-trees/
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From zhong.j.yu at gmail.com  Tue Jul 23 10:06:13 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 23 Jul 2013 09:06:13 -0500
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
	<0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
Message-ID: <CACuKZqFJHfOo4kLa=8z31TO8+5OfVeHZ+AL7QwFafHb6JZHQDA@mail.gmail.com>

same concept in RDBMS -
http://docs.oracle.com/javadb/10.6.2.1/devguide/cdevconcepts36402.html

On Tue, Jul 23, 2013 at 12:28 AM, Niall Gallagher <niall at npgall.com> wrote:
> Hi Zhong,
>
> Not exactly, but that is very close indeed. The only difference is in POST it does not acquire the read lock, so it's simply like this:
>
>    updateLock.lock();  // exclusive
>   ...
>    if(...)  // occasionally
>        writeLock.lock();  // upgrade
>        ...
>        writeLock.unlock();
>    ...
>    updateLock.unlock();
>
>
> I initially did think that I'd have to roll my own ReentrantReadWriteLock for the reason you mentioned, but then I found a way around it. My reasoning is, it's not necessary to acquire an actual read lock from ReentrantReadWriteLock after the update lock has been acquired, because here we basically invent a new type of lock anyway (the update lock) so we just say in the documentation that the update lock gives the application "permission" to read but not write. So the number of threads with read "permission" at any one time can be <number of threads holding regular read locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update lock>.
>
> The write lock is only granted when two conditions hold: the requesting thread must hold the update lock (enforced in the logic in RWULock), and other threads holding regular read locks must release those read locks (enforced by ReentrantReadWriteLock itself which RWULock delegates to internally). So this way it re-uses the ReentrantReadWriteLock of the JDK, no need for a custom implementation.
>
> Best regards,
> Niall Gallagher
>
> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Hi Niall, my understanding is that you are trying to implement
>> something like this
>>
>> Lock readLock = ...;
>> Lock writeLock = ...;
>> Lock updateLock = ...;
>>
>> GET:
>>
>>    readLock.lock();
>>    ...
>>    readLock.unlock();
>>
>>
>> POST:
>>
>>    updateLock.lock();  // exclusive
>>    readLock.lock();
>>   ...
>>    if(...)  // occasionally
>>        writeLock.lock();  // upgrade
>>        ...
>>        writeLock.unlock();
>>    ...
>>    readLock.unlock();
>>    updateLock.unlock();
>>
>>
>> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
>> you need to roll your own. Is that correct?
>>
>> Zhong Yu
>>
>>
>>
>>
>> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
>>> Actually the read-write-update lock solves that deadlock problem.
>>>
>>> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>>>
>>> [1] http://code.google.com/p/concurrent-locks/
>>>
>>>
>>> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>>>
>>>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>>>
>>>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>>>
>>>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>>>
>>>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>>>
>>>>
>>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>>> The motivation for this read-write-update lock was actually a
>>>>> distributed systems problem.
>>>>>
>>>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>>>
>>>>> Broadly, requests entering a front-end system could be classified as
>>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>>>> On receiving a request, the front-end system would read from a local
>>>>> data store and then forward the request to back-end systems. The
>>>>> front-end system would gather responses from the back-end systems and
>>>>> from those determine if its local data store needed to be updated.
>>>>>
>>>>> In a system like that, it's not possible for the front-end system to
>>>>> "try" the request using a read lock, and then if it later detects that
>>>>> it needs to write, drop the read lock and re-play the request using a
>>>>> write lock. First, because dropping the read lock would allow another
>>>>> thread to steal the write lock, causing lost updates, and second because
>>>>> re-playing the request over multiple back-end systems would be
>>>>> expensive. The only solution would be to acquire the write lock on every
>>>>> HTTP POST.
>>>>>
>>>>> So I started to view the read-write lock as somewhat limiting.
>>>>>
>>>>> There is also the performance aspect. For simplicity say 50% of requests
>>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>>> write lock which would (1) block all reading threads even if only a
>>>>> small fraction of POSTs would require writes, and (2) would block all
>>>>> reading threads for the entire latency of interactions with back-end
>>>>> servers.
>>>>>
>>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>>> threads unless a write actually is performed, and (2) only blocks
>>>>> reading threads from the point at which the lock is upgraded, which in
>>>>> the case above is after responses from back-end systems have been
>>>>> gathered and so is for only a fraction of the overall processing time.
>>>>>
>>>>> I think the example scales-down to simpler situations too. The key
>>>>> advantage of a read-write-update lock is in read-before-write access
>>>>> patterns.
>>>>>
>>>>> Read-before-write is common. Many applications do it in three steps: (1)
>>>>> read data, (2) do some computations on the data, (3) write out the
>>>>> resulting data. A conventional read-write lock does not support this use
>>>>> case well.
>>>>>
>>>>> At the very least, a read-write-update lock would increase concurrency
>>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>>> read-write lock would block reads for all three steps.
>>>>>
>>>>> I think also in some systems it's basically tricky to implement re-play
>>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>>> proposal. YMMV!
>>>>>
>>>>> Niall Gallagher
>>>>> www.npgall.com <http://www.npgall.com>
>>>>>
>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>> for Java
>>>>> http://code.google.com/p/concurrent-locks/
>>>>>
>>>>>
>>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>
>>>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>>> detects that it needs the write lock, it releases the read lock and
>>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>>> the thread collected in the read lock is considered invalid.  This
>>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>>> performance.  This is because  writes mostly happen at start up and
>>>>>> are rare otherwise.
>>>>>>
>>>>>> The above logic works great and there hasn't been a need for a
>>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>>> writes.  The thread has to check the protected state and then decide
>>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>>> the same contention I had with all threads doing write acquires.
>>>>>>
>>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>>> contention, I always solve the problem without it.  If I had such a
>>>>>> lock in my toolbox, maybe I would select it.
>>>>>>
>>>>>> Can you give a good example of where update acquires can actually
>>>>>> help?  A toy example is one method which always needs to do reads and
>>>>>> another method which has to do a lot of reads with a very small
>>>>>> portion to do writes.  The first method is called very frequently
>>>>>> relative to the second method.  Is there such a situation in software?
>>>>>> -Nathan
>>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>>> Hi,
>>>>>>>
>>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>>>
>>>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>>>
>>>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>>>
>>>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>>>
>>>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>>>
>>>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>>>
>>>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>>>
>>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>>>
>>>>>>> Best regards,
>>>>>>>
>>>>>>> Niall Gallagher
>>>>>>> www.npgall.com
>>>>>>>
>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>>>    http://code.google.com/p/concurrent-locks/
>>>>>>>
>>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>>>    http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>>>
>>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>>>    http://code.google.com/p/concurrent-trees/
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From niall at npgall.com  Tue Jul 23 15:07:41 2013
From: niall at npgall.com (Niall Gallagher)
Date: Tue, 23 Jul 2013 20:07:41 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEEEJPAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEEEJPAA.davidcholmes@aapt.net.au>
Message-ID: <98CA5663-CB8D-46B0-AFAB-B5E95849F9A3@npgall.com>

It is allowed to ask for the write lock without first acquiring the update lock. None -> Write is supported.

However the implementation accounts for the issue of visibility. When the write lock is requested, that code path will acquire the update lock (reentrant-ly / again) before it actually acquires the write lock, regardless of whether or not the thread had earlier acquired the update lock.

When writeLock().unlock() is called, both locks will be unlocked. But If the thread had in fact acquired the update lock in the first place, then its hold count will simply be decremented and so it will still hold the update lock when the downgrade is complete. So basically all writes are guarded by both the update lock and the write lock, and it's not possible to acquire the write lock without first acquiring the update lock. This is a detail of the implementation - the application might be oblivious that this is going on.

You can find the source code here[1].

Niall Gallagher

[1] http://code.google.com/p/concurrent-locks/source/browse/concurrent-locks/trunk/src/main/java/com/googlecode/concurentlocks/ReentrantReadWriteUpdateLock.java


On 23 Jul 2013, at 07:35, David Holmes <davidcholmes at aapt.net.au> wrote:

> Martin,
>  
> "Assuming no one acquires the WriteLock without first acquiring UpdateLock."
>  
> I don't recall seeing anything that indicates you can't just ask for the WriteLock.
>  
> David
>  
>  -----Original Message-----
> From: Martin Buchholz [mailto:martinrb at google.com]
> Sent: Tuesday, 23 July 2013 4:30 PM
> To: David Holmes
> Cc: Niall Gallagher; concurrency-interest
> Subject: Re: [concurrency-interest] Read-write-update lock
> 
> 
> 
> 
> On Mon, Jul 22, 2013 at 10:41 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Niall,
> 
> Under the Java Memory Model a thread that acquires the updateLock is not
> guaranteed to see any updates performed under the WriteLock. You would need
> to actually acquire the readLock to be strictly correct.
> 
> I don't see this.  After doing updates under the WriteLock, both the WriteLock and the UpdateLock are unlocked, providing visibility to changes to readers acquiring either the ReadLock or the UpdateLock.  Assuming no one acquires the WriteLock without first acquiring UpdateLock. 
> No virus found in this message.
> Checked by AVG - www.avg.com
> Version: 2013.0.3349 / Virus Database: 3204/6509 - Release Date: 07/21/13
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130723/d43c59d5/attachment.html>

From pramalhe at gmail.com  Tue Jul 23 16:15:36 2013
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Tue, 23 Jul 2013 22:15:36 +0200
Subject: [concurrency-interest] Combining the ScalableRWLock with the
	StampedLock
Message-ID: <CAAApjO1S2F4-O+uGhGfQm7cO-+bLeD_HgzSh_hOzZ9aLc9d4Xg@mail.gmail.com>

 Hello,

Some time ago, we showed an implementation of a Reader-Writer Lock similar
to the one described on the paper "NUMA-Aware Reader-Writer Locks", which
we named ScalableRWLock. One of the faults that was pointed out of that
implementation was that the fairness proprieties of the ScalableRWLock are
poor, partly because it has a "statistical" Writer-Preference, and because
it doesn't perform well with a large number of Writes, particularly when
compared with other recent locks like the StampedLock.
To solve these issues, we developed a new solution that combines the
StampedLock and the ScalableRWLock into one, thus providing the best of
both worlds, i.e. fairness and decent performance with a high number of
Writes, and scalability with a high number of Reads.
We called this solution the ScalableStampedRWLock and the source code is
available here as part of the new release of Concurrency Freaks Library:
https://sourceforge.net/projects/ccfreaks/files/java/src/com/concurrencyfreaks/locks/ScalableStampedRWLock.java/download
and the reentrant version:
https://sourceforge.net/projects/ccfreaks/files/java/src/com/concurrencyfreaks/locks/ScalableStampedReentrantRWLock.java/download

The main change in this new lock is in the combination technique which is
similar to the one described in the paper mentioned above, and it consists
of replacing the CAS loop on the exclusiveLock() with a call to
StampedLock.writeLock(), and on the sharedLock(), instead of yielding when
there is a Writer, the Reader will go directly to StampedLock.readLock().
With this change, we keep much of the performance provided by the
ScalableRWLock when the number of Writers is low, and at the same time we
get most of the advantages that the StampedLock offers: fairness; the
park/unpark queuing mechanism; spin-lock for a random time before park(),
etc.


A performance study was done on an 80 core machine using our
microbenchmark, and can be seen at this link:
http://concurrencyfreaks.blogspot.fr/2013/04/performance-plots-of-scalable-stamped.html
The summary of this particular benchmark is, that the new
ScalableStampedRWLock looses at most 40% of the performance when compared
with the StampedLock, and with a low number of Writes it can easily provide
3 times the performance, or in certain scenarios even go up to 800 times
the performance of the StampedLock when there is a very large number of
threads, mostly doing Reads and only a few occasional Writes.
Here are the comparison plots for 4 threads:
http://pramalheshared.s3.amazonaws.com/Concurrency/ScalableStamped/4%20threads.gif
and for 64 threads (this one has a vertical log scale so that we can see
the performance of the runs with a low number of threads):
http://pramalheshared.s3.amazonaws.com/Concurrency/ScalableStamped/64%20threads.gif


We are interested in getting performance results of this new
ScalableStampedRWLock in other people's benchmarks or applications, so if
you have an application that needs a Reader-Writer lock with the
proprieties mentioned below, give it a try and let us know how it turned
out. We recommend the usage of the ScalableStampedRWLock in the following
scenarios:
- You don't know the Read/Write pattern of your application, or you know
that it can vary from many Writes to few Writes (if you know that there are
always as many Writes as Reads then you're better off using the
StampedLock).
- Fairness in the lock is important in your application (the
ScalableStampedRWLock provides fairness guarantees approximate to the
StampedLock).
- You need a reentrant/recursive lock (use the
ScalableStampedReentrantRWLock).

We believe that due to its fairness (inherited from the StampedLock) and
good scalability with low Writes, the ScalableStampedRWLock can provide a
valuable wide-usage lock.


Thanks,
Pedro & Andreia
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130723/01e7b2de/attachment.html>

From niall at npgall.com  Tue Jul 23 17:16:03 2013
From: niall at npgall.com (Niall Gallagher)
Date: Tue, 23 Jul 2013 22:16:03 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <CACuKZqFJHfOo4kLa=8z31TO8+5OfVeHZ+AL7QwFafHb6JZHQDA@mail.gmail.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
	<0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
	<CACuKZqFJHfOo4kLa=8z31TO8+5OfVeHZ+AL7QwFafHb6JZHQDA@mail.gmail.com>
Message-ID: <4C5361F1-941A-42FB-878E-4B5B6619CAC2@npgall.com>

Yes indeed, that is the same concept.

I mentioned in my earlier post that it's not a new concept, as the earliest reference to it that I could find, was on the Linux kernel mailing list circa 2000 (thirteen years ago). It might be older still. I guess RDBMSes have implemented it, at least via cursors in that case. Your link shows that JavaDB/Derby implements it in Java, just not via a programmatic API (and perhaps not reentrant). I don't know if it was added to the Linux kernel in the end.

I also found ReaderWriterLockSlim[1] on the .Net platform. It is the default readers-writer lock implementation on that platform.

[1] http://msdn.microsoft.com/en-us/library/system.threading.readerwriterlockslim.aspx


On 23 Jul 2013, at 15:06, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> same concept in RDBMS -
> http://docs.oracle.com/javadb/10.6.2.1/devguide/cdevconcepts36402.html
> 
> On Tue, Jul 23, 2013 at 12:28 AM, Niall Gallagher <niall at npgall.com> wrote:
>> Hi Zhong,
>> 
>> Not exactly, but that is very close indeed. The only difference is in POST it does not acquire the read lock, so it's simply like this:
>> 
>>   updateLock.lock();  // exclusive
>>  ...
>>   if(...)  // occasionally
>>       writeLock.lock();  // upgrade
>>       ...
>>       writeLock.unlock();
>>   ...
>>   updateLock.unlock();
>> 
>> 
>> I initially did think that I'd have to roll my own ReentrantReadWriteLock for the reason you mentioned, but then I found a way around it. My reasoning is, it's not necessary to acquire an actual read lock from ReentrantReadWriteLock after the update lock has been acquired, because here we basically invent a new type of lock anyway (the update lock) so we just say in the documentation that the update lock gives the application "permission" to read but not write. So the number of threads with read "permission" at any one time can be <number of threads holding regular read locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update lock>.
>> 
>> The write lock is only granted when two conditions hold: the requesting thread must hold the update lock (enforced in the logic in RWULock), and other threads holding regular read locks must release those read locks (enforced by ReentrantReadWriteLock itself which RWULock delegates to internally). So this way it re-uses the ReentrantReadWriteLock of the JDK, no need for a custom implementation.
>> 
>> Best regards,
>> Niall Gallagher
>> 
>> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>> 
>>> Hi Niall, my understanding is that you are trying to implement
>>> something like this
>>> 
>>> Lock readLock = ...;
>>> Lock writeLock = ...;
>>> Lock updateLock = ...;
>>> 
>>> GET:
>>> 
>>>   readLock.lock();
>>>   ...
>>>   readLock.unlock();
>>> 
>>> 
>>> POST:
>>> 
>>>   updateLock.lock();  // exclusive
>>>   readLock.lock();
>>>  ...
>>>   if(...)  // occasionally
>>>       writeLock.lock();  // upgrade
>>>       ...
>>>       writeLock.unlock();
>>>   ...
>>>   readLock.unlock();
>>>   updateLock.unlock();
>>> 
>>> 
>>> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
>>> you need to roll your own. Is that correct?
>>> 
>>> Zhong Yu
>>> 
>>> 
>>> 
>>> 
>>> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
>>>> Actually the read-write-update lock solves that deadlock problem.
>>>> 
>>>> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>>>> 
>>>> [1] http://code.google.com/p/concurrent-locks/
>>>> 
>>>> 
>>>> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>>>> 
>>>>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>>>> 
>>>>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>>>> 
>>>>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>>>> 
>>>>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>>>> 
>>>>> 
>>>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>>>> The motivation for this read-write-update lock was actually a
>>>>>> distributed systems problem.
>>>>>> 
>>>>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>>>> 
>>>>>> Broadly, requests entering a front-end system could be classified as
>>>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>>>>> On receiving a request, the front-end system would read from a local
>>>>>> data store and then forward the request to back-end systems. The
>>>>>> front-end system would gather responses from the back-end systems and
>>>>>> from those determine if its local data store needed to be updated.
>>>>>> 
>>>>>> In a system like that, it's not possible for the front-end system to
>>>>>> "try" the request using a read lock, and then if it later detects that
>>>>>> it needs to write, drop the read lock and re-play the request using a
>>>>>> write lock. First, because dropping the read lock would allow another
>>>>>> thread to steal the write lock, causing lost updates, and second because
>>>>>> re-playing the request over multiple back-end systems would be
>>>>>> expensive. The only solution would be to acquire the write lock on every
>>>>>> HTTP POST.
>>>>>> 
>>>>>> So I started to view the read-write lock as somewhat limiting.
>>>>>> 
>>>>>> There is also the performance aspect. For simplicity say 50% of requests
>>>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>>>> write lock which would (1) block all reading threads even if only a
>>>>>> small fraction of POSTs would require writes, and (2) would block all
>>>>>> reading threads for the entire latency of interactions with back-end
>>>>>> servers.
>>>>>> 
>>>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>>>> threads unless a write actually is performed, and (2) only blocks
>>>>>> reading threads from the point at which the lock is upgraded, which in
>>>>>> the case above is after responses from back-end systems have been
>>>>>> gathered and so is for only a fraction of the overall processing time.
>>>>>> 
>>>>>> I think the example scales-down to simpler situations too. The key
>>>>>> advantage of a read-write-update lock is in read-before-write access
>>>>>> patterns.
>>>>>> 
>>>>>> Read-before-write is common. Many applications do it in three steps: (1)
>>>>>> read data, (2) do some computations on the data, (3) write out the
>>>>>> resulting data. A conventional read-write lock does not support this use
>>>>>> case well.
>>>>>> 
>>>>>> At the very least, a read-write-update lock would increase concurrency
>>>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>>>> read-write lock would block reads for all three steps.
>>>>>> 
>>>>>> I think also in some systems it's basically tricky to implement re-play
>>>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>>>> proposal. YMMV!
>>>>>> 
>>>>>> Niall Gallagher
>>>>>> www.npgall.com <http://www.npgall.com>
>>>>>> 
>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>>> for Java
>>>>>> http://code.google.com/p/concurrent-locks/
>>>>>> 
>>>>>> 
>>>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>> 
>>>>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>>>> detects that it needs the write lock, it releases the read lock and
>>>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>>>> the thread collected in the read lock is considered invalid.  This
>>>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>>>> performance.  This is because  writes mostly happen at start up and
>>>>>>> are rare otherwise.
>>>>>>> 
>>>>>>> The above logic works great and there hasn't been a need for a
>>>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>>>> writes.  The thread has to check the protected state and then decide
>>>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>>>> the same contention I had with all threads doing write acquires.
>>>>>>> 
>>>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>>>> contention, I always solve the problem without it.  If I had such a
>>>>>>> lock in my toolbox, maybe I would select it.
>>>>>>> 
>>>>>>> Can you give a good example of where update acquires can actually
>>>>>>> help?  A toy example is one method which always needs to do reads and
>>>>>>> another method which has to do a lot of reads with a very small
>>>>>>> portion to do writes.  The first method is called very frequently
>>>>>>> relative to the second method.  Is there such a situation in software?
>>>>>>> -Nathan
>>>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>>>> Hi,
>>>>>>>> 
>>>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>>>> 
>>>>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>>>> 
>>>>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>>>> 
>>>>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>>>> 
>>>>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>>>> 
>>>>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>>>> 
>>>>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>>>> 
>>>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>>>> 
>>>>>>>> Best regards,
>>>>>>>> 
>>>>>>>> Niall Gallagher
>>>>>>>> www.npgall.com
>>>>>>>> 
>>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>>>>   http://code.google.com/p/concurrent-locks/
>>>>>>>> 
>>>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>>>>   http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>>>> 
>>>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>>>>   http://code.google.com/p/concurrent-trees/
>>>>>>>> 
>>>>>>>> 
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>> 
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>> 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From GJAllen at wellington.com  Thu Jul 25 06:20:34 2013
From: GJAllen at wellington.com (Allen, Greg J)
Date: Thu, 25 Jul 2013 10:20:34 +0000
Subject: [concurrency-interest] RecursiveTask example has order of
	evaluation bug?
Message-ID: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>

Isn't there a flaw in the RecursiveTask example in the javadoc?

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html

class Fibonacci extends RecursiveTask<Integer> {
   final int n;
   Fibonacci(int n) { this.n = n; }
   Integer compute() {
     if (n <= 1)
        return n;
     Fibonacci f1 = new Fibonacci(n - 1);
     f1.fork();
     Fibonacci f2 = new Fibonacci(n - 2);
     return f2.compute() + f1.join();

Surely Java order of evaluation is not guaranteed for compute() vs. join() in the return statement. If join gets called first, all parallelism is lost!

I was also a bit surprised to see the default commonPool size was nCPU - 1. Dual core processor are very common, so will likely get slower performance from the parallel sorts than the plain ones. Perhaps there should be a special case for nCPU==2?

Greg

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130725/39da454c/attachment.html>

From mthornton at optrak.co.uk  Thu Jul 25 06:33:53 2013
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Thu, 25 Jul 2013 11:33:53 +0100
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
Message-ID: <CAC_SY70qJ2pxdqgQVgReoJne=N1CngKswBcr0h3Di0c6Yte4NQ@mail.gmail.com>

Unlike C, Java does define the order of evaluation.

Mark Thornton


On Thursday, 25 July 2013, Allen, Greg J wrote:

>  Isn?t there a flaw in the RecursiveTask example in the javadoc?
>
> *
> http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html
> *<http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html>
>
> class Fibonacci extends RecursiveTask<Integer> {
>    final int n;
>    Fibonacci(int n) { this.n = n; }
>    Integer compute() {
>      if (n <= 1)
>         return n;
>      Fibonacci f1 = new Fibonacci(n - 1);
>      f1.fork();
>      Fibonacci f2 = new Fibonacci(n - 2);
>      return f2.compute() + f1.join();
>
> Surely Java order of evaluation is not guaranteed for compute() vs. join()
> in the return statement. If join gets called first, all parallelism is lost!
>
> I was also a bit surprised to see the default commonPool size was nCPU ?
> 1. Dual core processor are very common, so will likely get slower
> performance from the parallel sorts than the plain ones. Perhaps there
> should be a special case for nCPU==2?
>
> Greg
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130725/d05f43cb/attachment.html>

From aleksey.shipilev at oracle.com  Thu Jul 25 06:38:58 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 25 Jul 2013 14:38:58 +0400
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
Message-ID: <51F10042.4010208@oracle.com>

On 07/25/2013 02:20 PM, Allen, Greg J wrote:
> Isn?t there a flaw in the RecursiveTask example in the javadoc?
>  
> _http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html_
>  
> class Fibonacci extends RecursiveTask<Integer> {
>    final int n;
>    Fibonacci(int n) { this.n = n; }
>    Integer compute() {
>      if (n <= 1)
>         return n;
>      Fibonacci f1 = new Fibonacci(n - 1);
>      f1.fork();
>      Fibonacci f2 = new Fibonacci(n - 2);
>      return f2.compute() + f1.join();
>  
> Surely Java order of evaluation is not guaranteed for compute() vs.
> join() in the return statement. If join gets called first, all
> parallelism is lost!

Eh?

http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.7

"The Java programming language guarantees that the operands of operators
appear to be evaluated in a specific evaluation order, namely, from left
to right."

"The Java programming language guarantees that every operand of an
operator (except the conditional operators &&, ||, and ? :) appears to
be fully evaluated before any part of the operation itself is performed."

-Aleksey.

From GJAllen at wellington.com  Thu Jul 25 06:41:17 2013
From: GJAllen at wellington.com (Allen, Greg J)
Date: Thu, 25 Jul 2013 10:41:17 +0000
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <CAC_SY70qJ2pxdqgQVgReoJne=N1CngKswBcr0h3Di0c6Yte4NQ@mail.gmail.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
	<CAC_SY70qJ2pxdqgQVgReoJne=N1CngKswBcr0h3Di0c6Yte4NQ@mail.gmail.com>
Message-ID: <152F7A0CFF3680418DF7B5D5F41FD198205CECC9@WDCPRDWINMBX14.wellmanage.com>

Good point, missed that. However, since the order is so important here, perhaps the example should at least make that clear? As the spec itself says:

                15.7. Evaluation Order

The Java programming language guarantees that the operands of operators appear to be evaluated in a specific evaluation order, namely, from left to right.

*It is recommended that code not rely crucially on this specification.* Code is usually clearer when each expression contains at most one side effect, as its outermost operation, and when code does not depend on exactly which exception arises as a consequence of the left-to-right evaluation of expressions.

Greg

From: mthornton at optrak.com [mailto:mthornton at optrak.com] On Behalf Of Mark Thornton
Sent: Thursday 25 July 2013 11:34
To: Allen, Greg J
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] RecursiveTask example has order of evaluation bug?

Unlike C, Java does define the order of evaluation.

Mark Thornton


On Thursday, 25 July 2013, Allen, Greg J wrote:
Isn't there a flaw in the RecursiveTask example in the javadoc?

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html

class Fibonacci extends RecursiveTask<Integer> {
   final int n;
   Fibonacci(int n) { this.n = n; }
   Integer compute() {
     if (n <= 1)
        return n;
     Fibonacci f1 = new Fibonacci(n - 1);
     f1.fork();
     Fibonacci f2 = new Fibonacci(n - 2);
     return f2.compute() + f1.join();

Surely Java order of evaluation is not guaranteed for compute() vs. join() in the return statement. If join gets called first, all parallelism is lost!

I was also a bit surprised to see the default commonPool size was nCPU - 1. Dual core processor are very common, so will likely get slower performance from the parallel sorts than the plain ones. Perhaps there should be a special case for nCPU==2?

Greg

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130725/1c1cbcaf/attachment.html>

From davidcholmes at aapt.net.au  Thu Jul 25 06:52:23 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 25 Jul 2013 20:52:23 +1000
Subject: [concurrency-interest] RecursiveTask example has order of
	evaluation bug?
In-Reply-To: <152F7A0CFF3680418DF7B5D5F41FD198205CECC9@WDCPRDWINMBX14.wellmanage.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEEMJPAA.davidcholmes@aapt.net.au>

That statement is only there to stop people from writing code like:

int j = i++ + ++i - --i - ++i;

Relying on well defined order of evaluation is second nature in Java not
something to be cautioned against.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Allen, Greg
J
  Sent: Thursday, 25 July 2013 8:41 PM
  To: Mark Thornton
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RecursiveTask example has order of
evaluation bug?


  Good point, missed that. However, since the order is so important here,
perhaps the example should at least make that clear? As the spec itself
says:



                  15.7. Evaluation Order



  The Java programming language guarantees that the operands of operators
appear to be evaluated in a specific evaluation order, namely, from left to
right.



  *It is recommended that code not rely crucially on this specification.*
Code is usually clearer when each expression contains at most one side
effect, as its outermost operation, and when code does not depend on exactly
which exception arises as a consequence of the left-to-right evaluation of
expressions.



  Greg



  From: mthornton at optrak.com [mailto:mthornton at optrak.com] On Behalf Of Mark
Thornton
  Sent: Thursday 25 July 2013 11:34
  To: Allen, Greg J
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] RecursiveTask example has order of
evaluation bug?



  Unlike C, Java does define the order of evaluation.



  Mark Thornton



  On Thursday, 25 July 2013, Allen, Greg J wrote:

  Isn't there a flaw in the RecursiveTask example in the javadoc?



  http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTas
k.html



  class Fibonacci extends RecursiveTask<Integer> {

     final int n;

     Fibonacci(int n) { this.n = n; }

     Integer compute() {

       if (n <= 1)

          return n;

       Fibonacci f1 = new Fibonacci(n - 1);

       f1.fork();

       Fibonacci f2 = new Fibonacci(n - 2);

       return f2.compute() + f1.join();



  Surely Java order of evaluation is not guaranteed for compute() vs. join()
in the return statement. If join gets called first, all parallelism is lost!



  I was also a bit surprised to see the default commonPool size was nCPU -
1. Dual core processor are very common, so will likely get slower
performance from the parallel sorts than the plain ones. Perhaps there
should be a special case for nCPU==2?



  Greg


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130725/4b1cd910/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jul 25 08:32:08 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 25 Jul 2013 13:32:08 +0100
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <51F10042.4010208@oracle.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
	<51F10042.4010208@oracle.com>
Message-ID: <51F11AC8.3040909@oracle.com>

While we are on this topic, it will be helpful to document the barriers 
that fork() and join() introduce.

(eg fork() is probably the same as a volatile store, and a join() is the 
same as a volatile store followed by volatile load)


Also, the quote documents the behaviour observed by the thread 
performing the computation, not the order of actions observed 
externally. For example, it will be silly to expect (x / 100) + y + (x % 
100) to perform two divisions just because the second one is declared 
after (x / 100) + y.

Pure evaluations usually get reordered freely. If one can prove X and Y 
do not depend on each other, they can be reordered. However, they still 
have one important side-effect: elapsed time.

So, the point still stands. We'll want to parallellize pure computation 
because of elapsed time side-effect. Yet, something must preclude 
reordering even if the computation is pure.


Alex


On 25/07/2013 11:38, Aleksey Shipilev wrote:
> On 07/25/2013 02:20 PM, Allen, Greg J wrote:
>> Isn?t there a flaw in the RecursiveTask example in the javadoc?
>>   
>> _http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveTask.html_
>>   
>> class Fibonacci extends RecursiveTask<Integer> {
>>     final int n;
>>     Fibonacci(int n) { this.n = n; }
>>     Integer compute() {
>>       if (n <= 1)
>>          return n;
>>       Fibonacci f1 = new Fibonacci(n - 1);
>>       f1.fork();
>>       Fibonacci f2 = new Fibonacci(n - 2);
>>       return f2.compute() + f1.join();
>>   
>> Surely Java order of evaluation is not guaranteed for compute() vs.
>> join() in the return statement. If join gets called first, all
>> parallelism is lost!
> Eh?
>
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.7
>
> "The Java programming language guarantees that the operands of operators
> appear to be evaluated in a specific evaluation order, namely, from left
> to right."
>
> "The Java programming language guarantees that every operand of an
> operator (except the conditional operators &&, ||, and ? :) appears to
> be fully evaluated before any part of the operation itself is performed."
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Fri Jul 26 14:21:08 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 26 Jul 2013 11:21:08 -0700
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <51F11AC8.3040909@oracle.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
	<51F10042.4010208@oracle.com> <51F11AC8.3040909@oracle.com>
Message-ID: <CA+kOe09=zhtp_pq-HcxQXO6vvyUjACk6KbALitU3RHRbasU8=g@mail.gmail.com>

On Thu, Jul 25, 2013 at 5:32 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> While we are on this topic, it will be helpful to document the barriers
> that fork() and join() introduce.
>
> (eg fork() is probably the same as a volatile store, and a join() is the
> same as a volatile store followed by volatile load)
>
>
It seems reasonable to add some happens-before guarantees for fork/join to
j.u.c.package-info.java
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130726/8f549f8b/attachment.html>

From martinrb at google.com  Fri Jul 26 14:22:14 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 26 Jul 2013 11:22:14 -0700
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <51F11AC8.3040909@oracle.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
	<51F10042.4010208@oracle.com> <51F11AC8.3040909@oracle.com>
Message-ID: <CA+kOe09Uc0tsnKynPk5+f6+WPh+PZ1TvsSEfkh6BikdQqcTFzQ@mail.gmail.com>

On Thu, Jul 25, 2013 at 5:32 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> While we are on this topic, it will be helpful to document the barriers
> that fork() and join() introduce.
>
> (eg fork() is probably the same as a volatile store, and a join() is the
> same as a volatile store followed by volatile load)
>
>
It seems reasonable to add some happens-before guarantees for fork/join to
j.u.c.package-info.java
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130726/64aa0b76/attachment.html>

From martinrb at google.com  Fri Jul 26 14:20:20 2013
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 26 Jul 2013 11:20:20 -0700
Subject: [concurrency-interest] RecursiveTask example has order of
 evaluation bug?
In-Reply-To: <51F11AC8.3040909@oracle.com>
References: <152F7A0CFF3680418DF7B5D5F41FD198205CEBB4@WDCPRDWINMBX14.wellmanage.com>
	<51F10042.4010208@oracle.com> <51F11AC8.3040909@oracle.com>
Message-ID: <CA+kOe08dpNQ56wrz42y=FospCO34CcmzB4WQXAz-zqGJ6TEy2Q@mail.gmail.com>

On Thu, Jul 25, 2013 at 5:32 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> While we are on this topic, it will be helpful to document the barriers
> that fork() and join() introduce.
>
> (eg fork() is probably the same as a volatile store, and a join() is the
> same as a volatile store followed by volatile load)
>
>
It seems reasonable to add some happens-before guarantees for fork/join to
j.u.c.package-info.java
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130726/ec0ca29f/attachment.html>

From zodiaczx6 at gmail.com  Wed Jul 31 04:04:42 2013
From: zodiaczx6 at gmail.com (Colin Vipurs)
Date: Wed, 31 Jul 2013 11:04:42 +0300
Subject: [concurrency-interest] Java 8 Thread.interrupt() random behaviour
Message-ID: <CAKi42738mcZPjLqB-ifo8sfezWpOG+wtK4nrd-H-QCjCR2m3Hg@mail.gmail.com>

I have been doing some testing of JDK8 with our existing codebase and find
what I _believe_ to be a change in behaviour between Java 7 & 8 with
future.get().

In Java 7, interrupting a Thread waiting on future.get() will consistently
raise an InterruptedException but in 8 this seems to be, well, random.  The
following test case exposes the behaviour:

public class FutureGetTestCase {
public static void main(String[] args) {
ExecutorService executor = Executors.newSingleThreadExecutor();
Future<?> future = executor.submit(ignoredRunnable());
try {
Thread.currentThread().interrupt();
future.get();
System.out.println("No exception thrown");
} catch (InterruptedException ie) {
System.out.println("Interrupted Exception thrown");
} catch (ExecutionException e) {
System.out.println("ExecutionException thrown");
}
System.exit(1);
}

private static Runnable ignoredRunnable() {
return new Runnable() {
@Override
public void run() {
}
};
}
}

On Java 7, running in a loop produces:

Interrupted Exception thrown

No matter how many times it is run.  Compiling and executing the same with
8 produces:

No exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
Interrupted Exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
Interrupted Exception thrown
Interrupted Exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
No exception thrown
Interrupted Exception thrown
No exception thrown

Tested against Java versions

java version "1.7.0_25"
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

java version "1.8.0-ea"
Java(TM) SE Runtime Environment (build 1.8.0-ea-b99)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b41, mixed mode)

java version "1.8.0-ea"
Java(TM) SE Runtime Environment (build 1.8.0-ea-b100)
Java HotSpot(TM) 64-Bit Server VM (build 25.0-b42, mixed mode)

Running on Mac OSX 10.6.8
And Linux xubuntu-vm 3.2.0-49-generic #75-Ubuntu SMP Tue Jun 18 17:39:32
UTC 2013 x86_64 x86_64 x86_64 GNU/Linux

Is this a real issue or PEBKAC?

-- 
Maybe she awoke to see the roommate's boyfriend swinging from the
chandelier wearing a boar's head.

Something which you, I, and everyone else would call "Tuesday", of course.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130731/d200908d/attachment.html>

From davidcholmes at aapt.net.au  Wed Jul 31 04:43:43 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 31 Jul 2013 18:43:43 +1000
Subject: [concurrency-interest] Java 8 Thread.interrupt() random
	behaviour
In-Reply-To: <CAKi42738mcZPjLqB-ifo8sfezWpOG+wtK4nrd-H-QCjCR2m3Hg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHDJPAA.davidcholmes@aapt.net.au>

Hi Colin,

FutureTask has been completely reimplemented in Java 8. In Java 7 it was
based around an AbstractQueuedSynchronizer which explicitly eagerly checks
for a thread being interrupted before attempting a potentially blocking
operation. The JDK 8 implementation moves that interrupted check lower in
the code, so if the result of the FutureTask is available, or it has been
cancelled, then those code paths will avoid the interruption check and hence
no InterruptedException is thrown. Hence depending on thread scheduling your
test program will usually not throw IE but occasionally it can.

The JDK 8 implementation is strictly speaking more correct as the
specification of Future.get indicates that IE is only thrown if interrupted
while waiting - so if there is no need to wait then no IE. That said the
j.u.c classes have tended to favour using eager interruption checks simply
because other code typically does not.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Colin Vipurs
  Sent: Wednesday, 31 July 2013 6:05 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Java 8 Thread.interrupt() random behaviour


  I have been doing some testing of JDK8 with our existing codebase and find
what I _believe_ to be a change in behaviour between Java 7 & 8 with
future.get().


  In Java 7, interrupting a Thread waiting on future.get() will consistently
raise an InterruptedException but in 8 this seems to be, well, random.  The
following test case exposes the behaviour:


  public class FutureGetTestCase {
  public static void main(String[] args) {
  ExecutorService executor = Executors.newSingleThreadExecutor();
  Future<?> future = executor.submit(ignoredRunnable());
  try {
  Thread.currentThread().interrupt();
  future.get();
  System.out.println("No exception thrown");
  } catch (InterruptedException ie) {
  System.out.println("Interrupted Exception thrown");
  } catch (ExecutionException e) {
  System.out.println("ExecutionException thrown");
  }
  System.exit(1);
  }


  private static Runnable ignoredRunnable() {
  return new Runnable() {
  @Override
  public void run() {
  }
  };
  }
  }


  On Java 7, running in a loop produces:


  Interrupted Exception thrown



  No matter how many times it is run.  Compiling and executing the same with
8 produces:


  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  Interrupted Exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  Interrupted Exception thrown
  Interrupted Exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  No exception thrown
  Interrupted Exception thrown
  No exception thrown


  Tested against Java versions


  java version "1.7.0_25"
  Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
  Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)


  java version "1.8.0-ea"
  Java(TM) SE Runtime Environment (build 1.8.0-ea-b99)
  Java HotSpot(TM) 64-Bit Server VM (build 25.0-b41, mixed mode)


  java version "1.8.0-ea"
  Java(TM) SE Runtime Environment (build 1.8.0-ea-b100)
  Java HotSpot(TM) 64-Bit Server VM (build 25.0-b42, mixed mode)


  Running on Mac OSX 10.6.8
  And Linux xubuntu-vm 3.2.0-49-generic #75-Ubuntu SMP Tue Jun 18 17:39:32
UTC 2013 x86_64 x86_64 x86_64 GNU/Linux


  Is this a real issue or PEBKAC?


  --
  Maybe she awoke to see the roommate's boyfriend swinging from the
chandelier wearing a boar's head.

  Something which you, I, and everyone else would call "Tuesday", of course.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130731/b7c147de/attachment.html>

From zodiaczx6 at gmail.com  Wed Jul 31 06:59:53 2013
From: zodiaczx6 at gmail.com (Colin Vipurs)
Date: Wed, 31 Jul 2013 13:59:53 +0300
Subject: [concurrency-interest] Java 8 Thread.interrupt() random
	behaviour
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEHDJPAA.davidcholmes@aapt.net.au>
References: <CAKi42738mcZPjLqB-ifo8sfezWpOG+wtK4nrd-H-QCjCR2m3Hg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEHDJPAA.davidcholmes@aapt.net.au>
Message-ID: <CAKi4270_kHE4kS2givNizb2M4hnXVnOwuzLKZMh6mOedcU2XyQ@mail.gmail.com>

Hi David,

Does that mean that in Java 7 it is possible that a task has completed but
an IE could still be thrown? If that is the case, this is essentially fixed
in 8?

Either way, thanks for your response - our code was actually correct but
our test case was built specifically around the Java 7 implementation.


On Wed, Jul 31, 2013 at 11:43 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Hi Colin,
>
> FutureTask has been completely reimplemented in Java 8. In Java 7 it was
> based around an AbstractQueuedSynchronizer which explicitly eagerly checks
> for a thread being interrupted before attempting a potentially blocking
> operation. The JDK 8 implementation moves that interrupted check lower in
> the code, so if the result of the FutureTask is available, or it has been
> cancelled, then those code paths will avoid the interruption check and
> hence no InterruptedException is thrown. Hence depending on thread
> scheduling your test program will usually not throw IE but occasionally it
> can.
>
> The JDK 8 implementation is strictly speaking more correct as the
> specification of Future.get indicates that IE is only thrown if interrupted
> while waiting - so if there is no need to wait then no IE. That said the
> j.u.c classes have tended to favour using eager interruption checks simply
> because other code typically does not.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Colin Vipurs
> *Sent:* Wednesday, 31 July 2013 6:05 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Java 8 Thread.interrupt() random
> behaviour
>
> I have been doing some testing of JDK8 with our existing codebase and find
> what I _believe_ to be a change in behaviour between Java 7 & 8 with
> future.get().
>
> In Java 7, interrupting a Thread waiting on future.get() will consistently
> raise an InterruptedException but in 8 this seems to be, well, random.  The
> following test case exposes the behaviour:
>
>  public class FutureGetTestCase {
> public static void main(String[] args) {
> ExecutorService executor = Executors.newSingleThreadExecutor();
> Future<?> future = executor.submit(ignoredRunnable());
> try {
> Thread.currentThread().interrupt();
> future.get();
> System.out.println("No exception thrown");
> } catch (InterruptedException ie) {
> System.out.println("Interrupted Exception thrown");
> } catch (ExecutionException e) {
> System.out.println("ExecutionException thrown");
> }
> System.exit(1);
> }
>
> private static Runnable ignoredRunnable() {
> return new Runnable() {
> @Override
> public void run() {
> }
> };
> }
> }
>
> On Java 7, running in a loop produces:
>
> Interrupted Exception thrown
>
> No matter how many times it is run.  Compiling and executing the same with
> 8 produces:
>
>  No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> Interrupted Exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> Interrupted Exception thrown
> Interrupted Exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> No exception thrown
> Interrupted Exception thrown
> No exception thrown
>
> Tested against Java versions
>
>  java version "1.7.0_25"
> Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
> Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)
>
>  java version "1.8.0-ea"
> Java(TM) SE Runtime Environment (build 1.8.0-ea-b99)
> Java HotSpot(TM) 64-Bit Server VM (build 25.0-b41, mixed mode)
>
>  java version "1.8.0-ea"
> Java(TM) SE Runtime Environment (build 1.8.0-ea-b100)
> Java HotSpot(TM) 64-Bit Server VM (build 25.0-b42, mixed mode)
>
> Running on Mac OSX 10.6.8
> And Linux xubuntu-vm 3.2.0-49-generic #75-Ubuntu SMP Tue Jun 18 17:39:32
> UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
>
> Is this a real issue or PEBKAC?
>
> --
> Maybe she awoke to see the roommate's boyfriend swinging from the
> chandelier wearing a boar's head.
>
> Something which you, I, and everyone else would call "Tuesday", of course.
>
>


-- 
Maybe she awoke to see the roommate's boyfriend swinging from the
chandelier wearing a boar's head.

Something which you, I, and everyone else would call "Tuesday", of course.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130731/eb00ae62/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Jul 31 07:07:06 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 31 Jul 2013 21:07:06 +1000
Subject: [concurrency-interest] Java 8 Thread.interrupt() random
	behaviour
In-Reply-To: <CAKi4270_kHE4kS2givNizb2M4hnXVnOwuzLKZMh6mOedcU2XyQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEHEJPAA.davidcholmes@aapt.net.au>

Yes, the Java 7 implementation gave preference to checking for interruption
over checking for completion. Whether you consider this "fixed" in 8 is a
matter of perspective. The fact the thread has been interrupted indicates
that thread has been "cancelled" so it shouldn't care whether there is a
result or not. Code should never rely on a race between completion and
interruption.

David
  -----Original Message-----
  From: Colin Vipurs [mailto:zodiaczx6 at gmail.com]
  Sent: Wednesday, 31 July 2013 9:00 PM
  To: dholmes at ieee.org
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Java 8 Thread.interrupt() random
behaviour


  Hi David,


  Does that mean that in Java 7 it is possible that a task has completed but
an IE could still be thrown? If that is the case, this is essentially fixed
in 8?


  Either way, thanks for your response - our code was actually correct but
our test case was built specifically around the Java 7 implementation.



  On Wed, Jul 31, 2013 at 11:43 AM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    Hi Colin,

    FutureTask has been completely reimplemented in Java 8. In Java 7 it was
based around an AbstractQueuedSynchronizer which explicitly eagerly checks
for a thread being interrupted before attempting a potentially blocking
operation. The JDK 8 implementation moves that interrupted check lower in
the code, so if the result of the FutureTask is available, or it has been
cancelled, then those code paths will avoid the interruption check and hence
no InterruptedException is thrown. Hence depending on thread scheduling your
test program will usually not throw IE but occasionally it can.

    The JDK 8 implementation is strictly speaking more correct as the
specification of Future.get indicates that IE is only thrown if interrupted
while waiting - so if there is no need to wait then no IE. That said the
j.u.c classes have tended to favour using eager interruption checks simply
because other code typically does not.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Colin Vipurs
      Sent: Wednesday, 31 July 2013 6:05 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Java 8 Thread.interrupt() random
behaviour


      I have been doing some testing of JDK8 with our existing codebase and
find what I _believe_ to be a change in behaviour between Java 7 & 8 with
future.get().


      In Java 7, interrupting a Thread waiting on future.get() will
consistently raise an InterruptedException but in 8 this seems to be, well,
random.  The following test case exposes the behaviour:


      public class FutureGetTestCase {
      public static void main(String[] args) {
      ExecutorService executor = Executors.newSingleThreadExecutor();
      Future<?> future = executor.submit(ignoredRunnable());
      try {
      Thread.currentThread().interrupt();
      future.get();
      System.out.println("No exception thrown");
      } catch (InterruptedException ie) {
      System.out.println("Interrupted Exception thrown");
      } catch (ExecutionException e) {
      System.out.println("ExecutionException thrown");
      }
      System.exit(1);
      }


      private static Runnable ignoredRunnable() {
      return new Runnable() {
      @Override
      public void run() {
      }
      };
      }
      }


      On Java 7, running in a loop produces:


      Interrupted Exception thrown



      No matter how many times it is run.  Compiling and executing the same
with 8 produces:


      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      Interrupted Exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      Interrupted Exception thrown
      Interrupted Exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      No exception thrown
      Interrupted Exception thrown
      No exception thrown


      Tested against Java versions


      java version "1.7.0_25"
      Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
      Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)


      java version "1.8.0-ea"
      Java(TM) SE Runtime Environment (build 1.8.0-ea-b99)
      Java HotSpot(TM) 64-Bit Server VM (build 25.0-b41, mixed mode)


      java version "1.8.0-ea"
      Java(TM) SE Runtime Environment (build 1.8.0-ea-b100)
      Java HotSpot(TM) 64-Bit Server VM (build 25.0-b42, mixed mode)


      Running on Mac OSX 10.6.8
      And Linux xubuntu-vm 3.2.0-49-generic #75-Ubuntu SMP Tue Jun 18
17:39:32 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux


      Is this a real issue or PEBKAC?


      --
      Maybe she awoke to see the roommate's boyfriend swinging from the
chandelier wearing a boar's head.

      Something which you, I, and everyone else would call "Tuesday", of
course.





  --
  Maybe she awoke to see the roommate's boyfriend swinging from the
chandelier wearing a boar's head.

  Something which you, I, and everyone else would call "Tuesday", of course.
  No virus found in this message.
  Checked by AVG - www.avg.com
  Version: 2013.0.3392 / Virus Database: 3209/6537 - Release Date: 07/30/13
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130731/8074db74/attachment.html>

