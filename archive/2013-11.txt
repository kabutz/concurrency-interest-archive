From kimo at webnetic.net  Mon Nov  4 20:31:27 2013
From: kimo at webnetic.net (Kimo Crossman)
Date: Mon, 4 Nov 2013 19:31:27 -0600
Subject: [concurrency-interest] A Practical Wait-Free Multi-Word
	Compare-and-Swap Operation
Message-ID: <CALV1V492FXQJtU5UrvmwGYaXu8Toa7sGPeEkU2yu3V-3g0cPDA@mail.gmail.com>

> http://www.mscs.mu.edu/~brylow/SPLASH-MARC-2013/Feldman.pdf
>
> A Practical Wait-Free Multi-Word Compare-and-Swap
> Operation
>
> Steven Feldman
> University of Central Florida
> Feldman at knights.ucf.edu
> Pierre LaBorde
> University of Central Florida
> Damian Dechev
> University of Central Florida
> dechev at eecs.ucf.edu
>
> ABSTRACT
> Algorithms designed for current and future multi-core systems,
> which are expected to experience an increase of the
> number of cores by 100x over the next decade, must exhibit
> strong scaling. The guarantee of progress provided
> by wait-free algorithms and the fine-grained synchronization
> methods used in their designs, make them desirable for
> achieving this goal. However, the design and development
> of advanced wait-free algorithms is often inhibited by the
> limitations of portable atomic hardware operations. Typically
> these operations can manipulate a single address at
> a time, where many concurrent algorithms need to perform
> a series of operations on multiple addresses, requiring more
> advanced synchronization mechanisms such as a wait-free
> Multi-Word-Compare-and-Swap (MCAS).
> In this paper, we present the first practical MCAS design
> that is wait-free in all scenarios. This property holds even if
> interrupts consistently cause a thread to retry a portion of
> its operation. Our design is practical in that it is built from
> only portable atomic operations (e.g. atomic reads, atomic
> writes, compare-and-swap), it is efficient in its utilization of
> memory (i. e. requiring only a single bit to be reserved from
> each word, not requiring use of explicit memory barriers, and
> requiring only four words per address in the operation). Our
> performance evaluation reveals that on average our wait-free
> MCAS algorithm performs 8.3% faster than other practical
> approaches in all tested scenarios.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131104/50568c57/attachment.html>

From aph at redhat.com  Fri Nov  8 12:08:14 2013
From: aph at redhat.com (Andrew Haley)
Date: Fri, 08 Nov 2013 17:08:14 +0000
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
Message-ID: <527D1A7E.3070900@redhat.com>

On 10/21/2013 04:21 AM, Andy Nuss wrote:

> I was wondering, if one is building a type of re-entrant lock, do
> you get better performance by paying the cost of
> Thread.currentThread() for each lock operation, or using a
> ThreadLocal variable.  If one chooses the latter, is there an impact
> by having lots of effective thread local storage bloat?

In addition to David Holmes' response, you may be interested to know
just how littel overhead Thread.currentThread() actually is.  On a
modern RISC, it is exactly

       ldr	x13, [xthread,#480]

i.e. the cost of Thread.currentThread() is the same as that of a field
access.  A ThreadLocal variable is considerably more expensive.

Andrew.

From jso at quartetfs.com  Mon Nov 18 11:35:13 2013
From: jso at quartetfs.com (Jonathan Soto)
Date: Mon, 18 Nov 2013 17:35:13 +0100
Subject: [concurrency-interest] Fork and and timed get
Message-ID: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>

Hello everyone,
It has been a few days I am thinking about this problem. Could you help me ?

Suppose that we have two tasks
- task1 waits for 5s
- task2 forks task1 and does a timed get of 2.5 seconds

The main thread submits task2 to a FJP.

I observe the following behavior.
- Thread Main   :
FPJ.submit(task2)

- Thread FJPO-1 : task1.fork
As we are in the FJP it adds task1 into the workQueue of the current thread
Task1 does not start in another thread

- Thread FJPO-1 : task1.get(2.5, SECONDS)
Starts by checking if the current thread has some local tasks to be done.
As there is task1 that has been previously stored, we execute it.

Task1 takes 5 seconds to be executed. The 2.5 seconds timeout is totally
ignored.

Is that the expected behavior ?

Thanks

Jonathan

P.s in attachment a unit test that prints FAIL when the test fails
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131118/cfc410ee/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: TestForkTimedGet.java
Type: application/octet-stream
Size: 1595 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131118/cfc410ee/attachment.obj>

From tball at google.com  Wed Nov 20 15:12:38 2013
From: tball at google.com (Tom Ball)
Date: Wed, 20 Nov 2013 12:12:38 -0800
Subject: [concurrency-interest] Java atomics questions
Message-ID: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>

I'm a member of the J2ObjC <https://code.google.com/p/j2objc/> project, an
open-source Jave to Objective-C transpiler. The java.util.concurrent
packages have recently been added to its JRE emulation library, but we have
some unanswered questions.

The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have a
partial implementation based on Android's version. I'm fuzzy on its
distinction between "ordered" and "volatile" field access, though. I
assumed that ordered means "requires a memory barrier", while volatile
doesn't. Is that correct? Here is our
putObjectVolatile()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201>
 and putOrderedObject()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287>
with
that assumption. Look reasonable? (The native comments hold Objective-C,
but in this case it's vanilla C.)

Also, is anyone familiar with the C11 optional atomic
operations<http://en.cppreference.com/w/c/atomic> support?
Clang has the necessary atomic intrinsic functions to support it and
generates reasonably looking code, so we're hoping to fix translation of
Java volatile fields with it. Currently we just add the volatile keyword to
the Objective-C field declaration, which doesn't support Java's definition.
Our thoughts for a better translation look something like:

  volatile int i;
...
  i = 42;
  i += 666;

becomes:
  atomic_int i;
...
  atomic_store(&i, 42);
  atomic_fetch_add(&i, 666);

Is that a reasonable approximation of Java volatile field semantics in C11,
or is there a better approach?

Tom

P.S. Forgive me my concurrency fumbling -- I realize compiler writers
should leave such design to the experts. :-)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/cfdac8e6/attachment.html>

From martinrb at google.com  Wed Nov 20 15:24:31 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 20 Nov 2013 12:24:31 -0800
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
Message-ID: <CA+kOe09qL2-=U31fW+GVmS-ReTZQ70+4Fkhhaq8S55LXBje=RA@mail.gmail.com>

Doug, I think the answers to Tom's questions belong in the JSR-133 Cookbook
(i.e. please add coverage for Unsafe operations and C++11 equivalents,
especially given that some implementers will be speaking C++11).
http://g.oswego.edu/dl/jmm/cookbook.html


On Wed, Nov 20, 2013 at 12:12 PM, Tom Ball <tball at google.com> wrote:

> I'm a member of the J2ObjC <https://code.google.com/p/j2objc/> project,
> an open-source Jave to Objective-C transpiler. The java.util.concurrent
> packages have recently been added to its JRE emulation library, but we have
> some unanswered questions.
>
> The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have
> a partial implementation based on Android's version. I'm fuzzy on its
> distinction between "ordered" and "volatile" field access, though. I
> assumed that ordered means "requires a memory barrier", while volatile
> doesn't. Is that correct? Here is our putObjectVolatile()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201>
>  and putOrderedObject()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with
> that assumption. Look reasonable? (The native comments hold Objective-C,
> but in this case it's vanilla C.)
>
> Also, is anyone familiar with the C11 optional atomic operations<http://en.cppreference.com/w/c/atomic> support?
> Clang has the necessary atomic intrinsic functions to support it and
> generates reasonably looking code, so we're hoping to fix translation of
> Java volatile fields with it. Currently we just add the volatile keyword to
> the Objective-C field declaration, which doesn't support Java's definition.
> Our thoughts for a better translation look something like:
>
>   volatile int i;
> ...
>   i = 42;
>   i += 666;
>
> becomes:
>   atomic_int i;
> ...
>   atomic_store(&i, 42);
>   atomic_fetch_add(&i, 666);
>
> Is that a reasonable approximation of Java volatile field semantics in
> C11, or is there a better approach?
>
> Tom
>
> P.S. Forgive me my concurrency fumbling -- I realize compiler writers
> should leave such design to the experts. :-)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/dfcbe7b1/attachment.html>

From vitalyd at gmail.com  Wed Nov 20 15:28:32 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 20 Nov 2013 15:28:32 -0500
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
Message-ID: <CAHjP37HTMQv24YvPS7nmK_p_Sru=cPUw5diatWDSv-JTXATz-w@mail.gmail.com>

Hi Tom,

Google for Doug Lea's compiler cookbook - it should have the JMM details
for you.

Volatile writes entail sequential consistency (with corresponding read) so
it's a full fence.  Ordered write

Sent from my phone
On Nov 20, 2013 3:18 PM, "Tom Ball" <tball at google.com> wrote:

> I'm a member of the J2ObjC <https://code.google.com/p/j2objc/> project,
> an open-source Jave to Objective-C transpiler. The java.util.concurrent
> packages have recently been added to its JRE emulation library, but we have
> some unanswered questions.
>
> The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have
> a partial implementation based on Android's version. I'm fuzzy on its
> distinction between "ordered" and "volatile" field access, though. I
> assumed that ordered means "requires a memory barrier", while volatile
> doesn't. Is that correct? Here is our putObjectVolatile()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201>
>  and putOrderedObject()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with
> that assumption. Look reasonable? (The native comments hold Objective-C,
> but in this case it's vanilla C.)
>
> Also, is anyone familiar with the C11 optional atomic operations<http://en.cppreference.com/w/c/atomic> support?
> Clang has the necessary atomic intrinsic functions to support it and
> generates reasonably looking code, so we're hoping to fix translation of
> Java volatile fields with it. Currently we just add the volatile keyword to
> the Objective-C field declaration, which doesn't support Java's definition.
> Our thoughts for a better translation look something like:
>
>   volatile int i;
> ...
>   i = 42;
>   i += 666;
>
> becomes:
>   atomic_int i;
> ...
>   atomic_store(&i, 42);
>   atomic_fetch_add(&i, 666);
>
> Is that a reasonable approximation of Java volatile field semantics in
> C11, or is there a better approach?
>
> Tom
>
> P.S. Forgive me my concurrency fumbling -- I realize compiler writers
> should leave such design to the experts. :-)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/0c7a11e5/attachment.html>

From vitalyd at gmail.com  Wed Nov 20 15:32:14 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 20 Nov 2013 15:32:14 -0500
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <CAHjP37HTMQv24YvPS7nmK_p_Sru=cPUw5diatWDSv-JTXATz-w@mail.gmail.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
	<CAHjP37HTMQv24YvPS7nmK_p_Sru=cPUw5diatWDSv-JTXATz-w@mail.gmail.com>
Message-ID: <CAHjP37EwEHSMFL00Myr1emLw-TSfX3gCdqmKHiuBb7nFek2pQg@mail.gmail.com>

Sorry, hit send by accident ...

Ordered write just orders stores; on x86/64 this equates to just compiler
barrier - no mem fence.  Volatile stores on this arch emit full machine
fence instructions (this used to be mfence, now a lock add r/esp 0 nop that
provides same fence) + corresponding compiler fence, of course.

Sent from my phone
On Nov 20, 2013 3:28 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Hi Tom,
>
> Google for Doug Lea's compiler cookbook - it should have the JMM details
> for you.
>
> Volatile writes entail sequential consistency (with corresponding read) so
> it's a full fence.  Ordered write
>
> Sent from my phone
> On Nov 20, 2013 3:18 PM, "Tom Ball" <tball at google.com> wrote:
>
>> I'm a member of the J2ObjC <https://code.google.com/p/j2objc/> project,
>> an open-source Jave to Objective-C transpiler. The java.util.concurrent
>> packages have recently been added to its JRE emulation library, but we have
>> some unanswered questions.
>>
>> The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have
>> a partial implementation based on Android's version. I'm fuzzy on its
>> distinction between "ordered" and "volatile" field access, though. I
>> assumed that ordered means "requires a memory barrier", while volatile
>> doesn't. Is that correct? Here is our putObjectVolatile()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201>
>>  and putOrderedObject()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with
>> that assumption. Look reasonable? (The native comments hold Objective-C,
>> but in this case it's vanilla C.)
>>
>> Also, is anyone familiar with the C11 optional atomic operations<http://en.cppreference.com/w/c/atomic> support?
>> Clang has the necessary atomic intrinsic functions to support it and
>> generates reasonably looking code, so we're hoping to fix translation of
>> Java volatile fields with it. Currently we just add the volatile keyword to
>> the Objective-C field declaration, which doesn't support Java's definition.
>> Our thoughts for a better translation look something like:
>>
>>   volatile int i;
>> ...
>>   i = 42;
>>   i += 666;
>>
>> becomes:
>>   atomic_int i;
>> ...
>>   atomic_store(&i, 42);
>>   atomic_fetch_add(&i, 666);
>>
>> Is that a reasonable approximation of Java volatile field semantics in
>> C11, or is there a better approach?
>>
>> Tom
>>
>> P.S. Forgive me my concurrency fumbling -- I realize compiler writers
>> should leave such design to the experts. :-)
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/303b901a/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Nov 20 15:39:11 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 20 Nov 2013 13:39:11 -0700
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
Message-ID: <528D1DEF.9020300@oracle.com>

I am not the expert here but I would like to take a stab at it... and be 
corrected.

I think putObjectVolatile() has a full memory fence after it. Whereas, 
putOrderedObject() has a store-store memory fence after it.  On x86, the 
store-store memory fence doesn't emit an instruction since the x86 
memory model already forces stores to proceed in order.  However, the 
memory fence prevents JIT from rearranging the store instructions.

On x86, atomic_fetch_add() ideally would compile into a single 
instruction: lock xadd.  This is much stronger guarantee than doing a += 
on a Java volatile.  += would compile to the following...

tmp = atomic_load(&i);
atomic_store(&i, tmp + 666);

 > Currently we just add the volatile keyword to the Objective-C field 
declaration, which doesn't support Java's definition.

This is going to come back to bite you.  I highly recommend supporting 
Java's definition.  This simply means all accesses to the Java volatile 
variables must be done with atomic_load and atomic_store.

-Nathan

On 11/20/2013 1:12 PM, Tom Ball wrote:
> I'm a member of the J2ObjC 
> <https://code.google.com/p/j2objc/> project, an open-source Jave to 
> Objective-C transpiler. The java.util.concurrent packages have 
> recently been added to its JRE emulation library, but we have some 
> unanswered questions.
>
> The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we 
> have a partial implementation based on Android's version. I'm fuzzy on 
> its distinction between "ordered" and "volatile" field access, though. 
> I assumed that ordered means "requires a memory barrier", while 
> volatile doesn't. Is that correct? Here is our putObjectVolatile() 
> <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201> and 
> putOrderedObject() 
> <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with 
> that assumption. Look reasonable? (The native comments hold 
> Objective-C, but in this case it's vanilla C.)
>
> Also, is anyone familiar with the C11 optional atomic operations 
> <http://en.cppreference.com/w/c/atomic> support? Clang has the 
> necessary atomic intrinsic functions to support it and generates 
> reasonably looking code, so we're hoping to fix translation of Java 
> volatile fields with it. Currently we just add the volatile keyword to 
> the Objective-C field declaration, which doesn't support Java's 
> definition. Our thoughts for a better translation look something like:
>
> volatile int i;
> ...
>   i = 42;
>   i += 666;
>
> becomes:
> atomic_int i;
> ...
> atomic_store(&i, 42);
> atomic_fetch_add(&i, 666);
>
> Is that a reasonable approximation of Java volatile field semantics in 
> C11, or is there a better approach?
>
> Tom
>
> P.S. Forgive me my concurrency fumbling -- I realize compiler writers 
> should leave such design to the experts. :-)
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/9f846cf9/attachment.html>

From hans.boehm at hp.com  Wed Nov 20 18:31:43 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 20 Nov 2013 23:31:43 +0000
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <528D1DEF.9020300@oracle.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
	<528D1DEF.9020300@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD23D5031F2@G4W3214.americas.hpqcorp.net>

I would assume that putOrderedObject corresponds to lazySet and C11/C++11 memory_order_release (only the last of which has a precise definition).  Which means it kind of, sort of has a StoreStore fence BEFORE it, but that's in general stronger than its definition.

putObjectVolatile presumably corresponds to C/C++ memory_order_seq_cst, and has a trailing fence on x86.  (On Power and ARM, things are much more complicated, and that's not IBM's implementation recommendation.  See http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html.)

Other than that, I agree with what follows.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan Reynolds
Sent: Wednesday, November 20, 2013 12:39 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Java atomics questions

I am not the expert here but I would like to take a stab at it... and be corrected.

I think putObjectVolatile() has a full memory fence after it.  Whereas, putOrderedObject() has a store-store memory fence after it.  On x86, the store-store memory fence doesn't emit an instruction since the x86 memory model already forces stores to proceed in order.  However, the memory fence prevents JIT from rearranging the store instructions.

On x86, atomic_fetch_add() ideally would compile into a single instruction: lock xadd.  This is much stronger guarantee than doing a += on a Java volatile.  += would compile to the following...

tmp = atomic_load(&i);
atomic_store(&i, tmp + 666);

> Currently we just add the volatile keyword to the Objective-C field declaration, which doesn't support Java's definition.

This is going to come back to bite you.  I highly recommend supporting Java's definition.  This simply means all accesses to the Java volatile variables must be done with atomic_load and atomic_store.


-Nathan
On 11/20/2013 1:12 PM, Tom Ball wrote:
I'm a member of the J2ObjC<https://code.google.com/p/j2objc/> project, an open-source Jave to Objective-C transpiler. The java.util.concurrent packages have recently been added to its JRE emulation library, but we have some unanswered questions.

The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have a partial implementation based on Android's version. I'm fuzzy on its distinction between "ordered" and "volatile" field access, though. I assumed that ordered means "requires a memory barrier", while volatile doesn't. Is that correct? Here is our putObjectVolatile()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201> and putOrderedObject()<https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with that assumption. Look reasonable? (The native comments hold Objective-C, but in this case it's vanilla C.)

Also, is anyone familiar with the C11 optional atomic operations<http://en.cppreference.com/w/c/atomic> support? Clang has the necessary atomic intrinsic functions to support it and generates reasonably looking code, so we're hoping to fix translation of Java volatile fields with it. Currently we just add the volatile keyword to the Objective-C field declaration, which doesn't support Java's definition. Our thoughts for a better translation look something like:

  volatile int i;
...
  i = 42;
  i += 666;

becomes:
  atomic_int i;
...
  atomic_store(&i, 42);
  atomic_fetch_add(&i, 666);

Is that a reasonable approximation of Java volatile field semantics in C11, or is there a better approach?

Tom

P.S. Forgive me my concurrency fumbling -- I realize compiler writers should leave such design to the experts. :-)




_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/23a9341d/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Nov 20 18:43:39 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 20 Nov 2013 23:43:39 +0000
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <528D1DEF.9020300@oracle.com>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
	<528D1DEF.9020300@oracle.com>
Message-ID: <528D492B.50300@oracle.com>

You want the semantics of "write(y) implies write(x)" in order for 
"read(y) implies read(x)". So you will want a store-store between x and 
y, not after y.

Alex

On 20/11/2013 20:39, Nathan Reynolds wrote:
> I am not the expert here but I would like to take a stab at it... and 
> be corrected.
>
> I think putObjectVolatile() has a full memory fence after it. Whereas, 
> putOrderedObject() has a store-store memory fence after it.  On x86, 
> the store-store memory fence doesn't emit an instruction since the x86 
> memory model already forces stores to proceed in order.  However, the 
> memory fence prevents JIT from rearranging the store instructions.
>
> On x86, atomic_fetch_add() ideally would compile into a single 
> instruction: lock xadd.  This is much stronger guarantee than doing a 
> += on a Java volatile.  += would compile to the following...
>
> tmp = atomic_load(&i);
> atomic_store(&i, tmp + 666);
>
> > Currently we just add the volatile keyword to the Objective-C field 
> declaration, which doesn't support Java's definition.
>
> This is going to come back to bite you.  I highly recommend supporting 
> Java's definition.  This simply means all accesses to the Java 
> volatile variables must be done with atomic_load and atomic_store.
> -Nathan
> On 11/20/2013 1:12 PM, Tom Ball wrote:
>> I'm a member of the J2ObjC 
>> <https://code.google.com/p/j2objc/> project, an open-source Jave to 
>> Objective-C transpiler. The java.util.concurrent packages have 
>> recently been added to its JRE emulation library, but we have some 
>> unanswered questions.
>>
>> The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we 
>> have a partial implementation based on Android's version. I'm fuzzy 
>> on its distinction between "ordered" and "volatile" field access, 
>> though. I assumed that ordered means "requires a memory barrier", 
>> while volatile doesn't. Is that correct? Here is our 
>> putObjectVolatile() 
>> <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201> and 
>> putOrderedObject() 
>> <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with 
>> that assumption. Look reasonable? (The native comments hold 
>> Objective-C, but in this case it's vanilla C.)
>>
>> Also, is anyone familiar with the C11 optional atomic operations 
>> <http://en.cppreference.com/w/c/atomic> support? Clang has the 
>> necessary atomic intrinsic functions to support it and generates 
>> reasonably looking code, so we're hoping to fix translation of Java 
>> volatile fields with it. Currently we just add the volatile keyword 
>> to the Objective-C field declaration, which doesn't support Java's 
>> definition. Our thoughts for a better translation look something like:
>>
>> volatile int i;
>> ...
>>   i = 42;
>>   i += 666;
>>
>> becomes:
>> atomic_int i;
>> ...
>> atomic_store(&i, 42);
>> atomic_fetch_add(&i, 666);
>>
>> Is that a reasonable approximation of Java volatile field semantics 
>> in C11, or is there a better approach?
>>
>> Tom
>>
>> P.S. Forgive me my concurrency fumbling -- I realize compiler writers 
>> should leave such design to the experts. :-)
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/8266c4d8/attachment.html>

From dl at cs.oswego.edu  Wed Nov 20 18:52:50 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 20 Nov 2013 18:52:50 -0500
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD23D5031F2@G4W3214.americas.hpqcorp.net>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
	<528D1DEF.9020300@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D5031F2@G4W3214.americas.hpqcorp.net>
Message-ID: <528D4B52.6020105@cs.oswego.edu>

On 11/20/2013 06:31 PM, Boehm, Hans wrote:
> I would assume that putOrderedObject corresponds to lazySet and C11/C++11
> memory_order_release (only the last of which has a precise definition).  Which
> means it kind of, sort of has a StoreStore fence BEFORE it, but that?s in
> general stronger than its definition.

Yes, implementations of putOrderedX use the equivalent of a C++
memory_order_release. Without a JMM overhaul, we cannot give it
as precise specs as it deserves.

>
> putObjectVolatile presumably corresponds to C/C++ memory_order_seq_cst, and has
> a trailing fence on x86.  (On Power and ARM, things are much more complicated,
> and that?s not IBM?s implementation recommendation.  See
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html.)
>
> Other than that, I agree with what follows.

Me too.

-Doug


>
> Hans
>
> *From:*concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Nathan Reynolds
> *Sent:* Wednesday, November 20, 2013 12:39 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Java atomics questions
>
> I am not the expert here but I would like to take a stab at it... and be corrected.
>
> I think putObjectVolatile() has a full memory fence after it.  Whereas,
> putOrderedObject() has a store-store memory fence after it.  On x86, the
> store-store memory fence doesn't emit an instruction since the x86 memory model
> already forces stores to proceed in order.  However, the memory fence prevents
> JIT from rearranging the store instructions.
>
> On x86, atomic_fetch_add() ideally would compile into a single instruction: lock
> xadd.  This is much stronger guarantee than doing a += on a Java volatile.  +=
> would compile to the following...
>
> tmp = atomic_load(&i);
> atomic_store(&i, tmp + 666);
>
>  > Currently we just add the volatile keyword to the Objective-C field
> declaration, which doesn't support Java's definition.
>
> This is going to come back to bite you.  I highly recommend supporting Java's
> definition.  This simply means all accesses to the Java volatile variables must
> be done with atomic_load and atomic_store.
>
> -Nathan
>
> On 11/20/2013 1:12 PM, Tom Ball wrote:
>
>     I'm a member of the J2ObjC <https://code.google.com/p/j2objc/> project, an
>     open-source Jave to Objective-C transpiler. The java.util.concurrent
>     packages have recently been added to its JRE emulation library, but we have
>     some unanswered questions.
>
>     The sun.misc.Unsafe class is used by java.util.concurrent.atomic; we have a
>     partial implementation based on Android's version. I'm fuzzy on its
>     distinction between "ordered" and "volatile" field access, though. I assumed
>     that ordered means "requires a memory barrier", while volatile doesn't. Is
>     that correct? Here is our putObjectVolatile()
>     <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#201> and
>     putOrderedObject()
>     <https://code.google.com/p/j2objc/source/browse/jre_emul/Classes/sun/misc/Unsafe.java#287> with
>     that assumption. Look reasonable? (The native comments hold Objective-C, but
>     in this case it's vanilla C.)
>
>     Also, is anyone familiar with the C11 optional atomic operations
>     <http://en.cppreference.com/w/c/atomic> support? Clang has the necessary
>     atomic intrinsic functions to support it and generates reasonably looking
>     code, so we're hoping to fix translation of Java volatile fields with it.
>     Currently we just add the volatile keyword to the Objective-C field
>     declaration, which doesn't support Java's definition. Our thoughts for a
>     better translation look something like:
>
>        volatile int i;
>
>     ...
>
>        i = 42;
>
>        i += 666;
>
>     becomes:
>
>        atomic_int i;
>
>     ...
>
>        atomic_store(&i, 42);
>
>        atomic_fetch_add(&i, 666);
>
>     Is that a reasonable approximation of Java volatile field semantics in C11,
>     or is there a better approach?
>
>     Tom
>
>     P.S. Forgive me my concurrency fumbling -- I realize compiler writers should
>     leave such design to the experts. :-)
>
>
>
>
>     _______________________________________________
>
>     Concurrency-interest mailing list
>
>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>




From martinrb at google.com  Thu Nov 21 01:14:44 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 20 Nov 2013 22:14:44 -0800
Subject: [concurrency-interest] Java atomics questions
In-Reply-To: <528D4B52.6020105@cs.oswego.edu>
References: <CAPLadK7z_+Tm2k5C_a189D5oKDmWZmSZbRLowLfJeC9j8_zh9Q@mail.gmail.com>
	<528D1DEF.9020300@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D5031F2@G4W3214.americas.hpqcorp.net>
	<528D4B52.6020105@cs.oswego.edu>
Message-ID: <CA+kOe0-6ZDG5cMHjW3u-u7wZwt1fbzjnd+W+DVnhyQxWXPfbUg@mail.gmail.com>

On Wed, Nov 20, 2013 at 3:52 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 11/20/2013 06:31 PM, Boehm, Hans wrote:
>
>> I would assume that putOrderedObject corresponds to lazySet and C11/C++11
>> memory_order_release (only the last of which has a precise definition).
>>  Which
>> means it kind of, sort of has a StoreStore fence BEFORE it, but that?s in
>> general stronger than its definition.
>>
>
> Yes, implementations of putOrderedX use the equivalent of a C++
> memory_order_release. Without a JMM overhaul, we cannot give it
> as precise specs as it deserves.


We may feel the need to use weasel words with the spec of the public API
like lazySet, but it should be easier to provide useful documentation for a
method in Unsafe, and no one can stop you from updating the Cookbook.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/f5a7c1cb/attachment.html>

From martinrb at google.com  Thu Nov 21 02:35:28 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 20 Nov 2013 23:35:28 -0800
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
Message-ID: <CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>

Your message got ignored...

You created a pool with parallelism of 1, so it's not surprising that no
new thread was created to run action1.  The action of the pool seems
reasonable to me - how would you do better?

Also, posting your code with a friendlier legal notice might help.


On Mon, Nov 18, 2013 at 8:35 AM, Jonathan Soto <jso at quartetfs.com> wrote:

> Hello everyone,
> It has been a few days I am thinking about this problem. Could you help me
> ?
>
> Suppose that we have two tasks
> - task1 waits for 5s
> - task2 forks task1 and does a timed get of 2.5 seconds
>
> The main thread submits task2 to a FJP.
>
> I observe the following behavior.
> - Thread Main   :
> FPJ.submit(task2)
>
> - Thread FJPO-1 : task1.fork
> As we are in the FJP it adds task1 into the workQueue of the current thread
> Task1 does not start in another thread
>
> - Thread FJPO-1 : task1.get(2.5, SECONDS)
> Starts by checking if the current thread has some local tasks to be done.
> As there is task1 that has been previously stored, we execute it.
>
> Task1 takes 5 seconds to be executed. The 2.5 seconds timeout is totally
> ignored.
>
> Is that the expected behavior ?
>
> Thanks
>
> Jonathan
>
> P.s in attachment a unit test that prints FAIL when the test fails
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131120/aaf5bd1e/attachment-0001.html>

From sanne.grinovero at gmail.com  Fri Nov 22 11:11:06 2013
From: sanne.grinovero at gmail.com (Sanne Grinovero)
Date: Fri, 22 Nov 2013 16:11:06 +0000
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <527D1A7E.3070900@redhat.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
	<527D1A7E.3070900@redhat.com>
Message-ID: <CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>

Hi Andrew,
that comment was very interesting as it doesn't match with what we
have observed by sampling on relatively complex applications.

Could you confirm that it would be similar to a field access even on
OpenJDK / x86-64 ?

Could this be a particularly suited instruction to have the current
thread de-scheduled? The systems on which I observed this have often
way too many threads than what I'd like them to have (given the amount
of real cores), so I expect to always have many threads parked, that
would affect our sampling information.
Currently the idea we've got from the diagnostics - by looking at
sampling only so far - is that we should try avoiding some invocations
to Thread.currentThread().

Sanne

On 8 November 2013 17:08, Andrew Haley <aph at redhat.com> wrote:
> On 10/21/2013 04:21 AM, Andy Nuss wrote:
>
>> I was wondering, if one is building a type of re-entrant lock, do
>> you get better performance by paying the cost of
>> Thread.currentThread() for each lock operation, or using a
>> ThreadLocal variable.  If one chooses the latter, is there an impact
>> by having lots of effective thread local storage bloat?
>
> In addition to David Holmes' response, you may be interested to know
> just how littel overhead Thread.currentThread() actually is.  On a
> modern RISC, it is exactly
>
>        ldr      x13, [xthread,#480]
>
> i.e. the cost of Thread.currentThread() is the same as that of a field
> access.  A ThreadLocal variable is considerably more expensive.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From aph at redhat.com  Fri Nov 22 14:04:03 2013
From: aph at redhat.com (Andrew Haley)
Date: Fri, 22 Nov 2013 19:04:03 +0000
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
	<527D1A7E.3070900@redhat.com>
	<CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>
Message-ID: <528FAAA3.3070807@redhat.com>

Hi,

On 11/22/2013 04:11 PM, Sanne Grinovero wrote:
> Hi Andrew,
> that comment was very interesting as it doesn't match with what we
> have observed by sampling on relatively complex applications.
> 
> Could you confirm that it would be similar to a field access even on
> OpenJDK / x86-64 ?

I think so, yes.  r15 is loaded with the address of the thread-local
storage when the VM is entered, so currentThread() generates:

     mov    0x1e0(%r15),%r11

This is most probably super-fast because TLS is in cache.

> Could this be a particularly suited instruction to have the current
> thread de-scheduled? The systems on which I observed this have often
> way too many threads than what I'd like them to have (given the amount
> of real cores), so I expect to always have many threads parked, that
> would affect our sampling information.
> Currently the idea we've got from the diagnostics - by looking at
> sampling only so far - is that we should try avoiding some invocations
> to Thread.currentThread().

That's unlikely IMO.  You can do a little experiment by timing
currentThread() in a loop.  This is hard, because the only way to
prevent HotSpot from optimizing the code away is to use a volatile:

    volatile Thread t;

    void x() {
        t = Thread.currentThread();
    }

and I think you will find that the time will be dominated by the
volatile access, not the call to currentThread().  But please try it.

In the end, if you really want to know what's going on in HotSpot you
have to look at the code that's generated.  I've heard all sorts of
crackpot theories about how to optimize HotSpot that are based on a
completely false idea of how it works.

Andrew.

From aph at redhat.com  Fri Nov 22 14:36:04 2013
From: aph at redhat.com (Andrew Haley)
Date: Fri, 22 Nov 2013 19:36:04 +0000
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <528FAAA3.3070807@redhat.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
	<527D1A7E.3070900@redhat.com>
	<CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>
	<528FAAA3.3070807@redhat.com>
Message-ID: <528FB224.8020202@redhat.com>

On 11/22/2013 07:04 PM, Andrew Haley wrote:
> In the end, if you really want to know what's going on in HotSpot you
> have to look at the code that's generated.  I've heard all sorts of
> crackpot theories about how to optimize HotSpot that are based on a
> completely false idea of how it works.

Sorry, that came across really badly.  I didn't mean to suggest that
you were a crackpot, just to point out the dangers in coming to
conclusions about performance based on incomplete evidence.

Andrew.


From zhong.j.yu at gmail.com  Fri Nov 22 15:24:57 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 22 Nov 2013 14:24:57 -0600
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
Message-ID: <CACuKZqEmk5=WMXJdYgTAT_7yPp=VBf-439QKAxo8PAFv0m4vrw@mail.gmail.com>

On Sun, Oct 20, 2013 at 10:21 PM, Andy Nuss <andrew_nuss at yahoo.com> wrote:
> Hi,
>
> I was wondering, if one is building a type of re-entrant lock, do you get
> better performance by paying the cost of Thread.currentThread() for each
> lock operation, or using a ThreadLocal variable.  If one chooses the latter,
> is there an impact by having lots of effective thread local storage bloat?

I guess you are not really concerned with space (how many acquired
locks could you have in a thread?) but rather the throughput (which
could be impacted by extra GC incurred by thread locals).

I have a library that heavily depends on ThreadLocal. I was concerned
with performance, so I compared it to a modified version that simply
reads/writes a static field. Amazingly I couldn't detect any
difference in performance in typical applications of the library.


>
> Andy
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From vitalyd at gmail.com  Fri Nov 22 16:14:46 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 22 Nov 2013 16:14:46 -0500
Subject: [concurrency-interest] implementing reentrant locks with
	Thread.currentThread
In-Reply-To: <CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>
References: <1382325668.19344.YahooMailNeo@web141101.mail.bf1.yahoo.com>
	<527D1A7E.3070900@redhat.com>
	<CAFm4XO0xKgj=v0CX_xjGG0+nwBhZPwFhQ-7nWwTvePLoZTQyKw@mail.gmail.com>
Message-ID: <CAHjP37EiHGWSt69RkK8YeMu-_FWp8w4z=B-ueY9_m5VP3h-xdQ@mail.gmail.com>

Getting current thread is typically optimized on most VMs and runtime libs,
and they all do it very similarly (stashing reference to the object in TLS).

As for hotspot, are you sure your tests were done on calls to
Thread.currentThread inside methods that were JIT'd? How do you know that
it's getting the current thread that was slow?

Vitaly

Sent from my phone
On Nov 22, 2013 11:17 AM, "Sanne Grinovero" <sanne.grinovero at gmail.com>
wrote:

> Hi Andrew,
> that comment was very interesting as it doesn't match with what we
> have observed by sampling on relatively complex applications.
>
> Could you confirm that it would be similar to a field access even on
> OpenJDK / x86-64 ?
>
> Could this be a particularly suited instruction to have the current
> thread de-scheduled? The systems on which I observed this have often
> way too many threads than what I'd like them to have (given the amount
> of real cores), so I expect to always have many threads parked, that
> would affect our sampling information.
> Currently the idea we've got from the diagnostics - by looking at
> sampling only so far - is that we should try avoiding some invocations
> to Thread.currentThread().
>
> Sanne
>
> On 8 November 2013 17:08, Andrew Haley <aph at redhat.com> wrote:
> > On 10/21/2013 04:21 AM, Andy Nuss wrote:
> >
> >> I was wondering, if one is building a type of re-entrant lock, do
> >> you get better performance by paying the cost of
> >> Thread.currentThread() for each lock operation, or using a
> >> ThreadLocal variable.  If one chooses the latter, is there an impact
> >> by having lots of effective thread local storage bloat?
> >
> > In addition to David Holmes' response, you may be interested to know
> > just how littel overhead Thread.currentThread() actually is.  On a
> > modern RISC, it is exactly
> >
> >        ldr      x13, [xthread,#480]
> >
> > i.e. the cost of Thread.currentThread() is the same as that of a field
> > access.  A ThreadLocal variable is considerably more expensive.
> >
> > Andrew.
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131122/f01d673b/attachment.html>

From gustavohenrique.86 at gmail.com  Tue Nov 26 19:18:41 2013
From: gustavohenrique.86 at gmail.com (Gustavo Henrique Lima Pinto)
Date: Tue, 26 Nov 2013 19:18:41 -0500
Subject: [concurrency-interest] ForkJoin Usage
Message-ID: <CAB2wjjtqXzU3ByB_B-qpa5WYK6pspoVzBgHhz8cUgnFTmFR=HA@mail.gmail.com>

Hi all,

I'm starting using ForkJoin in some experiments, and I was wondering which
are the differences between use FJ recursively, that is, creating a
RecursiveTask, defining a threshold, invoking and joining the tasks, in a
very similar way that is recommend here[1], and use it like the Executor
framework, for instance:

ForkJoinPool pool = new ForkJoinPool();
pool.execute(new Thread() {
public void run(){ someComputation)()}
});
pool.shutdown();

I saw on the ForkJoinPool source code that each invocation of the submit
and/or execute methods create a new ForkJoinTask, and each task is then
enqueued and processed normally.

Thanks!

Gustavo

1-
http://docs.oracle.com/javase/tutorial/essential/concurrency/forkjoin.html
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131126/6af477f6/attachment.html>

From aleksey.shipilev at oracle.com  Wed Nov 27 07:15:08 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 27 Nov 2013 16:15:08 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
Message-ID: <5295E24C.40608@oracle.com>

Hi,

This matter was discussed some time ago privately, but I want to have
the more rigorous public discussion on this. Let us take the specific
test as the example. Suppose you have the class:

   class A {
      volatile int f;
      A() {
         f = 42;
      }
   }

...and this test case (note it deliberately omits NPE cases to make
reasoning simpler):

                 A a;
   ---------------|----------------
    a = new A();  |   r1 = a.f;

Except for NPEs, what are the possible values for r1? Both Doug, me, and
some other folks agree that the only allowed value is {42}, even  though
it is not evident at first sight, especially if you are contaminated
with reordering/roach-motel thinking.

The sketch for the proof follows. The surprising consequence of the
proof below is that roach motel semantics is not always applicable. I
would be happy to see the flaws in this reasoning or maybe even the
better proof for this.

In order to answer what outcomes are possible we need to dump the
usual/partial/misleading "reorderings" and "happens-before" mindset, and
get to the ground of spec. That is, we need to construct the possible
traces and see if those traces are committable, as per JLS 17.4.8.

Constructing the traces is arguably easy because we have the volatile
ops in the trace. It means we can construct two basic traces, for the
two only juxtapositions of volatile ops in the trace:

Trace A: the one that reads (r1=0):

   vread(a.f, 0)
           \----sw---> vstore(a.f, 42)

Trace B: the one that reads (r1=42):

   vstore(a.f, 42)
            \----sw---> vread(a.f, 42)

Now, extending that that skeleton with the program order, will yield two
final traces:

Trace A: one that reads (r1=0):

 read(a, !null)
    \--po--> vread(a.f, 0)
                  \---sw---> vstore(a.f, 42)
                                  \---po---> store(a)

Note this yields the transitive closure over {po, sw}, and that is
happens-before:

 read(a, !null)
    \--hb--> vread(a.f, 0)
                  \---hb---> vstore(a.f, 42)
                                  \---hb---> store(a)

Trace B: one that reads (r1=42):

   vstore(a.f, 42)
     \       \-----------sw------------------> vread(a.f, 42)
      \                      read(a, !null) ----po----^
       \----po--->store(a)

Happens-before-wise, this trace only closes over the vstore-vread:

   vstore(a.f, 42)
             \-----------hb------------------> vread(a.f, 42)
                             read(a, !null)
                  store(a)

Now, let's figure out if these traces are committable. You might notice
the required commit sequence for both these traces are from left to
right, by accident.

Let us start from the trace B, because it appears simpler. We are
committing the actions in this order:
  C0 = {}
  C1 = C0 union { vstore(a.f, 42) }
  C2 = C1 union { store(a) }
  C3 = C2 union { read(a, !null) }  // sees the store(a)
  C4 = C3 union { vread(a.f, 42) }  // sees the vstore(a.f, 42)

Thus, trace B is committable, and then the behaviors expressed in this
trace are sound JMM-wise.

Now, let's take the trace A:
 a) We can not commit read(a, !null) before store(a) is committed,
because it violates the written values consistency (i.e. V[i]|C[i] =
V|C[i] in spec).
 b) We can not commit store(a) before vstore(a.f, 42) is committed,
because it violates happens-before (i.e. hb[i]|C[i] = hb|C[i] in spec)
c) We can not commit that vstore(a.f, 42) either because we should
commit vread(a.f, 0) before, otherwise we violate the written values
consistency again.
 d) And in the end, we can only commit vread(a.f, 0) after the read(a,
!null) is committed, otherwise it violates happens-before again.

Hence, we end up with the causality loop, and this trace is not
committable. Hence, the result of trace A is not sound JMM-wise.

The only committable trace is trace B, and it yields (r1 = 42).
Q.E.D.

Thanks,
-Aleksey.

P.S. Note that if a.f was not volatile, happens-before would not be
induced, and we will end up with the committable trace yielding (r1=0):

                                         store(a, 42)
  store(a)
             read(a, !null)
                               read(a, 0)

P.P.S. Doug had this to say in the internal review for this post:

"I think the confusion in this particular case might have arisen years
ago in a discussion about how you don't actually need a full fence after
each volatile assignment in code-less constructors (because
their relative orderings cannot matter), except for the last trailing
one, which can in turn be weakened to a (final-field style) store fence
to prevent reorderings wrt actions by the caller. Maybe this
side-condition became forgotten."

From oleksandr.otenko at oracle.com  Wed Nov 27 08:13:30 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 27 Nov 2013 13:13:30 +0000
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295E24C.40608@oracle.com>
References: <5295E24C.40608@oracle.com>
Message-ID: <5295EFFA.5090701@oracle.com>

The actual instructions on the left are:

r2=new A();
r2.f=42;
a=r2;


You have no mention of store(a) being strictly after store(a.f,42) in 
your reasoning; which is I think what Doug means by "final field" semantics.

If JLS already mentions that constructors with volatile stores in them 
are treated differently, then there is no ambiguity already.

Alex


On 27/11/2013 12:15, Aleksey Shipilev wrote:
> Hi,
>
> This matter was discussed some time ago privately, but I want to have
> the more rigorous public discussion on this. Let us take the specific
> test as the example. Suppose you have the class:
>
>     class A {
>        volatile int f;
>        A() {
>           f = 42;
>        }
>     }
>
> ...and this test case (note it deliberately omits NPE cases to make
> reasoning simpler):
>
>                   A a;
>     ---------------|----------------
>      a = new A();  |   r1 = a.f;
>
> Except for NPEs, what are the possible values for r1? Both Doug, me, and
> some other folks agree that the only allowed value is {42}, even  though
> it is not evident at first sight, especially if you are contaminated
> with reordering/roach-motel thinking.
>
> The sketch for the proof follows. The surprising consequence of the
> proof below is that roach motel semantics is not always applicable. I
> would be happy to see the flaws in this reasoning or maybe even the
> better proof for this.
>
> In order to answer what outcomes are possible we need to dump the
> usual/partial/misleading "reorderings" and "happens-before" mindset, and
> get to the ground of spec. That is, we need to construct the possible
> traces and see if those traces are committable, as per JLS 17.4.8.
>
> Constructing the traces is arguably easy because we have the volatile
> ops in the trace. It means we can construct two basic traces, for the
> two only juxtapositions of volatile ops in the trace:
>
> Trace A: the one that reads (r1=0):
>
>     vread(a.f, 0)
>             \----sw---> vstore(a.f, 42)
>
> Trace B: the one that reads (r1=42):
>
>     vstore(a.f, 42)
>              \----sw---> vread(a.f, 42)
>
> Now, extending that that skeleton with the program order, will yield two
> final traces:
>
> Trace A: one that reads (r1=0):
>
>   read(a, !null)
>      \--po--> vread(a.f, 0)
>                    \---sw---> vstore(a.f, 42)
>                                    \---po---> store(a)
>
> Note this yields the transitive closure over {po, sw}, and that is
> happens-before:
>
>   read(a, !null)
>      \--hb--> vread(a.f, 0)
>                    \---hb---> vstore(a.f, 42)
>                                    \---hb---> store(a)
>
> Trace B: one that reads (r1=42):
>
>     vstore(a.f, 42)
>       \       \-----------sw------------------> vread(a.f, 42)
>        \                      read(a, !null) ----po----^
>         \----po--->store(a)
>
> Happens-before-wise, this trace only closes over the vstore-vread:
>
>     vstore(a.f, 42)
>               \-----------hb------------------> vread(a.f, 42)
>                               read(a, !null)
>                    store(a)
>
> Now, let's figure out if these traces are committable. You might notice
> the required commit sequence for both these traces are from left to
> right, by accident.
>
> Let us start from the trace B, because it appears simpler. We are
> committing the actions in this order:
>    C0 = {}
>    C1 = C0 union { vstore(a.f, 42) }
>    C2 = C1 union { store(a) }
>    C3 = C2 union { read(a, !null) }  // sees the store(a)
>    C4 = C3 union { vread(a.f, 42) }  // sees the vstore(a.f, 42)
>
> Thus, trace B is committable, and then the behaviors expressed in this
> trace are sound JMM-wise.
>
> Now, let's take the trace A:
>   a) We can not commit read(a, !null) before store(a) is committed,
> because it violates the written values consistency (i.e. V[i]|C[i] =
> V|C[i] in spec).
>   b) We can not commit store(a) before vstore(a.f, 42) is committed,
> because it violates happens-before (i.e. hb[i]|C[i] = hb|C[i] in spec)
> c) We can not commit that vstore(a.f, 42) either because we should
> commit vread(a.f, 0) before, otherwise we violate the written values
> consistency again.
>   d) And in the end, we can only commit vread(a.f, 0) after the read(a,
> !null) is committed, otherwise it violates happens-before again.
>
> Hence, we end up with the causality loop, and this trace is not
> committable. Hence, the result of trace A is not sound JMM-wise.
>
> The only committable trace is trace B, and it yields (r1 = 42).
> Q.E.D.
>
> Thanks,
> -Aleksey.
>
> P.S. Note that if a.f was not volatile, happens-before would not be
> induced, and we will end up with the committable trace yielding (r1=0):
>
>                                           store(a, 42)
>    store(a)
>               read(a, !null)
>                                 read(a, 0)
>
> P.P.S. Doug had this to say in the internal review for this post:
>
> "I think the confusion in this particular case might have arisen years
> ago in a discussion about how you don't actually need a full fence after
> each volatile assignment in code-less constructors (because
> their relative orderings cannot matter), except for the last trailing
> one, which can in turn be weakened to a (final-field style) store fence
> to prevent reorderings wrt actions by the caller. Maybe this
> side-condition became forgotten."
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aleksey.shipilev at oracle.com  Wed Nov 27 08:17:21 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 27 Nov 2013 17:17:21 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295EFFA.5090701@oracle.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
Message-ID: <5295F0E1.7040707@oracle.com>

Hi Oleksandr,

On 11/27/2013 05:13 PM, Oleksandr Otenko wrote:
> The actual instructions on the left are:
> 
> r2=new A();
> r2.f=42;
> a=r2;
> 
> 
> You have no mention of store(a) being strictly after store(a.f,42) in
> your reasoning; which is I think what Doug means by "final field"
> semantics.

Yeah, traces are implying this order there, see the program order
constraints.

> If JLS already mentions that constructors with volatile stores in them
> are treated differently, then there is no ambiguity already.

Hm. Care to share the exact pointer?

-Aleksey.

From andreas.lochbihler at inf.ethz.ch  Wed Nov 27 08:17:40 2013
From: andreas.lochbihler at inf.ethz.ch (Andreas Lochbihler)
Date: Wed, 27 Nov 2013 14:17:40 +0100
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295E24C.40608@oracle.com>
References: <5295E24C.40608@oracle.com>
Message-ID: <5295F0F4.8070500@inf.ethz.ch>

Hi Aleksey,

I think that 0 is also an acceptable outcome. In your reasoning, you do not distinguish 
between synchronization order (so) and synchronizes-with order (sw), and only sw if part 
of the happens-before order (hb). The difference is that so orders all access to volatile 
whereas sw only orders writes before subsequent reads, but not reads before (subsequent) 
writes (JLS 17.4.4, 2nd bullet). Consequently, trace A looks as follows:

Trace A: one that reads (r1=0):

   read(a, !null)
      \--po--> vread(a.f, 0)
                    \---so---> vstore(a.f, 42)
                                    \---po---> store(a)

In particular, we don't get vread(a.f, 0) ---hb---> vstore(a.f, 42). Hence, we can commit 
trace A as follows (I write A for the object location of the A object):

1. Commit the initializations a = null, A.f = 0, the initial thread actions, store(a, A).
    read(a, null) sees the initialization a = null
    vread(a.f, 0) actually does not yet exist (if it did, assume it sees A.f = 0)

2. Commit read(a, null), write-seen is as before.

3. Change read(a, null) to now see store(a, A).

    vread(a.f, 0) now executes, so we must add it to the synchronisation order, pick:
    vstore(A.f, null) --so--> vread(A.f, null) --so--> vstore(A.f, 42)

    Hence, vread(a.f, 0) sees vstore(A.f, 0)

    Commit all remaining actions, i.e., vstore(A.f, 42) and vread(A.f, 0).

Andreas

On 27/11/13 13:15, Aleksey Shipilev wrote:
> Hi,
>
> This matter was discussed some time ago privately, but I want to have
> the more rigorous public discussion on this. Let us take the specific
> test as the example. Suppose you have the class:
>
>     class A {
>        volatile int f;
>        A() {
>           f = 42;
>        }
>     }
>
> ...and this test case (note it deliberately omits NPE cases to make
> reasoning simpler):
>
>                   A a;
>     ---------------|----------------
>      a = new A();  |   r1 = a.f;
>
> Except for NPEs, what are the possible values for r1? Both Doug, me, and
> some other folks agree that the only allowed value is {42}, even  though
> it is not evident at first sight, especially if you are contaminated
> with reordering/roach-motel thinking.
>
> The sketch for the proof follows. The surprising consequence of the
> proof below is that roach motel semantics is not always applicable. I
> would be happy to see the flaws in this reasoning or maybe even the
> better proof for this.
>
> In order to answer what outcomes are possible we need to dump the
> usual/partial/misleading "reorderings" and "happens-before" mindset, and
> get to the ground of spec. That is, we need to construct the possible
> traces and see if those traces are committable, as per JLS 17.4.8.
>
> Constructing the traces is arguably easy because we have the volatile
> ops in the trace. It means we can construct two basic traces, for the
> two only juxtapositions of volatile ops in the trace:
>
> Trace A: the one that reads (r1=0):
>
>     vread(a.f, 0)
>             \----sw---> vstore(a.f, 42)
>
> Trace B: the one that reads (r1=42):
>
>     vstore(a.f, 42)
>              \----sw---> vread(a.f, 42)
>
> Now, extending that that skeleton with the program order, will yield two
> final traces:
>
> Trace A: one that reads (r1=0):
>
>   read(a, !null)
>      \--po--> vread(a.f, 0)
>                    \---sw---> vstore(a.f, 42)
>                                    \---po---> store(a)
>
> Note this yields the transitive closure over {po, sw}, and that is
> happens-before:
>
>   read(a, !null)
>      \--hb--> vread(a.f, 0)
>                    \---hb---> vstore(a.f, 42)
>                                    \---hb---> store(a)
>
> Trace B: one that reads (r1=42):
>
>     vstore(a.f, 42)
>       \       \-----------sw------------------> vread(a.f, 42)
>        \                      read(a, !null) ----po----^
>         \----po--->store(a)
>
> Happens-before-wise, this trace only closes over the vstore-vread:
>
>     vstore(a.f, 42)
>               \-----------hb------------------> vread(a.f, 42)
>                               read(a, !null)
>                    store(a)
>
> Now, let's figure out if these traces are committable. You might notice
> the required commit sequence for both these traces are from left to
> right, by accident.
>
> Let us start from the trace B, because it appears simpler. We are
> committing the actions in this order:
>    C0 = {}
>    C1 = C0 union { vstore(a.f, 42) }
>    C2 = C1 union { store(a) }
>    C3 = C2 union { read(a, !null) }  // sees the store(a)
>    C4 = C3 union { vread(a.f, 42) }  // sees the vstore(a.f, 42)
>
> Thus, trace B is committable, and then the behaviors expressed in this
> trace are sound JMM-wise.
>
> Now, let's take the trace A:
>   a) We can not commit read(a, !null) before store(a) is committed,
> because it violates the written values consistency (i.e. V[i]|C[i] =
> V|C[i] in spec).
>   b) We can not commit store(a) before vstore(a.f, 42) is committed,
> because it violates happens-before (i.e. hb[i]|C[i] = hb|C[i] in spec)
> c) We can not commit that vstore(a.f, 42) either because we should
> commit vread(a.f, 0) before, otherwise we violate the written values
> consistency again.
>   d) And in the end, we can only commit vread(a.f, 0) after the read(a,
> !null) is committed, otherwise it violates happens-before again.
>
> Hence, we end up with the causality loop, and this trace is not
> committable. Hence, the result of trace A is not sound JMM-wise.
>
> The only committable trace is trace B, and it yields (r1 = 42).
> Q.E.D.
>
> Thanks,
> -Aleksey.
>
> P.S. Note that if a.f was not volatile, happens-before would not be
> induced, and we will end up with the committable trace yielding (r1=0):
>
>                                           store(a, 42)
>    store(a)
>               read(a, !null)
>                                 read(a, 0)
>
> P.P.S. Doug had this to say in the internal review for this post:
>
> "I think the confusion in this particular case might have arisen years
> ago in a discussion about how you don't actually need a full fence after
> each volatile assignment in code-less constructors (because
> their relative orderings cannot matter), except for the last trailing
> one, which can in turn be weakened to a (final-field style) store fence
> to prevent reorderings wrt actions by the caller. Maybe this
> side-condition became forgotten."
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From oleksandr.otenko at oracle.com  Wed Nov 27 08:21:08 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 27 Nov 2013 13:21:08 +0000
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295EFFA.5090701@oracle.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
Message-ID: <5295F1C4.3070405@oracle.com>

pardon me, store(a,r2) and store(r2.f,42) were meant.

You need a statement in JLS where constructors with volatile stores need 
to be executed differently (compare to permitted orderings of the same 
code, but f being non-volatile).

Alex


On 27/11/2013 13:13, Oleksandr Otenko wrote:
> The actual instructions on the left are:
>
> r2=new A();
> r2.f=42;
> a=r2;
>
>
> You have no mention of *store(a) being strictly after store(a.f,42)* 
> in your reasoning; which is I think what Doug means by "final field" 
> semantics.
>
> If JLS already mentions that constructors with volatile stores in them 
> are treated differently, then there is no ambiguity already.
>
> Alex
>
>
> On 27/11/2013 12:15, Aleksey Shipilev wrote:
>> Hi,
>>
>> This matter was discussed some time ago privately, but I want to have
>> the more rigorous public discussion on this. Let us take the specific
>> test as the example. Suppose you have the class:
>>
>>     class A {
>>        volatile int f;
>>        A() {
>>           f = 42;
>>        }
>>     }
>>
>> ...and this test case (note it deliberately omits NPE cases to make
>> reasoning simpler):
>>
>>                   A a;
>>     ---------------|----------------
>>      a = new A();  |   r1 = a.f;
>>
>> Except for NPEs, what are the possible values for r1? Both Doug, me, and
>> some other folks agree that the only allowed value is {42}, even  though
>> it is not evident at first sight, especially if you are contaminated
>> with reordering/roach-motel thinking.
>>
>> The sketch for the proof follows. The surprising consequence of the
>> proof below is that roach motel semantics is not always applicable. I
>> would be happy to see the flaws in this reasoning or maybe even the
>> better proof for this.
>>
>> In order to answer what outcomes are possible we need to dump the
>> usual/partial/misleading "reorderings" and "happens-before" mindset, and
>> get to the ground of spec. That is, we need to construct the possible
>> traces and see if those traces are committable, as per JLS 17.4.8.
>>
>> Constructing the traces is arguably easy because we have the volatile
>> ops in the trace. It means we can construct two basic traces, for the
>> two only juxtapositions of volatile ops in the trace:
>>
>> Trace A: the one that reads (r1=0):
>>
>>     vread(a.f, 0)
>>             \----sw---> vstore(a.f, 42)
>>
>> Trace B: the one that reads (r1=42):
>>
>>     vstore(a.f, 42)
>>              \----sw---> vread(a.f, 42)
>>
>> Now, extending that that skeleton with the program order, will yield two
>> final traces:
>>
>> Trace A: one that reads (r1=0):
>>
>>   read(a, !null)
>>      \--po--> vread(a.f, 0)
>>                    \---sw---> vstore(a.f, 42)
>>                                    \---po---> store(a)
>>
>> Note this yields the transitive closure over {po, sw}, and that is
>> happens-before:
>>
>>   read(a, !null)
>>      \--hb--> vread(a.f, 0)
>>                    \---hb---> vstore(a.f, 42)
>>                                    \---hb---> store(a)
>>
>> Trace B: one that reads (r1=42):
>>
>>     vstore(a.f, 42)
>>       \       \-----------sw------------------> vread(a.f, 42)
>>        \                      read(a, !null) ----po----^
>>         \----po--->store(a)
>>
>> Happens-before-wise, this trace only closes over the vstore-vread:
>>
>>     vstore(a.f, 42)
>>               \-----------hb------------------> vread(a.f, 42)
>>                               read(a, !null)
>>                    store(a)
>>
>> Now, let's figure out if these traces are committable. You might notice
>> the required commit sequence for both these traces are from left to
>> right, by accident.
>>
>> Let us start from the trace B, because it appears simpler. We are
>> committing the actions in this order:
>>    C0 = {}
>>    C1 = C0 union { vstore(a.f, 42) }
>>    C2 = C1 union { store(a) }
>>    C3 = C2 union { read(a, !null) }  // sees the store(a)
>>    C4 = C3 union { vread(a.f, 42) }  // sees the vstore(a.f, 42)
>>
>> Thus, trace B is committable, and then the behaviors expressed in this
>> trace are sound JMM-wise.
>>
>> Now, let's take the trace A:
>>   a) We can not commit read(a, !null) before store(a) is committed,
>> because it violates the written values consistency (i.e. V[i]|C[i] =
>> V|C[i] in spec).
>>   b) We can not commit store(a) before vstore(a.f, 42) is committed,
>> because it violates happens-before (i.e. hb[i]|C[i] = hb|C[i] in spec)
>> c) We can not commit that vstore(a.f, 42) either because we should
>> commit vread(a.f, 0) before, otherwise we violate the written values
>> consistency again.
>>   d) And in the end, we can only commit vread(a.f, 0) after the read(a,
>> !null) is committed, otherwise it violates happens-before again.
>>
>> Hence, we end up with the causality loop, and this trace is not
>> committable. Hence, the result of trace A is not sound JMM-wise.
>>
>> The only committable trace is trace B, and it yields (r1 = 42).
>> Q.E.D.
>>
>> Thanks,
>> -Aleksey.
>>
>> P.S. Note that if a.f was not volatile, happens-before would not be
>> induced, and we will end up with the committable trace yielding (r1=0):
>>
>>                                           store(a, 42)
>>    store(a)
>>               read(a, !null)
>>                                 read(a, 0)
>>
>> P.P.S. Doug had this to say in the internal review for this post:
>>
>> "I think the confusion in this particular case might have arisen years
>> ago in a discussion about how you don't actually need a full fence after
>> each volatile assignment in code-less constructors (because
>> their relative orderings cannot matter), except for the last trailing
>> one, which can in turn be weakened to a (final-field style) store fence
>> to prevent reorderings wrt actions by the caller. Maybe this
>> side-condition became forgotten."
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/5e8c2f38/attachment.html>

From oleksandr.otenko at oracle.com  Wed Nov 27 08:32:41 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 27 Nov 2013 13:32:41 +0000
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295F0E1.7040707@oracle.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F0E1.7040707@oracle.com>
Message-ID: <5295F479.3070709@oracle.com>

That is actually my point - I don't have a pointer to JLS where that 
restriction would be there. Until then you have no proof that 
store(a,r2) is strictly after store(r2.f,42).


Alex


On 27/11/2013 13:17, Aleksey Shipilev wrote:
> Hi Oleksandr,
>
> On 11/27/2013 05:13 PM, Oleksandr Otenko wrote:
>> The actual instructions on the left are:
>>
>> r2=new A();
>> r2.f=42;
>> a=r2;
>>
>>
>> You have no mention of store(a) being strictly after store(a.f,42) in
>> your reasoning; which is I think what Doug means by "final field"
>> semantics.
> Yeah, traces are implying this order there, see the program order
> constraints.
>
>> If JLS already mentions that constructors with volatile stores in them
>> are treated differently, then there is no ambiguity already.
> Hm. Care to share the exact pointer?
>
> -Aleksey.


From dl at cs.oswego.edu  Wed Nov 27 08:49:14 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 27 Nov 2013 08:49:14 -0500
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295EFFA.5090701@oracle.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
Message-ID: <5295F85A.40500@cs.oswego.edu>

On 11/27/2013 08:13 AM, Oleksandr Otenko wrote:

> If JLS already mentions that constructors with volatile stores in them are
> treated differently, then there is no ambiguity already.

It doesn't, but there are common cases where the effects can be
made identical to how final fields are treated (which is
convenient for JVMs).

Using only the simple cookbook approximations to JMM rules,
including that you cannot ever reorder volatile stores with
any other stores, plus the basic weakening transformations
(see "Removing Barriers" section in 
http://gee.cs.oswego.edu/dl/jmm/cookbook.html), you can reason as follows for 
the example:

class C {
   volatile int x, y;
   C(int a, int b) { x = a; y = b; }
}

There are no previous accesses of x or y.
And none except for the stores within the constructor.
And none until after the second store,
which cannot be reordered wrt other stores.

In this case, you don't need any fencing between the stores
to x and y. But you still need to prevent store reorderings
by the caller of the constructor, which can be done
in the same way it is done for classes with final fields:
placing a store fence at end of constructor.

-Doug



From vitalyd at gmail.com  Wed Nov 27 09:06:34 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 27 Nov 2013 09:06:34 -0500
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295F85A.40500@cs.oswego.edu>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
Message-ID: <CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>

Doug,

But the issue is whether assignment of the new C instance to some other
memory must occur after the volatile stores.  JMM and your cookbook allow
subsequent normal stores to float above prior volatile stores, which is
exactly the case of assigning the C instance.  Final field assignments
explicitly disallow this reordering, but where's the same for volatiles?

Constructors are just regular methods for this example *unless* we call
them out explicitly for this case, like finals.  Otherwise, if pretend the
constructor is just some method foo() with exact same logic, everyone
agrees that the reordering is possible.

Thanks

Sent from my phone
On Nov 27, 2013 9:01 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 11/27/2013 08:13 AM, Oleksandr Otenko wrote:
>
>  If JLS already mentions that constructors with volatile stores in them are
>> treated differently, then there is no ambiguity already.
>>
>
> It doesn't, but there are common cases where the effects can be
> made identical to how final fields are treated (which is
> convenient for JVMs).
>
> Using only the simple cookbook approximations to JMM rules,
> including that you cannot ever reorder volatile stores with
> any other stores, plus the basic weakening transformations
> (see "Removing Barriers" section in http://gee.cs.oswego.edu/dl/
> jmm/cookbook.html), you can reason as follows for the example:
>
> class C {
>   volatile int x, y;
>   C(int a, int b) { x = a; y = b; }
> }
>
> There are no previous accesses of x or y.
> And none except for the stores within the constructor.
> And none until after the second store,
> which cannot be reordered wrt other stores.
>
> In this case, you don't need any fencing between the stores
> to x and y. But you still need to prevent store reorderings
> by the caller of the constructor, which can be done
> in the same way it is done for classes with final fields:
> placing a store fence at end of constructor.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/daa98e0b/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Nov 27 09:08:16 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 27 Nov 2013 14:08:16 +0000
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295F85A.40500@cs.oswego.edu>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
Message-ID: <5295FCD0.4080104@oracle.com>

Yes, the elimination of barriers between multiple stores is clear. I am 
only saying that here:

r2=new A();
r2.f=42;
a=r2;

we know store(a,r2) can go ahead of store(r2.f,42), even if 
store(r2.f,42) is a volatile store - which makes a.f==0 observable.

Adding final field semantics makes sense, I was only commenting on 
Alexey's proof. It wasn't particularly clear whether he was saying "this 
is how we want it to behave" or "look, this is how it already behaves". 
His statement about "we can't always use reordering logic" made it vague 
- did he mean there is a JLS statement that already precludes that 
"obvious" reordering.


Alex

On 27/11/2013 13:49, Doug Lea wrote:
> On 11/27/2013 08:13 AM, Oleksandr Otenko wrote:
>
>> If JLS already mentions that constructors with volatile stores in 
>> them are
>> treated differently, then there is no ambiguity already.
>
> It doesn't, but there are common cases where the effects can be
> made identical to how final fields are treated (which is
> convenient for JVMs).
>
> Using only the simple cookbook approximations to JMM rules,
> including that you cannot ever reorder volatile stores with
> any other stores, plus the basic weakening transformations
> (see "Removing Barriers" section in 
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html), you can reason as 
> follows for the example:
>
> class C {
>   volatile int x, y;
>   C(int a, int b) { x = a; y = b; }
> }
>
> There are no previous accesses of x or y.
> And none except for the stores within the constructor.
> And none until after the second store,
> which cannot be reordered wrt other stores.
>
> In this case, you don't need any fencing between the stores
> to x and y. But you still need to prevent store reorderings
> by the caller of the constructor, which can be done
> in the same way it is done for classes with final fields:
> placing a store fence at end of constructor.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From rco at quartetfs.com  Wed Nov 27 11:09:20 2013
From: rco at quartetfs.com (Romain Colle)
Date: Wed, 27 Nov 2013 17:09:20 +0100
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
Message-ID: <CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>

Hi all,

I'd like to second Jonathan's post on this subject.
The basic idea here is that, as far as I can tell, the get(long timeout,
TimeUnit unit) method of a ForkJoinTask does not respect the Future
contract.

Before blocking and waiting for the task to complete, the timed get()
method tries to either steal and execute the task, or at least help it
complete.
If it succeeds doing so, it will for instance execute the task that it is
waiting for. And in this case, there is no telling how long it will take to
execute it ...

Although this is great for the overall throughput, a thread calling the
timed get() method with a timeout of 1 second might end up returning a
minute later (if the task does take a minute to execute).
This is what Jonathan reproduced in its unit test (with a parallelism of 1
in its pool to consistently reproduce this behavior).

This behavior does causes some problems in our code, and to my knowledge
there are no ways of invoking a reliable timed get() on a ForkJoinPool.
Any feedback on this topic would be greatly appreciated!

Thanks
Romain


On Thu, Nov 21, 2013 at 8:35 AM, Martin Buchholz <martinrb at google.com>wrote:

> Your message got ignored...
>
> You created a pool with parallelism of 1, so it's not surprising that no
> new thread was created to run action1.  The action of the pool seems
> reasonable to me - how would you do better?
>
> Also, posting your code with a friendlier legal notice might help.
>
>
> On Mon, Nov 18, 2013 at 8:35 AM, Jonathan Soto <jso at quartetfs.com> wrote:
>
>> Hello everyone,
>> It has been a few days I am thinking about this problem. Could you help
>> me ?
>>
>> Suppose that we have two tasks
>> - task1 waits for 5s
>> - task2 forks task1 and does a timed get of 2.5 seconds
>>
>> The main thread submits task2 to a FJP.
>>
>> I observe the following behavior.
>> - Thread Main   :
>> FPJ.submit(task2)
>>
>> - Thread FJPO-1 : task1.fork
>> As we are in the FJP it adds task1 into the workQueue of the current
>> thread
>> Task1 does not start in another thread
>>
>> - Thread FJPO-1 : task1.get(2.5, SECONDS)
>> Starts by checking if the current thread has some local tasks to be done.
>> As there is task1 that has been previously stored, we execute it.
>>
>> Task1 takes 5 seconds to be executed. The 2.5 seconds timeout is totally
>> ignored.
>>
>> Is that the expected behavior ?
>>
>> Thanks
>>
>> Jonathan
>>
>> P.s in attachment a unit test that prints FAIL when the test fails
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Romain Colle
R&D Project Manager
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/6f3bed89/attachment.html>

From valentin.male.kovalenko at gmail.com  Wed Nov 27 12:06:25 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Wed, 27 Nov 2013 21:06:25 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
Message-ID: <CAO-wXwLQWGX3wjhD3yDDe55P1CtaW_wFonX9hv5U4JY+HCCQXw@mail.gmail.com>

Aleksey Shipilev wrote
>>Trace A: one that reads (r1=0):
>> read(a, !null)
>>    \--po--> vread(a.f, 0)
>>                   \---sw---> vstore(a.f, 42)
>>                                   \---po---> store(a)
Andreas Lochbihler wrote
>>In your reasoning, you do not distinguish between synchronization order
(so) and synchronizes-with order (sw)

I agree with Andreas: can't see why there is an sw(vread(a.f, 0), vstore(a.f,
42)) because as JSL states: "A write to a volatile variable v
synchronizes-with all subsequent reads of v by any thread (where
"subsequent" is defined according to the synchronization order)". And
according to Aleksey's trace vread(a.f, 0) is antecedent (not subsequent)
of vstore(a.f, 42) so there is no SW.

Doug Lea wrote
>>you cannot ever reorder volatile stores with any other stores

How is that can be true? Considering the following example:
store(nonVolatile1, 1);
vstore(a.f, 42);
store(nonVolatile2, 2);
I agree that store(nonVolatile1, 1) can't be reordered with vstore(a.f,
42), but store(nonVolatile2, 2) can be reordered with vstore(a.f, 42) and
this is exactly what we have in the original example: store(a) can be
reordered with vstore(a.f, 42). Where am I wrong?

Still I believe that "the only allowed value is {42}", however I can't
understand why it's correct...
-- 
Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/dadb9217/attachment.html>

From aleksey.shipilev at oracle.com  Wed Nov 27 12:42:26 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 27 Nov 2013 21:42:26 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5295F0F4.8070500@inf.ethz.ch>
References: <5295E24C.40608@oracle.com> <5295F0F4.8070500@inf.ethz.ch>
Message-ID: <52962F02.6090604@oracle.com>

Hi Andreas,

On 11/27/2013 05:17 PM, Andreas Lochbihler wrote:
> I think that 0 is also an acceptable outcome. In your reasoning, you do
> not distinguish between synchronization order (so) and synchronizes-with
> order (sw), and only sw if part of the happens-before order (hb). The
> difference is that so orders all access to volatile whereas sw only
> orders writes before subsequent reads, but not reads before (subsequent)
> writes (JLS 17.4.4, 2nd bullet).

Thanks Andreas, that's indeed the flaw which demolishes the significant
part of the proof. It seems like both traces are committable, yielding
both {0, 42}. Let me go through the JMM again to understand your commit
sequence completely.

-Aleksey.

From dl at cs.oswego.edu  Wed Nov 27 13:41:47 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 27 Nov 2013 13:41:47 -0500
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
	<CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>
Message-ID: <52963CEB.5070703@cs.oswego.edu>

On 11/27/2013 09:06 AM, Vitaly Davidovich wrote:

> But the issue is whether assignment of the new C instance to some other memory
> must occur after the volatile stores.

Sorry for mis-remembering why I had treated this issue as basically settled:
Unless a JVM always pre-zeros memory (which usually not a good option), then
even if not explicitly initialized, volatile fields must be zeroed
in the constructor body, with a release fence before publication.
And so even though there are cases in which the JMM does not
strictly require mechanics preventing publication reordering
in constructors of classes with volatile fields, the only good
implementation choices for JVMs are either to use non-volatile writes
with a trailing release fence, or to perform each volatile write
with full fencing. Either way, there is no reordering with publication.
Unfortunately, programmers cannot rely on a spec to guarantee
it, at least until the JMM is revised.

-Doug




From dl at cs.oswego.edu  Wed Nov 27 14:01:43 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 27 Nov 2013 14:01:43 -0500
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
Message-ID: <52964197.5010801@cs.oswego.edu>

On 11/27/2013 11:09 AM, Romain Colle wrote:

> The basic idea here is that, as far as I can tell, the get(long timeout,
> TimeUnit unit) method of a ForkJoinTask does not respect the Future contract.
>

Well, it does meet spec in that it times out no earlier than required.

> Before blocking and waiting for the task to complete, the timed get() method
> tries to either steal and execute the task, or at least help it complete.
> If it succeeds doing so, it will for instance execute the task that it is
> waiting for. And in this case, there is no telling how long it will take to
> execute it ...

Yes. Given the documentation of FJ, people ought to expect this
and would claim it to be a bug if it did otherwise. Maybe we
should add better method documentation for timed get, and also
explain a few ways to get non-participatory timeouts.
For example, creating a new thread to perform the invocation
and timing out on Thread.join. Suggestions welcome.

-Doug


From vitalyd at gmail.com  Wed Nov 27 14:06:06 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 27 Nov 2013 14:06:06 -0500
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <52963CEB.5070703@cs.oswego.edu>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
	<CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>
	<52963CEB.5070703@cs.oswego.edu>
Message-ID: <CAHjP37GLPJs1Ue0R6mVR55bUYJc1GGgz1a0vyiZs7QctAj-GkA@mail.gmail.com>

Why do volatile fields need explicit pre zeroing (with a fence) while
normal fields don't then? Also, I think JVM can only avoid pre zeroing if
it can prove that subsequent code overwrites the value before anything can
be observed.  It definitely tries to do this for array allocations; if it
didn't pre zero who's going to ensure that unset slots are actually
zero/default if read? But either way, optimizations to avoid zeroing memory
is an implementation detail and thus cannot be relied upon from JMM
standpoint, which I think you're saying.

The other issue is that even if we determine that volatiles don't get this
treatment, if JVM is already ensuring it, it's not going to be practical to
change it and risk hard to debug problems creeping in.  May as well update
the spec now ...

Sent from my phone
On Nov 27, 2013 1:41 PM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 11/27/2013 09:06 AM, Vitaly Davidovich wrote:
>
>  But the issue is whether assignment of the new C instance to some other
>> memory
>> must occur after the volatile stores.
>>
>
> Sorry for mis-remembering why I had treated this issue as basically
> settled:
> Unless a JVM always pre-zeros memory (which usually not a good option),
> then
> even if not explicitly initialized, volatile fields must be zeroed
> in the constructor body, with a release fence before publication.
> And so even though there are cases in which the JMM does not
> strictly require mechanics preventing publication reordering
> in constructors of classes with volatile fields, the only good
> implementation choices for JVMs are either to use non-volatile writes
> with a trailing release fence, or to perform each volatile write
> with full fencing. Either way, there is no reordering with publication.
> Unfortunately, programmers cannot rely on a spec to guarantee
> it, at least until the JMM is revised.
>
> -Doug
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/08a39366/attachment.html>

From dl at cs.oswego.edu  Wed Nov 27 14:26:10 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 27 Nov 2013 14:26:10 -0500
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <CAHjP37GLPJs1Ue0R6mVR55bUYJc1GGgz1a0vyiZs7QctAj-GkA@mail.gmail.com>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
	<CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>
	<52963CEB.5070703@cs.oswego.edu>
	<CAHjP37GLPJs1Ue0R6mVR55bUYJc1GGgz1a0vyiZs7QctAj-GkA@mail.gmail.com>
Message-ID: <52964752.3020104@cs.oswego.edu>

On 11/27/2013 02:06 PM, Vitaly Davidovich wrote:
> But either
> way, optimizations to avoid zeroing memory is an implementation detail and thus
> cannot be relied upon from JMM standpoint, which I think you're saying.
>
> The other issue is that even if we determine that volatiles don't get this
> treatment, if JVM is already ensuring it, it's not going to be practical to
> change it and risk hard to debug problems creeping in.  May as well update the
> spec now ...

Right. To summarize:

* Programmers do not expect that even though final fields are specifically
publication-safe, volatile fields are not always so.

* For various implementation reasons, JVMs arrange that
volatile fields are publication safe anyway, at least in
cases we know about.

* Actually updating the JMM/JLS to mandate this is not easy
(no small tweak that I know applies). But now is a good time
to be considering a full revision for JDK9.

* In the mean time, it would make sense to further test
and validate JVMs as meeting this likely future spec.

-Doug


From viktor.klang at gmail.com  Wed Nov 27 15:03:35 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 27 Nov 2013 12:03:35 -0800
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <52964197.5010801@cs.oswego.edu>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
Message-ID: <CANPzfU-6Qvw8FbW5xabmphLE=RbrNK9Hsgct+s=AUjrp9PAKuQ@mail.gmail.com>

On Wed, Nov 27, 2013 at 11:01 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 11/27/2013 11:09 AM, Romain Colle wrote:
>
>  The basic idea here is that, as far as I can tell, the get(long timeout,
>> TimeUnit unit) method of a ForkJoinTask does not respect the Future
>> contract.
>>
>>
> Well, it does meet spec in that it times out no earlier than required.
>
>
>  Before blocking and waiting for the task to complete, the timed get()
>> method
>> tries to either steal and execute the task, or at least help it complete.
>> If it succeeds doing so, it will for instance execute the task that it is
>> waiting for. And in this case, there is no telling how long it will take
>> to
>> execute it ...
>>
>
> Yes. Given the documentation of FJ, people ought to expect this
> and would claim it to be a bug if it did otherwise. Maybe we
> should add better method documentation for timed get, and also
> explain a few ways to get non-participatory timeouts.
> For example, creating a new thread to perform the invocation
> and timing out on Thread.join. Suggestions welcome.


Have ``get`` use managed blocking? Is that an option?

Cheers,
?


>
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?

*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/d2e95af9/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Nov 27 17:38:45 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Nov 2013 08:38:45 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <5295F85A.40500@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEKBKCAA.davidcholmes@aapt.net.au>

Hi Doug,

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Doug Lea
> Sent: Wednesday, 27 November 2013 11:49 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Volatile stores in constructors,
> disallowed to see the default value
> 
> 
> On 11/27/2013 08:13 AM, Oleksandr Otenko wrote:
> 
> > If JLS already mentions that constructors with volatile stores 
> in them are
> > treated differently, then there is no ambiguity already.
> 
> It doesn't, but there are common cases where the effects can be
> made identical to how final fields are treated (which is
> convenient for JVMs).
> 
> Using only the simple cookbook approximations to JMM rules,
> including that you cannot ever reorder volatile stores with
> any other stores

Hold on a minute - where did that rule come from?

David
-----

, plus the basic weakening transformations
> (see "Removing Barriers" section in 
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html), you can reason as 
> follows for 
> the example:
> 
> class C {
>    volatile int x, y;
>    C(int a, int b) { x = a; y = b; }
> }
> 
> There are no previous accesses of x or y.
> And none except for the stores within the constructor.
> And none until after the second store,
> which cannot be reordered wrt other stores.
> 
> In this case, you don't need any fencing between the stores
> to x and y. But you still need to prevent store reorderings
> by the caller of the constructor, which can be done
> in the same way it is done for classes with final fields:
> placing a store fence at end of constructor.
> 
> -Doug
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From davidcholmes at aapt.net.au  Wed Nov 27 17:44:06 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Nov 2013 08:44:06 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <52963CEB.5070703@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEKBKCAA.davidcholmes@aapt.net.au>

Doug writes:
> On 11/27/2013 09:06 AM, Vitaly Davidovich wrote:
>
> > But the issue is whether assignment of the new C instance to
> > some other memory must occur after the volatile stores.
>
> Sorry for mis-remembering why I had treated this issue as
> basically settled:
> Unless a JVM always pre-zeros memory (which usually not a good
> option), then
> even if not explicitly initialized, volatile fields must be zeroed
> in the constructor body, with a release fence before publication.

Thank you for clarifying that. So what you are describing is an
implementation action that is only necessary if the memory is not
pre-zeroed. There is nothing in the spec to require this.

David
-----

> And so even though there are cases in which the JMM does not
> strictly require mechanics preventing publication reordering
> in constructors of classes with volatile fields, the only good
> implementation choices for JVMs are either to use non-volatile writes
> with a trailing release fence, or to perform each volatile write
> with full fencing. Either way, there is no reordering with publication.
> Unfortunately, programmers cannot rely on a spec to guarantee
> it, at least until the JMM is revised.
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Wed Nov 27 17:46:41 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Nov 2013 08:46:41 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <5295E24C.40608@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEKBKCAA.davidcholmes@aapt.net.au>

Aleksey,

Aleksey Shipilev wrote on 27/11/2013:
Hi,
>
> This matter was discussed some time ago privately, but I want to have
> the more rigorous public discussion on this. Let us take the specific
> test as the example. Suppose you have the class:
>
>    class A {
>       volatile int f;
>       A() {
>          f = 42;
>       }
>    }
>
> ...and this test case (note it deliberately omits NPE cases to make
> reasoning simpler):
>
>                  A a;
>    ---------------|----------------
>     a = new A();  |   r1 = a.f;
>
> Except for NPEs, what are the possible values for r1? Both Doug, me, and
> some other folks agree that the only allowed value is {42}, even  though
> it is not evident at first sight, especially if you are contaminated
> with reordering/roach-motel thinking.
>
> The sketch for the proof follows. The surprising consequence of the
> proof below is that roach motel semantics is not always applicable. I
> would be happy to see the flaws in this reasoning or maybe even the
> better proof for this.
>
> In order to answer what outcomes are possible we need to dump the
> usual/partial/misleading "reorderings" and "happens-before" mindset, and
> get to the ground of spec. That is, we need to construct the possible
> traces and see if those traces are committable, as per JLS 17.4.8.

And therein lies your mistake. The "happens-before mindset" as you put it is
what determines what the legal executions are. It was obvious from
happens-before considerations that your premise was invalid.

Q.E.D

David
-----

> Constructing the traces is arguably easy because we have the volatile
> ops in the trace. It means we can construct two basic traces, for the
> two only juxtapositions of volatile ops in the trace:
>
> Trace A: the one that reads (r1=0):
>
>    vread(a.f, 0)
>            \----sw---> vstore(a.f, 42)
>
> Trace B: the one that reads (r1=42):
>
>    vstore(a.f, 42)
>             \----sw---> vread(a.f, 42)
>
> Now, extending that that skeleton with the program order, will yield two
> final traces:
>
> Trace A: one that reads (r1=0):
>
>  read(a, !null)
>     \--po--> vread(a.f, 0)
>                   \---sw---> vstore(a.f, 42)
>                                   \---po---> store(a)
>
> Note this yields the transitive closure over {po, sw}, and that is
> happens-before:
>
>  read(a, !null)
>     \--hb--> vread(a.f, 0)
>                   \---hb---> vstore(a.f, 42)
>                                   \---hb---> store(a)
>
> Trace B: one that reads (r1=42):
>
>    vstore(a.f, 42)
>      \       \-----------sw------------------> vread(a.f, 42)
>       \                      read(a, !null) ----po----^
>        \----po--->store(a)
>
> Happens-before-wise, this trace only closes over the vstore-vread:
>
>    vstore(a.f, 42)
>              \-----------hb------------------> vread(a.f, 42)
>                              read(a, !null)
>                   store(a)
>
> Now, let's figure out if these traces are committable. You might notice
> the required commit sequence for both these traces are from left to
> right, by accident.
>
> Let us start from the trace B, because it appears simpler. We are
> committing the actions in this order:
>   C0 = {}
>   C1 = C0 union { vstore(a.f, 42) }
>   C2 = C1 union { store(a) }
>   C3 = C2 union { read(a, !null) }  // sees the store(a)
>   C4 = C3 union { vread(a.f, 42) }  // sees the vstore(a.f, 42)
>
> Thus, trace B is committable, and then the behaviors expressed in this
> trace are sound JMM-wise.
>
> Now, let's take the trace A:
>  a) We can not commit read(a, !null) before store(a) is committed,
> because it violates the written values consistency (i.e. V[i]|C[i] =
> V|C[i] in spec).
>  b) We can not commit store(a) before vstore(a.f, 42) is committed,
> because it violates happens-before (i.e. hb[i]|C[i] = hb|C[i] in spec)
> c) We can not commit that vstore(a.f, 42) either because we should
> commit vread(a.f, 0) before, otherwise we violate the written values
> consistency again.
>  d) And in the end, we can only commit vread(a.f, 0) after the read(a,
> !null) is committed, otherwise it violates happens-before again.
>
> Hence, we end up with the causality loop, and this trace is not
> committable. Hence, the result of trace A is not sound JMM-wise.
>
> The only committable trace is trace B, and it yields (r1 = 42).
> Q.E.D.
>
> Thanks,
> -Aleksey.
>
> P.S. Note that if a.f was not volatile, happens-before would not be
> induced, and we will end up with the committable trace yielding (r1=0):
>
>                                          store(a, 42)
>   store(a)
>              read(a, !null)
>                                read(a, 0)
>
> P.P.S. Doug had this to say in the internal review for this post:
>
> "I think the confusion in this particular case might have arisen years
> ago in a discussion about how you don't actually need a full fence after
> each volatile assignment in code-less constructors (because
> their relative orderings cannot matter), except for the last trailing
> one, which can in turn be weakened to a (final-field style) store fence
> to prevent reorderings wrt actions by the caller. Maybe this
> side-condition became forgotten."
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Wed Nov 27 17:51:25 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Nov 2013 08:51:25 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <52964752.3020104@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEKCKCAA.davidcholmes@aapt.net.au>

Doug writes:
> On 11/27/2013 02:06 PM, Vitaly Davidovich wrote:
> > But either
> > way, optimizations to avoid zeroing memory is an implementation
> detail and thus
> > cannot be relied upon from JMM standpoint, which I think you're saying.
> >
> > The other issue is that even if we determine that volatiles
> don't get this
> > treatment, if JVM is already ensuring it, it's not going to be
> practical to
> > change it and risk hard to debug problems creeping in.  May as
> > well update the spec now ...
>
> Right. To summarize:
>
> * Programmers do not expect that even though final fields are specifically
> publication-safe, volatile fields are not always so.

Programmers don't grok "unsafe publication" - full stop! Once it is
explained to them they have no reason to assume v0latile fields are special
because only final fields are called out as such.

> * For various implementation reasons, JVMs arrange that
> volatile fields are publication safe anyway, at least in
> cases we know about.

I think not - that is how this whole debate reignited due to a proposed
change from the PPC64 folk to add the constructor barrier for volatiles (but
that was in a misguided attempt to pass the jcstress tests that assumed the
barrier was required).

> * Actually updating the JMM/JLS to mandate this is not easy
> (no small tweak that I know applies). But now is a good time
> to be considering a full revision for JDK9.

I agree with that :)

David
-----

> * In the mean time, it would make sense to further test
> and validate JVMs as meeting this likely future spec.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Wed Nov 27 20:40:57 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 27 Nov 2013 20:40:57 -0500
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
Message-ID: <CAHjP37FuT2ohbYNCz6Ck3ix5F13X-r2x083Xa6BP84oE9WMM7A@mail.gmail.com>

So instead of timed get() is it possible to use async
continuations/callbacks? If enough time goes by without result being
available, calling thread just cancels and moves on.  Otherwise, in cases
like this, I don't see how it can be solved at the FJ framework level
without ugly workarounds.

Sent from my phone
On Nov 27, 2013 11:17 AM, "Romain Colle" <rco at quartetfs.com> wrote:

> Hi all,
>
> I'd like to second Jonathan's post on this subject.
> The basic idea here is that, as far as I can tell, the get(long timeout,
> TimeUnit unit) method of a ForkJoinTask does not respect the Future
> contract.
>
> Before blocking and waiting for the task to complete, the timed get()
> method tries to either steal and execute the task, or at least help it
> complete.
> If it succeeds doing so, it will for instance execute the task that it is
> waiting for. And in this case, there is no telling how long it will take to
> execute it ...
>
> Although this is great for the overall throughput, a thread calling the
> timed get() method with a timeout of 1 second might end up returning a
> minute later (if the task does take a minute to execute).
> This is what Jonathan reproduced in its unit test (with a parallelism of 1
> in its pool to consistently reproduce this behavior).
>
> This behavior does causes some problems in our code, and to my knowledge
> there are no ways of invoking a reliable timed get() on a ForkJoinPool.
> Any feedback on this topic would be greatly appreciated!
>
> Thanks
> Romain
>
>
> On Thu, Nov 21, 2013 at 8:35 AM, Martin Buchholz <martinrb at google.com>wrote:
>
>> Your message got ignored...
>>
>> You created a pool with parallelism of 1, so it's not surprising that no
>> new thread was created to run action1.  The action of the pool seems
>> reasonable to me - how would you do better?
>>
>> Also, posting your code with a friendlier legal notice might help.
>>
>>
>> On Mon, Nov 18, 2013 at 8:35 AM, Jonathan Soto <jso at quartetfs.com> wrote:
>>
>>> Hello everyone,
>>> It has been a few days I am thinking about this problem. Could you help
>>> me ?
>>>
>>> Suppose that we have two tasks
>>> - task1 waits for 5s
>>> - task2 forks task1 and does a timed get of 2.5 seconds
>>>
>>> The main thread submits task2 to a FJP.
>>>
>>> I observe the following behavior.
>>> - Thread Main   :
>>> FPJ.submit(task2)
>>>
>>> - Thread FJPO-1 : task1.fork
>>> As we are in the FJP it adds task1 into the workQueue of the current
>>> thread
>>> Task1 does not start in another thread
>>>
>>> - Thread FJPO-1 : task1.get(2.5, SECONDS)
>>> Starts by checking if the current thread has some local tasks to be
>>> done. As there is task1 that has been previously stored, we execute it.
>>>
>>> Task1 takes 5 seconds to be executed. The 2.5 seconds timeout is totally
>>> ignored.
>>>
>>> Is that the expected behavior ?
>>>
>>> Thanks
>>>
>>> Jonathan
>>>
>>> P.s in attachment a unit test that prints FAIL when the test fails
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Romain Colle
> R&D Project Manager
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/d2536b72/attachment-0001.html>

From vitalyd at gmail.com  Wed Nov 27 20:41:20 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 27 Nov 2013 20:41:20 -0500
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CANPzfU-6Qvw8FbW5xabmphLE=RbrNK9Hsgct+s=AUjrp9PAKuQ@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
	<CANPzfU-6Qvw8FbW5xabmphLE=RbrNK9Hsgct+s=AUjrp9PAKuQ@mail.gmail.com>
Message-ID: <CAHjP37GX9yROz_JqeESR2pbja70Soct=Md1k2SrL6UOADXNUDA@mail.gmail.com>

What is managed blocking?

Sent from my phone
On Nov 27, 2013 3:16 PM, "?iktor ?lang" <viktor.klang at gmail.com> wrote:

>
>
>
> On Wed, Nov 27, 2013 at 11:01 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 11/27/2013 11:09 AM, Romain Colle wrote:
>>
>>  The basic idea here is that, as far as I can tell, the get(long timeout,
>>> TimeUnit unit) method of a ForkJoinTask does not respect the Future
>>> contract.
>>>
>>>
>> Well, it does meet spec in that it times out no earlier than required.
>>
>>
>>  Before blocking and waiting for the task to complete, the timed get()
>>> method
>>> tries to either steal and execute the task, or at least help it complete.
>>> If it succeeds doing so, it will for instance execute the task that it is
>>> waiting for. And in this case, there is no telling how long it will take
>>> to
>>> execute it ...
>>>
>>
>> Yes. Given the documentation of FJ, people ought to expect this
>> and would claim it to be a bug if it did otherwise. Maybe we
>> should add better method documentation for timed get, and also
>> explain a few ways to get non-participatory timeouts.
>> For example, creating a new thread to perform the invocation
>> and timing out on Thread.join. Suggestions welcome.
>
>
> Have ``get`` use managed blocking? Is that an option?
>
> Cheers,
> ?
>
>
>>
>>
>> -Doug
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Cheers,
> ?
>
> * Viktor Klang*
> *Director of Engineering*
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131127/acc3c285/attachment.html>

From aleksey.shipilev at oracle.com  Thu Nov 28 01:27:32 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 28 Nov 2013 10:27:32 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEKBKCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEKBKCAA.davidcholmes@aapt.net.au>
Message-ID: <5296E254.6020802@oracle.com>

On 11/28/2013 02:46 AM, David Holmes wrote:
>> In order to answer what outcomes are possible we need to dump the
>> usual/partial/misleading "reorderings" and "happens-before" mindset, and
>> get to the ground of spec. That is, we need to construct the possible
>> traces and see if those traces are committable, as per JLS 17.4.8.
> 
> And therein lies your mistake. The "happens-before mindset" as you put it is
> what determines what the legal executions are. It was obvious from
> happens-before considerations that your premise was invalid.

Happens-before alone is not enough to mandate the semantics of Java
Memory Model. Even though it can cover the significant part of the
behaviors, the complete behavior of the model is governed by
well-formedness of executions, as per JLS 17.4.{6-8}. The classic
example (even shown in spec!) when the happens-before consistent program
produces causality violations.

> Q.E.D

So, there was nothing "obvious" unless you have the concrete proof. The
only reasonable way to prove this particular thing *was* to show the
committable traces leading to particular results (like I did), *and* to
find flaws in the reasoning in larger JMM. "La-la-la, happens-before,
la-la-la" is hardly a proof, even though the conclusion is the same in
this particular case. Make no mistake about it.

-Aleksey

From aleksey.shipilev at oracle.com  Thu Nov 28 01:46:34 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 28 Nov 2013 10:46:34 +0400
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEKCKCAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEKCKCAA.davidcholmes@aapt.net.au>
Message-ID: <5296E6CA.3090200@oracle.com>

On 11/28/2013 02:51 AM, David Holmes wrote:
> Doug writes:
>> * Programmers do not expect that even though final fields are specifically
>> publication-safe, volatile fields are not always so.
> 
> Programmers don't grok "unsafe publication" - full stop! Once it is
> explained to them they have no reason to assume v0latile fields are special
> because only final fields are called out as such.

As I said before, the JMM is clear on the "special treatment of final
fields". It is a logical error to add "only" there and deny the
antedecent: "If P, then Q; Not P. Therefore, not Q". If field is final
(P) then there is a publication safety (Q). The field is not final (Not
P). Therefore, the publication safety is not gainable (Not Q).

This mistake will be even more evident if/when the JMM is overhauled to
allow volatiles to have the same formal semantics.

>> * For various implementation reasons, JVMs arrange that
>> volatile fields are publication safe anyway, at least in
>> cases we know about.
> 
> I think not - that is how this whole debate reignited due to a proposed
> change from the PPC64 folk to add the constructor barrier for volatiles (but
> that was in a misguided attempt to pass the jcstress tests that assumed the
> barrier was required).

I am going to re-assert the affected tests, marking these behaviors as
"not explicitly required by spec, but reasonably expected by users"
(there are already some tests that need the same marking, e.g.
ByteBuffer atomicity tests). If the correct implementation may make
users happier by providing stronger guarantees that is actually required
by spec, that is good, and aligned with what Doug wants in "further test
and validate JVMs as meeting this likely future spec."

-Aleksey.

From davidcholmes at aapt.net.au  Thu Nov 28 05:07:39 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Nov 2013 20:07:39 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <5296E254.6020802@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEKHKCAA.davidcholmes@aapt.net.au>

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Aleksey
> Shipilev
> Sent: Thursday, 28 November 2013 4:28 PM
> To: dholmes at ieee.org; concurrency-interest
> Subject: Re: [concurrency-interest] Volatile stores in constructors,
> disallowed to see the default value
>
>
> On 11/28/2013 02:46 AM, David Holmes wrote:
> >> In order to answer what outcomes are possible we need to dump the
> >> usual/partial/misleading "reorderings" and "happens-before"
> mindset, and
> >> get to the ground of spec. That is, we need to construct the possible
> >> traces and see if those traces are committable, as per JLS 17.4.8.
> >
> > And therein lies your mistake. The "happens-before mindset" as
> you put it is
> > what determines what the legal executions are. It was obvious from
> > happens-before considerations that your premise was invalid.
>
> Happens-before alone is not enough to mandate the semantics of Java
> Memory Model. Even though it can cover the significant part of the
> behaviors, the complete behavior of the model is governed by
> well-formedness of executions, as per JLS 17.4.{6-8}. The classic
> example (even shown in spec!) when the happens-before consistent program
> produces causality violations.

All that shows is that the presence of happens-before is not sufficient to
guarantee an allowed ordering. But you were arguing for an enforced ordering
when happens-before was absent. So I would maintain that the absence of
happens-before showed that the given reordering was permissible.

> > Q.E.D
>
> So, there was nothing "obvious" unless you have the concrete proof. The
> only reasonable way to prove this particular thing *was* to show the
> committable traces leading to particular results (like I did), *and* to
> find flaws in the reasoning in larger JMM. "La-la-la, happens-before,
> la-la-la" is hardly a proof, even though the conclusion is the same in
> this particular case. Make no mistake about it.

Well obviously we should just remove happens-before from the spec as it is
obviously completely superfluous to any kind of reasoning about the JMM.

David


> -Aleksey
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From oleksandr.otenko at oracle.com  Thu Nov 28 06:36:25 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 28 Nov 2013 11:36:25 +0000
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <5296E254.6020802@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEKBKCAA.davidcholmes@aapt.net.au>
	<5296E254.6020802@oracle.com>
Message-ID: <52972AB9.7050509@oracle.com>

On 28/11/2013 06:27, Aleksey Shipilev wrote:
> On 11/28/2013 02:46 AM, David Holmes wrote:
>>> In order to answer what outcomes are possible we need to dump the
>>> usual/partial/misleading "reorderings" and "happens-before" mindset, and
>>> get to the ground of spec. That is, we need to construct the possible
>>> traces and see if those traces are committable, as per JLS 17.4.8.
>> And therein lies your mistake. The "happens-before mindset" as you put it is
>> what determines what the legal executions are. It was obvious from
>> happens-before considerations that your premise was invalid.
> Happens-before alone is not enough to mandate the semantics of Java
> Memory Model. Even though it can cover the significant part of the
> behaviors, the complete behavior of the model is governed by
> well-formedness of executions, as per JLS 17.4.{6-8}. The classic
> example (even shown in spec!) when the happens-before consistent program
> produces causality violations.
>
>> Q.E.D
> So, there was nothing "obvious" unless you have the concrete proof. The
> only reasonable way to prove this particular thing *was* to show the
> committable traces leading to particular results (like I did), *and* to

On the contrary, the minimal thing here is not to show how 0 can be 
achieved, but to prove 42 is achievable *always*. If you cannot prove 42 
is achievable always, it means there may be a execution order that 
results in something other than 42.

This requires a proof of "observation of store(a,r2) implies store(r2.f, 
42) is observable". A sufficient proof for this is a store-store 
barrier. If you have other proofs that don't allow reordering of these 
stores (hence make the implication true), you can use those. (eg 
final-field semantics is one alternative proof) In the absence of such 
proof I don't need to show */how/* 0 may be achieved.


Alex

> find flaws in the reasoning in larger JMM. "La-la-la, happens-before,
> la-la-la" is hardly a proof, even though the conclusion is the same in
> this particular case. Make no mistake about it.
>
> -Aleksey
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/eab82dde/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Nov 28 09:59:45 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 28 Nov 2013 14:59:45 +0000
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
Message-ID: <52975A61.3090803@oracle.com>

But what is that contract? "The timeout being observed no later than 
..."? I don't think you can implement that in a non-RT system (hence 
cannot rely on that in a non-RT system).

It does uphold the "non-performance is a failure" contract.

Compare: whereas you can devise a protocol where the server responds 
within N units of time, you will still want a socket timeout - you don't 
want to rely on the callee to always be in a position to enforce the 
condition (halting problem and all that).

Alex

On 27/11/2013 16:09, Romain Colle wrote:
> Hi all,
>
> I'd like to second Jonathan's post on this subject.
> The basic idea here is that, as far as I can tell, the get(long 
> timeout, TimeUnit unit) method of a ForkJoinTask does not respect the 
> Future contract.
>
> Before blocking and waiting for the task to complete, the timed get() 
> method tries to either steal and execute the task, or at least help it 
> complete.
> If it succeeds doing so, it will for instance execute the task that it 
> is waiting for. And in this case, there is no telling how long it will 
> take to execute it ...
>
> Although this is great for the overall throughput, a thread calling 
> the timed get() method with a timeout of 1 second might end up 
> returning a minute later (if the task does take a minute to execute).
> This is what Jonathan reproduced in its unit test (with a parallelism 
> of 1 in its pool to consistently reproduce this behavior).
>
> This behavior does causes some problems in our code, and to my 
> knowledge there are no ways of invoking a reliable timed get() on a 
> ForkJoinPool.
> Any feedback on this topic would be greatly appreciated!
>
> Thanks
> Romain
>
>
> On Thu, Nov 21, 2013 at 8:35 AM, Martin Buchholz <martinrb at google.com 
> <mailto:martinrb at google.com>> wrote:
>
>     Your message got ignored...
>
>     You created a pool with parallelism of 1, so it's not surprising
>     that no new thread was created to run action1.  The action of the
>     pool seems reasonable to me - how would you do better?
>
>     Also, posting your code with a friendlier legal notice might help.
>
>
>     On Mon, Nov 18, 2013 at 8:35 AM, Jonathan Soto <jso at quartetfs.com
>     <mailto:jso at quartetfs.com>> wrote:
>
>         Hello everyone,
>         It has been a few days I am thinking about this problem. Could
>         you help me ?
>
>         Suppose that we have two tasks
>         - task1 waits for 5s
>         - task2 forks task1 and does a timed get of 2.5 seconds
>
>         The main thread submits task2 to a FJP.
>
>         I observe the following behavior.
>         - Thread Main   :
>         FPJ.submit(task2)
>
>         - Thread FJPO-1 : task1.fork
>         As we are in the FJP it adds task1 into the workQueue of the
>         current thread
>         Task1 does not start in another thread
>
>         - Thread FJPO-1 : task1.get(2.5, SECONDS)
>         Starts by checking if the current thread has some local tasks
>         to be done. As there is task1 that has been previously stored,
>         we execute it.
>
>         Task1 takes 5 seconds to be executed. The 2.5 seconds timeout
>         is totally ignored.
>
>         Is that the expected behavior ?
>
>         Thanks
>
>         Jonathan
>
>         P.s in attachment a unit test that prints FAIL when the test fails
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> Romain Colle
> R&D Project Manager
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com <http://www.quartetfs.com/>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/b0c5aad7/attachment.html>

From rco at quartetfs.com  Thu Nov 28 10:24:31 2013
From: rco at quartetfs.com (Romain Colle)
Date: Thu, 28 Nov 2013 16:24:31 +0100
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <52964197.5010801@cs.oswego.edu>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
Message-ID: <CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>

Hi Doug and all,

On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Well, it does meet spec in that it times out no earlier than required.
>

As far as I can tell, the javadoc for get(long, TimeUnit) reads: "Waits if
necessary for at most the given time for the computation to complete, and
then retrieves its result, if available. "
I would therefore expect it to return no later than when the given timeout
has expired.

On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> Yes. Given the documentation of FJ, people ought to expect this
> and would claim it to be a bug if it did otherwise. Maybe we
> should add better method documentation for timed get, and also
> explain a few ways to get non-participatory timeouts.
> For example, creating a new thread to perform the invocation
> and timing out on Thread.join. Suggestions welcome.


Agreed. However, I'd really like the thread management to be handled by the
pool, exactly the way it is done today after trying to help the task in
get():
tryCompensate() / set SIGNAL / wait / incrementActiveCount().
This is in fact doing exactly the managed block that Viktor suggested.

Basically what I need is a version of get(long, TimeUnit) that simply does
not try to help :-)

Thanks!

-- 
Romain Colle
R&D Project Manager
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/7291888d/attachment.html>

From oleksandr.otenko at oracle.com  Thu Nov 28 11:04:27 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 28 Nov 2013 16:04:27 +0000
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
	<CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
Message-ID: <5297698B.8050903@oracle.com>

It doesn't promise how long it takes to retrieve the result, though.

(I agree that the user expectation is that the thread management is 
behaving similarly to the promises of the host system - so behaving like 
CPU is contended when it is not, is not expected by the user)

Alex


On 28/11/2013 15:24, Romain Colle wrote:
> Hi Doug and all,
>
> On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu 
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     Well, it does meet spec in that it times out no earlier than required.
>
>
> As far as I can tell, the javadoc for get(long, TimeUnit) reads: 
> "Waits if necessary for at most the given time for the computationto 
> complete, and then retrieves its result, if available. "
> I would therefore expect it to return no later than when the given 
> timeout has expired.
>
> On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu 
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     Yes. Given the documentation of FJ, people ought to expect this
>     and would claim it to be a bug if it did otherwise. Maybe we
>     should add better method documentation for timed get, and also
>     explain a few ways to get non-participatory timeouts.
>     For example, creating a new thread to perform the invocation
>     and timing out on Thread.join. Suggestions welcome.
>
>
> Agreed. However, I'd really like the thread management to be handled 
> by the pool, exactly the way it is done today after trying to help the 
> task in get():
> tryCompensate() / set SIGNAL / wait / incrementActiveCount().
> This is in fact doing exactly the managed block that Viktor suggested.
>
> Basically what I need is a version of get(long, TimeUnit) that simply 
> does not try to help :-)
>
> Thanks!
>
> -- 
> Romain Colle
> R&D Project Manager
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com <http://www.quartetfs.com/>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/6d2431e1/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu Nov 28 12:38:34 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 28 Nov 2013 11:38:34 -0600
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEKHKCAA.davidcholmes@aapt.net.au>
References: <5296E254.6020802@oracle.com>
	<NFBBKALFDCPFIDBNKAPCCEKHKCAA.davidcholmes@aapt.net.au>
Message-ID: <CACuKZqEZGc8LTqi__Cw1ganXLP+Og2p3wMiJnCFHUuTX+N=Ksg@mail.gmail.com>

On Thu, Nov 28, 2013 at 4:07 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Aleksey
>> Shipilev
>> Sent: Thursday, 28 November 2013 4:28 PM
>> To: dholmes at ieee.org; concurrency-interest
>> Subject: Re: [concurrency-interest] Volatile stores in constructors,
>> disallowed to see the default value
>>
>>
>> On 11/28/2013 02:46 AM, David Holmes wrote:
>> >> In order to answer what outcomes are possible we need to dump the
>> >> usual/partial/misleading "reorderings" and "happens-before"
>> mindset, and
>> >> get to the ground of spec. That is, we need to construct the possible
>> >> traces and see if those traces are committable, as per JLS 17.4.8.
>> >
>> > And therein lies your mistake. The "happens-before mindset" as
>> you put it is
>> > what determines what the legal executions are. It was obvious from
>> > happens-before considerations that your premise was invalid.
>>
>> Happens-before alone is not enough to mandate the semantics of Java
>> Memory Model. Even though it can cover the significant part of the
>> behaviors, the complete behavior of the model is governed by
>> well-formedness of executions, as per JLS 17.4.{6-8}. The classic
>> example (even shown in spec!) when the happens-before consistent program
>> produces causality violations.
>
> All that shows is that the presence of happens-before is not sufficient to
> guarantee an allowed ordering. But you were arguing for an enforced ordering
> when happens-before was absent. So I would maintain that the absence of
> happens-before showed that the given reordering was permissible.

We know that, data race free => sequential consistency. You seem to
imply that the converse theorem is also true (barring some cases). It
sounds quite plausible, but is there a proof?

>
>> > Q.E.D
>>
>> So, there was nothing "obvious" unless you have the concrete proof. The
>> only reasonable way to prove this particular thing *was* to show the
>> committable traces leading to particular results (like I did), *and* to
>> find flaws in the reasoning in larger JMM. "La-la-la, happens-before,
>> la-la-la" is hardly a proof, even though the conclusion is the same in
>> this particular case. Make no mistake about it.
>
> Well obviously we should just remove happens-before from the spec as it is
> obviously completely superfluous to any kind of reasoning about the JMM.
>
> David
>
>
>> -Aleksey
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From tkountis at gmail.com  Thu Nov 28 17:23:45 2013
From: tkountis at gmail.com (Thomas Kountis)
Date: Thu, 28 Nov 2013 22:23:45 +0000
Subject: [concurrency-interest] Single writer multiple readers no barriers
	-- safe ?
Message-ID: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>

Hi all,

This is my first time posting on this list, been follower for quite some
time now and really enjoying all the knowledge sharing :) .

I was looking on optimizing a solution today at work, and I came across the
following.
We have a scenario where we keep a simple cache (HashMap) and this is
accessed by multiple
readers on an application server, millions of times per day and highly
contented. This cache is immutable and only gets updated by a single writer
by replacing the reference that the variable points to every 5 mins. This
is currently done as a volatile field. I was looking for a way to lose
completely the memory barriers and rely on that field being eventually
visible across all other threads (no problem by reading stale data for a
few seconds).

Would that be possible with the current JMM ? I tried to test that scenario
with some code, and it seems to work most of the times, but some threads
read stale data for longer that I would expect (lots of seconds). Is there
any platform dependency on such implementation ? Its going to run on x86
environments. Is there any assumption we can make as of how long that
'eventually' part can be ? (could it be more than 5 mins, when the next
write occurs?). My understanding is that, that write even if re-ordered
will have to happen. I came across an article about using the
AtomicReference doing a lazySet (store-store) for the write, and then the
Unsafe to do a getObject (direct) instead of the default get which is based
on the volatile access. Would that be a better solution?

Any ideas, alternatives?

PS. Sorry for the question-bombing :/

Regards,
Thomas
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/61e9f5c2/attachment.html>

From davidcholmes at aapt.net.au  Thu Nov 28 17:26:45 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 29 Nov 2013 08:26:45 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <CACuKZqEZGc8LTqi__Cw1ganXLP+Og2p3wMiJnCFHUuTX+N=Ksg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEKIKCAA.davidcholmes@aapt.net.au>

Zhong Yu writes:
> On Thu, Nov 28, 2013 at 4:07 AM, David Holmes
> <davidcholmes at aapt.net.au> wrote:
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Aleksey
> >> Shipilev
> >> Sent: Thursday, 28 November 2013 4:28 PM
> >> To: dholmes at ieee.org; concurrency-interest
> >> Subject: Re: [concurrency-interest] Volatile stores in constructors,
> >> disallowed to see the default value
> >>
> >>
> >> On 11/28/2013 02:46 AM, David Holmes wrote:
> >> >> In order to answer what outcomes are possible we need to dump the
> >> >> usual/partial/misleading "reorderings" and "happens-before"
> >> mindset, and
> >> >> get to the ground of spec. That is, we need to construct
> the possible
> >> >> traces and see if those traces are committable, as per JLS 17.4.8.
> >> >
> >> > And therein lies your mistake. The "happens-before mindset" as
> >> you put it is
> >> > what determines what the legal executions are. It was obvious from
> >> > happens-before considerations that your premise was invalid.
> >>
> >> Happens-before alone is not enough to mandate the semantics of Java
> >> Memory Model. Even though it can cover the significant part of the
> >> behaviors, the complete behavior of the model is governed by
> >> well-formedness of executions, as per JLS 17.4.{6-8}. The classic
> >> example (even shown in spec!) when the happens-before
> consistent program
> >> produces causality violations.
> >
> > All that shows is that the presence of happens-before is not
> sufficient to
> > guarantee an allowed ordering. But you were arguing for an
> enforced ordering
> > when happens-before was absent. So I would maintain that the absence of
> > happens-before showed that the given reordering was permissible.
>
> We know that, data race free => sequential consistency. You seem to
> imply that the converse theorem is also true (barring some cases). It
> sounds quite plausible, but is there a proof?

No I wasn't saying anything about DRF or SC. Simply saying that if there is
no happens-before relation that forces A->B, then B->A is quite permissible.

David
-----


> >
> >> > Q.E.D
> >>
> >> So, there was nothing "obvious" unless you have the concrete proof. The
> >> only reasonable way to prove this particular thing *was* to show the
> >> committable traces leading to particular results (like I did), *and* to
> >> find flaws in the reasoning in larger JMM. "La-la-la, happens-before,
> >> la-la-la" is hardly a proof, even though the conclusion is the same in
> >> this particular case. Make no mistake about it.
> >
> > Well obviously we should just remove happens-before from the
> spec as it is
> > obviously completely superfluous to any kind of reasoning about the JMM.
> >
> > David
> >
> >
> >> -Aleksey
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From martinrb at google.com  Thu Nov 28 18:06:39 2013
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 28 Nov 2013 15:06:39 -0800
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
Message-ID: <CA+kOe08kcxHztyytJAW5MaOrtN7wYWzi6tV3shi6kXjuAAHYew@mail.gmail.com>

You have one thread writing to the HashMap and another thread reading it,
so there needs to be some kind of safe publication ("memory barrier" -
which might end up being a no-op in the hardware) in between somewhere.  A
volatile HashMap field sounds good to me.



On Thu, Nov 28, 2013 at 2:23 PM, Thomas Kountis <tkountis at gmail.com> wrote:

> Hi all,
>
> This is my first time posting on this list, been follower for quite some
> time now and really enjoying all the knowledge sharing :) .
>
> I was looking on optimizing a solution today at work, and I came across
> the following.
> We have a scenario where we keep a simple cache (HashMap) and this is
> accessed by multiple
> readers on an application server, millions of times per day and highly
> contented. This cache is immutable and only gets updated by a single writer
> by replacing the reference that the variable points to every 5 mins. This
> is currently done as a volatile field. I was looking for a way to lose
> completely the memory barriers and rely on that field being eventually
> visible across all other threads (no problem by reading stale data for a
> few seconds).
>
> Would that be possible with the current JMM ? I tried to test that
> scenario with some code, and it seems to work most of the times, but some
> threads read stale data for longer that I would expect (lots of seconds).
> Is there any platform dependency on such implementation ? Its going to run
> on x86 environments. Is there any assumption we can make as of how long
> that 'eventually' part can be ? (could it be more than 5 mins, when the
> next write occurs?). My understanding is that, that write even if
> re-ordered will have to happen. I came across an article about using the
> AtomicReference doing a lazySet (store-store) for the write, and then the
> Unsafe to do a getObject (direct) instead of the default get which is based
> on the volatile access. Would that be a better solution?
>
> Any ideas, alternatives?
>
> PS. Sorry for the question-bombing :/
>
> Regards,
> Thomas
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/7ca10a26/attachment.html>

From martinrb at google.com  Thu Nov 28 18:48:44 2013
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 28 Nov 2013 15:48:44 -0800
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <52964752.3020104@cs.oswego.edu>
References: <5295E24C.40608@oracle.com> <5295EFFA.5090701@oracle.com>
	<5295F85A.40500@cs.oswego.edu>
	<CAHjP37HjObC1K3Ef+w9Ore+NXt2dofYurzYpFKLLPgia6EbYwg@mail.gmail.com>
	<52963CEB.5070703@cs.oswego.edu>
	<CAHjP37GLPJs1Ue0R6mVR55bUYJc1GGgz1a0vyiZs7QctAj-GkA@mail.gmail.com>
	<52964752.3020104@cs.oswego.edu>
Message-ID: <CA+kOe09xeaVjUAKXQibZ+QnroY=By40upUCtCzN98qdEcVZ-qg@mail.gmail.com>

On Wed, Nov 27, 2013 at 11:26 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> Right. To summarize:
>
> * Programmers do not expect that even though final fields are specifically
> publication-safe, volatile fields are not always so.
>
>
Many programmers intuitively expect *all* fields assigned in constructors
to be publication-safe....


> * For various implementation reasons, JVMs arrange that
> volatile fields are publication safe anyway, at least in
> cases we know about.
>
>
... and at least on x86 with current JVMs, naive programmers are right -
*all* fields are in practice publication-safe.
... and I believe even some JDK classes depend on this being true ...

Which suggests we could actually expand final field guarantees to all
fields in the spec.  What's the actual expected loss of performance on
current hardware?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/b48b7808/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Nov 28 18:55:04 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 29 Nov 2013 09:55:04 +1000
Subject: [concurrency-interest] Volatile stores in constructors,
	disallowed to see the default value
In-Reply-To: <CA+kOe09xeaVjUAKXQibZ+QnroY=By40upUCtCzN98qdEcVZ-qg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEKJKCAA.davidcholmes@aapt.net.au>

Martin,

Guaranteeing safe-publication has been discussed before - please search here
or on JMM list. Costs vary by platform.

JDK classes do continue to have safe-publication bugs (fixed on just last
week) but this is not because anything is assumed as such, just the regular
ignorance that things can be published unsafely.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Martin
Buchholz
  Sent: Friday, 29 November 2013 9:49 AM
  To: Doug Lea
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] Volatile stores in constructors,
disallowed to see the default value







  On Wed, Nov 27, 2013 at 11:26 AM, Doug Lea <dl at cs.oswego.edu> wrote:

    Right. To summarize:


    * Programmers do not expect that even though final fields are
specifically
    publication-safe, volatile fields are not always so.




  Many programmers intuitively expect *all* fields assigned in constructors
to be publication-safe....

    * For various implementation reasons, JVMs arrange that
    volatile fields are publication safe anyway, at least in
    cases we know about.




  ... and at least on x86 with current JVMs, naive programmers are right -
*all* fields are in practice publication-safe.
  ... and I believe even some JDK classes depend on this being true ...


  Which suggests we could actually expand final field guarantees to all
fields in the spec.  What's the actual expected loss of performance on
current hardware?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/8202b0a2/attachment.html>

From zhong.j.yu at gmail.com  Thu Nov 28 19:43:08 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 28 Nov 2013 18:43:08 -0600
Subject: [concurrency-interest] Volatile stores in constructors,
 disallowed to see the default value
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEKJKCAA.davidcholmes@aapt.net.au>
References: <CA+kOe09xeaVjUAKXQibZ+QnroY=By40upUCtCzN98qdEcVZ-qg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEKJKCAA.davidcholmes@aapt.net.au>
Message-ID: <CACuKZqHJoRWXwDJko8xmu-3m1md9K2Qw4=a0UgoFgbgYCxm81w@mail.gmail.com>

On Thu, Nov 28, 2013 at 5:55 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Martin,
>
> Guaranteeing safe-publication has been discussed before - please search here
> or on JMM list. Costs vary by platform.
>
> JDK classes do continue to have safe-publication bugs (fixed on just last
> week) but this is not because anything is assumed as such, just the regular
> ignorance that things can be published unsafely.

But this part of the language is too counter-intuitive, it leads to
too many bugs; it forbids some seemingly innocent solutions (e.g.
double checked locking). The cost is too high; the benefit is unclear.

>
> David
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Martin
> Buchholz
> Sent: Friday, 29 November 2013 9:49 AM
> To: Doug Lea
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] Volatile stores in constructors,
> disallowed to see the default value
>
>
>
>
> On Wed, Nov 27, 2013 at 11:26 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> Right. To summarize:
>>
>> * Programmers do not expect that even though final fields are specifically
>> publication-safe, volatile fields are not always so.
>>
>
> Many programmers intuitively expect *all* fields assigned in constructors to
> be publication-safe....
>
>>
>> * For various implementation reasons, JVMs arrange that
>> volatile fields are publication safe anyway, at least in
>> cases we know about.
>>
>
> ... and at least on x86 with current JVMs, naive programmers are right -
> *all* fields are in practice publication-safe.
> ... and I believe even some JDK classes depend on this being true ...
>
> Which suggests we could actually expand final field guarantees to all fields
> in the spec.  What's the actual expected loss of performance on current
> hardware?
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From vitalyd at gmail.com  Thu Nov 28 22:21:24 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 28 Nov 2013 22:21:24 -0500
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
Message-ID: <CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>

AtomicReference.lazySet is the way to go here - on x86 this is just normal
mov instruction with compiler barrier only (StoreStore).  If you don't want
overhead of AtomicReference wrapper (doesn't sound like that would be an
issue) you can get same effect with Unsafe.putObjectOrdered.

I wouldn't worry about AtomicReference.get() performance on x86 - this is a
read from memory but if you read frequently, you'll hit L1 cache anyway.

HTH

Sent from my phone
On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:

> Hi all,
>
> This is my first time posting on this list, been follower for quite some
> time now and really enjoying all the knowledge sharing :) .
>
> I was looking on optimizing a solution today at work, and I came across
> the following.
> We have a scenario where we keep a simple cache (HashMap) and this is
> accessed by multiple
> readers on an application server, millions of times per day and highly
> contented. This cache is immutable and only gets updated by a single writer
> by replacing the reference that the variable points to every 5 mins. This
> is currently done as a volatile field. I was looking for a way to lose
> completely the memory barriers and rely on that field being eventually
> visible across all other threads (no problem by reading stale data for a
> few seconds).
>
> Would that be possible with the current JMM ? I tried to test that
> scenario with some code, and it seems to work most of the times, but some
> threads read stale data for longer that I would expect (lots of seconds).
> Is there any platform dependency on such implementation ? Its going to run
> on x86 environments. Is there any assumption we can make as of how long
> that 'eventually' part can be ? (could it be more than 5 mins, when the
> next write occurs?). My understanding is that, that write even if
> re-ordered will have to happen. I came across an article about using the
> AtomicReference doing a lazySet (store-store) for the write, and then the
> Unsafe to do a getObject (direct) instead of the default get which is based
> on the volatile access. Would that be a better solution?
>
> Any ideas, alternatives?
>
> PS. Sorry for the question-bombing :/
>
> Regards,
> Thomas
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/32c84ab2/attachment.html>

From nitsanw at yahoo.com  Fri Nov 29 01:51:41 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 28 Nov 2013 22:51:41 -0800 (PST)
Subject: [concurrency-interest] Single writer multiple readers no
	barriers -- safe ?
In-Reply-To: <CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
Message-ID: <1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>

>From my experience, lazySet is indeed your best choice (but only a valid choice for a single writer). You need a volatile read to match the HB relationship otherwise the compiler is free to optimize the value you read, so someone using your map in a loop may end up stuck if you don't do it.




On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
 
AtomicReference.lazySet is the way to go here - on x86 this is just normal mov instruction with compiler barrier only (StoreStore).? If you don't want overhead of AtomicReference wrapper (doesn't sound like that would be an issue) you can get same effect with Unsafe.putObjectOrdered.
I wouldn't worry about AtomicReference.get() performance on x86 - this is a read from memory but if you read frequently, you'll hit L1 cache anyway.
HTH
Sent from my phone
On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:

Hi all,
>
>
>This is my first time posting on this list, been follower for quite some time now and really enjoying all the knowledge sharing :) .
>
>
>I was looking on optimizing a solution today at work, and I came across the following.
>We have a scenario where we keep a simple cache (HashMap) and this is accessed by multiple
>readers on an application server, millions of times per day and highly contented. This cache is immutable and only gets updated by a single writer by replacing the reference that the variable points to every 5 mins. This is currently done as a volatile field. I was looking for a way to lose completely the memory barriers and rely on that field being eventually visible across all other threads (no problem by reading stale data for a few seconds).
>
>
>Would that be possible with the current JMM ? I tried to test that scenario with some code, and it seems to work most of the times, but some threads read stale data for longer that I would expect (lots of seconds). Is there any platform dependency on such implementation ? Its going to run on x86 environments. Is there any assumption we can make as of how long that 'eventually' part can be ? (could it be more than 5 mins, when the next write occurs?). My understanding is that, that write even if re-ordered will have to happen. I came across an article about using the AtomicReference doing a lazySet (store-store) for the write, and then the Unsafe to do a getObject (direct) instead of the default get which is based on the volatile access. Would that be a better solution?
>
>
>Any ideas, alternatives?
>
>
>PS. Sorry for the question-bombing :/
>
>
>Regards,
>Thomas
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131128/020fac45/attachment-0001.html>

From tkountis at gmail.com  Fri Nov 29 03:01:51 2013
From: tkountis at gmail.com (Thomas Kountis)
Date: Fri, 29 Nov 2013 08:01:51 +0000
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>

Thanks for the responses guys.
I do understand all the above, but what I don't understand is what could go
wrong with the no barrier approach on x86 ? Wouldn't that write eventually
get flushed to main memory, and the other processors must invalidate cache
at some point also ? I know this is a lot of "at some point", therefore, I
guess its very vague to be trusted, but is there anything else apart from
timing that could go wrong and that write not become visible?

t.


On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:

> From my experience, lazySet is indeed your best choice (but only a valid
> choice for a single writer). You need a volatile read to match the HB
> relationship otherwise the compiler is free to optimize the value you read,
> so someone using your map in a loop may end up stuck if you don't do it.
>
>
>
>   On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <
> vitalyd at gmail.com> wrote:
>  AtomicReference.lazySet is the way to go here - on x86 this is just
> normal mov instruction with compiler barrier only (StoreStore).  If you
> don't want overhead of AtomicReference wrapper (doesn't sound like that
> would be an issue) you can get same effect with Unsafe.putObjectOrdered.
> I wouldn't worry about AtomicReference.get() performance on x86 - this is
> a read from memory but if you read frequently, you'll hit L1 cache anyway.
> HTH
> Sent from my phone
> On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:
>
> Hi all,
>
> This is my first time posting on this list, been follower for quite some
> time now and really enjoying all the knowledge sharing :) .
>
> I was looking on optimizing a solution today at work, and I came across
> the following.
> We have a scenario where we keep a simple cache (HashMap) and this is
> accessed by multiple
> readers on an application server, millions of times per day and highly
> contented. This cache is immutable and only gets updated by a single writer
> by replacing the reference that the variable points to every 5 mins. This
> is currently done as a volatile field. I was looking for a way to lose
> completely the memory barriers and rely on that field being eventually
> visible across all other threads (no problem by reading stale data for a
> few seconds).
>
> Would that be possible with the current JMM ? I tried to test that
> scenario with some code, and it seems to work most of the times, but some
> threads read stale data for longer that I would expect (lots of seconds).
> Is there any platform dependency on such implementation ? Its going to run
> on x86 environments. Is there any assumption we can make as of how long
> that 'eventually' part can be ? (could it be more than 5 mins, when the
> next write occurs?). My understanding is that, that write even if
> re-ordered will have to happen. I came across an article about using the
> AtomicReference doing a lazySet (store-store) for the write, and then the
> Unsafe to do a getObject (direct) instead of the default get which is based
> on the volatile access. Would that be a better solution?
>
> Any ideas, alternatives?
>
> PS. Sorry for the question-bombing :/
>
> Regards,
> Thomas
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/897c358a/attachment.html>

From nitsanw at yahoo.com  Fri Nov 29 03:42:15 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Fri, 29 Nov 2013 00:42:15 -0800 (PST)
Subject: [concurrency-interest] Single writer multiple readers no
	barriers -- safe ?
In-Reply-To: <CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
Message-ID: <1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>

What could go wrong is:
? ?while(getMap().get(key)){
? ? ? // wait for key
? ?}
Can in theory (and sometimes in practice) spin forever. The value gets hoisted and never checked again. The JIT assumes there's no way for the read value to change, so won't check again. At that point the 'at some point' becomes 'never'.
Reading through your original post again I would take a step back and evaluate the performance win vs. correctness. Are you 100% sure this is a bottleneck for your program? have you tried and measured using CHM and it makes a measurable difference to use your alternative?
If the answer to the above is "yes we did, and yes it does" then it would be good to see some concrete code to demonstrate how you are using this map.



On Friday, November 29, 2013 10:01 AM, Thomas Kountis <tkountis at gmail.com> wrote:
 
Thanks for the responses guys.
I do understand all the above, but what I don't understand is what could go wrong with the no barrier approach on x86 ? Wouldn't that write eventually get flushed to main memory, and the other processors must invalidate cache at some point also ? I know this is a lot of "at some point", therefore, I guess its very vague to be trusted, but is there anything else apart from timing that could go wrong and that write not become visible?

t.



On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:

From my experience, lazySet is indeed your best choice (but only a valid choice for a single writer). You need a volatile read to match the HB relationship otherwise the compiler is free to optimize the value you read, so someone using your map in a loop may end up stuck if you don't do it.
>
>
>
>
>
>On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
>AtomicReference.lazySet is the way to go here - on x86 this is just normal mov instruction with compiler barrier only (StoreStore).? If you don't want overhead of AtomicReference wrapper (doesn't sound like that would be an issue) you can get same effect with Unsafe.putObjectOrdered.
>I wouldn't worry about AtomicReference.get() performance on x86 - this is a read from memory but if you read frequently, you'll hit L1 cache anyway.
>HTH
>Sent from my phone
>On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:
>
>Hi all,
>>
>>
>>This is my first time posting on this list, been follower for quite some time now and really enjoying all the knowledge sharing :) .
>>
>>
>>I was looking on optimizing a solution today at work, and I came across the following.
>>We have a scenario where we keep a simple cache (HashMap) and this is accessed by multiple
>>readers on an application server, millions of times per day and highly contented. This cache is immutable and only gets updated by a single writer by replacing the reference that the variable points to every 5 mins. This is currently done as a volatile field. I was looking for a way to lose completely the memory barriers and rely on that field being eventually visible across all other threads (no problem by reading stale data for a few seconds).
>>
>>
>>Would that be possible with the current JMM ? I tried to test that scenario with some code, and it seems to work most of the times, but some threads read stale data for longer that I would expect (lots of seconds). Is there any platform dependency on such implementation ? Its going to run on x86 environments. Is there any assumption we can make as of how long that 'eventually' part can be ? (could it be more than 5 mins, when the next write occurs?). My understanding is that, that write even if re-ordered will have to happen. I came across an article about using the AtomicReference doing a lazySet (store-store) for the write, and then the Unsafe to do a getObject (direct) instead of the default get which is based on the volatile access. Would that be a better solution?
>>
>>
>>Any ideas, alternatives?
>>
>>
>>PS. Sorry for the question-bombing :/
>>
>>
>>Regards,
>>Thomas
>>_______________________________________________
>>Concurrency-interest mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/db2e0b32/attachment-0001.html>

From tkountis at gmail.com  Fri Nov 29 04:06:49 2013
From: tkountis at gmail.com (Thomas Kountis)
Date: Fri, 29 Nov 2013 09:06:49 +0000
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
Message-ID: <CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>

Ok that makes sense.

I measured the performance and no this is not close to being a bottleneck
on the current setup, since there are other places which are much slower.
So the initial question was more into the "lets experiment" side of things
and learn. My thinking was that since we only have one writer and thats
only on a specific interval (5 mins) there would be a waste of using the
CHM to host the data. Taking a snapshot of the current state during a
write, replace of things and reassign the map variable to the snapshot
seemed more interesting.

Thanks again :)



On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:

> What could go wrong is:
>    while(getMap().get(key)){
>       // wait for key
>    }
> Can in theory (and sometimes in practice) spin forever. The value gets
> hoisted and never checked again. The JIT assumes there's no way for the
> read value to change, so won't check again. At that point the 'at some
> point' becomes 'never'.
> Reading through your original post again I would take a step back and
> evaluate the performance win vs. correctness. Are you 100% sure this is a
> bottleneck for your program? have you tried and measured using CHM and it
> makes a measurable difference to use your alternative?
> If the answer to the above is "yes we did, and yes it does" then it would
> be good to see some concrete code to demonstrate how you are using this map.
>
>
>   On Friday, November 29, 2013 10:01 AM, Thomas Kountis <
> tkountis at gmail.com> wrote:
>  Thanks for the responses guys.
> I do understand all the above, but what I don't understand is what could
> go wrong with the no barrier approach on x86 ? Wouldn't that write
> eventually get flushed to main memory, and the other processors must
> invalidate cache at some point also ? I know this is a lot of "at some
> point", therefore, I guess its very vague to be trusted, but is there
> anything else apart from timing that could go wrong and that write not
> become visible?
>
> t.
>
>
> On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>
> From my experience, lazySet is indeed your best choice (but only a valid
> choice for a single writer). You need a volatile read to match the HB
> relationship otherwise the compiler is free to optimize the value you read,
> so someone using your map in a loop may end up stuck if you don't do it.
>
>
>
>   On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <
> vitalyd at gmail.com> wrote:
>  AtomicReference.lazySet is the way to go here - on x86 this is just
> normal mov instruction with compiler barrier only (StoreStore).  If you
> don't want overhead of AtomicReference wrapper (doesn't sound like that
> would be an issue) you can get same effect with Unsafe.putObjectOrdered.
> I wouldn't worry about AtomicReference.get() performance on x86 - this is
> a read from memory but if you read frequently, you'll hit L1 cache anyway.
> HTH
> Sent from my phone
> On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:
>
> Hi all,
>
> This is my first time posting on this list, been follower for quite some
> time now and really enjoying all the knowledge sharing :) .
>
> I was looking on optimizing a solution today at work, and I came across
> the following.
> We have a scenario where we keep a simple cache (HashMap) and this is
> accessed by multiple
> readers on an application server, millions of times per day and highly
> contented. This cache is immutable and only gets updated by a single writer
> by replacing the reference that the variable points to every 5 mins. This
> is currently done as a volatile field. I was looking for a way to lose
> completely the memory barriers and rely on that field being eventually
> visible across all other threads (no problem by reading stale data for a
> few seconds).
>
> Would that be possible with the current JMM ? I tried to test that
> scenario with some code, and it seems to work most of the times, but some
> threads read stale data for longer that I would expect (lots of seconds).
> Is there any platform dependency on such implementation ? Its going to run
> on x86 environments. Is there any assumption we can make as of how long
> that 'eventually' part can be ? (could it be more than 5 mins, when the
> next write occurs?). My understanding is that, that write even if
> re-ordered will have to happen. I came across an article about using the
> AtomicReference doing a lazySet (store-store) for the write, and then the
> Unsafe to do a getObject (direct) instead of the default get which is based
> on the volatile access. Would that be a better solution?
>
> Any ideas, alternatives?
>
> PS. Sorry for the question-bombing :/
>
> Regards,
> Thomas
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>


-- 
Thomas Kountis
PGP: 0x069D29A3

Q: "Whats the object-oriented way to become wealthy?"
A:  Inheritance
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/6807b4d6/attachment.html>

From viktor.klang at gmail.com  Fri Nov 29 08:05:03 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 29 Nov 2013 05:05:03 -0800
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
	<CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
Message-ID: <CANPzfU8B+Sck_dNOTWrguH1kPLGimd--hmXRofcNb5uCRGFpaw@mail.gmail.com>

On Thu, Nov 28, 2013 at 7:24 AM, Romain Colle <rco at quartetfs.com> wrote:

> Hi Doug and all,
>
> On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> Well, it does meet spec in that it times out no earlier than required.
>>
>
> As far as I can tell, the javadoc for get(long, TimeUnit) reads: "Waits
> if necessary for at most the given time for the computation to complete,
> and then retrieves its result, if available. "
> I would therefore expect it to return no later than when the given timeout
> has expired.
>
>  On Wed, Nov 27, 2013 at 8:01 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> Yes. Given the documentation of FJ, people ought to expect this
>> and would claim it to be a bug if it did otherwise. Maybe we
>> should add better method documentation for timed get, and also
>> explain a few ways to get non-participatory timeouts.
>> For example, creating a new thread to perform the invocation
>> and timing out on Thread.join. Suggestions welcome.
>
>
> Agreed. However, I'd really like the thread management to be handled by
> the pool, exactly the way it is done today after trying to help the task in
> get():
> tryCompensate() / set SIGNAL / wait / incrementActiveCount().
> This is in fact doing exactly the managed block that Viktor suggested.
>

Isn't there a difference in that it doesn't spawn an additional thread?

Cheers,
?


>
> Basically what I need is a version of get(long, TimeUnit) that simply does
> not try to help :-)
>
> Thanks!
>
> --
> Romain Colle
> R&D Project Manager
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?

*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/428de65e/attachment-0001.html>

From rco at quartetfs.com  Fri Nov 29 08:33:51 2013
From: rco at quartetfs.com (Romain Colle)
Date: Fri, 29 Nov 2013 14:33:51 +0100
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CANPzfU8B+Sck_dNOTWrguH1kPLGimd--hmXRofcNb5uCRGFpaw@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
	<CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
	<CANPzfU8B+Sck_dNOTWrguH1kPLGimd--hmXRofcNb5uCRGFpaw@mail.gmail.com>
Message-ID: <CAJp3eRBH_dOGZiz==8+9+7S9Yr4Cz9b8Buw558dymzvtJZ6j-Q@mail.gmail.com>

On Fri, Nov 29, 2013 at 2:05 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

>  Agreed. However, I'd really like the thread management to be handled by
>> the pool, exactly the way it is done today after trying to help the task in
>> get():
>> tryCompensate() / set SIGNAL / wait / incrementActiveCount().
>> This is in fact doing exactly the managed block that Viktor suggested.
>>
>
> Isn't there a difference in that it doesn't spawn an additional thread?
>

I would say it's exactly the same thing since managedBlock does:
tryCompensate()
/ block / incrementActiveCount()
An extra thread should be created (if possible) to compensate for the
thread doing the blocking get() through the tryCompensate().

Cheers,

-- 
Romain Colle
R&D Project Manager
QuartetFS
2 rue Jean Lantier, 75001 Paris, France
http://www.quartetfs.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/aadb1f7b/attachment.html>

From viktor.klang at gmail.com  Fri Nov 29 08:37:12 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 29 Nov 2013 05:37:12 -0800
Subject: [concurrency-interest] Fork and and timed get
In-Reply-To: <CAJp3eRBH_dOGZiz==8+9+7S9Yr4Cz9b8Buw558dymzvtJZ6j-Q@mail.gmail.com>
References: <CAHUekz1GrJTcWrBNPtYabZhYroKiOgJAE6drYTPycpUJAEaLJw@mail.gmail.com>
	<CA+kOe0-H50YwmT-JPQe97NAxLVmeL21TFcmxB8HeT16LN4ZnyQ@mail.gmail.com>
	<CAJp3eRBJz3Tx=C3O0cne2kE2LZvLgM8iMaf=dERpVthLj2yrZw@mail.gmail.com>
	<52964197.5010801@cs.oswego.edu>
	<CAJp3eRDk-DxT5Q347inDMDdQ6WLZ9aaBVWuaM5b5ARskx=aKSQ@mail.gmail.com>
	<CANPzfU8B+Sck_dNOTWrguH1kPLGimd--hmXRofcNb5uCRGFpaw@mail.gmail.com>
	<CAJp3eRBH_dOGZiz==8+9+7S9Yr4Cz9b8Buw558dymzvtJZ6j-Q@mail.gmail.com>
Message-ID: <CANPzfU_qiOH2X-Bx4RoFtH1ssJVVePF4haeLNuH76Lrq4B3Lhg@mail.gmail.com>

On Fri, Nov 29, 2013 at 5:33 AM, Romain Colle <rco at quartetfs.com> wrote:

> On Fri, Nov 29, 2013 at 2:05 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:
>
>>  Agreed. However, I'd really like the thread management to be handled by
>>> the pool, exactly the way it is done today after trying to help the task in
>>> get():
>>> tryCompensate() / set SIGNAL / wait / incrementActiveCount().
>>> This is in fact doing exactly the managed block that Viktor suggested.
>>>
>>
>> Isn't there a difference in that it doesn't spawn an additional thread?
>>
>
> I would say it's exactly the same thing since managedBlock does: tryCompensate()
> / block / incrementActiveCount()
> An extra thread should be created (if possible) to compensate for the
> thread doing the blocking get() through the tryCompensate().
>

Great, thanks for the clarification!

Cheers,
?


>
> Cheers,
>
> --
> Romain Colle
> R&D Project Manager
> QuartetFS
> 2 rue Jean Lantier, 75001 Paris, France
> http://www.quartetfs.com
>



-- 
Cheers,
?

*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/65ec0fc4/attachment.html>

From oleksandr.otenko at oracle.com  Fri Nov 29 09:23:15 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 29 Nov 2013 14:23:15 +0000
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
Message-ID: <5298A353.90207@oracle.com>

How do you distinguish stale data from new data?

If, for example, you mean a new key should appear in HashMap (otherwise 
the new HashMap is a copy of the previous), then not using the volatile 
field can result in wrong order of the key becoming visible - the reader 
threads might already see the new reference, but not the new keys yet. 
The timing of the new keys appearing depends on how soon the writer gets 
to flush the write buffers. This may appear as "reading stale data".

Alex

On 28/11/2013 22:23, Thomas Kountis wrote:
> Hi all,
>
> This is my first time posting on this list, been follower for quite 
> some time now and really enjoying all the knowledge sharing :) .
>
> I was looking on optimizing a solution today at work, and I came 
> across the following.
> We have a scenario where we keep a simple cache (HashMap) and this is 
> accessed by multiple
> readers on an application server, millions of times per day and highly 
> contented. This cache is immutable and only gets updated by a single 
> writer by replacing the reference that the variable points to every 5 
> mins. This is currently done as a volatile field. I was looking for a 
> way to lose completely the memory barriers and rely on that field 
> being eventually visible across all other threads (no problem by 
> reading stale data for a few seconds).
>
> Would that be possible with the current JMM ? I tried to test that 
> scenario with some code, and it seems to work most of the times, but 
> some threads read stale data for longer that I would expect (lots of 
> seconds). Is there any platform dependency on such implementation ? 
> Its going to run on x86 environments. Is there any assumption we can 
> make as of how long that 'eventually' part can be ? (could it be more 
> than 5 mins, when the next write occurs?). My understanding is that, 
> that write even if re-ordered will have to happen. I came across an 
> article about using the AtomicReference doing a lazySet (store-store) 
> for the write, and then the Unsafe to do a getObject (direct) instead 
> of the default get which is based on the volatile access. Would that 
> be a better solution?
>
> Any ideas, alternatives?
>
> PS. Sorry for the question-bombing :/
>
> Regards,
> Thomas
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/0a7bd490/attachment.html>

From mr.chrisvest at gmail.com  Fri Nov 29 14:30:44 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Fri, 29 Nov 2013 20:30:44 +0100
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
Message-ID: <CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>

Note that AtomicReferenceFieldUpdater also has lazySet, so you don't need
to go to the Unsafe if you want to avoid the indirection of AtomicReference.

--
Chris


On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com> wrote:

> Ok that makes sense.
>
> I measured the performance and no this is not close to being a bottleneck
> on the current setup, since there are other places which are much slower.
> So the initial question was more into the "lets experiment" side of things
> and learn. My thinking was that since we only have one writer and thats
> only on a specific interval (5 mins) there would be a waste of using the
> CHM to host the data. Taking a snapshot of the current state during a
> write, replace of things and reassign the map variable to the snapshot
> seemed more interesting.
>
> Thanks again :)
>
>
>
> On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>
>> What could go wrong is:
>>    while(getMap().get(key)){
>>       // wait for key
>>    }
>> Can in theory (and sometimes in practice) spin forever. The value gets
>> hoisted and never checked again. The JIT assumes there's no way for the
>> read value to change, so won't check again. At that point the 'at some
>> point' becomes 'never'.
>> Reading through your original post again I would take a step back and
>> evaluate the performance win vs. correctness. Are you 100% sure this is a
>> bottleneck for your program? have you tried and measured using CHM and it
>> makes a measurable difference to use your alternative?
>> If the answer to the above is "yes we did, and yes it does" then it would
>> be good to see some concrete code to demonstrate how you are using this map.
>>
>>
>>   On Friday, November 29, 2013 10:01 AM, Thomas Kountis <
>> tkountis at gmail.com> wrote:
>>   Thanks for the responses guys.
>> I do understand all the above, but what I don't understand is what could
>> go wrong with the no barrier approach on x86 ? Wouldn't that write
>> eventually get flushed to main memory, and the other processors must
>> invalidate cache at some point also ? I know this is a lot of "at some
>> point", therefore, I guess its very vague to be trusted, but is there
>> anything else apart from timing that could go wrong and that write not
>> become visible?
>>
>> t.
>>
>>
>> On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>>
>> From my experience, lazySet is indeed your best choice (but only a valid
>> choice for a single writer). You need a volatile read to match the HB
>> relationship otherwise the compiler is free to optimize the value you read,
>> so someone using your map in a loop may end up stuck if you don't do it.
>>
>>
>>
>>   On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <
>> vitalyd at gmail.com> wrote:
>>  AtomicReference.lazySet is the way to go here - on x86 this is just
>> normal mov instruction with compiler barrier only (StoreStore).  If you
>> don't want overhead of AtomicReference wrapper (doesn't sound like that
>> would be an issue) you can get same effect with Unsafe.putObjectOrdered.
>> I wouldn't worry about AtomicReference.get() performance on x86 - this is
>> a read from memory but if you read frequently, you'll hit L1 cache anyway.
>> HTH
>> Sent from my phone
>> On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:
>>
>> Hi all,
>>
>> This is my first time posting on this list, been follower for quite some
>> time now and really enjoying all the knowledge sharing :) .
>>
>> I was looking on optimizing a solution today at work, and I came across
>> the following.
>> We have a scenario where we keep a simple cache (HashMap) and this is
>> accessed by multiple
>> readers on an application server, millions of times per day and highly
>> contented. This cache is immutable and only gets updated by a single writer
>> by replacing the reference that the variable points to every 5 mins. This
>> is currently done as a volatile field. I was looking for a way to lose
>> completely the memory barriers and rely on that field being eventually
>> visible across all other threads (no problem by reading stale data for a
>> few seconds).
>>
>> Would that be possible with the current JMM ? I tried to test that
>> scenario with some code, and it seems to work most of the times, but some
>> threads read stale data for longer that I would expect (lots of seconds).
>> Is there any platform dependency on such implementation ? Its going to run
>> on x86 environments. Is there any assumption we can make as of how long
>> that 'eventually' part can be ? (could it be more than 5 mins, when the
>> next write occurs?). My understanding is that, that write even if
>> re-ordered will have to happen. I came across an article about using the
>> AtomicReference doing a lazySet (store-store) for the write, and then the
>> Unsafe to do a getObject (direct) instead of the default get which is based
>> on the volatile access. Would that be a better solution?
>>
>> Any ideas, alternatives?
>>
>> PS. Sorry for the question-bombing :/
>>
>> Regards,
>> Thomas
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>
>
>
> --
> Thomas Kountis
> PGP: 0x069D29A3
>
> Q: "Whats the object-oriented way to become wealthy?"
> A:  Inheritance
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/400ee370/attachment.html>

From vitalyd at gmail.com  Fri Nov 29 16:04:17 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 29 Nov 2013 16:04:17 -0500
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
Message-ID: <CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>

AtomicReferenceFieldUpdater has some safety/defensive checking overhead,
IIRC.  Although with writes every 5 mins only I don't think the write side
perf matters all that much.

Sent from my phone
On Nov 29, 2013 2:37 PM, "Chris Vest" <mr.chrisvest at gmail.com> wrote:

> Note that AtomicReferenceFieldUpdater also has lazySet, so you don't need
> to go to the Unsafe if you want to avoid the indirection of AtomicReference.
>
> --
> Chris
>
>
> On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com> wrote:
>
>> Ok that makes sense.
>>
>> I measured the performance and no this is not close to being a bottleneck
>> on the current setup, since there are other places which are much slower.
>> So the initial question was more into the "lets experiment" side of things
>> and learn. My thinking was that since we only have one writer and thats
>> only on a specific interval (5 mins) there would be a waste of using the
>> CHM to host the data. Taking a snapshot of the current state during a
>> write, replace of things and reassign the map variable to the snapshot
>> seemed more interesting.
>>
>> Thanks again :)
>>
>>
>>
>> On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>>
>>> What could go wrong is:
>>>    while(getMap().get(key)){
>>>       // wait for key
>>>    }
>>> Can in theory (and sometimes in practice) spin forever. The value gets
>>> hoisted and never checked again. The JIT assumes there's no way for the
>>> read value to change, so won't check again. At that point the 'at some
>>> point' becomes 'never'.
>>> Reading through your original post again I would take a step back and
>>> evaluate the performance win vs. correctness. Are you 100% sure this is a
>>> bottleneck for your program? have you tried and measured using CHM and it
>>> makes a measurable difference to use your alternative?
>>> If the answer to the above is "yes we did, and yes it does" then it
>>> would be good to see some concrete code to demonstrate how you are using
>>> this map.
>>>
>>>
>>>   On Friday, November 29, 2013 10:01 AM, Thomas Kountis <
>>> tkountis at gmail.com> wrote:
>>>   Thanks for the responses guys.
>>> I do understand all the above, but what I don't understand is what could
>>> go wrong with the no barrier approach on x86 ? Wouldn't that write
>>> eventually get flushed to main memory, and the other processors must
>>> invalidate cache at some point also ? I know this is a lot of "at some
>>> point", therefore, I guess its very vague to be trusted, but is there
>>> anything else apart from timing that could go wrong and that write not
>>> become visible?
>>>
>>> t.
>>>
>>>
>>> On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart <nitsanw at yahoo.com>wrote:
>>>
>>> From my experience, lazySet is indeed your best choice (but only a valid
>>> choice for a single writer). You need a volatile read to match the HB
>>> relationship otherwise the compiler is free to optimize the value you read,
>>> so someone using your map in a loop may end up stuck if you don't do it.
>>>
>>>
>>>
>>>   On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich <
>>> vitalyd at gmail.com> wrote:
>>>  AtomicReference.lazySet is the way to go here - on x86 this is just
>>> normal mov instruction with compiler barrier only (StoreStore).  If you
>>> don't want overhead of AtomicReference wrapper (doesn't sound like that
>>> would be an issue) you can get same effect with Unsafe.putObjectOrdered.
>>> I wouldn't worry about AtomicReference.get() performance on x86 - this
>>> is a read from memory but if you read frequently, you'll hit L1 cache
>>> anyway.
>>> HTH
>>> Sent from my phone
>>> On Nov 28, 2013 5:34 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:
>>>
>>> Hi all,
>>>
>>> This is my first time posting on this list, been follower for quite some
>>> time now and really enjoying all the knowledge sharing :) .
>>>
>>> I was looking on optimizing a solution today at work, and I came across
>>> the following.
>>> We have a scenario where we keep a simple cache (HashMap) and this is
>>> accessed by multiple
>>> readers on an application server, millions of times per day and highly
>>> contented. This cache is immutable and only gets updated by a single writer
>>> by replacing the reference that the variable points to every 5 mins. This
>>> is currently done as a volatile field. I was looking for a way to lose
>>> completely the memory barriers and rely on that field being eventually
>>> visible across all other threads (no problem by reading stale data for a
>>> few seconds).
>>>
>>> Would that be possible with the current JMM ? I tried to test that
>>> scenario with some code, and it seems to work most of the times, but some
>>> threads read stale data for longer that I would expect (lots of seconds).
>>> Is there any platform dependency on such implementation ? Its going to run
>>> on x86 environments. Is there any assumption we can make as of how long
>>> that 'eventually' part can be ? (could it be more than 5 mins, when the
>>> next write occurs?). My understanding is that, that write even if
>>> re-ordered will have to happen. I came across an article about using the
>>> AtomicReference doing a lazySet (store-store) for the write, and then the
>>> Unsafe to do a getObject (direct) instead of the default get which is based
>>> on the volatile access. Would that be a better solution?
>>>
>>> Any ideas, alternatives?
>>>
>>> PS. Sorry for the question-bombing :/
>>>
>>> Regards,
>>> Thomas
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>
>>
>> --
>> Thomas Kountis
>> PGP: 0x069D29A3
>>
>> Q: "Whats the object-oriented way to become wealthy?"
>> A:  Inheritance
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/121f06b9/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Fri Nov 29 17:00:02 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Fri, 29 Nov 2013 23:00:02 +0100
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
Message-ID: <52990E62.9080209@infinite-source.de>

Isn't the problem with both AtomicReferenceFieldUpdater and 
AtomicReference that they don't have a lazyGet? The get is equivalent to 
a volatile read. Only Unsafe can give you normal get and ordered put, 
i.e. safe single-writer publishing.


On 29.11.2013 22:04, Vitaly Davidovich wrote:
> AtomicReferenceFieldUpdater has some safety/defensive checking overhead,
> IIRC.  Although with writes every 5 mins only I don't think the write
> side perf matters all that much.
>
> Sent from my phone
>
> On Nov 29, 2013 2:37 PM, "Chris Vest" <mr.chrisvest at gmail.com
> <mailto:mr.chrisvest at gmail.com>> wrote:
>
>     Note that AtomicReferenceFieldUpdater also has lazySet, so you don't
>     need to go to the Unsafe if you want to avoid the indirection of
>     AtomicReference.
>
>     --
>     Chris
>
>
>     On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com
>     <mailto:tkountis at gmail.com>> wrote:
>
>         Ok that makes sense.
>
>         I measured the performance and no this is not close to being a
>         bottleneck on the current setup, since there are other places
>         which are much slower. So the initial question was more into the
>         "lets experiment" side of things and learn. My thinking was that
>         since we only have one writer and thats only on a specific
>         interval (5 mins) there would be a waste of using the CHM to
>         host the data. Taking a snapshot of the current state during a
>         write, replace of things and reassign the map variable to the
>         snapshot seemed more interesting.
>
>         Thanks again :)
>
>
>
>         On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart
>         <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>
>             What could go wrong is:
>                 while(getMap().get(key)){
>                    // wait for key
>               }
>             Can in theory (and sometimes in practice) spin forever. The
>             value gets hoisted and never checked again. The JIT assumes
>             there's no way for the read value to change, so won't check
>             again. At that point the 'at some point' becomes 'never'.
>             Reading through your original post again I would take a step
>             back and evaluate the performance win vs. correctness. Are
>             you 100% sure this is a bottleneck for your program? have
>             you tried and measured using CHM and it makes a measurable
>             difference to use your alternative?
>             If the answer to the above is "yes we did, and yes it does"
>             then it would be good to see some concrete code to
>             demonstrate how you are using this map.
>
>
>             On Friday, November 29, 2013 10:01 AM, Thomas Kountis
>             <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>             Thanks for the responses guys.
>             I do understand all the above, but what I don't understand
>             is what could go wrong with the no barrier approach on x86 ?
>             Wouldn't that write eventually get flushed to main memory,
>             and the other processors must invalidate cache at some point
>             also ? I know this is a lot of "at some point", therefore, I
>             guess its very vague to be trusted, but is there anything
>             else apart from timing that could go wrong and that write
>             not become visible?
>
>             t.
>
>
>             On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart
>             <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>
>                  From my experience, lazySet is indeed your best choice
>                 (but only a valid choice for a single writer). You need
>                 a volatile read to match the HB relationship otherwise
>                 the compiler is free to optimize the value you read, so
>                 someone using your map in a loop may end up stuck if you
>                 don't do it.
>
>
>
>                 On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich
>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>                 AtomicReference.lazySet is the way to go here - on x86
>                 this is just normal mov instruction with compiler
>                 barrier only (StoreStore).  If you don't want overhead
>                 of AtomicReference wrapper (doesn't sound like that
>                 would be an issue) you can get same effect with
>                 Unsafe.putObjectOrdered.
>                 I wouldn't worry about AtomicReference.get() performance
>                 on x86 - this is a read from memory but if you read
>                 frequently, you'll hit L1 cache anyway.
>                 HTH
>                 Sent from my phone
>                 On Nov 28, 2013 5:34 PM, "Thomas Kountis"
>                 <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>
>                     Hi all,
>
>                     This is my first time posting on this list, been
>                     follower for quite some time now and really enjoying
>                     all the knowledge sharing :) .
>
>                     I was looking on optimizing a solution today at
>                     work, and I came across the following.
>                     We have a scenario where we keep a simple cache
>                     (HashMap) and this is accessed by multiple
>                     readers on an application server, millions of times
>                     per day and highly contented. This cache is
>                     immutable and only gets updated by a single writer
>                     by replacing the reference that the variable points
>                     to every 5 mins. This is currently done as a
>                     volatile field. I was looking for a way to lose
>                     completely the memory barriers and rely on that
>                     field being eventually visible across all other
>                     threads (no problem by reading stale data for a few
>                     seconds).
>
>                     Would that be possible with the current JMM ? I
>                     tried to test that scenario with some code, and it
>                     seems to work most of the times, but some threads
>                     read stale data for longer that I would expect (lots
>                     of seconds). Is there any platform dependency on
>                     such implementation ? Its going to run on x86
>                     environments. Is there any assumption we can make as
>                     of how long that 'eventually' part can be ? (could
>                     it be more than 5 mins, when the next write
>                     occurs?). My understanding is that, that write even
>                     if re-ordered will have to happen. I came across an
>                     article about using the AtomicReference doing a
>                     lazySet (store-store) for the write, and then the
>                     Unsafe to do a getObject (direct) instead of the
>                     default get which is based on the volatile access.
>                     Would that be a better solution?
>
>                     Any ideas, alternatives?
>
>                     PS. Sorry for the question-bombing :/
>
>                     Regards,
>                     Thomas
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>
>
>
>         --
>         Thomas Kountis
>         PGP: 0x069D29A3
>
>         Q: "Whats the object-oriented way to become wealthy?"
>         A:  Inheritance
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Fri Nov 29 17:44:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 29 Nov 2013 17:44:13 -0500
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <52990E62.9080209@infinite-source.de>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
	<52990E62.9080209@infinite-source.de>
Message-ID: <CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>

On x86 the volatile read isn't going to cost much of anything over a normal
one - it's just a compiler barrier.  If it's read often it'll be in L1
cache and be just a few clocks.

Using a naked read leaves you open to the scenario Nitsan mentioned earlier
in this thread.

Sent from my phone
On Nov 29, 2013 5:02 PM, "Aaron Grunthal" <aaron.grunthal at infinite-source.de>
wrote:

> Isn't the problem with both AtomicReferenceFieldUpdater and
> AtomicReference that they don't have a lazyGet? The get is equivalent to a
> volatile read. Only Unsafe can give you normal get and ordered put, i.e.
> safe single-writer publishing.
>
>
> On 29.11.2013 22:04, Vitaly Davidovich wrote:
>
>> AtomicReferenceFieldUpdater has some safety/defensive checking overhead,
>> IIRC.  Although with writes every 5 mins only I don't think the write
>> side perf matters all that much.
>>
>> Sent from my phone
>>
>> On Nov 29, 2013 2:37 PM, "Chris Vest" <mr.chrisvest at gmail.com
>> <mailto:mr.chrisvest at gmail.com>> wrote:
>>
>>     Note that AtomicReferenceFieldUpdater also has lazySet, so you don't
>>     need to go to the Unsafe if you want to avoid the indirection of
>>     AtomicReference.
>>
>>     --
>>     Chris
>>
>>
>>     On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com
>>     <mailto:tkountis at gmail.com>> wrote:
>>
>>         Ok that makes sense.
>>
>>         I measured the performance and no this is not close to being a
>>         bottleneck on the current setup, since there are other places
>>         which are much slower. So the initial question was more into the
>>         "lets experiment" side of things and learn. My thinking was that
>>         since we only have one writer and thats only on a specific
>>         interval (5 mins) there would be a waste of using the CHM to
>>         host the data. Taking a snapshot of the current state during a
>>         write, replace of things and reassign the map variable to the
>>         snapshot seemed more interesting.
>>
>>         Thanks again :)
>>
>>
>>
>>         On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart
>>         <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>
>>             What could go wrong is:
>>                 while(getMap().get(key)){
>>                    // wait for key
>>               }
>>             Can in theory (and sometimes in practice) spin forever. The
>>             value gets hoisted and never checked again. The JIT assumes
>>             there's no way for the read value to change, so won't check
>>             again. At that point the 'at some point' becomes 'never'.
>>             Reading through your original post again I would take a step
>>             back and evaluate the performance win vs. correctness. Are
>>             you 100% sure this is a bottleneck for your program? have
>>             you tried and measured using CHM and it makes a measurable
>>             difference to use your alternative?
>>             If the answer to the above is "yes we did, and yes it does"
>>             then it would be good to see some concrete code to
>>             demonstrate how you are using this map.
>>
>>
>>             On Friday, November 29, 2013 10:01 AM, Thomas Kountis
>>             <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>             Thanks for the responses guys.
>>             I do understand all the above, but what I don't understand
>>             is what could go wrong with the no barrier approach on x86 ?
>>             Wouldn't that write eventually get flushed to main memory,
>>             and the other processors must invalidate cache at some point
>>             also ? I know this is a lot of "at some point", therefore, I
>>             guess its very vague to be trusted, but is there anything
>>             else apart from timing that could go wrong and that write
>>             not become visible?
>>
>>             t.
>>
>>
>>             On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart
>>             <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>
>>                  From my experience, lazySet is indeed your best choice
>>                 (but only a valid choice for a single writer). You need
>>                 a volatile read to match the HB relationship otherwise
>>                 the compiler is free to optimize the value you read, so
>>                 someone using your map in a loop may end up stuck if you
>>                 don't do it.
>>
>>
>>
>>                 On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich
>>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>                 AtomicReference.lazySet is the way to go here - on x86
>>                 this is just normal mov instruction with compiler
>>                 barrier only (StoreStore).  If you don't want overhead
>>                 of AtomicReference wrapper (doesn't sound like that
>>                 would be an issue) you can get same effect with
>>                 Unsafe.putObjectOrdered.
>>                 I wouldn't worry about AtomicReference.get() performance
>>                 on x86 - this is a read from memory but if you read
>>                 frequently, you'll hit L1 cache anyway.
>>                 HTH
>>                 Sent from my phone
>>                 On Nov 28, 2013 5:34 PM, "Thomas Kountis"
>>                 <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>
>>                     Hi all,
>>
>>                     This is my first time posting on this list, been
>>                     follower for quite some time now and really enjoying
>>                     all the knowledge sharing :) .
>>
>>                     I was looking on optimizing a solution today at
>>                     work, and I came across the following.
>>                     We have a scenario where we keep a simple cache
>>                     (HashMap) and this is accessed by multiple
>>                     readers on an application server, millions of times
>>                     per day and highly contented. This cache is
>>                     immutable and only gets updated by a single writer
>>                     by replacing the reference that the variable points
>>                     to every 5 mins. This is currently done as a
>>                     volatile field. I was looking for a way to lose
>>                     completely the memory barriers and rely on that
>>                     field being eventually visible across all other
>>                     threads (no problem by reading stale data for a few
>>                     seconds).
>>
>>                     Would that be possible with the current JMM ? I
>>                     tried to test that scenario with some code, and it
>>                     seems to work most of the times, but some threads
>>                     read stale data for longer that I would expect (lots
>>                     of seconds). Is there any platform dependency on
>>                     such implementation ? Its going to run on x86
>>                     environments. Is there any assumption we can make as
>>                     of how long that 'eventually' part can be ? (could
>>                     it be more than 5 mins, when the next write
>>                     occurs?). My understanding is that, that write even
>>                     if re-ordered will have to happen. I came across an
>>                     article about using the AtomicReference doing a
>>                     lazySet (store-store) for the write, and then the
>>                     Unsafe to do a getObject (direct) instead of the
>>                     default get which is based on the volatile access.
>>                     Would that be a better solution?
>>
>>                     Any ideas, alternatives?
>>
>>                     PS. Sorry for the question-bombing :/
>>
>>                     Regards,
>>                     Thomas
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/
>> listinfo/concurrency-interest
>>
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest at cs.oswego.edu
>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                 http://cs.oswego.edu/mailman/
>> listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>         --
>>         Thomas Kountis
>>         PGP: 0x069D29A3
>>
>>         Q: "Whats the object-oriented way to become wealthy?"
>>         A:  Inheritance
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/2258919c/attachment-0001.html>

From tkountis at gmail.com  Fri Nov 29 18:00:04 2013
From: tkountis at gmail.com (Thomas Kountis)
Date: Fri, 29 Nov 2013 23:00:04 +0000
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
	<52990E62.9080209@infinite-source.de>
	<CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>
Message-ID: <CAHbGfh3SakAr2K22p3bVZjdTmegb2jk+Epoknncf18Er5x9ZKQ@mail.gmail.com>

That last part is a bit unclear. If we are marking a field as volatile and
use the direct get (through Unsafe) how and why would JIT hoist
instructions ? Isn't that what volatile guards from as well ?


On Fri, Nov 29, 2013 at 10:44 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> On x86 the volatile read isn't going to cost much of anything over a
> normal one - it's just a compiler barrier.  If it's read often it'll be in
> L1 cache and be just a few clocks.
>
> Using a naked read leaves you open to the scenario Nitsan mentioned
> earlier in this thread.
>
> Sent from my phone
> On Nov 29, 2013 5:02 PM, "Aaron Grunthal" <
> aaron.grunthal at infinite-source.de> wrote:
>
>> Isn't the problem with both AtomicReferenceFieldUpdater and
>> AtomicReference that they don't have a lazyGet? The get is equivalent to a
>> volatile read. Only Unsafe can give you normal get and ordered put, i.e.
>> safe single-writer publishing.
>>
>>
>> On 29.11.2013 22:04, Vitaly Davidovich wrote:
>>
>>> AtomicReferenceFieldUpdater has some safety/defensive checking overhead,
>>> IIRC.  Although with writes every 5 mins only I don't think the write
>>> side perf matters all that much.
>>>
>>> Sent from my phone
>>>
>>> On Nov 29, 2013 2:37 PM, "Chris Vest" <mr.chrisvest at gmail.com
>>> <mailto:mr.chrisvest at gmail.com>> wrote:
>>>
>>>     Note that AtomicReferenceFieldUpdater also has lazySet, so you don't
>>>     need to go to the Unsafe if you want to avoid the indirection of
>>>     AtomicReference.
>>>
>>>     --
>>>     Chris
>>>
>>>
>>>     On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com
>>>     <mailto:tkountis at gmail.com>> wrote:
>>>
>>>         Ok that makes sense.
>>>
>>>         I measured the performance and no this is not close to being a
>>>         bottleneck on the current setup, since there are other places
>>>         which are much slower. So the initial question was more into the
>>>         "lets experiment" side of things and learn. My thinking was that
>>>         since we only have one writer and thats only on a specific
>>>         interval (5 mins) there would be a waste of using the CHM to
>>>         host the data. Taking a snapshot of the current state during a
>>>         write, replace of things and reassign the map variable to the
>>>         snapshot seemed more interesting.
>>>
>>>         Thanks again :)
>>>
>>>
>>>
>>>         On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart
>>>         <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>>
>>>             What could go wrong is:
>>>                 while(getMap().get(key)){
>>>                    // wait for key
>>>               }
>>>             Can in theory (and sometimes in practice) spin forever. The
>>>             value gets hoisted and never checked again. The JIT assumes
>>>             there's no way for the read value to change, so won't check
>>>             again. At that point the 'at some point' becomes 'never'.
>>>             Reading through your original post again I would take a step
>>>             back and evaluate the performance win vs. correctness. Are
>>>             you 100% sure this is a bottleneck for your program? have
>>>             you tried and measured using CHM and it makes a measurable
>>>             difference to use your alternative?
>>>             If the answer to the above is "yes we did, and yes it does"
>>>             then it would be good to see some concrete code to
>>>             demonstrate how you are using this map.
>>>
>>>
>>>             On Friday, November 29, 2013 10:01 AM, Thomas Kountis
>>>             <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>>             Thanks for the responses guys.
>>>             I do understand all the above, but what I don't understand
>>>             is what could go wrong with the no barrier approach on x86 ?
>>>             Wouldn't that write eventually get flushed to main memory,
>>>             and the other processors must invalidate cache at some point
>>>             also ? I know this is a lot of "at some point", therefore, I
>>>             guess its very vague to be trusted, but is there anything
>>>             else apart from timing that could go wrong and that write
>>>             not become visible?
>>>
>>>             t.
>>>
>>>
>>>             On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart
>>>             <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>>
>>>                  From my experience, lazySet is indeed your best choice
>>>                 (but only a valid choice for a single writer). You need
>>>                 a volatile read to match the HB relationship otherwise
>>>                 the compiler is free to optimize the value you read, so
>>>                 someone using your map in a loop may end up stuck if you
>>>                 don't do it.
>>>
>>>
>>>
>>>                 On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich
>>>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>>                 AtomicReference.lazySet is the way to go here - on x86
>>>                 this is just normal mov instruction with compiler
>>>                 barrier only (StoreStore).  If you don't want overhead
>>>                 of AtomicReference wrapper (doesn't sound like that
>>>                 would be an issue) you can get same effect with
>>>                 Unsafe.putObjectOrdered.
>>>                 I wouldn't worry about AtomicReference.get() performance
>>>                 on x86 - this is a read from memory but if you read
>>>                 frequently, you'll hit L1 cache anyway.
>>>                 HTH
>>>                 Sent from my phone
>>>                 On Nov 28, 2013 5:34 PM, "Thomas Kountis"
>>>                 <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>>
>>>                     Hi all,
>>>
>>>                     This is my first time posting on this list, been
>>>                     follower for quite some time now and really enjoying
>>>                     all the knowledge sharing :) .
>>>
>>>                     I was looking on optimizing a solution today at
>>>                     work, and I came across the following.
>>>                     We have a scenario where we keep a simple cache
>>>                     (HashMap) and this is accessed by multiple
>>>                     readers on an application server, millions of times
>>>                     per day and highly contented. This cache is
>>>                     immutable and only gets updated by a single writer
>>>                     by replacing the reference that the variable points
>>>                     to every 5 mins. This is currently done as a
>>>                     volatile field. I was looking for a way to lose
>>>                     completely the memory barriers and rely on that
>>>                     field being eventually visible across all other
>>>                     threads (no problem by reading stale data for a few
>>>                     seconds).
>>>
>>>                     Would that be possible with the current JMM ? I
>>>                     tried to test that scenario with some code, and it
>>>                     seems to work most of the times, but some threads
>>>                     read stale data for longer that I would expect (lots
>>>                     of seconds). Is there any platform dependency on
>>>                     such implementation ? Its going to run on x86
>>>                     environments. Is there any assumption we can make as
>>>                     of how long that 'eventually' part can be ? (could
>>>                     it be more than 5 mins, when the next write
>>>                     occurs?). My understanding is that, that write even
>>>                     if re-ordered will have to happen. I came across an
>>>                     article about using the AtomicReference doing a
>>>                     lazySet (store-store) for the write, and then the
>>>                     Unsafe to do a getObject (direct) instead of the
>>>                     default get which is based on the volatile access.
>>>                     Would that be a better solution?
>>>
>>>                     Any ideas, alternatives?
>>>
>>>                     PS. Sorry for the question-bombing :/
>>>
>>>                     Regards,
>>>                     Thomas
>>>
>>>                     _______________________________________________
>>>                     Concurrency-interest mailing list
>>>                     Concurrency-interest at cs.oswego.edu
>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>                     http://cs.oswego.edu/mailman/
>>> listinfo/concurrency-interest
>>>
>>>
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 http://cs.oswego.edu/mailman/
>>> listinfo/concurrency-interest
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/6384707f/attachment.html>

From vitalyd at gmail.com  Fri Nov 29 18:13:37 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 29 Nov 2013 18:13:37 -0500
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHbGfh3SakAr2K22p3bVZjdTmegb2jk+Epoknncf18Er5x9ZKQ@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
	<52990E62.9080209@infinite-source.de>
	<CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>
	<CAHbGfh3SakAr2K22p3bVZjdTmegb2jk+Epoknncf18Er5x9ZKQ@mail.gmail.com>
Message-ID: <CAHjP37HAJUfp7BH6BwMaq+PvTnS7RKNPAnZ_E3s5sq12TQzHUQ@mail.gmail.com>

If the field is volatile but accessed through unsafe direct member access,
I'm not sure whether the access is treated as volatile or not in that
case.  However, I don't see the point in marking the field as volatile and
accessing through unsafe and expecting compiler barrier - this is what
AtomicReference.get() will achieve.

At the end of the day, you need to make sure the reference of the hashmap
is read from memory so you can detect changes; you can read that into a
local and then read the actual hashmap internals from the cached address.
So you need a volatile load here (and on x86 and 64 this translates into
just compiler barrier, as mentioned before, which is pretty cheap).

Sent from my phone
On Nov 29, 2013 6:00 PM, "Thomas Kountis" <tkountis at gmail.com> wrote:

> That last part is a bit unclear. If we are marking a field as volatile and
> use the direct get (through Unsafe) how and why would JIT hoist
> instructions ? Isn't that what volatile guards from as well ?
>
>
> On Fri, Nov 29, 2013 at 10:44 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> On x86 the volatile read isn't going to cost much of anything over a
>> normal one - it's just a compiler barrier.  If it's read often it'll be in
>> L1 cache and be just a few clocks.
>>
>> Using a naked read leaves you open to the scenario Nitsan mentioned
>> earlier in this thread.
>>
>> Sent from my phone
>> On Nov 29, 2013 5:02 PM, "Aaron Grunthal" <
>> aaron.grunthal at infinite-source.de> wrote:
>>
>>> Isn't the problem with both AtomicReferenceFieldUpdater and
>>> AtomicReference that they don't have a lazyGet? The get is equivalent to a
>>> volatile read. Only Unsafe can give you normal get and ordered put, i.e.
>>> safe single-writer publishing.
>>>
>>>
>>> On 29.11.2013 22:04, Vitaly Davidovich wrote:
>>>
>>>> AtomicReferenceFieldUpdater has some safety/defensive checking overhead,
>>>> IIRC.  Although with writes every 5 mins only I don't think the write
>>>> side perf matters all that much.
>>>>
>>>> Sent from my phone
>>>>
>>>> On Nov 29, 2013 2:37 PM, "Chris Vest" <mr.chrisvest at gmail.com
>>>> <mailto:mr.chrisvest at gmail.com>> wrote:
>>>>
>>>>     Note that AtomicReferenceFieldUpdater also has lazySet, so you don't
>>>>     need to go to the Unsafe if you want to avoid the indirection of
>>>>     AtomicReference.
>>>>
>>>>     --
>>>>     Chris
>>>>
>>>>
>>>>     On 29 November 2013 10:06, Thomas Kountis <tkountis at gmail.com
>>>>     <mailto:tkountis at gmail.com>> wrote:
>>>>
>>>>         Ok that makes sense.
>>>>
>>>>         I measured the performance and no this is not close to being a
>>>>         bottleneck on the current setup, since there are other places
>>>>         which are much slower. So the initial question was more into the
>>>>         "lets experiment" side of things and learn. My thinking was that
>>>>         since we only have one writer and thats only on a specific
>>>>         interval (5 mins) there would be a waste of using the CHM to
>>>>         host the data. Taking a snapshot of the current state during a
>>>>         write, replace of things and reassign the map variable to the
>>>>         snapshot seemed more interesting.
>>>>
>>>>         Thanks again :)
>>>>
>>>>
>>>>
>>>>         On Fri, Nov 29, 2013 at 8:42 AM, Nitsan Wakart
>>>>         <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>>>
>>>>             What could go wrong is:
>>>>                 while(getMap().get(key)){
>>>>                    // wait for key
>>>>               }
>>>>             Can in theory (and sometimes in practice) spin forever. The
>>>>             value gets hoisted and never checked again. The JIT assumes
>>>>             there's no way for the read value to change, so won't check
>>>>             again. At that point the 'at some point' becomes 'never'.
>>>>             Reading through your original post again I would take a step
>>>>             back and evaluate the performance win vs. correctness. Are
>>>>             you 100% sure this is a bottleneck for your program? have
>>>>             you tried and measured using CHM and it makes a measurable
>>>>             difference to use your alternative?
>>>>             If the answer to the above is "yes we did, and yes it does"
>>>>             then it would be good to see some concrete code to
>>>>             demonstrate how you are using this map.
>>>>
>>>>
>>>>             On Friday, November 29, 2013 10:01 AM, Thomas Kountis
>>>>             <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>>>             Thanks for the responses guys.
>>>>             I do understand all the above, but what I don't understand
>>>>             is what could go wrong with the no barrier approach on x86 ?
>>>>             Wouldn't that write eventually get flushed to main memory,
>>>>             and the other processors must invalidate cache at some point
>>>>             also ? I know this is a lot of "at some point", therefore, I
>>>>             guess its very vague to be trusted, but is there anything
>>>>             else apart from timing that could go wrong and that write
>>>>             not become visible?
>>>>
>>>>             t.
>>>>
>>>>
>>>>             On Fri, Nov 29, 2013 at 6:51 AM, Nitsan Wakart
>>>>             <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>> wrote:
>>>>
>>>>                  From my experience, lazySet is indeed your best choice
>>>>                 (but only a valid choice for a single writer). You need
>>>>                 a volatile read to match the HB relationship otherwise
>>>>                 the compiler is free to optimize the value you read, so
>>>>                 someone using your map in a loop may end up stuck if you
>>>>                 don't do it.
>>>>
>>>>
>>>>
>>>>                 On Friday, November 29, 2013 5:35 AM, Vitaly Davidovich
>>>>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>>>                 AtomicReference.lazySet is the way to go here - on x86
>>>>                 this is just normal mov instruction with compiler
>>>>                 barrier only (StoreStore).  If you don't want overhead
>>>>                 of AtomicReference wrapper (doesn't sound like that
>>>>                 would be an issue) you can get same effect with
>>>>                 Unsafe.putObjectOrdered.
>>>>                 I wouldn't worry about AtomicReference.get() performance
>>>>                 on x86 - this is a read from memory but if you read
>>>>                 frequently, you'll hit L1 cache anyway.
>>>>                 HTH
>>>>                 Sent from my phone
>>>>                 On Nov 28, 2013 5:34 PM, "Thomas Kountis"
>>>>                 <tkountis at gmail.com <mailto:tkountis at gmail.com>> wrote:
>>>>
>>>>                     Hi all,
>>>>
>>>>                     This is my first time posting on this list, been
>>>>                     follower for quite some time now and really enjoying
>>>>                     all the knowledge sharing :) .
>>>>
>>>>                     I was looking on optimizing a solution today at
>>>>                     work, and I came across the following.
>>>>                     We have a scenario where we keep a simple cache
>>>>                     (HashMap) and this is accessed by multiple
>>>>                     readers on an application server, millions of times
>>>>                     per day and highly contented. This cache is
>>>>                     immutable and only gets updated by a single writer
>>>>                     by replacing the reference that the variable points
>>>>                     to every 5 mins. This is currently done as a
>>>>                     volatile field. I was looking for a way to lose
>>>>                     completely the memory barriers and rely on that
>>>>                     field being eventually visible across all other
>>>>                     threads (no problem by reading stale data for a few
>>>>                     seconds).
>>>>
>>>>                     Would that be possible with the current JMM ? I
>>>>                     tried to test that scenario with some code, and it
>>>>                     seems to work most of the times, but some threads
>>>>                     read stale data for longer that I would expect (lots
>>>>                     of seconds). Is there any platform dependency on
>>>>                     such implementation ? Its going to run on x86
>>>>                     environments. Is there any assumption we can make as
>>>>                     of how long that 'eventually' part can be ? (could
>>>>                     it be more than 5 mins, when the next write
>>>>                     occurs?). My understanding is that, that write even
>>>>                     if re-ordered will have to happen. I came across an
>>>>                     article about using the AtomicReference doing a
>>>>                     lazySet (store-store) for the write, and then the
>>>>                     Unsafe to do a getObject (direct) instead of the
>>>>                     default get which is based on the volatile access.
>>>>                     Would that be a better solution?
>>>>
>>>>                     Any ideas, alternatives?
>>>>
>>>>                     PS. Sorry for the question-bombing :/
>>>>
>>>>                     Regards,
>>>>                     Thomas
>>>>
>>>>                     _______________________________________________
>>>>                     Concurrency-interest mailing list
>>>>                     Concurrency-interest at cs.oswego.edu
>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                     http://cs.oswego.edu/mailman/
>>>> listinfo/concurrency-interest
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/
>>>> listinfo/concurrency-interest
>>>>
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu
>>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.oswego.edu
>>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/2a97e244/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Fri Nov 29 18:49:22 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Sat, 30 Nov 2013 00:49:22 +0100
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <CAHjP37HAJUfp7BH6BwMaq+PvTnS7RKNPAnZ_E3s5sq12TQzHUQ@mail.gmail.com>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>	<52990E62.9080209@infinite-source.de>	<CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>	<CAHbGfh3SakAr2K22p3bVZjdTmegb2jk+Epoknncf18Er5x9ZKQ@mail.gmail.com>
	<CAHjP37HAJUfp7BH6BwMaq+PvTnS7RKNPAnZ_E3s5sq12TQzHUQ@mail.gmail.com>
Message-ID: <52992802.5060302@infinite-source.de>

On 30.11.2013 00:13, Vitaly Davidovich wrote:
> If the field is volatile but accessed through unsafe direct member
> access, I'm not sure whether the access is treated as volatile or not in
> that case.  However, I don't see the point in marking the field as
> volatile and accessing through unsafe and expecting compiler barrier -
> this is what AtomicReference.get() will achieve.

I've tried that a while ago and it looked like that a plain Unsafe read 
to a volatile field (or a normal field for that matter) will not get 
loop-hoisted. So it seems to sort-of act like a compiler barrier. 
Whether that's intentional behavior or just an implementation detail was 
never figured out.

Although it might be useful for concurrent data structures on non-x86 
platforms to optimistically attempt a stale read to avoid 
cache-coherency costs. I think I've seen a few places the j.u.c Classes 
where that's done. If the compiler were free to reorder through the 
plain unsafe reads it might yield more optimal assembly output.


But you always have the reverse option of accessing a normal field 
through unsafe, which gives you a barrier-free reads if needed and all 
the other ordering options as needed. But that's something for 
fine-tuned algorithms where every single instruction counts and not for 
Thomas' example.

From vitalyd at gmail.com  Fri Nov 29 19:01:57 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 29 Nov 2013 19:01:57 -0500
Subject: [concurrency-interest] Single writer multiple readers no
 barriers -- safe ?
In-Reply-To: <52992802.5060302@infinite-source.de>
References: <CAHbGfh1-5txgrnPn3V1Ee5yY261Mg8_iaaaWAVJkSVo9wKTL8Q@mail.gmail.com>
	<CAHjP37EtmdQCCoBXpbktUr2q-E6KuKGO38r+Gkd8WONuWQsW=w@mail.gmail.com>
	<1385707901.22231.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CAHbGfh1mWzk5UDa8CZpCXmUb-g_dNiruN3FA-ZuRrUzSns4NCg@mail.gmail.com>
	<1385714535.28894.YahooMailNeo@web120705.mail.ne1.yahoo.com>
	<CAHbGfh3Ek-Ddiacp4Zbx6oxZqwKq_aB33nQgS2XM9GSzAGg_tQ@mail.gmail.com>
	<CAHXi_0fz__OUjtEh98dcYa7jMNWxcHyGL1-j=nDaF21B3JkcTw@mail.gmail.com>
	<CAHjP37FeWnWBEqd+obiKCvScY1yYsiNL04afG4Tw732SiVX9mg@mail.gmail.com>
	<52990E62.9080209@infinite-source.de>
	<CAHjP37HDk+mDozHAPEkf9p+=_RMcrKraw=tTKYcQf8cg+UPRGA@mail.gmail.com>
	<CAHbGfh3SakAr2K22p3bVZjdTmegb2jk+Epoknncf18Er5x9ZKQ@mail.gmail.com>
	<CAHjP37HAJUfp7BH6BwMaq+PvTnS7RKNPAnZ_E3s5sq12TQzHUQ@mail.gmail.com>
	<52992802.5060302@infinite-source.de>
Message-ID: <CAHjP37HQtPMOH0kh30MqLbc2x1ykLtuYVw1q3raeDmi-OH_SGA@mail.gmail.com>

Right, for archs where volatile loads actually have a cost (I.e. not just a
compiler barrier but also hardware level order enforcement), it may be
prudent to only issue a compiler barrier load at certain strategic points
(if such control is available).  It's not really cache coherence that's the
issue (coherence is always there even for plain load/store) but rather
effects on out of order execution/pipelining that the volatile load may
cause.

Sent from my phone
On Nov 29, 2013 6:49 PM, "Aaron Grunthal" <aaron.grunthal at infinite-source.de>
wrote:

> On 30.11.2013 00:13, Vitaly Davidovich wrote:
>
>> If the field is volatile but accessed through unsafe direct member
>> access, I'm not sure whether the access is treated as volatile or not in
>> that case.  However, I don't see the point in marking the field as
>> volatile and accessing through unsafe and expecting compiler barrier -
>> this is what AtomicReference.get() will achieve.
>>
>
> I've tried that a while ago and it looked like that a plain Unsafe read to
> a volatile field (or a normal field for that matter) will not get
> loop-hoisted. So it seems to sort-of act like a compiler barrier. Whether
> that's intentional behavior or just an implementation detail was never
> figured out.
>
> Although it might be useful for concurrent data structures on non-x86
> platforms to optimistically attempt a stale read to avoid cache-coherency
> costs. I think I've seen a few places the j.u.c Classes where that's done.
> If the compiler were free to reorder through the plain unsafe reads it
> might yield more optimal assembly output.
>
>
> But you always have the reverse option of accessing a normal field through
> unsafe, which gives you a barrier-free reads if needed and all the other
> ordering options as needed. But that's something for fine-tuned algorithms
> where every single instruction counts and not for Thomas' example.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20131129/cc206474/attachment.html>

