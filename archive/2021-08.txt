Subject: Re: [concurrency-interest] Are there real use cases with the Java access modes?
From: Gregg Wonderly via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-10, 19:20
To: Andrew Dinn via Concurrency-interest <concurrency-interest at cs.oswego.edu>
CC: Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
Reply-To: Gregg Wonderly <gergg at cox.net>

The problem is that the value is not repetitively read because of the hoist out of the loop.  All the optimizations to make ineffective hardware sharing work faster yet, not do what the code says is the issue I am trying to stress.  When the code says do something, that’s what the developer sees should happen.  When the compiler alters the code snd that doesn’t happen, that’s when we’ve let the hardware limitations create a barrier to correctly operating software. 

The JMM was designed to let various memory models all operate with the same outcomes.  Yet in this case an optimization for a memory model detail is intruding on the developers ability to reason about the source of misbehavior they see.  Non-volatile access is being used to justify an optimization that invalidates the logic of the written code.

Gregg Wonderly

Sent from my iPhone

> > On Jul 20, 2021, at 5:42 AM, Andrew Dinn via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
> > 
> > ﻿On 19/07/2021 22:39, Gregg Wonderly via Concurrency-interest wrote:
>> >> Thanks for you comments.   I am still trying to assert that the problem is this kind of assumption about people writing the code, actually being trained software engineers.  Instead, think of them as self taught coders.  People who only every wrote “basic” or “vb” style integrations with just some knowledge that threads even exist, let alone, as in this case, knowing that the AWT event queue is involved and that there are threads (or more in the case of dialogs and other blocking actions) reaping events from the queue and dispatching them into your code.  Even the term callback or listener, for these people, doesn’t invoke any picture of “two” things working together.
> > 
> > And I am still trying to assert that we should not hobble the language implementation to cater for that category of 'programmer' (rabbit ears de rigeur). Java has been specified and implemented for use by skilled and knowledgeable professionals. That includes knowing about and having the skill to deal with the presence of multi-threading as a core element of the language, with all its attendant complexities.
> > 
> > Of course, as a Java implementor I make no assumption that all those who use Java will be skilled, knowledgeable professionals. What I do assume is that I don't have to take the concerns or failings of unskilled or ignorant 'would-be' coders into account.
> > 
> > By the way, I believe you are having your cake and eating it in the way you present your arguments here. You cited an example program as simple to understand and behaving as expected until you remove some logging code to simplify it and suddenly ... oh,. how surprising, a non-volatile access gets hoisted! At which point this simple to understand code becomes incomprehensible to the average reader.
> > 
> > In truth, if you know how to read the different versions of this code with an awareness that Java is a multi-threaded language the complexity is never absent. The surprise you describe is prima facie evidence that your posited average reader is not reading the code correctly, whether in the original or reduced version. They just think they understand it.
> > 
> > regards,
> > 
> > 
> > Andrew Dinn
> > -----------
> > Red Hat Distinguished Engineer
> > Red Hat UK Ltd
> > Registered in England and Wales under Company Registration No. 03798903
> > Directors: Michael Cunningham, Michael ("Mike") O'Neill
> > 

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Gregg Wonderly via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 03:23
To: Thorsten <tg at freigmbh.de>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Gregg Wonderly <gergg at cox.net>



Sent from my iPhone

> > On Jul 21, 2021, at 4:01 AM, Thorsten via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:

> > So the usecase Greg presented looks like this: theres a loop, and the condition is changed from another thread. That is indeed a real world scenario. But in your example there is only one field "done" for which you want protection. But in any real usecase there will be more fields.

There may be, but if I push a disconnect or close button, I want any running threads to stop, and let them finish cleanup.  Any other UI data passing might be a single reference too.

In the hobby of amateur radio, there are lots of things that involve just doing one thing.  Imagine a UI that has a frequency display, a place to type a new frequency and a tuning button or knob for subtle changes in tuning needed during satellite passes when the Doppler effect is changing the relationship of the frequency you are transmitting on relative to the receiver in a satellite.

All of these things are just ways to say “change a floating point number that is the frequency”.   

There are just many different things that I do in remote data gathering as well that involve starting and stopping work with a single native value.

The fact that they are not shared, readily, across threads by default, in a multithreaded language just seems completely counter to what inter-thread visibility would suggest.  You have to know that non-volatile is in effect and you have to know it means not shared.  This is all computer science nomenclature and that’s what my issue is.  People who are not computer scientist but write code because of the internet explosion of HTML authors migrating to JS and into the back end now, is just reducing the overall use of Java (as the licensing change has caused huge churn too).  

I am an avid Java user and I think you can look around on the internet in all the places I’ve participated i the community to see my convictions to Java.  But this one single issue continues to be the 10minutes lecture I have to give to anyone who is learning Java as the first thing to learn.  I can’t teach things about cool APIs that simplify things and show a pretty picture.  Instead I have to make excuses, point out the huge set of problems with anything involving data sharing.   GUIs share data all over the place.  Across thread sharing is a paramount point of Java AWT/Swing fm development.

Yet the vast majority of arguments are about huge server application needs regarding ROI on huge servers.  I get that, but that’s not the write once run anywhere paradigm that AWT/Swing enable for GUI development.  Maybe the hoisting could just be turned off in the client JIT by default with a command line argument to set it explicitly so that users could also choose to disable it.

Having to wrap all native type values with synchronized setter/getter behavior is just a sad statement on the effectiveness of multi-threading over all, in my opinion.  I really keep dwelling on this, because it’s just counter to many expectations in my experience.  Things like people saying final or volatile on every class value, speaks directly to what a surprise and a hazard it can be to software systems.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Brian S O'Neill via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 05:17
To: concurrency-interest@cs.oswego.edu
Reply-To: Brian S O'Neill <bronee at gmail.com>

On 2021-08-10 05:23 PM, Gregg Wonderly via Concurrency-interest wrote:
>
> Across thread sharing is a paramount point of Java AWT/Swing fm development.
>

And this is where AWT got it wrong. There was never any reason to design a GUI framework that didn't behave like a good-ol' single-threaded event loop. The thread that starts the app should be "main", and this is the same thread that should be processing events. Background tasks can be performed by separate threads, but this can be safely integrated such that users don't have to deal with race conditions.

If you look back at Java in 1995, you'll see that it got threading wrong pretty much everywhere. Many of the demo applets want to show off Java threads, and they issue things like stop/suspend/resume and do all sorts of unsafe state sharing.

Most of these mistakes have been fixed -- APIs have been deprecated, replacements created, better frameworks exist now, there's a proper memory model specification, and all of the old demos are gone. And yet the AWT still exists. Back in 1995 we called it the "Awkward Window Toolkit". It never made much sense, but somehow it needed to show that threads were useful, even for toy applets. At the time, that's all Java was -- an applet demo system.

I'm not a radio enthusiast, but I assume that like any hobby, there's different levels. I can jump into the hobby with requiring an EE degree, but I'll want that to achieve expert level. If I had to start off by designing and building my own radio, that's not a lot of fun.

Is your argument that Java is like this? Start with a pile of parts and design your own radio? If I want to write a GUI app, I shouldn't have to write my own framework for doing this. I'd like to use one that's already built-in and works just fine. With Java, such a thing doesn't exist. The starter radio kit has broken insulation and frequently electrocutes you.

Java has never been very popular on the desktop, or in the browser, and it never will be until the mistakes of 1995 are wiped clean. It needs a completely new GUI framework, modeled after something that actually works. Until that happens, it's completely inappropriate to expose new users to writing GUI apps in Java.

I've not looked at JavaFX. Is it a rewrite, or is it layered on top of AWT just like Swing? Is it single-threaded from the user's perspective, or does it suffer all the same problems by trying to be multi-threaded?

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Thorsten via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 10:25
To: Brian S O'Neill <bronee at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Thorsten <tg at freigmbh.de>

Hello,

Am 11/08/2021 um 04:17 schrieb Brian S O'Neill via Concurrency-interest:
>
> I've not looked at JavaFX. Is it a rewrite, or is it layered on top of AWT just like Swing? Is it single-threaded from the user's perspective, or does it suffer all the same problems by trying to be multi-threaded? 

JavaFX is single-Threaded, Platform.runLater(). Swing is also single-Threaded, SwingUtilities.invokeLater(). They can also be put on the same thread. I cannot comment on how threading was supposed to be handled in pure AWT, but swing renders this question kinda obsolete since 1998, thats more than 20 years...

Best regards,

Thorsten


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
.


-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Thorsten via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 11:46
To: Gregg Wonderly <gergg at cox.net>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Thorsten <tg at freigmbh.de>

Hello,
> The fact that they are not shared, readily, across threads by default, in a multithreaded language just seems completely counter to what inter-thread visibility would suggest.  You have to know that non-volatile is in effect and you have to know it means not shared.  This is all computer science nomenclature and that’s what my issue is.
Yes, thats correct. However It's creating a programming language for someone who cannot program and is unwilling to learn creates a lot of problems. Same goes for multithreading.
>  
> I am an avid Java user and I think you can look around on the internet in all the places I’ve participated i the community to see my convictions to Java.  But this one single issue continues to be the 10minutes lecture I have to give to anyone who is learning Java as the first thing to learn.

Its my strong opinion if this "single issue" would be "fixed",it would simply be replaced by another threading issue/requirement. The new issue would be even harder to understand.


>   I can’t teach things about cool APIs that simplify things and show a pretty picture.  Instead I have to make excuses, point out the huge set of problems with anything involving data sharing.   GUIs share data all over the place.  Across thread sharing is a paramount point of Java AWT/Swing fm development.
That's just your programming style, I personally recommend Executers/SwingUtilities.invokeLater() over data sharing. You can't just say "data sharing" and expect the runtime to do the thing you think is the "correct" behaviour.
>
> Yet the vast majority of arguments are about huge server application needs regarding ROI on huge servers.  I get that, but that’s not the write once run anywhere paradigm that AWT/Swing enable for GUI development.  Maybe the hoisting could just be turned off in the client JIT by default with a command line argument to set it explicitly so that users could also choose to disable it.
That was answered by Alex, you can do that using byte code instrumentation, results may be underwhelming.
>
> Having to wrap all native type values with synchronized setter/getter behavior is just a sad statement on the effectiveness of multi-threading over all, in my opinion.  I really keep dwelling on this, because it’s just counter to many expectations in my experience.  Things like people saying final or volatile on every class value, speaks directly to what a surprise and a hazard it can be to software systems.

As said above, ultimatily that would mean java needs to create a threading/memory model for people who don't understand threading and aren't even trying. For comparison if you have a SQL-Database using unique and and foreign keys and now someone sees keys violation, does not understand what that means, but he realizes that deleting all the keys makes the error "disappear", so he simply does that for every project he's in, does that mean SQL has a fundamental problem and should not support keys?

My main point is even if anything/a lot gets changed about the way java does multithreading, you will still find a lot of ways how someone who doesn't even remotly try to do the right thing will break it.


Best regards,

Thorsten


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Gregg Wonderly via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 15:28
To: Thorsten <tg at freigmbh.de>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Gregg Wonderly <gergg at cox.net>

Thanks for your thoughts Thorsten!

> > On Aug 11, 2021, at 3:46 AM, Thorsten via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
> > 
> > Hello,
>> >> The fact that they are not shared, readily, across threads by default, in a multithreaded language just seems completely counter to what inter-thread visibility would suggest.  You have to know that non-volatile is in effect and you have to know it means not shared.  This is all computer science nomenclature and that’s what my issue is.
> > Yes, thats correct. However It's creating a programming language for someone who cannot program and is unwilling to learn creates a lot of problems. Same goes for multithreading.

They want to learn in my experience.  But the unknowns created by this loop condition hoisting single handedly makes it hard to “get” what’s happening and requires a pretty deep dive into the complete JMM, which I contend is something that should enable portable Java implementations, but should not be the first thing a Java developer has to contend with.

>> >>  
>> >> I am an avid Java user and I think you can look around on the internet in all the places I’ve participated i the community to see my convictions to Java.  But this one single issue continues to be the 10minutes lecture I have to give to anyone who is learning Java as the first thing to learn.
> > 
> > Its my strong opinion if this "single issue" would be "fixed",it would simply be replaced by another threading issue/requirement. The new issue would be even harder to understand.

It’s simply the logic invalidation for the boolean loop control that I am referring to.  Data sharing with races should look racy with random values and half written 64 bit values etc.  But a single value shared between threads where one thread writes and the other thread reads should be a simple, multi-threading construct.  I’ll point at the most common multi-threaded solution for mutating data is to copy on write and replace a reference so that there are no locks required while building the new value, and the reference replaced with the new, copied and mutated object.  I don’t want to start a new discussion on compareAndSet, but yes, I know that’s the requirement for reference swapping when there are multiple threads involved.

> > 
> > 
>> >>  I can’t teach things about cool APIs that simplify things and show a pretty picture.  Instead I have to make excuses, point out the huge set of problems with anything involving data sharing.   GUIs share data all over the place.  Across thread sharing is a paramount point of Java AWT/Swing fm development.
> > That's just your programming style, I personally recommend Executers/SwingUtilities.invokeLater() over data sharing. You can't just say "data sharing" and expect the runtime to do the thing you think is the "correct" behaviour.

I personally use a class called ComponentUpdateThread that I created for use in my Jini based client systems decades ago.  It handles lots of details around security and maintaining login context and other things between worker threads and the like.  I use it to “set” the boolean to try when the button is pushed.   The problem is that the loop check is no longer evaluating the boolean, so it doesn’t matter what I do with SwingWorker and the like.  That loop is not going to exit until I put multi-threaded state management or volatile into play on references to that boolean for the loop check.

>> >> 
>> >> Yet the vast majority of arguments are about huge server application needs regarding ROI on huge servers.  I get that, but that’s not the write once run anywhere paradigm that AWT/Swing enable for GUI development.  Maybe the hoisting could just be turned off in the client JIT by default with a command line argument to set it explicitly so that users could also choose to disable it.
> > That was answered by Alex, you can do that using byte code instrumentation, results may be underwhelming.

This gets back into whether they know about all these tools and how much time are they willing to spend learning just to write a simple application.

>> >> 
>> >> Having to wrap all native type values with synchronized setter/getter behavior is just a sad statement on the effectiveness of multi-threading over all, in my opinion.  I really keep dwelling on this, because it’s just counter to many expectations in my experience.  Things like people saying final or volatile on every class value, speaks directly to what a surprise and a hazard it can be to software systems.
> > 
> > As said above, ultimatily that would mean java needs to create a threading/memory model for people who don't understand threading and aren't even trying. For comparison if you have a SQL-Database using unique and and foreign keys and now someone sees keys violation, does not understand what that means, but he realizes that deleting all the keys makes the error "disappear", so he simply does that for every project he's in, does that mean SQL has a fundamental problem and should not support keys?

It’s really about the loop control hoist due to no visible sites of mutation by the same thread.  Alex has continued to argue that this is a false narrative and that it will never happen because there will somehow be code in place that will keep the hoist from happening.  Yet my experience was and still is that you can make it happen with simple programs that people would create to investigate Java.  Again, it’s also an example problem in that .mil Java things-not-to-do document and that implies that it’s really a well recognized thing to manage.

> > My main point is even if anything/a lot gets changed about the way java does multithreading, you will still find a lot of ways how someone who doesn't even remotly try to do the right thing will break it.

My specific point is about this loop check hoist.  It’s about that fact that its really improbable that anyone would write code with a non-volatile loop control expression with a class level, implicitly shared, value.  What the JIT should be doing is looking at all other functions and noticing that there are mutations and noting that the hoist can not occur because single-threaded, non-sharing can not be validated.  The Java compiler could issue a warning about this during compilation.  That would also advise someone that any intentional opportunity for optimization is not occurring and that a local, non-shared reference would be needed to keep the class level, implicitly shared value from controlling the potential for optimization.

The point being that this would keep loop control logic from failing to demonstrate the coded behavior so that people could actually avoid having their code optimized to not work as coded.

Gregg Wonderly
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Gregg Wonderly via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 15:43
To: Brian S O'Neill <bronee at gmail.com>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Gregg Wonderly <gergg at cox.net>



> > On Aug 10, 2021, at 9:17 PM, Brian S O'Neill via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:

> > Java has never been very popular on the desktop, or in the browser, and it never will be until the mistakes of 1995 are wiped clean. It needs a completely new GUI framework, modeled after something that actually works. Until that happens, it's completely inappropriate to expose new users to writing GUI apps in Java.

This is a pretty consistent view that I hear over and over.  Yet, countless people and examples exist of people creating jar based applications that they’ve passed around to various people groups.  People have these jars on their desktops and just double click on them to run.  Java web start applications were used for volatile codebases of large desktop apps to download all the code and then use it.  In the Jini community, we’ve used java desktop applications to talk to remote services.

When the Java 9 modularity release plan was being pushed by Mark, he asserted in his desire to “turn off reflection” in that release, that he also had no idea what was happening in the client/desktop environment.  Eventually, enough people raised enough issues to get the plan changed to keep reflection in for a while.   And it was left in for many releases after.  What Sun and Oracle engineers never understood was that the licensing for Java very much made desktop app development free, because most Java applications on the desktop were not used continuously and were not the only application running.  These desktop apps have greatly enhance portability between Windows, MacOS as well as linux and other UNIX-like OSes where Java was ported to.   It’s that wealth of undocumented (because Sun didn’t sell a $99.00 per seat license or something to track the use of Java on the desktop) use which makes it hard to understand just how much opportunity was missed for a completely different market that what Sun and Oracle still seem to think is the only/primary market.

As Thorsten said, the SwingWorker class makes it possible to get things done in swing with multiple threads readily.  All the java.util.concurrent work makes data sharing and multi-threaded activities work with the JMM.  There really is only this one specific thing that I am complaining about as being the problem because it creates intractable behavior that is really not a useful optimization in any case that I can think of.  Alex has lots of C++ examples that he’s shared with me and lots of other bantering of my experience and arguments and words I’ve used.   I appreciate the fact that I am not always able to put down coherent words here about the total picture.  I am just trying to make it clear that this one single behavior of the JIT allowed by the loose structure of the JMM is a real hazard to creating software systems where data sharing should be readily possible without complex constructs and APIs.  But that single non-volatile hoist operation, is done all over the place in many different languages and as such, Alex (and I am sure others) really feel like I’m talking nonsense about this issue.

I am focused on Java right now, but I really believe the loop control hoist of non-volatile references in any language is just a poorly implemented optimization when it keeps the loop from ever exiting.  It’s just nonsensical that a developer would ever have that intent and the optimizer assuming that is a bit to sure of itself, or just to limited in its analysis of what’s actually possible in terms of mutations and references.

Gregg Wonderly 
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Roland Kuhn via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 15:59
To: Gregg Wonderly <gergg at cox.net>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Roland Kuhn <rk at rkuhn.info>

11.08.2021 kl. 14:43 skrev Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu>:
> > I am focused on Java right now, but I really believe the loop control hoist of non-volatile references in any language is just a poorly implemented optimization when it keeps the loop from ever exiting.  It’s just nonsensical that a developer would ever have that intent and the optimizer assuming that is a bit to sure of itself, or just to limited in its analysis of what’s actually possible in terms of mutations and references.

While I have no desire to drag this thread further, this raises an important point. Programming is a very precise activity, everybody relies on this precision, we use computers because they are blindingly fast _and stupid_. Programming languages like Perl have demonstrated that “do what I mean” is a flawed approach — trying to make the semantics “more intuitive” actually hampers our ability to correctly predict the program’s behaviour. The smaller the rule set, and the more rigorously it is upheld, the better we can reason about our programs.

So, trying to second-guess the programmer’s intention is guaranteed to lead you astray. If someone writes an infinite loop, then they must have meant it (note how this doesn’t preclude warnings).

Regards,

Roland
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Brian S O'Neill via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 17:03
To: Thorsten <tg at freigmbh.de>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Brian S O'Neill <bronee at gmail.com>

I should have been more clear here. Gregg's example awhile back showed how easy it was for a new user to inadvertently create a multi-threaded swing app, simply because the thread which created the JFrame was "main", which then underneath the covers created a second thread for processing events. This problem could be avoided if the app must be launched via a callback (to ensure it's running in the event loop thread), or if the app itself was responsible for driving events, and thus no extra thread would be created.

-- Brian

On 2021-08-11 12:25 AM, Thorsten wrote:
> Hello,
>
> Am 11/08/2021 um 04:17 schrieb Brian S O'Neill via Concurrency-interest:
>>
>> I've not looked at JavaFX. Is it a rewrite, or is it layered on top of AWT just like Swing? Is it single-threaded from the user's perspective, or does it suffer all the same problems by trying to be multi-threaded? 
>
> JavaFX is single-Threaded, Platform.runLater(). Swing is also single-Threaded, SwingUtilities.invokeLater(). They can also be put on the same thread. I cannot comment on how threading was supposed to be handled in pure AWT, but swing renders this question kinda obsolete since 1998, thats more than 20 years...
>
> Best regards,
>
> Thorsten
>
>
> .
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Brian S O'Neill via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 17:12
To: Gregg Wonderly <gergg at cox.net>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Brian S O'Neill <bronee at gmail.com>

Given that there are a bunch of these apps being written successfully, is the strange infinite loop problem the only potential pain point worth mentioning? Is the Swing/AWT concurrency issue a non-issue except for this one lingering case? Because if it is, then I think this entire thread is moot.

It's pretty easy to explain to new users how to launch a Swing app correctly to ensure that the event loop is in control, and then you have a single-thread app and no odd race conditions or confusing optimizations. Even if the loop optimization was removed, you'd never want a new user creating a multi-threaded app in the first place.

-- Brian


On 2021-08-11 05:43 AM, Gregg Wonderly wrote:
>
>
>> On Aug 10, 2021, at 9:17 PM, Brian S O'Neill via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>> Java has never been very popular on the desktop, or in the browser, and it never will be until the mistakes of 1995 are wiped clean. It needs a completely new GUI framework, modeled after something that actually works. Until that happens, it's completely inappropriate to expose new users to writing GUI apps in Java.
>
> This is a pretty consistent view that I hear over and over.  Yet, countless people and examples exist of people creating jar based applications that they’ve passed around to various people groups.  People have these jars on their desktops and just double click on them to run.  Java web start applications were used for volatile codebases of large desktop apps to download all the code and then use it.  In the Jini community, we’ve used java desktop applications to talk to remote services.
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Swing/AWT concurrency
From: Thorsten via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-11, 17:16
To: Brian S O'Neill <bronee at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Thorsten <tg at freigmbh.de>

Hello,
Am 11/08/2021 um 16:03 schrieb Brian S O'Neill:
> I should have been more clear here. Gregg's example awhile back showed how easy it was for a new user to inadvertently create a multi-threaded swing app, simply because the thread which created the JFrame was "main", which then underneath the covers created a second thread for processing events. This problem could be avoided if the app must be launched via a callback (to ensure it's running in the event loop thread), or if the app itself was responsible for driving events, and thus no extra thread would be created.

Javafx offers to Subclass javafx.application.Application as an alternative of providing a "main" method. As for swing I would agree that its not beginner friendly that you need to remember that your first Statement should be SwingUtilities.invokeLater() and if you forget it you dont get a stern error like you get from javafx, instead it works sometimes, and sometimes not... Not the end of the world though.

Best regards,

Thorsten


>
> -- Brian
>
> On 2021-08-11 12:25 AM, Thorsten wrote:
>> Hello,
>>
>> Am 11/08/2021 um 04:17 schrieb Brian S O'Neill via Concurrency-interest:
>>>
>>> I've not looked at JavaFX. Is it a rewrite, or is it layered on top of AWT just like Swing? Is it single-threaded from the user's perspective, or does it suffer all the same problems by trying to be multi-threaded? 
>>
>> JavaFX is single-Threaded, Platform.runLater(). Swing is also single-Threaded, SwingUtilities.invokeLater(). They can also be put on the same thread. I cannot comment on how threading was supposed to be handled in pure AWT, but swing renders this question kinda obsolete since 1998, thats more than 20 years...
>>
>> Best regards,
>>
>> Thorsten
>>
>>
>> .
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: Re: [concurrency-interest] Are there real use cases with the Java access modes?
From: Gregg Wonderly via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 07:36
To: Gregg Wonderly <gergg at cox.net>
CC: Joe Bowbeer <joe.bowbeer at gmail.com>, concurrency-interest <concurrency-interest at cs.oswego.edu>, Valentin Kovalenko <valentin.male.kovalenko at gmail.com>
Reply-To: Gregg Wonderly <gergg at cox.net>

All of your examples of misordering and other data corruption at least execute the blocks of code and can be debugged more readily than when the logic of a loop control is removed so that the loop never exits.  It’s only that specific moment of hoisting where the developer is losing the visible execution of the flow of execution that I am focused on.

I completely understand the hoist as a speed optimization.  Again, that’s a throughput/speed improvement in many places.  I used to write code in V7 UNIX where I used register to hoist to registers.  It’s nice to have a compiler do that for us.

Again I am not focused on fixing the code I am focused on being able to see something that matches the structure of the code.  People don’t need an optimizer to be turned on like this from the start.  Especially in the cases I am trying to discuss.

It’s just hard to do this in email because the conversation goes around in circles and zigs and zags due to time zones and each persons desire and ability to participate at any moment in time.

I’d like to find some way to just focus on the specific optimization of the hoisting a non-volatile class level (and hence shared across all functions) reference in a logic expression.  That’s a non-trivial evaluation already for the optimization to be conditional on the detection of unreachable mutation.  The fact that it’s conditional means that long term software system support is hazardous because it can turned on and off just by simple changes to the code.  Again, this is why people who I’ve heard from, say that final or volatile must be on every class level variable because you never know what will happen with future software use of a class where shared references might suddenly appear due to multi-threading.  

Yes we can have all kinds of discussions about proper software design. The problem is that humans are imperfect and even code reviews don’t find all bugs.

What is more important overall?  Software that executed the intended logic but with data races, or software that executes in unexpected ways, but does it really fast?  We’d like for there to be a third choice of completely correct software executing as effectively as desired in speed and logic.  That’s where this discussion keeps going.  There’s a path to that third choice and I contend that the second choice is never helpful to encounter.

Gregg Wonderly

Sent from my iPhone

> > On Jul 21, 2021, at 10:28 AM, Gregg Wonderly via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
> > 
> > ﻿
> > 
>> >> On Jul 21, 2021, at 4:11 AM, Alex Otenko via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>> >> The principle of least astonishment is misapplied, too. Astonishment to whom?
> > 
> > The programmer is surprised by an optimization that happens at one point in the code due to the relationship and exact nature of code elsewhere in the block.  Automated hoisting of references is certainly a great optimization for things that are being computed with, but a ’truth’ or ‘control’ statement implies that mutation would have to happen.  In the very specific case of a loop, would the programmer really be purposefully creating an infinite loop with an expression/condition involving a non-volatile, class level value reference?
> > 
>> >> In C all loops terminate, so a C programmer may be astonished that the loop you gave as an example does anything at all, if we throw away everything out of the loop body. A Python or Javascript programmer may be astonished that that loop ever terminates, because it never yields control to a routine that mutates the flag.
> > 
> > Depending on ones background, you could be used to something like green threads of old.  You could just be guessing that there is an interrupt/timer thing that calls out to other code like the V8 runtime does.
> > 
>> >> A Rust programmer may be surprised that it compiles - mutability is not a given.
> > 
> > And this is more precisely the type of thing that I am trying to speak to.  I don’t know a lot about Rust, because I haven’t been able to make time to dive in and use it for something significant.  But my casual knowledge of it, informs me that people are actually trying to make safe software systems by eliminating all of the things that “runtime”, “compiler” and “extensions” to software systems can hide form the user and invalidate their software without them knowing it.
> > 
> > If the compiler is going to hoist a value, and it’s part of a control statement that would be invalidated by the hoist, it seems like warning the developer of the mis-declaration or use of concurrent access might be prudent so that they can understand what is being overlooked in the current software design.  What would happen to large java applications if the java compiler did this today?  Would we find bugs?  I am going to bet that we would find lots of non-volatile use that is not valid, technically, but works practically because of the compiler not being able to reach a conclusion allowing it to hoist the read.  So, due to the use of fences for so many other things, there is an implicit cache coherency state that allows a write to be read across thread boundaries and everyone is happy because all tests past.
> > 
> > Gregg Wonderly
> > 
> > 
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest@cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Are there real use cases with the Java access modes?
From: Brian S O'Neill via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 08:00
To: concurrency-interest@cs.oswego.edu
Reply-To: Brian S O'Neill <bronee at gmail.com>

On 2021-08-11 09:36 PM, Gregg Wonderly via Concurrency-interest wrote:

> What is more important overall?  Software that executed the intended logic but with data races, or software that executes in unexpected ways, but does it really fast?  We’d like for there to be a third choice of completely correct software executing as effectively as desired in speed and logic.  That’s where this discussion keeps going.  There’s a path to that third choice and I contend that the second choice is never helpful to encounter.

Isn't the third choice what database systems are trying to do? Although they've (mostly) solved the problem of correctness, it comes with a price in performance. Wasn't transactional memory going to somehow solve all the world's problems? What happened to it?

-- Brian
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------------------

Subject: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Peter Veentjer via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 10:51
To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Reply-To: Peter Veentjer <alarmnummer at gmail.com>

The happens-before (HB) order is defined using:

Synchronization order (SO): total order over all synchronization actions.

Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.

Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.

And the HB relation is defined as the transitive closure of the union of the SW and PO.

My question is how do relaxed access modes like opaque and acquire/release fit into the HB?

Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).

I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?

Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?

Regards,

Peter.





_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 11:14
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Alex Otenko <oleksandr.otenko at gmail.com>

I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.

Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.

Alex

On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:

    The happens-before (HB) order is defined using:

    Synchronization order (SO): total order over all synchronization actions.

    Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.

    Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.

    And the HB relation is defined as the transitive closure of the union of the SW and PO.

    My question is how do relaxed access modes like opaque and acquire/release fit into the HB?

    Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).

    I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?

    Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?

    Regards,

    Peter.




    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Peter Veentjer via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 11:17
To: Alex Otenko <oleksandr.otenko at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Peter Veentjer <alarmnummer at gmail.com>

Hi Alex,

Thanks for your answer. That sounds like a very sensible approach.

I need to think about this.

Regards,

Peter.

On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:

    I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.

    Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.

    Alex

    On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:

        The happens-before (HB) order is defined using:

        Synchronization order (SO): total order over all synchronization actions.

        Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.

        Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.

        And the HB relation is defined as the transitive closure of the union of the SW and PO.

        My question is how do relaxed access modes like opaque and acquire/release fit into the HB?

        Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).

        I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?

        Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?

        Regards,

        Peter.




        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest@cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Shuyang Liu via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-12, 11:43
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Shuyang Liu <sliu44 at cs.ucla.edu>

Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
(Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

Hope this helps!

Best Regards,
Shuyang

> On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
> ﻿
> Hi Alex,
>
> Thanks for your answer. That sounds like a very sensible approach.
>
> I need to think about this.
>
> Regards,
>
> Peter.
>
> On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>     I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>     Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>     Alex
>
>     On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>         The happens-before (HB) order is defined using:
>
>         Synchronization order (SO): total order over all synchronization actions.
>
>         Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>         Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>         And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>         My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>         Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>         I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>         Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>         Regards,
>
>         Peter.
>
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest@cs.oswego.edu
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Peter Veentjer via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 07:02
To: Shuyang Liu <sliu44 at cs.ucla.edu>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Peter Veentjer <alarmnummer at gmail.com>

@Alex Otenko

I think there is a problem with the approach.

The SO is a total order over all synchronization actions that is consistent with PO. So it shouldn't happen that e.g. in the PO you have A->B and in the SO you have B->A.

Let's assume the following program:

CPU1:
    A=1
    B=1

And these to writes are opaque writes, than in the PO A->B, but in the synchronization order A is not ordered before B because opaque creates a total order over the loads/stores if a single address (coherence), but will not order loads/stores of different addresses. So if opaque would be part of the SO, it could violate the SO being consistent with PO.



On Thu, Aug 12, 2021 at 11:43 AM Shuyang Liu <sliu44@cs.ucla.edu> wrote:

    Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
    (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

    In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

    In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

    The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

    One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

    Hope this helps!

    Best Regards,
    Shuyang

>     On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>     ﻿
>     Hi Alex,
>
>     Thanks for your answer. That sounds like a very sensible approach.
>
>     I need to think about this.
>
>     Regards,
>
>     Peter.
>
>     On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>         I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>         Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>         Alex
>
>         On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>             The happens-before (HB) order is defined using:
>
>             Synchronization order (SO): total order over all synchronization actions.
>
>             Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>             Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>             And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>             My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>             Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>             I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>             Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>             Regards,
>
>             Peter.
>
>
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest@cs.oswego.edu
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest@cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Peter Veentjer via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 07:39
To: Shuyang Liu <sliu44 at cs.ucla.edu>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Peter Veentjer <alarmnummer at gmail.com>

Thanks Shuyang,

I'll check out the paper.

Is the plan that this new model is going to replace the happens-before model?

Regards,

Peter.

On Fri, Aug 13, 2021 at 7:02 AM Peter Veentjer <alarmnummer@gmail.com> wrote:

    @Alex Otenko

    I think there is a problem with the approach.

    The SO is a total order over all synchronization actions that is consistent with PO. So it shouldn't happen that e.g. in the PO you have A->B and in the SO you have B->A.

    Let's assume the following program:

    CPU1:
        A=1
        B=1

    And these to writes are opaque writes, than in the PO A->B, but in the synchronization order A is not ordered before B because opaque creates a total order over the loads/stores if a single address (coherence), but will not order loads/stores of different addresses. So if opaque would be part of the SO, it could violate the SO being consistent with PO.



    On Thu, Aug 12, 2021 at 11:43 AM Shuyang Liu <sliu44@cs.ucla.edu> wrote:

        Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
        (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

        In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

        In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

        The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

        One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

        Hope this helps!

        Best Regards,
        Shuyang

>         On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>         ﻿
>         Hi Alex,
>
>         Thanks for your answer. That sounds like a very sensible approach.
>
>         I need to think about this.
>
>         Regards,
>
>         Peter.
>
>         On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>             I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>             Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>             Alex
>
>             On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>                 The happens-before (HB) order is defined using:
>
>                 Synchronization order (SO): total order over all synchronization actions.
>
>                 Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>                 Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>                 And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>                 My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>                 Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>                 I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>                 Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>                 Regards,
>
>                 Peter.
>
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest@cs.oswego.edu
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest@cs.oswego.edu
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: SHUYANG LIU via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 07:48
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: SHUYANG LIU <sliu44 at cs.ucla.edu>

We are certainly working toward that goal :) 

Best,
Shuyang 

On Thu, Aug 12, 2021 at 9:40 PM Peter Veentjer <alarmnummer@gmail.com> wrote:

    Thanks Shuyang,

    I'll check out the paper.

    Is the plan that this new model is going to replace the happens-before model?

    Regards,

    Peter.

    On Fri, Aug 13, 2021 at 7:02 AM Peter Veentjer <alarmnummer@gmail.com> wrote:

        @Alex Otenko

        I think there is a problem with the approach.

        The SO is a total order over all synchronization actions that is consistent with PO. So it shouldn't happen that e.g. in the PO you have A->B and in the SO you have B->A.

        Let's assume the following program:

        CPU1:
            A=1
            B=1

        And these to writes are opaque writes, than in the PO A->B, but in the synchronization order A is not ordered before B because opaque creates a total order over the loads/stores if a single address (coherence), but will not order loads/stores of different addresses. So if opaque would be part of the SO, it could violate the SO being consistent with PO.



        On Thu, Aug 12, 2021 at 11:43 AM Shuyang Liu <sliu44@cs.ucla.edu> wrote:

            Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
            (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

            In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

            In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

            The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

            One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

            Hope this helps!

            Best Regards,
            Shuyang

>             On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>             ﻿
>             Hi Alex,
>
>             Thanks for your answer. That sounds like a very sensible approach.
>
>             I need to think about this.
>
>             Regards,
>
>             Peter.
>
>             On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>                 I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>                 Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>                 Alex
>
>                 On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>                     The happens-before (HB) order is defined using:
>
>                     Synchronization order (SO): total order over all synchronization actions.
>
>                     Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>                     Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>                     And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>                     My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>                     Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>                     I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>                     Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>                     Regards,
>
>                     Peter.
>
>
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest@cs.oswego.edu
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest@cs.oswego.edu
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 08:25
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Alex Otenko <oleksandr.otenko at gmail.com>

Yes, you are right, it's more nuanced. I didn't use this intuition in earnest.

Alex

On Fri, 13 Aug 2021, 05:02 Peter Veentjer, <alarmnummer@gmail.com> wrote:

    @Alex Otenko

    I think there is a problem with the approach.

    The SO is a total order over all synchronization actions that is consistent with PO. So it shouldn't happen that e.g. in the PO you have A->B and in the SO you have B->A.

    Let's assume the following program:

    CPU1:
        A=1
        B=1

    And these to writes are opaque writes, than in the PO A->B, but in the synchronization order A is not ordered before B because opaque creates a total order over the loads/stores if a single address (coherence), but will not order loads/stores of different addresses. So if opaque would be part of the SO, it could violate the SO being consistent with PO.



    On Thu, Aug 12, 2021 at 11:43 AM Shuyang Liu <sliu44@cs.ucla.edu> wrote:

        Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
        (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

        In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

        In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

        The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

        One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

        Hope this helps!

        Best Regards,
        Shuyang

>         On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>         ﻿
>         Hi Alex,
>
>         Thanks for your answer. That sounds like a very sensible approach.
>
>         I need to think about this.
>
>         Regards,
>
>         Peter.
>
>         On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>             I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>             Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>             Alex
>
>             On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>                 The happens-before (HB) order is defined using:
>
>                 Synchronization order (SO): total order over all synchronization actions.
>
>                 Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>                 Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>                 And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>                 My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>                 Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>                 I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>                 Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>                 Regards,
>
>                 Peter.
>
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest@cs.oswego.edu
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest@cs.oswego.edu
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 10:02
To: Shuyang Liu <sliu44 at cs.ucla.edu>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Alex Otenko <oleksandr.otenko at gmail.com>

Hi Shuyang, 

Is it ok to ask for clarifications here?

I am confused by co_0 and co going in opposite directions in the diagrams, but the text claims co_0 are treated just like other co. Doesn't that introduce cycles?..

Alex

On Thu, 12 Aug 2021, 09:43 Shuyang Liu, <sliu44@cs.ucla.edu> wrote:

    Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
    (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

    In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

    In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

    The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

    One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

    Hope this helps!

    Best Regards,
    Shuyang

>     On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>     ﻿
>     Hi Alex,
>
>     Thanks for your answer. That sounds like a very sensible approach.
>
>     I need to think about this.
>
>     Regards,
>
>     Peter.
>
>     On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>         I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>         Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>         Alex
>
>         On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>             The happens-before (HB) order is defined using:
>
>             Synchronization order (SO): total order over all synchronization actions.
>
>             Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>             Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>             And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>             My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>             Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>             I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>             Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>             Regards,
>
>             Peter.
>
>
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest@cs.oswego.edu
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest@cs.oswego.edu
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 11:01
To: Shuyang Liu <sliu44 at cs.ucla.edu>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Alex Otenko <oleksandr.otenko at gmail.com>

Ok, re-reading it today it is clear those were counterexamples.

But what is trace order? The definition says a total order of all memory accesses. Does this include plain accesses? Or is it all accesses considered thus far - opaque, ra, volatile?

Alex

On Fri, 13 Aug 2021, 08:02 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:

    Hi Shuyang, 

    Is it ok to ask for clarifications here?

    I am confused by co_0 and co going in opposite directions in the diagrams, but the text claims co_0 are treated just like other co. Doesn't that introduce cycles?..

    Alex

    On Thu, 12 Aug 2021, 09:43 Shuyang Liu, <sliu44@cs.ucla.edu> wrote:

        Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
        (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 

        In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).

        In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 

        The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 

        One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 

        Hope this helps!

        Best Regards,
        Shuyang

>         On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
>         ﻿
>         Hi Alex,
>
>         Thanks for your answer. That sounds like a very sensible approach.
>
>         I need to think about this.
>
>         Regards,
>
>         Peter.
>
>         On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
>             I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>
>             Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>
>             Alex
>
>             On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>
>                 The happens-before (HB) order is defined using:
>
>                 Synchronization order (SO): total order over all synchronization actions.
>
>                 Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>
>                 Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>
>                 And the HB relation is defined as the transitive closure of the union of the SW and PO.
>
>                 My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>
>                 Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>
>                 I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>
>                 Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>
>                 Regards,
>
>                 Peter.
>
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest@cs.oswego.edu
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest@cs.oswego.edu
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] How do relaxed access modes like opaque/acquire/release fit into the happens-before order.
From: Shuyang Liu via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-13, 17:20
To: Alex Otenko <oleksandr.otenko at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Shuyang Liu <sliu44 at cs.ucla.edu>

Hi Alex,

Yes those are counter-examples that are forbidden by the rules of JAM. In general, we look for certain kinds of cycles in an execution graph to determine the consistency of it (whether it is allowed or forbidden by the memory model)

Trace order is a total order among all events that is compatible with some basic relations in the execution, so that includes plain accesses. However, it is not used as a global order to determine the consistency of an execution. It is mainly just a mechanism used to compute a total order among full fences and a total order for RMWs.

Best Regards,
Shuyang

> On Aug 13, 2021, at 1:01 AM, Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>
> ﻿
> Ok, re-reading it today it is clear those were counterexamples.
>
> But what is trace order? The definition says a total order of all memory accesses. Does this include plain accesses? Or is it all accesses considered thus far - opaque, ra, volatile?
>
> Alex
>
> On Fri, 13 Aug 2021, 08:02 Alex Otenko, <oleksandr.otenko@gmail.com> wrote:
>
>     Hi Shuyang, 
>
>     Is it ok to ask for clarifications here?
>
>     I am confused by co_0 and co going in opposite directions in the diagrams, but the text claims co_0 are treated just like other co. Doesn't that introduce cycles?..
>
>     Alex
>
>     On Thu, 12 Aug 2021, 09:43 Shuyang Liu, <sliu44@cs.ucla.edu> wrote:
>
>         Our group had a paper in 2019 formalizing the access modes in Java: https://dl.acm.org/doi/10.1145/3360568
>         (Note that there was a small problem on the semantics of volatile. In particular, one should use either leading or trailing fence insertion scheme consistently for volatile reads and writes, instead of mixing them. We have fixed it but still in the process of publishing it) 
>
>         In our model, we did not use the happens-before approach anymore. Instead we formalized it in terms of visibility order. (Details can be found in the paper).
>
>         In general, opaque mode accesses do not preserve program orders for accesses to different locations. They do, however, follows the coherence rules. The release-acquire mode accesses preserves program orders but not necessarily global orders. This has to do with the non-MCA nature of the Power architecture that it compiles to. You might find it strange that there is no sw order for release-acquire mode in our model. This is because we simplified the cumulative effect of lwsync and hwsync using rf while not considering fr (we have proved the compilation is correct in our on-going paper) and x86 and ARMv8 are MCA. Finally, there is a total order among volatile accesses when they carry out “push” orders emulating the effect of full fences. 
>
>         The formal definition of data race is defined in terms of sw order though: a pair of accesses is said to form a race if they are 1) conflicting, and 2) not ordered by happens-before. We use the conventional definition for happens before, which is (po | sw)+ (the transitive closure of the union of program order and synchronizes-with), where sw is defined as the reads-from order from a release write to an acquire read. 
>
>         One last thing, in our formal model, opaque reads preserves the local program order. But this is purely a work-around to prevent out-of-thin-air results. As a consequence, this requires the compiler to yield a “fake” dependency after each read instruction, which is not true in practice. 
>
>         Hope this helps!
>
>         Best Regards,
>         Shuyang
>
>>         On Aug 12, 2021, at 1:18 AM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>>
>>         ﻿
>>         Hi Alex,
>>
>>         Thanks for your answer. That sounds like a very sensible approach.
>>
>>         I need to think about this.
>>
>>         Regards,
>>
>>         Peter.
>>
>>         On Thu, Aug 12, 2021 at 11:14 AM Alex Otenko <oleksandr.otenko@gmail.com> wrote:
>>
>>             I treat opaque read/write as part of SO, but which do not introduce SW edges - no transitive closure of program orders. They observe each other, because SO specifies who is before who.
>>
>>             Then acquire/ release introduce corresponding parts of transitive closure. In the end volatile load/store are just that.
>>
>>             Alex
>>
>>             On Thu, 12 Aug 2021, 08:52 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:
>>
>>                 The happens-before (HB) order is defined using:
>>
>>                 Synchronization order (SO): total order over all synchronization actions.
>>
>>                 Synchronizes with order (SW): a sub order of the SO that only orders e.g. a volatile write of X with all subsequent volatile reads of X.
>>
>>                 Program Order (PO): a partial order that  orders all memory actions issued by a single CPU.
>>
>>                 And the HB relation is defined as the transitive closure of the union of the SW and PO.
>>
>>                 My question is how do relaxed access modes like opaque and acquire/release fit into the HB?
>>
>>                 Let's start with opaque; is an opaque write/read part of the SO? If so, then it will be part of the SW and HB. And because of this, it will order loads/stores to different addresses which is not desirable. So I guess the logical solution would be that an opaque read/write is not part of the SO and hence we don't get this problem.  However now we have the problem that an opaque read/write is not ordered by the HB and we have a data race (read will still be hb-consistent).
>>
>>                 I'm running into a similar problem with the acquire/release. Traditionally they are called synchronization actions since a release-store will prevent any older load/store to be reordered with the release-store and acquire-load will prevent any later load/store to be reordered with the acquire-load. So they provide some level of  'synchronization'; but is this sufficient for them to be part of the SO order? Or are they excluded from the SO and we end up with a data-race?
>>
>>                 Or could it be that the happens-before model isn't a suitable model to deal with relaxed access modes?
>>
>>                 Regards,
>>
>>                 Peter.
>>
>>
>>
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest@cs.oswego.edu
>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest@cs.oswego.edu
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: [concurrency-interest] Opaque: coherence partial order.
From: Peter Veentjer via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-16, 08:39
To: "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu>
Reply-To: Peter Veentjer <alarmnummer at gmail.com>

Hi,

One of the many definitions of coherence is the following:
1) a total order of all loads/stores per address.
2) a load needs to see the most recent store before it in the coherence order.
3) the coherence order should be compatible with the (preserved) program order of each thread.

There are important relaxations:

1) The actual order of loads and stores isn't relevant, it is all about keeping up appearances. So in theory the above requirements can be violated, as long as nobody can prove it happened. This goes for many other memory models like sequential consistency or TSO.

2) Coherence order doesn't imply any real time guarantees, reads and writes can be skewed similar as with sequential consistency. So a read doesn't need to see the most recent write before it in physical clock terms; it just needs to return the most recent write before it in the coherence order.

What i understand from the  information about opaque:
http://gee.cs.oswego.edu/dl/html/j9mm.html
is that opaque doesn't provide a total order on a single address. So its usage of coherence is slightly different from the general notion of coherence.

Given the fact that coherence to some extent is quite relaxed, what is the reason that it wasn't specified in terms of a total order? Is it possible, in theory, to observe a violation of the total order per address when opaque is used?

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Opaque: coherence partial order.
From: Shuyang Liu via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-16, 08:55
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest@cs.oswego.edu
Reply-To: Shuyang Liu <sliu44 at cs.ucla.edu>

Hello,

Coherence is the union of (total or partial) orders among writes to the same location. My understanding is that, there is no observable differences between total or partial coherence (I think that was one of the results proved in the OOPSLA’19 paper). But the model uses partial coherence order so that it doesn’t have to be enforced as a total order. 

Best Regards,
Shuyang

> On Aug 15, 2021, at 10:40 PM, Peter Veentjer via Concurrency-interest <concurrency-interest@cs.oswego.edu> wrote:
>
> ﻿
> Hi,
>
> One of the many definitions of coherence is the following:
> 1) a total order of all loads/stores per address.
> 2) a load needs to see the most recent store before it in the coherence order.
> 3) the coherence order should be compatible with the (preserved) program order of each thread.
>
> There are important relaxations:
>
> 1) The actual order of loads and stores isn't relevant, it is all about keeping up appearances. So in theory the above requirements can be violated, as long as nobody can prove it happened. This goes for many other memory models like sequential consistency or TSO.
>
> 2) Coherence order doesn't imply any real time guarantees, reads and writes can be skewed similar as with sequential consistency. So a read doesn't need to see the most recent write before it in physical clock terms; it just needs to return the most recent write before it in the coherence order.
>
> What i understand from the  information about opaque:
> http://gee.cs.oswego.edu/dl/html/j9mm.html
> is that opaque doesn't provide a total order on a single address. So its usage of coherence is slightly different from the general notion of coherence.
>
> Given the fact that coherence to some extent is quite relaxed, what is the reason that it wasn't specified in terms of a total order? Is it possible, in theory, to observe a violation of the total order per address when opaque is used?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------------------

Subject: Re: [concurrency-interest] Opaque: coherence partial order.
From: Alex Otenko via Concurrency-interest <concurrency-interest at cs.oswego.edu>
Date: 2021-08-16, 10:04
To: Peter Veentjer <alarmnummer at gmail.com>
CC: concurrency-interest <concurrency-interest at cs.oswego.edu>
Reply-To: Alex Otenko <oleksandr.otenko at gmail.com>

How can you even define "the most recent write" on a partial order?

Alex

On Mon, 16 Aug 2021, 06:41 Peter Veentjer via Concurrency-interest, <concurrency-interest@cs.oswego.edu> wrote:

    Hi,

    One of the many definitions of coherence is the following:
    1) a total order of all loads/stores per address.
    2) a load needs to see the most recent store before it in the coherence order.
    3) the coherence order should be compatible with the (preserved) program order of each thread.

    There are important relaxations:

    1) The actual order of loads and stores isn't relevant, it is all about keeping up appearances. So in theory the above requirements can be violated, as long as nobody can prove it happened. This goes for many other memory models like sequential consistency or TSO.

    2) Coherence order doesn't imply any real time guarantees, reads and writes can be skewed similar as with sequential consistency. So a read doesn't need to see the most recent write before it in physical clock terms; it just needs to return the most recent write before it in the coherence order.

    What i understand from the  information about opaque:
    http://gee.cs.oswego.edu/dl/html/j9mm.html
    is that opaque doesn't provide a total order on a single address. So its usage of coherence is slightly different from the general notion of coherence.

    Given the fact that coherence to some extent is quite relaxed, what is the reason that it wasn't specified in terms of a total order? Is it possible, in theory, to observe a violation of the total order per address when opaque is used?
    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest@cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest@cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
