From concurrency-interest at stefan-marr.de  Sun Jul  1 12:34:55 2012
From: concurrency-interest at stefan-marr.de (Stefan Marr)
Date: Sun, 1 Jul 2012 18:34:55 +0200
Subject: [concurrency-interest] RACES'12: Relaxing Synchronization for
	Multicore and Manycore Scalability, SPLASH'12 Workshop
Message-ID: <07131472-74F3-44DD-B7AE-A60AF8B7C3EE@stefan-marr.de>

==============================================================================
                            Call for Participation


                             R A C E S    2 0 1 2

       Relaxing Synchronization for Multicore and Manycore Scalability

              Workshop Co-located with SPLASH in Tucson, Arizona
                              Sunday, October 21

                    Submission deadline: Monday, August 6 
                         http://soft.vub.ac.be/races/

==============================================================================



Massively-parallel systems are coming: core counts keep rising - whether
conventional cores as in multicore and manycore systems, or specialized cores
as in GPUs. Conventional wisdom has been to utilize this parallelism by
reducing synchronization to the minimum required to preserve determinism - in
particular, by eliminating data races. However, Amdahl's law implies that on
highly-parallel systems even a small amount of synchronization that introduces
serialization will limit scaling. Thus, we are forced to confront the
trade-off between synchronization and the ability of an implementation to
scale performance with the number of processors: synchronization inherently
limits parallelism. This workshop focuses on harnessing parallelism by
limiting synchronization, even to the point where programs will compute
inconsistent or approximate rather than exact answers.


Organizers:

Andrew P. Black, Portland State University
Theo D'Hondt, Vrije Universiteit Brussel
Doug Kimelman, IBM Thomas J. Watson Research Center
Martin Rinard, MIT CSAIL
David Ungar, IBM Thomas J. Watson Research Center


Theme and Topics
----------------

A new school of thought is arising: one that accepts and even embraces
nondeterminism (including data races), and in return is able to dramatically
reduce synchronization, or even eliminate it completely. However, this
approach requires that we leave the realm of the certain and enter the realm
of the merely probable. How can we cast aside the security of correctness, the
logic of a proof, and adopt a new way of thinking, where answers are good
enough but not certain, and where many processors work together in parallel
without quite knowing the states that the others are in? We may need some
amount of synchronization, but how much? Or better yet, how little? What
mental tools and linguistic devices can we give programmers to help them adapt
to this challenge? This workshop focuses on these questions and related ones:
harnessing parallelism by limiting synchronization, even to the point where
programs will compute inconsistent or approximate rather than exact answers.


This workshop aims to bring together researchers who, in the quest for
scalability, have been exploring the limits of how much synchronization can be
avoided. We invite submissions on any topic related to the theme of the
workshop, pro or con. We want to hear from those who have experimented with
formalisms, algorithms, data structures, programming languages, and mental
models that push the limits. In addition, we hope to hear from a few voices
with wilder ideas: those who may not have reduced their notions to practice
yet, but who have thoughts that can inspire us as we head towards this
yet-uncertain future. For example, biology may yield fruitful insights. The
ideal presentation for this workshop will focus on a grand idea, but will be
backed by some experimental result.


Submission
----------

Authors are invited to submit short position papers, technical papers, or
experience reports. Submissions may range from a single paragraph to as long
as desired, but the committee can only commit to reading one full page.
Nonetheless, we expect that in many cases reviewers will read farther than
that. Submissions should be formatted according to the ACM SIG Proceedings
style at http://www.acm.org/sigs/publications/proceedings-templates and should
be submitted via EasyChair at
http://www.easychair.org/conferences/?conf=races2012 in PDF format.


PLEASE NOTE: All submissions (except for those retracted by their authors)
will be posted on the workshop website, along with reviews, which will be
signed by the reviewers, and a rating assigned by the program committee.
Further, the submissions to be presented at the workshop will be selected by a
vote of all registered attendees. As well, submissions to be published in an
official proceedings will be selected by the program committee. Please see the
sections below concerning the rationale and details for this process.


Program Committee
-----------------

Andrew P. Black, Portland State University
Yvonne Coady, University of Victoria
Tom Van Cutsem, Vrije Universiteit Brussel
Theo D'Hondt, Vrije Universiteit Brussel
Phil Howard, Portland State University
Doug Kimelman, IBM Thomas J. Watson Research Center
Eddie Kohler, Harvard SEAS
Jim Larus, Microsoft Research
Stefan Marr, Vrije Universiteit Brussel
Tim Mattson, Intel
Paul McKenney, IBM
Hannes Payer, University of Salzburg
Dan Prenner, IBM
Lakshmi Renganarayana, IBM
David Ungar, IBM Thomas J. Watson Research Center

- TBC -


Important Dates
---------------

August     6     Submission deadline.
August    29     Reviews sent to authors.
September  3     Last date for retraction by authors.
September  4     Papers, reviews, ratings posted on web site. Voting opens.
September 11     Voting closes.
September 14     Notification of papers accepted for presentation and/or publication.
August    21     SPLASH early registration deadline.
October   21     Workshop.
mid-November     Camera-ready copy due for papers selected for proceedings.


Goals and Outcomes
------------------

We will consider the workshop a success if attendees come away with new
insights into fundamental principles, and new ideas for algorithms, data
structures, programming languages, and mental models, leading to improving
scaling by limiting synchronization, even to the point where programs will
compute inconsistent or approximate rather than exact answers. The goal of
this workshop is both to influence current programming practice and to
initiate the coalescence of a new research community giving rise to a new
subfield within the general area of concurrent and parallel programming.
Results generated by the workshop will be made persistent via the workshop
website and possibly via the ACM Digital Library.



  The RACES 2012 Review Process and Workshop Presentation Selection Process
  =========================================================================
  
                                 David Ungar
                     IBM Thomas J. Watson Research Center
                     PC Chair for Workshop  Presentations

Technology has changed the economic tradeoffs that once shaped the reviewing
process. It has become cheap and easy to share submissions, reviews and the
preferences of the attendees. What remains scarce is the number of hours in a
day, and as a consequence the time we have in our workshop in which to learn
and share with each other. I believe that this change in the balance of
factors affords us the opportunity to significantly improve the review and
selection processes.


Sadly, all too often, those who spend their precious time attending a workshop
are not served as well they could be with respect to enlightenment, thought
provoking discussions, and being challenged by new ideas. The fault lies not
in the people who generously donate their time to serve on program committees
and do external reviews. Rather, the fault lies in the process itself. The
very notion of acceptance by committee forces us to boil a rich stew of
reactions, insights, and opinions, down to a single carrot. As a result, it is
common for PC members to come away from a meeting feeling that either some
fraud will be perpetrated on the audience by a fundamentally flawed paper, or,
more often, feeling that a sin of omission will be committed on the audience
by the suppression of a significant but controversial new idea. Sometimes
instead of a carrot we get a lump of gristle.

There are other, lesser, flaws in this process. Although reviewer anonymity
protects negative reviewers from resentment and reprisal, all too often it
prevents an open debate that would promote mutual understanding. Further, in
some cases anonymity allows a reviewer to cast aspersions on authors without
being accountable. Finally, we fail to take maximal advantage of the time and
effort spent in creating insightful reviews when we withhold them from the
audience. Attendees and readers could benefit from expert reactions as they
try to glean the wisdom embedded in the authors' papers.

In this workshop, we have an opportunity to try a different process, one that
we hope will serve all parties better: All reviews will be signed, all
submissions and reviews will be posted on the web (unless an author chooses to
retract a submission), and the attendees will be the ones selecting which
papers will be presented.

Here are the details:
---------------------

At least three committee members will review each submission, and each review
will be signed. Once all the reviews for a submission are in, they will be
sent to the author, who can decide to retract the paper if so desired. Then,
all submissions (except any that are retracted) will be posted on the workshop
website, along with all reviews and a net score determined for each submission
by the program committee.

At this point, prior to the workshop, all registered attendees will be invited
to read the submissions and the reviews, and vote on which of the papers they
want to see presented. Of course, an attendee who so wishes will be free to
merely vote according to the recommendation of the PC, or to not vote and to
accept the wisdom of the rest of the attendees. But the important point
remains: it will be those who will be spending the time in the room who get to
decide how that time is spent. Please note that a submission being posted on
the workshop website and/or presented at the workshop are not intended to
constitute prior publication for purposes of publishing in other workshops,
major conferences, or journals.

This process is a grand experiment, designed to exploit the technologies we
Computer Scientists have created, in order to better serve the advancement of
Computer Science. We hope that its potential excites you as much as it excites
us!



         The RACES 2012 Published Proceedings Paper Selection Process
         ============================================================

                                 Theo D'Hondt
                          Vrije Universiteit Brussel
                        PC Chair for Proceedings Papers

We understand that many submitters may want to publish their paper in an
official proceedings in addition to having it posted on the workshop website.
In order to satisfy that desire, we will publish a proceedings via the ACM
Digital Library. To satisfy ACM DL selectivity requirements, a separate and
more conventional process will be employed for selecting papers to be included
in the published proceedings: Even though all submissions will be posted on
the workshop website (unless retracted by the author), the program committee
will select a smaller number of papers to be included in the published
proceedings based on the signed and posted reviews. Authors of the selected
papers will be asked to submit revised and extended papers mid-November,
taking into account the reviews and the publisher's guidelines. Page limits
for the revised and extended papers to be included in the published
proceedings are anticipated to be 10 pages for research papers, and 5 pages
for position papers. Please note that inclusion in the ACM Digital Library
published proceedings may well be considered to be a prior publication for
purposes of publication in other workshops, major conferences, or journals.
For that reason, authors may choose to decline to have their submission
included in the published proceedings, even if it was presented at the
workshop.



              For questions please contact: races at soft.vub.ac.be
              For updates, follow us on Twitter: @races_workshop


==============================================================================



From jeremy.manson at gmail.com  Sun Jul  1 14:29:32 2012
From: jeremy.manson at gmail.com (Jeremy Manson)
Date: Sun, 1 Jul 2012 11:29:32 -0700
Subject: [concurrency-interest] [Javamemorymodel-discussion] is my
 counter thread safe?
In-Reply-To: <CAChcVumKSCb1wLf7n7ApQQYKrVHiqr7vU=m1vM_ru=LMgZNb3g@mail.gmail.com>
References: <CAFAd71W3Ct6s28wZhR2E4PK6oKWj9zYrQoeW0ZQVr=L2_YH0qg@mail.gmail.com>
	<CAChcVumPoPyQToXmWFQO7YS1rmTyqk9xEdT=vN6nKAKiSY9RpA@mail.gmail.com>
	<CAFAd71XiBA5qZCpkO8u--Ay_s1Sn+ffkHowmw8jUrBrpAgL1uQ@mail.gmail.com>
	<CAChcVumKSCb1wLf7n7ApQQYKrVHiqr7vU=m1vM_ru=LMgZNb3g@mail.gmail.com>
Message-ID: <CAHF5=kQv4ir1hNFhio0ajGQUfem8AKPA55v9sYfw6oyECofi2g@mail.gmail.com>

The counter field should be final, because another thread could read
the enclosing object before it is finished being constructed (unless
youve taken other precautions to prevent that.

Jeremy

On Fri, Jun 29, 2012 at 4:24 AM, Pavel Rappo <pavel.rappo at gmail.com> wrote:
> 1. I can't see any obvious flaws in it.
> 2. http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/package-summary.html
> (see "Memory Consistency Properties")
>
> On 6/29/12, Li Li <fancyerii at gmail.com> wrote:
>> do you mean my counter is safe?
>> putIfAbsent has a happen-before semantic? any document about this? Or
>> only current implementation guarantee this?
>>
>> On Fri, Jun 29, 2012 at 6:54 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:
>>> Hi,
>>>
>>> This 'counter.get(key)' cannot simply return "not well constructed"
>>> object. The reason is that you use 'putIfAbsent' to put it in the map.
>>> Either 'get' will return 'null' or it will return perfectly valid
>>> AtomicLong object. There's a happen-before edge between these two
>>> actions.
>>>
>>> On 6/29/12, Li Li <fancyerii at gmail.com> wrote:
>>>> hi all
>>>>    I have read
>>>> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>>>> and know a little about Java memory model.
>>>>    I want to implement a thread safe and efficient Map<String,int>
>>>> counter. I found a related question in stackoverflow
>>>> http://stackoverflow.com/questions/8477352/global-in-memory-counter-that-is-thread-safe-and-flushes-to-mysql-every-x-increm
>>>>    my implementation is :
>>>>
>>>>         private ConcurrentHashMap<String, AtomicLong> counter=new
>>>> ConcurrentHashMap<String, AtomicLong>();
>>>>
>>>>       public void addCount(String key, long count){
>>>>               if(count<=0) return;
>>>>               AtomicLong current = counter.get(key);
>>>>                 if(current == null) {
>>>>                   current=counter.putIfAbsent(key, new AtomicLong());
>>>>                   if(current == null) current=counter.get(key);
>>>>                 }
>>>>
>>>>                 assert current!=null;
>>>>                 current.addAndGet(count);
>>>>
>>>>       }
>>>>    but after I reading
>>>> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>>>> . it seems there exists a state that may fail.
>>>>    if thread1 call addCount("key",2); and thread2 call
>>>> addCount("key",1); at the same time.
>>>>    thread1 executes AtomicLong current = counter.get(key);  and gets
>>>> null.
>>>>    then thread 1 execute
>>>>           if(current == null) {
>>>>                   current=counter.putIfAbsent(key, new AtomicLong());
>>>>    as compilers/cpus may disorder, new AtomicLong() will not be null
>>>> but may be well constructed.
>>>>    Then thread 2 call AtomicLong current = counter.get(key); it's not
>>>> null but not well constructed. then it call current.addAndGet().
>>>>     Will thread 2 crash if current is not well constructed?
>>>>
>>>>     I also find similar implementation in a popular lib -- guava
>>>> https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/util/concurrent/AtomicLongMap.java?name=v11.0-rc1
>>>>     it may also fail like my codes.
>>>>     e.g. thread 1 call atomic = map.putIfAbsent(key, new
>>>> AtomicLong(delta));
>>>>            thread 2 get a not null atomic and call it's get();
>>>>
>>>> public long addAndGet(K key, long delta) {
>>>>     outer: for (;;) {
>>>>       AtomicLong atomic = map.get(key);
>>>>       if (atomic == null) {
>>>>         atomic = map.putIfAbsent(key, new AtomicLong(delta));
>>>>         if (atomic == null) {
>>>>           return delta;
>>>>         }
>>>>         // atomic is now non-null; fall through
>>>>       }
>>>>
>>>>       for (;;) {
>>>>         long oldValue = atomic.get();
>>>>         if (oldValue == 0L) {
>>>>           // don't compareAndSet a zero
>>>>           if (map.replace(key, atomic, new AtomicLong(delta))) {
>>>>             return delta;
>>>>           }
>>>>           // atomic replaced
>>>>           continue outer;
>>>>         }
>>>>
>>>>         long newValue = oldValue + delta;
>>>>         if (atomic.compareAndSet(oldValue, newValue)) {
>>>>           return newValue;
>>>>         }
>>>>         // value changed
>>>>       }
>>>>     }
>>>>   }
>>>> _______________________________________________
>>>> Javamemorymodel-discussion mailing list
>>>> Javamemorymodel-discussion at cs.umd.edu
>>>> https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion
>>>>
>>>
>>>
>>> --
>>> Sincerely yours, Pavel Rappo.
>>
>
>
> --
> Sincerely yours, Pavel Rappo.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From vijay at saraswat.org  Sun Jul  1 15:23:19 2012
From: vijay at saraswat.org (Vijay Saraswat)
Date: Sun, 01 Jul 2012 15:23:19 -0400
Subject: [concurrency-interest] [Javamemorymodel-discussion] is my
 counter thread safe?
In-Reply-To: <CAHF5=kQv4ir1hNFhio0ajGQUfem8AKPA55v9sYfw6oyECofi2g@mail.gmail.com>
References: <CAFAd71W3Ct6s28wZhR2E4PK6oKWj9zYrQoeW0ZQVr=L2_YH0qg@mail.gmail.com>
	<CAChcVumPoPyQToXmWFQO7YS1rmTyqk9xEdT=vN6nKAKiSY9RpA@mail.gmail.com>
	<CAFAd71XiBA5qZCpkO8u--Ay_s1Sn+ffkHowmw8jUrBrpAgL1uQ@mail.gmail.com>
	<CAChcVumKSCb1wLf7n7ApQQYKrVHiqr7vU=m1vM_ru=LMgZNb3g@mail.gmail.com>
	<CAHF5=kQv4ir1hNFhio0ajGQUfem8AKPA55v9sYfw6oyECofi2g@mail.gmail.com>
Message-ID: <4FF0A3A7.6070401@saraswat.org>

such as using the hardhat rules implemented in X10 see Object 
initialization paper in ECOOP 2012.



On 7/1/12 2:29 PM, Jeremy Manson wrote:
> The counter field should be final, because another thread could read
> the enclosing object before it is finished being constructed (unless
> youve taken other precautions to prevent that.
>
> Jeremy
>
> On Fri, Jun 29, 2012 at 4:24 AM, Pavel Rappo <pavel.rappo at gmail.com> wrote:
>> 1. I can't see any obvious flaws in it.
>> 2. http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/package-summary.html
>> (see "Memory Consistency Properties")
>>
>> On 6/29/12, Li Li <fancyerii at gmail.com> wrote:
>>> do you mean my counter is safe?
>>> putIfAbsent has a happen-before semantic? any document about this? Or
>>> only current implementation guarantee this?
>>>
>>> On Fri, Jun 29, 2012 at 6:54 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:
>>>> Hi,
>>>>
>>>> This 'counter.get(key)' cannot simply return "not well constructed"
>>>> object. The reason is that you use 'putIfAbsent' to put it in the map.
>>>> Either 'get' will return 'null' or it will return perfectly valid
>>>> AtomicLong object. There's a happen-before edge between these two
>>>> actions.
>>>>
>>>> On 6/29/12, Li Li <fancyerii at gmail.com> wrote:
>>>>> hi all
>>>>>     I have read
>>>>> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>>>>> and know a little about Java memory model.
>>>>>     I want to implement a thread safe and efficient Map<String,int>
>>>>> counter. I found a related question in stackoverflow
>>>>> http://stackoverflow.com/questions/8477352/global-in-memory-counter-that-is-thread-safe-and-flushes-to-mysql-every-x-increm
>>>>>     my implementation is :
>>>>>
>>>>>          private ConcurrentHashMap<String, AtomicLong> counter=new
>>>>> ConcurrentHashMap<String, AtomicLong>();
>>>>>
>>>>>        public void addCount(String key, long count){
>>>>>                if(count<=0) return;
>>>>>                AtomicLong current = counter.get(key);
>>>>>                  if(current == null) {
>>>>>                    current=counter.putIfAbsent(key, new AtomicLong());
>>>>>                    if(current == null) current=counter.get(key);
>>>>>                  }
>>>>>
>>>>>                  assert current!=null;
>>>>>                  current.addAndGet(count);
>>>>>
>>>>>        }
>>>>>     but after I reading
>>>>> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>>>>> . it seems there exists a state that may fail.
>>>>>     if thread1 call addCount("key",2); and thread2 call
>>>>> addCount("key",1); at the same time.
>>>>>     thread1 executes AtomicLong current = counter.get(key);  and gets
>>>>> null.
>>>>>     then thread 1 execute
>>>>>            if(current == null) {
>>>>>                    current=counter.putIfAbsent(key, new AtomicLong());
>>>>>     as compilers/cpus may disorder, new AtomicLong() will not be null
>>>>> but may be well constructed.
>>>>>     Then thread 2 call AtomicLong current = counter.get(key); it's not
>>>>> null but not well constructed. then it call current.addAndGet().
>>>>>      Will thread 2 crash if current is not well constructed?
>>>>>
>>>>>      I also find similar implementation in a popular lib -- guava
>>>>> https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/util/concurrent/AtomicLongMap.java?name=v11.0-rc1
>>>>>      it may also fail like my codes.
>>>>>      e.g. thread 1 call atomic = map.putIfAbsent(key, new
>>>>> AtomicLong(delta));
>>>>>             thread 2 get a not null atomic and call it's get();
>>>>>
>>>>> public long addAndGet(K key, long delta) {
>>>>>      outer: for (;;) {
>>>>>        AtomicLong atomic = map.get(key);
>>>>>        if (atomic == null) {
>>>>>          atomic = map.putIfAbsent(key, new AtomicLong(delta));
>>>>>          if (atomic == null) {
>>>>>            return delta;
>>>>>          }
>>>>>          // atomic is now non-null; fall through
>>>>>        }
>>>>>
>>>>>        for (;;) {
>>>>>          long oldValue = atomic.get();
>>>>>          if (oldValue == 0L) {
>>>>>            // don't compareAndSet a zero
>>>>>            if (map.replace(key, atomic, new AtomicLong(delta))) {
>>>>>              return delta;
>>>>>            }
>>>>>            // atomic replaced
>>>>>            continue outer;
>>>>>          }
>>>>>
>>>>>          long newValue = oldValue + delta;
>>>>>          if (atomic.compareAndSet(oldValue, newValue)) {
>>>>>            return newValue;
>>>>>          }
>>>>>          // value changed
>>>>>        }
>>>>>      }
>>>>>    }
>>>>> _______________________________________________
>>>>> Javamemorymodel-discussion mailing list
>>>>> Javamemorymodel-discussion at cs.umd.edu
>>>>> https://mailman.cs.umd.edu/mailman/listinfo/javamemorymodel-discussion
>>>>>
>>>>
>>>> --
>>>> Sincerely yours, Pavel Rappo.
>>
>> --
>> Sincerely yours, Pavel Rappo.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



From dl at cs.oswego.edu  Tue Jul  3 19:47:51 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 03 Jul 2012 19:47:51 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports Spliterator
Message-ID: <4FF384A7.1020807@cs.oswego.edu>


In addition to including the compute/computeIfAbsent changes
discussed on this list last week, the updated version of
ConcurrentHashMapV8 contains basic support for performing
parallel (normally via ForkJoin) operations on the keys,
values, or entries of a map.

The added Spliterator interface (see
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/jsr166e/ConcurrentHashMapV8.Spliterator.html)
is for now a nested interface that contains a subset of
some functionality likely to be available in JDK8.
There are also three new methods to provide them:
   public Spliterator<K> keySpliterator()
   public Spliterator<V> valueSpliterator()
   public Spliterator<Map.Entry<K,V>> entrySpliterator()
See the Spliterator javadocs for a usage example.

These methods only provide a foothold for writing your
own parallel operations. CHMV8 doesn't now itself supply
the map, filter, reduce, forEach, etc operations expected
to be supported under JDK8 lambda-ized collections. However,
they do enable those familiar with ForkJoin programming
to get a head start evaluating such code you write yourself.
If you do this, please report back any interesting experiences.

The usual links:
     *  API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
     * jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled 
using Java7 javac).
     * Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/

-Doug


From aleksandar.prokopec at gmail.com  Wed Jul  4 04:20:17 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 04 Jul 2012 09:20:17 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports
	Spliterator
In-Reply-To: <4FF384A7.1020807@cs.oswego.edu>
References: <4FF384A7.1020807@cs.oswego.edu>
Message-ID: <2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>

Wow, this is great news!
It may allow us to write a wrapper for chmv8 in Scala parallel collections. I wish concurrent skip lists had a similar thing.

Cheers,
Alex
-- 
Aleksandar Prokopec
LAMP, IC, EPFL

Sent from my Android phone with K-9 Mail. Please excuse my brevity.

Doug Lea <dl at cs.oswego.edu> wrote:


In addition to including the compute/computeIfAbsent changes
discussed on this list last week, the updated version of
ConcurrentHashMapV8 contains basic support for performing
parallel (normally via ForkJoin) operations on the keys,
values, or entries of a map.

The added Spliterator interface (see
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/jsr166e/ConcurrentHashMapV8.Spliterator.html)
is for now a nested interface that contains a subset of
some functionality likely to be available in JDK8.
There are also three new methods to provide them:
public Spliterator<K> keySpliterator()
public Spliterator<V> valueSpliterator()
public Spliterator<Map.Entry<K,V>> entrySpliterator()
See the Spliterator javadocs for a usage example.

These methods only provide a foothold for writing your
own parallel operations. CHMV8 doesn't now itself supply
the map, filter, reduce, forEach, etc operations expected
to be supported under JDK8 lambda-ized collections. However,
they do enable those familiar with ForkJoin programming
to get a head start evaluating such code you write yourself.
If you do this, please report back any interesting experiences.

The usual links:
* API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/
* jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166e.jar (compiled 
using Java7 javac).
* Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/

-Doug

_____________________________________________

Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120704/2776d3f8/attachment.html>

From dl at cs.oswego.edu  Wed Jul  4 07:29:26 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 04 Jul 2012 07:29:26 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports
	Spliterator
In-Reply-To: <2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>
References: <4FF384A7.1020807@cs.oswego.edu>
	<2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>
Message-ID: <4FF42916.4020806@cs.oswego.edu>

On 07/04/12 04:20, Aleksandar Prokopec wrote:
> Wow, this is great news!
> It may allow us to write a wrapper for chmv8 in Scala parallel collections. I
> wish concurrent skip lists had a similar thing.

Yes, adding the new functionality of ConcurrentHashMap to
ConcurrentSkipListMap is on the todo list. ConcurrentHashMap
will almost always be a lot faster for parallelism though.
Performing parallel operations on collections that maintain
ordering (which is basically ignored for purposes of parallelism)
is usually more costly than just using a hash table (or array)
and then sorting (in parallel) later when the ordering is needed.

-Doug


From alexlamsl at gmail.com  Wed Jul  4 09:27:02 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Wed, 4 Jul 2012 14:27:02 +0100
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports
	Spliterator
In-Reply-To: <4FF42916.4020806@cs.oswego.edu>
References: <4FF384A7.1020807@cs.oswego.edu>
	<2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>
	<4FF42916.4020806@cs.oswego.edu>
Message-ID: <CAGpACNtFOWR+DbMH_FgR3weS_jmhGrEQ1dDc=jeQjKN7szrcXw@mail.gmail.com>

On Wed, Jul 4, 2012 at 12:29 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 07/04/12 04:20, Aleksandar Prokopec wrote:
>>
>> Wow, this is great news!
>> It may allow us to write a wrapper for chmv8 in Scala parallel
>> collections. I
>> wish concurrent skip lists had a similar thing.
>
>
> Yes, adding the new functionality of ConcurrentHashMap to
> ConcurrentSkipListMap is on the todo list. ConcurrentHashMap
> will almost always be a lot faster for parallelism though.
> Performing parallel operations on collections that maintain
> ordering (which is basically ignored for purposes of parallelism)
> is usually more costly than just using a hash table (or array)
> and then sorting (in parallel) later when the ordering is needed.


That's good to know.

In the case of frequently performing:

1) subset (range) of an ordered map
2) perform parallel operation on subset
3) ordering of elements in subset does not matter

Today I just call ConcurrentSkipListMap.subMap() then perform the
operation. Would there be an alternative which might potentially be
more performant?



Alex.

From dl at cs.oswego.edu  Wed Jul  4 09:54:17 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 04 Jul 2012 09:54:17 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8 now
	supports	Spliterator
In-Reply-To: <CAGpACNtFOWR+DbMH_FgR3weS_jmhGrEQ1dDc=jeQjKN7szrcXw@mail.gmail.com>
References: <4FF384A7.1020807@cs.oswego.edu>	<2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>	<4FF42916.4020806@cs.oswego.edu>
	<CAGpACNtFOWR+DbMH_FgR3weS_jmhGrEQ1dDc=jeQjKN7szrcXw@mail.gmail.com>
Message-ID: <4FF44B09.9020805@cs.oswego.edu>

On 07/04/12 09:27, Alex Lam S.L. wrote:
>> Performing parallel operations on collections that maintain
>> ordering (which is basically ignored for purposes of parallelism)
>> is usually more costly than just using a hash table (or array)
>> and then sorting (in parallel) later when the ordering is needed.
>
> In the case of frequently performing:
>
> 1) subset (range) of an ordered map
> 2) perform parallel operation on subset
> 3) ordering of elements in subset does not matter
>
> Today I just call ConcurrentSkipListMap.subMap() then perform the
> operation. Would there be an alternative which might potentially be
> more performant?

Good question. This is the opposite of the case I was
alluding to (i.e., keep in CHM, sort only when needed,
vs keep in CSLM, perform parallel ops only when needed).
Considering that the extended version of ConcurrentSkipListMap
doesn't exist yet, it's too early to tell. You'd like to avoid
the sequential bottleneck and space overhead of copying a lot
of elements from one collection to another (although there is
some room for parallelism). It may be the case that next version
of ConcurrentSkipListMap will be able to internally streamline
the equivalent of cslm.subMap(...).forEach(...) etc.

-Doug

From aleksandar.prokopec at gmail.com  Wed Jul  4 12:40:43 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 04 Jul 2012 18:40:43 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports
	Spliterator
In-Reply-To: <4FF42916.4020806@cs.oswego.edu>
References: <4FF384A7.1020807@cs.oswego.edu>
	<2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>
	<4FF42916.4020806@cs.oswego.edu>
Message-ID: <4FF4720B.3040805@gmail.com>

Indeed - this is true. The cost of maintaining the order during `map`, 
`filter` and friends is high.

It's still good to hear that splitters will be included.
I was thinking of an application where you just do lookups in the 
concurrent linked lists most of the time, with occasional inserts and 
occasional parallel reduce methods (which do not construct a new linked 
list, but just traverse the existing one to obtain the reduction).

Alex


On 7/4/12 1:29 PM, Doug Lea wrote:
> On 07/04/12 04:20, Aleksandar Prokopec wrote:
>> Wow, this is great news!
>> It may allow us to write a wrapper for chmv8 in Scala parallel 
>> collections. I
>> wish concurrent skip lists had a similar thing.
>
> Yes, adding the new functionality of ConcurrentHashMap to
> ConcurrentSkipListMap is on the todo list. ConcurrentHashMap
> will almost always be a lot faster for parallelism though.
> Performing parallel operations on collections that maintain
> ordering (which is basically ignored for purposes of parallelism)
> is usually more costly than just using a hash table (or array)
> and then sorting (in parallel) later when the ordering is needed.
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From aleksandar.prokopec at gmail.com  Wed Jul  4 12:57:05 2012
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Wed, 04 Jul 2012 18:57:05 +0200
Subject: [concurrency-interest] ConcurrentHashMapV8 now supports
	Spliterator
In-Reply-To: <4FF4720B.3040805@gmail.com>
References: <4FF384A7.1020807@cs.oswego.edu>
	<2f2df2f0-1257-4141-be96-675b22163fb3@email.android.com>
	<4FF42916.4020806@cs.oswego.edu> <4FF4720B.3040805@gmail.com>
Message-ID: <4FF475E1.3000108@gmail.com>

On 7/4/12 6:40 PM, Aleksandar Prokopec wrote:
> Indeed - this is true. The cost of maintaining the order during `map`, 
> `filter` and friends is high.
>
> It's still good to hear that splitters will be included.
> I was thinking of an application where you just do lookups in the 
> concurrent linked lists most of the time, with occasional inserts and 
> occasional parallel reduce methods (which do not construct a new 
> linked list, but just traverse the existing one to obtain the reduction).
>
> Alex

I should have said range lookups concurrent skip lists, otherwise there 
is little sense in having a concurrent skip list.

From sergi at vladykin.com  Fri Jul 13 19:16:31 2012
From: sergi at vladykin.com (Sergi Vladykin)
Date: Sat, 14 Jul 2012 03:16:31 +0400
Subject: [concurrency-interest] Scaling ConcurrentNavigableMap
Message-ID: <CA+eZwrHa=Ge7pn7crN6uyrynEW0hL6OzwjdVDRERk65KOm28+w@mail.gmail.com>

Hi,

I'm working on component which is based on ConcurrentNavigableMap and
it has strong requirement to scale well on updates in multicore
environment. But I'm experiencing some problems with that since
ConcurrentSkipListMap or SnapTreeMap don't seem to be scalable enough
because in my simple test where I'm putting pregenerated random UUIDs
in multiple threads I can't reach full CPU utilization even on 8
cores. As far as I understand there are two main scalability problems
here, the first is threads contention and the second is memory
management/gc (although FullGC never happens in my test).
My first attempt to improve throughput was to reduce contention by
splitting map to multiple segments and choose which one to update
using key's hashCode, assuming that hash codes are uniformly
distributed, but it didn't help much. ConcurrentSkipListMap is
lock-free so I expect to have full CPU utilization when threads number
is greater then or equal to number of cores but it is not the case
too. So it seems to me that memory management is major bottleneck
here. Am I right? May be someone can point me other directions to
solution of this problem?
I should point right away that adding data to unsorted map and sort on
demand will not be a good idea since there can be millions of entries
and the sorting will take too long. Also I'm thinking about creating
some special array-based data structure which will not generate
additional garbage but it will probably have problems with insertion
to the middle.

Any thoughts and suggestions are very appreciated.

Thanks!
Sergi

From nathan.reynolds at oracle.com  Fri Jul 13 19:58:37 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 13 Jul 2012 16:58:37 -0700
Subject: [concurrency-interest] Scaling ConcurrentNavigableMap
In-Reply-To: <CA+eZwrHa=Ge7pn7crN6uyrynEW0hL6OzwjdVDRERk65KOm28+w@mail.gmail.com>
References: <CA+eZwrHa=Ge7pn7crN6uyrynEW0hL6OzwjdVDRERk65KOm28+w@mail.gmail.com>
Message-ID: <5000B62D.2040106@oracle.com>

Unless I missed something in ConcurrentSkipListMap's code, there isn't a 
synchronized or LockSupport.park().  So, it would seem there is no way 
for a thread to block.  If so, it suggests that the problem isn't in the 
data structure but in the code outside of the data structure.

  * Check GC logs
  * Try profiling with elapsed time measurement
  * Try taking stack traces (poor man's profiler) and see where the
    thread's are
  * Consider NUMA binding (e.g. taskset, numactl) might prevent threads
    from running on the other cores

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/13/2012 4:16 PM, Sergi Vladykin wrote:
> Hi,
>
> I'm working on component which is based on ConcurrentNavigableMap and
> it has strong requirement to scale well on updates in multicore
> environment. But I'm experiencing some problems with that since
> ConcurrentSkipListMap or SnapTreeMap don't seem to be scalable enough
> because in my simple test where I'm putting pregenerated random UUIDs
> in multiple threads I can't reach full CPU utilization even on 8
> cores. As far as I understand there are two main scalability problems
> here, the first is threads contention and the second is memory
> management/gc (although FullGC never happens in my test).
> My first attempt to improve throughput was to reduce contention by
> splitting map to multiple segments and choose which one to update
> using key's hashCode, assuming that hash codes are uniformly
> distributed, but it didn't help much. ConcurrentSkipListMap is
> lock-free so I expect to have full CPU utilization when threads number
> is greater then or equal to number of cores but it is not the case
> too. So it seems to me that memory management is major bottleneck
> here. Am I right? May be someone can point me other directions to
> solution of this problem?
> I should point right away that adding data to unsorted map and sort on
> demand will not be a good idea since there can be millions of entries
> and the sorting will take too long. Also I'm thinking about creating
> some special array-based data structure which will not generate
> additional garbage but it will probably have problems with insertion
> to the middle.
>
> Any thoughts and suggestions are very appreciated.
>
> Thanks!
> Sergi
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120713/a6c5e992/attachment.html>

From david.lloyd at redhat.com  Sat Jul 14 14:01:59 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Sat, 14 Jul 2012 13:01:59 -0500
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
Message-ID: <5001B417.1050405@redhat.com>

What is the purpose of the access-time access check in the atomic field 
updater classes?

If I construct an atomic field updater, to me, this should be exactly 
equivalent to having an accessible=true Field instance.  If I want to 
manage the visibility of that instance, I will restrict access to the 
updater itself.

The problem, as I am sure you are all well aware, is that when the 
updater classes are used on an object instance which is subclassed, even 
if the field being updated, the field updater, and the method calling 
into the field updater all exist on the base class and are private, an 
expensive access check has to occur.

Even when the class is final, the access check is still expensive 
relative to the operation itself!

I mean I'm using field updaters in the first place because I can't 
afford to have a jillion Atomic* objects floating around; obviously 
performance is critical here.  And to add to this problem, you can't 
even rely on using Unsafe, even though it's becoming more and more 
prevalent in JDKs, as some platforms may not have atomic 64-bit 
operations, and there's no way I can see to test for that other than 
relying on the AtomicLong implementation to be similar to OpenJDK's.

Is there any recourse to solve this issue?
-- 
- DML

From dl at cs.oswego.edu  Sat Jul 14 14:18:04 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 14 Jul 2012 14:18:04 -0400
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5001B417.1050405@redhat.com>
References: <5001B417.1050405@redhat.com>
Message-ID: <5001B7DC.4050405@cs.oswego.edu>

On 07/14/12 14:01, David M. Lloyd wrote:
> What is the purpose of the access-time access check in the atomic field updater
> classes?

It is because there is no way to check that you haven't handed
your Updater to some untrusted party, so the caller context must
be checked on each use. I agree it is very annoying and slow.

>
> I mean I'm using field updaters in the first place because I can't afford to
> have a jillion Atomic* objects floating around; obviously performance is
> critical here. And to add to this problem, you can't even rely on using Unsafe,
> even though it's becoming more and more prevalent in JDKs, as some platforms may
> not have atomic 64-bit operations, and there's no way I can see to test for that
> other than relying on the AtomicLong implementation to be similar to OpenJDK's.
>

We stopped using Updaters entirely inside j.u.c as of JDK7 and
use straight Unsafe calls without even checking for 64bit atomics.
Which means that we are relying on every JDK7+ VM to somehow
implement the 64bit version of Unsafe CAS. So if you are targeting JDK7+
only, you might take some comfort that if j.u.c cannot run, then it
probably doesn't matter if your code runs, and so don't worry about
the checks :-)

> Is there any recourse to solve this issue?

Maybe someday. Remi Forax has mentioned a few times that JDK8 method
handle and invokeDynamic support should make this or some variant
API a lot faster. We'll see...

-Doug



From stanimir at riflexo.com  Sat Jul 14 16:01:59 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 14 Jul 2012 23:01:59 +0300
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5001B7DC.4050405@cs.oswego.edu>
References: <5001B417.1050405@redhat.com>
	<5001B7DC.4050405@cs.oswego.edu>
Message-ID: <CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>

On Sat, Jul 14, 2012 at 9:18 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 07/14/12 14:01, David M. Lloyd wrote:
>
>> What is the purpose of the access-time access check in the atomic field
>> updater
>> classes?
>>
>
> It is because there is no way to check that you haven't handed
> your Updater to some untrusted party, so the caller context must
> be checked on each use. I agree it is very annoying and slow.
>

But how that's a problem - you create, you hand it over, exactly as adding
an extra method public. I can't even see how that can be an argument. The
checks should be during creation only, not during  usage time - the calling
class is trivial to infer. I wanted to voice that for very long time and
used to always used to forget.
Alternatively the SecurityManager should be checked and if not present a
non-checking sub-class shall be returned. W/o SecurityManager it's possible
to create modifiable java.lang.String not even touching
Field/Method.setAccessible or Unsafe.
That option doesn't require API changes. Since there would be a single
subclass only instantiated in the JVM there would be no performance loss
from multi-site invocations.

Exactly that reason [pointless checks] has forced quite a few developers
(and libs) going straight to sun.misc.Unsafe... or extend the AtomicXXX
instead java.lang.Object (that at least is portable).


Kind Regards
Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120714/0a32d507/attachment.html>

From viktor.klang at gmail.com  Sat Jul 14 16:16:10 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sat, 14 Jul 2012 22:16:10 +0200
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
References: <5001B417.1050405@redhat.com> <5001B7DC.4050405@cs.oswego.edu>
	<CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
Message-ID: <CANPzfU9nVBy9Fmfk4MuxGJTRz8t5skQHNfnpoR_T6O2WTVyuVw@mail.gmail.com>

On Sat, Jul 14, 2012 at 10:01 PM, Stanimir Simeonoff
<stanimir at riflexo.com>wrote:

>
>
> On Sat, Jul 14, 2012 at 9:18 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 07/14/12 14:01, David M. Lloyd wrote:
>>
>>> What is the purpose of the access-time access check in the atomic field
>>> updater
>>> classes?
>>>
>>
>> It is because there is no way to check that you haven't handed
>> your Updater to some untrusted party, so the caller context must
>> be checked on each use. I agree it is very annoying and slow.
>>
>
> But how that's a problem - you create, you hand it over, exactly as adding
> an extra method public. I can't even see how that can be an argument. The
> checks should be during creation only, not during  usage time - the calling
> class is trivial to infer. I wanted to voice that for very long time and
> used to always used to forget.
> Alternatively the SecurityManager should be checked and if not present a
> non-checking sub-class shall be returned. W/o SecurityManager it's possible
> to create modifiable java.lang.String not even touching
> Field/Method.setAccessible or Unsafe.
> That option doesn't require API changes. Since there would be a single
> subclass only instantiated in the JVM there would be no performance loss
> from multi-site invocations.
>
> Exactly that reason [pointless checks] has forced quite a few developers
> (and libs) going straight to sun.misc.Unsafe... or extend the AtomicXXX
> instead java.lang.Object (that at least is portable).
>
>
Yup. Personally I'd rather go Unsafe or extend Atomic rather than use
FieldUpdaters.

Cheers,
?


>
> Kind Regards
> Stanimir
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120714/472c8cd4/attachment.html>

From forax at univ-mlv.fr  Sat Jul 14 17:22:52 2012
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sat, 14 Jul 2012 23:22:52 +0200
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
References: <5001B417.1050405@redhat.com> <5001B7DC.4050405@cs.oswego.edu>
	<CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
Message-ID: <5001E32C.8070704@univ-mlv.fr>

On 07/14/2012 10:01 PM, Stanimir Simeonoff wrote:
>
>
> On Sat, Jul 14, 2012 at 9:18 PM, Doug Lea <dl at cs.oswego.edu 
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     On 07/14/12 14:01, David M. Lloyd wrote:
>
>         What is the purpose of the access-time access check in the
>         atomic field updater
>         classes?
>
>
>     It is because there is no way to check that you haven't handed
>     your Updater to some untrusted party, so the caller context must
>     be checked on each use. I agree it is very annoying and slow.
>
>
> But how that's a problem - you create, you hand it over, exactly as 
> adding an extra method public. I can't even see how that can be an 
> argument. The checks should be during creation only, not during  usage 
> time - the calling class is trivial to infer. I wanted to voice that 
> for very long time and used to always used to forget.
> Alternatively the SecurityManager should be checked and if not present 
> a non-checking sub-class shall be returned. W/o SecurityManager it's 
> possible to create modifiable java.lang.String not even touching 
> Field/Method.setAccessible or Unsafe.
> That option doesn't require API changes.

I agree with you that creating several subclasses is better than the 
current strategy which use several final field
that aren't considered as constant by the VM. The other solution is to 
mark this field as trusted i.e will not be changed
by reflection, which is in my opinion a better solution.

The next question is how to guarantee that the update value as the right 
type, otherwise you can
corrupt the memory by putting a String in an Integer field (and you 
can't rely on generics for that).

> Since there would be a single subclass only instantiated in the JVM 
> there would be no performance loss from multi-site invocations.

You don't care if there are several subclasses because you've put your 
AtomicReferenceFieldUpdater in a static final field.

>
> Exactly that reason [pointless checks] has forced quite a few 
> developers (and libs) going straight to sun.misc.Unsafe... or extend 
> the AtomicXXX instead java.lang.Object (that at least is portable).

yes, far from ideal.

>
>
> Kind Regards
> Stanimir

regards,
R?mi



From stanimir at riflexo.com  Sat Jul 14 17:58:57 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sun, 15 Jul 2012 00:58:57 +0300
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5001E32C.8070704@univ-mlv.fr>
References: <5001B417.1050405@redhat.com> <5001B7DC.4050405@cs.oswego.edu>
	<CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
	<5001E32C.8070704@univ-mlv.fr>
Message-ID: <CAEJX8oog8Bd0TEtdDFKkC9p-P14pJz6TqvoKzX=WhVZMMoC40Q@mail.gmail.com>

The next question is how to guarantee that the update value as the right
> type, otherwise you can
> corrupt the memory by putting a String in an Integer field (and you can't
> rely on generics for that).
>
>
Absolutely true, it's not needed to java.lang.Object, which is a start. C2
can eliminate redundant CHECKCAST since the class is a constant, dunno if
Class.isAssignableFrom enjoys the same treatment. Hence, a simple class-gen
will do the trick. Class-gen happen very often w/ reflect, so I do not
consider it too special. Again, when System.ClassLoader is not present or
the caller is from the bootstrap loader that class-gen can be omitted.



>  Since there would be a single subclass only instantiated in the JVM there
>> would be no performance loss from multi-site invocations.
>>
>
> You don't care if there are several subclasses because you've put your
> AtomicReferenceFieldUpdater in a static final field.
>
True that, it can be proven it won't change, even if it's an interface.

Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120715/36aff1d6/attachment.html>

From forax at univ-mlv.fr  Sat Jul 14 18:46:38 2012
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Sun, 15 Jul 2012 00:46:38 +0200
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <CAEJX8oog8Bd0TEtdDFKkC9p-P14pJz6TqvoKzX=WhVZMMoC40Q@mail.gmail.com>
References: <5001B417.1050405@redhat.com> <5001B7DC.4050405@cs.oswego.edu>
	<CAEJX8oqeCdFPG+_Ak5ApfQxNYWXtp+w-jcsDpaQ4a5gaF4aA9Q@mail.gmail.com>
	<5001E32C.8070704@univ-mlv.fr>
	<CAEJX8oog8Bd0TEtdDFKkC9p-P14pJz6TqvoKzX=WhVZMMoC40Q@mail.gmail.com>
Message-ID: <5001F6CE.70105@univ-mlv.fr>

On 07/14/2012 11:58 PM, Stanimir Simeonoff wrote:
>
>
>     The next question is how to guarantee that the update value as the
>     right type, otherwise you can
>     corrupt the memory by putting a String in an Integer field (and
>     you can't rely on generics for that).
>
> Absolutely true, it's not needed to java.lang.Object, which is a 
> start. C2 can eliminate redundant CHECKCAST since the class is a 
> constant, dunno if Class.isAssignableFrom enjoys the same treatment. 
> Hence, a simple class-gen will do the trick. Class-gen happen very 
> often w/ reflect, so I do not consider it too special. Again, when 
> System.ClassLoader is not present or the caller is from the bootstrap 
> loader that class-gen can be omitted.

Class-gen is not as cool as it seems, it uses a lot of memory (bytecode 
+ VM metadata) and
classloader/class unloading is really tricky. MethodHandle should be 
used instead, but from perf point of view
the JIT backend is still too young for this particular case, in fact, a 
MethodHandle.invoke() is exactly what we need.
And I really hope that at some point in the future j.l.r.Method will be 
re-written to use method handles instead.

As a meta-comment, if they're was a way to trap Java method call to 
transform them into invokedynamic,
all the problems mentioned above disappear because we will be in control 
of the callsite.

>
>
>
>         Since there would be a single subclass only instantiated in
>         the JVM there would be no performance loss from multi-site
>         invocations.
>
>
>     You don't care if there are several subclasses because you've put
>     your AtomicReferenceFieldUpdater in a static final field.
>
> True that, it can be proven it won't change, even if it's an interface.
>
> Stanimir

R?mi



From dl at cs.oswego.edu  Sun Jul 15 06:37:38 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 15 Jul 2012 06:37:38 -0400
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5001B417.1050405@redhat.com>
References: <5001B417.1050405@redhat.com>
Message-ID: <50029D72.5070205@cs.oswego.edu>

On 07/14/12 14:01, David M. Lloyd wrote:
> What is the purpose of the access-time access check in the atomic field updater
> classes?
> ...
>
> Is there any recourse to solve this issue?

So to summarize for the moment: Whenever this issue comes up,
there seem to be some upcoming options that will someday
provide good solutions. Along with some shorter-term bandaids that
would make them a little better in some cases. We always end up
not applying the bandaids because nearly all the people impacted
by the performance issues are developing infrastructure-level
components, and know how to correctly encapsulate "Unsafe" bypasses.
This is ugly and uncomfortable, but still probably better
than alternatives. I think the main downside is that people
not in that category reading source code can get the impression
that using Unsafe directly for atomic and fenced accesses is a
recommended technique rather than an act of desperation.

-Doug




From alexlamsl at gmail.com  Sun Jul 15 07:04:06 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Sun, 15 Jul 2012 12:04:06 +0100
Subject: [concurrency-interest] MappingFunction for ConcrrentMap.replace()
Message-ID: <CAGpACNuZ18BXPuqvNuPbtofx9t4S5eZ+OGddiGNMLFJRY=3PBA@mail.gmail.com>

After encountering the need for CSKLM.computeIfAbsent(), I now run
into a case where I would like a variation of replace(K,V,V) which
does:

replace(key, expected, mappingFunction)

In my particular use-case I am currently doing this:

ConcurrentMap<K, Reference<V>> map;

V get(K key) {
  Reference<V> ref = map.get(key);
  V value = ref == null ? null : ref.get();
  while (value == null) {
    value = createValue(key);
    ref = map.replace(key, ref, wrap(value));
    if (ref != null)
      value = ref.get();
  }
  return value;
}



Alex.

From davidcholmes at aapt.net.au  Sun Jul 15 07:14:27 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 15 Jul 2012 21:14:27 +1000
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5001B417.1050405@redhat.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>

David M. Lloyd writes:
> What is the purpose of the access-time access check in the atomic field
> updater classes?

Do you mean the ensureProtectedAccess check?

This is to ensure that you can't get an updater for a protected inherited
field in SubtypeA, then use that updater to access/modify the field in an
instance of SubtypeB.

> If I construct an atomic field updater, to me, this should be exactly
> equivalent to having an accessible=true Field instance.  If I want to
> manage the visibility of that instance, I will restrict access to the
> updater itself.
>
> The problem, as I am sure you are all well aware, is that when the
> updater classes are used on an object instance which is subclassed, even
> if the field being updated, the field updater, and the method calling
> into the field updater all exist on the base class and are private, an
> expensive access check has to occur.

Now you've lost me. Which access check is this? All of the methods do:

if (obj == null || obj.getClass() != tclass || cclass != null)
fullCheck(obj);

where fullCheck is:

        private void fullCheck(T obj) {
            if (!tclass.isInstance(obj))
                throw new ClassCastException();
            if (cclass != null)
                ensureProtectedAccess(obj);
        }

and ensureProtectedAccess is:

       private void ensureProtectedAccess(T obj) {
            if (cclass.isInstance(obj)) {
                return;
            }
            throw new RuntimeException(
                new IllegalAccessException("Class " +
                    cclass.getName() +
                    " can not access a protected member of class " +
                    tclass.getName() +
                    " using an instance of " +
                    obj.getClass().getName()
                )
            );
        }

So if everything is in the same class we don't do any additional checks.

David
-----


> Even when the class is final, the access check is still expensive
> relative to the operation itself!
>
> I mean I'm using field updaters in the first place because I can't
> afford to have a jillion Atomic* objects floating around; obviously
> performance is critical here.  And to add to this problem, you can't
> even rely on using Unsafe, even though it's becoming more and more
> prevalent in JDKs, as some platforms may not have atomic 64-bit
> operations, and there's no way I can see to test for that other than
> relying on the AtomicLong implementation to be similar to OpenJDK's.
>
> Is there any recourse to solve this issue?
> --
> - DML
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Sun Jul 15 07:14:35 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 15 Jul 2012 07:14:35 -0400
Subject: [concurrency-interest] MappingFunction for
	ConcrrentMap.replace()
In-Reply-To: <CAGpACNuZ18BXPuqvNuPbtofx9t4S5eZ+OGddiGNMLFJRY=3PBA@mail.gmail.com>
References: <CAGpACNuZ18BXPuqvNuPbtofx9t4S5eZ+OGddiGNMLFJRY=3PBA@mail.gmail.com>
Message-ID: <5002A61B.4090108@cs.oswego.edu>

On 07/15/12 07:04, Alex Lam S.L. wrote:
> After encountering the need for CSKLM.computeIfAbsent(), I now run
> into a case where I would like a variation of replace(K,V,V) which
> does:
>
> replace(key, expected, mappingFunction)

A variant of this that you should be able to use in such cases
is also in ConcurrentHashMapV8
   compute(key, remappingFunction);
and is on the todo list for CSLM for JDK8. There probably
will not be a preview of this out soon though.
The CSLM updates for both computed-updates and
lambda-ized collection support (Spliterator-like constructions)
are intertwined, and we're still experimenting with the
lambda-support APIs mainly using CHM.

-Doug


From viktor.klang at gmail.com  Sun Jul 15 11:22:13 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 15 Jul 2012 17:22:13 +0200
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>
References: <5001B417.1050405@redhat.com>
	<NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>
Message-ID: <CANPzfU8GiB1NDQ49C5JCesz-U9Nz+HNNvb8h2rmVMZoHbyr6nA@mail.gmail.com>

On Sun, Jul 15, 2012 at 1:14 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> David M. Lloyd writes:
> > What is the purpose of the access-time access check in the atomic field
> > updater classes?
>
> Do you mean the ensureProtectedAccess check?
>
> This is to ensure that you can't get an updater for a protected inherited
> field in SubtypeA, then use that updater to access/modify the field in an
> instance of SubtypeB.
>
> > If I construct an atomic field updater, to me, this should be exactly
> > equivalent to having an accessible=true Field instance.  If I want to
> > manage the visibility of that instance, I will restrict access to the
> > updater itself.
> >
> > The problem, as I am sure you are all well aware, is that when the
> > updater classes are used on an object instance which is subclassed, even
> > if the field being updated, the field updater, and the method calling
> > into the field updater all exist on the base class and are private, an
> > expensive access check has to occur.
>
> Now you've lost me. Which access check is this? All of the methods do:
>
> if (obj == null || obj.getClass() != tclass || cclass != null)
> fullCheck(obj);
>
> where fullCheck is:
>
>         private void fullCheck(T obj) {
>             if (!tclass.isInstance(obj))
>                 throw new ClassCastException();
>             if (cclass != null)
>                 ensureProtectedAccess(obj);
>         }
>
> and ensureProtectedAccess is:
>
>        private void ensureProtectedAccess(T obj) {
>             if (cclass.isInstance(obj)) {
>                 return;
>             }
>             throw new RuntimeException(
>                 new IllegalAccessException("Class " +
>                     cclass.getName() +
>                     " can not access a protected member of class " +
>                     tclass.getName() +
>                     " using an instance of " +
>                     obj.getClass().getName()
>                 )
>             );
>         }
>
> So if everything is in the same class we don't do any additional checks.
>

Which is a huge issue in langs that don't do statics, like Scala, you end
up having to have a Java superclass to hold the final statics and then
implement the meaty part in Scala, which is a huge pain due to these
defensive checks.
A new @static annotation is on the way for Scala to try to work around this
and to solve issues with libraries/frameworks that build on static members.

So I'm forced to either inherit Atomic* or go directly for Unsafe, and
since there are no traits in Java, I almost always have to go Unsafe to
have multiple fields.

Cheers,
?


>
> David
> -----
>
>
> > Even when the class is final, the access check is still expensive
> > relative to the operation itself!
> >
> > I mean I'm using field updaters in the first place because I can't
> > afford to have a jillion Atomic* objects floating around; obviously
> > performance is critical here.  And to add to this problem, you can't
> > even rely on using Unsafe, even though it's becoming more and more
> > prevalent in JDKs, as some platforms may not have atomic 64-bit
> > operations, and there's no way I can see to test for that other than
> > relying on the AtomicLong implementation to be similar to OpenJDK's.
> >
> > Is there any recourse to solve this issue?
> > --
> > - DML
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120715/9594c8a0/attachment.html>

From david.lloyd at redhat.com  Sun Jul 15 12:19:17 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Sun, 15 Jul 2012 11:19:17 -0500
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>
Message-ID: <5002ED85.6080004@redhat.com>

On 07/15/2012 06:14 AM, David Holmes wrote:
> David M. Lloyd writes:
>> What is the purpose of the access-time access check in the atomic field
>> updater classes?
>
> Do you mean the ensureProtectedAccess check?
>
> This is to ensure that you can't get an updater for a protected inherited
> field in SubtypeA, then use that updater to access/modify the field in an
> instance of SubtypeB.

Which is ridiculous because if a subclass "inherited" access to a normal 
protected field on my class, I'd still be able to access it directly in 
the base class... because fields aren't actually inherited.

>> If I construct an atomic field updater, to me, this should be exactly
>> equivalent to having an accessible=true Field instance.  If I want to
>> manage the visibility of that instance, I will restrict access to the
>> updater itself.
>>
>> The problem, as I am sure you are all well aware, is that when the
>> updater classes are used on an object instance which is subclassed, even
>> if the field being updated, the field updater, and the method calling
>> into the field updater all exist on the base class and are private, an
>> expensive access check has to occur.
>
> Now you've lost me. Which access check is this? All of the methods do:
>
> if (obj == null || obj.getClass() != tclass || cclass != null)
> fullCheck(obj);
>
> where fullCheck is:
>
>          private void fullCheck(T obj) {
>              if (!tclass.isInstance(obj))
>                  throw new ClassCastException();
>              if (cclass != null)
>                  ensureProtectedAccess(obj);
>          }
>
> and ensureProtectedAccess is:
>
>         private void ensureProtectedAccess(T obj) {
>              if (cclass.isInstance(obj)) {
>                  return;
>              }
>              throw new RuntimeException(
>                  new IllegalAccessException("Class " +
>                      cclass.getName() +
>                      " can not access a protected member of class " +
>                      tclass.getName() +
>                      " using an instance of " +
>                      obj.getClass().getName()
>                  )
>              );
>          }
>
> So if everything is in the same class we don't do any additional checks.

I believe this is inaccurate.  If I have a private static updater for a 
private field in the base class, and even if I access the updater only 
from the base class, the additional checks are still run because 
obj.getClass() != tclass (it's actually a subclass, not tclass).

But even if it weren't, the original access check is relatively 
expensive and completely unneeded.  Even assuming these access checks 
aren't completely pointless, which of course they are - if I want public 
access to a private field, I'd make the updater public.  If I want 
private access then the updater will be private.  No access-time checks 
are necessary; all the appropriate verification should have been done 
(usually at class init) when the updater is constructed by examining the 
caller class.  And I strongly, *strongly* doubt there is any code in the 
wild which relies on these access-time checks to ensure access protection.

And while I'm on this tirade - the whole design of using subclasses for 
the different variations in updater implementation seems unnecessary. 
If it were all in one final class, and the VM_SUPPORTS_LONG_CAS flag 
were a constant static field (which of course it is), then having e.g. 
compareAndSet be implemented like this:

public /*final*/ boolean compareAndSet(T obj, long expect, long update) {
     if (VM_SUPPORTS_LONG_CAS) {
         return unsafe.compareAndSwapLong(obj, offset, expect, update);
     } else synchronized (this) {
         long v = unsafe.getLong(obj, offset);
         if (v != expect) return false;
         unsafe.putLong(obj, offset, update);
         return true;
     }
}

...it seems to me that the method is much more likely to be inlined, and 
furthermore the un-followed branch should be elided, which would 
essentially make this a single instruction on most platforms.

>> Even when the class is final, the access check is still expensive
>> relative to the operation itself!
>>
>> I mean I'm using field updaters in the first place because I can't
>> afford to have a jillion Atomic* objects floating around; obviously
>> performance is critical here.  And to add to this problem, you can't
>> even rely on using Unsafe, even though it's becoming more and more
>> prevalent in JDKs, as some platforms may not have atomic 64-bit
>> operations, and there's no way I can see to test for that other than
>> relying on the AtomicLong implementation to be similar to OpenJDK's.
>>
>> Is there any recourse to solve this issue?
>> --
>> - DML
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


-- 
- DML

From jwesleysmith at atlassian.com  Sun Jul 15 22:31:29 2012
From: jwesleysmith at atlassian.com (Jed Wesley-Smith)
Date: Mon, 16 Jul 2012 12:31:29 +1000
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <CANPzfU8GiB1NDQ49C5JCesz-U9Nz+HNNvb8h2rmVMZoHbyr6nA@mail.gmail.com>
References: <5001B417.1050405@redhat.com>
	<NFBBKALFDCPFIDBNKAPCMEJEJFAA.davidcholmes@aapt.net.au>
	<CANPzfU8GiB1NDQ49C5JCesz-U9Nz+HNNvb8h2rmVMZoHbyr6nA@mail.gmail.com>
Message-ID: <CAKh+yi_spVZUvnyXVfXr9rnOttyR0OpboZqeabAmJyy_upF6iw@mail.gmail.com>

here's some further info on the subject:

http://stackoverflow.com/questions/5574502/how-do-i-get-an-atomicreferencefieldupdater-from-a-scala-companion-object

On 16 July 2012 01:22, ?iktor ?lang <viktor.klang at gmail.com> wrote:

>
>
> On Sun, Jul 15, 2012 at 1:14 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> David M. Lloyd writes:
>> > What is the purpose of the access-time access check in the atomic field
>> > updater classes?
>>
>> Do you mean the ensureProtectedAccess check?
>>
>> This is to ensure that you can't get an updater for a protected inherited
>> field in SubtypeA, then use that updater to access/modify the field in an
>> instance of SubtypeB.
>>
>> > If I construct an atomic field updater, to me, this should be exactly
>> > equivalent to having an accessible=true Field instance.  If I want to
>> > manage the visibility of that instance, I will restrict access to the
>> > updater itself.
>> >
>> > The problem, as I am sure you are all well aware, is that when the
>> > updater classes are used on an object instance which is subclassed, even
>> > if the field being updated, the field updater, and the method calling
>> > into the field updater all exist on the base class and are private, an
>> > expensive access check has to occur.
>>
>> Now you've lost me. Which access check is this? All of the methods do:
>>
>> if (obj == null || obj.getClass() != tclass || cclass != null)
>> fullCheck(obj);
>>
>> where fullCheck is:
>>
>>         private void fullCheck(T obj) {
>>             if (!tclass.isInstance(obj))
>>                 throw new ClassCastException();
>>             if (cclass != null)
>>                 ensureProtectedAccess(obj);
>>         }
>>
>> and ensureProtectedAccess is:
>>
>>        private void ensureProtectedAccess(T obj) {
>>             if (cclass.isInstance(obj)) {
>>                 return;
>>             }
>>             throw new RuntimeException(
>>                 new IllegalAccessException("Class " +
>>                     cclass.getName() +
>>                     " can not access a protected member of class " +
>>                     tclass.getName() +
>>                     " using an instance of " +
>>                     obj.getClass().getName()
>>                 )
>>             );
>>         }
>>
>> So if everything is in the same class we don't do any additional checks.
>>
>
> Which is a huge issue in langs that don't do statics, like Scala, you end
> up having to have a Java superclass to hold the final statics and then
> implement the meaty part in Scala, which is a huge pain due to these
> defensive checks.
> A new @static annotation is on the way for Scala to try to work around
> this and to solve issues with libraries/frameworks that build on static
> members.
>
> So I'm forced to either inherit Atomic* or go directly for Unsafe, and
> since there are no traits in Java, I almost always have to go Unsafe to
> have multiple fields.
>
> Cheers,
> ?
>
>
>>
>> David
>> -----
>>
>>
>> > Even when the class is final, the access check is still expensive
>> > relative to the operation itself!
>> >
>> > I mean I'm using field updaters in the first place because I can't
>> > afford to have a jillion Atomic* objects floating around; obviously
>> > performance is critical here.  And to add to this problem, you can't
>> > even rely on using Unsafe, even though it's becoming more and more
>> > prevalent in JDKs, as some platforms may not have atomic 64-bit
>> > operations, and there's no way I can see to test for that other than
>> > relying on the AtomicLong implementation to be similar to OpenJDK's.
>> >
>> > Is there any recourse to solve this issue?
>> > --
>> > - DML
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120716/6b29db4f/attachment.html>

From davidcholmes at aapt.net.au  Sun Jul 15 22:35:52 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 16 Jul 2012 12:35:52 +1000
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5002ED85.6080004@redhat.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEJHJFAA.davidcholmes@aapt.net.au>

David,

David M. Lloyd writes:
> On 07/15/2012 06:14 AM, David Holmes wrote:
> > David M. Lloyd writes:
> >> What is the purpose of the access-time access check in the atomic field
> >> updater classes?
> >
> > Do you mean the ensureProtectedAccess check?
> >
> > This is to ensure that you can't get an updater for a protected
> > inherited field in SubtypeA, then use that updater to access/modify the
> > field in an instance of SubtypeB.
>
> Which is ridiculous because if a subclass "inherited" access to a normal
> protected field on my class, I'd still be able to access it directly in
> the base class... because fields aren't actually inherited.

I think you are missing the point. Consider this:

class Base {
  protected volatile int x;
}

final class Secure extends Base { ... }

Instances of Secure are only supposed to be used through their public API,
as defined by Base and Secure. I hope you will agree that if I have an
instance of Secure I shouldn't be able to arbitrarily mess with the value of
x?

But given

class BackDoor extends Base {
   static AtomicIntegerFieldUpdater<Base> u = ... (Base.class, "x");

   public static void setBaseX(Base target, int newX) {
      u.set(target, newX);
   }
}

without these protected access checks, BackDoor.setBaseX would allow you to
update _any_ instance of Base.


> >> If I construct an atomic field updater, to me, this should be exactly
> >> equivalent to having an accessible=true Field instance.  If I want to
> >> manage the visibility of that instance, I will restrict access to the
> >> updater itself.
> >>
> >> The problem, as I am sure you are all well aware, is that when the
> >> updater classes are used on an object instance which is
> subclassed, even
> >> if the field being updated, the field updater, and the method calling
> >> into the field updater all exist on the base class and are private, an
> >> expensive access check has to occur.
> >
> > Now you've lost me. Which access check is this? All of the methods do:
> >
> > if (obj == null || obj.getClass() != tclass || cclass != null)
> > fullCheck(obj);
<snip>
> >
> > So if everything is in the same class we don't do any additional checks.
>
> I believe this is inaccurate.  If I have a private static updater for a
> private field in the base class, and even if I access the updater only
> from the base class, the additional checks are still run because
> obj.getClass() != tclass (it's actually a subclass, not tclass).

Sorry. I stated "if everything is in the same class" and here the target
object is not the same class. I misinterpreted the conditions you were
establishing.

Access checks are always pure overhead for correct code. But they aren't
there for correct code.

David
-----


> But even if it weren't, the original access check is relatively
> expensive and completely unneeded.  Even assuming these access checks
> aren't completely pointless, which of course they are - if I want public
> access to a private field, I'd make the updater public.  If I want
> private access then the updater will be private.  No access-time checks
> are necessary; all the appropriate verification should have been done
> (usually at class init) when the updater is constructed by examining the
> caller class.  And I strongly, *strongly* doubt there is any code in the
> wild which relies on these access-time checks to ensure access protection.
>
> And while I'm on this tirade - the whole design of using subclasses for
> the different variations in updater implementation seems unnecessary.
> If it were all in one final class, and the VM_SUPPORTS_LONG_CAS flag
> were a constant static field (which of course it is), then having e.g.
> compareAndSet be implemented like this:
>
> public /*final*/ boolean compareAndSet(T obj, long expect, long update) {
>      if (VM_SUPPORTS_LONG_CAS) {
>          return unsafe.compareAndSwapLong(obj, offset, expect, update);
>      } else synchronized (this) {
>          long v = unsafe.getLong(obj, offset);
>          if (v != expect) return false;
>          unsafe.putLong(obj, offset, update);
>          return true;
>      }
> }
>
> ...it seems to me that the method is much more likely to be inlined, and
> furthermore the un-followed branch should be elided, which would
> essentially make this a single instruction on most platforms.
>
> >> Even when the class is final, the access check is still expensive
> >> relative to the operation itself!
> >>
> >> I mean I'm using field updaters in the first place because I can't
> >> afford to have a jillion Atomic* objects floating around; obviously
> >> performance is critical here.  And to add to this problem, you can't
> >> even rely on using Unsafe, even though it's becoming more and more
> >> prevalent in JDKs, as some platforms may not have atomic 64-bit
> >> operations, and there's no way I can see to test for that other than
> >> relying on the AtomicLong implementation to be similar to OpenJDK's.
> >>
> >> Is there any recourse to solve this issue?
> >> --
> >> - DML
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
>
>
> --
> - DML
>


From david.lloyd at redhat.com  Sun Jul 15 22:43:59 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Sun, 15 Jul 2012 21:43:59 -0500
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEJHJFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEJHJFAA.davidcholmes@aapt.net.au>
Message-ID: <50037FEF.5020705@redhat.com>

On 07/15/2012 09:35 PM, David Holmes wrote:
> David,
>
> David M. Lloyd writes:
>> On 07/15/2012 06:14 AM, David Holmes wrote:
>>> David M. Lloyd writes:
>>>> What is the purpose of the access-time access check in the atomic field
>>>> updater classes?
>>>
>>> Do you mean the ensureProtectedAccess check?
>>>
>>> This is to ensure that you can't get an updater for a protected
>>> inherited field in SubtypeA, then use that updater to access/modify the
>>> field in an instance of SubtypeB.
>>
>> Which is ridiculous because if a subclass "inherited" access to a normal
>> protected field on my class, I'd still be able to access it directly in
>> the base class... because fields aren't actually inherited.
>
> I think you are missing the point. Consider this:
>
> class Base {
>    protected volatile int x;
> }
>
> final class Secure extends Base { ... }
>
> Instances of Secure are only supposed to be used through their public API,
> as defined by Base and Secure. I hope you will agree that if I have an
> instance of Secure I shouldn't be able to arbitrarily mess with the value of
> x?
>
> But given
>
> class BackDoor extends Base {
>     static AtomicIntegerFieldUpdater<Base>  u = ... (Base.class, "x");
>
>     public static void setBaseX(Base target, int newX) {
>        u.set(target, newX);
>     }
> }
>
> without these protected access checks, BackDoor.setBaseX would allow you to
> update _any_ instance of Base.

I see your point, but in this case, wouldn't it be better to restrict 
updaters from being created on any class but their own, regardless of 
the access of the field?  In other words, we'd allow:

    static AtomicIntegerFieldUpdater<BackDoor> u = ... (Backdoor.class, 
"x");

but not:

    static AtomicIntegerFieldUpdater<Base> u = ... (Base.class, "x");

from BackDoor.
-- 
- DML

From davidcholmes at aapt.net.au  Sun Jul 15 22:52:36 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 16 Jul 2012 12:52:36 +1000
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <50037FEF.5020705@redhat.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEJIJFAA.davidcholmes@aapt.net.au>

David M. Lloyd writes:
> On 07/15/2012 09:35 PM, David Holmes wrote:
> > David M. Lloyd writes:
> >> On 07/15/2012 06:14 AM, David Holmes wrote:
> >>> David M. Lloyd writes:
> >>>> What is the purpose of the access-time access check in the
> >>>> atomic field updater classes?
> >>>
> >>> Do you mean the ensureProtectedAccess check?
> >>>
> >>> This is to ensure that you can't get an updater for a protected
> >>> inherited field in SubtypeA, then use that updater to
> >>> access/modify the field in an instance of SubtypeB.
> >>
> >> Which is ridiculous because if a subclass "inherited" access
> >> to a normal
> >> protected field on my class, I'd still be able to access it directly in
> >> the base class... because fields aren't actually inherited.
> >
> > I think you are missing the point. Consider this:
> >
> > class Base {
> >    protected volatile int x;
> > }
> >
> > final class Secure extends Base { ... }
> >
> > Instances of Secure are only supposed to be used through their
> public API,
> > as defined by Base and Secure. I hope you will agree that if I have an
> > instance of Secure I shouldn't be able to arbitrarily mess with
> the value of
> > x?
> >
> > But given
> >
> > class BackDoor extends Base {
> >     static AtomicIntegerFieldUpdater<Base>  u = ... (Base.class, "x");
> >
> >     public static void setBaseX(Base target, int newX) {
> >        u.set(target, newX);
> >     }
> > }
> >
> > without these protected access checks, BackDoor.setBaseX would
> allow you to
> > update _any_ instance of Base.
>
> I see your point, but in this case, wouldn't it be better to restrict
> updaters from being created on any class but their own, regardless of
> the access of the field?  In other words, we'd allow:
>
>     static AtomicIntegerFieldUpdater<BackDoor> u = ... (Backdoor.class,
> "x");
>
> but not:
>
>     static AtomicIntegerFieldUpdater<Base> u = ... (Base.class, "x");
>
> from BackDoor.

In hindsight, in the context of the current discussion, it may have been
better to restrict things this way to allow access checks to be elided. But
that has to be weighed up against use-cases where it is an inherited field
that needs the atomic update.

David
-----


> --
> - DML
>


From smourachov at gmail.com  Mon Jul 16 20:55:04 2012
From: smourachov at gmail.com (Serguei Mourachov)
Date: Mon, 16 Jul 2012 17:55:04 -0700
Subject: [concurrency-interest] High CPU utilization on CentOS6 in TPE using
	LinkedTransferQueue
Message-ID: <5004B7E8.8050703@gmail.com>

We are having problem migrating our system from CentOS 5.5(Linux kernel 
version 2.6.18) to CentOS 6 (kernel version 2.6.32) running JDK7.
We are using Amazon EC2 c1.xlarge instance with 8 virtual cores.
Here is the code snippet to reproduce the problem, simulating our workload :

public static void main(String[] args) throws Exception {
         ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
         Runnable r = new Runnable() {
             public void run() {
                 LockSupport.parkNanos(1_000_000);
             }
         };
         ThreadPoolExecutor pool = new ThreadPoolExecutor(128, 128, 0, 
TimeUnit.MINUTES, new LinkedTransferQueue<Runnable>());
         long cpuTotalTimeOnStart = threadMxBean.getCurrentThreadCpuTime();

         for (int i = 0; i < 2_000_000; i++) {
             pool.execute(r);
         }
         long cpuTotalTime = (threadMxBean.getCurrentThreadCpuTime() - 
cpuTotalTimeOnStart) / 1_000_000;
         System.out.println(" cpuTotalTime=" + cpuTotalTime + "ms");
         System.exit(0);
     }
}
When executed on CentOS 5.5 the main thread is consuming <300ms of cpu 
time, but in  case of CentOS 6 that value is growing to >15s of cpu time,
We traced that high cpu utilization down to LockSupport.unpark() calls 
in LinkedTransferQueue.xfer() method.
If we replace LinkedTransferQueue with "traditional" 
LinkedBlockingQueue, then the cpu utilization becomes comparable to 
CentOS 5.5, but we really like much better latency of LTQ.
Decreasing TPE size to 64 threads also brings cpu utilization down to 
<500ms values, but this is not a feasible solution in case of our system.
Apparently,  the issue is caused by switching to "Completely Fair 
Scheduler"  and, in case of  CentOS 6, 80-90% of total cpu time is the 
system time.
A the moment we are trying to play with some kernel scheduling 
parameters,  but without any major success, so any help or advice will 
be highly appreciated

Serguei Mourachov

From javapk at gmail.com  Thu Jul 19 11:12:38 2012
From: javapk at gmail.com (Praveen Kumar Jha)
Date: Thu, 19 Jul 2012 20:42:38 +0530
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of Vector
Message-ID: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>

Hello,

Please help me in understanding the thread-safety of hasMoreElements() of
Vector from visibility point of view. The hasMoreElements()
accesses the 'elementCount' field without synchronizing over 'Vector.this',
like it is done in nextElement() method.

public Enumeration<E> elements() {
    return new Enumeration<E>() {
        int count = 0;

        public boolean hasMoreElements() {
        return count < elementCount;
        }

        public E nextElement() {
        synchronized (Vector.this) {
            if (count < elementCount) {
            return (E)elementData[count++];
            }
        }
        throw new NoSuchElementException("Vector Enumeration");
        }
    };
    }

This might cause a thread to see stale value of 'elementCount' and thus
hasMoreElements() might incorrectly return true or false.
Javadoc of neither Enumeration nor elements() of Vector says anything about
thread-safety.

Regards,
Praveen
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120719/d894e7e4/attachment.html>

From alexlamsl at gmail.com  Thu Jul 19 11:41:56 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Thu, 19 Jul 2012 16:41:56 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning new
	thread
Message-ID: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>

I am trying to get my application to use ForkJoinPool. Specifically, I
want it to automatically occupy all 8 CPUs even when one of the thread
is blocked by BlockingQueue.take() - below is an implementation based
on what I can understand from the javadocs:


  ForkJoinPool pool = new ForkJoinPool();
  BlockingQueue<V> queue = new LinkedBlockingQueue<>();
  Callable<V> workload = ...;

  class QueueBlocker<V> implements ForkJoinPool.ManagedBlocker {
    private V value;

    public V get() {
      return value;
    }

    public boolean block() throws InterruptedException {
      value = queue.take();
      return true;
    }

    public boolean isReleasable() {
      return value != null || (value = queue.poll()) != null;
    }
  }

  class Task<V> extends ForkJoinTask<Void> {
    private final Callable<V> callable;

    Task(Callable<V> callable) {
      this.callable = callable;
    }

    public Void getRawResult() {
      return null;
    }

    protected void setRawResultVoid value) {
    }

    protected boolean exec() {
      try {
        queue.add(callable.call());
        return true;
      } catch (Exception ex) {
        throw new RuntimeException(ex);
      }
    }
  }

  final int N = ...;

  pool.execute(new Runnable() {
    public void run() {
      for (int i = 0; i < N; i++)
        pool.execute(new Task<V>(workload));

      for (int i = 0; i < N; i++) {
        final QueueBlocker blocker = new QueueBlocker();
        ForkJoinPool.managedBlock(blocker);
        process(blocker.get());
      }
    }
  });


Now on my 8-core machine, I can see from JConsole that FJ pool have 8
threads, with one of them consistently blocking on queue.take(), thus
only leaving 7 CPUs busy.

Is this behaviour intentional? If so, what am I doing wrong here?


Thanks,
Alex.

From joe.bowbeer at gmail.com  Thu Jul 19 12:11:15 2012
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 19 Jul 2012 09:11:15 -0700
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
References: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
Message-ID: <CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>

The Enumeration API is not suitable for multi-threaded use because there is
a window for change between the hasMoreElements call and the subsequent
nextElement call.

In almost all cases the Enumeration is consumed in the same thread in which
it is produced.

The strange corner case that you envision is: one thread creates an
Enumeration and hands it to another thread.  The other thread calls
hasNextElement and may see a stale value for elementCount, causing it to
end too early (not calling nextElement) or to fail unexpectedly when it
does call nextElement.

With this scenario in mind, using "synchronized" in hasMoreElements is more
correct, though in practice I doubt it will make any difference.

Joe

On Thu, Jul 19, 2012 at 8:12 AM, Praveen Kumar Jha wrote:

> Hello,
>
> Please help me in understanding the thread-safety of hasMoreElements() of
> Vector from visibility point of view. The hasMoreElements()
> accesses the 'elementCount' field without synchronizing over
> 'Vector.this', like it is done in nextElement() method.
>
> public Enumeration<E> elements() {
>     return new Enumeration<E>() {
>         int count = 0;
>
>         public boolean hasMoreElements() {
>         return count < elementCount;
>         }
>
>         public E nextElement() {
>         synchronized (Vector.this) {
>             if (count < elementCount) {
>             return (E)elementData[count++];
>             }
>         }
>         throw new NoSuchElementException("Vector Enumeration");
>         }
>     };
>     }
>
> This might cause a thread to see stale value of 'elementCount' and thus
> hasMoreElements() might incorrectly return true or false.
> Javadoc of neither Enumeration nor elements() of Vector says anything
> about thread-safety.
>
> Regards,
> Praveen
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120719/41261a04/attachment.html>

From yshavit at akiban.com  Thu Jul 19 12:40:16 2012
From: yshavit at akiban.com (Yuval Shavit)
Date: Thu, 19 Jul 2012 12:40:16 -0400
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>
References: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
	<CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>
Message-ID: <CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>

It seems like it's more problematic than that. Vector.elementCount isn't
volatile, so if you create an Enumeration (and keep it thread-local), but
the backing Vector changes on some other thread, hasMoreElements() is
broken. I think that method needs to be synchronized on Vector.this.

On Thu, Jul 19, 2012 at 12:11 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> The Enumeration API is not suitable for multi-threaded use because there
> is a window for change between the hasMoreElements call and the subsequent
> nextElement call.
>
> In almost all cases the Enumeration is consumed in the same thread in
> which it is produced.
>
> The strange corner case that you envision is: one thread creates an
> Enumeration and hands it to another thread.  The other thread calls
> hasNextElement and may see a stale value for elementCount, causing it to
> end too early (not calling nextElement) or to fail unexpectedly when it
> does call nextElement.
>
> With this scenario in mind, using "synchronized" in hasMoreElements is
> more correct, though in practice I doubt it will make any difference.
>
> Joe
>
> On Thu, Jul 19, 2012 at 8:12 AM, Praveen Kumar Jha wrote:
>
> Hello,
>>
>> Please help me in understanding the thread-safety of hasMoreElements() of
>> Vector from visibility point of view. The hasMoreElements()
>> accesses the 'elementCount' field without synchronizing over
>> 'Vector.this', like it is done in nextElement() method.
>>
>> public Enumeration<E> elements() {
>>     return new Enumeration<E>() {
>>         int count = 0;
>>
>>         public boolean hasMoreElements() {
>>         return count < elementCount;
>>         }
>>
>>         public E nextElement() {
>>         synchronized (Vector.this) {
>>             if (count < elementCount) {
>>             return (E)elementData[count++];
>>             }
>>         }
>>         throw new NoSuchElementException("Vector Enumeration");
>>         }
>>     };
>>     }
>>
>> This might cause a thread to see stale value of 'elementCount' and thus
>> hasMoreElements() might incorrectly return true or false.
>> Javadoc of neither Enumeration nor elements() of Vector says anything
>> about thread-safety.
>>
>> Regards,
>> Praveen
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120719/e7972785/attachment.html>

From joe.bowbeer at gmail.com  Thu Jul 19 12:56:04 2012
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 19 Jul 2012 09:56:04 -0700
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
References: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
	<CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>
	<CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
Message-ID: <CAHzJPEoE+1XPTdNcMfOv7DFRtojC81hqjHt=iJkHpakaL87BeA@mail.gmail.com>

If the backing vector is changing on some other thread then Enumeration is
not a suitable API.  (This is probably the logic applied by the implementer
who decided not to bother sync'ing hasMoreElements.)

On Thu, Jul 19, 2012 at 9:40 AM, Yuval Shavit wrote:

> It seems like it's more problematic than that. Vector.elementCount isn't
> volatile, so if you create an Enumeration (and keep it thread-local), but
> the backing Vector changes on some other thread, hasMoreElements() is
> broken. I think that method needs to be synchronized on Vector.this.
>
> On Thu, Jul 19, 2012 at 12:11 PM, Joe Bowbeer wrote:
>
>> The Enumeration API is not suitable for multi-threaded use because there
>> is a window for change between the hasMoreElements call and the subsequent
>> nextElement call.
>>
>> In almost all cases the Enumeration is consumed in the same thread in
>> which it is produced.
>>
>> The strange corner case that you envision is: one thread creates an
>> Enumeration and hands it to another thread.  The other thread calls
>> hasNextElement and may see a stale value for elementCount, causing it to
>> end too early (not calling nextElement) or to fail unexpectedly when it
>> does call nextElement.
>>
>> With this scenario in mind, using "synchronized" in hasMoreElements is
>> more correct, though in practice I doubt it will make any difference.
>>
>> Joe
>>
>> On Thu, Jul 19, 2012 at 8:12 AM, Praveen Kumar Jha wrote:
>>
>> Hello,
>>>
>>> Please help me in understanding the thread-safety of hasMoreElements()
>>> of Vector from visibility point of view. The hasMoreElements()
>>> accesses the 'elementCount' field without synchronizing over
>>> 'Vector.this', like it is done in nextElement() method.
>>>
>>> public Enumeration<E> elements() {
>>>     return new Enumeration<E>() {
>>>         int count = 0;
>>>
>>>         public boolean hasMoreElements() {
>>>         return count < elementCount;
>>>         }
>>>
>>>         public E nextElement() {
>>>         synchronized (Vector.this) {
>>>             if (count < elementCount) {
>>>             return (E)elementData[count++];
>>>             }
>>>         }
>>>         throw new NoSuchElementException("Vector Enumeration");
>>>         }
>>>     };
>>>     }
>>>
>>> This might cause a thread to see stale value of 'elementCount' and thus
>>> hasMoreElements() might incorrectly return true or false.
>>> Javadoc of neither Enumeration nor elements() of Vector says anything
>>> about thread-safety.
>>>
>>> Regards,
>>> Praveen
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120719/99d5bb20/attachment.html>

From gregg at cytetech.com  Thu Jul 19 12:58:55 2012
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 19 Jul 2012 11:58:55 -0500
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
 Vector
In-Reply-To: <CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
References: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
	<CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>
	<CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
Message-ID: <50083CCF.20005@cytetech.com>

On 7/19/2012 11:40 AM, Yuval Shavit wrote:
> It seems like it's more problematic than that. Vector.elementCount isn't
> volatile, so if you create an Enumeration (and keep it thread-local), but the
> backing Vector changes on some other thread, hasMoreElements() is broken. I
> think that method needs to be synchronized on Vector.this.

If you do

while (true) {
	boolean more;
	synchronized(vector) {
		more = vector.hasMoreElements();
	}
	if( !more) break;

	...

}

Then you can "synchronize" with the state of other threads, obviously.  But it 
definitely looks like it's broken for that use case.  One could argue, that it 
would not be very productive for two threads to use an instance in this mode, 
without some other "happens-before" going on anyway.  For example, if you have a 
thread adding elements and another processing them, there should be a semaphore 
of some sort.  I would really be surprised to find code that used an instance 
without some other happens-before relationship between the two threads, which 
would cause the value to be correct.

Gregg

> On Thu, Jul 19, 2012 at 12:11 PM, Joe Bowbeer <joe.bowbeer at gmail.com
> <mailto:joe.bowbeer at gmail.com>> wrote:
>
>     The Enumeration API is not suitable for multi-threaded use because there is
>     a window for change between the hasMoreElements call and the subsequent
>     nextElement call.
>
>     In almost all cases the Enumeration is consumed in the same thread in which
>     it is produced.
>
>     The strange corner case that you envision is: one thread creates an
>     Enumeration and hands it to another thread.  The other thread calls
>     hasNextElement and may see a stale value for elementCount, causing it to end
>     too early (not calling nextElement) or to fail unexpectedly when it does
>     call nextElement.
>
>     With this scenario in mind, using "synchronized" in hasMoreElements is more
>     correct, though in practice I doubt it will make any difference.
>
>     Joe
>
>     On Thu, Jul 19, 2012 at 8:12 AM, Praveen Kumar Jha wrote:
>
>         Hello,
>
>         Please help me in understanding the thread-safety of hasMoreElements()
>         of Vector from visibility point of view. The hasMoreElements()
>         accesses the 'elementCount' field without synchronizing over
>         'Vector.this', like it is done in nextElement() method.
>
>         public Enumeration<E> elements() {
>              return new Enumeration<E>() {
>                  int count = 0;
>
>                  public boolean hasMoreElements() {
>                  return count < elementCount;
>                  }
>
>                  public E nextElement() {
>                  synchronized (Vector.this) {
>                      if (count < elementCount) {
>                      return (E)elementData[count++];
>                      }
>                  }
>                  throw new NoSuchElementException("Vector Enumeration");
>                  }
>              };
>              }
>
>         This might cause a thread to see stale value of 'elementCount' and thus
>         hasMoreElements() might incorrectly return true or false.
>         Javadoc of neither Enumeration nor elements() of Vector says anything
>         about thread-safety.
>
>         Regards,
>         Praveen
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Thu Jul 19 13:01:25 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Jul 2012 13:01:25 -0400
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
References: <CAKYmnUoQ8kLoBf_17xkuRnpg-D3N7w5ouVfQwAGQgvbV37xnDg@mail.gmail.com>
	<CAHzJPEoW5FQUmxNPSf7bxLm4y2jjgi1F3aKWme3oy6xkAiTFbg@mail.gmail.com>
	<CAC2Zdp3G0e8569JzJsY4zj+zoEEot6kOsfPKgmFk6VOk9iFEkQ@mail.gmail.com>
Message-ID: <CAHjP37Exumd2eWF2SJczXVYpyeLRWpEaoDF7esK7at-GWSvVAA@mail.gmail.com>

The javadoc for the class mentions that the enumerations returned are not
fail-fast.  In addition, iteration is not thread-safe.  So you have to
synchronize on the instance while iterating/enumerating anyway, at which
point visibility will be guaranteed.

Sent from my phone
On Jul 19, 2012 12:44 PM, "Yuval Shavit" <yshavit at akiban.com> wrote:

> It seems like it's more problematic than that. Vector.elementCount isn't
> volatile, so if you create an Enumeration (and keep it thread-local), but
> the backing Vector changes on some other thread, hasMoreElements() is
> broken. I think that method needs to be synchronized on Vector.this.
>
> On Thu, Jul 19, 2012 at 12:11 PM, Joe Bowbeer <joe.bowbeer at gmail.com>wrote:
>
>> The Enumeration API is not suitable for multi-threaded use because there
>> is a window for change between the hasMoreElements call and the subsequent
>> nextElement call.
>>
>> In almost all cases the Enumeration is consumed in the same thread in
>> which it is produced.
>>
>> The strange corner case that you envision is: one thread creates an
>> Enumeration and hands it to another thread.  The other thread calls
>> hasNextElement and may see a stale value for elementCount, causing it to
>> end too early (not calling nextElement) or to fail unexpectedly when it
>> does call nextElement.
>>
>> With this scenario in mind, using "synchronized" in hasMoreElements is
>> more correct, though in practice I doubt it will make any difference.
>>
>> Joe
>>
>> On Thu, Jul 19, 2012 at 8:12 AM, Praveen Kumar Jha wrote:
>>
>> Hello,
>>>
>>> Please help me in understanding the thread-safety of hasMoreElements()
>>> of Vector from visibility point of view. The hasMoreElements()
>>> accesses the 'elementCount' field without synchronizing over
>>> 'Vector.this', like it is done in nextElement() method.
>>>
>>> public Enumeration<E> elements() {
>>>     return new Enumeration<E>() {
>>>         int count = 0;
>>>
>>>         public boolean hasMoreElements() {
>>>         return count < elementCount;
>>>         }
>>>
>>>         public E nextElement() {
>>>         synchronized (Vector.this) {
>>>             if (count < elementCount) {
>>>             return (E)elementData[count++];
>>>             }
>>>         }
>>>         throw new NoSuchElementException("Vector Enumeration");
>>>         }
>>>     };
>>>     }
>>>
>>> This might cause a thread to see stale value of 'elementCount' and thus
>>> hasMoreElements() might incorrectly return true or false.
>>> Javadoc of neither Enumeration nor elements() of Vector says anything
>>> about thread-safety.
>>>
>>> Regards,
>>> Praveen
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120719/5d913a8a/attachment-0001.html>

From alexlamsl at gmail.com  Thu Jul 19 13:31:52 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Thu, 19 Jul 2012 18:31:52 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
	new thread
In-Reply-To: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
Message-ID: <CAGpACNs6D9XmNPOfQgHwpnsJvEw8zvQcYT7dybNqjLW8_Mxh6Q@mail.gmail.com>

I have cut it down to a simpler (but arguably not equivalent) example
and see no effective difference in behaviour (blocked in
ForkJoinTask.join() instead of BlockingQueue.take()):

  ForkJoinPool pool = new ForkJoinPool();
  BlockingQueue<Future<V>> queue = new LinkedBlockingQueue<>();
  Callable<V> workload = ...;

  final int N = ...;

  pool.execute(new Runnable() {
    public void run() {
      for (int i = 0; i < N; i++)
        queue.add(pool.submit(workload));

      for (int i = 0; i < N; i++) {
        process(queue.poll().join());
      }
    }
  });

I have also made some measurements using AtomicXXX and
System.nanoTime() on the first version of the code, i.e. the one which
uses ManagedBlocker.

Under my specific workload, ManagedBlocker.block() is being called
~75% of the time, with an average duration of ~3.5ms when called, i.e.
does not count when ManagedBlocker.isReleasable() returns true.


Alex.



On Thu, Jul 19, 2012 at 4:41 PM, Alex Lam S.L. <alexlamsl at gmail.com> wrote:
> I am trying to get my application to use ForkJoinPool. Specifically, I
> want it to automatically occupy all 8 CPUs even when one of the thread
> is blocked by BlockingQueue.take() - below is an implementation based
> on what I can understand from the javadocs:
>
>
>   ForkJoinPool pool = new ForkJoinPool();
>   BlockingQueue<V> queue = new LinkedBlockingQueue<>();
>   Callable<V> workload = ...;
>
>   class QueueBlocker<V> implements ForkJoinPool.ManagedBlocker {
>     private V value;
>
>     public V get() {
>       return value;
>     }
>
>     public boolean block() throws InterruptedException {
>       value = queue.take();
>       return true;
>     }
>
>     public boolean isReleasable() {
>       return value != null || (value = queue.poll()) != null;
>     }
>   }
>
>   class Task<V> extends ForkJoinTask<Void> {
>     private final Callable<V> callable;
>
>     Task(Callable<V> callable) {
>       this.callable = callable;
>     }
>
>     public Void getRawResult() {
>       return null;
>     }
>
>     protected void setRawResultVoid value) {
>     }
>
>     protected boolean exec() {
>       try {
>         queue.add(callable.call());
>         return true;
>       } catch (Exception ex) {
>         throw new RuntimeException(ex);
>       }
>     }
>   }
>
>   final int N = ...;
>
>   pool.execute(new Runnable() {
>     public void run() {
>       for (int i = 0; i < N; i++)
>         pool.execute(new Task<V>(workload));
>
>       for (int i = 0; i < N; i++) {
>         final QueueBlocker blocker = new QueueBlocker();
>         ForkJoinPool.managedBlock(blocker);
>         process(blocker.get());
>       }
>     }
>   });
>
>
> Now on my 8-core machine, I can see from JConsole that FJ pool have 8
> threads, with one of them consistently blocking on queue.take(), thus
> only leaving 7 CPUs busy.
>
> Is this behaviour intentional? If so, what am I doing wrong here?
>
>
> Thanks,
> Alex.

From dl at cs.oswego.edu  Thu Jul 19 13:52:49 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 19 Jul 2012 13:52:49 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
 new	thread
In-Reply-To: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
Message-ID: <50084971.7080708@cs.oswego.edu>

On 07/19/12 11:41, Alex Lam S.L. wrote:
> I am trying to get my application to use ForkJoinPool. Specifically, I
> want it to automatically occupy all 8 CPUs even when one of the thread
> is blocked by BlockingQueue.take() - below is an implementation based
> on what I can understand from the javadocs:

>
> Now on my 8-core machine, I can see from JConsole that FJ pool have 8
> threads, with one of them consistently blocking on queue.take(), thus
> only leaving 7 CPUs busy.
>
> Is this behaviour intentional? If so, what am I doing wrong here?
>

It is intentional that the ManagedBlocker API not continuously
spawn threads -- this leads to positive feedback loops that
can lead to unbounded resource usage. This also means that
it might not spawn a thread when you might think it should.

See discussions about CountedCompleters on this list for a
generally more well-behaved approach to tasks that may block.

Additionally, or alternatively, you can pad the pool size to a
value that better estimates actual CPU-intensive load. In general,
work-stealing has the property that as you use more threads than
CPU cores, performance only slowly degrades due to more context switching.
So, if you can make estimates are not grossly wrong, this may
lead to better average utilization and throughput.

-Doug

From alexlamsl at gmail.com  Thu Jul 19 15:05:26 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Thu, 19 Jul 2012 20:05:26 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
 new thread
In-Reply-To: <50084971.7080708@cs.oswego.edu>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
	<50084971.7080708@cs.oswego.edu>
Message-ID: <CAGpACNt3H7XoWn9pgimQMDWs0EYfaL3JgocpDkcXJScct9szLg@mail.gmail.com>

On Thu, Jul 19, 2012 at 6:52 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> It is intentional that the ManagedBlocker API not continuously
> spawn threads -- this leads to positive feedback loops that
> can lead to unbounded resource usage. This also means that
> it might not spawn a thread when you might think it should.
>
> See discussions about CountedCompleters on this list for a
> generally more well-behaved approach to tasks that may block.
>
> Additionally, or alternatively, you can pad the pool size to a
> value that better estimates actual CPU-intensive load. In general,
> work-stealing has the property that as you use more threads than
> CPU cores, performance only slowly degrades due to more context switching.
> So, if you can make estimates are not grossly wrong, this may
> lead to better average utilization and throughput.

Thank you very much for the detailed explanation - CountedCompleters
doesn't quite match my usage pattern in this case, at least not I can
think of at the moment.

For the record, I went with the following approach instead:

  class QueueBlocker<V> implements ForkJoinPool.ManagedBlocker {
    ...

    public boolean block() throws InterruptedException {
      if (Task.steal()) {
        return false;
      } else {
        value = queue.take();
        return true;
      }
    }
  }

  class Task<V> extends ForkJoinTask<Void> {
    ...

    public static boolean steal() {
      final ForkJoinTask<?> task = ForkJoinTask.pollTask();
      if (task == null) {
        return false;
      } else {
        task.invoke();
        return true;
      }
    }
  }

And now my program is running at 100% CPU, i.e. I'm struggling to type
this email with GMail interface!


Alex.

From dl at cs.oswego.edu  Fri Jul 20 06:23:25 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 20 Jul 2012 06:23:25 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
 new thread
In-Reply-To: <CAGpACNt3H7XoWn9pgimQMDWs0EYfaL3JgocpDkcXJScct9szLg@mail.gmail.com>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>	<50084971.7080708@cs.oswego.edu>
	<CAGpACNt3H7XoWn9pgimQMDWs0EYfaL3JgocpDkcXJScct9szLg@mail.gmail.com>
Message-ID: <5009319D.4070100@cs.oswego.edu>

On 07/19/12 15:05, Alex Lam S.L. wrote:

> For the record, I went with the following approach instead:
>
>    class QueueBlocker<V>  implements ForkJoinPool.ManagedBlocker {
>      ...
>
>      public boolean block() throws InterruptedException {
>        if (Task.steal()) {
>          return false;
>        } else {
>          value = queue.take();
>          return true;
>        }
>      }
>    }
>

Yes. We cannot do anything like this in general, because
the steals here might violate task dependencies -- joining
the stolen task could block waiting for the current action
to finish, which it never will. But so long
as you know that this is not possible, it can work well.

-Doug



From jason.greene at redhat.com  Fri Jul 20 14:42:34 2012
From: jason.greene at redhat.com (Jason T. Greene)
Date: Fri, 20 Jul 2012 13:42:34 -0500
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEJIJFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEJIJFAA.davidcholmes@aapt.net.au>
Message-ID: <5009A69A.3080107@redhat.com>

On 7/15/12 9:52 PM, David Holmes wrote:
> David M. Lloyd writes:
>> On 07/15/2012 09:35 PM, David Holmes wrote:
>>> David M. Lloyd writes:
>>>> On 07/15/2012 06:14 AM, David Holmes wrote:
>>>>> David M. Lloyd writes:
>>>>>> What is the purpose of the access-time access check in the
>>>>>> atomic field updater classes?
>>>>>
>>>>> Do you mean the ensureProtectedAccess check?
>>>>>
>>>>> This is to ensure that you can't get an updater for a protected
>>>>> inherited field in SubtypeA, then use that updater to
>>>>> access/modify the field in an instance of SubtypeB.
>>>>
>>>> Which is ridiculous because if a subclass "inherited" access
>>>> to a normal
>>>> protected field on my class, I'd still be able to access it directly in
>>>> the base class... because fields aren't actually inherited.
>>>
>>> I think you are missing the point. Consider this:
>>>
>>> class Base {
>>>     protected volatile int x;
>>> }
>>>
>>> final class Secure extends Base { ... }
>>>
>>> Instances of Secure are only supposed to be used through their
>> public API,
>>> as defined by Base and Secure. I hope you will agree that if I have an
>>> instance of Secure I shouldn't be able to arbitrarily mess with
>> the value of
>>> x?
>>>
>>> But given
>>>
>>> class BackDoor extends Base {
>>>      static AtomicIntegerFieldUpdater<Base>  u = ... (Base.class, "x");
>>>
>>>      public static void setBaseX(Base target, int newX) {
>>>         u.set(target, newX);
>>>      }
>>> }
>>>
>>> without these protected access checks, BackDoor.setBaseX would
>> allow you to
>>> update _any_ instance of Base.
>>
>> I see your point, but in this case, wouldn't it be better to restrict
>> updaters from being created on any class but their own, regardless of
>> the access of the field?  In other words, we'd allow:
>>
>>      static AtomicIntegerFieldUpdater<BackDoor> u = ... (Backdoor.class,
>> "x");
>>
>> but not:
>>
>>      static AtomicIntegerFieldUpdater<Base> u = ... (Base.class, "x");
>>
>> from BackDoor.
>
> In hindsight, in the context of the current discussion, it may have been
> better to restrict things this way to allow access checks to be elided. But
> that has to be weighed up against use-cases where it is an inherited field
> that needs the atomic update.

I still don't get why this needs to operate any differently than 
reflection. With reflection I can do the same thing in Backdoor. I just 
make a call to setAccessible, and a security check is done with a 
security manager. Why can't the updaters do the same?


-- 
Jason T. Greene
JBoss AS Lead / EAP Platform Architect
JBoss, a division of Red Hat



From alexlamsl at gmail.com  Fri Jul 20 14:49:10 2012
From: alexlamsl at gmail.com (Alex Lam S.L.)
Date: Fri, 20 Jul 2012 19:49:10 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
 new thread
In-Reply-To: <5009319D.4070100@cs.oswego.edu>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
	<50084971.7080708@cs.oswego.edu>
	<CAGpACNt3H7XoWn9pgimQMDWs0EYfaL3JgocpDkcXJScct9szLg@mail.gmail.com>
	<5009319D.4070100@cs.oswego.edu>
Message-ID: <CAGpACNvAi9GE5q+dDZRAFnku_Eg5cshv2dxdFu6_OvHB6U-nmA@mail.gmail.com>

On Fri, Jul 20, 2012 at 11:23 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> Yes. We cannot do anything like this in general, because
> the steals here might violate task dependencies -- joining
> the stolen task could block waiting for the current action
> to finish, which it never will. But so long
> as you know that this is not possible, it can work well.

Update: after some wall clock measurements, the naive version (which
only uses ~88% CPU) is not statistically slower (i.e. sometimes
faster) than the Task.steal() version.

So though I feel a bit surprised by it, I think I will live with the
simpler code and have some spare CPU cycles for typing emails :-)


Alex.

From viktor.klang at gmail.com  Fri Jul 20 15:00:46 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 20 Jul 2012 21:00:46 +0200
Subject: [concurrency-interest] ForkJoinPool.managedBlock() not spawning
 new thread
In-Reply-To: <CAGpACNvAi9GE5q+dDZRAFnku_Eg5cshv2dxdFu6_OvHB6U-nmA@mail.gmail.com>
References: <CAGpACNtNpJ6-NS9RsDeobPBN7P-sDDHC1GF7=kWonrL13ZpQug@mail.gmail.com>
	<50084971.7080708@cs.oswego.edu>
	<CAGpACNt3H7XoWn9pgimQMDWs0EYfaL3JgocpDkcXJScct9szLg@mail.gmail.com>
	<5009319D.4070100@cs.oswego.edu>
	<CAGpACNvAi9GE5q+dDZRAFnku_Eg5cshv2dxdFu6_OvHB6U-nmA@mail.gmail.com>
Message-ID: <CANPzfU_w68asDRPhEChiD7gF2EmVzM07ybysAKz2Em5WLL29oA@mail.gmail.com>

On Fri, Jul 20, 2012 at 8:49 PM, Alex Lam S.L. <alexlamsl at gmail.com> wrote:

> On Fri, Jul 20, 2012 at 11:23 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> > Yes. We cannot do anything like this in general, because
> > the steals here might violate task dependencies -- joining
> > the stolen task could block waiting for the current action
> > to finish, which it never will. But so long
> > as you know that this is not possible, it can work well.
>
> Update: after some wall clock measurements, the naive version (which
> only uses ~88% CPU) is not statistically slower (i.e. sometimes
> faster) than the Task.steal() version.
>
> So though I feel a bit surprised by it, I think I will live with the
> simpler code and have some spare CPU cycles for typing emails :-)
>

Wisdom of the day: Busy != Producing value


>
>
> Alex.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120720/d6d59d36/attachment.html>

From davidcholmes at aapt.net.au  Fri Jul 20 21:34:08 2012
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 21 Jul 2012 11:34:08 +1000
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <5009A69A.3080107@redhat.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOELCJFAA.davidcholmes@aapt.net.au>

Hi Jason,

> Jason T. Greene writes:
> On 7/15/12 9:52 PM, David Holmes wrote:
> > David M. Lloyd writes:
> >> On 07/15/2012 09:35 PM, David Holmes wrote:
> >>> David M. Lloyd writes:
> >>>> On 07/15/2012 06:14 AM, David Holmes wrote:
> >>>>> David M. Lloyd writes:
> >>>>>> What is the purpose of the access-time access check in the
> >>>>>> atomic field updater classes?
> >>>>>
> >>>>> Do you mean the ensureProtectedAccess check?
> >>>>>
> >>>>> This is to ensure that you can't get an updater for a protected
> >>>>> inherited field in SubtypeA, then use that updater to
> >>>>> access/modify the field in an instance of SubtypeB.
> >>>>
> >>>> Which is ridiculous because if a subclass "inherited" access
> >>>> to a normal
> >>>> protected field on my class, I'd still be able to access it
> directly in
> >>>> the base class... because fields aren't actually inherited.
> >>>
> >>> I think you are missing the point. Consider this:
> >>>
> >>> class Base {
> >>>     protected volatile int x;
> >>> }
> >>>
> >>> final class Secure extends Base { ... }
> >>>
> >>> Instances of Secure are only supposed to be used through their
> >> public API,
> >>> as defined by Base and Secure. I hope you will agree that if I have an
> >>> instance of Secure I shouldn't be able to arbitrarily mess with
> >> the value of
> >>> x?
> >>>
> >>> But given
> >>>
> >>> class BackDoor extends Base {
> >>>      static AtomicIntegerFieldUpdater<Base>  u = ...
> (Base.class, "x");
> >>>
> >>>      public static void setBaseX(Base target, int newX) {
> >>>         u.set(target, newX);
> >>>      }
> >>> }
> >>>
> >>> without these protected access checks, BackDoor.setBaseX would
> >> allow you to
> >>> update _any_ instance of Base.
> >>
> >> I see your point, but in this case, wouldn't it be better to restrict
> >> updaters from being created on any class but their own, regardless of
> >> the access of the field?  In other words, we'd allow:
> >>
> >>      static AtomicIntegerFieldUpdater<BackDoor> u = ...
> (Backdoor.class,
> >> "x");
> >>
> >> but not:
> >>
> >>      static AtomicIntegerFieldUpdater<Base> u = ... (Base.class, "x");
> >>
> >> from BackDoor.
> >
> > In hindsight, in the context of the current discussion, it may have been
> > better to restrict things this way to allow access checks to be
> elided. But
> > that has to be weighed up against use-cases where it is an
> inherited field that needs the atomic update.
>
> I still don't get why this needs to operate any differently than
> reflection. With reflection I can do the same thing in Backdoor. I just
> make a call to setAccessible, and a security check is done with a
> security manager. Why can't the updaters do the same?

Because it is not reflection. Reflection is (in part) a mechanism to step
outside the language to do things the language wouldn't normally permit. The
atomic updaters are intended as an extension to normal field operations
(putfield/getfield) and so they enforce language level access controls.

David

>
> --
> Jason T. Greene
> JBoss AS Lead / EAP Platform Architect
> JBoss, a division of Red Hat
>
>
>


From david.lloyd at redhat.com  Sun Jul 22 16:47:57 2012
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Sun, 22 Jul 2012 15:47:57 -0500
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOELCJFAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOELCJFAA.davidcholmes@aapt.net.au>
Message-ID: <500C66FD.9080605@redhat.com>

On 07/20/2012 08:34 PM, David Holmes wrote:
> Hi Jason,
>
>> Jason T. Greene writes:
>> On 7/15/12 9:52 PM, David Holmes wrote:
>>> David M. Lloyd writes:
>>>> On 07/15/2012 09:35 PM, David Holmes wrote:
>>>>> David M. Lloyd writes:
>>>>>> On 07/15/2012 06:14 AM, David Holmes wrote:
>>>>>>> David M. Lloyd writes:
>>>>>>>> What is the purpose of the access-time access check in the
>>>>>>>> atomic field updater classes?
>>>>>>>
>>>>>>> Do you mean the ensureProtectedAccess check?
>>>>>>>
>>>>>>> This is to ensure that you can't get an updater for a protected
>>>>>>> inherited field in SubtypeA, then use that updater to
>>>>>>> access/modify the field in an instance of SubtypeB.
>>>>>>
>>>>>> Which is ridiculous because if a subclass "inherited" access
>>>>>> to a normal
>>>>>> protected field on my class, I'd still be able to access it
>> directly in
>>>>>> the base class... because fields aren't actually inherited.
>>>>>
>>>>> I think you are missing the point. Consider this:
>>>>>
>>>>> class Base {
>>>>>      protected volatile int x;
>>>>> }
>>>>>
>>>>> final class Secure extends Base { ... }
>>>>>
>>>>> Instances of Secure are only supposed to be used through their
>>>> public API,
>>>>> as defined by Base and Secure. I hope you will agree that if I have an
>>>>> instance of Secure I shouldn't be able to arbitrarily mess with
>>>> the value of
>>>>> x?
>>>>>
>>>>> But given
>>>>>
>>>>> class BackDoor extends Base {
>>>>>       static AtomicIntegerFieldUpdater<Base>   u = ...
>> (Base.class, "x");
>>>>>
>>>>>       public static void setBaseX(Base target, int newX) {
>>>>>          u.set(target, newX);
>>>>>       }
>>>>> }
>>>>>
>>>>> without these protected access checks, BackDoor.setBaseX would
>>>> allow you to
>>>>> update _any_ instance of Base.
>>>>
>>>> I see your point, but in this case, wouldn't it be better to restrict
>>>> updaters from being created on any class but their own, regardless of
>>>> the access of the field?  In other words, we'd allow:
>>>>
>>>>       static AtomicIntegerFieldUpdater<BackDoor>  u = ...
>> (Backdoor.class,
>>>> "x");
>>>>
>>>> but not:
>>>>
>>>>       static AtomicIntegerFieldUpdater<Base>  u = ... (Base.class, "x");
>>>>
>>>> from BackDoor.
>>>
>>> In hindsight, in the context of the current discussion, it may have been
>>> better to restrict things this way to allow access checks to be
>> elided. But
>>> that has to be weighed up against use-cases where it is an
>> inherited field that needs the atomic update.
>>
>> I still don't get why this needs to operate any differently than
>> reflection. With reflection I can do the same thing in Backdoor. I just
>> make a call to setAccessible, and a security check is done with a
>> security manager. Why can't the updaters do the same?
>
> Because it is not reflection. Reflection is (in part) a mechanism to step
> outside the language to do things the language wouldn't normally permit. The
> atomic updaters are intended as an extension to normal field operations
> (putfield/getfield) and so they enforce language level access controls.

That is sort of baloney logic.  The atomic updaters are intended as a 
way to implement atomic operations, which are used when performance is a 
priority.  Trying to make it smell like a language feature is very 
misguided IMO; it is far more consistent with the language *and* the 
purpose of the class if the protection of the object itself was used for 
the purpose of protecting the field.

You don't have to give things extra runtime behavior in the name of 
being able to "step outside of the language".  I think it takes a fairly 
imaginative interpretation of the JLS to arrive at the conclusion that 
object classes which act like language features must act in the exact 
same way as the language.  The language doesn't include atomic field 
accesses; does this then mean that by this logic, specifying classes 
which do include this functionality should be disallowed?

If you really want extensions to normal field operations that act just 
like the language then they have to be in the language - otherwise that 
card just isn't playable.  It doesn't add anything useful to the 
specification; on the contrary it adds unreasonable and useless 
constraints.  If adding this functionality to the language *isn't* an 
option, then it makes far more sense to conform idiomatically to the 
mechanism that *is* available, which is an accessor object, which the 
user should be allowed to use as they set fit.

-- 
- DML

From dl at cs.oswego.edu  Mon Jul 23 06:44:28 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 23 Jul 2012 06:44:28 -0400
Subject: [concurrency-interest] The Atomic*FieldUpdater situation
In-Reply-To: <500C66FD.9080605@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCOELCJFAA.davidcholmes@aapt.net.au>
	<500C66FD.9080605@redhat.com>
Message-ID: <500D2B0C.7000503@cs.oswego.edu>

On 07/22/12 16:47, David M. Lloyd wrote:
>>>
>>> I still don't get why this needs to operate any differently than
>>> reflection. With reflection I can do the same thing in Backdoor. I just
>>> make a call to setAccessible, and a security check is done with a
>>> security manager. Why can't the updaters do the same?

This is a good point. Among the ideas for internal overhaul
would be to build special versions for fields that have been
marked setAccessible.

I am holding out some hope that between this and use of
upcoming indy extensions, we might be able to provide
much better versions. (Although ultimately, the only
way to do this completely seamlessly would be to add
bytecodes and front-end compiler support.)

Until then, I'm not especially tempted to create band-aid version.
The mechanics available are very fragile. When introducing and
revising these, we've had many exchanges with Java security folks
who are insistent about the use of particular constructions.
They do tend to be very conservative, but that's mainly
because this is hard to get exactly right, especially in
the presence of erased generics.

However, in the mean time, it would be completely possible for
you to create your own AccessibleAtomicXFieldUpdater classes
(so long as you have access to Unsafe) and use them.

-Doug

From stanimir at riflexo.com  Mon Jul 23 10:27:40 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 23 Jul 2012 17:27:40 +0300
Subject: [concurrency-interest] subList, subMap and navigable
Message-ID: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>

It looks subMaps returned by the implementers of SortedMap are serializable
as rule of the thumb. All subLists returned by COWArrayList, ArrayList,
LinkedList, etc are not. The latter is a major issue with RPC since if
methods accept just java.util.List it'd be excepted to work remotely just
as they work in the same JVM. In our case it was rarely a subList to be
serialized, so it was not caught early on.

The behavior clearly favors SortedMap for unknown reason, Maps can be just
as big or even bigger than Lists (List can have more than 2^31 elementes or
all get/set, ListIterator fail)
While I realize  it's an official bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922 I do not quite
understand the closing reason - writeReplace is always an option to
minimize serialization footprint. Also SortedMaps (like TreeMap) do not
even try to optimize that.
Is there any objective reason the bug/RFE to still be considered "closed,
will not implemented" [as COW follows the same path]?

Another  question, based mostly on curiosity. Was NavigableSet/Map
introduced in JDK1.6 to mostly accommodate the CSLM or if was fixing
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4155650 ?


Thanks
Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/67fc8bc0/attachment.html>

From viktor.klang at gmail.com  Mon Jul 23 10:48:38 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 23 Jul 2012 16:48:38 +0200
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
Message-ID: <CANPzfU-Vy4aAN7eXOvDQH5yDL-j5=Hgk1G7ebJL_pa+ou0sAaA@mail.gmail.com>

On Mon, Jul 23, 2012 at 4:27 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:

> It looks subMaps returned by the implementers of SortedMap are
> serializable as rule of the thumb. All subLists returned by COWArrayList,
> ArrayList, LinkedList, etc are not. The latter is a major issue with RPC
> since if methods accept just java.util.List it'd be excepted to work
> remotely just as they work in the same JVM. In our case it was rarely a
> subList to be serialized, so it was not caught early on.
>
> The behavior clearly favors SortedMap for unknown reason, Maps can be just
> as big or even bigger than Lists (List can have more than 2^31 elementes or
> all get/set, ListIterator fail)
> While I realize  it's an official bug:
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922 I do not quite
> understand the closing reason - writeReplace is always an option to
> minimize serialization footprint. Also SortedMaps (like TreeMap) do not
> even try to optimize that.
> Is there any objective reason the bug/RFE to still be considered "closed,
> will not implemented" [as COW follows the same path]?
>
> Another  question, based mostly on curiosity. Was NavigableSet/Map
> introduced in JDK1.6 to mostly accommodate the CSLM or if was fixing
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4155650 ?
>
>
Using Java Serialization is definitely not something you want to use for
RPC for performance reasons.

Cheers,
?


>
> Thanks
> Stanimir
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/c567372b/attachment.html>

From stanimir at riflexo.com  Mon Jul 23 11:01:52 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 23 Jul 2012 18:01:52 +0300
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CANPzfU-Vy4aAN7eXOvDQH5yDL-j5=Hgk1G7ebJL_pa+ou0sAaA@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<CANPzfU-Vy4aAN7eXOvDQH5yDL-j5=Hgk1G7ebJL_pa+ou0sAaA@mail.gmail.com>
Message-ID: <CAEJX8opYwUM9J=_VGMBeQ6BAzdfB-VhBE=dP1M3RoQZ-yDspWQ@mail.gmail.com>

Well, for custom objects and interfaces you can't get much better than
serialization [w/o adding boiler plate code at least]. If the performance
of serialization is still the bootleneck, one can opt for Externalizeable.
Again, I am not concerned with performance in that aspect at all.

Stanimir

On Mon, Jul 23, 2012 at 5:48 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

>
>
> On Mon, Jul 23, 2012 at 4:27 PM, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> It looks subMaps returned by the implementers of SortedMap are
>> serializable as rule of the thumb. All subLists returned by COWArrayList,
>> ArrayList, LinkedList, etc are not. The latter is a major issue with RPC
>> since if methods accept just java.util.List it'd be excepted to work
>> remotely just as they work in the same JVM. In our case it was rarely a
>> subList to be serialized, so it was not caught early on.
>>
>> The behavior clearly favors SortedMap for unknown reason, Maps can be
>> just as big or even bigger than Lists (List can have more than 2^31
>> elementes or all get/set, ListIterator fail)
>> While I realize  it's an official bug:
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922 I do not
>> quite understand the closing reason - writeReplace is always an option to
>> minimize serialization footprint. Also SortedMaps (like TreeMap) do not
>> even try to optimize that.
>> Is there any objective reason the bug/RFE to still be considered "closed,
>> will not implemented" [as COW follows the same path]?
>>
>> Another  question, based mostly on curiosity. Was NavigableSet/Map
>> introduced in JDK1.6 to mostly accommodate the CSLM or if was fixing
>> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4155650 ?
>>
>>
> Using Java Serialization is definitely not something you want to use for
> RPC for performance reasons.
>
> Cheers,
> ?
>
>
>>
>> Thanks
>> Stanimir
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Viktor Klang
>
> Akka Tech Lead
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
>
> Twitter: @viktorklang
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/fd0e3b9d/attachment-0001.html>

From gregg at cytetech.com  Mon Jul 23 12:11:02 2012
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 23 Jul 2012 11:11:02 -0500
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CAEJX8opYwUM9J=_VGMBeQ6BAzdfB-VhBE=dP1M3RoQZ-yDspWQ@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<CANPzfU-Vy4aAN7eXOvDQH5yDL-j5=Hgk1G7ebJL_pa+ou0sAaA@mail.gmail.com>
	<CAEJX8opYwUM9J=_VGMBeQ6BAzdfB-VhBE=dP1M3RoQZ-yDspWQ@mail.gmail.com>
Message-ID: <500D7796.5010604@cytetech.com>

Serializable can be fragile and slow, when you just blindly use it.  But if you 
manage object version etc., you can make Serialization be quite performant for 
RPC stuff.  I use it over a TCP Stream (in a Jini application) which blows 
through 1000's of events per second, no problem.  It really depends on what you 
are wrapping in the serialization.

Gregg Wonderly

On 7/23/2012 10:01 AM, Stanimir Simeonoff wrote:
> Well, for custom objects and interfaces you can't get much better than
> serialization [w/o adding boiler plate code at least]. If the performance of
> serialization is still the bootleneck, one can opt for Externalizeable. Again, I
> am not concerned with performance in that aspect at all.
>
> Stanimir
>
> On Mon, Jul 23, 2012 at 5:48 PM, ?iktor ?lang <viktor.klang at gmail.com
> <mailto:viktor.klang at gmail.com>> wrote:
>
>
>
>     On Mon, Jul 23, 2012 at 4:27 PM, Stanimir Simeonoff <stanimir at riflexo.com
>     <mailto:stanimir at riflexo.com>> wrote:
>
>         It looks subMaps returned by the implementers of SortedMap are
>         serializable as rule of the thumb. All subLists returned by
>         COWArrayList, ArrayList, LinkedList, etc are not. The latter is a major
>         issue with RPC since if methods accept just java.util.List it'd be
>         excepted to work remotely just as they work in the same JVM. In our case
>         it was rarely a subList to be serialized, so it was not caught early on.
>
>         The behavior clearly favors SortedMap for unknown reason, Maps can be
>         just as big or even bigger than Lists (List can have more than 2^31
>         elementes or all get/set, ListIterator fail)
>         While I realize  it's an official bug:
>         http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922 I do not
>         quite understand the closing reason - writeReplace is always an option
>         to minimize serialization footprint. Also SortedMaps (like TreeMap) do
>         not even try to optimize that.
>         Is there any objective reason the bug/RFE to still be considered
>         "closed, will not implemented" [as COW follows the same path]?
>
>         Another  question, based mostly on curiosity. Was NavigableSet/Map
>         introduced in JDK1.6 to mostly accommodate the CSLM or if was fixing
>         http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4155650 ?
>
>
>     Using Java Serialization is definitely not something you want to use for RPC
>     for performance reasons.
>
>     Cheers,
>     ?
>
>
>         Thanks
>         Stanimir
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>     --
>     Viktor Klang
>
>     Akka Tech Lead
>     Typesafe <http://www.typesafe.com/>- The software stack for applications
>     that scale
>
>     Twitter: @viktorklang
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From viktor.klang at gmail.com  Mon Jul 23 12:13:23 2012
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 23 Jul 2012 18:13:23 +0200
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <500D7796.5010604@cytetech.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<CANPzfU-Vy4aAN7eXOvDQH5yDL-j5=Hgk1G7ebJL_pa+ou0sAaA@mail.gmail.com>
	<CAEJX8opYwUM9J=_VGMBeQ6BAzdfB-VhBE=dP1M3RoQZ-yDspWQ@mail.gmail.com>
	<500D7796.5010604@cytetech.com>
Message-ID: <CANPzfU-UweSWmoqHrvwLzz3rnaZMJNUyYBh=vksijZC3axxWqw@mail.gmail.com>

On Mon, Jul 23, 2012 at 6:11 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> Serializable can be fragile and slow, when you just blindly use it.  But
> if you manage object version etc., you can make Serialization be quite
> performant for RPC stuff.  I use it over a TCP Stream (in a Jini
> application) which blows through 1000's of events per second, no problem.


That's a quite modest load though. And Single-threaded I presume?


>  It really depends on what you are wrapping in the serialization.
>

Well of course.

Cheers,
?


>
> Gregg Wonderly
>
>
> On 7/23/2012 10:01 AM, Stanimir Simeonoff wrote:
>
>> Well, for custom objects and interfaces you can't get much better than
>> serialization [w/o adding boiler plate code at least]. If the performance
>> of
>> serialization is still the bootleneck, one can opt for Externalizeable.
>> Again, I
>> am not concerned with performance in that aspect at all.
>>
>> Stanimir
>>
>> On Mon, Jul 23, 2012 at 5:48 PM, ?iktor ?lang <viktor.klang at gmail.com
>> <mailto:viktor.klang at gmail.com**>> wrote:
>>
>>
>>
>>     On Mon, Jul 23, 2012 at 4:27 PM, Stanimir Simeonoff <
>> stanimir at riflexo.com
>>     <mailto:stanimir at riflexo.com>> wrote:
>>
>>         It looks subMaps returned by the implementers of SortedMap are
>>         serializable as rule of the thumb. All subLists returned by
>>         COWArrayList, ArrayList, LinkedList, etc are not. The latter is a
>> major
>>         issue with RPC since if methods accept just java.util.List it'd be
>>         excepted to work remotely just as they work in the same JVM. In
>> our case
>>         it was rarely a subList to be serialized, so it was not caught
>> early on.
>>
>>         The behavior clearly favors SortedMap for unknown reason, Maps
>> can be
>>         just as big or even bigger than Lists (List can have more than
>> 2^31
>>         elementes or all get/set, ListIterator fail)
>>         While I realize  it's an official bug:
>>         http://bugs.sun.com/**bugdatabase/view_bug.do?bug_**id=4782922<http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922>I do not
>>         quite understand the closing reason - writeReplace is always an
>> option
>>         to minimize serialization footprint. Also SortedMaps (like
>> TreeMap) do
>>         not even try to optimize that.
>>         Is there any objective reason the bug/RFE to still be considered
>>         "closed, will not implemented" [as COW follows the same path]?
>>
>>         Another  question, based mostly on curiosity. Was NavigableSet/Map
>>         introduced in JDK1.6 to mostly accommodate the CSLM or if was
>> fixing
>>         http://bugs.sun.com/**bugdatabase/view_bug.do?bug_**id=4155650<http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4155650>?
>>
>>
>>     Using Java Serialization is definitely not something you want to use
>> for RPC
>>     for performance reasons.
>>
>>     Cheers,
>>     ?
>>
>>
>>         Thanks
>>         Stanimir
>>
>>         ______________________________**_________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>
>>         http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>     --
>>     Viktor Klang
>>
>>     Akka Tech Lead
>>     Typesafe <http://www.typesafe.com/>- The software stack for
>> applications
>>     that scale
>>
>>     Twitter: @viktorklang
>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>


-- 
Viktor Klang

Akka Tech Lead
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/6bace3b1/attachment.html>

From dl at cs.oswego.edu  Mon Jul 23 12:20:18 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 23 Jul 2012 12:20:18 -0400
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
Message-ID: <500D79C2.7040406@cs.oswego.edu>

On 07/23/12 10:27, Stanimir Simeonoff wrote:
> It looks subMaps returned by the implementers of SortedMap are serializable as
> rule of the thumb. All subLists returned by COWArrayList, ArrayList, LinkedList,
> etc are not.

The reason for not serializing views (sub{Set/List/Map},
Map.keySet(), etc) is given in

> While I realize  it's an official bug:
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922

There is no way to serialize a view without serializing the
full backing source. Doing so violates all reasonable expectations
about what serialization should do. So the Serialization exception
is telling you to first store the view in an actual collection/map,
and serialize that.

This has been a general (although under-explained) policy
ever since the introduction of Collections. But there
are a few classes that don't follow it.

-Doug

From stanimir at riflexo.com  Mon Jul 23 14:00:15 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 23 Jul 2012 21:00:15 +0300
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <500D79C2.7040406@cs.oswego.edu>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<500D79C2.7040406@cs.oswego.edu>
Message-ID: <CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>

.

>
> The reason for not serializing views (sub{Set/List/*Map*}
> Map.keySet(), etc) is given in


But subMaps are serializable for instance:
TreeMap:: NavigableSubMap<K,V> extends AbstractMap<K,V>
        implements NavigableMap<K,V>, java.io.Serializable {
}
AscendingSubMap extends NavigableSubMap has even serialVersionUID.

ConcurrentSkipListMap::    static final class SubMap<K,V> extends
AbstractMap<K,V>
        implements ConcurrentNavigableMap<K,V>, Cloneable,
                   java.io.Serializable {
private static final long serialVersionUID = -7647078645895051609L;
...
}

That's the greatest part of the confusion: TreeMap.subMap returns a
serializable view and when serialized it does serialize the real map -
pretty much expected behavior, it reasoning in the bug applies just as well
on Maps, not just Collecions. It's possible to use writeReplace and create
the same case TreeMap.subMap - so the class is the same would work via
readResolve. ConcurrentSkipListMap is no different - it also returns
serializeable subMaps.
None of maps' collection views are serializable though - which is
consistent with the fact that collection views are not serializable.

Btw, any insight on the 2nd question about the introduction of Navigable?

Thanks again
Stanimir

While I realize  it's an official bug:
>> http://bugs.sun.com/**bugdatabase/view_bug.do?bug_**id=4782922<http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4782922>
>>
>
> There is no way to serialize a view without serializing the
> full backing source. Doing so violates all reasonable expectations
> about what serialization should do. So the Serialization exception
> is telling you to first store the view in an actual collection/map,
> and serialize that.
>
> This has been a general (although under-explained) policy
> ever since the introduction of Collections. But there
> are a few classes that don't follow it.
>
> -Doug
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/b9393f0d/attachment.html>

From dl at cs.oswego.edu  Mon Jul 23 14:18:51 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 23 Jul 2012 14:18:51 -0400
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>	<500D79C2.7040406@cs.oswego.edu>
	<CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>
Message-ID: <500D958B.7010207@cs.oswego.edu>

On 07/23/12 14:00, Stanimir Simeonoff wrote:

> TreeMap:: NavigableSubMap<K,V> extends AbstractMap<K,V>
>          implements NavigableMap<K,V>, java.io.Serializable {

> ConcurrentSkipListMap::    static final class SubMap<K,V> extends AbstractMap<K,V>
>          implements ConcurrentNavigableMap<K,V>, Cloneable,
>                     java.io.Serializable {

I'm not sure why this was done. My fuzzy recollection is that
TreeMap initially did do for some reason, and CSLM did the same
to maintain pluggability. But in general, you can't count on
serializability of views.

>
> Btw, any insight on the 2nd question about the introduction of Navigable?

The short answer is that Navigable exists because we didn't
have anything like upcoming "defenders" -- Sorted didn't
describe all the common functionality, and there was no way
to do so except to introduce a new interface. In practice,
I'm sure "Sorted" is still used much more often than "Navigable"
as a declaration type, because most people don't need the
methods defined in Navigable but not Sorted. Plus "Navigable"
is just not a very nice name :-)

-Doug

From stanimir at riflexo.com  Mon Jul 23 15:16:16 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 23 Jul 2012 22:16:16 +0300
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <500D958B.7010207@cs.oswego.edu>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<500D79C2.7040406@cs.oswego.edu>
	<CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>
	<500D958B.7010207@cs.oswego.edu>
Message-ID: <CAEJX8ooSqFPZj-7rYaHvGprtmCWo8F8aRML0WtR8Qt3Ww17UyA@mail.gmail.com>

>
> I'm not sure why this was done. My fuzzy recollection is that
> TreeMap initially did do for some reason, and CSLM did the same
> to maintain pluggability. But in general, you can't count on
> serializability of views.
>
> I have subclasses ObjectOutputStream and implemented replaceObject to
handle not serializable views by returning a serializable one but that is a
true hack.

Imagine the case
SomeInterface{
  void process(java.util.List)
}
If the code is invoked locally it just works and not copy but supplying the
view is potentially more performance friendly. Receiving an interface that
will be invoked through some form of RPC (rmi included) breaks with
NotSerializableException. It's an IOException and unless handled separately
(ObjectStreamException) it is quite the same as broken connection, speaking
too broadly.
The real-world case was removing the first element of the list but instead
of copying it a subList was used.Not serializing also partly breaks the doc
contract: *"Any operation that expects list can be used as a range
operation by passing a subList view instead of a whole list"*
I do not expect the bug/RFE to be reopen but probably docs should reflect
the case.


btw, TreeSet.subSet() is also serializable - the subSet is just another
TreeSet [return new TreeSet<E>(m.subMap(fromElement,
fromInclusive,toElement,   toInclusive));]
Also TreeSet.writeObject follows the pattern to write the objects one by
one and on readObject creates a new TreeMap and fills it back -- so
definitely serializes only the part being viewed.
So the main offender is the subList which worsen the case - given the
TreeSet example, it's possible w/o breaking anything to be implemented for
subList.


Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120723/4af00a13/attachment.html>

From javapk at gmail.com  Tue Jul 24 11:19:33 2012
From: javapk at gmail.com (Praveen Kumar Jha)
Date: Tue, 24 Jul 2012 20:49:33 +0530
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
Message-ID: <CAKYmnUqNA64NtkrZmj53BHwM8+O3-GoDoxXuJr2AATJ0B_MPdQ@mail.gmail.com>

Hi,

Sorry for responding late. Please find my response(s) inline below:


> Date: Thu, 19 Jul 2012 09:11:15 -0700
> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Thread-safety of hasMoreElements()
> 	of	Vector

>
> The Enumeration API is not suitable for multi-threaded use because there is
> a window for change between the hasMoreElements call and the subsequent
> nextElement call.
>
> In almost all cases the Enumeration is consumed in the same thread in which
> it is produced.
>
> The strange corner case that you envision is: one thread creates an
> Enumeration and hands it to another thread.  The other thread calls
> hasNextElement and may see a stale value for elementCount, causing it to
> end too early (not calling nextElement) or to fail unexpectedly when it
> does call nextElement.
>
> With this scenario in mind, using "synchronized" in hasMoreElements is more
> correct, though in practice I doubt it will make any difference.
>
> Joe
-------------------------
> Date: Thu, 19 Jul 2012 12:40:16 -0400
> From: Yuval Shavit <yshavit at akiban.com>
> To: Joe Bowbeer <joe.bowbeer at gmail.com>
> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>

>
> It seems like it's more problematic than that. Vector.elementCount isn't
> volatile, so if you create an Enumeration (and keep it thread-local), but
> the backing Vector changes on some other thread, hasMoreElements() is
> broken. I think that method needs to be synchronized on Vector.this.
>
> ----------------------
> Date: Thu, 19 Jul 2012 09:56:04 -0700
> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>

>
> If the backing vector is changing on some other thread then Enumeration is
> not a suitable API.  (This is probably the logic applied by the implementer
> who decided not to bother sync'ing hasMoreElements.)
>
> On Thu, Jul 19, 2012 at 9:40 AM, Yuval Shavit wrote:
>



But if that is the case then why is 'Vector.this' synchronized in
nextElement() method?
As I understand, the behavior of both hasMoreElements() and
nextElement() should be on the same lines as far as use of Enumeration
in multithreaded case is concerned. And, I am considering the case
when a thread is reading Vector using Enumeration while other thread
is mutating it.





> Date: Thu, 19 Jul 2012 11:58:55 -0500
> From: Gregg Wonderly <gregg at cytetech.com>
> To: Yuval Shavit <yshavit at akiban.com>
> Cc: Joe Bowbeer <joe.bowbeer at gmail.com>,	concurrency-interest
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Thread-safety of hasMoreElements()
> 	of Vector
> Message-ID: <50083CCF.20005 at cytetech.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> On 7/19/2012 11:40 AM, Yuval Shavit wrote:
>> It seems like it's more problematic than that. Vector.elementCount isn't
>> volatile, so if you create an Enumeration (and keep it thread-local), but
>> the
>> backing Vector changes on some other thread, hasMoreElements() is broken.
>> I
>> think that method needs to be synchronized on Vector.this.
>
> If you do
>
> while (true) {
> 	boolean more;
> 	synchronized(vector) {
> 		more = vector.hasMoreElements();
> 	}
> 	if( !more) break;
>
> 	...
>
> }




If I have to synchronize over "vector" for hasMoreElements() then I
can do it for nextElement() method too while enumerating, so then why
is nextElement() synchronizing over 'Vector.this'. Shouldn't it be
left for the clients of Vector to synchronize it manually while
enumerating?




>
> Then you can "synchronize" with the state of other threads, obviously.  But
> it
> definitely looks like it's broken for that use case.  One could argue, that
> it
> would not be very productive for two threads to use an instance in this
> mode,
> without some other "happens-before" going on anyway.  For example, if you
> have a
> thread adding elements and another processing them, there should be a
> semaphore
> of some sort.  I would really be surprised to find code that used an
> instance
> without some other happens-before relationship between the two threads,
> which
> would cause the value to be correct.
>
> Gregg

From rgransberger at gmx.de  Tue Jul 24 13:37:05 2012
From: rgransberger at gmx.de (Rabea Gransberger)
Date: Tue, 24 Jul 2012 19:37:05 +0200
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <CAEJX8ooSqFPZj-7rYaHvGprtmCWo8F8aRML0WtR8Qt3Ww17UyA@mail.gmail.com>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<500D79C2.7040406@cs.oswego.edu>
	<CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>
	<500D958B.7010207@cs.oswego.edu>
	<CAEJX8ooSqFPZj-7rYaHvGprtmCWo8F8aRML0WtR8Qt3Ww17UyA@mail.gmail.com>
Message-ID: <500EDD41.8050404@gmx.de>

Am 23.07.2012 21:16, schrieb Stanimir Simeonoff:
>
> Imagine the case
> SomeInterface{
>    void process(java.util.List)
> }
> If the code is invoked locally it just works and not copy but supplying
> the view is potentially more performance friendly.

A SerializableList interface would help at this point to explicitly only 
allow Lists which are Serializable and making it clear which objects can 
be passed in and handled as expected.

But as there is no such interface, creating a new interface and 
object/proxy might not be worth it and does not really make a difference 
to your approach with ObjectOutputStream.replaceObject.

public interface SerializableList<E> extends Serializable, List<E> {
  public static class Factory {
   private Factory() {
   }

   public static <E> SerializableList<E> create(List<E> list) {
    if (Serializable.class.isInstance(list)) {
     return createProxy(list);
    } else {
     return createProxy(new ArrayList<E>(list));
    }
   }

   @SuppressWarnings("unchecked") private static <E> SerializableList<E> 
createProxy(List<E> list) {
    return (SerializableList<E>) 
Proxy.newProxyInstance(Factory.class.getClassLoader(), new Class[] 
{Serializable.class, List.class, SerializableList.class}, new 
SerializableListProxy(list));
   }
  }

  public static class SerializableListProxy implements InvocationHandler {

   private final List<?> delegate;

   public SerializableListProxy(List<?> delegate) {
    this.delegate = delegate;
   }

   @Override public Object invoke(Object proxy, Method method, Object[] 
args) throws Throwable {
    return method.invoke(this.delegate, args);
   }
  }
}

From mike.duigou at oracle.com  Tue Jul 24 14:14:21 2012
From: mike.duigou at oracle.com (Mike Duigou)
Date: Tue, 24 Jul 2012 11:14:21 -0700
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <CAKYmnUqNA64NtkrZmj53BHwM8+O3-GoDoxXuJr2AATJ0B_MPdQ@mail.gmail.com>
References: <CAKYmnUqNA64NtkrZmj53BHwM8+O3-GoDoxXuJr2AATJ0B_MPdQ@mail.gmail.com>
Message-ID: <F1405BB2-1153-49AB-9714-009148DB758C@oracle.com>


On Jul 24 2012, at 08:19 , Praveen Kumar Jha wrote:
>> 
> 
>> 
>> If the backing vector is changing on some other thread then Enumeration is
>> not a suitable API.  (This is probably the logic applied by the implementer
>> who decided not to bother sync'ing hasMoreElements.)
> 
> 
> But if that is the case then why is 'Vector.this' synchronized in
> nextElement() method?

To avoid accidentally generating a ArrayIndexOutOfBoundsException if the Vector shrinks or (just as bad) returning an element from the array that is not part of the Vector.

> As I understand, the behavior of both hasMoreElements() and
> nextElement() should be on the same lines as far as use of Enumeration
> in multithreaded case is concerned.

They could be but since the possibility of modification between hasMoreElements and nextElement exists making hasMoreElements synchronized on the vector wouldn't really solve anything. For true safety you would have to synchronize on the Vector outside of calls to hasMoreElements/nextElement. ie.

Vector vector;
Enumeration enumer = vector.elements();
synchronized(enumer) {
   while(enumer.hasMoreElements()) {
     Object object = enumer.nextElement();
     System.out.println(object);
   }
}

Without the surrounding synchronized it is possible that this snippet will.
- print all of the elements. (Yay!)
- print something odd (duplicate elements, miss elements, etc.) based upon concurrent modification
- fail with a NoSuchElementException if final element is concurrently removed after hasMoreElements() but before nextElement()

> And, I am considering the case
> when a thread is reading Vector using Enumeration while other thread
> is mutating it.

Probably a bad idea. Concurrent use of Vector should probably be limited to either non-structural modifications. ie. changes which don't change the size of the collection or very well ordered changes such as using Vector as a LIFO or FIFO (and there are better choices for this). If you are using Vector as a FIFO then perhaps just use removeElementAt(int index). Assuming only one thread is removing elements you aren't going to run into problems with atomicity in this usage.

Mike

From javapk at gmail.com  Tue Jul 24 23:57:18 2012
From: javapk at gmail.com (Praveen Kumar Jha)
Date: Wed, 25 Jul 2012 09:27:18 +0530
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <F1405BB2-1153-49AB-9714-009148DB758C@oracle.com>
References: <CAKYmnUqNA64NtkrZmj53BHwM8+O3-GoDoxXuJr2AATJ0B_MPdQ@mail.gmail.com>
	<F1405BB2-1153-49AB-9714-009148DB758C@oracle.com>
Message-ID: <CAKYmnUoQmPLVEoDSmKtCMV7bs9erCZ2kHRYTPKtrR0R7hmUSrw@mail.gmail.com>

Hi Mike,

Thanks for your response, now it makes sense for me. I suppose
you meant

synchronized('vector')  [not synchronized(enumer)]

in the example you cited.


Regards,
Praveen


On Tue, Jul 24, 2012 at 11:44 PM, Mike Duigou <mike.duigou at oracle.com>wrote:

>
> On Jul 24 2012, at 08:19 , Praveen Kumar Jha wrote:
> >>
> >
> >>
> >> If the backing vector is changing on some other thread then Enumeration
> is
> >> not a suitable API.  (This is probably the logic applied by the
> implementer
> >> who decided not to bother sync'ing hasMoreElements.)
> >
> >
> > But if that is the case then why is 'Vector.this' synchronized in
> > nextElement() method?
>
> To avoid accidentally generating a ArrayIndexOutOfBoundsException if the
> Vector shrinks or (just as bad) returning an element from the array that is
> not part of the Vector.
>
> > As I understand, the behavior of both hasMoreElements() and
> > nextElement() should be on the same lines as far as use of Enumeration
> > in multithreaded case is concerned.
>
> They could be but since the possibility of modification between
> hasMoreElements and nextElement exists making hasMoreElements synchronized
> on the vector wouldn't really solve anything. For true safety you would
> have to synchronize on the Vector outside of calls to
> hasMoreElements/nextElement. ie.
>
> Vector vector;
> Enumeration enumer = vector.elements();
> synchronized(enumer) {
>    while(enumer.hasMoreElements()) {
>      Object object = enumer.nextElement();
>      System.out.println(object);
>    }
> }
>
> Without the surrounding synchronized it is possible that this snippet will.
> - print all of the elements. (Yay!)
> - print something odd (duplicate elements, miss elements, etc.) based upon
> concurrent modification
> - fail with a NoSuchElementException if final element is concurrently
> removed after hasMoreElements() but before nextElement()
>
> > And, I am considering the case
> > when a thread is reading Vector using Enumeration while other thread
> > is mutating it.
>
> Probably a bad idea. Concurrent use of Vector should probably be limited
> to either non-structural modifications. ie. changes which don't change the
> size of the collection or very well ordered changes such as using Vector as
> a LIFO or FIFO (and there are better choices for this). If you are using
> Vector as a FIFO then perhaps just use removeElementAt(int index). Assuming
> only one thread is removing elements you aren't going to run into problems
> with atomicity in this usage.
>
> Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/e01c795f/attachment.html>

From javapk at gmail.com  Tue Jul 24 23:59:30 2012
From: javapk at gmail.com (Praveen Kumar Jha)
Date: Wed, 25 Jul 2012 09:29:30 +0530
Subject: [concurrency-interest] Thread-safety of hasMoreElements() of
	Vector
In-Reply-To: <290F2890-AA84-43F5-B1BD-78586E49B45D@cox.net>
References: <CAKYmnUqNA64NtkrZmj53BHwM8+O3-GoDoxXuJr2AATJ0B_MPdQ@mail.gmail.com>
	<290F2890-AA84-43F5-B1BD-78586E49B45D@cox.net>
Message-ID: <CAKYmnUr30CsmUgM=kUF97up7QjTUtu-hpYJrJ9BfirEKHWwFkg@mail.gmail.com>

Hi,

Thank you, and thanks to all the responders for their valuable response.

Regards,
Praveen


On Wed, Jul 25, 2012 at 12:30 AM, Gregg Wonderly <gergg at cox.net> wrote:

>
> On Jul 24, 2012, at 10:19 AM, Praveen Kumar Jha wrote:
>
> > Hi,
> >
> > Sorry for responding late. Please find my response(s) inline below:
> >
> >
> >> Date: Thu, 19 Jul 2012 09:11:15 -0700
> >> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> >> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> >> Subject: Re: [concurrency-interest] Thread-safety of hasMoreElements()
> >>      of      Vector
> >
> >>
> >> The Enumeration API is not suitable for multi-threaded use because
> there is
> >> a window for change between the hasMoreElements call and the subsequent
> >> nextElement call.
> >>
> >> In almost all cases the Enumeration is consumed in the same thread in
> which
> >> it is produced.
> >>
> >> The strange corner case that you envision is: one thread creates an
> >> Enumeration and hands it to another thread.  The other thread calls
> >> hasNextElement and may see a stale value for elementCount, causing it to
> >> end too early (not calling nextElement) or to fail unexpectedly when it
> >> does call nextElement.
> >>
> >> With this scenario in mind, using "synchronized" in hasMoreElements is
> more
> >> correct, though in practice I doubt it will make any difference.
> >>
> >> Joe
> > -------------------------
> >> Date: Thu, 19 Jul 2012 12:40:16 -0400
> >> From: Yuval Shavit <yshavit at akiban.com>
> >> To: Joe Bowbeer <joe.bowbeer at gmail.com>
> >> Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
> >
> >>
> >> It seems like it's more problematic than that. Vector.elementCount isn't
> >> volatile, so if you create an Enumeration (and keep it thread-local),
> but
> >> the backing Vector changes on some other thread, hasMoreElements() is
> >> broken. I think that method needs to be synchronized on Vector.this.
> >>
> >> ----------------------
> >> Date: Thu, 19 Jul 2012 09:56:04 -0700
> >> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> >> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> >
> >>
> >> If the backing vector is changing on some other thread then Enumeration
> is
> >> not a suitable API.  (This is probably the logic applied by the
> implementer
> >> who decided not to bother sync'ing hasMoreElements.)
> >>
> >> On Thu, Jul 19, 2012 at 9:40 AM, Yuval Shavit wrote:
> >>
> >
> >
> >
> > But if that is the case then why is 'Vector.this' synchronized in
> > nextElement() method?
> > As I understand, the behavior of both hasMoreElements() and
> > nextElement() should be on the same lines as far as use of Enumeration
> > in multithreaded case is concerned. And, I am considering the case
> > when a thread is reading Vector using Enumeration while other thread
> > is mutating it.
>
> I'm not stating that this is correct, just indicating a way to make it
> work correctly.  The bigger issue is the use case that seems to be driving
> your question.  If you are inserting in one thread, and enumerating in
> another, then it would be better to use a completely different kind of data
> structure it would seem.  If the inserts and removals are random, a Map
> might be a better interface.  If they are FIFO or LIFO as Mike said, then a
> different kind of list, queue or stack, might be better.  Any time you have
> two threads implementing the worker pattern, you need some point of
> rendezvous anyway.  It is that "happens before" relationship, around that
> rendezvous, which would make hasMoreElements() work, if you did use it.
>
> Gregg Wonderly
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/25a267b6/attachment.html>

From stanimir at riflexo.com  Wed Jul 25 10:42:42 2012
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 25 Jul 2012 17:42:42 +0300
Subject: [concurrency-interest] subList, subMap and navigable
In-Reply-To: <500EDD41.8050404@gmx.de>
References: <CAEJX8orW1RZbm0--tJT2UsNxLwmL8tk3Gs5G8i80MNjFoU_aSA@mail.gmail.com>
	<500D79C2.7040406@cs.oswego.edu>
	<CAEJX8oocTCwPmZ0UkCSuvK+hPVZN7zq89+Y7wDOyXtYZ+tqniA@mail.gmail.com>
	<500D958B.7010207@cs.oswego.edu>
	<CAEJX8ooSqFPZj-7rYaHvGprtmCWo8F8aRML0WtR8Qt3Ww17UyA@mail.gmail.com>
	<500EDD41.8050404@gmx.de>
Message-ID: <CAEJX8ooC6ujPC-TGp3wH1LOrFS39kdkNZ5iH=_TdSxR16amUoA@mail.gmail.com>

>
> A SerializableList interface would help at this point to explicitly only
> allow Lists which are Serializable and making it clear which objects can be
> passed in and handled as expected.
>
> The List/Collection/Map can be buried anywhere in the object graph, not
just the front method of a "service". The provided case was trivial just to
illustrate the issue. "replaceObject" receives any interesting object about
to be serialized and effectively adds writeReplace, if needed. Hence, the
user space code is left alone and happy.
In order to take use of SerializableList, the entire object model has to be
declared like that which is cumbersome, probably in that case using plain
arrays[] would be easier&faster, just not elegant and defeats the purpose
of the collection framework. Overall java.io.Serializable is a tagging
interface and it should not be used as a parameter/type explicitly. And it
doesn't help, it won't matter if an object implements java.io.Serializable
but any of its non-transient/standard-serialized fields does not.

The issue stems from the fact that not all collection views are
serializable but some are. That leads to wrong expectations/observations,
etc. Some views even do the right thing and serialize only the objects
being viewed (TreeSet), so the behavior is not consistent and not mentioned
in the docs.

Cheers
Stanimir

But as there is no such interface, creating a new interface and
> object/proxy might not be worth it and does not really make a difference to
> your approach with ObjectOutputStream.**replaceObject.
>
> public interface SerializableList<E> extends Serializable, List<E> {
>  public static class Factory {
>   private Factory() {
>   }
>
>   public static <E> SerializableList<E> create(List<E> list) {
>    if (Serializable.class.**isInstance(list)) {
>     return createProxy(list);
>    } else {
>     return createProxy(new ArrayList<E>(list));
>    }
>   }
>
>   @SuppressWarnings("unchecked") private static <E> SerializableList<E>
> createProxy(List<E> list) {
>    return (SerializableList<E>) Proxy.newProxyInstance(**
> Factory.class.getClassLoader()**, new Class[] {Serializable.class,
> List.class, SerializableList.class}, new SerializableListProxy(list));
>   }
>  }
>
>  public static class SerializableListProxy implements InvocationHandler {
>
>   private final List<?> delegate;
>
>   public SerializableListProxy(List<?> delegate) {
>    this.delegate = delegate;
>   }
>
>   @Override public Object invoke(Object proxy, Method method, Object[]
> args) throws Throwable {
>    return method.invoke(this.delegate, args);
>   }
>  }
> }
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/706c866c/attachment-0001.html>

From Johannes.Frank at kisters.de  Wed Jul 25 11:25:05 2012
From: Johannes.Frank at kisters.de (Johannes Frank)
Date: Wed, 25 Jul 2012 17:25:05 +0200
Subject: [concurrency-interest] "Intelligent" queue for thread pool
	application.
Message-ID: <OFB214559E.0F4CDABC-ONC1257A46.0052F1E4-C1257A46.0054AC88@kisters.de>

Hello!

This is my first post to this list after reading it for some time (though 
most things are some way from the use cases I have to handle at the 
moment).

I have the following scenario:

We have an application that transfers abstract data from some sort of 
source location (can be anything from ftp, mail, sms, fax, local folder 
and so on) to some other sort of target location (from the same pool), 
with additional conversion in between. 
Our thread model right now is rather crude: Per one periodic transfer job 
we configure, we have one thread that does it all.

I would like to alter that approach to that we have a dispatching thread 
that monitors all the jobs and automatically creates task containing the 
transfer command for ONE item from source to target (and possibly via an 
optional converter). This would allow the transfers to be running in 
parallel (where applicable) and enable a much bigger throughput, 
especially when the system is managing a lot of small items where the 
management logic (which is parallelizable) is taking up a considerably 
high amount of runtime for transmission of a single item compared to the 
pure data pumping action.


So far so good.
My problem now is that I need to be able to limit the amount of concurrent 
transmissions for every location so that for example pop3 mail boxes only 
have one concurrent connection (because during one connection, the mailbox 
is locked and all other attempts will fail until the first connection is 
shut down), leaving all other outstanding tasks for this specific source 
on halt until the one task already running has completed.

So basically what I need is this:

A thread pool which has an intelligent concept on which runnable to 
dispatch next to the pool and on which runnable some prequsites need to be 
fullfilled before they can be dispatched.
I want to be able to define that "Runnables that fulfill this and that 
requirement may only have n running instances in the thread pool at any 
given time" without having to implement the actual queueing myself.

Is there already an implementation of such behavior or are there at least 
existing concepts I could read into to help me on completing this task?

Kind Regards,
Johannes Frank

--------------------------------------------------------------------------------------------------------------------------------------------
Johannes Frank - KISTERS AG - Charlottenburger Allee 5 - 52068 Aachen - Germany
Handelsregister Aachen, HRB-Nr. 7838 | Vorstand: Klaus Kisters, Hanns Kisters | Aufsichtsratsvorsitzender: Dr. Thomas Klevers
Phone: +49 241 9671 -0 | Fax: +49 241 9671 -555 | E-Mail: Johannes.Frank at kisters.de | WWW: 
--------------------------------------------------------------------------------------------------------------------------------------------
This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorised copying, disclosure or distribution of the material in this e-mail is strictly forbidden.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/ea3d16d8/attachment.html>

From gergg at cox.net  Wed Jul 25 11:38:10 2012
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 25 Jul 2012 10:38:10 -0500
Subject: [concurrency-interest] "Intelligent" queue for thread pool
	application.
In-Reply-To: <OFB214559E.0F4CDABC-ONC1257A46.0052F1E4-C1257A46.0054AC88@kisters.de>
References: <OFB214559E.0F4CDABC-ONC1257A46.0052F1E4-C1257A46.0054AC88@kisters.de>
Message-ID: <E4467E4E-B00A-4E7C-91FE-5739A8E27892@cox.net>

I think that what you need, is a Queue provider SPI that you provide implementations for, for each pool type/instance.  That SPI would return the Queue which is appropriate for submitting work to that destination pool.

Each pool service provider, would then manage how many threads are going on, concurrently, at any given time.

The end result, is that you have a tree like structure of queueing.  The work is submitted at the top level, in FIFO order as it enters into your application.  The application then says ohh, this is a XYZ task, let's ask that service provider for the queue to submit the work too.  Then, you submit the work there, and it is therefore enqueued into the correct number of threads.

From the other direction, the service providers, could ask a global ThreadPool for a new thread, so that you could manage the total amount of work going on.   That ThreadPool might be a FJ pool that would allow for work stealing and other activities to keep all the threads busy.

For network I/O, if you use the NIO infrastructure, you can greatly parallelize activities there, of course.

Gregg Wonderly

On Jul 25, 2012, at 10:25 AM, Johannes Frank wrote:

> Hello!
> 
> This is my first post to this list after reading it for some time (though most things are some way from the use cases I have to handle at the moment). 
> 
> I have the following scenario: 
> 
> We have an application that transfers abstract data from some sort of source location (can be anything from ftp, mail, sms, fax, local folder and so on) to some other sort of target location (from the same pool), with additional conversion in between. 
> Our thread model right now is rather crude: Per one periodic transfer job we configure, we have one thread that does it all. 
> 
> I would like to alter that approach to that we have a dispatching thread that monitors all the jobs and automatically creates task containing the transfer command for ONE item from source to target (and possibly via an optional converter). This would allow the transfers to be running in parallel (where applicable) and enable a much bigger throughput, especially when the system is managing a lot of small items where the management logic (which is parallelizable) is taking up a considerably high amount of runtime for transmission of a single item compared to the pure data pumping action. 
> 
> 
> So far so good. 
> My problem now is that I need to be able to limit the amount of concurrent transmissions for every location so that for example pop3 mail boxes only have one concurrent connection (because during one connection, the mailbox is locked and all other attempts will fail until the first connection is shut down), leaving all other outstanding tasks for this specific source on halt until the one task already running has completed. 
> 
> So basically what I need is this: 
> 
> A thread pool which has an intelligent concept on which runnable to dispatch next to the pool and on which runnable some prequsites need to be fullfilled before they can be dispatched. 
> I want to be able to define that "Runnables that fulfill this and that requirement may only have n running instances in the thread pool at any given time" without having to implement the actual queueing myself. 
> 
> Is there already an implementation of such behavior or are there at least existing concepts I could read into to help me on completing this task? 
> 
> Kind Regards, 
> Johannes Frank 
> 
> Johannes Frank - KISTERS AG - Charlottenburger Allee 5 - 52068 Aachen - Germany
> Handelsregister Aachen, HRB-Nr. 7838 | Vorstand: Klaus Kisters, Hanns Kisters | Aufsichtsratsvorsitzender: Dr. Thomas Klevers
> Phone: +49 241 9671 -0 | Fax: +49 241 9671 -555 | E-Mail: Johannes.Frank at kisters.de | WWW:
> This e-mail may contain confidential and/or privileged information. If you are not the intended recipient (or have received this e-mail in error) please notify the sender immediately and destroy this e-mail. Any unauthorised copying, disclosure or distribution of the material in this e-mail is strictly forbidden. _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/f75f4b0a/attachment.html>

From nathan.reynolds at oracle.com  Wed Jul 25 13:25:33 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 25 Jul 2012 10:25:33 -0700
Subject: [concurrency-interest] "Intelligent" queue for thread pool
	application.
In-Reply-To: <E4467E4E-B00A-4E7C-91FE-5739A8E27892@cox.net>
References: <OFB214559E.0F4CDABC-ONC1257A46.0052F1E4-C1257A46.0054AC88@kisters.de>
	<E4467E4E-B00A-4E7C-91FE-5739A8E27892@cox.net>
Message-ID: <50102C0D.8050308@oracle.com>

I don't mean to push a product but give this as an idea.  The idea is 
that application servers may already have the functionality you are 
looking for.

Oracle's Weblogic Application Server has pool of threads that work on 
jobs.  The size of the pool is automatically adjusted to achieve maximum 
throughput.  In the server, there are several WorkManagers.   Each 
WorkManager is a queue with minimum and maximum thread constraints.   
The maximum constraint ensures that only up to that number of threads 
will process jobs concurrently. The minimum constraint ensures that at 
least that number of threads will process jobs concurrently (if there 
are jobs in the queue).  The minimum constraint could force the size of 
the thread pool to be increased just to satisfy this constraint.

The minimum and maximum constraints can be shared across a subset of the 
WorkManagers.  For example, you could have 3 WorkManagers to handle 3 
types of jobs: pull emails from POP3, convert emails, push emails to the 
same POP3.  The WorkManagers which deal with POP3 could share a maximum 
constraint so that only 1 thread will pull or push an email.  When an 
email is pulled, a job is created and put into the conversion 
WorkManager.  When the conversion is completed, a job is created and put 
into the push email.  Now, you will only have 1 thread working on POP3 
but several threads doing the conversion process.

You can then extend this out to all of the different types of locations 
(ftp, sms, fax, local folder).  FTP might have a maximum constraint of 
10 because the server crashes with more connections.  Sms might not have 
a maximum constraint since the sms throughput flat-lines once maximum 
throughput is achieved. Fax might have a maximum constraint of 1 since 
it can only send or receive 1 fax at a time.  Local folder might have a 
maximum constraint of 1 to optimize for HDD throughput or 16 to optimize 
for SSD throughput.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/25/2012 8:38 AM, Gregg Wonderly wrote:
> I think that what you need, is a Queue provider SPI that you provide 
> implementations for, for each pool type/instance.  That SPI would 
> return the Queue which is appropriate for submitting work to that 
> destination pool.
>
> Each pool service provider, would then manage how many threads are 
> going on, concurrently, at any given time.
>
> The end result, is that you have a tree like structure of queueing. 
>  The work is submitted at the top level, in FIFO order as it enters 
> into your application.  The application then says ohh, this is a XYZ 
> task, let's ask that service provider for the queue to submit the work 
> too.  Then, you submit the work there, and it is therefore enqueued 
> into the correct number of threads.
>
> From the other direction, the service providers, could ask a global 
> ThreadPool for a new thread, so that you could manage the total amount 
> of work going on.   That ThreadPool might be a FJ pool that would 
> allow for work stealing and other activities to keep all the threads busy.
>
> For network I/O, if you use the NIO infrastructure, you can greatly 
> parallelize activities there, of course.
>
> Gregg Wonderly
>
> On Jul 25, 2012, at 10:25 AM, Johannes Frank wrote:
>
>> Hello!
>>
>> This is my first post to this list after reading it for some time 
>> (though most things are some way from the use cases I have to handle 
>> at the moment).
>>
>> I have the following scenario:
>>
>> We have an application that transfers abstract data from some sort of 
>> source location (can be anything from ftp, mail, sms, fax, local 
>> folder and so on) to some other sort of target location (from the 
>> same pool), with additional conversion in between.
>> Our thread model right now is rather crude: Per one periodic transfer 
>> job we configure, we have one thread that does it all.
>>
>> I would like to alter that approach to that we have a dispatching 
>> thread that monitors all the jobs and automatically creates task 
>> containing the transfer command for ONE item from source to target 
>> (and possibly via an optional converter). This would allow the 
>> transfers to be running in parallel (where applicable) and enable a 
>> much bigger throughput, especially when the system is managing a lot 
>> of small items where the management logic (which is parallelizable) 
>> is taking up a considerably high amount of runtime for transmission 
>> of a single item compared to the pure data pumping action.
>>
>>
>> So far so good.
>> My problem now is that I need to be able to limit the amount of 
>> concurrent transmissions for every location so that for example pop3 
>> mail boxes only have one concurrent connection (because during one 
>> connection, the mailbox is locked and all other attempts will fail 
>> until the first connection is shut down), leaving all other 
>> outstanding tasks for this specific source on halt until the one task 
>> already running has completed.
>>
>> So basically what I need is this:
>>
>> A thread pool which has an intelligent concept on which runnable to 
>> dispatch next to the pool and on which runnable some prequsites need 
>> to be fullfilled before they can be dispatched.
>> I want to be able to define that "Runnables that fulfill this and 
>> that requirement may only have n running instances in the thread pool 
>> at any given time" without having to implement the actual queueing 
>> myself.
>>
>> Is there already an implementation of such behavior or are there at 
>> least existing concepts I could read into to help me on completing 
>> this task?
>>
>> Kind Regards,
>> Johannes Frank
>>
>> ------------------------------------------------------------------------
>> Johannes Frank - KISTERS AG - Charlottenburger Allee 5 - 52068 Aachen 
>> - Germany
>> Handelsregister Aachen, HRB-Nr. 7838 | Vorstand: Klaus Kisters, Hanns 
>> Kisters | Aufsichtsratsvorsitzender: Dr. Thomas Klevers
>> Phone: +49 241 9671 -0 | Fax: +49 241 9671 -555 | E-Mail: 
>> Johannes.Frank at kisters.de <mailto:Johannes.Frank at kisters.de> | WWW:
>> ------------------------------------------------------------------------
>> This e-mail may contain confidential and/or privileged information. 
>> If you are not the intended recipient (or have received this e-mail 
>> in error) please notify the sender immediately and destroy this 
>> e-mail. Any unauthorised copying, disclosure or distribution of the 
>> material in this e-mail is strictly forbidden. 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120725/92b59a15/attachment.html>

From leizhao833 at gmail.com  Fri Jul 27 12:57:11 2012
From: leizhao833 at gmail.com (Lei Zhao)
Date: Fri, 27 Jul 2012 12:57:11 -0400
Subject: [concurrency-interest] If LoadLoad barrier is reduced to no-op,
	why lfence?
Message-ID: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>

Hello Everyone,

  I am currently reading the JMM cookbook (
http://gee.cs.oswego.edu/dl/jmm/cookbook.html) and have a (maybe hardware
related) question about barrier instructions: if the LoadLoad barrier is
going to be no-op on x86-TSO, why does lfence instruction exist at all?
(similarly StoreStore and sfence). I am a little confused about whether
x86-TSO intrinsically guarantees load-load ordering or not. Thank you.

- Lei
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120727/a79f54bd/attachment.html>

From vitalyd at gmail.com  Fri Jul 27 13:25:27 2012
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 27 Jul 2012 13:25:27 -0400
Subject: [concurrency-interest] If LoadLoad barrier is reduced to no-op,
 why lfence?
In-Reply-To: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>
References: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>
Message-ID: <CAHjP37GbNFVKchfqQ3-+4MNomvvfagzcSiWYLAX2bt0SSYCLow@mail.gmail.com>

There are SSE write combining instructions that don't provide the same
order guarantee that write-back memory ones do (the typical mov
instruction) - this is where lfence would be needed.

Vitaly

Sent from my phone
On Jul 27, 2012 1:03 PM, "Lei Zhao" <leizhao833 at gmail.com> wrote:

> Hello Everyone,
>
>   I am currently reading the JMM cookbook (
> http://gee.cs.oswego.edu/dl/jmm/cookbook.html) and have a (maybe hardware
> related) question about barrier instructions: if the LoadLoad barrier is
> going to be no-op on x86-TSO, why does lfence instruction exist at all?
> (similarly StoreStore and sfence). I am a little confused about whether
> x86-TSO intrinsically guarantees load-load ordering or not. Thank you.
>
> - Lei
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120727/aacd4765/attachment.html>

From nathan.reynolds at oracle.com  Fri Jul 27 13:47:48 2012
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 27 Jul 2012 10:47:48 -0700
Subject: [concurrency-interest] If LoadLoad barrier is reduced to no-op,
 why lfence?
In-Reply-To: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>
References: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>
Message-ID: <5012D444.7020203@oracle.com>

This confused me too a while back.  I had to read and re-read the 
Software Developer's Manual back then and again now.  Compare the first 
paragraphs of lfence, mfence and sfence (included below).  If I am 
interpreting it right, the mfence and sfence instructions only impact 
data loads and data stores.  The lfence instruction serializes the 
instruction stream itself.  In other words, lfence prevents instructions 
after it from executing until the lfence has retired.

For example, if the instruction stream has "lfence" followed by "inc 
eax", then the "inc eax" won't begin executing until the "lfence" 
retires.  This is interesting since "inc eax" only affects a register.

There is an interesting point in JSR-133 Cookbook.  "The x86 processors 
supporting "streaming SIMD" SSE2 extensions require LoadLoad "lfence" 
only only in connection with these streaming instructions."  That gives 
us a hint for when lfence would be useful.

http://download.intel.com/design/processor/manuals/253666.pdf
http://download.intel.com/products/processor/manual/253667.pdf

Page 3-588 Vol. 2A

LFENCE---Load Fence

Performs a serializing operation on all load-from-memory instructions 
that were issued prior the LFENCE instruction. Specifically, LFENCE does 
not execute until all prior instructions have completed locally, and no 
later instruction begins execution until LFENCE completes. In 
particular, an instruction that loads from memory and that precedes an 
LFENCE receives data from memory prior to completion of the LFENCE. (An 
LFENCE that follows an instruction that stores to memory might complete 
before the data being stored have become globally visible.) Instructions 
following an LFENCE may be fetched from memory before the LFENCE, but 
they will not execute until the LFENCE completes.

Page 3-628 Vol. 2A

MFENCE - Memory Fence

Performs a serializing operation on all load-from-memory and 
store-to-memory instructions that were issued prior the MFENCE 
instruction. This serializing operation guarantees that every load and 
store instruction that precedes the MFENCE instruction in program order 
becomes globally visible before any load or store instruction that 
follows the MFENCE instruction.1 The MFENCE instruction is ordered with 
respect to all load and store instructions, other MFENCE instructions, 
any LFENCE and SFENCE instructions, and any serializing instructions 
(such as the CPUID instruction).  MFENCE does not serialize the 
instruction stream.

Page 4-525 Vol. 2B

SFENCE - Store Fence

Performs a serializing operation on all store-to-memory instructions 
that were issued prior the SFENCE instruction. This serializing 
operation guarantees that every store instruction that precedes the 
SFENCE instruction in program order becomes globally visible before any 
store instruction that follows the SFENCE instruction. The SFENCE 
instruction is ordered with respect to store instructions, other SFENCE 
instructions, any LFENCE and MFENCE instructions, and any serializing 
instructions (such as the CPUID instruction). It is not ordered with 
respect to load instructions.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Consulting Member of Technical Staff | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 7/27/2012 9:57 AM, Lei Zhao wrote:
> Hello Everyone,
>
>   I am currently reading the JMM cookbook 
> (http://gee.cs.oswego.edu/dl/jmm/cookbook.html) and have a (maybe 
> hardware related) question about barrier instructions: if the LoadLoad 
> barrier is going to be no-op on x86-TSO, why does lfence instruction 
> exist at all? (similarly StoreStore and sfence). I am a little 
> confused about whether x86-TSO intrinsically guarantees load-load 
> ordering or not. Thank you.
>
> - Lei
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120727/1fc90fad/attachment.html>

From hans.boehm at hp.com  Fri Jul 27 14:36:33 2012
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 27 Jul 2012 18:36:33 +0000
Subject: [concurrency-interest] If LoadLoad barrier is reduced to no-op,
 why lfence?
In-Reply-To: <CAHjP37GbNFVKchfqQ3-+4MNomvvfagzcSiWYLAX2bt0SSYCLow@mail.gmail.com>
References: <CAJb8b9Ndhw0Oy-4z35TO+_jWuSdFJkrL+1pEY8oGUfX7X-0Xjg@mail.gmail.com>
	<CAHjP37GbNFVKchfqQ3-+4MNomvvfagzcSiWYLAX2bt0SSYCLow@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD235F5313E@G4W3296.americas.hpqcorp.net>

As far as I can tell, SFENCEs are usually needed at least after non-temporal stores like MOVNTQ, to hide the non-TSO properties of those instructions.

I haven't figured out a use for LFENCE instructions with WB cacheable memory and non-privileged instructions.  (And I haven't looked at interactions with privileged instructions.)  I don't currently understand the ordering properties of the other x86 mapping types.  I suspect it may be more useful with some of those.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Vitaly Davidovich
Sent: Friday, July 27, 2012 10:25 AM
To: Lei Zhao
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] If LoadLoad barrier is reduced to no-op, why lfence?


There are SSE write combining instructions that don't provide the same order guarantee that write-back memory ones do (the typical mov instruction) - this is where lfence would be needed.

Vitaly

Sent from my phone
On Jul 27, 2012 1:03 PM, "Lei Zhao" <leizhao833 at gmail.com<mailto:leizhao833 at gmail.com>> wrote:
Hello Everyone,

  I am currently reading the JMM cookbook (http://gee.cs.oswego.edu/dl/jmm/cookbook.html) and have a (maybe hardware related) question about barrier instructions: if the LoadLoad barrier is going to be no-op on x86-TSO, why does lfence instruction exist at all? (similarly StoreStore and sfence). I am a little confused about whether x86-TSO intrinsically guarantees load-load ordering or not. Thank you.

- Lei

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120727/34acd777/attachment-0001.html>

From mz1999 at gmail.com  Tue Jul 31 01:43:42 2012
From: mz1999 at gmail.com (ma zhen)
Date: Tue, 31 Jul 2012 13:43:42 +0800
Subject: [concurrency-interest] a special detaila about ConcurrentHashMapV8
Message-ID: <CA+U33_Prb0MkkjWx_BXxhjahEMDAEvar6F688+iRYBuZWZzdzw@mail.gmail.com>

I notice a very small change from ConcurrentHashMap to ConcurrentHashMapV8

ConcurrentHashMap
static final class HashEntry<K,V> {
        final K key;
        final int hash;
        volatile V value;
        final HashEntry<K,V> next;

ConcurrentHashMapV8
static class Node {
        volatile int hash;
        final Object key;
        volatile Object val;
        volatile Node next;


In ConcurrentHashMap, Entry's 'next' field is final, which means the
relationship among entries is read-only after it have been created.
In contrast, 'next' field defined in ConcurrentHashMapV8 is volatile, so
the structure of nodes can be modified.
I think read-only structure can provide more concurrency because it need
not add lock when go through the enties, although this way involves many
object copy.

I wonder what's the purpose to make this change, and what's advantage to do
so.

Thanks!
Kurt
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120731/e239de00/attachment.html>

From piotr at bzdyl.net  Tue Jul 31 05:54:37 2012
From: piotr at bzdyl.net (Piotr Bzdyl)
Date: Tue, 31 Jul 2012 11:54:37 +0200
Subject: [concurrency-interest] Java 8 "automatically-parallelizable
	bulk-data operations"
Message-ID: <CAOhYZa3uuuku+hyf1Mqy8=zTkAyeR4zaZP91=uXzAu6=GnbrQA@mail.gmail.com>

Hello,

I tried to google it but could not find anything specific.

Java 8 release contents JSR-337
<http://www.jcp.org/en/jsr/detail?id=337> mentiones in
the Performance section:

*"In this release we will take another big step by enhancing the
Collections Framework and related APIs to support
automatically-parallelizable bulk-data operations such as filter, map, and
reduce"*.

Does it mean just ParallelArray or also implementations of other
collections such as lists, sets and maps?

Best regards,
Piotr
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20120731/26cf8ffb/attachment.html>

From dl at cs.oswego.edu  Tue Jul 31 08:11:58 2012
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 31 Jul 2012 08:11:58 -0400
Subject: [concurrency-interest] a special detaila about
	ConcurrentHashMapV8
In-Reply-To: <CA+U33_Prb0MkkjWx_BXxhjahEMDAEvar6F688+iRYBuZWZzdzw@mail.gmail.com>
References: <CA+U33_Prb0MkkjWx_BXxhjahEMDAEvar6F688+iRYBuZWZzdzw@mail.gmail.com>
Message-ID: <5017CB8E.2090103@cs.oswego.edu>

On 07/31/12 01:43, ma zhen wrote:
> I notice a very small change from ConcurrentHashMap to ConcurrentHashMapV8
>
> ConcurrentHashMap
> static final class HashEntry<K,V> {
>          final K key;
>          final int hash;
>          volatile V value;
>          final HashEntry<K,V> next;
>
> ConcurrentHashMapV8
> static class Node {
>          volatile int hash;
>          final Object key;
>          volatile Object val;
>          volatile Node next;
>
>
> In ConcurrentHashMap, Entry's 'next' field is final, which means the
> relationship among entries is read-only after it have been created.
> In contrast, 'next' field defined in ConcurrentHashMapV8 is volatile, so the
> structure of nodes can be modified.
> I think read-only structure can provide more concurrency because it need not add
> lock when go through the enties, although this way involves many object copy.
>

Use of the more complicated volatile-next-based scheme is needed to
support new/upcoming functionality including partitioned
access. There is still no copying involved except in some
cases during resizing, but there are more internal constraints
about access, described in the internal documentation.

-Doug


From aleksey.shipilev at gmail.com  Tue Jul 31 08:13:48 2012
From: aleksey.shipilev at gmail.com (Aleksey Shipilev)
Date: Tue, 31 Jul 2012 16:13:48 +0400
Subject: [concurrency-interest] Java 8 "automatically-parallelizable
 bulk-data operations"
In-Reply-To: <CAOhYZa3uuuku+hyf1Mqy8=zTkAyeR4zaZP91=uXzAu6=GnbrQA@mail.gmail.com>
References: <CAOhYZa3uuuku+hyf1Mqy8=zTkAyeR4zaZP91=uXzAu6=GnbrQA@mail.gmail.com>
Message-ID: <CA+1LWGF+Rtiy9KrywMYuhkM65+tk0aR3y-v4-3A=L4U0BBpF_A@mail.gmail.com>

Hi Piotr,

That means operations on standard collections as well [1]. This
piggybacks on lambda-fication of collections [2]. The development is
done under lambda project umbrella [3], the prototype for bulk
operations is available in lambda/lambda forest in OpenJDK, or binary
builds at [3] as well.

Please join the fun at lambda-dev@ if you are interested.

-Aleksey.

[1] http://openjdk.java.net/jeps/107
[2] http://openjdk.java.net/jeps/109
[3] http://openjdk.java.net/projects/lambda/
[4] http://mail.openjdk.java.net/mailman/listinfo/lambda-dev

On Tue, Jul 31, 2012 at 1:54 PM, Piotr Bzdyl <piotr at bzdyl.net> wrote:
> Hello,
>
> I tried to google it but could not find anything specific.
>
> Java 8 release contents JSR-337 mentiones in the Performance section:
>
> "In this release we will take another big step by enhancing the Collections
> Framework and related APIs to support automatically-parallelizable bulk-data
> operations such as filter, map, and reduce".
>
> Does it mean just ParallelArray or also implementations of other collections
> such as lists, sets and maps?
>
> Best regards,
> Piotr
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

