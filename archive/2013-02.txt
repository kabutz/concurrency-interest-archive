From ylt at letallec.org  Sun Feb  3 13:37:03 2013
From: ylt at letallec.org (Yann Le Tallec)
Date: Sun, 3 Feb 2013 18:37:03 +0000
Subject: [concurrency-interest] Reordering and Immutability
Message-ID: <CAOPu7EiZ7ay4WA+8mz4x5Wf0Q4jd2QFNSTOMpvs7CCfMSq5BvA@mail.gmail.com>

Hello,

I have a couple of questions regarding the following class:

public class SomeClass {
  private static Resource resource = null; //w0

  public static Resource getInstance() {
    if (resource == null)                  //r1
      resource = new Resource();           //w1
    return resource;                       //r2
  }
}

(1) Can getInstance() return null?

Example of an execution that returns null, with three threads T0, T1 and T2:
- T0 initialises resource to null (write w0)
- T1 reads null at r1 and assigns a new Resource instance to resource (write w1)
- T1 returns a non null value
- T2 reads w1 at r1 in the if condition and does not execute the if body
- T2 reads w0 at r2 and returns null

The execution seems happens-before consistent as both w0 and w1 are
observable by r1 and r2 (but I'm not sure if it is valid with regards
to the causality requirements of the JMM).
This possible reordering is one of the reasons why String#hashcode
uses a local variable I believe.

(2) Same question with resource declared as "private static Resource
resource;" (without the "= null") and with the additional assumption
that Resource is immutable

- I am told that "Resource resource; can't race whereas Resource
resource = null; can", but I don't see why they are different from a
JMM perspective.
- The resource variable being non-final, I don't think immutability
makes a difference here as null is still a valid and observable value.

Regards,
Yann

ps: JCiP has an UnsafeLazyInitialization example similar to SomeClass
(without the "=null") and states in 16.3 that it would be safe if
Resource were immutable, so I guess I'm missing something and that
null is not a possible outcome.

From vitalyd at gmail.com  Sun Feb  3 14:01:02 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 3 Feb 2013 14:01:02 -0500
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <CAOPu7EiZ7ay4WA+8mz4x5Wf0Q4jd2QFNSTOMpvs7CCfMSq5BvA@mail.gmail.com>
References: <CAOPu7EiZ7ay4WA+8mz4x5Wf0Q4jd2QFNSTOMpvs7CCfMSq5BvA@mail.gmail.com>
Message-ID: <CAHjP37HMxydnQ+kdZOUDJn=bosMESMMNF4Q2ddevdVGtLnuMiw@mail.gmail.com>

You can get null if the following (valid) transformation is done:

Resource tmp = resource; // null here
if (resource != null) // resource not null here
{
    resource = tmp = new Resource();
}
return tmp; // returns null

If resource is marked volatile, then everything's fine and above cannot
occur.

Also, with data race publishing, you can also observe a non-null reference
but not see the writes done in the constructor if Resource has non-final
fields.

The above reordering is why String.hashCode only deals with the temp value
explicitly.

Sent from my phone
On Feb 3, 2013 1:42 PM, "Yann Le Tallec" <ylt at letallec.org> wrote:

> Hello,
>
> I have a couple of questions regarding the following class:
>
> public class SomeClass {
>   private static Resource resource = null; //w0
>
>   public static Resource getInstance() {
>     if (resource == null)                  //r1
>       resource = new Resource();           //w1
>     return resource;                       //r2
>   }
> }
>
> (1) Can getInstance() return null?
>
> Example of an execution that returns null, with three threads T0, T1 and
> T2:
> - T0 initialises resource to null (write w0)
> - T1 reads null at r1 and assigns a new Resource instance to resource
> (write w1)
> - T1 returns a non null value
> - T2 reads w1 at r1 in the if condition and does not execute the if body
> - T2 reads w0 at r2 and returns null
>
> The execution seems happens-before consistent as both w0 and w1 are
> observable by r1 and r2 (but I'm not sure if it is valid with regards
> to the causality requirements of the JMM).
> This possible reordering is one of the reasons why String#hashcode
> uses a local variable I believe.
>
> (2) Same question with resource declared as "private static Resource
> resource;" (without the "= null") and with the additional assumption
> that Resource is immutable
>
> - I am told that "Resource resource; can't race whereas Resource
> resource = null; can", but I don't see why they are different from a
> JMM perspective.
> - The resource variable being non-final, I don't think immutability
> makes a difference here as null is still a valid and observable value.
>
> Regards,
> Yann
>
> ps: JCiP has an UnsafeLazyInitialization example similar to SomeClass
> (without the "=null") and states in 16.3 that it would be safe if
> Resource were immutable, so I guess I'm missing something and that
> null is not a possible outcome.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130203/c92da19d/attachment.html>

From jeffhain at rocketmail.com  Sun Feb  3 15:07:47 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sun, 3 Feb 2013 20:07:47 +0000 (GMT)
Subject: [concurrency-interest] volatile acrobacy
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369787BB@G9W0725.americas.hpqcorp.net>
References: <1358897318.4994.YahooMailNeo@web132104.mail.ird.yahoo.com>
	<CAHjP37HRQMdFt9WrNE-1UxmMC6pOiZgb6WGBquS++iOaN3bz6g@mail.gmail.com>
	<CAOwENiJEF9krkfVUQ50y79oTkDqrRLAuzUAJS9K=YHcnAeKdxw@mail.gmail.com>
	<50FFD61D.50205@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369786AC@G9W0725.americas.hpqcorp.net>
	<CAG65PLpQtt+PxGe5wkSvwZLmAS7fLAa7pt+YOsJJ+zskTCwQOQ@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369787BB@G9W0725.americas.hpqcorp.net>
Message-ID: <1359922067.45366.YahooMailNeo@web132103.mail.ird.yahoo.com>



>But my understanding is that lazySet() was designed to isolate
>you from this sort of ugliness (and a few others).


So to summarize (correct me if I'm wrong), even though JMM
(currently) gives no guarantee about it, (1,0) can't happen
unless of a bug in the JVM, OS or hardware.


Is it safe (and future-proof) for developpers to rely on this
"volatile transitivity", or is it just a unofficial and possibly
only temporary guard against concurrency bugs and should not
be relied on?


-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130203/fc809809/attachment.html>

From davidcholmes at aapt.net.au  Sun Feb  3 16:21:20 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 4 Feb 2013 07:21:20 +1000
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <CAOPu7EiZ7ay4WA+8mz4x5Wf0Q4jd2QFNSTOMpvs7CCfMSq5BvA@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEBJJKAA.davidcholmes@aapt.net.au>

Yann Le Tallec writes:
>
> I have a couple of questions regarding the following class:
>
> public class SomeClass {
>   private static Resource resource = null; //w0
>
>   public static Resource getInstance() {
>     if (resource == null)                  //r1
>       resource = new Resource();           //w1
>     return resource;                       //r2
>   }
> }
>
> (1) Can getInstance() return null?

As Vitaly described there is a transformation that could cause null to be
returned. Establishing the legality, or not, of such a transformation is
very difficult as there is no Java source to Java bytecode compilation
specification. The transformation does not seem sensible but that doesn't
mean it is prohibited.

> (2) Same question with resource declared as "private static Resource
> resource;" (without the "= null") and with the additional assumption
> that Resource is immutable
>
> - I am told that "Resource resource; can't race whereas Resource
> resource = null; can", but I don't see why they are different from a
> JMM perspective.

There is no difference. "Resource resource;" implicitly sets resource to
null during static initialization of the class.

> - The resource variable being non-final, I don't think immutability
> makes a difference here as null is still a valid and observable value.

Immutability, through final fields, is more concerned with safe-publication
of the object. If you can get a reference to it then you are guaranteed to
see it properly constructed, if it is immutable.

> ps: JCiP has an UnsafeLazyInitialization example similar to SomeClass
> (without the "=null") and states in 16.3 that it would be safe if
> Resource were immutable, so I guess I'm missing something and that
> null is not a possible outcome.

The JCiP example is not considering null as a possible outcome and is only
looking at the safe-publication aspect.

David Holmes
------------

> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From oleksandr.otenko at oracle.com  Wed Feb  6 12:46:59 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 06 Feb 2013 17:46:59 +0000
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEBJJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEBJJKAA.davidcholmes@aapt.net.au>
Message-ID: <51129713.4050104@oracle.com>

On 03/02/2013 21:21, David Holmes wrote:
> Yann Le Tallec writes:
> ...
>> (2) Same question with resource declared as "private static Resource
>> resource;" (without the "= null") and with the additional assumption
>> that Resource is immutable
>>
>> - I am told that "Resource resource; can't race whereas Resource
>> resource = null; can", but I don't see why they are different from a
>> JMM perspective.
> There is no difference. "Resource resource;" implicitly sets resource to
> null during static initialization of the class.

Then it may also return a non-null value that is not even a valid 
reference, and certainly doesn't necessarily reference a valid Resource 
instance.

Right?

Alex

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/4b0841c4/attachment.html>

From stanimir at riflexo.com  Wed Feb  6 14:21:01 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 6 Feb 2013 21:21:01 +0200
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <51129713.4050104@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCMEBJJKAA.davidcholmes@aapt.net.au>
	<51129713.4050104@oracle.com>
Message-ID: <CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>

> Then it may also return a non-null value that is not even a valid
> reference,
>
It will be a valid reference. It may not properly initialized instance but
still a valid reference.


> and certainly doesn't necessarily reference a valid Resource instance.
>
> There is no definition of a valid Resource coming from the snippet but the
Resource c-tor can be invoked multiple times given the lack of
synchronization/locking.


Stanimir
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/c4d490aa/attachment.html>

From javamann at cox.net  Wed Feb  6 14:39:38 2013
From: javamann at cox.net (javamann at cox.net)
Date: Wed, 6 Feb 2013 14:39:38 -0500
Subject: [concurrency-interest] Effort to create a Blocking Queue between
	Java processes
In-Reply-To: <CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>
Message-ID: <20130206193938.A2NIN.137691.imail@fed1rmwml114>

Hi all,
    Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?

Thanks

-Pete

From gergg at cox.net  Wed Feb  6 15:05:19 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 6 Feb 2013 14:05:19 -0600
Subject: [concurrency-interest] Effort to create a Blocking Queue
	between Java processes
In-Reply-To: <20130206193938.A2NIN.137691.imail@fed1rmwml114>
References: <20130206193938.A2NIN.137691.imail@fed1rmwml114>
Message-ID: <80A4608D-DD80-46FE-BEF9-3C8C30F77FD6@cox.net>

There are lots of different queuing mechanisms available in distributed environments.  The most largest and complex are JMS implementations.  There are simpler versions of that using RMI or CORBA to talk between java processes.  You might want to look at the Javaspaces as a more robust and more readily distributed data exchange system.  There are products which use Javaspaces if you need a product, and there are also open source projects such as Apache River, which has the original Jini/Javaspaces work, and the Blitz Javaspaces and probably some others I am not remembering.   There are deployment toolkits for Jini services such as Rio, which can provide some pretty compelling capabilities for system deployment and management.

Gregg Wonderly

On Feb 6, 2013, at 1:39 PM, javamann at cox.net wrote:

> Hi all,
>    Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
> 
> Thanks
> 
> -Pete
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From daniel.yokomizo at gmail.com  Wed Feb  6 15:09:24 2013
From: daniel.yokomizo at gmail.com (Daniel Yokomizo)
Date: Wed, 6 Feb 2013 18:09:24 -0200
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <20130206193938.A2NIN.137691.imail@fed1rmwml114>
References: <CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>
	<20130206193938.A2NIN.137691.imail@fed1rmwml114>
Message-ID: <CAOF0wK87d+He3Q65Lw0thQBN7j2Tq8Yt4kU3+QExzzVycruCLQ@mail.gmail.com>

Hi,

Hazelcast offers a distributed BlockingQueue implementation. You can
check with them to see if they have effort records.

http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue

Daniel Yokomizo.

On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
> Hi all,
>     Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>
> Thanks
>
> -Pete
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From stanimir at riflexo.com  Wed Feb  6 15:10:18 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 6 Feb 2013 22:10:18 +0200
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <20130206193938.A2NIN.137691.imail@fed1rmwml114>
References: <CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>
	<20130206193938.A2NIN.137691.imail@fed1rmwml114>
Message-ID: <CAEJX8oqXbz0poAZ_x9_PY1MYud=Y3M7B=DcMKuudK2j-tqX=1w@mail.gmail.com>

Sockets work fine and there is JMS framework already.
If you think about BlockingQueue via shared memory that's more challenging
unless you wish to use busy wait - the latter is good for extra low latency
hand offs.

Stanimir

On Wed, Feb 6, 2013 at 9:39 PM, <javamann at cox.net> wrote:

> Hi all,
>     Sorry if this isn't the place to ask this but I was wondering what
> would be the amount of effort to create a framework where you can create a
> Blocking Queue type structure between two (or more) Java Processes?
>
> Thanks
>
> -Pete
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/d6503b03/attachment.html>

From javamann at cox.net  Wed Feb  6 15:20:56 2013
From: javamann at cox.net (javamann at cox.net)
Date: Wed, 6 Feb 2013 15:20:56 -0500
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <80A4608D-DD80-46FE-BEF9-3C8C30F77FD6@cox.net>
Message-ID: <20130206202056.5658K.138201.imail@fed1rmwml114>

That is true but it's hard to beat the simplicity of..

final BlockingQueue<String> m_messageQueue = new MagicQueue<>(id);
m_messageQueue.put("Message");

-----------------------------------------

Thanks

-Pete


---- Gregg Wonderly <gergg at cox.net> wrote: 

=============
There are lots of different queuing mechanisms available in distributed environments.  The most largest and complex are JMS implementations.  There are simpler versions of that using RMI or CORBA to talk between java processes.  You might want to look at the Javaspaces as a more robust and more readily distributed data exchange system.  There are products which use Javaspaces if you need a product, and there are also open source projects such as Apache River, which has the original Jini/Javaspaces work, and the Blitz Javaspaces and probably some others I am not remembering.   There are deployment toolkits for Jini services such as Rio, which can provide some pretty compelling capabilities for system deployment and management.

Gregg Wonderly

On Feb 6, 2013, at 1:39 PM, javamann at cox.net wrote:

> Hi all,
>    Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
> 
> Thanks
> 
> -Pete
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers



From javamann at cox.net  Wed Feb  6 15:25:53 2013
From: javamann at cox.net (javamann at cox.net)
Date: Wed, 6 Feb 2013 15:25:53 -0500
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <CAOF0wK87d+He3Q65Lw0thQBN7j2Tq8Yt4kU3+QExzzVycruCLQ@mail.gmail.com>
Message-ID: <20130206202553.T168B.138243.imail@fed1rmwml114>

It appears Hazelcast is using IP, I was more interested in using Shared Memory.

Thanks though, I didn't know about Hazelcast before.

-Pete

---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote: 

=============
Hi,

Hazelcast offers a distributed BlockingQueue implementation. You can
check with them to see if they have effort records.

http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue

Daniel Yokomizo.

On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
> Hi all,
>     Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>
> Thanks
>
> -Pete
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers



From oleksandr.otenko at oracle.com  Wed Feb  6 15:42:59 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 06 Feb 2013 20:42:59 +0000
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEBJJKAA.davidcholmes@aapt.net.au>
	<51129713.4050104@oracle.com>
	<CAEJX8oqAff4YEtqeSY7PQRfUcXOEYQh+XErC2Od+NiB9vaM_LA@mail.gmail.com>
Message-ID: <5112C053.7020200@oracle.com>

I am thinking of a case where the CPU already has the value at that 
address in cache. The value cached has been GCed long since, and now 
that memory is reused by a reference to Resource. If accesses to static 
members are not volatile, then what stops the thread from observing the 
static initialization completed, and the reference that was not nulled 
yet, and pointing somewhere dark.

On the other hand, if there is a ordering constraint on static 
initializer finishing, the other threads observing this, and those 
threads loading the values of static fields, then any static field 
initialization is thread safe without extra requirement to mark it 
volatile. Since it seems the point of the remark was that any value can 
be observed, I assume there is no ordering constraint, go to original 
question: what enforces the order of observing the class is initialized 
and loading the reference.

Alex


On 06/02/2013 19:21, Stanimir Simeonoff wrote:
>
>     Then it may also return a non-null value that is not even a valid
>     reference,
>
> It will be a valid reference. It may not properly initialized instance 
> but still a valid reference.
>
>     and certainly doesn't necessarily reference a valid Resource instance.
>
> There is no definition of a valid Resource coming from the snippet but 
> the Resource c-tor can be invoked multiple times given the lack of 
> synchronization/locking.
>
>
> Stanimir
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/cfc4a2a8/attachment.html>

From davidcholmes at aapt.net.au  Wed Feb  6 15:51:59 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 7 Feb 2013 06:51:59 +1000
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <51129713.4050104@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMECFJKAA.davidcholmes@aapt.net.au>

Sorry I have non-html based mailer so responding inline is not reasonable.

The implicit initialization of static fields to their default values will
always happen-before any thread can see those fields due to the class
initialization process. So the reference is either null or a proper
reference to an object that may or may not be itself properly initialized.
It can never be a random set of bits.

David
  -----Original Message-----
  From: oleksandr otenko [mailto:oleksandr.otenko at oracle.com]
  Sent: Thursday, 7 February 2013 3:47 AM
  To: dholmes at ieee.org
  Cc: David Holmes; Yann Le Tallec; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Reordering and Immutability


  On 03/02/2013 21:21, David Holmes wrote:

Yann Le Tallec writes:
...
(2) Same question with resource declared as "private static Resource
resource;" (without the "= null") and with the additional assumption
that Resource is immutable

- I am told that "Resource resource; can't race whereas Resource
resource = null; can", but I don't see why they are different from a
JMM perspective.
There is no difference. "Resource resource;" implicitly sets resource to
null during static initialization of the class.
  Then it may also return a non-null value that is not even a valid
reference, and certainly doesn't necessarily reference a valid Resource
instance.

  Right?

  Alex

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/24d948d8/attachment.html>

From javamann at cox.net  Wed Feb  6 16:46:11 2013
From: javamann at cox.net (javamann at cox.net)
Date: Wed, 6 Feb 2013 16:46:11 -0500
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <CAG65PLrOGLQm0txuBVbKX3fcba=MNDMB=Gy+A6s=5Usorbn5ww@mail.gmail.com>
Message-ID: <20130206214611.JS20H.139248.imail@fed1rmwml114>

I was looking at solutions that used MMap but the problem is the coordination between the processes. I would really like the situation where the consuming thread would block until there was data available vs. polling. This would be really cool if it was part of the JVM.

Thanks

-Pete

---- Adam Browning <adbrowning at gmail.com> wrote: 

=============
If you're looking for shared memory between processes, you could set
up something with mmap (assuming you control both sides of the
communication). It probably wouldn't be as fast as the shared memory
of threads, but I suspect that it'd probably be faster than going
through the TCP/IP stack.

Adam

On Wed, Feb 6, 2013 at 3:25 PM,  <javamann at cox.net> wrote:
> It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>
> Thanks though, I didn't know about Hazelcast before.
>
> -Pete
>
> ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote:
>
> =============
> Hi,
>
> Hazelcast offers a distributed BlockingQueue implementation. You can
> check with them to see if they have effort records.
>
> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>
> Daniel Yokomizo.
>
> On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
>> Hi all,
>>     Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>>
>> Thanks
>>
>> -Pete
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers



From nathan.reynolds at oracle.com  Wed Feb  6 16:54:57 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 06 Feb 2013 14:54:57 -0700
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <20130206214611.JS20H.139248.imail@fed1rmwml114>
References: <20130206214611.JS20H.139248.imail@fed1rmwml114>
Message-ID: <5112D131.2030605@oracle.com>

If Process A adds to the queue, how does it wake up a thread in Process 
B?  On Windows, you could do this with a named Event.  Both Processes 
have to agree upon a name when creating the event.  They could then 
signal the event and wake up threads.  I am not sure if other OS's have 
the ability.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/6/2013 2:46 PM, javamann at cox.net wrote:
> I was looking at solutions that used MMap but the problem is the coordination between the processes. I would really like the situation where the consuming thread would block until there was data available vs. polling. This would be really cool if it was part of the JVM.
>
> Thanks
>
> -Pete
>
> ---- Adam Browning <adbrowning at gmail.com> wrote:
>
> =============
> If you're looking for shared memory between processes, you could set
> up something with mmap (assuming you control both sides of the
> communication). It probably wouldn't be as fast as the shared memory
> of threads, but I suspect that it'd probably be faster than going
> through the TCP/IP stack.
>
> Adam
>
> On Wed, Feb 6, 2013 at 3:25 PM,  <javamann at cox.net> wrote:
>> It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>>
>> Thanks though, I didn't know about Hazelcast before.
>>
>> -Pete
>>
>> ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote:
>>
>> =============
>> Hi,
>>
>> Hazelcast offers a distributed BlockingQueue implementation. You can
>> check with them to see if they have effort records.
>>
>> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>>
>> Daniel Yokomizo.
>>
>> On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
>>> Hi all,
>>>      Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>>>
>>> Thanks
>>>
>>> -Pete
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> --
>>
>> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>>
>> 2. Behind every great woman... Is a man checking out her ass
>>
>> 3. I am not a member of any organized political party. I am a Democrat.*
>>
>> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>>
>> 5. A process is what you need when all your good people have left.
>>
>>
>> *Will Rogers
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/c90a317b/attachment.html>

From david.lloyd at redhat.com  Wed Feb  6 17:05:40 2013
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 06 Feb 2013 16:05:40 -0600
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <5112D131.2030605@oracle.com>
References: <20130206214611.JS20H.139248.imail@fed1rmwml114>
	<5112D131.2030605@oracle.com>
Message-ID: <5112D3B4.3060202@redhat.com>

On Linux you could use futexes or semaphores (the latter being POSIX and 
so available in lots of other places too), though you're all JNI at that 
point.

Perhaps better would be if the JDK implemented AF_LOCAL socket support 
(likely using "named pipes" on Windows which can be full duplex).

On 02/06/2013 03:54 PM, Nathan Reynolds wrote:
> If Process A adds to the queue, how does it wake up a thread in Process
> B?  On Windows, you could do this with a named Event.  Both Processes
> have to agree upon a name when creating the event.  They could then
> signal the event and wake up threads.  I am not sure if other OS's have
> the ability.
>
> Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/6/2013 2:46 PM, javamann at cox.net wrote:
>> I was looking at solutions that used MMap but the problem is the coordination between the processes. I would really like the situation where the consuming thread would block until there was data available vs. polling. This would be really cool if it was part of the JVM.
>>
>> Thanks
>>
>> -Pete
>>
>> ---- Adam Browning<adbrowning at gmail.com>  wrote:
>>
>> =============
>> If you're looking for shared memory between processes, you could set
>> up something with mmap (assuming you control both sides of the
>> communication). It probably wouldn't be as fast as the shared memory
>> of threads, but I suspect that it'd probably be faster than going
>> through the TCP/IP stack.
>>
>> Adam
>>
>> On Wed, Feb 6, 2013 at 3:25 PM,<javamann at cox.net>  wrote:
>>> It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>>>
>>> Thanks though, I didn't know about Hazelcast before.
>>>
>>> -Pete
>>>
>>> ---- Daniel Yokomizo<daniel.yokomizo at gmail.com>  wrote:
>>>
>>> =============
>>> Hi,
>>>
>>> Hazelcast offers a distributed BlockingQueue implementation. You can
>>> check with them to see if they have effort records.
>>>
>>> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>>>
>>> Daniel Yokomizo.
>>>
>>> On Wed, Feb 6, 2013 at 5:39 PM,<javamann at cox.net>  wrote:
>>>> Hi all,
>>>>      Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>>>>
>>>> Thanks
>>>>
>>>> -Pete
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> --
>>>
>>> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>>>
>>> 2. Behind every great woman... Is a man checking out her ass
>>>
>>> 3. I am not a member of any organized political party. I am a Democrat.*
>>>
>>> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>>>
>>> 5. A process is what you need when all your good people have left.
>>>
>>>
>>> *Will Rogers
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> --
>>
>> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>>
>> 2. Behind every great woman... Is a man checking out her ass
>>
>> 3. I am not a member of any organized political party. I am a Democrat.*
>>
>> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>>
>> 5. A process is what you need when all your good people have left.
>>
>>
>> *Will Rogers
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
- DML

From javamann at cox.net  Wed Feb  6 17:10:34 2013
From: javamann at cox.net (javamann at cox.net)
Date: Wed, 6 Feb 2013 17:10:34 -0500
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <5112D131.2030605@oracle.com>
Message-ID: <20130206221034.TRBGF.139548.imail@fed1rmwml114>

Thanks, I'll look into this.

-Pete

---- Nathan Reynolds <nathan.reynolds at oracle.com> wrote: 

=============
If Process A adds to the queue, how does it wake up a thread in Process 
B?  On Windows, you could do this with a named Event.  Both Processes 
have to agree upon a name when creating the event.  They could then 
signal the event and wake up threads.  I am not sure if other OS's have 
the ability.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/6/2013 2:46 PM, javamann at cox.net wrote:
> I was looking at solutions that used MMap but the problem is the coordination between the processes. I would really like the situation where the consuming thread would block until there was data available vs. polling. This would be really cool if it was part of the JVM.
>
> Thanks
>
> -Pete
>
> ---- Adam Browning <adbrowning at gmail.com> wrote:
>
> =============
> If you're looking for shared memory between processes, you could set
> up something with mmap (assuming you control both sides of the
> communication). It probably wouldn't be as fast as the shared memory
> of threads, but I suspect that it'd probably be faster than going
> through the TCP/IP stack.
>
> Adam
>
> On Wed, Feb 6, 2013 at 3:25 PM,  <javamann at cox.net> wrote:
>> It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>>
>> Thanks though, I didn't know about Hazelcast before.
>>
>> -Pete
>>
>> ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote:
>>
>> =============
>> Hi,
>>
>> Hazelcast offers a distributed BlockingQueue implementation. You can
>> check with them to see if they have effort records.
>>
>> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>>
>> Daniel Yokomizo.
>>
>> On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
>>> Hi all,
>>>      Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>>>
>>> Thanks
>>>
>>> -Pete
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> --
>>
>> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>>
>> 2. Behind every great woman... Is a man checking out her ass
>>
>> 3. I am not a member of any organized political party. I am a Democrat.*
>>
>> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>>
>> 5. A process is what you need when all your good people have left.
>>
>>
>> *Will Rogers
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers



From stanimir at riflexo.com  Wed Feb  6 17:12:24 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 7 Feb 2013 00:12:24 +0200
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <5112D131.2030605@oracle.com>
References: <20130206214611.JS20H.139248.imail@fed1rmwml114>
	<5112D131.2030605@oracle.com>
Message-ID: <CAEJX8opOspban4=LDGndjY+ukajDjGGrrZhh3CmGaKBPXuaeRA@mail.gmail.com>

AFAIK Events are racy on windows. On windows it's possible to use standard
PostMessage as well (but needs window handle).
You do have pipes and sockets on *NIX. Technically you can use OS events
but that may not be fast hand off.
You can use busy spin for xxx time and then fall back on standard
inter-process communication (i.e. sockets). This is why I said
"challenging".


Stanimir

On Wed, Feb 6, 2013 at 11:54 PM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  If Process A adds to the queue, how does it wake up a thread in Process
> B?  On Windows, you could do this with a named Event.  Both Processes have
> to agree upon a name when creating the event.  They could then signal the
> event and wake up threads.  I am not sure if other OS's have the ability.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/6/2013 2:46 PM, javamann at cox.net wrote:
>
> I was looking at solutions that used MMap but the problem is the coordination between the processes. I would really like the situation where the consuming thread would block until there was data available vs. polling. This would be really cool if it was part of the JVM.
>
> Thanks
>
> -Pete
>
> ---- Adam Browning <adbrowning at gmail.com> <adbrowning at gmail.com> wrote:
>
> =============
> If you're looking for shared memory between processes, you could set
> up something with mmap (assuming you control both sides of the
> communication). It probably wouldn't be as fast as the shared memory
> of threads, but I suspect that it'd probably be faster than going
> through the TCP/IP stack.
>
> Adam
>
> On Wed, Feb 6, 2013 at 3:25 PM,  <javamann at cox.net> <javamann at cox.net> wrote:
>
>  It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>
> Thanks though, I didn't know about Hazelcast before.
>
> -Pete
>
> ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> <daniel.yokomizo at gmail.com> wrote:
>
> =============
> Hi,
>
> Hazelcast offers a distributed BlockingQueue implementation. You can
> check with them to see if they have effort records.
> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>
> Daniel Yokomizo.
>
> On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> <javamann at cox.net> wrote:
>
>  Hi all,
>     Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>
> Thanks
>
> -Pete
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/e1c1f4e6/attachment.html>

From viktor.klang at gmail.com  Wed Feb  6 17:16:46 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 6 Feb 2013 23:16:46 +0100
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <20130206214611.JS20H.139248.imail@fed1rmwml114>
References: <CAG65PLrOGLQm0txuBVbKX3fcba=MNDMB=Gy+A6s=5Usorbn5ww@mail.gmail.com>
	<20130206214611.JS20H.139248.imail@fed1rmwml114>
Message-ID: <CANPzfU--JzsMvTh7b-un2V0NaLfTjH+sBcqe1gfUx7yYRni8GA@mail.gmail.com>

On Wed, Feb 6, 2013 at 10:46 PM, <javamann at cox.net> wrote:

> I was looking at solutions that used MMap but the problem is the
> coordination between the processes. I would really like the situation where
> the consuming thread would block until there was data available vs.
> polling. This would be really cool if it was part of the JVM.
>

There is. It's called InputStream.

Cheers,
?


>
> Thanks
>
> -Pete
>
> ---- Adam Browning <adbrowning at gmail.com> wrote:
>
> =============
> If you're looking for shared memory between processes, you could set
> up something with mmap (assuming you control both sides of the
> communication). It probably wouldn't be as fast as the shared memory
> of threads, but I suspect that it'd probably be faster than going
> through the TCP/IP stack.
>
> Adam
>
> On Wed, Feb 6, 2013 at 3:25 PM,  <javamann at cox.net> wrote:
> > It appears Hazelcast is using IP, I was more interested in using Shared
> Memory.
> >
> > Thanks though, I didn't know about Hazelcast before.
> >
> > -Pete
> >
> > ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote:
> >
> > =============
> > Hi,
> >
> > Hazelcast offers a distributed BlockingQueue implementation. You can
> > check with them to see if they have effort records.
> >
> > http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
> >
> > Daniel Yokomizo.
> >
> > On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
> >> Hi all,
> >>     Sorry if this isn't the place to ask this but I was wondering what
> would be the amount of effort to create a framework where you can create a
> Blocking Queue type structure between two (or more) Java Processes?
> >>
> >> Thanks
> >>
> >> -Pete
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > --
> >
> > 1. If a man is standing in the middle of the forest talking, and there
> is no woman around to hear him, is he still wrong?
> >
> > 2. Behind every great woman... Is a man checking out her ass
> >
> > 3. I am not a member of any organized political party. I am a Democrat.*
> >
> > 4. Diplomacy is the art of saying "Nice doggie" until you can find a
> rock.*
> >
> > 5. A process is what you need when all your good people have left.
> >
> >
> > *Will Rogers
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
>
> 1. If a man is standing in the middle of the forest talking, and there is
> no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/c7ce4ef2/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Feb  6 17:21:34 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 06 Feb 2013 22:21:34 +0000
Subject: [concurrency-interest] Reordering and Immutability
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMECFJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMECFJKAA.davidcholmes@aapt.net.au>
Message-ID: <5112D76E.6020103@oracle.com>

ok. But static initializer itself is also guaranteed to be executed by 
just one thread, and field accesses aren't happening before the static 
initializer is done. So either a lock, or some other way (eg trampoline 
recompilation) to ensure the thread sees the static initializer is done, 
and all member accesses strictly after the initializer is done.

So, if the threads can see uninitialized field (set to null or 0), even 
if the static initializer finished concurrently, then it is an 
interesting consequence I haven't thought of before. But if the threads 
are meant to see the result of static initializer (kind of less 
surprising behaviour), then null cannot be observed in the case at hand.

Alex


On 06/02/2013 20:51, David Holmes wrote:
> Sorry I have non-html based mailer so responding inline is not reasonable.
> The implicit initialization of static fields to their default values 
> will always happen-before any thread can see those fields due to the 
> class initialization process. So the reference is either null or a 
> proper reference to an object that may or may not be itself properly 
> initialized. It can never be a random set of bits.
> David
>
>     -----Original Message-----
>     *From:* oleksandr otenko [mailto:oleksandr.otenko at oracle.com]
>     *Sent:* Thursday, 7 February 2013 3:47 AM
>     *To:* dholmes at ieee.org
>     *Cc:* David Holmes; Yann Le Tallec; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] Reordering and Immutability
>
>     On 03/02/2013 21:21, David Holmes wrote:
>>     Yann Le Tallec writes:
>>     ...
>>>     (2) Same question with resource declared as "private static Resource
>>>     resource;" (without the "= null") and with the additional assumption
>>>     that Resource is immutable
>>>
>>>     - I am told that "Resource resource; can't race whereas Resource
>>>     resource = null; can", but I don't see why they are different from a
>>>     JMM perspective.
>>     There is no difference. "Resource resource;" implicitly sets resource to
>>     null during static initialization of the class.
>
>     Then it may also return a non-null value that is not even a valid
>     reference, and certainly doesn't necessarily reference a valid
>     Resource instance.
>
>     Right?
>
>     Alex
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130206/1607fa72/attachment.html>

From radhakrishnan.mohan at gmail.com  Wed Feb  6 22:20:04 2013
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 7 Feb 2013 08:50:04 +0530
Subject: [concurrency-interest] Thread priority
Message-ID: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>

Hi,
          Can the Thread priority setting in the API still be reliably used
uniformly across processors ? There are other concurrency patterns in the
API but this setting is still there.


Thanks,
Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/6cdcc72e/attachment.html>

From davidcholmes at aapt.net.au  Wed Feb  6 22:29:45 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 7 Feb 2013 13:29:45 +1000
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEECJJKAA.davidcholmes@aapt.net.au>

Thread priority has never been reliable. How it acts is OS specific and even
scheduling-class specific depending on the OS, and dependent on the flags
given the VM.

David

 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Mohan
Radhakrishnan
Sent: Thursday, 7 February 2013 1:20 PM
To: Concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Thread priority


  Hi,
            Can the Thread priority setting in the API still be reliably
used uniformly across processors ? There are other concurrency patterns in
the API but this setting is still there.




  Thanks,
  Mohan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/010b6788/attachment.html>

From gregg at cytetech.com  Wed Feb  6 22:41:52 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 06 Feb 2013 21:41:52 -0600
Subject: [concurrency-interest] Effort to create a Blocking Queue
 between Java processes
In-Reply-To: <20130206202553.T168B.138243.imail@fed1rmwml114>
References: <20130206202553.T168B.138243.imail@fed1rmwml114>
Message-ID: <51132280.6060905@cytetech.com>

The fastest systems are created by burying latency somewhere besides "processor 
to memory to processsor" which will always burden the primary mechanism of 
"work" with the thrashing of "communications".  Really, networks are not always 
problems for "speed".  They do introduce latency, but I/O paths in and out of 
processors that are separated from memory can provide an increase in performance.

Gregg Wonderly

On 2/6/2013 2:25 PM, javamann at cox.net wrote:
> It appears Hazelcast is using IP, I was more interested in using Shared Memory.
>
> Thanks though, I didn't know about Hazelcast before.
>
> -Pete
>
> ---- Daniel Yokomizo <daniel.yokomizo at gmail.com> wrote:
>
> =============
> Hi,
>
> Hazelcast offers a distributed BlockingQueue implementation. You can
> check with them to see if they have effort records.
>
> http://www.hazelcast.com/docs/2.5/manual/single_html/#Queue
>
> Daniel Yokomizo.
>
> On Wed, Feb 6, 2013 at 5:39 PM,  <javamann at cox.net> wrote:
>> Hi all,
>>      Sorry if this isn't the place to ask this but I was wondering what would be the amount of effort to create a framework where you can create a Blocking Queue type structure between two (or more) Java Processes?
>>
>> Thanks
>>
>> -Pete
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
>
> 1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?
>
> 2. Behind every great woman... Is a man checking out her ass
>
> 3. I am not a member of any organized political party. I am a Democrat.*
>
> 4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*
>
> 5. A process is what you need when all your good people have left.
>
>
> *Will Rogers
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From heinz at javaspecialists.eu  Thu Feb  7 01:10:05 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 07 Feb 2013 07:10:05 +0100
Subject: [concurrency-interest] Phaser and StampedLock Presentation
Message-ID: <5113453D.8090403@javaspecialists.eu>

For those of you who might be interested in the new StampedLock that 
Doug Lea wrote, I gave a talk on Tuesday at JFokus in Sweden about it, 
also showing some interesting performance results.

You can download the slides here:

http://javaspecialists.eu/talks/jfokus13/PhaserAndStampedLock.pdf

Stephen Chin and I also did a "Nighthacking" session, where we looked at 
Fork/Join and then again StampedLock:

http://vimeo.com/javaspecialists/jfokus-2013-nighthacking

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


From heinz at javaspecialists.eu  Thu Feb  7 01:16:49 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 07 Feb 2013 07:16:49 +0100
Subject: [concurrency-interest] Thread priority
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEECJJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEECJJKAA.davidcholmes@aapt.net.au>
Message-ID: <511346D1.40700@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/700e1d65/attachment-0001.html>

From stanimir at riflexo.com  Thu Feb  7 02:21:40 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 7 Feb 2013 09:21:40 +0200
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
Message-ID: <CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>

Thread priorities are usually NOT applied at all.
For insance:
     intx DefaultThreadPriority                     = -1
{product}
    intx JavaPriority10_To_OSPriority              = -1
{product}
     intx JavaPriority1_To_OSPriority               = -1
{product}
     intx JavaPriority2_To_OSPriority               = -1
{product}
     intx JavaPriority3_To_OSPriority               = -1
{product}
     intx JavaPriority4_To_OSPriority               = -1
{product}
     intx JavaPriority5_To_OSPriority               = -1
{product}
     intx JavaPriority6_To_OSPriority               = -1
{product}
     intx JavaPriority7_To_OSPriority               = -1
{product}
     intx JavaPriority8_To_OSPriority               = -1
{product}
     intx JavaPriority9_To_OSPriority               = -1
{product}

in other words unless specified : -XXJavaPriority10_To_OSPriority=
it won't be mapped.

If applied the JVM compiler/GC threads may become starved which you don't
want, so they have to work above normal prir (that request root
privileges). Alternatively the normal java threads have to run w/ lower
prir which means other process will have higher priority - also unpleasant.

Stanimir

On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan <
radhakrishnan.mohan at gmail.com> wrote:

> Hi,
>           Can the Thread priority setting in the API still be reliably
> used uniformly across processors ? There are other concurrency patterns in
> the API but this setting is still there.
>
>
> Thanks,
> Mohan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/6a6b48bd/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb  7 10:49:53 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 07 Feb 2013 08:49:53 -0700
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
Message-ID: <5113CD21.5090901@oracle.com>

With tiered compilation, once a method reaches 1,000 invocations 
(configurable?), it is compiled with instrumentation.  Then when it 
reaches 10,000 invocations (configurable), it is fully optimized using 
the instrumentation profiling data.  For these operations, the JIT 
threads should run at a higher priority.  However, there are some 
optimizations which are too heavy to do at a high priority.  These 
optimizations should be done at a low priority.  Also, methods, which 
haven't quite reached the 1,000 invocations but are being execute, could 
be compiled with instrumentation at a low priority.

The low priority work will only be done if the CPU isn't maxed out.  If 
any other thread needs the CPU, then the low priority compiler thread 
will be immediately context switched off the core.  So, the low priority 
compilation will never significantly hurt the performance of the high 
priority threads.  For some work loads, the low priority threads may 
never get a chance to run. That's okay because the work isn't that 
important.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
> Thread priorities are usually NOT applied at all.
> For insance:
>      intx DefaultThreadPriority                     = -1              
> {product}
>     intx JavaPriority10_To_OSPriority              = -1              
> {product}
>      intx JavaPriority1_To_OSPriority               = -1              
> {product}
>      intx JavaPriority2_To_OSPriority               = -1              
> {product}
>      intx JavaPriority3_To_OSPriority               = -1              
> {product}
>      intx JavaPriority4_To_OSPriority               = -1              
> {product}
>      intx JavaPriority5_To_OSPriority               = -1              
> {product}
>      intx JavaPriority6_To_OSPriority               = -1              
> {product}
>      intx JavaPriority7_To_OSPriority               = -1              
> {product}
>      intx JavaPriority8_To_OSPriority               = -1              
> {product}
>      intx JavaPriority9_To_OSPriority               = -1              
> {product}
>
> in other words unless specified : -XXJavaPriority10_To_OSPriority=
> it won't be mapped.
>
> If applied the JVM compiler/GC threads may become starved which you 
> don't want, so they have to work above normal prir (that request root 
> privileges). Alternatively the normal java threads have to run w/ 
> lower prir which means other process will have higher priority - also 
> unpleasant.
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan 
> <radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan at gmail.com>> 
> wrote:
>
>     Hi,
>               Can the Thread priority setting in the API still be
>     reliably used uniformly across processors ? There are other
>     concurrency patterns in the API but this setting is still there.
>
>
>     Thanks,
>     Mohan
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/5b72dfc2/attachment.html>

From gregg at cytetech.com  Thu Feb  7 11:08:22 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 10:08:22 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113CD21.5090901@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com>
Message-ID: <5113D176.5070309@cytetech.com>

I've always wondered why these kinds of "large" invocation counts are used.  In 
many methods, there will be a single entry in most applications, and "loops" 
inside of that method could be optimized much sooner.  In many of my desktop 
applications, I set the invocation count (on the command line) to 100 or even 
25, and get faster startups, and better performance for the small amount of time 
that I use the apps.  For the client VM, it really seems strange to wait so long 
(1000 invocations), to compile with instrumentation.  Then waiting for 10 times 
that many invocations to decide on the final optimizations seems a bit of a stretch.

Are there real data values from lots of different users which indicate that 
these "counts" are when people are ready to be more productive?  I know that 
there are probably lots of degenerative cases where optimizations will be missed 
without enough data.  But it would seem better to go to native code early, and 
adapt occasionally, rather than wait until you can be sure to be perfect.

Gregg Wonderly

On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
> With tiered compilation, once a method reaches 1,000 invocations
> (configurable?), it is compiled with instrumentation.  Then when it reaches
> 10,000 invocations (configurable), it is fully optimized using the
> instrumentation profiling data.  For these operations, the JIT threads should
> run at a higher priority.  However, there are some optimizations which are too
> heavy to do at a high priority.  These optimizations should be done at a low
> priority.  Also, methods, which haven't quite reached the 1,000 invocations but
> are being execute, could be compiled with instrumentation at a low priority.
>
> The low priority work will only be done if the CPU isn't maxed out.  If any
> other thread needs the CPU, then the low priority compiler thread will be
> immediately context switched off the core.  So, the low priority compilation
> will never significantly hurt the performance of the high priority threads.  For
> some work loads, the low priority threads may never get a chance to run. That's
> okay because the work isn't that important.
>
> Nathan Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>> Thread priorities are usually NOT applied at all.
>> For insance:
>>      intx DefaultThreadPriority                     = -1              {product}
>>     intx JavaPriority10_To_OSPriority              = -1              {product}
>>      intx JavaPriority1_To_OSPriority               = -1              {product}
>>      intx JavaPriority2_To_OSPriority               = -1              {product}
>>      intx JavaPriority3_To_OSPriority               = -1              {product}
>>      intx JavaPriority4_To_OSPriority               = -1              {product}
>>      intx JavaPriority5_To_OSPriority               = -1              {product}
>>      intx JavaPriority6_To_OSPriority               = -1              {product}
>>      intx JavaPriority7_To_OSPriority               = -1              {product}
>>      intx JavaPriority8_To_OSPriority               = -1              {product}
>>      intx JavaPriority9_To_OSPriority               = -1              {product}
>>
>> in other words unless specified : -XXJavaPriority10_To_OSPriority=
>> it won't be mapped.
>>
>> If applied the JVM compiler/GC threads may become starved which you don't
>> want, so they have to work above normal prir (that request root privileges).
>> Alternatively the normal java threads have to run w/ lower prir which means
>> other process will have higher priority - also unpleasant.
>>
>> Stanimir
>>
>> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>> <radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan at gmail.com>> wrote:
>>
>>     Hi,
>>               Can the Thread priority setting in the API still be reliably
>>     used uniformly across processors ? There are other concurrency patterns in
>>     the API but this setting is still there.
>>
>>
>>     Thanks,
>>     Mohan
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Thu Feb  7 11:20:51 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 7 Feb 2013 11:20:51 -0500
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D176.5070309@cytetech.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
Message-ID: <CAHjP37EXXpcdaTWef422H9JUb40KQuxffDod9FFWYq-G0_nGkw@mail.gmail.com>

Methods with loops have the loop backedges count towards the compilation
threshold (and may get OSR compilation) as well so they'll get hot quickly.

Sent from my phone
On Feb 7, 2013 11:12 AM, "Gregg Wonderly" <gregg at cytetech.com> wrote:

> I've always wondered why these kinds of "large" invocation counts are
> used.  In many methods, there will be a single entry in most applications,
> and "loops" inside of that method could be optimized much sooner.  In many
> of my desktop applications, I set the invocation count (on the command
> line) to 100 or even 25, and get faster startups, and better performance
> for the small amount of time that I use the apps.  For the client VM, it
> really seems strange to wait so long (1000 invocations), to compile with
> instrumentation.  Then waiting for 10 times that many invocations to decide
> on the final optimizations seems a bit of a stretch.
>
> Are there real data values from lots of different users which indicate
> that these "counts" are when people are ready to be more productive?  I
> know that there are probably lots of degenerative cases where optimizations
> will be missed without enough data.  But it would seem better to go to
> native code early, and adapt occasionally, rather than wait until you can
> be sure to be perfect.
>
> Gregg Wonderly
>
> On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>> With tiered compilation, once a method reaches 1,000 invocations
>> (configurable?), it is compiled with instrumentation.  Then when it
>> reaches
>> 10,000 invocations (configurable), it is fully optimized using the
>> instrumentation profiling data.  For these operations, the JIT threads
>> should
>> run at a higher priority.  However, there are some optimizations which
>> are too
>> heavy to do at a high priority.  These optimizations should be done at a
>> low
>> priority.  Also, methods, which haven't quite reached the 1,000
>> invocations but
>> are being execute, could be compiled with instrumentation at a low
>> priority.
>>
>> The low priority work will only be done if the CPU isn't maxed out.  If
>> any
>> other thread needs the CPU, then the low priority compiler thread will be
>> immediately context switched off the core.  So, the low priority
>> compilation
>> will never significantly hurt the performance of the high priority
>> threads.  For
>> some work loads, the low priority threads may never get a chance to run.
>> That's
>> okay because the work isn't that important.
>>
>> Nathan Reynolds <http://psr.us.oracle.com/**wiki/index.php/User:Nathan_**
>> Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>
>> |
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>
>>> Thread priorities are usually NOT applied at all.
>>> For insance:
>>>      intx DefaultThreadPriority                     = -1
>>>  {product}
>>>     intx JavaPriority10_To_OSPriority              = -1
>>>  {product}
>>>      intx JavaPriority1_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority2_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority3_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority4_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority5_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority6_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority7_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority8_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority9_To_OSPriority               = -1
>>>  {product}
>>>
>>> in other words unless specified : -XXJavaPriority10_To_**OSPriority=
>>> it won't be mapped.
>>>
>>> If applied the JVM compiler/GC threads may become starved which you don't
>>> want, so they have to work above normal prir (that request root
>>> privileges).
>>> Alternatively the normal java threads have to run w/ lower prir which
>>> means
>>> other process will have higher priority - also unpleasant.
>>>
>>> Stanimir
>>>
>>> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>> <radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan@**gmail.com<radhakrishnan.mohan at gmail.com>>>
>>> wrote:
>>>
>>>     Hi,
>>>               Can the Thread priority setting in the API still be
>>> reliably
>>>     used uniformly across processors ? There are other concurrency
>>> patterns in
>>>     the API but this setting is still there.
>>>
>>>
>>>     Thanks,
>>>     Mohan
>>>
>>>     ______________________________**_________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu><mailto:
>>> Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/3a9847d0/attachment-0001.html>

From nathan.reynolds at oracle.com  Thu Feb  7 11:30:12 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 07 Feb 2013 09:30:12 -0700
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D176.5070309@cytetech.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
Message-ID: <5113D694.40002@oracle.com>

Sorry, I wasn't very specific.  The 10,000 invocation is the default for 
server VM.  For client, I think it is 1,500 invocations.

I simplified my explanation.  Optimization for server doesn't start 
until 10,000 method invocations or 10,000 loop iterations.  However, the 
optimized code won't be used unless the thread enters the method or 
iteration after the compilation is done.

JRockit JVM never executed bytecode.  When the class was loaded, it 
immediately created native code albeit not fully optimized.  This made 
JRockit startup times much slower than HotSpot.  However, JRockit was 
usually able to get to full speed much faster.  This prompted HotSpot to 
adopted a tiered compilation strategy.  1,000 invocations was used to 
avoid the slow startup times yet get to native code sooner than waiting 
until 10,000 invocations.  If configurable, you can probably get HotSpot 
to behave like JRockit by setting the first compilation to 0 or 1 
invocations.

1,000 and 10,000 seem like arbitrary values with a little bit of 
experience or science behind them.  Sure, anyone could tune these values 
specific for their application to get the optimal start up and warm up 
performance.  But, how can we pick the best value averaged for all 
workloads?  How do we even collect that data?  We could run several 
tests and find the best value averaged for those tests, but there will 
always be a more optimal value for each individual application.  It 
seems this is something best for the program writer to tune.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/7/2013 9:08 AM, Gregg Wonderly wrote:
> I've always wondered why these kinds of "large" invocation counts are 
> used. In many methods, there will be a single entry in most 
> applications, and "loops" inside of that method could be optimized 
> much sooner.  In many of my desktop applications, I set the invocation 
> count (on the command line) to 100 or even 25, and get faster 
> startups, and better performance for the small amount of time that I 
> use the apps.  For the client VM, it really seems strange to wait so 
> long (1000 invocations), to compile with instrumentation.  Then 
> waiting for 10 times that many invocations to decide on the final 
> optimizations seems a bit of a stretch.
>
> Are there real data values from lots of different users which indicate 
> that these "counts" are when people are ready to be more productive?  
> I know that there are probably lots of degenerative cases where 
> optimizations will be missed without enough data.  But it would seem 
> better to go to native code early, and adapt occasionally, rather than 
> wait until you can be sure to be perfect.
>
> Gregg Wonderly
>
> On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>> With tiered compilation, once a method reaches 1,000 invocations
>> (configurable?), it is compiled with instrumentation.  Then when it 
>> reaches
>> 10,000 invocations (configurable), it is fully optimized using the
>> instrumentation profiling data.  For these operations, the JIT 
>> threads should
>> run at a higher priority.  However, there are some optimizations 
>> which are too
>> heavy to do at a high priority.  These optimizations should be done 
>> at a low
>> priority.  Also, methods, which haven't quite reached the 1,000 
>> invocations but
>> are being execute, could be compiled with instrumentation at a low 
>> priority.
>>
>> The low priority work will only be done if the CPU isn't maxed out.  
>> If any
>> other thread needs the CPU, then the low priority compiler thread 
>> will be
>> immediately context switched off the core.  So, the low priority 
>> compilation
>> will never significantly hurt the performance of the high priority 
>> threads.  For
>> some work loads, the low priority threads may never get a chance to 
>> run. That's
>> okay because the work isn't that important.
>>
>> Nathan Reynolds 
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>> Thread priorities are usually NOT applied at all.
>>> For insance:
>>>      intx DefaultThreadPriority                     = 
>>> -1              {product}
>>>     intx JavaPriority10_To_OSPriority              = -1              
>>> {product}
>>>      intx JavaPriority1_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority2_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority3_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority4_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority5_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority6_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority7_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority8_To_OSPriority               = 
>>> -1              {product}
>>>      intx JavaPriority9_To_OSPriority               = 
>>> -1              {product}
>>>
>>> in other words unless specified : -XXJavaPriority10_To_OSPriority=
>>> it won't be mapped.
>>>
>>> If applied the JVM compiler/GC threads may become starved which you 
>>> don't
>>> want, so they have to work above normal prir (that request root 
>>> privileges).
>>> Alternatively the normal java threads have to run w/ lower prir 
>>> which means
>>> other process will have higher priority - also unpleasant.
>>>
>>> Stanimir
>>>
>>> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>> <radhakrishnan.mohan at gmail.com 
>>> <mailto:radhakrishnan.mohan at gmail.com>> wrote:
>>>
>>>     Hi,
>>>               Can the Thread priority setting in the API still be 
>>> reliably
>>>     used uniformly across processors ? There are other concurrency 
>>> patterns in
>>>     the API but this setting is still there.
>>>
>>>
>>>     Thanks,
>>>     Mohan
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/80943d09/attachment.html>

From stanimir at riflexo.com  Thu Feb  7 11:56:49 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 7 Feb 2013 18:56:49 +0200
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D176.5070309@cytetech.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
Message-ID: <CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>

Besides the iteration counts the JVM (may) add profiling data. For
instance: call sites. If the method is compiled w/ a single class only and
then another is loaded the method has to be deoptimized and optimized again
- obviously unwanted behavior. The same holds true for inline caches.

To put it simply: w/o the profiling data the resulting machine code may not
be of high quality. More also if the compiling threshold is too low - that
would result in a lot of "code cache" used and it increases memory
footprint + it may reach the memory limit for the code cache as many
invocations do not reach c2 (10k). There was such a bug introduced w/ the
tiered compilation in 6.0.25 - basically it reached the 32mb of code cache
super fast - and a lot of code didn't reach C2 at all (or even c1 and
remained interpreted). I had to increase the code cache limits to ~256mb
(staring off 32mb) to reach to quota and decided to turn off tiered
compilation altogether. There was a rare bug as well, replacing c1 w/ c2
code resulted in crashing the process.
So, having low threshold is probably good for some microbenchmarks but not
so much for real applications.

Stanimir

On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> I've always wondered why these kinds of "large" invocation counts are
> used.  In many methods, there will be a single entry in most applications,
> and "loops" inside of that method could be optimized much sooner.  In many
> of my desktop applications, I set the invocation count (on the command
> line) to 100 or even 25, and get faster startups, and better performance
> for the small amount of time that I use the apps.  For the client VM, it
> really seems strange to wait so long (1000 invocations), to compile with
> instrumentation.  Then waiting for 10 times that many invocations to decide
> on the final optimizations seems a bit of a stretch.
>
> Are there real data values from lots of different users which indicate
> that these "counts" are when people are ready to be more productive?  I
> know that there are probably lots of degenerative cases where optimizations
> will be missed without enough data.  But it would seem better to go to
> native code early, and adapt occasionally, rather than wait until you can
> be sure to be perfect.
>
> Gregg Wonderly
>
>
> On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>> With tiered compilation, once a method reaches 1,000 invocations
>> (configurable?), it is compiled with instrumentation.  Then when it
>> reaches
>> 10,000 invocations (configurable), it is fully optimized using the
>> instrumentation profiling data.  For these operations, the JIT threads
>> should
>> run at a higher priority.  However, there are some optimizations which
>> are too
>> heavy to do at a high priority.  These optimizations should be done at a
>> low
>> priority.  Also, methods, which haven't quite reached the 1,000
>> invocations but
>> are being execute, could be compiled with instrumentation at a low
>> priority.
>>
>> The low priority work will only be done if the CPU isn't maxed out.  If
>> any
>> other thread needs the CPU, then the low priority compiler thread will be
>> immediately context switched off the core.  So, the low priority
>> compilation
>> will never significantly hurt the performance of the high priority
>> threads.  For
>> some work loads, the low priority threads may never get a chance to run.
>> That's
>> okay because the work isn't that important.
>>
>> Nathan Reynolds <http://psr.us.oracle.com/**wiki/index.php/User:Nathan_**
>> Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>
>> |
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>
>>> Thread priorities are usually NOT applied at all.
>>> For insance:
>>>      intx DefaultThreadPriority                     = -1
>>>  {product}
>>>     intx JavaPriority10_To_OSPriority              = -1
>>>  {product}
>>>      intx JavaPriority1_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority2_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority3_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority4_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority5_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority6_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority7_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority8_To_OSPriority               = -1
>>>  {product}
>>>      intx JavaPriority9_To_OSPriority               = -1
>>>  {product}
>>>
>>> in other words unless specified : -XXJavaPriority10_To_**OSPriority=
>>> it won't be mapped.
>>>
>>> If applied the JVM compiler/GC threads may become starved which you don't
>>> want, so they have to work above normal prir (that request root
>>> privileges).
>>> Alternatively the normal java threads have to run w/ lower prir which
>>> means
>>> other process will have higher priority - also unpleasant.
>>>
>>> Stanimir
>>>
>>> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>> <radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan@**gmail.com<radhakrishnan.mohan at gmail.com>>>
>>> wrote:
>>>
>>>     Hi,
>>>               Can the Thread priority setting in the API still be
>>> reliably
>>>     used uniformly across processors ? There are other concurrency
>>> patterns in
>>>     the API but this setting is still there.
>>>
>>>
>>>     Thanks,
>>>     Mohan
>>>
>>>     ______________________________**_________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu><mailto:
>>> Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>
>>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/ba24a0ff/attachment-0001.html>

From nitsanw at yahoo.com  Thu Feb  7 12:17:46 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 7 Feb 2013 09:17:46 -0800 (PST)
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
Message-ID: <1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>

This is probably mostly relevant for server applications, where the usage and hardware are static and the same software is run for months on end. Why is there no way to serialize the performance statistics collected, or even the JIT compiled code and reload those on startup?




________________________________
 From: Stanimir Simeonoff <stanimir at riflexo.com>
To: gregg.wonderly at pobox.com 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Thursday, February 7, 2013 4:56 PM
Subject: Re: [concurrency-interest] Thread priority
 

Besides the iteration counts the JVM (may) add profiling data. For instance: call sites. If the method is compiled w/ a single class only and then another is loaded the method has to be deoptimized and optimized again - obviously unwanted behavior. The same holds true for inline caches.

To put it simply: w/o the profiling data the resulting machine code may not be of high quality. More also if the compiling threshold is too low - that would result in a lot of "code cache" used and it increases memory footprint + it may reach the memory limit for the code cache as many invocations do not reach c2 (10k). There was such a bug introduced w/ the tiered compilation in 6.0.25 - basically it reached the 32mb of code cache super fast - and a lot of code didn't reach C2 at all (or even c1 and remained interpreted). I had to increase the code cache limits to ~256mb (staring off 32mb) to reach to quota and decided to turn off tiered compilation altogether. There was a rare bug as well, replacing c1 w/ c2 code resulted in crashing the process.
So, having low threshold is probably good for some microbenchmarks but not so much for real applications.

Stanimir


On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

I've always wondered why these kinds of "large" invocation counts are used. ?In many methods, there will be a single entry in most applications, and "loops" inside of that method could be optimized much sooner. ?In many of my desktop applications, I set the invocation count (on the command line) to 100 or even 25, and get faster startups, and better performance for the small amount of time that I use the apps. ?For the client VM, it really seems strange to wait so long (1000 invocations), to compile with instrumentation. ?Then waiting for 10 times that many invocations to decide on the final optimizations seems a bit of a stretch.
>
>Are there real data values from lots of different users which indicate that these "counts" are when people are ready to be more productive? ?I know that there are probably lots of degenerative cases where optimizations will be missed without enough data. ?But it would seem better to go to native code early, and adapt occasionally, rather than wait until you can be sure to be perfect.
>
>Gregg Wonderly
>
>
>On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>With tiered compilation, once a method reaches 1,000 invocations
>>(configurable?), it is compiled with instrumentation. ?Then when it reaches
>>10,000 invocations (configurable), it is fully optimized using the
>>instrumentation profiling data. ?For these operations, the JIT threads should
>>run at a higher priority. ?However, there are some optimizations which are too
>>heavy to do at a high priority. ?These optimizations should be done at a low
>>priority. ?Also, methods, which haven't quite reached the 1,000 invocations but
>>are being execute, could be compiled with instrumentation at a low priority.
>>
>>The low priority work will only be done if the CPU isn't maxed out. ?If any
>>other thread needs the CPU, then the low priority compiler thread will be
>>immediately context switched off the core. ?So, the low priority compilation
>>will never significantly hurt the performance of the high priority threads. ?For
>>some work loads, the low priority threads may never get a chance to run. That's
>>okay because the work isn't that important.
>>
>>
Nathan Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>Architect | 602.333.9091
>>Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>
>>On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>
>>Thread priorities are usually NOT applied at all.
>>>For insance:
>>>? ? ?intx DefaultThreadPriority ? ? ? ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? intx JavaPriority10_To_OSPriority ? ? ? ? ? ? ?= -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority1_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority2_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority3_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority4_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority5_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority6_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority7_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority8_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>? ? ?intx JavaPriority9_To_OSPriority ? ? ? ? ? ? ? = -1 ? ? ? ? ? ? ?{product}
>>>
>>>in other words unless specified : -XXJavaPriority10_To_OSPriority=
>>>it won't be mapped.
>>>
>>>If applied the JVM compiler/GC threads may become starved which you don't
>>>want, so they have to work above normal prir (that request root privileges).
>>>Alternatively the normal java threads have to run w/ lower prir which means
>>>other process will have higher priority - also unpleasant.
>>>
>>>Stanimir
>>>
>>>On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>>
>>><radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan at gmail.com>> wrote:
>>>
>>>? ? Hi,
>>>? ? ? ? ? ? ? Can the Thread priority setting in the API still be reliably
>>>? ? used uniformly across processors ? There are other concurrency patterns in
>>>? ? the API but this setting is still there.
>>>
>>>
>>>? ? Thanks,
>>>? ? Mohan
>>>
>>>? ? _______________________________________________
>>>? ? Concurrency-interest mailing list
>>>
? ? Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>
>>>? ? http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>_______________________________________________
>>>Concurrency-interest mailing list
>>>Concurrency-interest at cs.oswego.edu
>>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>>_______________________________________________
>>Concurrency-interest mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/e71be484/attachment.html>

From heinz at javaspecialists.eu  Thu Feb  7 12:28:38 2013
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 07 Feb 2013 18:28:38 +0100
Subject: [concurrency-interest] Thread priority
In-Reply-To: <1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>	<5113CD21.5090901@oracle.com>
	<5113D176.5070309@cytetech.com>	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
Message-ID: <5113E446.9090608@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/1e1cfd2e/attachment-0001.html>

From nathan.reynolds at oracle.com  Thu Feb  7 12:35:51 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 07 Feb 2013 10:35:51 -0700
Subject: [concurrency-interest] Thread priority
In-Reply-To: <1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
Message-ID: <5113E5F7.8050601@oracle.com>

 > Why is there no way to serialize the performance statistics 
collected, or even the JIT compiled code and reload those on startup?

I've wondered the same thing.  Before I brought it up a couple of years 
ago, it had already been considered.  The problem is that it takes more 
time to verify that the classes are the same than it does to just 
compile it anew.  Because of inlining, the number of classes that have 
to be considered is very large for just one optimized method.

There are Ahead of Time compilers out there.  (See 
http://en.wikipedia.org/wiki/AOT_compiler)  AOT has its drawbacks... 
"AOT can't usually perform some optimizations possible in JIT, like 
runtime profile-guided optimizations, pseudo-constant propagation or 
indirect/virtual function inlining."  HotSpot depends upon these 
optimizations heavily to improve performance.

On the bright side, Class Data Sharing 
(http://en.wikipedia.org/wiki/Java_performance#Class_data_sharing) does 
help reduce start up time.  I think this feature is being enhanced, but 
I am not sure.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/7/2013 10:17 AM, Nitsan Wakart wrote:
> This is probably mostly relevant for server applications, where the 
> usage and hardware are static and the same software is run for months 
> on end. Why is there no way to serialize the performance statistics 
> collected, or even the JIT compiled code and reload those on startup?
>
>
> ------------------------------------------------------------------------
> *From:* Stanimir Simeonoff <stanimir at riflexo.com>
> *To:* gregg.wonderly at pobox.com
> *Cc:* concurrency-interest at cs.oswego.edu
> *Sent:* Thursday, February 7, 2013 4:56 PM
> *Subject:* Re: [concurrency-interest] Thread priority
>
> Besides the iteration counts the JVM (may) add profiling data. For 
> instance: call sites. If the method is compiled w/ a single class only 
> and then another is loaded the method has to be deoptimized and 
> optimized again - obviously unwanted behavior. The same holds true for 
> inline caches.
>
> To put it simply: w/o the profiling data the resulting machine code 
> may not be of high quality. More also if the compiling threshold is 
> too low - that would result in a lot of "code cache" used and it 
> increases memory footprint + it may reach the memory limit for the 
> code cache as many invocations do not reach c2 (10k). There was such a 
> bug introduced w/ the tiered compilation in 6.0.25 - basically it 
> reached the 32mb of code cache super fast - and a lot of code didn't 
> reach C2 at all (or even c1 and remained interpreted). I had to 
> increase the code cache limits to ~256mb (staring off 32mb) to reach 
> to quota and decided to turn off tiered compilation altogether. There 
> was a rare bug as well, replacing c1 w/ c2 code resulted in crashing 
> the process.
> So, having low threshold is probably good for some microbenchmarks but 
> not so much for real applications.
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com 
> <mailto:gregg at cytetech.com>> wrote:
>
>     I've always wondered why these kinds of "large" invocation counts
>     are used.  In many methods, there will be a single entry in most
>     applications, and "loops" inside of that method could be optimized
>     much sooner.  In many of my desktop applications, I set the
>     invocation count (on the command line) to 100 or even 25, and get
>     faster startups, and better performance for the small amount of
>     time that I use the apps.  For the client VM, it really seems
>     strange to wait so long (1000 invocations), to compile with
>     instrumentation.  Then waiting for 10 times that many invocations
>     to decide on the final optimizations seems a bit of a stretch.
>
>     Are there real data values from lots of different users which
>     indicate that these "counts" are when people are ready to be more
>     productive?  I know that there are probably lots of degenerative
>     cases where optimizations will be missed without enough data.  But
>     it would seem better to go to native code early, and adapt
>     occasionally, rather than wait until you can be sure to be perfect.
>
>     Gregg Wonderly
>
>
>     On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>         With tiered compilation, once a method reaches 1,000 invocations
>         (configurable?), it is compiled with instrumentation.  Then
>         when it reaches
>         10,000 invocations (configurable), it is fully optimized using the
>         instrumentation profiling data.  For these operations, the JIT
>         threads should
>         run at a higher priority.  However, there are some
>         optimizations which are too
>         heavy to do at a high priority.  These optimizations should be
>         done at a low
>         priority.  Also, methods, which haven't quite reached the
>         1,000 invocations but
>         are being execute, could be compiled with instrumentation at a
>         low priority.
>
>         The low priority work will only be done if the CPU isn't maxed
>         out.  If any
>         other thread needs the CPU, then the low priority compiler
>         thread will be
>         immediately context switched off the core.  So, the low
>         priority compilation
>         will never significantly hurt the performance of the high
>         priority threads.  For
>         some work loads, the low priority threads may never get a
>         chance to run. That's
>         okay because the work isn't that important.
>
>         Nathan Reynolds
>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>         Architect | 602.333.9091
>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>         Technology
>
>         On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>
>             Thread priorities are usually NOT applied at all.
>             For insance:
>                  intx DefaultThreadPriority     = -1            
>              {product}
>                 intx JavaPriority10_To_OSPriority    = -1            
>              {product}
>                  intx JavaPriority1_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority2_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority3_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority4_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority5_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority6_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority7_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority8_To_OSPriority     = -1            
>              {product}
>                  intx JavaPriority9_To_OSPriority     = -1            
>              {product}
>
>             in other words unless specified :
>             -XXJavaPriority10_To_OSPriority=
>             it won't be mapped.
>
>             If applied the JVM compiler/GC threads may become starved
>             which you don't
>             want, so they have to work above normal prir (that request
>             root privileges).
>             Alternatively the normal java threads have to run w/ lower
>             prir which means
>             other process will have higher priority - also unpleasant.
>
>             Stanimir
>
>             On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>             <radhakrishnan.mohan at gmail.com
>             <mailto:radhakrishnan.mohan at gmail.com>
>             <mailto:radhakrishnan.mohan at gmail.com
>             <mailto:radhakrishnan.mohan at gmail.com>>> wrote:
>
>                 Hi,
>                           Can the Thread priority setting in the API
>             still be reliably
>                 used uniformly across processors ? There are other
>             concurrency patterns in
>                 the API but this setting is still there.
>
>
>                 Thanks,
>                 Mohan
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/b276ba4a/attachment-0001.html>

From gregg at cytetech.com  Thu Feb  7 12:55:34 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 11:55:34 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D694.40002@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<5113D694.40002@oracle.com>
Message-ID: <5113EA96.5020100@cytetech.com>

On 2/7/2013 10:30 AM, Nathan Reynolds wrote:
> How do we even collect that
> data?  We could run several tests and find the best value averaged for those
> tests, but there will always be a more optimal value for each individual
> application.  It seems this is something best for the program writer to tune.

I would say that using Preferences persistence would be perfectly acceptable. 
On the first application run, do it with 1000.  Get some data, and as you 
optimize methods, use the "numbers" to record what might work better.  I.e. if 
you instrument and then don't find any other odd things about "branch 
prediction" or some other metric, then record some indication to "optimize this 
early".  When the app starts up, go look in preferences, and use it to make some 
better choices.

Our computer systems are getting to be less and less worthy of hand holding and 
more and more capable of being "fully utilized" quickly.  Something which allows 
"history" to be recorded and used effectively would be great.

A documented API for developers to "pre-load" the optimization path would be 
awesome.  I can imagine being able to use the Jar manifest to provide some 
details about how code in a jar or packages in the jar, should be treated.

Many developers use profiling to find the "hot spots" and fix them up.  So, in 
the end, just "to-native" compilation should be all that needs to happen. 
Developers could indicate which "classes" and methods need specific 
instrumentation even, with javadoc like signatures.  Maybe even XML files stuck 
into a jar file, and pointed at by the manifest would be more flexible and tunable.

Gregg Wonderly


From gregg at cytetech.com  Thu Feb  7 12:56:52 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 11:56:52 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D694.40002@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<5113D694.40002@oracle.com>
Message-ID: <5113EAE4.7010306@cytetech.com>

On 2/7/2013 10:30 AM, Nathan Reynolds wrote:
> Sorry, I wasn't very specific.  The 10,000 invocation is the default for server
> VM.  For client, I think it is 1,500 invocations.

Okay, I knew that it used to be numbers like that for server vs client, and when 
you mentioned that there could be a second step some time later, I was confused, 
because I'd never seen that mentioned anywhere.

Gregg Wonderly


From gregg at cytetech.com  Thu Feb  7 13:01:49 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 12:01:49 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
Message-ID: <5113EC0D.8000103@cytetech.com>

Interesting points.  My experience has been, that setting it to 100 on many of 
my small desktop apps makes them startup much faster, and I don't seem to notice 
any performance problems with deoptimization occuring.  I haven't really done 
definitive analysis to prove it works just as good, but for desktop apps, launch 
time is often a very important productivity issue.

Gregg Wonderly

On 2/7/2013 10:56 AM, Stanimir Simeonoff wrote:
> Besides the iteration counts the JVM (may) add profiling data. For instance:
> call sites. If the method is compiled w/ a single class only and then another is
> loaded the method has to be deoptimized and optimized again - obviously unwanted
> behavior. The same holds true for inline caches.
>
> To put it simply: w/o the profiling data the resulting machine code may not be
> of high quality. More also if the compiling threshold is too low - that would
> result in a lot of "code cache" used and it increases memory footprint + it may
> reach the memory limit for the code cache as many invocations do not reach c2
> (10k). There was such a bug introduced w/ the tiered compilation in 6.0.25 -
> basically it reached the 32mb of code cache super fast - and a lot of code
> didn't reach C2 at all (or even c1 and remained interpreted). I had to increase
> the code cache limits to ~256mb (staring off 32mb) to reach to quota and decided
> to turn off tiered compilation altogether. There was a rare bug as well,
> replacing c1 w/ c2 code resulted in crashing the process.
> So, having low threshold is probably good for some microbenchmarks but not so
> much for real applications.
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com
> <mailto:gregg at cytetech.com>> wrote:
>
>     I've always wondered why these kinds of "large" invocation counts are used.
>       In many methods, there will be a single entry in most applications, and
>     "loops" inside of that method could be optimized much sooner.  In many of my
>     desktop applications, I set the invocation count (on the command line) to
>     100 or even 25, and get faster startups, and better performance for the
>     small amount of time that I use the apps.  For the client VM, it really
>     seems strange to wait so long (1000 invocations), to compile with
>     instrumentation.  Then waiting for 10 times that many invocations to decide
>     on the final optimizations seems a bit of a stretch.
>
>     Are there real data values from lots of different users which indicate that
>     these "counts" are when people are ready to be more productive?  I know that
>     there are probably lots of degenerative cases where optimizations will be
>     missed without enough data.  But it would seem better to go to native code
>     early, and adapt occasionally, rather than wait until you can be sure to be
>     perfect.
>
>     Gregg Wonderly
>
>
>     On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>         With tiered compilation, once a method reaches 1,000 invocations
>         (configurable?), it is compiled with instrumentation.  Then when it reaches
>         10,000 invocations (configurable), it is fully optimized using the
>         instrumentation profiling data.  For these operations, the JIT threads
>         should
>         run at a higher priority.  However, there are some optimizations which
>         are too
>         heavy to do at a high priority.  These optimizations should be done at a low
>         priority.  Also, methods, which haven't quite reached the 1,000
>         invocations but
>         are being execute, could be compiled with instrumentation at a low priority.
>
>         The low priority work will only be done if the CPU isn't maxed out.  If any
>         other thread needs the CPU, then the low priority compiler thread will be
>         immediately context switched off the core.  So, the low priority compilation
>         will never significantly hurt the performance of the high priority
>         threads.  For
>         some work loads, the low priority threads may never get a chance to run.
>         That's
>         okay because the work isn't that important.
>
>         Nathan Reynolds
>         <http://psr.us.oracle.com/__wiki/index.php/User:Nathan___Reynolds
>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>> |
>         Architect | 602.333.9091 <tel:602.333.9091>
>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>
>         On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>
>             Thread priorities are usually NOT applied at all.
>             For insance:
>                   intx DefaultThreadPriority                     = -1
>                 {product}
>                  intx JavaPriority10_To_OSPriority              = -1
>               {product}
>                   intx JavaPriority1_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority2_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority3_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority4_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority5_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority6_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority7_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority8_To_OSPriority               = -1
>                 {product}
>                   intx JavaPriority9_To_OSPriority               = -1
>                 {product}
>
>             in other words unless specified : -XXJavaPriority10_To___OSPriority=
>             it won't be mapped.
>
>             If applied the JVM compiler/GC threads may become starved which you
>             don't
>             want, so they have to work above normal prir (that request root
>             privileges).
>             Alternatively the normal java threads have to run w/ lower prir
>             which means
>             other process will have higher priority - also unpleasant.
>
>             Stanimir
>
>             On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>             <radhakrishnan.mohan at gmail.com
>             <mailto:radhakrishnan.mohan at gmail.com>
>             <mailto:radhakrishnan.mohan at __gmail.com
>             <mailto:radhakrishnan.mohan at gmail.com>>> wrote:
>
>                  Hi,
>                            Can the Thread priority setting in the API still be
>             reliably
>                  used uniformly across processors ? There are other concurrency
>             patterns in
>                  the API but this setting is still there.
>
>
>                  Thanks,
>                  Mohan
>
>                  _________________________________________________
>                  Concurrency-interest mailing list
>             Concurrency-interest at cs.__oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at __cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>
>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>             _________________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.__oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>         _________________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.__oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From vitalyd at gmail.com  Thu Feb  7 13:02:29 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 7 Feb 2013 13:02:29 -0500
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113E5F7.8050601@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<5113E5F7.8050601@oracle.com>
Message-ID: <CAHjP37GzWYrGxfipJv_KwdWFk4N3CSjx1ZLW1O_Z6Ex=4UrKWA@mail.gmail.com>

There are also VM args to override the compilation thresholds; if extra
trips through the methods don't yield better profile for a given app, they
can be reduced.

Sent from my phone
On Feb 7, 2013 12:38 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  > Why is there no way to serialize the performance statistics collected,
> or even the JIT compiled code and reload those on startup?
>
> I've wondered the same thing.  Before I brought it up a couple of years
> ago, it had already been considered.  The problem is that it takes more
> time to verify that the classes are the same than it does to just compile
> it anew.  Because of inlining, the number of classes that have to be
> considered is very large for just one optimized method.
>
> There are Ahead of Time compilers out there.  (See
> http://en.wikipedia.org/wiki/AOT_compiler)  AOT has its drawbacks... "AOT
> can't usually perform some optimizations possible in JIT, like runtime
> profile-guided optimizations, pseudo-constant propagation or
> indirect/virtual function inlining."  HotSpot depends upon these
> optimizations heavily to improve performance.
>
> On the bright side, Class Data Sharing (
> http://en.wikipedia.org/wiki/Java_performance#Class_data_sharing) does
> help reduce start up time.  I think this feature is being enhanced, but I
> am not sure.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/7/2013 10:17 AM, Nitsan Wakart wrote:
>
> This is probably mostly relevant for server applications, where the usage
> and hardware are static and the same software is run for months on end. Why
> is there no way to serialize the performance statistics collected, or even
> the JIT compiled code and reload those on startup?
>
>
>    ------------------------------
> *From:* Stanimir Simeonoff <stanimir at riflexo.com> <stanimir at riflexo.com>
> *To:* gregg.wonderly at pobox.com
> *Cc:* concurrency-interest at cs.oswego.edu
> *Sent:* Thursday, February 7, 2013 4:56 PM
> *Subject:* Re: [concurrency-interest] Thread priority
>
> Besides the iteration counts the JVM (may) add profiling data. For
> instance: call sites. If the method is compiled w/ a single class only and
> then another is loaded the method has to be deoptimized and optimized again
> - obviously unwanted behavior. The same holds true for inline caches.
>
> To put it simply: w/o the profiling data the resulting machine code may
> not be of high quality. More also if the compiling threshold is too low -
> that would result in a lot of "code cache" used and it increases memory
> footprint + it may reach the memory limit for the code cache as many
> invocations do not reach c2 (10k). There was such a bug introduced w/ the
> tiered compilation in 6.0.25 - basically it reached the 32mb of code cache
> super fast - and a lot of code didn't reach C2 at all (or even c1 and
> remained interpreted). I had to increase the code cache limits to ~256mb
> (staring off 32mb) to reach to quota and decided to turn off tiered
> compilation altogether. There was a rare bug as well, replacing c1 w/ c2
> code resulted in crashing the process.
> So, having low threshold is probably good for some microbenchmarks but not
> so much for real applications.
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com> wrote:
>
> I've always wondered why these kinds of "large" invocation counts are
> used.  In many methods, there will be a single entry in most applications,
> and "loops" inside of that method could be optimized much sooner.  In many
> of my desktop applications, I set the invocation count (on the command
> line) to 100 or even 25, and get faster startups, and better performance
> for the small amount of time that I use the apps.  For the client VM, it
> really seems strange to wait so long (1000 invocations), to compile with
> instrumentation.  Then waiting for 10 times that many invocations to decide
> on the final optimizations seems a bit of a stretch.
>
> Are there real data values from lots of different users which indicate
> that these "counts" are when people are ready to be more productive?  I
> know that there are probably lots of degenerative cases where optimizations
> will be missed without enough data.  But it would seem better to go to
> native code early, and adapt occasionally, rather than wait until you can
> be sure to be perfect.
>
> Gregg Wonderly
>
>
> On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>  With tiered compilation, once a method reaches 1,000 invocations
> (configurable?), it is compiled with instrumentation.  Then when it reaches
> 10,000 invocations (configurable), it is fully optimized using the
> instrumentation profiling data.  For these operations, the JIT threads
> should
> run at a higher priority.  However, there are some optimizations which are
> too
> heavy to do at a high priority.  These optimizations should be done at a
> low
> priority.  Also, methods, which haven't quite reached the 1,000
> invocations but
> are being execute, could be compiled with instrumentation at a low
> priority.
>
> The low priority work will only be done if the CPU isn't maxed out.  If any
> other thread needs the CPU, then the low priority compiler thread will be
> immediately context switched off the core.  So, the low priority
> compilation
> will never significantly hurt the performance of the high priority
> threads.  For
> some work loads, the low priority threads may never get a chance to run.
> That's
> okay because the work isn't that important.
>
>  Nathan Reynolds
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds><http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>|
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/><http://psr.us.oracle.com/>| Server Technology
>
> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>
>  Thread priorities are usually NOT applied at all.
> For insance:
>      intx DefaultThreadPriority                     = -1
>  {product}
>     intx JavaPriority10_To_OSPriority              = -1
>  {product}
>      intx JavaPriority1_To_OSPriority               = -1
>  {product}
>      intx JavaPriority2_To_OSPriority               = -1
>  {product}
>      intx JavaPriority3_To_OSPriority               = -1
>  {product}
>      intx JavaPriority4_To_OSPriority               = -1
>  {product}
>      intx JavaPriority5_To_OSPriority               = -1
>  {product}
>      intx JavaPriority6_To_OSPriority               = -1
>  {product}
>      intx JavaPriority7_To_OSPriority               = -1
>  {product}
>      intx JavaPriority8_To_OSPriority               = -1
>  {product}
>      intx JavaPriority9_To_OSPriority               = -1
>  {product}
>
> in other words unless specified : -XXJavaPriority10_To_OSPriority=
> it won't be mapped.
>
> If applied the JVM compiler/GC threads may become starved which you don't
> want, so they have to work above normal prir (that request root
> privileges).
> Alternatively the normal java threads have to run w/ lower prir which means
> other process will have higher priority - also unpleasant.
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>  <radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan at gmail.com>>
> wrote:
>
>     Hi,
>               Can the Thread priority setting in the API still be reliably
>     used uniformly across processors ? There are other concurrency
> patterns in
>     the API but this setting is still there.
>
>
>     Thanks,
>     Mohan
>
>     _______________________________________________
>     Concurrency-interest mailing list
>      Concurrency-interest at cs.oswego.edu <mailto:
> Concurrency-interest at cs.oswego.edu>
>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/3b9755f7/attachment-0001.html>

From william.louth at jinspired.com  Thu Feb  7 13:20:05 2013
From: william.louth at jinspired.com (william.louth at jinspired.com)
Date: Thu, 07 Feb 2013 18:20:05 +0000
Subject: [concurrency-interest] Thread priority
Message-ID: <W610072939227301360261205@webmail105>

This is best handled using QoS at the application/code level. The following link has numerous application showing how to influence the system dynamics of an application based on a runtime resource management framework providing dynamic service classification (with priority levels), virtual resources used to control flow, as well dynamic prioritization independent of service classification.

http://www.jinspired.com/research/quality-of-service-for-apps

William

>-----Original Message-----
>From: Mohan Radhakrishnan [mailto:radhakrishnan.mohan at gmail.com]
>Sent: Thursday, February 7, 2013 04:20 AM
>To: Concurrency-interest at cs.oswego.edu
>Subject: [concurrency-interest] Thread priority
>
>Hi,
>          Can the Thread priority setting in the API still be reliably used
>uniformly across processors ? There are other concurrency patterns in the
>API but this setting is still there.
>
>
>Thanks,
>Mohan
>




From william.louth at jinspired.com  Thu Feb  7 13:27:39 2013
From: william.louth at jinspired.com (william.louth at jinspired.com)
Date: Thu, 07 Feb 2013 18:27:39 +0000
Subject: [concurrency-interest] Thread priority
Message-ID: <W6156920699316131360261659@webmail101>


>Because you can't step into the same river twice ;-)  [http://en.wikiquote.org/wiki/Heraclitus]
Regards

I would not be so sure about that http://www.jinspired.com/site/how-to-execute-software-behavior-faster-than-wall-clock-time

> Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun Java Champion
IEEE Certified Software Development Professional
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz 


Nitsan Wakart wrote:
> This is probably mostly relevant for server applications, where the usage and hardware are static and the same software is run for months on end. Why is there no way to serialize the performance statistics collected, or even the JIT compiled code and reload those on startup?




From studdugie at gmail.com  Thu Feb  7 14:11:19 2013
From: studdugie at gmail.com (Dane Foster)
Date: Thu, 7 Feb 2013 14:11:19 -0500
Subject: [concurrency-interest] Phaser and StampedLock Presentation
In-Reply-To: <5113453D.8090403@javaspecialists.eu>
References: <5113453D.8090403@javaspecialists.eu>
Message-ID: <CA+Wxin+h1pn18wrYFMs94zDFc8-Cc-uNQvVvacP0vDDQZfMptQ@mail.gmail.com>

Good stuff. Thanks.

Dane


On Thu, Feb 7, 2013 at 1:10 AM, Dr Heinz M. Kabutz <heinz at javaspecialists.eu
> wrote:

> For those of you who might be interested in the new StampedLock that Doug
> Lea wrote, I gave a talk on Tuesday at JFokus in Sweden about it, also
> showing some interesting performance results.
>
> You can download the slides here:
>
> http://javaspecialists.eu/**talks/jfokus13/**PhaserAndStampedLock.pdf<http://javaspecialists.eu/talks/jfokus13/PhaserAndStampedLock.pdf>
>
> Stephen Chin and I also did a "Nighthacking" session, where we looked at
> Fork/Join and then again StampedLock:
>
> http://vimeo.com/**javaspecialists/jfokus-2013-**nighthacking<http://vimeo.com/javaspecialists/jfokus-2013-nighthacking>
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/c78fc3c7/attachment.html>

From nitsanw at yahoo.com  Thu Feb  7 15:20:41 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 7 Feb 2013 12:20:41 -0800 (PST)
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113E5F7.8050601@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<1360257466.23438.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<5113E5F7.8050601@oracle.com>
Message-ID: <1360268441.46127.YahooMailNeo@web120702.mail.ne1.yahoo.com>

This is a problem some people in the low latency space have had to find ways around by introducing all sorts of JVM warmup mechnisms to their systems. In the case of a static codebase/hardware I would think there is plenty to be won by utilizing the observations that went into JIT compiling the system before.
You may not be able to step in the same river twice, but you can learn something about rivers from your first visit :)
To put this in context consider a trading system where trades may be infrequent, but their latency is of high importance. Waiting for 10K trades to ly by before you get the performance you want is not so good. If I could combine the JIT profile with an AOT compiler to start from a better point the next time I start up I'd be very happy. This is similar to using table statistics to optimize a database table index.




________________________________
 From: Nathan Reynolds <nathan.reynolds at oracle.com>
To: concurrency-interest at cs.oswego.edu 
Sent: Thursday, February 7, 2013 5:35 PM
Subject: Re: [concurrency-interest] Thread priority
 

> Why is there no way to serialize the performance statistics collected, or even the JIT compiled code and reload those on startup?

I've wondered the same thing.? Before I brought it up a couple of
      years ago, it had already been considered.? The problem is that it
      takes more time to verify that the classes are the same than it
      does to just compile it anew.? Because of inlining, the number of
      classes that have to be considered is very large for just one
      optimized method.

There are Ahead of Time compilers out there.? (See
      http://en.wikipedia.org/wiki/AOT_compiler)? AOT has its
      drawbacks... "AOT can't usually perform some optimizations
      possible in JIT, like runtime profile-guided optimizations,
      pseudo-constant propagation or indirect/virtual function
      inlining."? HotSpot depends upon these optimizations heavily to
      improve performance.

On the bright side, Class Data Sharing
      (http://en.wikipedia.org/wiki/Java_performance#Class_data_sharing)
      does help reduce start up time.? I think this feature is being
      enhanced, but I am not sure.


Nathan Reynolds | Architect | 602.333.9091
Oracle PSR Engineering | Server Technology

On 2/7/2013 10:17 AM, Nitsan Wakart wrote:

This is probably mostly relevant for server applications, where the usage and hardware are static and the same software is run for months on end. Why is there no way to serialize the performance statistics collected, or even the JIT compiled code and reload those on startup?
>
>
>
>
>
>
>________________________________
> From: Stanimir Simeonoff <stanimir at riflexo.com>
>To: gregg.wonderly at pobox.com 
>Cc: concurrency-interest at cs.oswego.edu 
>Sent: Thursday, February 7, 2013 4:56 PM
>Subject: Re: [concurrency-interest] Thread priority
> 
>
>Besides the iteration counts the JVM (may) add profiling data. For instance: call sites. If the method is compiled w/ a single class only and then another is loaded the method has to be deoptimized and optimized again - obviously unwanted behavior. The same holds true for inline caches.
>
>To put it simply: w/o the profiling data the resulting
              machine code may not be of high quality. More also if the
              compiling threshold is too low - that would result in a
              lot of "code cache" used and it increases memory footprint
              + it may reach the memory limit for the code cache as many
              invocations do not reach c2 (10k). There was such a bug
              introduced w/ the tiered compilation in 6.0.25 - basically
              it reached the 32mb of code cache super fast - and a lot
              of code didn't reach C2 at all (or even c1 and remained
              interpreted). I had to increase the code cache limits to
              ~256mb (staring off 32mb) to reach to quota and decided to
              turn off tiered compilation altogether. There was a rare
              bug as well, replacing c1 w/ c2 code resulted in crashing
              the process.
>So, having low threshold is probably good for some
              microbenchmarks but not so much for real applications.
>
>Stanimir
>
>
>On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com> wrote:
>
>I've always wondered why these kinds of "large" invocation counts are used. ?In many methods, there will be a single entry in most applications, and "loops" inside of that method could be optimized much sooner. ?In many of my desktop applications, I set the invocation count (on the command line) to 100 or even 25, and get faster startups, and better performance for the small amount of time that I use the apps. ?For the client VM, it really seems strange to wait so long (1000 invocations), to compile with instrumentation. ?Then waiting for 10 times that many invocations to decide on the final optimizations seems a bit of a stretch.
>>
>>Are there real data values from lots of different
                  users which indicate that these "counts" are when
                  people are ready to be more productive? ?I know that
                  there are probably lots of degenerative cases where
                  optimizations will be missed without enough data. ?But
                  it would seem better to go to native code early, and
                  adapt occasionally, rather than wait until you can be
                  sure to be perfect.
>>
>>Gregg Wonderly 
>>
>>
>>On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>>
>>With tiered compilation, once a method reaches 1,000 invocations
>>>(configurable?), it is compiled with
                      instrumentation. ?Then when it reaches
>>>10,000 invocations (configurable), it is fully
                      optimized using the
>>>instrumentation profiling data. ?For these
                      operations, the JIT threads should
>>>run at a higher priority. ?However, there are some
                      optimizations which are too
>>>heavy to do at a high priority. ?These
                      optimizations should be done at a low
>>>priority. ?Also, methods, which haven't quite
                      reached the 1,000 invocations but
>>>are being execute, could be compiled with
                      instrumentation at a low priority.
>>>
>>>The low priority work will only be done if the CPU
                      isn't maxed out. ?If any
>>>other thread needs the CPU, then the low priority
                      compiler thread will be
>>>immediately context switched off the core. ?So,
                      the low priority compilation
>>>will never significantly hurt the performance of
                      the high priority threads. ?For
>>>some work loads, the low priority threads may
                      never get a chance to run. That's
>>>okay because the work isn't that important.
>>>
>>>
Nathan Reynolds <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>>Architect | 602.333.9091
>>>Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology 
>>>
>>>On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>>
>>>Thread priorities are usually NOT applied at all.
>>>>For insance:
>>>>? ? ?intx DefaultThreadPriority ? ? ? ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? intx JavaPriority10_To_OSPriority ? ? ? ? ?
                        ? ?= -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority1_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority2_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority3_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority4_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority5_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority6_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority7_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority8_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>? ? ?intx JavaPriority9_To_OSPriority ? ? ? ? ?
                        ? ? = -1 ? ? ? ? ? ? ?{product}
>>>>
>>>>in other words unless specified :
                        -XXJavaPriority10_To_OSPriority=
>>>>it won't be mapped.
>>>>
>>>>If applied the JVM compiler/GC threads may
                        become starved which you don't
>>>>want, so they have to work above normal prir
                        (that request root privileges).
>>>>Alternatively the normal java threads have to
                        run w/ lower prir which means
>>>>other process will have higher priority - also
                        unpleasant.
>>>>
>>>>Stanimir
>>>>
>>>>On Thu, Feb 7, 2013 at 5:20 AM, Mohan
                        Radhakrishnan
>>>>
>>>><radhakrishnan.mohan at gmail.com <mailto:radhakrishnan.mohan at gmail.com>> wrote:
>>>>
>>>>? ? Hi,
>>>>? ? ? ? ? ? ? Can the Thread priority setting in
                        the API still be reliably
>>>>? ? used uniformly across processors ? There are
                        other concurrency patterns in
>>>>? ? the API but this setting is still there.
>>>>
>>>>
>>>>? ? Thanks,
>>>>? ? Mohan
>>>>
>>>>? ? _______________________________________________
>>>>? ? Concurrency-interest mailing list
>>>>
? ? Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
>>>>
>>>>? ? http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>_______________________________________________
>>>>Concurrency-interest mailing list
>>>>Concurrency-interest at cs.oswego.edu
>>>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>>_______________________________________________
>>>Concurrency-interest mailing list
>>>Concurrency-interest at cs.oswego.edu
>>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>_______________________________________________
>>Concurrency-interest mailing list
>>Concurrency-interest at cs.oswego.edu
>>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/609639ae/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Feb  7 16:38:53 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 07 Feb 2013 21:38:53 +0000
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113D694.40002@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<5113D694.40002@oracle.com>
Message-ID: <51141EED.3010207@oracle.com>

-Xcomp

Alex

On 07/02/2013 16:30, Nathan Reynolds wrote:
> Sorry, I wasn't very specific.  The 10,000 invocation is the default 
> for server VM.  For client, I think it is 1,500 invocations.
>
> I simplified my explanation.  Optimization for server doesn't start 
> until 10,000 method invocations or 10,000 loop iterations. However, 
> the optimized code won't be used unless the thread enters the method 
> or iteration after the compilation is done.
>
> JRockit JVM never executed bytecode.  When the class was loaded, it 
> immediately created native code albeit not fully optimized. This made 
> JRockit startup times much slower than HotSpot. However, JRockit was 
> usually able to get to full speed much faster.  This prompted HotSpot 
> to adopted a tiered compilation strategy.  1,000 invocations was used 
> to avoid the slow startup times yet get to native code sooner than 
> waiting until 10,000 invocations.  If configurable, you can probably 
> get HotSpot to behave like JRockit by setting the first compilation to 
> 0 or 1 invocations.
>
> 1,000 and 10,000 seem like arbitrary values with a little bit of 
> experience or science behind them.  Sure, anyone could tune these 
> values specific for their application to get the optimal start up and 
> warm up performance.  But, how can we pick the best value averaged for 
> all workloads?  How do we even collect that data?  We could run 
> several tests and find the best value averaged for those tests, but 
> there will always be a more optimal value for each individual 
> application.  It seems this is something best for the program writer 
> to tune.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/7/2013 9:08 AM, Gregg Wonderly wrote:
>> I've always wondered why these kinds of "large" invocation counts are 
>> used. In many methods, there will be a single entry in most 
>> applications, and "loops" inside of that method could be optimized 
>> much sooner.  In many of my desktop applications, I set the 
>> invocation count (on the command line) to 100 or even 25, and get 
>> faster startups, and better performance for the small amount of time 
>> that I use the apps.  For the client VM, it really seems strange to 
>> wait so long (1000 invocations), to compile with instrumentation.  
>> Then waiting for 10 times that many invocations to decide on the 
>> final optimizations seems a bit of a stretch.
>>
>> Are there real data values from lots of different users which 
>> indicate that these "counts" are when people are ready to be more 
>> productive?  I know that there are probably lots of degenerative 
>> cases where optimizations will be missed without enough data.  But it 
>> would seem better to go to native code early, and adapt occasionally, 
>> rather than wait until you can be sure to be perfect.
>>
>> Gregg Wonderly
>>
>> On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>>> With tiered compilation, once a method reaches 1,000 invocations
>>> (configurable?), it is compiled with instrumentation.  Then when it 
>>> reaches
>>> 10,000 invocations (configurable), it is fully optimized using the
>>> instrumentation profiling data.  For these operations, the JIT 
>>> threads should
>>> run at a higher priority.  However, there are some optimizations 
>>> which are too
>>> heavy to do at a high priority.  These optimizations should be done 
>>> at a low
>>> priority.  Also, methods, which haven't quite reached the 1,000 
>>> invocations but
>>> are being execute, could be compiled with instrumentation at a low 
>>> priority.
>>>
>>> The low priority work will only be done if the CPU isn't maxed out.  
>>> If any
>>> other thread needs the CPU, then the low priority compiler thread 
>>> will be
>>> immediately context switched off the core.  So, the low priority 
>>> compilation
>>> will never significantly hurt the performance of the high priority 
>>> threads.  For
>>> some work loads, the low priority threads may never get a chance to 
>>> run. That's
>>> okay because the work isn't that important.
>>>
>>> Nathan Reynolds 
>>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>> Architect | 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>> On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>>> Thread priorities are usually NOT applied at all.
>>>> For insance:
>>>>      intx DefaultThreadPriority                     = 
>>>> -1              {product}
>>>>     intx JavaPriority10_To_OSPriority              = 
>>>> -1              {product}
>>>>      intx JavaPriority1_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority2_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority3_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority4_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority5_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority6_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority7_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority8_To_OSPriority               = 
>>>> -1              {product}
>>>>      intx JavaPriority9_To_OSPriority               = 
>>>> -1              {product}
>>>>
>>>> in other words unless specified : -XXJavaPriority10_To_OSPriority=
>>>> it won't be mapped.
>>>>
>>>> If applied the JVM compiler/GC threads may become starved which you 
>>>> don't
>>>> want, so they have to work above normal prir (that request root 
>>>> privileges).
>>>> Alternatively the normal java threads have to run w/ lower prir 
>>>> which means
>>>> other process will have higher priority - also unpleasant.
>>>>
>>>> Stanimir
>>>>
>>>> On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>>> <radhakrishnan.mohan at gmail.com 
>>>> <mailto:radhakrishnan.mohan at gmail.com>> wrote:
>>>>
>>>>     Hi,
>>>>               Can the Thread priority setting in the API still be 
>>>> reliably
>>>>     used uniformly across processors ? There are other concurrency 
>>>> patterns in
>>>>     the API but this setting is still there.
>>>>
>>>>
>>>>     Thanks,
>>>>     Mohan
>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu 
>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130207/50983402/attachment.html>

From oleksandr.otenko at oracle.com  Thu Feb  7 16:46:50 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 07 Feb 2013 21:46:50 +0000
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113EA96.5020100@cytetech.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<5113D694.40002@oracle.com> <5113EA96.5020100@cytetech.com>
Message-ID: <511420CA.5090207@oracle.com>

Essentially, core dump, then boot the compiled classes and compiler 
state from it, discarding application state. :-)

Alex


On 07/02/2013 17:55, Gregg Wonderly wrote:
> On 2/7/2013 10:30 AM, Nathan Reynolds wrote:
>> How do we even collect that
>> data?  We could run several tests and find the best value averaged 
>> for those
>> tests, but there will always be a more optimal value for each individual
>> application.  It seems this is something best for the program writer 
>> to tune.
>
> I would say that using Preferences persistence would be perfectly 
> acceptable. On the first application run, do it with 1000.  Get some 
> data, and as you optimize methods, use the "numbers" to record what 
> might work better.  I.e. if you instrument and then don't find any 
> other odd things about "branch prediction" or some other metric, then 
> record some indication to "optimize this early".  When the app starts 
> up, go look in preferences, and use it to make some better choices.
>
> Our computer systems are getting to be less and less worthy of hand 
> holding and more and more capable of being "fully utilized" quickly.  
> Something which allows "history" to be recorded and used effectively 
> would be great.
>
> A documented API for developers to "pre-load" the optimization path 
> would be awesome.  I can imagine being able to use the Jar manifest to 
> provide some details about how code in a jar or packages in the jar, 
> should be treated.
>
> Many developers use profiling to find the "hot spots" and fix them 
> up.  So, in the end, just "to-native" compilation should be all that 
> needs to happen. Developers could indicate which "classes" and methods 
> need specific instrumentation even, with javadoc like signatures.  
> Maybe even XML files stuck into a jar file, and pointed at by the 
> manifest would be more flexible and tunable.
>
> Gregg Wonderly
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Thu Feb  7 17:50:44 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 8 Feb 2013 00:50:44 +0200
Subject: [concurrency-interest] Thread priority
In-Reply-To: <5113EC0D.8000103@cytetech.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<5113EC0D.8000103@cytetech.com>
Message-ID: <CAEJX8oqvvGmbDumSi5A9xWW=H9u=Z41dXP9F=e5MLLE4QDUsFg@mail.gmail.com>

The shorter start-up times can be attributed usually to SSD (or disk cache)
rather than pure compilations. The client jvm/compiler already shares data
(i.e. classes.jsa), so it does boot faster.
Something like this should be a good start regarding the "advanced" XX
options about JIT.
$ java -XX:+PrintFlagsFinal -version | grep Compi | less

Stanimir

On Thu, Feb 7, 2013 at 8:01 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> Interesting points.  My experience has been, that setting it to 100 on
> many of my small desktop apps makes them startup much faster, and I don't
> seem to notice any performance problems with deoptimization occuring.  I
> haven't really done definitive analysis to prove it works just as good, but
> for desktop apps, launch time is often a very important productivity issue.
>
> Gregg Wonderly
>
>
> On 2/7/2013 10:56 AM, Stanimir Simeonoff wrote:
>
>> Besides the iteration counts the JVM (may) add profiling data. For
>> instance:
>> call sites. If the method is compiled w/ a single class only and then
>> another is
>> loaded the method has to be deoptimized and optimized again - obviously
>> unwanted
>> behavior. The same holds true for inline caches.
>>
>> To put it simply: w/o the profiling data the resulting machine code may
>> not be
>> of high quality. More also if the compiling threshold is too low - that
>> would
>> result in a lot of "code cache" used and it increases memory footprint +
>> it may
>> reach the memory limit for the code cache as many invocations do not
>> reach c2
>> (10k). There was such a bug introduced w/ the tiered compilation in
>> 6.0.25 -
>> basically it reached the 32mb of code cache super fast - and a lot of code
>> didn't reach C2 at all (or even c1 and remained interpreted). I had to
>> increase
>> the code cache limits to ~256mb (staring off 32mb) to reach to quota and
>> decided
>> to turn off tiered compilation altogether. There was a rare bug as well,
>> replacing c1 w/ c2 code resulted in crashing the process.
>> So, having low threshold is probably good for some microbenchmarks but
>> not so
>> much for real applications.
>>
>> Stanimir
>>
>> On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com
>> <mailto:gregg at cytetech.com>> wrote:
>>
>>     I've always wondered why these kinds of "large" invocation counts are
>> used.
>>       In many methods, there will be a single entry in most applications,
>> and
>>     "loops" inside of that method could be optimized much sooner.  In
>> many of my
>>     desktop applications, I set the invocation count (on the command
>> line) to
>>     100 or even 25, and get faster startups, and better performance for
>> the
>>     small amount of time that I use the apps.  For the client VM, it
>> really
>>     seems strange to wait so long (1000 invocations), to compile with
>>     instrumentation.  Then waiting for 10 times that many invocations to
>> decide
>>     on the final optimizations seems a bit of a stretch.
>>
>>     Are there real data values from lots of different users which
>> indicate that
>>     these "counts" are when people are ready to be more productive?  I
>> know that
>>     there are probably lots of degenerative cases where optimizations
>> will be
>>     missed without enough data.  But it would seem better to go to native
>> code
>>     early, and adapt occasionally, rather than wait until you can be sure
>> to be
>>     perfect.
>>
>>     Gregg Wonderly
>>
>>
>>     On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>>
>>         With tiered compilation, once a method reaches 1,000 invocations
>>         (configurable?), it is compiled with instrumentation.  Then when
>> it reaches
>>         10,000 invocations (configurable), it is fully optimized using the
>>         instrumentation profiling data.  For these operations, the JIT
>> threads
>>         should
>>         run at a higher priority.  However, there are some optimizations
>> which
>>         are too
>>         heavy to do at a high priority.  These optimizations should be
>> done at a low
>>         priority.  Also, methods, which haven't quite reached the 1,000
>>         invocations but
>>         are being execute, could be compiled with instrumentation at a
>> low priority.
>>
>>         The low priority work will only be done if the CPU isn't maxed
>> out.  If any
>>         other thread needs the CPU, then the low priority compiler thread
>> will be
>>         immediately context switched off the core.  So, the low priority
>> compilation
>>         will never significantly hurt the performance of the high priority
>>         threads.  For
>>         some work loads, the low priority threads may never get a chance
>> to run.
>>         That's
>>         okay because the work isn't that important.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/__**wiki/index.php/User:Nathan___**
>> Reynolds<http://psr.us.oracle.com/__wiki/index.php/User:Nathan___Reynolds>
>>         <http://psr.us.oracle.com/**wiki/index.php/User:Nathan_**Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>>
>> |
>>         Architect | 602.333.9091 <tel:602.333.9091>
>>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>> Technology
>>
>>         On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>>
>>             Thread priorities are usually NOT applied at all.
>>             For insance:
>>                   intx DefaultThreadPriority                     = -1
>>                 {product}
>>                  intx JavaPriority10_To_OSPriority              = -1
>>               {product}
>>                   intx JavaPriority1_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority2_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority3_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority4_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority5_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority6_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority7_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority8_To_OSPriority               = -1
>>                 {product}
>>                   intx JavaPriority9_To_OSPriority               = -1
>>                 {product}
>>
>>             in other words unless specified : -XXJavaPriority10_To___**
>> OSPriority=
>>
>>             it won't be mapped.
>>
>>             If applied the JVM compiler/GC threads may become starved
>> which you
>>             don't
>>             want, so they have to work above normal prir (that request
>> root
>>             privileges).
>>             Alternatively the normal java threads have to run w/ lower
>> prir
>>             which means
>>             other process will have higher priority - also unpleasant.
>>
>>             Stanimir
>>
>>             On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>>             <radhakrishnan.mohan at gmail.com
>>             <mailto:radhakrishnan.mohan@**gmail.com<radhakrishnan.mohan at gmail.com>
>> >
>>             <mailto:radhakrishnan.mohan at __**gmail.com <http://gmail.com>
>>
>>             <mailto:radhakrishnan.mohan@**gmail.com<radhakrishnan.mohan at gmail.com>>>>
>> wrote:
>>
>>                  Hi,
>>                            Can the Thread priority setting in the API
>> still be
>>             reliably
>>                  used uniformly across processors ? There are other
>> concurrency
>>             patterns in
>>                  the API but this setting is still there.
>>
>>
>>                  Thanks,
>>                  Mohan
>>
>>                  ______________________________**___________________
>>                  Concurrency-interest mailing list
>>             Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>             <mailto:Concurrency-interest at _**_cs.oswego.edu
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>
>>
>>             http://cs.oswego.edu/mailman/_**
>> _listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>             <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>
>>
>>             ______________________________**___________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>             http://cs.oswego.edu/mailman/_**
>> _listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>             <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>
>>
>>         ______________________________**___________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>         http://cs.oswego.edu/mailman/_**_listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>         <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>     ______________________________**___________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu> <mailto:
>> Concurrency-interest@**cs.oswego.edu <Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/_**_listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>     <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130208/818f9700/attachment-0001.html>

From gregg at cytetech.com  Thu Feb  7 23:15:32 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 22:15:32 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <511420CA.5090207@oracle.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<5113D694.40002@oracle.com> <5113EA96.5020100@cytetech.com>
	<511420CA.5090207@oracle.com>
Message-ID: <51147BE4.4030903@cytetech.com>

On 2/7/2013 3:46 PM, oleksandr otenko wrote:
> Essentially, core dump, then boot the compiled classes and compiler state from
> it, discarding application state. :-)

Yes we used to do this in the previous century.  I think we've come far enough 
in our practices to do something that is much more flexible, tunable and not 
dependent on memory layout from the last run.

Gregg

>
> Alex
>
>
> On 07/02/2013 17:55, Gregg Wonderly wrote:
>> On 2/7/2013 10:30 AM, Nathan Reynolds wrote:
>>> How do we even collect that
>>> data?  We could run several tests and find the best value averaged for those
>>> tests, but there will always be a more optimal value for each individual
>>> application.  It seems this is something best for the program writer to tune.
>>
>> I would say that using Preferences persistence would be perfectly acceptable.
>> On the first application run, do it with 1000.  Get some data, and as you
>> optimize methods, use the "numbers" to record what might work better.  I.e. if
>> you instrument and then don't find any other odd things about "branch
>> prediction" or some other metric, then record some indication to "optimize
>> this early".  When the app starts up, go look in preferences, and use it to
>> make some better choices.
>>
>> Our computer systems are getting to be less and less worthy of hand holding
>> and more and more capable of being "fully utilized" quickly. Something which
>> allows "history" to be recorded and used effectively would be great.
>>
>> A documented API for developers to "pre-load" the optimization path would be
>> awesome.  I can imagine being able to use the Jar manifest to provide some
>> details about how code in a jar or packages in the jar, should be treated.
>>
>> Many developers use profiling to find the "hot spots" and fix them up.  So, in
>> the end, just "to-native" compilation should be all that needs to happen.
>> Developers could indicate which "classes" and methods need specific
>> instrumentation even, with javadoc like signatures. Maybe even XML files stuck
>> into a jar file, and pointed at by the manifest would be more flexible and
>> tunable.
>>
>> Gregg Wonderly
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From gregg at cytetech.com  Thu Feb  7 23:19:23 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 07 Feb 2013 22:19:23 -0600
Subject: [concurrency-interest] Thread priority
In-Reply-To: <CAEJX8oqvvGmbDumSi5A9xWW=H9u=Z41dXP9F=e5MLLE4QDUsFg@mail.gmail.com>
References: <CAOoXFP-tt9jE7YWwkjWXQY7bu6jJ4gXmM54AY7cLXY3F2oXBDQ@mail.gmail.com>
	<CAEJX8opWOZ=ELoRNSeEKYpZDZcd62K-bGctt2P=kv8Dut_r31A@mail.gmail.com>
	<5113CD21.5090901@oracle.com> <5113D176.5070309@cytetech.com>
	<CAEJX8opbntfaN4Z_DDNfX51WdXq-cpKkYya+hh6C_w7y7zMVdg@mail.gmail.com>
	<5113EC0D.8000103@cytetech.com>
	<CAEJX8oqvvGmbDumSi5A9xWW=H9u=Z41dXP9F=e5MLLE4QDUsFg@mail.gmail.com>
Message-ID: <51147CCB.4010602@cytetech.com>

On 2/7/2013 4:50 PM, Stanimir Simeonoff wrote:
> The shorter start-up times can be attributed usually to SSD (or disk cache)
> rather than pure compilations.

That is probably true for some cases.  In my case, I tried both ways, 
repeatedly, and it was demonstrably faster.  In particular, one of my apps uses 
a security manager, and the security management schemes in Java are very 
expensive, using countless locks and thread strangling which benefits heavily 
from early compilation.

> The client jvm/compiler already shares data (i.e.
> classes.jsa), so it does boot faster.

I don't think there is much data being shared in my applications.  During 
startup, there are just thousands of objects being created and managed.

Gregg

> Something like this should be a good start regarding the "advanced" XX options
> about JIT.
> $ java -XX:+PrintFlagsFinal -version | grep Compi | less
>
> Stanimir
>
> On Thu, Feb 7, 2013 at 8:01 PM, Gregg Wonderly <gregg at cytetech.com
> <mailto:gregg at cytetech.com>> wrote:
>
>     Interesting points.  My experience has been, that setting it to 100 on many
>     of my small desktop apps makes them startup much faster, and I don't seem to
>     notice any performance problems with deoptimization occuring.  I haven't
>     really done definitive analysis to prove it works just as good, but for
>     desktop apps, launch time is often a very important productivity issue.
>
>     Gregg Wonderly
>
>
>     On 2/7/2013 10:56 AM, Stanimir Simeonoff wrote:
>
>         Besides the iteration counts the JVM (may) add profiling data. For instance:
>         call sites. If the method is compiled w/ a single class only and then
>         another is
>         loaded the method has to be deoptimized and optimized again - obviously
>         unwanted
>         behavior. The same holds true for inline caches.
>
>         To put it simply: w/o the profiling data the resulting machine code may
>         not be
>         of high quality. More also if the compiling threshold is too low - that
>         would
>         result in a lot of "code cache" used and it increases memory footprint +
>         it may
>         reach the memory limit for the code cache as many invocations do not
>         reach c2
>         (10k). There was such a bug introduced w/ the tiered compilation in 6.0.25 -
>         basically it reached the 32mb of code cache super fast - and a lot of code
>         didn't reach C2 at all (or even c1 and remained interpreted). I had to
>         increase
>         the code cache limits to ~256mb (staring off 32mb) to reach to quota and
>         decided
>         to turn off tiered compilation altogether. There was a rare bug as well,
>         replacing c1 w/ c2 code resulted in crashing the process.
>         So, having low threshold is probably good for some microbenchmarks but
>         not so
>         much for real applications.
>
>         Stanimir
>
>         On Thu, Feb 7, 2013 at 6:08 PM, Gregg Wonderly <gregg at cytetech.com
>         <mailto:gregg at cytetech.com>
>         <mailto:gregg at cytetech.com <mailto:gregg at cytetech.com>>> wrote:
>
>              I've always wondered why these kinds of "large" invocation counts
>         are used.
>                In many methods, there will be a single entry in most
>         applications, and
>              "loops" inside of that method could be optimized much sooner.  In
>         many of my
>              desktop applications, I set the invocation count (on the command
>         line) to
>              100 or even 25, and get faster startups, and better performance for the
>              small amount of time that I use the apps.  For the client VM, it really
>              seems strange to wait so long (1000 invocations), to compile with
>              instrumentation.  Then waiting for 10 times that many invocations
>         to decide
>              on the final optimizations seems a bit of a stretch.
>
>              Are there real data values from lots of different users which
>         indicate that
>              these "counts" are when people are ready to be more productive?  I
>         know that
>              there are probably lots of degenerative cases where optimizations
>         will be
>              missed without enough data.  But it would seem better to go to
>         native code
>              early, and adapt occasionally, rather than wait until you can be
>         sure to be
>              perfect.
>
>              Gregg Wonderly
>
>
>              On 2/7/2013 9:49 AM, Nathan Reynolds wrote:
>
>                  With tiered compilation, once a method reaches 1,000 invocations
>                  (configurable?), it is compiled with instrumentation.  Then
>         when it reaches
>                  10,000 invocations (configurable), it is fully optimized using the
>                  instrumentation profiling data.  For these operations, the JIT
>         threads
>                  should
>                  run at a higher priority.  However, there are some
>         optimizations which
>                  are too
>                  heavy to do at a high priority.  These optimizations should be
>         done at a low
>                  priority.  Also, methods, which haven't quite reached the 1,000
>                  invocations but
>                  are being execute, could be compiled with instrumentation at a
>         low priority.
>
>                  The low priority work will only be done if the CPU isn't maxed
>         out.  If any
>                  other thread needs the CPU, then the low priority compiler
>         thread will be
>                  immediately context switched off the core.  So, the low
>         priority compilation
>                  will never significantly hurt the performance of the high priority
>                  threads.  For
>                  some work loads, the low priority threads may never get a
>         chance to run.
>                  That's
>                  okay because the work isn't that important.
>
>                  Nathan Reynolds
>
>         <http://psr.us.oracle.com/____wiki/index.php/User:Nathan_____Reynolds
>         <http://psr.us.oracle.com/__wiki/index.php/User:Nathan___Reynolds>
>
>         <http://psr.us.oracle.com/__wiki/index.php/User:Nathan___Reynolds
>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>>> |
>                  Architect | 602.333.9091 <tel:602.333.9091> <tel:602.333.9091
>         <tel:602.333.9091>>
>
>                  Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>         Technology
>
>                  On 2/7/2013 12:21 AM, Stanimir Simeonoff wrote:
>
>                      Thread priorities are usually NOT applied at all.
>                      For insance:
>                            intx DefaultThreadPriority                     = -1
>                          {product}
>                           intx JavaPriority10_To_OSPriority              = -1
>                        {product}
>                            intx JavaPriority1_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority2_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority3_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority4_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority5_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority6_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority7_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority8_To_OSPriority               = -1
>                          {product}
>                            intx JavaPriority9_To_OSPriority               = -1
>                          {product}
>
>                      in other words unless specified :
>         -XXJavaPriority10_To_____OSPriority=
>
>                      it won't be mapped.
>
>                      If applied the JVM compiler/GC threads may become starved
>         which you
>                      don't
>                      want, so they have to work above normal prir (that request root
>                      privileges).
>                      Alternatively the normal java threads have to run w/ lower prir
>                      which means
>                      other process will have higher priority - also unpleasant.
>
>                      Stanimir
>
>                      On Thu, Feb 7, 2013 at 5:20 AM, Mohan Radhakrishnan
>                      <radhakrishnan.mohan at gmail.com
>         <mailto:radhakrishnan.mohan at gmail.com>
>                      <mailto:radhakrishnan.mohan at __gmail.com
>         <mailto:radhakrishnan.mohan at gmail.com>>
>                      <mailto:radhakrishnan.mohan@
>         <mailto:radhakrishnan.mohan@>____gmail.com <http://gmail.com>
>
>                      <mailto:radhakrishnan.mohan at __gmail.com
>         <mailto:radhakrishnan.mohan at gmail.com>>>> wrote:
>
>                           Hi,
>                                     Can the Thread priority setting in the API
>         still be
>                      reliably
>                           used uniformly across processors ? There are other
>         concurrency
>                      patterns in
>                           the API but this setting is still there.
>
>
>                           Thanks,
>                           Mohan
>
>                           ___________________________________________________
>                           Concurrency-interest mailing list
>                      Concurrency-interest at cs.__oswe__go.edu <http://oswego.edu>
>                      <mailto:Concurrency-interest at __cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>                      <mailto:Concurrency-interest@
>         <mailto:Concurrency-interest@>____cs.oswego.edu <http://cs.oswego.edu>
>                      <mailto:Concurrency-interest at __cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>>
>
>         http://cs.oswego.edu/mailman/____listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>>
>
>
>
>
>                      ___________________________________________________
>                      Concurrency-interest mailing list
>                      Concurrency-interest at cs.__oswe__go.edu <http://oswego.edu>
>                      <mailto:Concurrency-interest at __cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/____listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>>
>
>
>
>
>                  ___________________________________________________
>                  Concurrency-interest mailing list
>                  Concurrency-interest at cs.__oswe__go.edu <http://oswego.edu>
>                  <mailto:Concurrency-interest at __cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/____listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>                  <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>>
>
>
>              ___________________________________________________
>              Concurrency-interest mailing list
>              Concurrency-interest at cs.__oswe__go.edu <http://oswego.edu>
>         <mailto:Concurrency-interest at __cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/____listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>              <http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>>
>
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From radhakrishnan.mohan at gmail.com  Fri Feb  8 03:03:18 2013
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Fri, 8 Feb 2013 13:33:18 +0530
Subject: [concurrency-interest] Phaser and StampedLock Presentation
In-Reply-To: <5113453D.8090403@javaspecialists.eu>
References: <5113453D.8090403@javaspecialists.eu>
Message-ID: <CAOoXFP9J7wNXkzy21vScsrmVzQNXhLgK6bt-ajQEaEpe=spHDg@mail.gmail.com>

Example are more accessible to programmers.

Thanks,
Mohan

On Thu, Feb 7, 2013 at 11:40 AM, Dr Heinz M. Kabutz <
heinz at javaspecialists.eu> wrote:

> For those of you who might be interested in the new StampedLock that Doug
> Lea wrote, I gave a talk on Tuesday at JFokus in Sweden about it, also
> showing some interesting performance results.
>
> You can download the slides here:
>
> http://javaspecialists.eu/**talks/jfokus13/**PhaserAndStampedLock.pdf<http://javaspecialists.eu/talks/jfokus13/PhaserAndStampedLock.pdf>
>
> Stephen Chin and I also did a "Nighthacking" session, where we looked at
> Fork/Join and then again StampedLock:
>
> http://vimeo.com/**javaspecialists/jfokus-2013-**nighthacking<http://vimeo.com/javaspecialists/jfokus-2013-nighthacking>
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun Java Champion
> IEEE Certified Software Development Professional
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130208/99fc7693/attachment.html>

From thurston at nomagicsoftware.com  Mon Feb 11 15:52:10 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 11 Feb 2013 12:52:10 -0800
Subject: [concurrency-interest] =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
Message-ID: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>

Hello,

I made some initial attempts at using ForkJoin framework for some 
rather obvious recursively parallel use-cases, namely summing the 
elements of an int[] and also sorting an int[] (randomly filled).

The problem-sizes I used ranged from 1M - 10M.

I really enjoy using the framework and find it relatively easy to 
reason about my programs (if not necessarily the internals of the 
framework).
But I was disappointed with the results: in both cases they were slower 
than the corresponding Java serial implementation.

I completely understand the principle of YMMV, and I wasn't expecting 
2x speedup, but especially in the case of the sort, but I was expecting 
that ForkJoin would do at least a little better than the single-threaded 
version.  I guess what I'm asking is: are these results surprising?  
Does it mean I'm doing something wrong?

I'll give a brief description of my implementation:
single-threaded:  Arrays.sort(int[])

ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by 
trial-and-error)

int[] compute()
{
if int[].length < THRESHOLD
     return insertionSort(int[])
left = new MergeSort(int[] //split in half)
right = new MergeSort(int[] //other half)
return merge(right.compute(), left.join())
}

The insertionSort() and merge() methods are just standard 
implementations; there is a bit of apples to oranges comparison since 
Arrays.sort() uses an optimized quicksort, but we're still talking 
O(nlog(n))

Just ran it on my laptop:
Windows 7 64-bit
1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
Core 2 Duo==> 2 cores 2GHz

2M int[]:
single-threaded: ~330ms
ForkJoin (2 workers): ~390ms

Would appreciate any feedback and hopefully the #s are somewhat helpful 
(and please no scoffawing at my antiquated machine)

-T



From viktor.klang at gmail.com  Mon Feb 11 15:59:14 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 11 Feb 2013 21:59:14 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <CANPzfU9WNz7XKbRvJzqhdYXY5s082UGqpXsR+dOP6zbB7GaGSQ@mail.gmail.com>

On Mon, Feb 11, 2013 at 9:52 PM, <thurston at nomagicsoftware.com> wrote:

> Hello,
>
> I made some initial attempts at using ForkJoin framework for some rather
> obvious recursively parallel use-cases, namely summing the elements of an
> int[] and also sorting an int[] (randomly filled).
>
> The problem-sizes I used ranged from 1M - 10M.
>
> I really enjoy using the framework and find it relatively easy to reason
> about my programs (if not necessarily the internals of the framework).
> But I was disappointed with the results: in both cases they were slower
> than the corresponding Java serial implementation.
>
> I completely understand the principle of YMMV, and I wasn't expecting 2x
> speedup, but especially in the case of the sort, but I was expecting that
> ForkJoin would do at least a little better than the single-threaded
> version.  I guess what I'm asking is: are these results surprising?  Does
> it mean I'm doing something wrong?
>
> I'll give a brief description of my implementation:
> single-threaded:  Arrays.sort(int[])
>
> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
> trial-and-error)
>
> int[] compute()
> {
> if int[].length < THRESHOLD
>     return insertionSort(int[])
> left = new MergeSort(int[] //split in half)
> right = new MergeSort(int[] //other half)
> return merge(right.compute(), left.join())
> }
>
> The insertionSort() and merge() methods are just standard implementations;
> there is a bit of apples to oranges comparison since Arrays.sort() uses an
> optimized quicksort, but we're still talking O(nlog(n))
>
> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz
>
> 2M int[]:
> single-threaded: ~330ms
> ForkJoin (2 workers): ~390ms
>

4M int[]:

8M int[]:

16M int[]

32M int[]:

64M int[]:


Cheers,
?


>
> Would appreciate any feedback and hopefully the #s are somewhat helpful
> (and please no scoffawing at my antiquated machine)
>
> -T
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/b09cd8ce/attachment.html>

From coyotesqrl at gmail.com  Mon Feb 11 16:10:05 2013
From: coyotesqrl at gmail.com (R.A. Porter)
Date: Mon, 11 Feb 2013 14:10:05 -0700
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <CAFFMuLNizjcj=zbJBDY85E0kFS_CxednHa+CONYh03Uenxb0vg@mail.gmail.com>

Your threshold seems a bit small to me, but more importantly, I don't see
you forking the left branch prior to the right.compute/left.join.

R.A. Porter


On Mon, Feb 11, 2013 at 1:52 PM, <thurston at nomagicsoftware.com> wrote:

> Hello,
>
> I made some initial attempts at using ForkJoin framework for some rather
> obvious recursively parallel use-cases, namely summing the elements of an
> int[] and also sorting an int[] (randomly filled).
>
> The problem-sizes I used ranged from 1M - 10M.
>
> I really enjoy using the framework and find it relatively easy to reason
> about my programs (if not necessarily the internals of the framework).
> But I was disappointed with the results: in both cases they were slower
> than the corresponding Java serial implementation.
>
> I completely understand the principle of YMMV, and I wasn't expecting 2x
> speedup, but especially in the case of the sort, but I was expecting that
> ForkJoin would do at least a little better than the single-threaded
> version.  I guess what I'm asking is: are these results surprising?  Does
> it mean I'm doing something wrong?
>
> I'll give a brief description of my implementation:
> single-threaded:  Arrays.sort(int[])
>
> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
> trial-and-error)
>
> int[] compute()
> {
> if int[].length < THRESHOLD
>     return insertionSort(int[])
> left = new MergeSort(int[] //split in half)
> right = new MergeSort(int[] //other half)
> return merge(right.compute(), left.join())
> }
>
> The insertionSort() and merge() methods are just standard implementations;
> there is a bit of apples to oranges comparison since Arrays.sort() uses an
> optimized quicksort, but we're still talking O(nlog(n))
>
> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz
>
> 2M int[]:
> single-threaded: ~330ms
> ForkJoin (2 workers): ~390ms
>
> Would appreciate any feedback and hopefully the #s are somewhat helpful
> (and please no scoffawing at my antiquated machine)
>
> -T
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/d25ee8c6/attachment.html>

From davidcholmes at aapt.net.au  Mon Feb 11 16:19:24 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 07:19:24 +1000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEEDJKAA.davidcholmes@aapt.net.au>

Your sequential threshold is far too small for only two worker threads. You
would get near optimum speedup only splitting the problem into two.

The heuristic for sequential threshold is:

Threshold = N / (LoadBalanceFactor * NumCore)

where N is the "size" of the problem.

The LoadBalanceFactor in an ideal world would be 1 but real tasks encounter
delays and execute differently so a value > 1 is needed. Heuristically a
value of 8 is a good starting value.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> thurston at nomagicsoftware.com
> Sent: Tuesday, 12 February 2013 6:52 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Some interesting (confusing?) benchmark
> results
>
>
> Hello,
>
> I made some initial attempts at using ForkJoin framework for some
> rather obvious recursively parallel use-cases, namely summing the
> elements of an int[] and also sorting an int[] (randomly filled).
>
> The problem-sizes I used ranged from 1M - 10M.
>
> I really enjoy using the framework and find it relatively easy to
> reason about my programs (if not necessarily the internals of the
> framework).
> But I was disappointed with the results: in both cases they were slower
> than the corresponding Java serial implementation.
>
> I completely understand the principle of YMMV, and I wasn't expecting
> 2x speedup, but especially in the case of the sort, but I was expecting
> that ForkJoin would do at least a little better than the single-threaded
> version.  I guess what I'm asking is: are these results surprising?
> Does it mean I'm doing something wrong?
>
> I'll give a brief description of my implementation:
> single-threaded:  Arrays.sort(int[])
>
> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
> trial-and-error)
>
> int[] compute()
> {
> if int[].length < THRESHOLD
>      return insertionSort(int[])
> left = new MergeSort(int[] //split in half)
> right = new MergeSort(int[] //other half)
> return merge(right.compute(), left.join())
> }
>
> The insertionSort() and merge() methods are just standard
> implementations; there is a bit of apples to oranges comparison since
> Arrays.sort() uses an optimized quicksort, but we're still talking
> O(nlog(n))
>
> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz
>
> 2M int[]:
> single-threaded: ~330ms
> ForkJoin (2 workers): ~390ms
>
> Would appreciate any feedback and hopefully the #s are somewhat helpful
> (and please no scoffawing at my antiquated machine)
>
> -T
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From aleksey.shipilev at oracle.com  Mon Feb 11 16:19:48 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 12 Feb 2013 01:19:48 +0400
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <51196074.7070701@oracle.com>

On 02/12/2013 12:52 AM, thurston at nomagicsoftware.com wrote:
> Does it mean I'm doing something wrong?

Probably. Tuning FJP to fit the needs of the problem at hand is
something tricky to do for unexperienced user.

> I'll give a brief description of my implementation:

Complete source for the test will be helpful.

Also, try to use this:
  https://github.com/shipilev/java-forkjoin-trace
...to analyze the behavior of your workload.

> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz

I'm not mocking your machine, but your JDK version. That version is
terribly outdated, and probably contains quite old FJP code. Pull the
fresh JAR from here:
 http://g.oswego.edu/dl/concurrency-interest/
or here:
 http://shipilev.net/builds/jsr166/

...then -Xbootclasspath/p:<path> it. (check with -verbose:class you have
indeed loaded FJP from that JAR). Although you might have problems
running in on non-JDK8-lambda build. In that case, you can fallback to
java-forkjoin-trace JAR, which has recent enough FJP (but not the
bleeding edge to be incompatible with JDK7). Doug can have more ideas
how to triage FJP perf issues.

-Aleksey.

From thurston at nomagicsoftware.com  Mon Feb 11 16:49:18 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 11 Feb 2013 13:49:18 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEEDJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEEDJKAA.davidcholmes@aapt.net.au>
Message-ID: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>

Well, it's the algorithm that's dispositive in this case.
Insertion-sort is empirically better than mergesort up to around 
25-sized arrays.  That's where I started, and I played around with 
different thresholds and 250 seemed to be the best.
Anything larger than 1000 you start to see significant degradation.
I tried your formula (approximately) with N = 2 000 000 ==> 250K

That brought my laptop to its knees (as expected): over 33K ms as 
opposed to 390ms with 250.

Following my own admonition (the algorithm matters), it occurs to me 
that I just use my own serial mergesort as comparison to the Forkjoin 
(as opposed to Arrays.sort).  I'll let y'all know the results of that

-T



On 2013-02-11 13:19, David Holmes wrote:
> Your sequential threshold is far too small for only two worker 
> threads. You
> would get near optimum speedup only splitting the problem into two.
>
> The heuristic for sequential threshold is:
>
> Threshold = N / (LoadBalanceFactor * NumCore)
>
> where N is the "size" of the problem.
>
> The LoadBalanceFactor in an ideal world would be 1 but real tasks 
> encounter
> delays and execute differently so a value > 1 is needed. 
> Heuristically a
> value of 8 is a good starting value.
>
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> thurston at nomagicsoftware.com
>> Sent: Tuesday, 12 February 2013 6:52 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: [concurrency-interest] Some interesting (confusing?) 
>> benchmark
>> results
>>
>>
>> Hello,
>>
>> I made some initial attempts at using ForkJoin framework for some
>> rather obvious recursively parallel use-cases, namely summing the
>> elements of an int[] and also sorting an int[] (randomly filled).
>>
>> The problem-sizes I used ranged from 1M - 10M.
>>
>> I really enjoy using the framework and find it relatively easy to
>> reason about my programs (if not necessarily the internals of the
>> framework).
>> But I was disappointed with the results: in both cases they were 
>> slower
>> than the corresponding Java serial implementation.
>>
>> I completely understand the principle of YMMV, and I wasn't 
>> expecting
>> 2x speedup, but especially in the case of the sort, but I was 
>> expecting
>> that ForkJoin would do at least a little better than the 
>> single-threaded
>> version.  I guess what I'm asking is: are these results surprising?
>> Does it mean I'm doing something wrong?
>>
>> I'll give a brief description of my implementation:
>> single-threaded:  Arrays.sort(int[])
>>
>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at 
>> by
>> trial-and-error)
>>
>> int[] compute()
>> {
>> if int[].length < THRESHOLD
>>      return insertionSort(int[])
>> left = new MergeSort(int[] //split in half)
>> right = new MergeSort(int[] //other half)
>> return merge(right.compute(), left.join())
>> }
>>
>> The insertionSort() and merge() methods are just standard
>> implementations; there is a bit of apples to oranges comparison 
>> since
>> Arrays.sort() uses an optimized quicksort, but we're still talking
>> O(nlog(n))
>>
>> Just ran it on my laptop:
>> Windows 7 64-bit
>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> Core 2 Duo==> 2 cores 2GHz
>>
>> 2M int[]:
>> single-threaded: ~330ms
>> ForkJoin (2 workers): ~390ms
>>
>> Would appreciate any feedback and hopefully the #s are somewhat 
>> helpful
>> (and please no scoffawing at my antiquated machine)
>>
>> -T
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


From davidcholmes at aapt.net.au  Mon Feb 11 17:18:12 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 08:18:12 +1000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>

That doesn't make sense to me. If you had a threshold of 250 then you are
creating thousands of tasks and that overhead should potentially bring your
laptop to its knees. With 250K threshold you have far fewer tasks.

That said I don't know what the memory requirements are for your algorithm.
If you are allocating temporary arrays then a  large threshold could well
cause a problem.

The parallel array sort going into JDK 8 is a merge sort but uses a working
array equal in size to the original array to be sorted.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> thurston at nomagicsoftware.com
> Sent: Tuesday, 12 February 2013 7:49 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Some interesting (confusing?)
> benchmark results
>
>
> Well, it's the algorithm that's dispositive in this case.
> Insertion-sort is empirically better than mergesort up to around
> 25-sized arrays.  That's where I started, and I played around with
> different thresholds and 250 seemed to be the best.
> Anything larger than 1000 you start to see significant degradation.
> I tried your formula (approximately) with N = 2 000 000 ==> 250K
>
> That brought my laptop to its knees (as expected): over 33K ms as
> opposed to 390ms with 250.
>
> Following my own admonition (the algorithm matters), it occurs to me
> that I just use my own serial mergesort as comparison to the Forkjoin
> (as opposed to Arrays.sort).  I'll let y'all know the results of that
>
> -T
>
>
>
> On 2013-02-11 13:19, David Holmes wrote:
> > Your sequential threshold is far too small for only two worker
> > threads. You
> > would get near optimum speedup only splitting the problem into two.
> >
> > The heuristic for sequential threshold is:
> >
> > Threshold = N / (LoadBalanceFactor * NumCore)
> >
> > where N is the "size" of the problem.
> >
> > The LoadBalanceFactor in an ideal world would be 1 but real tasks
> > encounter
> > delays and execute differently so a value > 1 is needed.
> > Heuristically a
> > value of 8 is a good starting value.
> >
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> thurston at nomagicsoftware.com
> >> Sent: Tuesday, 12 February 2013 6:52 AM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: [concurrency-interest] Some interesting (confusing?)
> >> benchmark
> >> results
> >>
> >>
> >> Hello,
> >>
> >> I made some initial attempts at using ForkJoin framework for some
> >> rather obvious recursively parallel use-cases, namely summing the
> >> elements of an int[] and also sorting an int[] (randomly filled).
> >>
> >> The problem-sizes I used ranged from 1M - 10M.
> >>
> >> I really enjoy using the framework and find it relatively easy to
> >> reason about my programs (if not necessarily the internals of the
> >> framework).
> >> But I was disappointed with the results: in both cases they were
> >> slower
> >> than the corresponding Java serial implementation.
> >>
> >> I completely understand the principle of YMMV, and I wasn't
> >> expecting
> >> 2x speedup, but especially in the case of the sort, but I was
> >> expecting
> >> that ForkJoin would do at least a little better than the
> >> single-threaded
> >> version.  I guess what I'm asking is: are these results surprising?
> >> Does it mean I'm doing something wrong?
> >>
> >> I'll give a brief description of my implementation:
> >> single-threaded:  Arrays.sort(int[])
> >>
> >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
> >> by
> >> trial-and-error)
> >>
> >> int[] compute()
> >> {
> >> if int[].length < THRESHOLD
> >>      return insertionSort(int[])
> >> left = new MergeSort(int[] //split in half)
> >> right = new MergeSort(int[] //other half)
> >> return merge(right.compute(), left.join())
> >> }
> >>
> >> The insertionSort() and merge() methods are just standard
> >> implementations; there is a bit of apples to oranges comparison
> >> since
> >> Arrays.sort() uses an optimized quicksort, but we're still talking
> >> O(nlog(n))
> >>
> >> Just ran it on my laptop:
> >> Windows 7 64-bit
> >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> >> Core 2 Duo==> 2 cores 2GHz
> >>
> >> 2M int[]:
> >> single-threaded: ~330ms
> >> ForkJoin (2 workers): ~390ms
> >>
> >> Would appreciate any feedback and hopefully the #s are somewhat
> >> helpful
> >> (and please no scoffawing at my antiquated machine)
> >>
> >> -T
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From viktor.klang at gmail.com  Mon Feb 11 17:24:53 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 11 Feb 2013 23:24:53 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
Message-ID: <CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>

On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> That doesn't make sense to me. If you had a threshold of 250 then you are
> creating thousands of tasks and that overhead should potentially bring your
> laptop to its knees. With 250K threshold you have far fewer tasks.
>

I'm not sure it makes sense to create more sort-tasks than there is Threads
in the pool.


>
> That said I don't know what the memory requirements are for your algorithm.
> If you are allocating temporary arrays then a  large threshold could well
> cause a problem.
>
> The parallel array sort going into JDK 8 is a merge sort but uses a working
> array equal in size to the original array to be sorted.
>

No in-place merge sort? ;-)


>
> David
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> > thurston at nomagicsoftware.com
> > Sent: Tuesday, 12 February 2013 7:49 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
> > benchmark results
> >
> >
> > Well, it's the algorithm that's dispositive in this case.
> > Insertion-sort is empirically better than mergesort up to around
> > 25-sized arrays.  That's where I started, and I played around with
> > different thresholds and 250 seemed to be the best.
> > Anything larger than 1000 you start to see significant degradation.
> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
> >
> > That brought my laptop to its knees (as expected): over 33K ms as
> > opposed to 390ms with 250.
> >
> > Following my own admonition (the algorithm matters), it occurs to me
> > that I just use my own serial mergesort as comparison to the Forkjoin
> > (as opposed to Arrays.sort).  I'll let y'all know the results of that
> >
> > -T
> >
> >
> >
> > On 2013-02-11 13:19, David Holmes wrote:
> > > Your sequential threshold is far too small for only two worker
> > > threads. You
> > > would get near optimum speedup only splitting the problem into two.
> > >
> > > The heuristic for sequential threshold is:
> > >
> > > Threshold = N / (LoadBalanceFactor * NumCore)
> > >
> > > where N is the "size" of the problem.
> > >
> > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
> > > encounter
> > > delays and execute differently so a value > 1 is needed.
> > > Heuristically a
> > > value of 8 is a good starting value.
> > >
> > > David Holmes
> > >
> > >> -----Original Message-----
> > >> From: concurrency-interest-bounces at cs.oswego.edu
> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> > >> thurston at nomagicsoftware.com
> > >> Sent: Tuesday, 12 February 2013 6:52 AM
> > >> To: concurrency-interest at cs.oswego.edu
> > >> Subject: [concurrency-interest] Some interesting (confusing?)
> > >> benchmark
> > >> results
> > >>
> > >>
> > >> Hello,
> > >>
> > >> I made some initial attempts at using ForkJoin framework for some
> > >> rather obvious recursively parallel use-cases, namely summing the
> > >> elements of an int[] and also sorting an int[] (randomly filled).
> > >>
> > >> The problem-sizes I used ranged from 1M - 10M.
> > >>
> > >> I really enjoy using the framework and find it relatively easy to
> > >> reason about my programs (if not necessarily the internals of the
> > >> framework).
> > >> But I was disappointed with the results: in both cases they were
> > >> slower
> > >> than the corresponding Java serial implementation.
> > >>
> > >> I completely understand the principle of YMMV, and I wasn't
> > >> expecting
> > >> 2x speedup, but especially in the case of the sort, but I was
> > >> expecting
> > >> that ForkJoin would do at least a little better than the
> > >> single-threaded
> > >> version.  I guess what I'm asking is: are these results surprising?
> > >> Does it mean I'm doing something wrong?
> > >>
> > >> I'll give a brief description of my implementation:
> > >> single-threaded:  Arrays.sort(int[])
> > >>
> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
> > >> by
> > >> trial-and-error)
> > >>
> > >> int[] compute()
> > >> {
> > >> if int[].length < THRESHOLD
> > >>      return insertionSort(int[])
> > >> left = new MergeSort(int[] //split in half)
> > >> right = new MergeSort(int[] //other half)
> > >> return merge(right.compute(), left.join())
> > >> }
> > >>
> > >> The insertionSort() and merge() methods are just standard
> > >> implementations; there is a bit of apples to oranges comparison
> > >> since
> > >> Arrays.sort() uses an optimized quicksort, but we're still talking
> > >> O(nlog(n))
> > >>
> > >> Just ran it on my laptop:
> > >> Windows 7 64-bit
> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> > >> Core 2 Duo==> 2 cores 2GHz
> > >>
> > >> 2M int[]:
> > >> single-threaded: ~330ms
> > >> ForkJoin (2 workers): ~390ms
> > >>
> > >> Would appreciate any feedback and hopefully the #s are somewhat
> > >> helpful
> > >> (and please no scoffawing at my antiquated machine)
> > >>
> > >> -T
> > >>
> > >>
> > >> _______________________________________________
> > >> Concurrency-interest mailing list
> > >> Concurrency-interest at cs.oswego.edu
> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/dfb688d7/attachment.html>

From davidcholmes at aapt.net.au  Mon Feb 11 17:34:30 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 08:34:30 +1000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEEFJKAA.davidcholmes@aapt.net.au>

Victor,

See the description of LoadBalanceFactor. It makes sense to create more
tasks than there are threads because things progress at different rates. A
thread can be delayed through scheduling, JIT activity, GC etc etc. Suppose
with 2 threads one had completed its half while the other was only a quarter
of the way through. You then have to wait much longer for completion. But if
you have broken things down into smaller subtasks (say 4) then that first
thread could be "helping" the second to complete the overall job. But break
it down too fine and you get too much task overhead. Hence the heuristic
value of 8 - which is generally a good starting point. Plus performance is
not super-sensitive to getting the LoadBalanceFactor exactly right.

David
  -----Original Message-----
  From: viktor ?lang [mailto:viktor.klang at gmail.com]
  Sent: Tuesday, 12 February 2013 8:25 AM
  To: dholmes at ieee.org
  Cc: thurston; concurrency-interest
  Subject: Re: [concurrency-interest] Some interesting (confusing?)
benchmark results







  On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    That doesn't make sense to me. If you had a threshold of 250 then you
are
    creating thousands of tasks and that overhead should potentially bring
your
    laptop to its knees. With 250K threshold you have far fewer tasks.



  I'm not sure it makes sense to create more sort-tasks than there is
Threads in the pool.


    That said I don't know what the memory requirements are for your
algorithm.
    If you are allocating temporary arrays then a  large threshold could
well
    cause a problem.

    The parallel array sort going into JDK 8 is a merge sort but uses a
working
    array equal in size to the original array to be sorted.



  No in-place merge sort? ;-)


    David


    > -----Original Message-----
    > From: concurrency-interest-bounces at cs.oswego.edu
    > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
    > thurston at nomagicsoftware.com

    > Sent: Tuesday, 12 February 2013 7:49 AM
    > To: concurrency-interest at cs.oswego.edu
    > Subject: Re: [concurrency-interest] Some interesting (confusing?)
    > benchmark results
    >
    >
    > Well, it's the algorithm that's dispositive in this case.
    > Insertion-sort is empirically better than mergesort up to around
    > 25-sized arrays.  That's where I started, and I played around with
    > different thresholds and 250 seemed to be the best.
    > Anything larger than 1000 you start to see significant degradation.
    > I tried your formula (approximately) with N = 2 000 000 ==> 250K
    >
    > That brought my laptop to its knees (as expected): over 33K ms as
    > opposed to 390ms with 250.
    >
    > Following my own admonition (the algorithm matters), it occurs to me
    > that I just use my own serial mergesort as comparison to the Forkjoin
    > (as opposed to Arrays.sort).  I'll let y'all know the results of that
    >
    > -T
    >
    >
    >
    > On 2013-02-11 13:19, David Holmes wrote:
    > > Your sequential threshold is far too small for only two worker
    > > threads. You
    > > would get near optimum speedup only splitting the problem into two.
    > >
    > > The heuristic for sequential threshold is:
    > >
    > > Threshold = N / (LoadBalanceFactor * NumCore)
    > >
    > > where N is the "size" of the problem.
    > >
    > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
    > > encounter
    > > delays and execute differently so a value > 1 is needed.
    > > Heuristically a
    > > value of 8 is a good starting value.
    > >
    > > David Holmes
    > >
    > >> -----Original Message-----
    > >> From: concurrency-interest-bounces at cs.oswego.edu
    > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
    > >> thurston at nomagicsoftware.com
    > >> Sent: Tuesday, 12 February 2013 6:52 AM
    > >> To: concurrency-interest at cs.oswego.edu
    > >> Subject: [concurrency-interest] Some interesting (confusing?)
    > >> benchmark
    > >> results
    > >>
    > >>
    > >> Hello,
    > >>
    > >> I made some initial attempts at using ForkJoin framework for some
    > >> rather obvious recursively parallel use-cases, namely summing the
    > >> elements of an int[] and also sorting an int[] (randomly filled).
    > >>
    > >> The problem-sizes I used ranged from 1M - 10M.
    > >>
    > >> I really enjoy using the framework and find it relatively easy to
    > >> reason about my programs (if not necessarily the internals of the
    > >> framework).
    > >> But I was disappointed with the results: in both cases they were
    > >> slower
    > >> than the corresponding Java serial implementation.
    > >>
    > >> I completely understand the principle of YMMV, and I wasn't
    > >> expecting
    > >> 2x speedup, but especially in the case of the sort, but I was
    > >> expecting
    > >> that ForkJoin would do at least a little better than the
    > >> single-threaded
    > >> version.  I guess what I'm asking is: are these results surprising?
    > >> Does it mean I'm doing something wrong?
    > >>
    > >> I'll give a brief description of my implementation:
    > >> single-threaded:  Arrays.sort(int[])
    > >>
    > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
    > >> by
    > >> trial-and-error)
    > >>
    > >> int[] compute()
    > >> {
    > >> if int[].length < THRESHOLD
    > >>      return insertionSort(int[])
    > >> left = new MergeSort(int[] //split in half)
    > >> right = new MergeSort(int[] //other half)
    > >> return merge(right.compute(), left.join())
    > >> }
    > >>
    > >> The insertionSort() and merge() methods are just standard
    > >> implementations; there is a bit of apples to oranges comparison
    > >> since
    > >> Arrays.sort() uses an optimized quicksort, but we're still talking
    > >> O(nlog(n))
    > >>
    > >> Just ran it on my laptop:
    > >> Windows 7 64-bit
    > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
    > >> Core 2 Duo==> 2 cores 2GHz
    > >>
    > >> 2M int[]:
    > >> single-threaded: ~330ms
    > >> ForkJoin (2 workers): ~390ms
    > >>
    > >> Would appreciate any feedback and hopefully the #s are somewhat
    > >> helpful
    > >> (and please no scoffawing at my antiquated machine)
    > >>
    > >> -T
    > >>
    > >>
    > >> _______________________________________________
    > >> Concurrency-interest mailing list
    > >> Concurrency-interest at cs.oswego.edu
    > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
    > >>
    >
    > _______________________________________________
    > Concurrency-interest mailing list
    > Concurrency-interest at cs.oswego.edu
    > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
    >

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest






  --

  Viktor Klang
  Director of Engineering


  Typesafe - The software stack for applications that scale
  Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/86c1da85/attachment-0001.html>

From stanimir at riflexo.com  Mon Feb 11 17:35:44 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 12 Feb 2013 00:35:44 +0200
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
References: <NFBBKALFDCPFIDBNKAPCMEEDJKAA.davidcholmes@aapt.net.au>
	<62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
Message-ID: <CAEJX8oqcjEfLFEwAerX2kH16-JQDcb=dJzLOet8S94nqCJ8wCA@mail.gmail.com>

Since the L2 cache size of the CPU is relatively low (I suppose 2M on
T6600), you'd like to keep the small portions within the cache. Perhaps
in-place sorting like smoothsort can help. Merge should be in-place as well.


On Mon, Feb 11, 2013 at 11:49 PM, <thurston at nomagicsoftware.com> wrote:

> Well, it's the algorithm that's dispositive in this case.
> Insertion-sort is empirically better than mergesort up to around 25-sized
> arrays.  That's where I started, and I played around with different
> thresholds and 250 seemed to be the best.
> Anything larger than 1000 you start to see significant degradation.
> I tried your formula (approximately) with N = 2 000 000 ==> 250K
>
> That brought my laptop to its knees (as expected): over 33K ms as opposed
> to 390ms with 250.
>
> Following my own admonition (the algorithm matters), it occurs to me that
> I just use my own serial mergesort as comparison to the Forkjoin (as
> opposed to Arrays.sort).  I'll let y'all know the results of that
>
> -T
>
>
>
>
> On 2013-02-11 13:19, David Holmes wrote:
>
>> Your sequential threshold is far too small for only two worker threads.
>> You
>> would get near optimum speedup only splitting the problem into two.
>>
>> The heuristic for sequential threshold is:
>>
>> Threshold = N / (LoadBalanceFactor * NumCore)
>>
>> where N is the "size" of the problem.
>>
>> The LoadBalanceFactor in an ideal world would be 1 but real tasks
>> encounter
>> delays and execute differently so a value > 1 is needed. Heuristically a
>> value of 8 is a good starting value.
>>
>> David Holmes
>>
>>  -----Original Message-----
>>> From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>> [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>> Behalf Of
>>> thurston at nomagicsoftware.com
>>> Sent: Tuesday, 12 February 2013 6:52 AM
>>> To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>> Subject: [concurrency-interest] Some interesting (confusing?) benchmark
>>> results
>>>
>>>
>>> Hello,
>>>
>>> I made some initial attempts at using ForkJoin framework for some
>>> rather obvious recursively parallel use-cases, namely summing the
>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>
>>> The problem-sizes I used ranged from 1M - 10M.
>>>
>>> I really enjoy using the framework and find it relatively easy to
>>> reason about my programs (if not necessarily the internals of the
>>> framework).
>>> But I was disappointed with the results: in both cases they were slower
>>> than the corresponding Java serial implementation.
>>>
>>> I completely understand the principle of YMMV, and I wasn't expecting
>>> 2x speedup, but especially in the case of the sort, but I was expecting
>>> that ForkJoin would do at least a little better than the single-threaded
>>> version.  I guess what I'm asking is: are these results surprising?
>>> Does it mean I'm doing something wrong?
>>>
>>> I'll give a brief description of my implementation:
>>> single-threaded:  Arrays.sort(int[])
>>>
>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
>>> trial-and-error)
>>>
>>> int[] compute()
>>> {
>>> if int[].length < THRESHOLD
>>>      return insertionSort(int[])
>>> left = new MergeSort(int[] //split in half)
>>> right = new MergeSort(int[] //other half)
>>> return merge(right.compute(), left.join())
>>> }
>>>
>>> The insertionSort() and merge() methods are just standard
>>> implementations; there is a bit of apples to oranges comparison since
>>> Arrays.sort() uses an optimized quicksort, but we're still talking
>>> O(nlog(n))
>>>
>>> Just ran it on my laptop:
>>> Windows 7 64-bit
>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>> Core 2 Duo==> 2 cores 2GHz
>>>
>>> 2M int[]:
>>> single-threaded: ~330ms
>>> ForkJoin (2 workers): ~390ms
>>>
>>> Would appreciate any feedback and hopefully the #s are somewhat helpful
>>> (and please no scoffawing at my antiquated machine)
>>>
>>> -T
>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/897564a7/attachment.html>

From viktor.klang at gmail.com  Mon Feb 11 17:39:06 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 11 Feb 2013 23:39:06 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEEFJKAA.davidcholmes@aapt.net.au>
References: <CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEEFJKAA.davidcholmes@aapt.net.au>
Message-ID: <CANPzfU8=FyvYBBQ2u+BSgHeXhJnEJR-odt=4kF7RO05+SGke5Q@mail.gmail.com>

David,


On Mon, Feb 11, 2013 at 11:34 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Victor,
>
> See the description of LoadBalanceFactor. It makes sense to create more
> tasks than there are threads because things progress at different rates. A
> thread can be delayed through scheduling, JIT activity, GC etc etc. Suppose
> with 2 threads one had completed its half while the other was only a
> quarter of the way through. You then have to wait much longer for
> completion. But if you have broken things down into smaller subtasks (say
> 4) then that first thread could be "helping" the second to complete the
> overall job. But break it down too fine and you get too much task overhead.
> Hence the heuristic value of 8 - which is generally a good starting point.
> Plus performance is not super-sensitive to getting the LoadBalanceFactor
> exactly right.
>

True, but cache-effects (& hardward) also has to be taken into
consideration (shared l2 vs shared l3 vs going cross-socket/NUMA).
Ideally the user API would be devoid of encoding such assumtptions, as they
will vary on a platform level?

Cheers,
?


>
> David
>
> -----Original Message-----
> *From:* viktor ?lang [mailto:viktor.klang at gmail.com]
> *Sent:* Tuesday, 12 February 2013 8:25 AM
> *To:* dholmes at ieee.org
> *Cc:* thurston; concurrency-interest
> *Subject:* Re: [concurrency-interest] Some interesting (confusing?)
> benchmark results
>
>
>
>
> On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>> That doesn't make sense to me. If you had a threshold of 250 then you are
>> creating thousands of tasks and that overhead should potentially bring
>> your
>> laptop to its knees. With 250K threshold you have far fewer tasks.
>>
>
> I'm not sure it makes sense to create more sort-tasks than there is
> Threads in the pool.
>
>
>>
>> That said I don't know what the memory requirements are for your
>> algorithm.
>> If you are allocating temporary arrays then a  large threshold could well
>> cause a problem.
>>
>> The parallel array sort going into JDK 8 is a merge sort but uses a
>> working
>> array equal in size to the original array to be sorted.
>>
>
> No in-place merge sort? ;-)
>
>
>>
>> David
>>
>> > -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> > thurston at nomagicsoftware.com
>>  > Sent: Tuesday, 12 February 2013 7:49 AM
>> > To: concurrency-interest at cs.oswego.edu
>> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> > benchmark results
>> >
>> >
>> > Well, it's the algorithm that's dispositive in this case.
>> > Insertion-sort is empirically better than mergesort up to around
>> > 25-sized arrays.  That's where I started, and I played around with
>> > different thresholds and 250 seemed to be the best.
>> > Anything larger than 1000 you start to see significant degradation.
>> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
>> >
>> > That brought my laptop to its knees (as expected): over 33K ms as
>> > opposed to 390ms with 250.
>> >
>> > Following my own admonition (the algorithm matters), it occurs to me
>> > that I just use my own serial mergesort as comparison to the Forkjoin
>> > (as opposed to Arrays.sort).  I'll let y'all know the results of that
>> >
>> > -T
>> >
>> >
>> >
>> > On 2013-02-11 13:19, David Holmes wrote:
>> > > Your sequential threshold is far too small for only two worker
>> > > threads. You
>> > > would get near optimum speedup only splitting the problem into two.
>> > >
>> > > The heuristic for sequential threshold is:
>> > >
>> > > Threshold = N / (LoadBalanceFactor * NumCore)
>> > >
>> > > where N is the "size" of the problem.
>> > >
>> > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
>> > > encounter
>> > > delays and execute differently so a value > 1 is needed.
>> > > Heuristically a
>> > > value of 8 is a good starting value.
>> > >
>> > > David Holmes
>> > >
>> > >> -----Original Message-----
>> > >> From: concurrency-interest-bounces at cs.oswego.edu
>> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> > >> thurston at nomagicsoftware.com
>> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>> > >> To: concurrency-interest at cs.oswego.edu
>> > >> Subject: [concurrency-interest] Some interesting (confusing?)
>> > >> benchmark
>> > >> results
>> > >>
>> > >>
>> > >> Hello,
>> > >>
>> > >> I made some initial attempts at using ForkJoin framework for some
>> > >> rather obvious recursively parallel use-cases, namely summing the
>> > >> elements of an int[] and also sorting an int[] (randomly filled).
>> > >>
>> > >> The problem-sizes I used ranged from 1M - 10M.
>> > >>
>> > >> I really enjoy using the framework and find it relatively easy to
>> > >> reason about my programs (if not necessarily the internals of the
>> > >> framework).
>> > >> But I was disappointed with the results: in both cases they were
>> > >> slower
>> > >> than the corresponding Java serial implementation.
>> > >>
>> > >> I completely understand the principle of YMMV, and I wasn't
>> > >> expecting
>> > >> 2x speedup, but especially in the case of the sort, but I was
>> > >> expecting
>> > >> that ForkJoin would do at least a little better than the
>> > >> single-threaded
>> > >> version.  I guess what I'm asking is: are these results surprising?
>> > >> Does it mean I'm doing something wrong?
>> > >>
>> > >> I'll give a brief description of my implementation:
>> > >> single-threaded:  Arrays.sort(int[])
>> > >>
>> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
>> > >> by
>> > >> trial-and-error)
>> > >>
>> > >> int[] compute()
>> > >> {
>> > >> if int[].length < THRESHOLD
>> > >>      return insertionSort(int[])
>> > >> left = new MergeSort(int[] //split in half)
>> > >> right = new MergeSort(int[] //other half)
>> > >> return merge(right.compute(), left.join())
>> > >> }
>> > >>
>> > >> The insertionSort() and merge() methods are just standard
>> > >> implementations; there is a bit of apples to oranges comparison
>> > >> since
>> > >> Arrays.sort() uses an optimized quicksort, but we're still talking
>> > >> O(nlog(n))
>> > >>
>> > >> Just ran it on my laptop:
>> > >> Windows 7 64-bit
>> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> > >> Core 2 Duo==> 2 cores 2GHz
>> > >>
>> > >> 2M int[]:
>> > >> single-threaded: ~330ms
>> > >> ForkJoin (2 workers): ~390ms
>> > >>
>> > >> Would appreciate any feedback and hopefully the #s are somewhat
>> > >> helpful
>> > >> (and please no scoffawing at my antiquated machine)
>> > >>
>> > >> -T
>> > >>
>> > >>
>> > >> _______________________________________________
>> > >> Concurrency-interest mailing list
>> > >> Concurrency-interest at cs.oswego.edu
>> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> > >>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/dd643bd0/attachment-0001.html>

From viktor.klang at gmail.com  Mon Feb 11 17:40:27 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Mon, 11 Feb 2013 23:40:27 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CAEJX8oqcjEfLFEwAerX2kH16-JQDcb=dJzLOet8S94nqCJ8wCA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCMEEDJKAA.davidcholmes@aapt.net.au>
	<62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<CAEJX8oqcjEfLFEwAerX2kH16-JQDcb=dJzLOet8S94nqCJ8wCA@mail.gmail.com>
Message-ID: <CANPzfU-Z8EejFWfnVmPa=Eh4ARWPt6jmzgrNgVnCLpxnRc6rAg@mail.gmail.com>

What's the state of the art on in-place mergesorts?
(It's been a while since I was researching that)

Cheers,
?


On Mon, Feb 11, 2013 at 11:35 PM, Stanimir Simeonoff
<stanimir at riflexo.com>wrote:

> Since the L2 cache size of the CPU is relatively low (I suppose 2M on
> T6600), you'd like to keep the small portions within the cache. Perhaps
> in-place sorting like smoothsort can help. Merge should be in-place as well.
>
>
>
> On Mon, Feb 11, 2013 at 11:49 PM, <thurston at nomagicsoftware.com> wrote:
>
>> Well, it's the algorithm that's dispositive in this case.
>> Insertion-sort is empirically better than mergesort up to around 25-sized
>> arrays.  That's where I started, and I played around with different
>> thresholds and 250 seemed to be the best.
>> Anything larger than 1000 you start to see significant degradation.
>> I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>
>> That brought my laptop to its knees (as expected): over 33K ms as opposed
>> to 390ms with 250.
>>
>> Following my own admonition (the algorithm matters), it occurs to me that
>> I just use my own serial mergesort as comparison to the Forkjoin (as
>> opposed to Arrays.sort).  I'll let y'all know the results of that
>>
>> -T
>>
>>
>>
>>
>> On 2013-02-11 13:19, David Holmes wrote:
>>
>>> Your sequential threshold is far too small for only two worker threads.
>>> You
>>> would get near optimum speedup only splitting the problem into two.
>>>
>>> The heuristic for sequential threshold is:
>>>
>>> Threshold = N / (LoadBalanceFactor * NumCore)
>>>
>>> where N is the "size" of the problem.
>>>
>>> The LoadBalanceFactor in an ideal world would be 1 but real tasks
>>> encounter
>>> delays and execute differently so a value > 1 is needed. Heuristically a
>>> value of 8 is a good starting value.
>>>
>>> David Holmes
>>>
>>>  -----Original Message-----
>>>> From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>>> [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>>> Behalf Of
>>>> thurston at nomagicsoftware.com
>>>> Sent: Tuesday, 12 February 2013 6:52 AM
>>>> To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>>> Subject: [concurrency-interest] Some interesting (confusing?) benchmark
>>>> results
>>>>
>>>>
>>>> Hello,
>>>>
>>>> I made some initial attempts at using ForkJoin framework for some
>>>> rather obvious recursively parallel use-cases, namely summing the
>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>
>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>
>>>> I really enjoy using the framework and find it relatively easy to
>>>> reason about my programs (if not necessarily the internals of the
>>>> framework).
>>>> But I was disappointed with the results: in both cases they were slower
>>>> than the corresponding Java serial implementation.
>>>>
>>>> I completely understand the principle of YMMV, and I wasn't expecting
>>>> 2x speedup, but especially in the case of the sort, but I was expecting
>>>> that ForkJoin would do at least a little better than the single-threaded
>>>> version.  I guess what I'm asking is: are these results surprising?
>>>> Does it mean I'm doing something wrong?
>>>>
>>>> I'll give a brief description of my implementation:
>>>> single-threaded:  Arrays.sort(int[])
>>>>
>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
>>>> trial-and-error)
>>>>
>>>> int[] compute()
>>>> {
>>>> if int[].length < THRESHOLD
>>>>      return insertionSort(int[])
>>>> left = new MergeSort(int[] //split in half)
>>>> right = new MergeSort(int[] //other half)
>>>> return merge(right.compute(), left.join())
>>>> }
>>>>
>>>> The insertionSort() and merge() methods are just standard
>>>> implementations; there is a bit of apples to oranges comparison since
>>>> Arrays.sort() uses an optimized quicksort, but we're still talking
>>>> O(nlog(n))
>>>>
>>>> Just ran it on my laptop:
>>>> Windows 7 64-bit
>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> Core 2 Duo==> 2 cores 2GHz
>>>>
>>>> 2M int[]:
>>>> single-threaded: ~330ms
>>>> ForkJoin (2 workers): ~390ms
>>>>
>>>> Would appreciate any feedback and hopefully the #s are somewhat helpful
>>>> (and please no scoffawing at my antiquated machine)
>>>>
>>>> -T
>>>>
>>>>
>>>> ______________________________**_________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>
>>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/73b3463a/attachment.html>

From davidcholmes at aapt.net.au  Mon Feb 11 19:36:34 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 10:36:34 +1000
Subject: [concurrency-interest] Some interesting (confusing?)
	benchmarkresults
In-Reply-To: <CANPzfU8=FyvYBBQ2u+BSgHeXhJnEJR-odt=4kF7RO05+SGke5Q@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEEHJKAA.davidcholmes@aapt.net.au>

Sure but that is why the heuristic value of 8 is a good starting point.

Of course it can be refined for a given platform, but how are you going to express the logic in a cross platform way?

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of viktor ?lang
  Sent: Tuesday, 12 February 2013 8:39 AM
  To: dholmes at ieee.org
  Cc: thurston; concurrency-interest
  Subject: Re: [concurrency-interest] Some interesting (confusing?) benchmarkresults


  David,




  On Mon, Feb 11, 2013 at 11:34 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Victor,

    See the description of LoadBalanceFactor. It makes sense to create more tasks than there are threads because things progress at different rates. A thread can be delayed through scheduling, JIT activity, GC etc etc. Suppose with 2 threads one had completed its half while the other was only a quarter of the way through. You then have to wait much longer for completion. But if you have broken things down into smaller subtasks (say 4) then that first thread could be "helping" the second to complete the overall job. But break it down too fine and you get too much task overhead. Hence the heuristic value of 8 - which is generally a good starting point. Plus performance is not super-sensitive to getting the LoadBalanceFactor exactly right.


  True, but cache-effects (& hardward) also has to be taken into consideration (shared l2 vs shared l3 vs going cross-socket/NUMA).
  Ideally the user API would be devoid of encoding such assumtptions, as they will vary on a platform level?


  Cheers,
  ?


    David
      -----Original Message-----
      From: viktor ?lang [mailto:viktor.klang at gmail.com]
      Sent: Tuesday, 12 February 2013 8:25 AM
      To: dholmes at ieee.org
      Cc: thurston; concurrency-interest
      Subject: Re: [concurrency-interest] Some interesting (confusing?) benchmark results







      On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

        That doesn't make sense to me. If you had a threshold of 250 then you are
        creating thousands of tasks and that overhead should potentially bring your
        laptop to its knees. With 250K threshold you have far fewer tasks.



      I'm not sure it makes sense to create more sort-tasks than there is Threads in the pool.


        That said I don't know what the memory requirements are for your algorithm.
        If you are allocating temporary arrays then a  large threshold could well
        cause a problem.

        The parallel array sort going into JDK 8 is a merge sort but uses a working
        array equal in size to the original array to be sorted.



      No in-place merge sort? ;-)


        David


        > -----Original Message-----
        > From: concurrency-interest-bounces at cs.oswego.edu
        > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
        > thurston at nomagicsoftware.com

        > Sent: Tuesday, 12 February 2013 7:49 AM
        > To: concurrency-interest at cs.oswego.edu
        > Subject: Re: [concurrency-interest] Some interesting (confusing?)
        > benchmark results
        >
        >
        > Well, it's the algorithm that's dispositive in this case.
        > Insertion-sort is empirically better than mergesort up to around
        > 25-sized arrays.  That's where I started, and I played around with
        > different thresholds and 250 seemed to be the best.
        > Anything larger than 1000 you start to see significant degradation.
        > I tried your formula (approximately) with N = 2 000 000 ==> 250K
        >
        > That brought my laptop to its knees (as expected): over 33K ms as
        > opposed to 390ms with 250.
        >
        > Following my own admonition (the algorithm matters), it occurs to me
        > that I just use my own serial mergesort as comparison to the Forkjoin
        > (as opposed to Arrays.sort).  I'll let y'all know the results of that
        >
        > -T
        >
        >
        >
        > On 2013-02-11 13:19, David Holmes wrote:
        > > Your sequential threshold is far too small for only two worker
        > > threads. You
        > > would get near optimum speedup only splitting the problem into two.
        > >
        > > The heuristic for sequential threshold is:
        > >
        > > Threshold = N / (LoadBalanceFactor * NumCore)
        > >
        > > where N is the "size" of the problem.
        > >
        > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
        > > encounter
        > > delays and execute differently so a value > 1 is needed.
        > > Heuristically a
        > > value of 8 is a good starting value.
        > >
        > > David Holmes
        > >
        > >> -----Original Message-----
        > >> From: concurrency-interest-bounces at cs.oswego.edu
        > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
        > >> thurston at nomagicsoftware.com
        > >> Sent: Tuesday, 12 February 2013 6:52 AM
        > >> To: concurrency-interest at cs.oswego.edu
        > >> Subject: [concurrency-interest] Some interesting (confusing?)
        > >> benchmark
        > >> results
        > >>
        > >>
        > >> Hello,
        > >>
        > >> I made some initial attempts at using ForkJoin framework for some
        > >> rather obvious recursively parallel use-cases, namely summing the
        > >> elements of an int[] and also sorting an int[] (randomly filled).
        > >>
        > >> The problem-sizes I used ranged from 1M - 10M.
        > >>
        > >> I really enjoy using the framework and find it relatively easy to
        > >> reason about my programs (if not necessarily the internals of the
        > >> framework).
        > >> But I was disappointed with the results: in both cases they were
        > >> slower
        > >> than the corresponding Java serial implementation.
        > >>
        > >> I completely understand the principle of YMMV, and I wasn't
        > >> expecting
        > >> 2x speedup, but especially in the case of the sort, but I was
        > >> expecting
        > >> that ForkJoin would do at least a little better than the
        > >> single-threaded
        > >> version.  I guess what I'm asking is: are these results surprising?
        > >> Does it mean I'm doing something wrong?
        > >>
        > >> I'll give a brief description of my implementation:
        > >> single-threaded:  Arrays.sort(int[])
        > >>
        > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
        > >> by
        > >> trial-and-error)
        > >>
        > >> int[] compute()
        > >> {
        > >> if int[].length < THRESHOLD
        > >>      return insertionSort(int[])
        > >> left = new MergeSort(int[] //split in half)
        > >> right = new MergeSort(int[] //other half)
        > >> return merge(right.compute(), left.join())
        > >> }
        > >>
        > >> The insertionSort() and merge() methods are just standard
        > >> implementations; there is a bit of apples to oranges comparison
        > >> since
        > >> Arrays.sort() uses an optimized quicksort, but we're still talking
        > >> O(nlog(n))
        > >>
        > >> Just ran it on my laptop:
        > >> Windows 7 64-bit
        > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
        > >> Core 2 Duo==> 2 cores 2GHz
        > >>
        > >> 2M int[]:
        > >> single-threaded: ~330ms
        > >> ForkJoin (2 workers): ~390ms
        > >>
        > >> Would appreciate any feedback and hopefully the #s are somewhat
        > >> helpful
        > >> (and please no scoffawing at my antiquated machine)
        > >>
        > >> -T
        > >>
        > >>
        > >> _______________________________________________
        > >> Concurrency-interest mailing list
        > >> Concurrency-interest at cs.oswego.edu
        > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
        > >>
        >
        > _______________________________________________
        > Concurrency-interest mailing list
        > Concurrency-interest at cs.oswego.edu
        > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
        >

        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest






      -- 

      Viktor Klang
      Director of Engineering 


      Typesafe - The software stack for applications that scale
      Twitter: @viktorklang





  -- 

  Viktor Klang
  Director of Engineering


  Typesafe - The software stack for applications that scale
  Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/d62be6c1/attachment-0001.html>

From gregg at cytetech.com  Mon Feb 11 22:24:18 2013
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 11 Feb 2013 21:24:18 -0600
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
Message-ID: <5119B5E2.6010502@cytetech.com>

On windows, when swapping starts, things go down hill quickly.  How much RAM 
does your laptop have?  What kind of active memory use differences would there 
be for the smaller sized arrays?  When there are lots of small arrays, I'd 
expect to see minimal swapping, because accesses for sorting would be confined 
to small amounts of contiguous memory.  Large arrays being sorted would create 
lots of non-contiguous memory access, which could create a large amount of swapping.

Gregg Wonderly

On 2/11/2013 4:18 PM, David Holmes wrote:
> That doesn't make sense to me. If you had a threshold of 250 then you are
> creating thousands of tasks and that overhead should potentially bring your
> laptop to its knees. With 250K threshold you have far fewer tasks.
>
> That said I don't know what the memory requirements are for your algorithm.
> If you are allocating temporary arrays then a  large threshold could well
> cause a problem.
>
> The parallel array sort going into JDK 8 is a merge sort but uses a working
> array equal in size to the original array to be sorted.
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> thurston at nomagicsoftware.com
>> Sent: Tuesday, 12 February 2013 7:49 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> benchmark results
>>
>>
>> Well, it's the algorithm that's dispositive in this case.
>> Insertion-sort is empirically better than mergesort up to around
>> 25-sized arrays.  That's where I started, and I played around with
>> different thresholds and 250 seemed to be the best.
>> Anything larger than 1000 you start to see significant degradation.
>> I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>
>> That brought my laptop to its knees (as expected): over 33K ms as
>> opposed to 390ms with 250.
>>
>> Following my own admonition (the algorithm matters), it occurs to me
>> that I just use my own serial mergesort as comparison to the Forkjoin
>> (as opposed to Arrays.sort).  I'll let y'all know the results of that
>>
>> -T
>>
>>
>>
>> On 2013-02-11 13:19, David Holmes wrote:
>>> Your sequential threshold is far too small for only two worker
>>> threads. You
>>> would get near optimum speedup only splitting the problem into two.
>>>
>>> The heuristic for sequential threshold is:
>>>
>>> Threshold = N / (LoadBalanceFactor * NumCore)
>>>
>>> where N is the "size" of the problem.
>>>
>>> The LoadBalanceFactor in an ideal world would be 1 but real tasks
>>> encounter
>>> delays and execute differently so a value > 1 is needed.
>>> Heuristically a
>>> value of 8 is a good starting value.
>>>
>>> David Holmes
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>> thurston at nomagicsoftware.com
>>>> Sent: Tuesday, 12 February 2013 6:52 AM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: [concurrency-interest] Some interesting (confusing?)
>>>> benchmark
>>>> results
>>>>
>>>>
>>>> Hello,
>>>>
>>>> I made some initial attempts at using ForkJoin framework for some
>>>> rather obvious recursively parallel use-cases, namely summing the
>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>
>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>
>>>> I really enjoy using the framework and find it relatively easy to
>>>> reason about my programs (if not necessarily the internals of the
>>>> framework).
>>>> But I was disappointed with the results: in both cases they were
>>>> slower
>>>> than the corresponding Java serial implementation.
>>>>
>>>> I completely understand the principle of YMMV, and I wasn't
>>>> expecting
>>>> 2x speedup, but especially in the case of the sort, but I was
>>>> expecting
>>>> that ForkJoin would do at least a little better than the
>>>> single-threaded
>>>> version.  I guess what I'm asking is: are these results surprising?
>>>> Does it mean I'm doing something wrong?
>>>>
>>>> I'll give a brief description of my implementation:
>>>> single-threaded:  Arrays.sort(int[])
>>>>
>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
>>>> by
>>>> trial-and-error)
>>>>
>>>> int[] compute()
>>>> {
>>>> if int[].length < THRESHOLD
>>>>       return insertionSort(int[])
>>>> left = new MergeSort(int[] //split in half)
>>>> right = new MergeSort(int[] //other half)
>>>> return merge(right.compute(), left.join())
>>>> }
>>>>
>>>> The insertionSort() and merge() methods are just standard
>>>> implementations; there is a bit of apples to oranges comparison
>>>> since
>>>> Arrays.sort() uses an optimized quicksort, but we're still talking
>>>> O(nlog(n))
>>>>
>>>> Just ran it on my laptop:
>>>> Windows 7 64-bit
>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> Core 2 Duo==> 2 cores 2GHz
>>>>
>>>> 2M int[]:
>>>> single-threaded: ~330ms
>>>> ForkJoin (2 workers): ~390ms
>>>>
>>>> Would appreciate any feedback and hopefully the #s are somewhat
>>>> helpful
>>>> (and please no scoffawing at my antiquated machine)
>>>>
>>>> -T
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From joe.bowbeer at gmail.com  Mon Feb 11 23:08:31 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 11 Feb 2013 20:08:31 -0800
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <CAHzJPEoJNzGs0OEzMcu4=tgMZHAz7kAnr=OVRdDnWmXJLwEN9w@mail.gmail.com>

It looks to me like your code is completely subdividing the task, creating
a huge computation tree consisting of tiny tasks.

Instead, I recommend the refinement that is illustrated in the sumOfSquares
example in the javadoc:

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/RecursiveAction.html

The getSurplusQueuedTaskCount is key:

while (h - l > 1 && getSurplusQueuedTaskCount() <= 3) {

Also, I suggest comparing notes with this article by Brian Goetz:

  Accelerate sorting and searching with the ParallelArray classes in Java 7
  http://www.ibm.com/developerworks/java/library/j-jtp03048/index.html

You might also consider comparing your technique with that employed in
ParallelArray.sort()

  http://gee.cs.oswego.edu/dl/jsr166/dist/

--Joe

On Mon, Feb 11, 2013 at 12:52 PM, T wrote:

> Hello,
>
> I made some initial attempts at using ForkJoin framework for some rather
> obvious recursively parallel use-cases, namely summing the elements of an
> int[] and also sorting an int[] (randomly filled).
>
> The problem-sizes I used ranged from 1M - 10M.
>
> I really enjoy using the framework and find it relatively easy to reason
> about my programs (if not necessarily the internals of the framework).
> But I was disappointed with the results: in both cases they were slower
> than the corresponding Java serial implementation.
>
> I completely understand the principle of YMMV, and I wasn't expecting 2x
> speedup, but especially in the case of the sort, but I was expecting that
> ForkJoin would do at least a little better than the single-threaded
> version.  I guess what I'm asking is: are these results surprising?  Does
> it mean I'm doing something wrong?
>
> I'll give a brief description of my implementation:
> single-threaded:  Arrays.sort(int[])
>
> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by
> trial-and-error)
>
> int[] compute()
> {
> if int[].length < THRESHOLD
>     return insertionSort(int[])
> left = new MergeSort(int[] //split in half)
> right = new MergeSort(int[] //other half)
> return merge(right.compute(), left.join())
> }
>
> The insertionSort() and merge() methods are just standard implementations;
> there is a bit of apples to oranges comparison since Arrays.sort() uses an
> optimized quicksort, but we're still talking O(nlog(n))
>
> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz
>
> 2M int[]:
> single-threaded: ~330ms
> ForkJoin (2 workers): ~390ms
>
> Would appreciate any feedback and hopefully the #s are somewhat helpful
> (and please no scoffawing at my antiquated machine)
>
> -T
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130211/93d9daad/attachment.html>

From thurston at nomagicsoftware.com  Tue Feb 12 00:13:53 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 11 Feb 2013 21:13:53 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
Message-ID: <ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>



On 2013-02-11 14:24, ?iktor ?lang wrote:
> On Mon, Feb 11, 2013 at 11:18 PM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
>
>> That doesn't make sense to me. If you had a threshold of 250 then 
>> you are
>> creating thousands of tasks and that overhead should potentially 
>> bring your
>> laptop to its knees. With 250K threshold you have far fewer tasks.
>
> I'm not sure it makes sense to create more sort-tasks than there is
> Threads in the pool.

Really, Viktor?  That surprises me coming from Akka.  Does that mean 
you guys only allocate as many message-processing "workers" as there are 
hardware threads?

To be clear, with the 2M size array data, and the threshold of 250-size 
array for the (serial) insertion sort, 16K+ sort tasks are constructed 
(as D. Holmes mentioned).  This is significantly *faster* than doing 
what either of you suggested.  If I simply split the problem in two, and 
just run a serial mergesort on each half and then merge the two halves 
(that's three total tasks), it's over 50% slower.

Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.

This is in line with my expectation; what amount of work-stealing could 
happen with two tasks (each merge-sorting a 1M item array)?  With 16K 
tasks, the work-stealing can really strut its stuff (and at least in my 
case,can overwhelm the overhead associated with scheduling the tasks)

If you only create as many tasks as there are hardware threads -- that 
seems a lot like a standard thread-pool implementation.  (Of course, if 
there are many more cores, the opportunity for work-stealing would 
increase even with the 1-1 ratio).

When I first used fork/join, that's the approach I took.
1.  split the problem (hardware threads * (1||2)
2.  have each task do its work serially (i.e. don't fork)
3.  combine the (sub)totals/results

Not only is that less appealing from a programming point of view, it 
performs slower; that's the promise, nee even purpose, of the fork/join 
framework at least when it comes to implementing naturally recursive 
algorithms.

-T


>
>> That said I don't know what the memory requirements are for your 
>> algorithm.
>> If you are allocating temporary arrays then a ?large threshold could 
>> well
>> cause a problem.
>>
>> The parallel array sort going into JDK 8 is a merge sort but uses a 
>> working
>> array equal in size to the original array to be sorted.
>
> No in-place merge sort? ;-)
> ?
>
>>> -----Original Message-----
>> > From: concurrency-interest-bounces at cs.oswego.edu
>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> > thurston at nomagicsoftware.com
>>
>>> Sent: Tuesday, 12 February 2013 7:49 AM
>> > To: concurrency-interest at cs.oswego.edu
>> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> > benchmark results
>> >
>> >
>> > Well, it's the algorithm that's dispositive in this case.
>> > Insertion-sort is empirically better than mergesort up to around
>> > 25-sized arrays. ?That's where I started, and I played around with
>> > different thresholds and 250 seemed to be the best.
>> > Anything larger than 1000 you start to see significant 
>> degradation.
>> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
>> >
>> > That brought my laptop to its knees (as expected): over 33K ms as
>> > opposed to 390ms with 250.
>> >
>> > Following my own admonition (the algorithm matters), it occurs to 
>> me
>> > that I just use my own serial mergesort as comparison to the 
>> Forkjoin
>> > (as opposed to Arrays.sort). ?I'll let y'all know the results of 
>> that
>> >
>> > -T
>> >
>> >
>> >
>> > On 2013-02-11 13:19, David Holmes wrote:
>> > > Your sequential threshold is far too small for only two worker
>> > > threads. You
>> > > would get near optimum speedup only splitting the problem into 
>> two.
>> > >
>> > > The heuristic for sequential threshold is:
>> > >
>> > > Threshold = N / (LoadBalanceFactor * NumCore)
>> > >
>> > > where N is the "size" of the problem.
>> > >
>> > > The LoadBalanceFactor in an ideal world would be 1 but real 
>> tasks
>> > > encounter
>> > > delays and execute differently so a value > 1 is needed.
>> > > Heuristically a
>> > > value of 8 is a good starting value.
>> > >
>> > > David Holmes
>> > >
>> > >> -----Original Message-----
>> > >> From: concurrency-interest-bounces at cs.oswego.edu
>> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> > >> thurston at nomagicsoftware.com
>> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>> > >> To: concurrency-interest at cs.oswego.edu
>> > >> Subject: [concurrency-interest] Some interesting (confusing?)
>> > >> benchmark
>> > >> results
>> > >>
>> > >>
>> > >> Hello,
>> > >>
>> > >> I made some initial attempts at using ForkJoin framework for 
>> some
>> > >> rather obvious recursively parallel use-cases, namely summing 
>> the
>> > >> elements of an int[] and also sorting an int[] (randomly 
>> filled).
>> > >>
>> > >> The problem-sizes I used ranged from 1M - 10M.
>> > >>
>> > >> I really enjoy using the framework and find it relatively easy 
>> to
>> > >> reason about my programs (if not necessarily the internals of 
>> the
>> > >> framework).
>> > >> But I was disappointed with the results: in both cases they 
>> were
>> > >> slower
>> > >> than the corresponding Java serial implementation.
>> > >>
>> > >> I completely understand the principle of YMMV, and I wasn't
>> > >> expecting
>> > >> 2x speedup, but especially in the case of the sort, but I was
>> > >> expecting
>> > >> that ForkJoin would do at least a little better than the
>> > >> single-threaded
>> > >> version. ?I guess what I'm asking is: are these results 
>> surprising?
>> > >> Does it mean I'm doing something wrong?
>> > >>
>> > >> I'll give a brief description of my implementation:
>> > >> single-threaded: ?Arrays.sort(int[])
>> > >>
>> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>> at
>> > >> by
>> > >> trial-and-error)
>> > >>
>> > >> int[] compute()
>> > >> {
>> > >> if int[].length < THRESHOLD
>> > >> ? ? ?return insertionSort(int[])
>> > >> left = new MergeSort(int[] //split in half)
>> > >> right = new MergeSort(int[] //other half)
>> > >> return merge(right.compute(), left.join())
>> > >> }
>> > >>
>> > >> The insertionSort() and merge() methods are just standard
>> > >> implementations; there is a bit of apples to oranges comparison
>> > >> since
>> > >> Arrays.sort() uses an optimized quicksort, but we're still 
>> talking
>> > >> O(nlog(n))
>> > >>
>> > >> Just ran it on my laptop:
>> > >> Windows 7 64-bit
>> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> > >> Core 2 Duo==> 2 cores 2GHz
>> > >>
>> > >> 2M int[]:
>> > >> single-threaded: ~330ms
>> > >> ForkJoin (2 workers): ~390ms
>> > >>
>> > >> Would appreciate any feedback and hopefully the #s are somewhat
>> > >> helpful
>> > >> (and please no scoffawing at my antiquated machine)
>> > >>
>> > >> -T
>> > >>
>> > >>
>> > >> _______________________________________________
>> > >> Concurrency-interest mailing list
>> > >> Concurrency-interest at cs.oswego.edu
>> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> > >>
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>
>> --
> 
> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;font-size:medium">VIKTOR
> KLANG
>  _Director of Engineering_
>
>  Typesafe [2]?- The software stack for applications that scale
>  Twitter: @viktorklang
>
> Links:
> ------
> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> [2] http://www.typesafe.com/


From davidcholmes at aapt.net.au  Tue Feb 12 00:26:00 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 15:26:00 +1000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEEJJKAA.davidcholmes@aapt.net.au>

> This is in line with my expectation; what amount of work-stealing could 
> happen with two tasks (each merge-sorting a 1M item array)?  With 16K 
> tasks, the work-stealing can really strut its stuff (and at least in my 
> case,can overwhelm the overhead associated with scheduling the tasks)

I think you are seeing affects that are not due to parallelisation of the algorithm as such. 16K tasks for two threads is just ridiculous task creation and management overhead. If you see a speed up compared to three tasks then you are either not comparing apples to oranges, or you are affected by other things - such as memory management when dealing with large arrays.

David
-----


> -----Original Message-----
> From: thurston at nomagicsoftware.com [mailto:thurston at nomagicsoftware.com]
> Sent: Tuesday, 12 February 2013 3:14 PM
> To: viktor ?lang
> Cc: dholmes at ieee.org; concurrency-interest
> Subject: Re: [concurrency-interest] Some interesting (confusing?)
> benchmark results
> 
> 
> 
> 
> On 2013-02-11 14:24, ?iktor ?lang wrote:
> > On Mon, Feb 11, 2013 at 11:18 PM, David Holmes 
> > <davidcholmes at aapt.net.au> wrote:
> >
> >> That doesn't make sense to me. If you had a threshold of 250 then 
> >> you are
> >> creating thousands of tasks and that overhead should potentially 
> >> bring your
> >> laptop to its knees. With 250K threshold you have far fewer tasks.
> >
> > I'm not sure it makes sense to create more sort-tasks than there is
> > Threads in the pool.
> 
> Really, Viktor?  That surprises me coming from Akka.  Does that mean 
> you guys only allocate as many message-processing "workers" as there are 
> hardware threads?
> 
> To be clear, with the 2M size array data, and the threshold of 250-size 
> array for the (serial) insertion sort, 16K+ sort tasks are constructed 
> (as D. Holmes mentioned).  This is significantly *faster* than doing 
> what either of you suggested.  If I simply split the problem in two, and 
> just run a serial mergesort on each half and then merge the two halves 
> (that's three total tasks), it's over 50% slower.
> 
> Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.
> 
> This is in line with my expectation; what amount of work-stealing could 
> happen with two tasks (each merge-sorting a 1M item array)?  With 16K 
> tasks, the work-stealing can really strut its stuff (and at least in my 
> case,can overwhelm the overhead associated with scheduling the tasks)
> 
> If you only create as many tasks as there are hardware threads -- that 
> seems a lot like a standard thread-pool implementation.  (Of course, if 
> there are many more cores, the opportunity for work-stealing would 
> increase even with the 1-1 ratio).
> 
> When I first used fork/join, that's the approach I took.
> 1.  split the problem (hardware threads * (1||2)
> 2.  have each task do its work serially (i.e. don't fork)
> 3.  combine the (sub)totals/results
> 
> Not only is that less appealing from a programming point of view, it 
> performs slower; that's the promise, nee even purpose, of the fork/join 
> framework at least when it comes to implementing naturally recursive 
> algorithms.
> 
> -T
> 
> 
> >
> >> That said I don't know what the memory requirements are for your 
> >> algorithm.
> >> If you are allocating temporary arrays then a  large threshold could 
> >> well
> >> cause a problem.
> >>
> >> The parallel array sort going into JDK 8 is a merge sort but uses a 
> >> working
> >> array equal in size to the original array to be sorted.
> >
> > No in-place merge sort? ;-)
> >  
> >
> >>> -----Original Message-----
> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> > thurston at nomagicsoftware.com
> >>
> >>> Sent: Tuesday, 12 February 2013 7:49 AM
> >> > To: concurrency-interest at cs.oswego.edu
> >> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
> >> > benchmark results
> >> >
> >> >
> >> > Well, it's the algorithm that's dispositive in this case.
> >> > Insertion-sort is empirically better than mergesort up to around
> >> > 25-sized arrays.  That's where I started, and I played around with
> >> > different thresholds and 250 seemed to be the best.
> >> > Anything larger than 1000 you start to see significant 
> >> degradation.
> >> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
> >> >
> >> > That brought my laptop to its knees (as expected): over 33K ms as
> >> > opposed to 390ms with 250.
> >> >
> >> > Following my own admonition (the algorithm matters), it occurs to 
> >> me
> >> > that I just use my own serial mergesort as comparison to the 
> >> Forkjoin
> >> > (as opposed to Arrays.sort).  I'll let y'all know the results of 
> >> that
> >> >
> >> > -T
> >> >
> >> >
> >> >
> >> > On 2013-02-11 13:19, David Holmes wrote:
> >> > > Your sequential threshold is far too small for only two worker
> >> > > threads. You
> >> > > would get near optimum speedup only splitting the problem into 
> >> two.
> >> > >
> >> > > The heuristic for sequential threshold is:
> >> > >
> >> > > Threshold = N / (LoadBalanceFactor * NumCore)
> >> > >
> >> > > where N is the "size" of the problem.
> >> > >
> >> > > The LoadBalanceFactor in an ideal world would be 1 but real 
> >> tasks
> >> > > encounter
> >> > > delays and execute differently so a value > 1 is needed.
> >> > > Heuristically a
> >> > > value of 8 is a good starting value.
> >> > >
> >> > > David Holmes
> >> > >
> >> > >> -----Original Message-----
> >> > >> From: concurrency-interest-bounces at cs.oswego.edu
> >> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> > >> thurston at nomagicsoftware.com
> >> > >> Sent: Tuesday, 12 February 2013 6:52 AM
> >> > >> To: concurrency-interest at cs.oswego.edu
> >> > >> Subject: [concurrency-interest] Some interesting (confusing?)
> >> > >> benchmark
> >> > >> results
> >> > >>
> >> > >>
> >> > >> Hello,
> >> > >>
> >> > >> I made some initial attempts at using ForkJoin framework for 
> >> some
> >> > >> rather obvious recursively parallel use-cases, namely summing 
> >> the
> >> > >> elements of an int[] and also sorting an int[] (randomly 
> >> filled).
> >> > >>
> >> > >> The problem-sizes I used ranged from 1M - 10M.
> >> > >>
> >> > >> I really enjoy using the framework and find it relatively easy 
> >> to
> >> > >> reason about my programs (if not necessarily the internals of 
> >> the
> >> > >> framework).
> >> > >> But I was disappointed with the results: in both cases they 
> >> were
> >> > >> slower
> >> > >> than the corresponding Java serial implementation.
> >> > >>
> >> > >> I completely understand the principle of YMMV, and I wasn't
> >> > >> expecting
> >> > >> 2x speedup, but especially in the case of the sort, but I was
> >> > >> expecting
> >> > >> that ForkJoin would do at least a little better than the
> >> > >> single-threaded
> >> > >> version.  I guess what I'm asking is: are these results 
> >> surprising?
> >> > >> Does it mean I'm doing something wrong?
> >> > >>
> >> > >> I'll give a brief description of my implementation:
> >> > >> single-threaded:  Arrays.sort(int[])
> >> > >>
> >> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
> >> at
> >> > >> by
> >> > >> trial-and-error)
> >> > >>
> >> > >> int[] compute()
> >> > >> {
> >> > >> if int[].length < THRESHOLD
> >> > >>      return insertionSort(int[])
> >> > >> left = new MergeSort(int[] //split in half)
> >> > >> right = new MergeSort(int[] //other half)
> >> > >> return merge(right.compute(), left.join())
> >> > >> }
> >> > >>
> >> > >> The insertionSort() and merge() methods are just standard
> >> > >> implementations; there is a bit of apples to oranges comparison
> >> > >> since
> >> > >> Arrays.sort() uses an optimized quicksort, but we're still 
> >> talking
> >> > >> O(nlog(n))
> >> > >>
> >> > >> Just ran it on my laptop:
> >> > >> Windows 7 64-bit
> >> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> >> > >> Core 2 Duo==> 2 cores 2GHz
> >> > >>
> >> > >> 2M int[]:
> >> > >> single-threaded: ~330ms
> >> > >> ForkJoin (2 workers): ~390ms
> >> > >>
> >> > >> Would appreciate any feedback and hopefully the #s are somewhat
> >> > >> helpful
> >> > >> (and please no scoffawing at my antiquated machine)
> >> > >>
> >> > >> -T
> >> > >>
> >> > >>
> >> > >> _______________________________________________
> >> > >> Concurrency-interest mailing list
> >> > >> Concurrency-interest at cs.oswego.edu
> >> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
> >> > >>
> >> >
> >> > _______________________________________________
> >> > Concurrency-interest mailing list
> >> > Concurrency-interest at cs.oswego.edu
> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
> >> >
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
> >>
> >> --
> > 
> > 
> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:
> normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;
> text-transform:none;white-space:normal;word-spacing:0px;font-size:
> medium">VIKTOR
> > KLANG
> >  _Director of Engineering_
> >
> >  Typesafe [2] - The software stack for applications that scale
> >  Twitter: @viktorklang
> >
> > Links:
> > ------
> > [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > [2] http://www.typesafe.com/
> 
> 



From thurston at nomagicsoftware.com  Tue Feb 12 00:45:08 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 11 Feb 2013 21:45:08 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEEJJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEEJJKAA.davidcholmes@aapt.net.au>
Message-ID: <4461c027da10d1bd444d236c2490ba54@nomagicsoftware.com>

The #s are the ultimate arbiter, aren't they?  I can't imagine that the 
3-task solution copies the array more often than the 16K solution, since 
each level of the DAG results in a duplicate of the original array (of 
course in smaller and smaller chunks)

16K:  ~385ms
3:    ~550ms

On 2013-02-11 21:26, David Holmes wrote:
>> This is in line with my expectation; what amount of work-stealing 
>> could
>> happen with two tasks (each merge-sorting a 1M item array)?  With 
>> 16K
>> tasks, the work-stealing can really strut its stuff (and at least in 
>> my
>> case,can overwhelm the overhead associated with scheduling the 
>> tasks)
>
> I think you are seeing affects that are not due to parallelisation of
> the algorithm as such. 16K tasks for two threads is just ridiculous
> task creation and management overhead. If you see a speed up compared
> to three tasks then you are either not comparing apples to oranges, 
> or
> you are affected by other things - such as memory management when
> dealing with large arrays.
>
> David
> -----
>
>
>> -----Original Message-----
>> From: thurston at nomagicsoftware.com 
>> [mailto:thurston at nomagicsoftware.com]
>> Sent: Tuesday, 12 February 2013 3:14 PM
>> To: viktor ?lang
>> Cc: dholmes at ieee.org; concurrency-interest
>> Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> benchmark results
>>
>>
>>
>>
>> On 2013-02-11 14:24, ?iktor ?lang wrote:
>> > On Mon, Feb 11, 2013 at 11:18 PM, David Holmes
>> > <davidcholmes at aapt.net.au> wrote:
>> >
>> >> That doesn't make sense to me. If you had a threshold of 250 then
>> >> you are
>> >> creating thousands of tasks and that overhead should potentially
>> >> bring your
>> >> laptop to its knees. With 250K threshold you have far fewer 
>> tasks.
>> >
>> > I'm not sure it makes sense to create more sort-tasks than there 
>> is
>> > Threads in the pool.
>>
>> Really, Viktor?  That surprises me coming from Akka.  Does that mean
>> you guys only allocate as many message-processing "workers" as there 
>> are
>> hardware threads?
>>
>> To be clear, with the 2M size array data, and the threshold of 
>> 250-size
>> array for the (serial) insertion sort, 16K+ sort tasks are 
>> constructed
>> (as D. Holmes mentioned).  This is significantly *faster* than doing
>> what either of you suggested.  If I simply split the problem in two, 
>> and
>> just run a serial mergesort on each half and then merge the two 
>> halves
>> (that's three total tasks), it's over 50% slower.
>>
>> Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.
>>
>> This is in line with my expectation; what amount of work-stealing 
>> could
>> happen with two tasks (each merge-sorting a 1M item array)?  With 
>> 16K
>> tasks, the work-stealing can really strut its stuff (and at least in 
>> my
>> case,can overwhelm the overhead associated with scheduling the 
>> tasks)
>>
>> If you only create as many tasks as there are hardware threads -- 
>> that
>> seems a lot like a standard thread-pool implementation.  (Of course, 
>> if
>> there are many more cores, the opportunity for work-stealing would
>> increase even with the 1-1 ratio).
>>
>> When I first used fork/join, that's the approach I took.
>> 1.  split the problem (hardware threads * (1||2)
>> 2.  have each task do its work serially (i.e. don't fork)
>> 3.  combine the (sub)totals/results
>>
>> Not only is that less appealing from a programming point of view, it
>> performs slower; that's the promise, nee even purpose, of the 
>> fork/join
>> framework at least when it comes to implementing naturally recursive
>> algorithms.
>>
>> -T
>>
>>
>> >
>> >> That said I don't know what the memory requirements are for your
>> >> algorithm.
>> >> If you are allocating temporary arrays then a  large threshold 
>> could
>> >> well
>> >> cause a problem.
>> >>
>> >> The parallel array sort going into JDK 8 is a merge sort but uses 
>> a
>> >> working
>> >> array equal in size to the original array to be sorted.
>> >
>> > No in-place merge sort? ;-)
>> >
>> >
>> >>> -----Original Message-----
>> >> > From: concurrency-interest-bounces at cs.oswego.edu
>> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> >> > thurston at nomagicsoftware.com
>> >>
>> >>> Sent: Tuesday, 12 February 2013 7:49 AM
>> >> > To: concurrency-interest at cs.oswego.edu
>> >> > Subject: Re: [concurrency-interest] Some interesting 
>> (confusing?)
>> >> > benchmark results
>> >> >
>> >> >
>> >> > Well, it's the algorithm that's dispositive in this case.
>> >> > Insertion-sort is empirically better than mergesort up to 
>> around
>> >> > 25-sized arrays.  That's where I started, and I played around 
>> with
>> >> > different thresholds and 250 seemed to be the best.
>> >> > Anything larger than 1000 you start to see significant
>> >> degradation.
>> >> > I tried your formula (approximately) with N = 2 000 000 ==> 
>> 250K
>> >> >
>> >> > That brought my laptop to its knees (as expected): over 33K ms 
>> as
>> >> > opposed to 390ms with 250.
>> >> >
>> >> > Following my own admonition (the algorithm matters), it occurs 
>> to
>> >> me
>> >> > that I just use my own serial mergesort as comparison to the
>> >> Forkjoin
>> >> > (as opposed to Arrays.sort).  I'll let y'all know the results 
>> of
>> >> that
>> >> >
>> >> > -T
>> >> >
>> >> >
>> >> >
>> >> > On 2013-02-11 13:19, David Holmes wrote:
>> >> > > Your sequential threshold is far too small for only two 
>> worker
>> >> > > threads. You
>> >> > > would get near optimum speedup only splitting the problem 
>> into
>> >> two.
>> >> > >
>> >> > > The heuristic for sequential threshold is:
>> >> > >
>> >> > > Threshold = N / (LoadBalanceFactor * NumCore)
>> >> > >
>> >> > > where N is the "size" of the problem.
>> >> > >
>> >> > > The LoadBalanceFactor in an ideal world would be 1 but real
>> >> tasks
>> >> > > encounter
>> >> > > delays and execute differently so a value > 1 is needed.
>> >> > > Heuristically a
>> >> > > value of 8 is a good starting value.
>> >> > >
>> >> > > David Holmes
>> >> > >
>> >> > >> -----Original Message-----
>> >> > >> From: concurrency-interest-bounces at cs.oswego.edu
>> >> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
>> Of
>> >> > >> thurston at nomagicsoftware.com
>> >> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>> >> > >> To: concurrency-interest at cs.oswego.edu
>> >> > >> Subject: [concurrency-interest] Some interesting 
>> (confusing?)
>> >> > >> benchmark
>> >> > >> results
>> >> > >>
>> >> > >>
>> >> > >> Hello,
>> >> > >>
>> >> > >> I made some initial attempts at using ForkJoin framework for
>> >> some
>> >> > >> rather obvious recursively parallel use-cases, namely 
>> summing
>> >> the
>> >> > >> elements of an int[] and also sorting an int[] (randomly
>> >> filled).
>> >> > >>
>> >> > >> The problem-sizes I used ranged from 1M - 10M.
>> >> > >>
>> >> > >> I really enjoy using the framework and find it relatively 
>> easy
>> >> to
>> >> > >> reason about my programs (if not necessarily the internals 
>> of
>> >> the
>> >> > >> framework).
>> >> > >> But I was disappointed with the results: in both cases they
>> >> were
>> >> > >> slower
>> >> > >> than the corresponding Java serial implementation.
>> >> > >>
>> >> > >> I completely understand the principle of YMMV, and I wasn't
>> >> > >> expecting
>> >> > >> 2x speedup, but especially in the case of the sort, but I 
>> was
>> >> > >> expecting
>> >> > >> that ForkJoin would do at least a little better than the
>> >> > >> single-threaded
>> >> > >> version.  I guess what I'm asking is: are these results
>> >> surprising?
>> >> > >> Does it mean I'm doing something wrong?
>> >> > >>
>> >> > >> I'll give a brief description of my implementation:
>> >> > >> single-threaded:  Arrays.sort(int[])
>> >> > >>
>> >> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 
>> (arrived
>> >> at
>> >> > >> by
>> >> > >> trial-and-error)
>> >> > >>
>> >> > >> int[] compute()
>> >> > >> {
>> >> > >> if int[].length < THRESHOLD
>> >> > >>      return insertionSort(int[])
>> >> > >> left = new MergeSort(int[] //split in half)
>> >> > >> right = new MergeSort(int[] //other half)
>> >> > >> return merge(right.compute(), left.join())
>> >> > >> }
>> >> > >>
>> >> > >> The insertionSort() and merge() methods are just standard
>> >> > >> implementations; there is a bit of apples to oranges 
>> comparison
>> >> > >> since
>> >> > >> Arrays.sort() uses an optimized quicksort, but we're still
>> >> talking
>> >> > >> O(nlog(n))
>> >> > >>
>> >> > >> Just ran it on my laptop:
>> >> > >> Windows 7 64-bit
>> >> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> >> > >> Core 2 Duo==> 2 cores 2GHz
>> >> > >>
>> >> > >> 2M int[]:
>> >> > >> single-threaded: ~330ms
>> >> > >> ForkJoin (2 workers): ~390ms
>> >> > >>
>> >> > >> Would appreciate any feedback and hopefully the #s are 
>> somewhat
>> >> > >> helpful
>> >> > >> (and please no scoffawing at my antiquated machine)
>> >> > >>
>> >> > >> -T
>> >> > >>
>> >> > >>
>> >> > >> _______________________________________________
>> >> > >> Concurrency-interest mailing list
>> >> > >> Concurrency-interest at cs.oswego.edu
>> >> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>> [1]
>> >> > >>
>> >> >
>> >> > _______________________________________________
>> >> > Concurrency-interest mailing list
>> >> > Concurrency-interest at cs.oswego.edu
>> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> >> >
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> >>
>> >> --
>> >
>> >
>> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:
>> normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;
>> text-transform:none;white-space:normal;word-spacing:0px;font-size:
>> medium">VIKTOR
>> > KLANG
>> >  _Director of Engineering_
>> >
>> >  Typesafe [2] - The software stack for applications that scale
>> >  Twitter: @viktorklang
>> >
>> > Links:
>> > ------
>> > [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> > [2] http://www.typesafe.com/
>>
>>


From davidcholmes at aapt.net.au  Tue Feb 12 00:53:00 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 12 Feb 2013 15:53:00 +1000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <4461c027da10d1bd444d236c2490ba54@nomagicsoftware.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEEKJKAA.davidcholmes@aapt.net.au>

Small arrays are handled quite differently in the VM to large arrays.

There is something happening here that you don't understand.

Is your full code available?

David

> -----Original Message-----
> From: thurston at nomagicsoftware.com [mailto:thurston at nomagicsoftware.com]
> Sent: Tuesday, 12 February 2013 3:45 PM
> To: dholmes at ieee.org
> Cc: viktor ?lang; concurrency-interest
> Subject: RE: [concurrency-interest] Some interesting (confusing?)
> benchmark results
> 
> 
> The #s are the ultimate arbiter, aren't they?  I can't imagine that the 
> 3-task solution copies the array more often than the 16K solution, since 
> each level of the DAG results in a duplicate of the original array (of 
> course in smaller and smaller chunks)
> 
> 16K:  ~385ms
> 3:    ~550ms
> 
> On 2013-02-11 21:26, David Holmes wrote:
> >> This is in line with my expectation; what amount of work-stealing 
> >> could
> >> happen with two tasks (each merge-sorting a 1M item array)?  With 
> >> 16K
> >> tasks, the work-stealing can really strut its stuff (and at least in 
> >> my
> >> case,can overwhelm the overhead associated with scheduling the 
> >> tasks)
> >
> > I think you are seeing affects that are not due to parallelisation of
> > the algorithm as such. 16K tasks for two threads is just ridiculous
> > task creation and management overhead. If you see a speed up compared
> > to three tasks then you are either not comparing apples to oranges, 
> > or
> > you are affected by other things - such as memory management when
> > dealing with large arrays.
> >
> > David
> > -----
> >
> >
> >> -----Original Message-----
> >> From: thurston at nomagicsoftware.com 
> >> [mailto:thurston at nomagicsoftware.com]
> >> Sent: Tuesday, 12 February 2013 3:14 PM
> >> To: viktor ?lang
> >> Cc: dholmes at ieee.org; concurrency-interest
> >> Subject: Re: [concurrency-interest] Some interesting (confusing?)
> >> benchmark results
> >>
> >>
> >>
> >>
> >> On 2013-02-11 14:24, ?iktor ?lang wrote:
> >> > On Mon, Feb 11, 2013 at 11:18 PM, David Holmes
> >> > <davidcholmes at aapt.net.au> wrote:
> >> >
> >> >> That doesn't make sense to me. If you had a threshold of 250 then
> >> >> you are
> >> >> creating thousands of tasks and that overhead should potentially
> >> >> bring your
> >> >> laptop to its knees. With 250K threshold you have far fewer 
> >> tasks.
> >> >
> >> > I'm not sure it makes sense to create more sort-tasks than there 
> >> is
> >> > Threads in the pool.
> >>
> >> Really, Viktor?  That surprises me coming from Akka.  Does that mean
> >> you guys only allocate as many message-processing "workers" as there 
> >> are
> >> hardware threads?
> >>
> >> To be clear, with the 2M size array data, and the threshold of 
> >> 250-size
> >> array for the (serial) insertion sort, 16K+ sort tasks are 
> >> constructed
> >> (as D. Holmes mentioned).  This is significantly *faster* than doing
> >> what either of you suggested.  If I simply split the problem in two, 
> >> and
> >> just run a serial mergesort on each half and then merge the two 
> >> halves
> >> (that's three total tasks), it's over 50% slower.
> >>
> >> Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.
> >>
> >> This is in line with my expectation; what amount of work-stealing 
> >> could
> >> happen with two tasks (each merge-sorting a 1M item array)?  With 
> >> 16K
> >> tasks, the work-stealing can really strut its stuff (and at least in 
> >> my
> >> case,can overwhelm the overhead associated with scheduling the 
> >> tasks)
> >>
> >> If you only create as many tasks as there are hardware threads -- 
> >> that
> >> seems a lot like a standard thread-pool implementation.  (Of course, 
> >> if
> >> there are many more cores, the opportunity for work-stealing would
> >> increase even with the 1-1 ratio).
> >>
> >> When I first used fork/join, that's the approach I took.
> >> 1.  split the problem (hardware threads * (1||2)
> >> 2.  have each task do its work serially (i.e. don't fork)
> >> 3.  combine the (sub)totals/results
> >>
> >> Not only is that less appealing from a programming point of view, it
> >> performs slower; that's the promise, nee even purpose, of the 
> >> fork/join
> >> framework at least when it comes to implementing naturally recursive
> >> algorithms.
> >>
> >> -T
> >>
> >>
> >> >
> >> >> That said I don't know what the memory requirements are for your
> >> >> algorithm.
> >> >> If you are allocating temporary arrays then a  large threshold 
> >> could
> >> >> well
> >> >> cause a problem.
> >> >>
> >> >> The parallel array sort going into JDK 8 is a merge sort but uses 
> >> a
> >> >> working
> >> >> array equal in size to the original array to be sorted.
> >> >
> >> > No in-place merge sort? ;-)
> >> >
> >> >
> >> >>> -----Original Message-----
> >> >> > From: concurrency-interest-bounces at cs.oswego.edu
> >> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> >> > thurston at nomagicsoftware.com
> >> >>
> >> >>> Sent: Tuesday, 12 February 2013 7:49 AM
> >> >> > To: concurrency-interest at cs.oswego.edu
> >> >> > Subject: Re: [concurrency-interest] Some interesting 
> >> (confusing?)
> >> >> > benchmark results
> >> >> >
> >> >> >
> >> >> > Well, it's the algorithm that's dispositive in this case.
> >> >> > Insertion-sort is empirically better than mergesort up to 
> >> around
> >> >> > 25-sized arrays.  That's where I started, and I played around 
> >> with
> >> >> > different thresholds and 250 seemed to be the best.
> >> >> > Anything larger than 1000 you start to see significant
> >> >> degradation.
> >> >> > I tried your formula (approximately) with N = 2 000 000 ==> 
> >> 250K
> >> >> >
> >> >> > That brought my laptop to its knees (as expected): over 33K ms 
> >> as
> >> >> > opposed to 390ms with 250.
> >> >> >
> >> >> > Following my own admonition (the algorithm matters), it occurs 
> >> to
> >> >> me
> >> >> > that I just use my own serial mergesort as comparison to the
> >> >> Forkjoin
> >> >> > (as opposed to Arrays.sort).  I'll let y'all know the results 
> >> of
> >> >> that
> >> >> >
> >> >> > -T
> >> >> >
> >> >> >
> >> >> >
> >> >> > On 2013-02-11 13:19, David Holmes wrote:
> >> >> > > Your sequential threshold is far too small for only two 
> >> worker
> >> >> > > threads. You
> >> >> > > would get near optimum speedup only splitting the problem 
> >> into
> >> >> two.
> >> >> > >
> >> >> > > The heuristic for sequential threshold is:
> >> >> > >
> >> >> > > Threshold = N / (LoadBalanceFactor * NumCore)
> >> >> > >
> >> >> > > where N is the "size" of the problem.
> >> >> > >
> >> >> > > The LoadBalanceFactor in an ideal world would be 1 but real
> >> >> tasks
> >> >> > > encounter
> >> >> > > delays and execute differently so a value > 1 is needed.
> >> >> > > Heuristically a
> >> >> > > value of 8 is a good starting value.
> >> >> > >
> >> >> > > David Holmes
> >> >> > >
> >> >> > >> -----Original Message-----
> >> >> > >> From: concurrency-interest-bounces at cs.oswego.edu
> >> >> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
> >> Of
> >> >> > >> thurston at nomagicsoftware.com
> >> >> > >> Sent: Tuesday, 12 February 2013 6:52 AM
> >> >> > >> To: concurrency-interest at cs.oswego.edu
> >> >> > >> Subject: [concurrency-interest] Some interesting 
> >> (confusing?)
> >> >> > >> benchmark
> >> >> > >> results
> >> >> > >>
> >> >> > >>
> >> >> > >> Hello,
> >> >> > >>
> >> >> > >> I made some initial attempts at using ForkJoin framework for
> >> >> some
> >> >> > >> rather obvious recursively parallel use-cases, namely 
> >> summing
> >> >> the
> >> >> > >> elements of an int[] and also sorting an int[] (randomly
> >> >> filled).
> >> >> > >>
> >> >> > >> The problem-sizes I used ranged from 1M - 10M.
> >> >> > >>
> >> >> > >> I really enjoy using the framework and find it relatively 
> >> easy
> >> >> to
> >> >> > >> reason about my programs (if not necessarily the internals 
> >> of
> >> >> the
> >> >> > >> framework).
> >> >> > >> But I was disappointed with the results: in both cases they
> >> >> were
> >> >> > >> slower
> >> >> > >> than the corresponding Java serial implementation.
> >> >> > >>
> >> >> > >> I completely understand the principle of YMMV, and I wasn't
> >> >> > >> expecting
> >> >> > >> 2x speedup, but especially in the case of the sort, but I 
> >> was
> >> >> > >> expecting
> >> >> > >> that ForkJoin would do at least a little better than the
> >> >> > >> single-threaded
> >> >> > >> version.  I guess what I'm asking is: are these results
> >> >> surprising?
> >> >> > >> Does it mean I'm doing something wrong?
> >> >> > >>
> >> >> > >> I'll give a brief description of my implementation:
> >> >> > >> single-threaded:  Arrays.sort(int[])
> >> >> > >>
> >> >> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 
> >> (arrived
> >> >> at
> >> >> > >> by
> >> >> > >> trial-and-error)
> >> >> > >>
> >> >> > >> int[] compute()
> >> >> > >> {
> >> >> > >> if int[].length < THRESHOLD
> >> >> > >>      return insertionSort(int[])
> >> >> > >> left = new MergeSort(int[] //split in half)
> >> >> > >> right = new MergeSort(int[] //other half)
> >> >> > >> return merge(right.compute(), left.join())
> >> >> > >> }
> >> >> > >>
> >> >> > >> The insertionSort() and merge() methods are just standard
> >> >> > >> implementations; there is a bit of apples to oranges 
> >> comparison
> >> >> > >> since
> >> >> > >> Arrays.sort() uses an optimized quicksort, but we're still
> >> >> talking
> >> >> > >> O(nlog(n))
> >> >> > >>
> >> >> > >> Just ran it on my laptop:
> >> >> > >> Windows 7 64-bit
> >> >> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> >> >> > >> Core 2 Duo==> 2 cores 2GHz
> >> >> > >>
> >> >> > >> 2M int[]:
> >> >> > >> single-threaded: ~330ms
> >> >> > >> ForkJoin (2 workers): ~390ms
> >> >> > >>
> >> >> > >> Would appreciate any feedback and hopefully the #s are 
> >> somewhat
> >> >> > >> helpful
> >> >> > >> (and please no scoffawing at my antiquated machine)
> >> >> > >>
> >> >> > >> -T
> >> >> > >>
> >> >> > >>
> >> >> > >> _______________________________________________
> >> >> > >> Concurrency-interest mailing list
> >> >> > >> Concurrency-interest at cs.oswego.edu
> >> >> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
> >> [1]
> >> >> > >>
> >> >> >
> >> >> > _______________________________________________
> >> >> > Concurrency-interest mailing list
> >> >> > Concurrency-interest at cs.oswego.edu
> >> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
> >> >> >
> >> >>
> >> >> _______________________________________________
> >> >> Concurrency-interest mailing list
> >> >> Concurrency-interest at cs.oswego.edu
> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
> >> >>
> >> >> --
> >> >
> >> >
> >> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:
> >> normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;
> >> text-transform:none;white-space:normal;word-spacing:0px;font-size:
> >> medium">VIKTOR
> >> > KLANG
> >> >  _Director of Engineering_
> >> >
> >> >  Typesafe [2] - The software stack for applications that scale
> >> >  Twitter: @viktorklang
> >> >
> >> > Links:
> >> > ------
> >> > [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >> > [2] http://www.typesafe.com/
> >>
> >>
> 
> 



From thurston at nomagicsoftware.com  Tue Feb 12 01:01:49 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 11 Feb 2013 22:01:49 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEEKJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEEKJKAA.davidcholmes@aapt.net.au>
Message-ID: <8d5d112861af4c066d52def2381868c8@nomagicsoftware.com>

Unfortunately, not at the moment.
I absolutely will do that in the next day or so.
I'd be interested in others running the code and comparing results -- 
some people will only be convinced when they run the code themselves 
(Hmmm, I'm like that)

On 2013-02-11 21:53, David Holmes wrote:
> Small arrays are handled quite differently in the VM to large arrays.
>
> There is something happening here that you don't understand.
>
> Is your full code available?
>
> David
>
>> -----Original Message-----
>> From: thurston at nomagicsoftware.com 
>> [mailto:thurston at nomagicsoftware.com]
>> Sent: Tuesday, 12 February 2013 3:45 PM
>> To: dholmes at ieee.org
>> Cc: viktor ?lang; concurrency-interest
>> Subject: RE: [concurrency-interest] Some interesting (confusing?)
>> benchmark results
>>
>>
>> The #s are the ultimate arbiter, aren't they?  I can't imagine that 
>> the
>> 3-task solution copies the array more often than the 16K solution, 
>> since
>> each level of the DAG results in a duplicate of the original array 
>> (of
>> course in smaller and smaller chunks)
>>
>> 16K:  ~385ms
>> 3:    ~550ms
>>
>> On 2013-02-11 21:26, David Holmes wrote:
>> >> This is in line with my expectation; what amount of work-stealing
>> >> could
>> >> happen with two tasks (each merge-sorting a 1M item array)?  With
>> >> 16K
>> >> tasks, the work-stealing can really strut its stuff (and at least 
>> in
>> >> my
>> >> case,can overwhelm the overhead associated with scheduling the
>> >> tasks)
>> >
>> > I think you are seeing affects that are not due to parallelisation 
>> of
>> > the algorithm as such. 16K tasks for two threads is just 
>> ridiculous
>> > task creation and management overhead. If you see a speed up 
>> compared
>> > to three tasks then you are either not comparing apples to 
>> oranges,
>> > or
>> > you are affected by other things - such as memory management when
>> > dealing with large arrays.
>> >
>> > David
>> > -----
>> >
>> >
>> >> -----Original Message-----
>> >> From: thurston at nomagicsoftware.com
>> >> [mailto:thurston at nomagicsoftware.com]
>> >> Sent: Tuesday, 12 February 2013 3:14 PM
>> >> To: viktor ?lang
>> >> Cc: dholmes at ieee.org; concurrency-interest
>> >> Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> >> benchmark results
>> >>
>> >>
>> >>
>> >>
>> >> On 2013-02-11 14:24, ?iktor ?lang wrote:
>> >> > On Mon, Feb 11, 2013 at 11:18 PM, David Holmes
>> >> > <davidcholmes at aapt.net.au> wrote:
>> >> >
>> >> >> That doesn't make sense to me. If you had a threshold of 250 
>> then
>> >> >> you are
>> >> >> creating thousands of tasks and that overhead should 
>> potentially
>> >> >> bring your
>> >> >> laptop to its knees. With 250K threshold you have far fewer
>> >> tasks.
>> >> >
>> >> > I'm not sure it makes sense to create more sort-tasks than 
>> there
>> >> is
>> >> > Threads in the pool.
>> >>
>> >> Really, Viktor?  That surprises me coming from Akka.  Does that 
>> mean
>> >> you guys only allocate as many message-processing "workers" as 
>> there
>> >> are
>> >> hardware threads?
>> >>
>> >> To be clear, with the 2M size array data, and the threshold of
>> >> 250-size
>> >> array for the (serial) insertion sort, 16K+ sort tasks are
>> >> constructed
>> >> (as D. Holmes mentioned).  This is significantly *faster* than 
>> doing
>> >> what either of you suggested.  If I simply split the problem in 
>> two,
>> >> and
>> >> just run a serial mergesort on each half and then merge the two
>> >> halves
>> >> (that's three total tasks), it's over 50% slower.
>> >>
>> >> Let me repeat that.  16K+ tasks is much, much faster than 3 
>> tasks.
>> >>
>> >> This is in line with my expectation; what amount of work-stealing
>> >> could
>> >> happen with two tasks (each merge-sorting a 1M item array)?  With
>> >> 16K
>> >> tasks, the work-stealing can really strut its stuff (and at least 
>> in
>> >> my
>> >> case,can overwhelm the overhead associated with scheduling the
>> >> tasks)
>> >>
>> >> If you only create as many tasks as there are hardware threads --
>> >> that
>> >> seems a lot like a standard thread-pool implementation.  (Of 
>> course,
>> >> if
>> >> there are many more cores, the opportunity for work-stealing 
>> would
>> >> increase even with the 1-1 ratio).
>> >>
>> >> When I first used fork/join, that's the approach I took.
>> >> 1.  split the problem (hardware threads * (1||2)
>> >> 2.  have each task do its work serially (i.e. don't fork)
>> >> 3.  combine the (sub)totals/results
>> >>
>> >> Not only is that less appealing from a programming point of view, 
>> it
>> >> performs slower; that's the promise, nee even purpose, of the
>> >> fork/join
>> >> framework at least when it comes to implementing naturally 
>> recursive
>> >> algorithms.
>> >>
>> >> -T
>> >>
>> >>
>> >> >
>> >> >> That said I don't know what the memory requirements are for 
>> your
>> >> >> algorithm.
>> >> >> If you are allocating temporary arrays then a  large threshold
>> >> could
>> >> >> well
>> >> >> cause a problem.
>> >> >>
>> >> >> The parallel array sort going into JDK 8 is a merge sort but 
>> uses
>> >> a
>> >> >> working
>> >> >> array equal in size to the original array to be sorted.
>> >> >
>> >> > No in-place merge sort? ;-)
>> >> >
>> >> >
>> >> >>> -----Original Message-----
>> >> >> > From: concurrency-interest-bounces at cs.oswego.edu
>> >> >> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
>> Of
>> >> >> > thurston at nomagicsoftware.com
>> >> >>
>> >> >>> Sent: Tuesday, 12 February 2013 7:49 AM
>> >> >> > To: concurrency-interest at cs.oswego.edu
>> >> >> > Subject: Re: [concurrency-interest] Some interesting
>> >> (confusing?)
>> >> >> > benchmark results
>> >> >> >
>> >> >> >
>> >> >> > Well, it's the algorithm that's dispositive in this case.
>> >> >> > Insertion-sort is empirically better than mergesort up to
>> >> around
>> >> >> > 25-sized arrays.  That's where I started, and I played 
>> around
>> >> with
>> >> >> > different thresholds and 250 seemed to be the best.
>> >> >> > Anything larger than 1000 you start to see significant
>> >> >> degradation.
>> >> >> > I tried your formula (approximately) with N = 2 000 000 ==>
>> >> 250K
>> >> >> >
>> >> >> > That brought my laptop to its knees (as expected): over 33K 
>> ms
>> >> as
>> >> >> > opposed to 390ms with 250.
>> >> >> >
>> >> >> > Following my own admonition (the algorithm matters), it 
>> occurs
>> >> to
>> >> >> me
>> >> >> > that I just use my own serial mergesort as comparison to the
>> >> >> Forkjoin
>> >> >> > (as opposed to Arrays.sort).  I'll let y'all know the 
>> results
>> >> of
>> >> >> that
>> >> >> >
>> >> >> > -T
>> >> >> >
>> >> >> >
>> >> >> >
>> >> >> > On 2013-02-11 13:19, David Holmes wrote:
>> >> >> > > Your sequential threshold is far too small for only two
>> >> worker
>> >> >> > > threads. You
>> >> >> > > would get near optimum speedup only splitting the problem
>> >> into
>> >> >> two.
>> >> >> > >
>> >> >> > > The heuristic for sequential threshold is:
>> >> >> > >
>> >> >> > > Threshold = N / (LoadBalanceFactor * NumCore)
>> >> >> > >
>> >> >> > > where N is the "size" of the problem.
>> >> >> > >
>> >> >> > > The LoadBalanceFactor in an ideal world would be 1 but 
>> real
>> >> >> tasks
>> >> >> > > encounter
>> >> >> > > delays and execute differently so a value > 1 is needed.
>> >> >> > > Heuristically a
>> >> >> > > value of 8 is a good starting value.
>> >> >> > >
>> >> >> > > David Holmes
>> >> >> > >
>> >> >> > >> -----Original Message-----
>> >> >> > >> From: concurrency-interest-bounces at cs.oswego.edu
>> >> >> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On 
>> Behalf
>> >> Of
>> >> >> > >> thurston at nomagicsoftware.com
>> >> >> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>> >> >> > >> To: concurrency-interest at cs.oswego.edu
>> >> >> > >> Subject: [concurrency-interest] Some interesting
>> >> (confusing?)
>> >> >> > >> benchmark
>> >> >> > >> results
>> >> >> > >>
>> >> >> > >>
>> >> >> > >> Hello,
>> >> >> > >>
>> >> >> > >> I made some initial attempts at using ForkJoin framework 
>> for
>> >> >> some
>> >> >> > >> rather obvious recursively parallel use-cases, namely
>> >> summing
>> >> >> the
>> >> >> > >> elements of an int[] and also sorting an int[] (randomly
>> >> >> filled).
>> >> >> > >>
>> >> >> > >> The problem-sizes I used ranged from 1M - 10M.
>> >> >> > >>
>> >> >> > >> I really enjoy using the framework and find it relatively
>> >> easy
>> >> >> to
>> >> >> > >> reason about my programs (if not necessarily the 
>> internals
>> >> of
>> >> >> the
>> >> >> > >> framework).
>> >> >> > >> But I was disappointed with the results: in both cases 
>> they
>> >> >> were
>> >> >> > >> slower
>> >> >> > >> than the corresponding Java serial implementation.
>> >> >> > >>
>> >> >> > >> I completely understand the principle of YMMV, and I 
>> wasn't
>> >> >> > >> expecting
>> >> >> > >> 2x speedup, but especially in the case of the sort, but I
>> >> was
>> >> >> > >> expecting
>> >> >> > >> that ForkJoin would do at least a little better than the
>> >> >> > >> single-threaded
>> >> >> > >> version.  I guess what I'm asking is: are these results
>> >> >> surprising?
>> >> >> > >> Does it mean I'm doing something wrong?
>> >> >> > >>
>> >> >> > >> I'll give a brief description of my implementation:
>> >> >> > >> single-threaded:  Arrays.sort(int[])
>> >> >> > >>
>> >> >> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250
>> >> (arrived
>> >> >> at
>> >> >> > >> by
>> >> >> > >> trial-and-error)
>> >> >> > >>
>> >> >> > >> int[] compute()
>> >> >> > >> {
>> >> >> > >> if int[].length < THRESHOLD
>> >> >> > >>      return insertionSort(int[])
>> >> >> > >> left = new MergeSort(int[] //split in half)
>> >> >> > >> right = new MergeSort(int[] //other half)
>> >> >> > >> return merge(right.compute(), left.join())
>> >> >> > >> }
>> >> >> > >>
>> >> >> > >> The insertionSort() and merge() methods are just standard
>> >> >> > >> implementations; there is a bit of apples to oranges
>> >> comparison
>> >> >> > >> since
>> >> >> > >> Arrays.sort() uses an optimized quicksort, but we're 
>> still
>> >> >> talking
>> >> >> > >> O(nlog(n))
>> >> >> > >>
>> >> >> > >> Just ran it on my laptop:
>> >> >> > >> Windows 7 64-bit
>> >> >> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> >> >> > >> Core 2 Duo==> 2 cores 2GHz
>> >> >> > >>
>> >> >> > >> 2M int[]:
>> >> >> > >> single-threaded: ~330ms
>> >> >> > >> ForkJoin (2 workers): ~390ms
>> >> >> > >>
>> >> >> > >> Would appreciate any feedback and hopefully the #s are
>> >> somewhat
>> >> >> > >> helpful
>> >> >> > >> (and please no scoffawing at my antiquated machine)
>> >> >> > >>
>> >> >> > >> -T
>> >> >> > >>
>> >> >> > >>
>> >> >> > >> _______________________________________________
>> >> >> > >> Concurrency-interest mailing list
>> >> >> > >> Concurrency-interest at cs.oswego.edu
>> >> >> > >> 
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >> [1]
>> >> >> > >>
>> >> >> >
>> >> >> > _______________________________________________
>> >> >> > Concurrency-interest mailing list
>> >> >> > Concurrency-interest at cs.oswego.edu
>> >> >> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>> [1]
>> >> >> >
>> >> >>
>> >> >> _______________________________________________
>> >> >> Concurrency-interest mailing list
>> >> >> Concurrency-interest at cs.oswego.edu
>> >> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> >> >>
>> >> >> --
>> >> >
>> >> >
>> >> 
>> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:
>> >> 
>> normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;
>> >> 
>> text-transform:none;white-space:normal;word-spacing:0px;font-size:
>> >> medium">VIKTOR
>> >> > KLANG
>> >> >  _Director of Engineering_
>> >> >
>> >> >  Typesafe [2] - The software stack for applications that scale
>> >> >  Twitter: @viktorklang
>> >> >
>> >> > Links:
>> >> > ------
>> >> > [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >> > [2] http://www.typesafe.com/
>> >>
>> >>
>>
>>


From viktor.klang at gmail.com  Tue Feb 12 02:47:32 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 12 Feb 2013 08:47:32 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
Message-ID: <CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>

On Tue, Feb 12, 2013 at 6:13 AM, <thurston at nomagicsoftware.com> wrote:

>
>
> On 2013-02-11 14:24, ?iktor ?lang wrote:
>
>> On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>  That doesn't make sense to me. If you had a threshold of 250 then you are
>>> creating thousands of tasks and that overhead should potentially bring
>>> your
>>> laptop to its knees. With 250K threshold you have far fewer tasks.
>>>
>>
>> I'm not sure it makes sense to create more sort-tasks than there is
>> Threads in the pool.
>>
>
> Really, Viktor?  That surprises me coming from Akka.


I have found that when I am surprised by someone, I get a much better
understanding by asking for clarification before I assume too much.


>  Does that mean you guys only allocate as many message-processing
> "workers" as there are hardware threads?


What's you definition of "worker"?

Consider this:

Do you agree that thread pool sizing depends on type of work? (IO bound vs
CPU bound, bursty vs steady etc etc)
Do you agree that a JVM Thread is not a unit of parallelism?
Do you agree that having more JVM Threads than hardware threads is bad for
CPU-bound workloads?


>
> To be clear, with the 2M size array data, and the threshold of 250-size
> array for the (serial) insertion sort, 16K+ sort tasks are constructed (as
> D. Holmes mentioned).  This is significantly *faster* than doing what
> either of you suggested.  If I simply split the problem in two, and just
> run a serial mergesort on each half and then merge the two halves (that's
> three total tasks), it's over 50% slower.
>
> Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.
>
> This is in line with my expectation; what amount of work-stealing could
> happen with two tasks (each merge-sorting a 1M item array)?  With 16K
> tasks, the work-stealing can really strut its stuff (and at least in my
> case,can overwhelm the overhead associated with scheduling the tasks)
>
> If you only create as many tasks as there are hardware threads -- that
> seems a lot like a standard thread-pool implementation.  (Of course, if
> there are many more cores, the opportunity for work-stealing would increase
> even with the 1-1 ratio).
>
> When I first used fork/join, that's the approach I took.
> 1.  split the problem (hardware threads * (1||2)
> 2.  have each task do its work serially (i.e. don't fork)
> 3.  combine the (sub)totals/results
>
> Not only is that less appealing from a programming point of view, it
> performs slower; that's the promise, nee even purpose, of the fork/join
> framework at least when it comes to implementing naturally recursive
> algorithms.
>

What I meant was to create as many tasks as workers, and only process a
chunk per task, and at the end of each task forking it. (See it as
continuation passing style over a pool of workers). Which essentially means
that there won't be more tasks than there are workers.

I hope I cleared things up.

Cheers,
?


>
> -T
>
>
>
>>  That said I don't know what the memory requirements are for your
>>> algorithm.
>>> If you are allocating temporary arrays then a  large threshold could well
>>> cause a problem.
>>>
>>> The parallel array sort going into JDK 8 is a merge sort but uses a
>>> working
>>> array equal in size to the original array to be sorted.
>>>
>>
>> No in-place merge sort? ;-)
>>
>>
>>  -----Original Message-----
>>>>
>>> > From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>> > [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>> Behalf Of
>>> > thurston at nomagicsoftware.com
>>>
>>>  Sent: Tuesday, 12 February 2013 7:49 AM
>>>>
>>> > To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
>>> > benchmark results
>>> >
>>> >
>>> > Well, it's the algorithm that's dispositive in this case.
>>> > Insertion-sort is empirically better than mergesort up to around
>>> > 25-sized arrays.  That's where I started, and I played around with
>>> > different thresholds and 250 seemed to be the best.
>>> > Anything larger than 1000 you start to see significant degradation.
>>> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>> >
>>> > That brought my laptop to its knees (as expected): over 33K ms as
>>> > opposed to 390ms with 250.
>>> >
>>> > Following my own admonition (the algorithm matters), it occurs to me
>>> > that I just use my own serial mergesort as comparison to the Forkjoin
>>> > (as opposed to Arrays.sort).  I'll let y'all know the results of that
>>> >
>>> > -T
>>> >
>>> >
>>> >
>>> > On 2013-02-11 13:19, David Holmes wrote:
>>> > > Your sequential threshold is far too small for only two worker
>>> > > threads. You
>>> > > would get near optimum speedup only splitting the problem into two.
>>> > >
>>> > > The heuristic for sequential threshold is:
>>> > >
>>> > > Threshold = N / (LoadBalanceFactor * NumCore)
>>> > >
>>> > > where N is the "size" of the problem.
>>> > >
>>> > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
>>> > > encounter
>>> > > delays and execute differently so a value > 1 is needed.
>>> > > Heuristically a
>>> > > value of 8 is a good starting value.
>>> > >
>>> > > David Holmes
>>> > >
>>> > >> -----Original Message-----
>>> > >> From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>> > >> [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>> Behalf Of
>>> > >> thurston at nomagicsoftware.com
>>> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>>> > >> To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>> > >> Subject: [concurrency-interest] Some interesting (confusing?)
>>> > >> benchmark
>>> > >> results
>>> > >>
>>> > >>
>>> > >> Hello,
>>> > >>
>>> > >> I made some initial attempts at using ForkJoin framework for some
>>> > >> rather obvious recursively parallel use-cases, namely summing the
>>> > >> elements of an int[] and also sorting an int[] (randomly filled).
>>> > >>
>>> > >> The problem-sizes I used ranged from 1M - 10M.
>>> > >>
>>> > >> I really enjoy using the framework and find it relatively easy to
>>> > >> reason about my programs (if not necessarily the internals of the
>>> > >> framework).
>>> > >> But I was disappointed with the results: in both cases they were
>>> > >> slower
>>> > >> than the corresponding Java serial implementation.
>>> > >>
>>> > >> I completely understand the principle of YMMV, and I wasn't
>>> > >> expecting
>>> > >> 2x speedup, but especially in the case of the sort, but I was
>>> > >> expecting
>>> > >> that ForkJoin would do at least a little better than the
>>> > >> single-threaded
>>> > >> version.  I guess what I'm asking is: are these results surprising?
>>> > >> Does it mean I'm doing something wrong?
>>> > >>
>>> > >> I'll give a brief description of my implementation:
>>> > >> single-threaded:  Arrays.sort(int[])
>>> > >>
>>> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
>>> > >> by
>>> > >> trial-and-error)
>>> > >>
>>> > >> int[] compute()
>>> > >> {
>>> > >> if int[].length < THRESHOLD
>>> > >>      return insertionSort(int[])
>>> > >> left = new MergeSort(int[] //split in half)
>>> > >> right = new MergeSort(int[] //other half)
>>> > >> return merge(right.compute(), left.join())
>>> > >> }
>>> > >>
>>> > >> The insertionSort() and merge() methods are just standard
>>> > >> implementations; there is a bit of apples to oranges comparison
>>> > >> since
>>> > >> Arrays.sort() uses an optimized quicksort, but we're still talking
>>> > >> O(nlog(n))
>>> > >>
>>> > >> Just ran it on my laptop:
>>> > >> Windows 7 64-bit
>>> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>> > >> Core 2 Duo==> 2 cores 2GHz
>>> > >>
>>> > >> 2M int[]:
>>> > >> single-threaded: ~330ms
>>> > >> ForkJoin (2 workers): ~390ms
>>> > >>
>>> > >> Would appreciate any feedback and hopefully the #s are somewhat
>>> > >> helpful
>>> > >> (and please no scoffawing at my antiquated machine)
>>> > >>
>>> > >> -T
>>> > >>
>>> > >>
>>> > >> ______________________________**_________________
>>> > >> Concurrency-interest mailing list
>>> > >> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> > >> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1]
>>>
>>> > >>
>>> >
>>> > ______________________________**_________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> > http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1]
>>>
>>> >
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1]
>>>
>>> --
>>>
>>
>> r:rgb(0,0,0);font-family:**Times;font-variant:normal;**
>> letter-spacing:normal;line-**height:normal;text-align:-**
>> webkit-auto;text-indent:0px;**text-transform:none;white-**
>> space:normal;word-spacing:0px;**font-size:medium">VIKTOR
>> KLANG
>>  _Director of Engineering_
>>
>>  Typesafe [2] - The software stack for applications that scale
>>  Twitter: @viktorklang
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> [2] http://www.typesafe.com/
>>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/dffb3582/attachment.html>

From thurston at nomagicsoftware.com  Tue Feb 12 04:40:04 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Tue, 12 Feb 2013 01:40:04 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
Message-ID: <dc180a8e0637b4fc05d464e34a597e02@nomagicsoftware.com>



On 2013-02-11 23:47, ?iktor ?lang wrote:
> On Tue, Feb 12, 2013 at 6:13 AM, <thurston at nomagicsoftware.com> 
> wrote:
>
>> On 2013-02-11 14:24, ?iktor ?lang wrote:
>>
>>> On Mon, Feb 11, 2013 at 11:18 PM, David Holmes 
>>> <davidcholmes at aapt.net.au> wrote:
>>>
>>>> That doesn't make sense to me. If you had a threshold of 250 then 
>>>> you are
>>>> creating thousands of tasks and that overhead should potentially 
>>>> bring your
>>>> laptop to its knees. With 250K threshold you have far fewer tasks.
>>>
>>> I'm not sure it makes sense to create more sort-tasks than there is
>>> Threads in the pool.
>>
>> Really, Viktor? ?That surprises me coming from Akka.
>
> I have found that when I am surprised by someone, I get a much better
> understanding by asking for clarification before I assume too much.
>
> ?
>
>> ?Does that mean you guys only allocate as many message-processing 
>> "workers" as there are hardware threads??
>
> What's you definition of "worker"?
Worker in above sense means the # of Recursive(Action/Task) *instances* 
(in fork/join terms), which isn't probably applicable for message 
passing/mailbox processing.
I guess I mean how many dedicated threads are there that can be 
actively processing Actors' messages (i.e. not waiting at the receive if 
that makes sense)

> Consider this:
>
> Do you agree that thread pool sizing depends on type of work? (IO
> bound vs CPU bound, bursty vs steady etc etc)
Well, yes; i.e. optimal thread-pool sizing (and of course it depends on 
a lot more than that)

> Do you agree that a JVM Thread is not a unit of parallelism?
Not sure what you mean.

> Do you agree that having more JVM Threads than hardware threads is
> bad for CPU-bound workloads?

Not necessarily. What is true is that you want as few threads as are 
necessary to keep all the CPUs at (or near) 100%.  In an ideal case, 
that would be == hardware threads (context switches are expensive), but 
OS thread scheduling is complex and opaque (I'm not pretending to be an 
expert). I don't know what the magic formula is--you have to measure. 
but it seems plausible that an extra buffer of software threads > 
hardware threads could perform better even in CPU-bound workload (does 
reading/writing to memory count as I/O? I guess technically it does; 
hard to think of a meaningful workload that wouldn't entail memory 
access)

> ?
>  To be clear, with the 2M size array data, and the threshold of
> 250-size array for the (serial) insertion sort, 16K+ sort tasks are
> constructed (as D. Holmes mentioned). ?This is significantly *faster*
> than doing what either of you suggested. ?If I simply split the
> problem in two, and just run a serial mergesort on each half and then
> merge the two halves (that's three total tasks), it
>
>> t of work-stealing could happen with two tasks (each merge-sorting a 
>> 1M item array)? ?With 16K tasks, the work-stealing can really strut 
>> its stuff (and at least in my case,can overwhelm the overhead 
>> associated with scheduling the tasks)
>>
>> If you only create as many tasks as there are hardware threads -- 
>> that seems a lot like a standard thread-pool implementation. ?(Of 
>> course, if there are many more cores, the opportunity for 
>> work-stealing would increase even with the 1-1 ratio).
>>
>> When I first used fork/join, that's the approach I took.
>> 1. ?split the problem (hardware threads * (1||2)
>> 2. ?have each task do its work serially (i.e. don't fork)
>> 3. ?combine the (sub)totals/results
>>
>> Not only is that less appealing from a programming point of view, it 
>> performs slower; that's the promise, nee even purpose, of the 
>> fork/join framework at least when it comes to implementing naturally 
>> recursive algorithms.
>>
>> What I meant was to create as many tasks as workers, and only 
>> process a chunk per task, and at the end of each task forking it. (See 
>> it as continuation passing style over a pool of workers). Which 
>> essentially means that there won't be more tasks than there are 
>> workers.

Now, I'm confused what you mean by tasks vs workers.  I understood you 
to write that # fork/join recursive task instances should == # of 
hardware threads (i.e. java threads in forkjoinpool). So in the parallel 
mergesort example, on my laptop that would mean 2 (actually 3 for the 
initial task), task *instances*; the way to think about it, is how many 
total nodes in the DAG (or tree) at its deepest == # of tasks.  At least 
in theory all of those 'nodes' could be active (still on a workqueue) at 
one time.
And in theory, two really should be enough, but that entails each 
'node' doing a bigger chunk since the total work (overhead aside) is the 
same.

It sure looks like (empirically) that 16K 'chunks' are processed faster 
(in total), than 3 'chunks'.  Why?  Clearly the 16K have more 
'overhead'.  But for some reason, they are utilizing more of the CPUs' 
power for a larger % of time; that is the only logical conclusion that I 
can draw


>>
>> I hope I cleared things up.
>>
>> Cheers,
>> ?
>> ?
> ,204);border-left-style:solid;padding-left:1ex">
>  -T
>
>  That said I don't know what the memory requirements are for your 
> algorithm.
>  If you are allocating temporary arrays then a ?large threshold could 
> well
>  cause a problem.
>
>> ote>
>> No in-place merge sort? ;-)
>> ?
>>
>>>> -----Original Message-----
>>> > From: concurrency-interest-bounces at cs.oswego.edu
>>> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>> > thurston at nomagicsoftware.com
>>>
>>>> Sent: Tuesday, 12 February 2013 7:49 AM
>>> > To: concurrency-interest at cs.oswego.edu
>>> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
>>> > benchmark results
>>> >
>>> >
>>> > Well, it's the algorithm that's dispositive in this case.
>>> > Insertion-sort is empirically better than mergesort up to around
>>> > 25-sized arrays. ?That's where I started, and I played around 
>>> with
>>> > different thresholds and 250 seemed to be the best.
>>> > Anything larger than 1000 you start to see significant 
>>> degradation.
>>> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>> >
>>> > That brought my laptop to its knees (as expected): over 33K ms as
>>> > opposed to 390ms with 250.
>>> >
>>> > Following my own admonition (the algorithm matters), it occurs to 
>>> me
>>> > that I just use my own serial mergesort as comparison to the 
>>> Forkjoin
>>> > (as opposed to Arrays.sort). ?I'll let y'all know the results of 
>>> that
>>> >
>>> > -T
>>> >
>>> >
>>> >
>>> > On 2013-02-11 13:19, David Holmes wrote:
>>> > > Your sequential threshold is far too small for only two worker
>>> > > threads. You
>>> > > would get near optimum speedup only splitting the problem into 
>>> two.
>>> > >
>>> > > The heuristic for sequential threshold is:
>>> > >
>>> > > Threshold = N / (LoadBalanceFactor * NumCore)
>>> > >
>>> > > where N is the "size" of the problem.
>>> > >
>>> > > The LoadBalanceFactor in an ideal world would be 1 but real 
>>> tasks
>>> > > encounter
>>> > > delays and execute differently so a value > 1 is needed.
>>> > > Heuristically a
>>> > > value of 8 is a good starting value.
>>> > >
>>> > > David Holmes
>>> > >
>>> > >> -----Original Message-----
>>> > >> From: concurrency-interest-bounces at cs.oswego.edu
>>> > >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf 
>>> Of
>>> > >> thurston at nomagicsoftware.com
>>> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>>> > >> To: concurrency-interest at cs.oswego.edu
>>> > >> Subject: [concurrency-interest] Some interesting (confusing?)
>>> > >> benchmark
>>> > >> results
>>> > >>
>>> > >>
>>> > >> Hello,
>>> > >>
>>> > >> I made some initial attempts at using ForkJoin framework for 
>>> some
>>> > >> rather obvious recursively parallel use-cases, namely summing 
>>> the
>>> > >> elements of an int[] and also sorting an int[] (randomly 
>>> filled).
>>> > >>
>>> > >> The problem-sizes I used ranged from 1M - 10M.
>>> > >>
>>> > >> I really enjoy using the framework and find it relatively easy 
>>> to
>>> > >> reason about my programs (if not necessarily the internals of 
>>> the
>>> > >> framework).
>>> > >> But I was disappointed with the results: in both cases they 
>>> were
>>> > >> slower
>>> > >> than the corresponding Java serial implementation.
>>> > >>
>>> > >> I completely understand the principle of YMMV, and I wasn't
>>> > >> expecting
>>> > >> 2x speedup, but especially in the case of the sort, but I was
>>> > >> expecting
>>> > >> that ForkJoin would do at least a little better than the
>>> > >> single-threaded
>>> > >> version. ?I guess what I'm asking is: are these results 
>>> surprising?
>>> > >> Does it mean I'm doing something wrong?
>>> > >>
>>> > >> I'll give a brief description of my implementation:
>>> > >> single-threaded: ?Arrays.sort(int[])
>>> > >>
>>> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 
>>> (arrived at
>>> > >> by
>>> > >> trial-and-error)
>>> > >>
>>> > >> int[] compute()
>>> > >> {
>>> > >> if int[].length < THRESHOLD
>>> > >> ? ? ?return insertionSort(int[])
>>> > >> left = new MergeSort(int[] //split in half)
>>> > >> right = new MergeSort(int[] //other half)
>>> > >> return merge(right.compute(), left.join())
>>> > >> }
>>> > >>
>>> > >> The insertionSort() and merge() methods are just standard
>>> > >> implementations; there is a bit of apples to oranges 
>>> comparison
>>> > >> since
>>> > >> Arrays.sort() uses an optimized quicksort, but we're still 
>>> talking
>>> > >> O(nlog(n))
>>> > >>
>>> > >> Just ran it on my laptop:
>>> > >> Windows 7 64-bit
>>> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>> > >> Core 2 Duo==> 2 cores 2GHz
>>> > >>
>>> > >> 2M int[]:
>>> > >> single-threaded: ~330ms
>>> > >> ForkJoin (2 workers): ~390ms
>>> > >>
>>> > >> Would appreciate any feedback and hopefully the #s are 
>>> somewhat
>>> > >> helpful
>>> > >> (and please no scoffawing at my antiquated machine)
>>> > >>
>>> > >> -T
>>> > >>
>>> > >>
>>> > >> _______________________________________________
>>> > >> Concurrency-interest mailing list
>>> > >> Concurrency-interest at cs.oswego.edu
>>> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1] 
>>> [1]
>>>
>>> > >>
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1] 
>>> [1]
>>>
>>> >
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1] [1]
>>>
>>> --
>>
>> 
>> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;font-size:medium">VIKTOR
>> KLANG
>> ?_Director of Engineering_
>>
>> ?Typesafe [2]?- The software stack for applications that scale
>> ?Twitter: @viktorklang
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>> [2] http://www.typesafe.com/ [2]
>>
>> --
>>
>> VIKTOR KLANG
>> _Director of Engineering_
> 
> t-align:-webkit-auto;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;font-size:medium">
>  Typesafe [2]?- The software stack for applications that scale
>  Twitter: @viktorklang
>
> Links:
> ------
> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> [2] http://www.typesafe.com/


From kirk at kodewerk.com  Tue Feb 12 04:50:10 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 12 Feb 2013 10:50:10 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
Message-ID: <24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>

Hi,
On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
Hi T,

Can you pub the raw data from your runs? Average tends to hide effects and it's difficult to say anything with only that measure.

Regards,
Kirk


> Hello,
> 
> I made some initial attempts at using ForkJoin framework for some rather obvious recursively parallel use-cases, namely summing the elements of an int[] and also sorting an int[] (randomly filled).
> 
> The problem-sizes I used ranged from 1M - 10M.
> 
> I really enjoy using the framework and find it relatively easy to reason about my programs (if not necessarily the internals of the framework).
> But I was disappointed with the results: in both cases they were slower than the corresponding Java serial implementation.
> 
> I completely understand the principle of YMMV, and I wasn't expecting 2x speedup, but especially in the case of the sort, but I was expecting that ForkJoin would do at least a little better than the single-threaded version.  I guess what I'm asking is: are these results surprising?  Does it mean I'm doing something wrong?
> 
> I'll give a brief description of my implementation:
> single-threaded:  Arrays.sort(int[])
> 
> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by trial-and-error)
> 
> int[] compute()
> {
> if int[].length < THRESHOLD
>    return insertionSort(int[])
> left = new MergeSort(int[] //split in half)
> right = new MergeSort(int[] //other half)
> return merge(right.compute(), left.join())
> }
> 
> The insertionSort() and merge() methods are just standard implementations; there is a bit of apples to oranges comparison since Arrays.sort() uses an optimized quicksort, but we're still talking O(nlog(n))
> 
> Just ran it on my laptop:
> Windows 7 64-bit
> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
> Core 2 Duo==> 2 cores 2GHz
> 
> 2M int[]:
> single-threaded: ~330ms
> ForkJoin (2 workers): ~390ms
> 
> Would appreciate any feedback and hopefully the #s are somewhat helpful (and please no scoffawing at my antiquated machine)
> 
> -T
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From thurston at nomagicsoftware.com  Tue Feb 12 04:55:09 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Tue, 12 Feb 2013 01:55:09 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
Message-ID: <c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>

Yes, I will push the code and my measurements some time this week.  I'm 
really interested in seeing what others' results would be.  What I can 
say is that the timings are surprisingly stable (range 30-40ms) and the 
input data is randomly generated for each test run which can certainly 
affect merge(int[], int[])'s performance

On 2013-02-12 01:50, Kirk Pepperdine wrote:
> Hi,
> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
> Hi T,
>
> Can you pub the raw data from your runs? Average tends to hide
> effects and it's difficult to say anything with only that measure.
>
> Regards,
> Kirk
>
>
>> Hello,
>>
>> I made some initial attempts at using ForkJoin framework for some 
>> rather obvious recursively parallel use-cases, namely summing the 
>> elements of an int[] and also sorting an int[] (randomly filled).
>>
>> The problem-sizes I used ranged from 1M - 10M.
>>
>> I really enjoy using the framework and find it relatively easy to 
>> reason about my programs (if not necessarily the internals of the 
>> framework).
>> But I was disappointed with the results: in both cases they were 
>> slower than the corresponding Java serial implementation.
>>
>> I completely understand the principle of YMMV, and I wasn't 
>> expecting 2x speedup, but especially in the case of the sort, but I 
>> was expecting that ForkJoin would do at least a little better than the 
>> single-threaded version.  I guess what I'm asking is: are these 
>> results surprising?  Does it mean I'm doing something wrong?
>>
>> I'll give a brief description of my implementation:
>> single-threaded:  Arrays.sort(int[])
>>
>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at 
>> by trial-and-error)
>>
>> int[] compute()
>> {
>> if int[].length < THRESHOLD
>>    return insertionSort(int[])
>> left = new MergeSort(int[] //split in half)
>> right = new MergeSort(int[] //other half)
>> return merge(right.compute(), left.join())
>> }
>>
>> The insertionSort() and merge() methods are just standard 
>> implementations; there is a bit of apples to oranges comparison since 
>> Arrays.sort() uses an optimized quicksort, but we're still talking 
>> O(nlog(n))
>>
>> Just ran it on my laptop:
>> Windows 7 64-bit
>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>> Core 2 Duo==> 2 cores 2GHz
>>
>> 2M int[]:
>> single-threaded: ~330ms
>> ForkJoin (2 workers): ~390ms
>>
>> Would appreciate any feedback and hopefully the #s are somewhat 
>> helpful (and please no scoffawing at my antiquated machine)
>>
>> -T
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Tue Feb 12 05:09:16 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 12 Feb 2013 11:09:16 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <dc180a8e0637b4fc05d464e34a597e02@nomagicsoftware.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<dc180a8e0637b4fc05d464e34a597e02@nomagicsoftware.com>
Message-ID: <CANPzfU_3Krr=CWv_QNKUiM8Exd=TT4WbzHKEhDZ+NynYCycj_Q@mail.gmail.com>

On Tue, Feb 12, 2013 at 10:40 AM, <thurston at nomagicsoftware.com> wrote:

>
>
> On 2013-02-11 23:47, ?iktor ?lang wrote:
>
>> On Tue, Feb 12, 2013 at 6:13 AM, <thurston at nomagicsoftware.com> wrote:
>>
>>  On 2013-02-11 14:24, ?iktor ?lang wrote:
>>>
>>>  On Mon, Feb 11, 2013 at 11:18 PM, David Holmes <
>>>> davidcholmes at aapt.net.au> wrote:
>>>>
>>>>  That doesn't make sense to me. If you had a threshold of 250 then you
>>>>> are
>>>>> creating thousands of tasks and that overhead should potentially bring
>>>>> your
>>>>> laptop to its knees. With 250K threshold you have far fewer tasks.
>>>>>
>>>>
>>>> I'm not sure it makes sense to create more sort-tasks than there is
>>>> Threads in the pool.
>>>>
>>>
>>> Really, Viktor?  That surprises me coming from Akka.
>>>
>>
>> I have found that when I am surprised by someone, I get a much better
>> understanding by asking for clarification before I assume too much.
>>
>>
>>
>>   Does that mean you guys only allocate as many message-processing
>>> "workers" as there are hardware threads?
>>>
>>
>> What's you definition of "worker"?
>>
> Worker in above sense means the # of Recursive(Action/Task) *instances*
> (in fork/join terms), which isn't probably applicable for message
> passing/mailbox processing.
>

Ok, for me the worker is the ForkJoinWorkerThread. The Task is just how to
do the work.


> I guess I mean how many dedicated threads are there that can be actively
> processing Actors' messages (i.e. not waiting at the receive if that makes
> sense)


We only schedule one task for an actor, no matter how many messages it has,
and then submit a new task at the end of each task (if there is more work
to be done, you can see it as continuation passing style over a thread
pool).


>
>
>  Consider this:
>>
>> Do you agree that thread pool sizing depends on type of work? (IO
>> bound vs CPU bound, bursty vs steady etc etc)
>>
> Well, yes; i.e. optimal thread-pool sizing (and of course it depends on a
> lot more than that)


>
>  Do you agree that a JVM Thread is not a unit of parallelism?
>>
> Not sure what you mean.


A JVM Thread is a unit of concurrency and the only unit of execution
available.


>
>
>  Do you agree that having more JVM Threads than hardware threads is
>> bad for CPU-bound workloads?
>>
>
> Not necessarily. What is true is that you want as few threads as are
> necessary to keep all the CPUs at (or near) 100%.  In an ideal case, that
> would be == hardware threads (context switches are expensive),



> but OS thread scheduling is complex and opaque (I'm not pretending to be
> an expert). I don't know what the magic formula is--you have to measure.


Actually, according to our heuristics the optimal setting (for us & for
memory-bound work) tends to be 0.6-0.7 x the number of available hardware
threads. (long topic that should probably discussed in a dedicated thread).


> but it seems plausible that an extra buffer of software threads > hardware
> threads could perform better even in CPU-bound workload


Have you ever observed that?


> (does reading/writing to memory count as I/O? I guess technically it does;
> hard to think of a meaningful workload that wouldn't entail memory access)
>

Technically all reads and writes are IO.


>
>
>>  To be clear, with the 2M size array data, and the threshold of
>> 250-size array for the (serial) insertion sort, 16K+ sort tasks are
>> constructed (as D. Holmes mentioned).  This is significantly *faster*
>> than doing what either of you suggested.  If I simply split the
>> problem in two, and just run a serial mergesort on each half and then
>> merge the two halves (that's three total tasks), it
>>
>>  t of work-stealing could happen with two tasks (each merge-sorting a 1M
>>> item array)?  With 16K tasks, the work-stealing can really strut its stuff
>>> (and at least in my case,can overwhelm the overhead associated with
>>> scheduling the tasks)
>>>
>>> If you only create as many tasks as there are hardware threads -- that
>>> seems a lot like a standard thread-pool implementation.  (Of course, if
>>> there are many more cores, the opportunity for work-stealing would increase
>>> even with the 1-1 ratio).
>>>
>>> When I first used fork/join, that's the approach I took.
>>> 1.  split the problem (hardware threads * (1||2)
>>> 2.  have each task do its work serially (i.e. don't fork)
>>> 3.  combine the (sub)totals/results
>>>
>>> Not only is that less appealing from a programming point of view, it
>>> performs slower; that's the promise, nee even purpose, of the fork/join
>>> framework at least when it comes to implementing naturally recursive
>>> algorithms.
>>>
>>> What I meant was to create as many tasks as workers, and only process a
>>> chunk per task, and at the end of each task forking it. (See it as
>>> continuation passing style over a pool of workers). Which essentially means
>>> that there won't be more tasks than there are workers.
>>>
>>
> Now, I'm confused what you mean by tasks vs workers.


worker == ForkJoinWorkerThread
task == ForkJoinTask


>  I understood you to write that # fork/join recursive task instances
> should == # of hardware threads (i.e. java threads in forkjoinpool). So in
> the parallel mergesort example, on my laptop that would mean 2 (actually 3
> for the initial task), task *instances*; the way to think about it, is how
> many total nodes in the DAG (or tree) at its deepest == # of tasks.  At
> least in theory all of those 'nodes' could be active (still on a workqueue)
> at one time.
> And in theory, two really should be enough, but that entails each 'node'
> doing a bigger chunk since the total work (overhead aside) is the same.


> It sure looks like (empirically) that 16K 'chunks' are processed faster
> (in total), than 3 'chunks'.  Why?  Clearly the 16K have more 'overhead'.
>  But for some reason, they are utilizing more of the CPUs' power for a
> larger % of time; that is the only logical conclusion that I can draw
>

Another interesting solution is to spawn as many tasks as workers and at
the beginning of executing every task, you fork a new task. The workload
per task is still a smaller chunk of the array itself. The effects might
just be that there is enough tasks to make sure that workers that finish
early has something to do (either take from local queue or steal) And you
don't need to create a ton of tasks up front.

Cheers,
?


>
>
>
>>> I hope I cleared things up.
>>>
>>> Cheers,
>>> ?
>>>
>>>
>> ,204);border-left-style:solid;**padding-left:1ex">
>>
>>  -T
>>
>>  That said I don't know what the memory requirements are for your
>> algorithm.
>>  If you are allocating temporary arrays then a  large threshold could well
>>  cause a problem.
>>
>>  ote>
>>>
>>> No in-place merge sort? ;-)
>>>
>>>
>>>  -----Original Message-----
>>>>>
>>>> > From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>>> > [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>>> Behalf Of
>>>> > thurston at nomagicsoftware.com
>>>>
>>>>  Sent: Tuesday, 12 February 2013 7:49 AM
>>>>>
>>>> > To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>>> > Subject: Re: [concurrency-interest] Some interesting (confusing?)
>>>> > benchmark results
>>>> >
>>>> >
>>>> > Well, it's the algorithm that's dispositive in this case.
>>>> > Insertion-sort is empirically better than mergesort up to around
>>>> > 25-sized arrays.  That's where I started, and I played around with
>>>> > different thresholds and 250 seemed to be the best.
>>>> > Anything larger than 1000 you start to see significant degradation.
>>>> > I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>>> >
>>>> > That brought my laptop to its knees (as expected): over 33K ms as
>>>> > opposed to 390ms with 250.
>>>> >
>>>> > Following my own admonition (the algorithm matters), it occurs to me
>>>> > that I just use my own serial mergesort as comparison to the Forkjoin
>>>> > (as opposed to Arrays.sort).  I'll let y'all know the results of that
>>>> >
>>>> > -T
>>>> >
>>>> >
>>>> >
>>>> > On 2013-02-11 13:19, David Holmes wrote:
>>>> > > Your sequential threshold is far too small for only two worker
>>>> > > threads. You
>>>> > > would get near optimum speedup only splitting the problem into two.
>>>> > >
>>>> > > The heuristic for sequential threshold is:
>>>> > >
>>>> > > Threshold = N / (LoadBalanceFactor * NumCore)
>>>> > >
>>>> > > where N is the "size" of the problem.
>>>> > >
>>>> > > The LoadBalanceFactor in an ideal world would be 1 but real tasks
>>>> > > encounter
>>>> > > delays and execute differently so a value > 1 is needed.
>>>> > > Heuristically a
>>>> > > value of 8 is a good starting value.
>>>> > >
>>>> > > David Holmes
>>>> > >
>>>> > >> -----Original Message-----
>>>> > >> From: concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>>> > >> [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]On
>>>> Behalf Of
>>>> > >> thurston at nomagicsoftware.com
>>>> > >> Sent: Tuesday, 12 February 2013 6:52 AM
>>>> > >> To: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>>> > >> Subject: [concurrency-interest] Some interesting (confusing?)
>>>> > >> benchmark
>>>> > >> results
>>>> > >>
>>>> > >>
>>>> > >> Hello,
>>>> > >>
>>>> > >> I made some initial attempts at using ForkJoin framework for some
>>>> > >> rather obvious recursively parallel use-cases, namely summing the
>>>> > >> elements of an int[] and also sorting an int[] (randomly filled).
>>>> > >>
>>>> > >> The problem-sizes I used ranged from 1M - 10M.
>>>> > >>
>>>> > >> I really enjoy using the framework and find it relatively easy to
>>>> > >> reason about my programs (if not necessarily the internals of the
>>>> > >> framework).
>>>> > >> But I was disappointed with the results: in both cases they were
>>>> > >> slower
>>>> > >> than the corresponding Java serial implementation.
>>>> > >>
>>>> > >> I completely understand the principle of YMMV, and I wasn't
>>>> > >> expecting
>>>> > >> 2x speedup, but especially in the case of the sort, but I was
>>>> > >> expecting
>>>> > >> that ForkJoin would do at least a little better than the
>>>> > >> single-threaded
>>>> > >> version.  I guess what I'm asking is: are these results surprising?
>>>> > >> Does it mean I'm doing something wrong?
>>>> > >>
>>>> > >> I'll give a brief description of my implementation:
>>>> > >> single-threaded:  Arrays.sort(int[])
>>>> > >>
>>>> > >> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at
>>>> > >> by
>>>> > >> trial-and-error)
>>>> > >>
>>>> > >> int[] compute()
>>>> > >> {
>>>> > >> if int[].length < THRESHOLD
>>>> > >>      return insertionSort(int[])
>>>> > >> left = new MergeSort(int[] //split in half)
>>>> > >> right = new MergeSort(int[] //other half)
>>>> > >> return merge(right.compute(), left.join())
>>>> > >> }
>>>> > >>
>>>> > >> The insertionSort() and merge() methods are just standard
>>>> > >> implementations; there is a bit of apples to oranges comparison
>>>> > >> since
>>>> > >> Arrays.sort() uses an optimized quicksort, but we're still talking
>>>> > >> O(nlog(n))
>>>> > >>
>>>> > >> Just ran it on my laptop:
>>>> > >> Windows 7 64-bit
>>>> > >> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> > >> Core 2 Duo==> 2 cores 2GHz
>>>> > >>
>>>> > >> 2M int[]:
>>>> > >> single-threaded: ~330ms
>>>> > >> ForkJoin (2 workers): ~390ms
>>>> > >>
>>>> > >> Would appreciate any feedback and hopefully the #s are somewhat
>>>> > >> helpful
>>>> > >> (and please no scoffawing at my antiquated machine)
>>>> > >>
>>>> > >> -T
>>>> > >>
>>>> > >>
>>>> > >> ______________________________**_________________
>>>> > >> Concurrency-interest mailing list
>>>> > >> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> > >> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1] [1]
>>>>
>>>>
>>>> > >>
>>>> >
>>>> > ______________________________**_________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> > http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1] [1]
>>>>
>>>>
>>>> >
>>>>
>>>> ______________________________**_________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1] [1]
>>>>
>>>> --
>>>>
>>>
>>>
>>> r:rgb(0,0,0);font-family:**Times;font-variant:normal;**
>>> letter-spacing:normal;line-**height:normal;text-align:-**
>>> webkit-auto;text-indent:0px;**text-transform:none;white-**
>>> space:normal;word-spacing:0px;**font-size:medium">VIKTOR
>>> KLANG
>>>  _Director of Engineering_
>>>
>>>  Typesafe [2] - The software stack for applications that scale
>>>  Twitter: @viktorklang
>>>
>>> Links:
>>> ------
>>> [1] http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1]
>>> [2] http://www.typesafe.com/ [2]
>>>
>>> --
>>>
>>>
>>> VIKTOR KLANG
>>> _Director of Engineering_
>>>
>>
>> t-align:-webkit-auto;text-**indent:0px;text-transform:**
>> none;white-space:normal;word-**spacing:0px;font-size:medium">
>>  Typesafe [2] - The software stack for applications that scale
>>  Twitter: @viktorklang
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> [2] http://www.typesafe.com/
>>
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/8d1f0f7e/attachment-0001.html>

From thurston at nomagicsoftware.com  Tue Feb 12 14:20:04 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Tue, 12 Feb 2013 11:20:04 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
Message-ID: <cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>

Here is all of the relevant code in a single blob.  I think it's 
selef-explanatory; sample test is at the end.

http://pastebin.com/WrfBHYSG



On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
> Yes, I will push the code and my measurements some time this week.
> I'm really interested in seeing what others' results would be.  What 
> I
> can say is that the timings are surprisingly stable (range 30-40ms)
> and the input data is randomly generated for each test run which can
> certainly affect merge(int[], int[])'s performance
>
> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>> Hi,
>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>> Hi T,
>>
>> Can you pub the raw data from your runs? Average tends to hide
>> effects and it's difficult to say anything with only that measure.
>>
>> Regards,
>> Kirk
>>
>>
>>> Hello,
>>>
>>> I made some initial attempts at using ForkJoin framework for some 
>>> rather obvious recursively parallel use-cases, namely summing the 
>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>
>>> The problem-sizes I used ranged from 1M - 10M.
>>>
>>> I really enjoy using the framework and find it relatively easy to 
>>> reason about my programs (if not necessarily the internals of the 
>>> framework).
>>> But I was disappointed with the results: in both cases they were 
>>> slower than the corresponding Java serial implementation.
>>>
>>> I completely understand the principle of YMMV, and I wasn't 
>>> expecting 2x speedup, but especially in the case of the sort, but I 
>>> was expecting that ForkJoin would do at least a little better than 
>>> the single-threaded version.  I guess what I'm asking is: are these 
>>> results surprising?  Does it mean I'm doing something wrong?
>>>
>>> I'll give a brief description of my implementation:
>>> single-threaded:  Arrays.sort(int[])
>>>
>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at 
>>> by trial-and-error)
>>>
>>> int[] compute()
>>> {
>>> if int[].length < THRESHOLD
>>>    return insertionSort(int[])
>>> left = new MergeSort(int[] //split in half)
>>> right = new MergeSort(int[] //other half)
>>> return merge(right.compute(), left.join())
>>> }
>>>
>>> The insertionSort() and merge() methods are just standard 
>>> implementations; there is a bit of apples to oranges comparison since 
>>> Arrays.sort() uses an optimized quicksort, but we're still talking 
>>> O(nlog(n))
>>>
>>> Just ran it on my laptop:
>>> Windows 7 64-bit
>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>> Core 2 Duo==> 2 cores 2GHz
>>>
>>> 2M int[]:
>>> single-threaded: ~330ms
>>> ForkJoin (2 workers): ~390ms
>>>
>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>> helpful (and please no scoffawing at my antiquated machine)
>>>
>>> -T
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From kirk at kodewerk.com  Tue Feb 12 14:23:10 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 12 Feb 2013 20:23:10 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEEJJKAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEEJJKAA.davidcholmes@aapt.net.au>
Message-ID: <6816CE2B-3BFC-44E8-9635-912B2A9E006A@kodewerk.com>

I'd like to see some of this mythical code ;-)

Regards,
Kirk
On 2013-02-12, at 6:26 AM, David Holmes <davidcholmes at aapt.net.au> wrote:

>> This is in line with my expectation; what amount of work-stealing could 
>> happen with two tasks (each merge-sorting a 1M item array)?  With 16K 
>> tasks, the work-stealing can really strut its stuff (and at least in my 
>> case,can overwhelm the overhead associated with scheduling the tasks)
> 
> I think you are seeing affects that are not due to parallelisation of the algorithm as such. 16K tasks for two threads is just ridiculous task creation and management overhead. If you see a speed up compared to three tasks then you are either not comparing apples to oranges, or you are affected by other things - such as memory management when dealing with large arrays.
> 
> David
> -----
> 
> 
>> -----Original Message-----
>> From: thurston at nomagicsoftware.com [mailto:thurston at nomagicsoftware.com]
>> Sent: Tuesday, 12 February 2013 3:14 PM
>> To: viktor ?lang
>> Cc: dholmes at ieee.org; concurrency-interest
>> Subject: Re: [concurrency-interest] Some interesting (confusing?)
>> benchmark results
>> 
>> 
>> 
>> 
>> On 2013-02-11 14:24, ?iktor ?lang wrote:
>>> On Mon, Feb 11, 2013 at 11:18 PM, David Holmes 
>>> <davidcholmes at aapt.net.au> wrote:
>>> 
>>>> That doesn't make sense to me. If you had a threshold of 250 then 
>>>> you are
>>>> creating thousands of tasks and that overhead should potentially 
>>>> bring your
>>>> laptop to its knees. With 250K threshold you have far fewer tasks.
>>> 
>>> I'm not sure it makes sense to create more sort-tasks than there is
>>> Threads in the pool.
>> 
>> Really, Viktor?  That surprises me coming from Akka.  Does that mean 
>> you guys only allocate as many message-processing "workers" as there are 
>> hardware threads?
>> 
>> To be clear, with the 2M size array data, and the threshold of 250-size 
>> array for the (serial) insertion sort, 16K+ sort tasks are constructed 
>> (as D. Holmes mentioned).  This is significantly *faster* than doing 
>> what either of you suggested.  If I simply split the problem in two, and 
>> just run a serial mergesort on each half and then merge the two halves 
>> (that's three total tasks), it's over 50% slower.
>> 
>> Let me repeat that.  16K+ tasks is much, much faster than 3 tasks.
>> 
>> This is in line with my expectation; what amount of work-stealing could 
>> happen with two tasks (each merge-sorting a 1M item array)?  With 16K 
>> tasks, the work-stealing can really strut its stuff (and at least in my 
>> case,can overwhelm the overhead associated with scheduling the tasks)
>> 
>> If you only create as many tasks as there are hardware threads -- that 
>> seems a lot like a standard thread-pool implementation.  (Of course, if 
>> there are many more cores, the opportunity for work-stealing would 
>> increase even with the 1-1 ratio).
>> 
>> When I first used fork/join, that's the approach I took.
>> 1.  split the problem (hardware threads * (1||2)
>> 2.  have each task do its work serially (i.e. don't fork)
>> 3.  combine the (sub)totals/results
>> 
>> Not only is that less appealing from a programming point of view, it 
>> performs slower; that's the promise, nee even purpose, of the fork/join 
>> framework at least when it comes to implementing naturally recursive 
>> algorithms.
>> 
>> -T
>> 
>> 
>>> 
>>>> That said I don't know what the memory requirements are for your 
>>>> algorithm.
>>>> If you are allocating temporary arrays then a  large threshold could 
>>>> well
>>>> cause a problem.
>>>> 
>>>> The parallel array sort going into JDK 8 is a merge sort but uses a 
>>>> working
>>>> array equal in size to the original array to be sorted.
>>> 
>>> No in-place merge sort? ;-)
>>> 
>>> 
>>>>> -----Original Message-----
>>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>> thurston at nomagicsoftware.com
>>>> 
>>>>> Sent: Tuesday, 12 February 2013 7:49 AM
>>>>> To: concurrency-interest at cs.oswego.edu
>>>>> Subject: Re: [concurrency-interest] Some interesting (confusing?)
>>>>> benchmark results
>>>>> 
>>>>> 
>>>>> Well, it's the algorithm that's dispositive in this case.
>>>>> Insertion-sort is empirically better than mergesort up to around
>>>>> 25-sized arrays.  That's where I started, and I played around with
>>>>> different thresholds and 250 seemed to be the best.
>>>>> Anything larger than 1000 you start to see significant 
>>>> degradation.
>>>>> I tried your formula (approximately) with N = 2 000 000 ==> 250K
>>>>> 
>>>>> That brought my laptop to its knees (as expected): over 33K ms as
>>>>> opposed to 390ms with 250.
>>>>> 
>>>>> Following my own admonition (the algorithm matters), it occurs to 
>>>> me
>>>>> that I just use my own serial mergesort as comparison to the 
>>>> Forkjoin
>>>>> (as opposed to Arrays.sort).  I'll let y'all know the results of 
>>>> that
>>>>> 
>>>>> -T
>>>>> 
>>>>> 
>>>>> 
>>>>> On 2013-02-11 13:19, David Holmes wrote:
>>>>>> Your sequential threshold is far too small for only two worker
>>>>>> threads. You
>>>>>> would get near optimum speedup only splitting the problem into 
>>>> two.
>>>>>> 
>>>>>> The heuristic for sequential threshold is:
>>>>>> 
>>>>>> Threshold = N / (LoadBalanceFactor * NumCore)
>>>>>> 
>>>>>> where N is the "size" of the problem.
>>>>>> 
>>>>>> The LoadBalanceFactor in an ideal world would be 1 but real 
>>>> tasks
>>>>>> encounter
>>>>>> delays and execute differently so a value > 1 is needed.
>>>>>> Heuristically a
>>>>>> value of 8 is a good starting value.
>>>>>> 
>>>>>> David Holmes
>>>>>> 
>>>>>>> -----Original Message-----
>>>>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>>>>> thurston at nomagicsoftware.com
>>>>>>> Sent: Tuesday, 12 February 2013 6:52 AM
>>>>>>> To: concurrency-interest at cs.oswego.edu
>>>>>>> Subject: [concurrency-interest] Some interesting (confusing?)
>>>>>>> benchmark
>>>>>>> results
>>>>>>> 
>>>>>>> 
>>>>>>> Hello,
>>>>>>> 
>>>>>>> I made some initial attempts at using ForkJoin framework for 
>>>> some
>>>>>>> rather obvious recursively parallel use-cases, namely summing 
>>>> the
>>>>>>> elements of an int[] and also sorting an int[] (randomly 
>>>> filled).
>>>>>>> 
>>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>> 
>>>>>>> I really enjoy using the framework and find it relatively easy 
>>>> to
>>>>>>> reason about my programs (if not necessarily the internals of 
>>>> the
>>>>>>> framework).
>>>>>>> But I was disappointed with the results: in both cases they 
>>>> were
>>>>>>> slower
>>>>>>> than the corresponding Java serial implementation.
>>>>>>> 
>>>>>>> I completely understand the principle of YMMV, and I wasn't
>>>>>>> expecting
>>>>>>> 2x speedup, but especially in the case of the sort, but I was
>>>>>>> expecting
>>>>>>> that ForkJoin would do at least a little better than the
>>>>>>> single-threaded
>>>>>>> version.  I guess what I'm asking is: are these results 
>>>> surprising?
>>>>>>> Does it mean I'm doing something wrong?
>>>>>>> 
>>>>>>> I'll give a brief description of my implementation:
>>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>> 
>>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>>>> at
>>>>>>> by
>>>>>>> trial-and-error)
>>>>>>> 
>>>>>>> int[] compute()
>>>>>>> {
>>>>>>> if int[].length < THRESHOLD
>>>>>>>     return insertionSort(int[])
>>>>>>> left = new MergeSort(int[] //split in half)
>>>>>>> right = new MergeSort(int[] //other half)
>>>>>>> return merge(right.compute(), left.join())
>>>>>>> }
>>>>>>> 
>>>>>>> The insertionSort() and merge() methods are just standard
>>>>>>> implementations; there is a bit of apples to oranges comparison
>>>>>>> since
>>>>>>> Arrays.sort() uses an optimized quicksort, but we're still 
>>>> talking
>>>>>>> O(nlog(n))
>>>>>>> 
>>>>>>> Just ran it on my laptop:
>>>>>>> Windows 7 64-bit
>>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>> 
>>>>>>> 2M int[]:
>>>>>>> single-threaded: ~330ms
>>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>> 
>>>>>>> Would appreciate any feedback and hopefully the #s are somewhat
>>>>>>> helpful
>>>>>>> (and please no scoffawing at my antiquated machine)
>>>>>>> 
>>>>>>> -T
>>>>>>> 
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>>>>>> 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>>> 
>>>> --
>>> 
>>> 
>> r:rgb(0,0,0);font-family:Times;font-variant:normal;letter-spacing:
>> normal;line-height:normal;text-align:-webkit-auto;text-indent:0px;
>> text-transform:none;white-space:normal;word-spacing:0px;font-size:
>> medium">VIKTOR
>>> KLANG
>>> _Director of Engineering_
>>> 
>>> Typesafe [2] - The software stack for applications that scale
>>> Twitter: @viktorklang
>>> 
>>> Links:
>>> ------
>>> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> [2] http://www.typesafe.com/
>> 
>> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From kirk at kodewerk.com  Tue Feb 12 14:28:19 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 12 Feb 2013 20:28:19 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
Message-ID: <8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>

> 
> Do you agree that thread pool sizing depends on type of work? (IO bound vs CPU bound, bursty vs steady etc etc)
Yes
> Do you agree that a JVM Thread is not a unit of parallelism?
Yes
> Do you agree that having more JVM Threads than hardware threads is bad for CPU-bound workloads?
No, even with CPU bound workloads I have found that the hardware/OS is much better at managing many workloads across many threads than I am. So a few more threads is ok, many more threads is bad fast.

Regards,
Kirk

From kirk at kodewerk.com  Tue Feb 12 14:30:46 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 12 Feb 2013 20:30:46 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
Message-ID: <76CC10E2-DADB-4341-ADDF-3914BECC8198@kodewerk.com>

oooo, thanks!!!

On 2013-02-12, at 8:20 PM, thurston at nomagicsoftware.com wrote:

> Here is all of the relevant code in a single blob.  I think it's selef-explanatory; sample test is at the end.
> 
> http://pastebin.com/WrfBHYSG
> 
> 
> 
> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>> Yes, I will push the code and my measurements some time this week.
>> I'm really interested in seeing what others' results would be.  What I
>> can say is that the timings are surprisingly stable (range 30-40ms)
>> and the input data is randomly generated for each test run which can
>> certainly affect merge(int[], int[])'s performance
>> 
>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>> Hi,
>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>> Hi T,
>>> 
>>> Can you pub the raw data from your runs? Average tends to hide
>>> effects and it's difficult to say anything with only that measure.
>>> 
>>> Regards,
>>> Kirk
>>> 
>>> 
>>>> Hello,
>>>> 
>>>> I made some initial attempts at using ForkJoin framework for some rather obvious recursively parallel use-cases, namely summing the elements of an int[] and also sorting an int[] (randomly filled).
>>>> 
>>>> The problem-sizes I used ranged from 1M - 10M.
>>>> 
>>>> I really enjoy using the framework and find it relatively easy to reason about my programs (if not necessarily the internals of the framework).
>>>> But I was disappointed with the results: in both cases they were slower than the corresponding Java serial implementation.
>>>> 
>>>> I completely understand the principle of YMMV, and I wasn't expecting 2x speedup, but especially in the case of the sort, but I was expecting that ForkJoin would do at least a little better than the single-threaded version.  I guess what I'm asking is: are these results surprising?  Does it mean I'm doing something wrong?
>>>> 
>>>> I'll give a brief description of my implementation:
>>>> single-threaded:  Arrays.sort(int[])
>>>> 
>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at by trial-and-error)
>>>> 
>>>> int[] compute()
>>>> {
>>>> if int[].length < THRESHOLD
>>>>   return insertionSort(int[])
>>>> left = new MergeSort(int[] //split in half)
>>>> right = new MergeSort(int[] //other half)
>>>> return merge(right.compute(), left.join())
>>>> }
>>>> 
>>>> The insertionSort() and merge() methods are just standard implementations; there is a bit of apples to oranges comparison since Arrays.sort() uses an optimized quicksort, but we're still talking O(nlog(n))
>>>> 
>>>> Just ran it on my laptop:
>>>> Windows 7 64-bit
>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> Core 2 Duo==> 2 cores 2GHz
>>>> 
>>>> 2M int[]:
>>>> single-threaded: ~330ms
>>>> ForkJoin (2 workers): ~390ms
>>>> 
>>>> Would appreciate any feedback and hopefully the #s are somewhat helpful (and please no scoffawing at my antiquated machine)
>>>> 
>>>> -T
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From oleksandr.otenko at oracle.com  Tue Feb 12 15:38:47 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 12 Feb 2013 20:38:47 +0000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
Message-ID: <511AA857.2070406@oracle.com>

There is some bug in your sorting algorithm.

When I set THRESHOLD to 1M, it hangs.

When I replace

             SortUtil.insertionSort(this.input); //is this multi-thread 
safe?

with
             Arrays.sort(this.input);

I get ~1M is the best THRESHOLD on my laptop, and ~100K on my 24-core 
machine, which is unsurprising.


Alex


On 12/02/2013 19:20, thurston at nomagicsoftware.com wrote:
> Here is all of the relevant code in a single blob.  I think it's 
> selef-explanatory; sample test is at the end.
>
> http://pastebin.com/WrfBHYSG
>
>
>
> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>> Yes, I will push the code and my measurements some time this week.
>> I'm really interested in seeing what others' results would be. What I
>> can say is that the timings are surprisingly stable (range 30-40ms)
>> and the input data is randomly generated for each test run which can
>> certainly affect merge(int[], int[])'s performance
>>
>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>> Hi,
>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>> Hi T,
>>>
>>> Can you pub the raw data from your runs? Average tends to hide
>>> effects and it's difficult to say anything with only that measure.
>>>
>>> Regards,
>>> Kirk
>>>
>>>
>>>> Hello,
>>>>
>>>> I made some initial attempts at using ForkJoin framework for some 
>>>> rather obvious recursively parallel use-cases, namely summing the 
>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>
>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>
>>>> I really enjoy using the framework and find it relatively easy to 
>>>> reason about my programs (if not necessarily the internals of the 
>>>> framework).
>>>> But I was disappointed with the results: in both cases they were 
>>>> slower than the corresponding Java serial implementation.
>>>>
>>>> I completely understand the principle of YMMV, and I wasn't 
>>>> expecting 2x speedup, but especially in the case of the sort, but I 
>>>> was expecting that ForkJoin would do at least a little better than 
>>>> the single-threaded version.  I guess what I'm asking is: are these 
>>>> results surprising?  Does it mean I'm doing something wrong?
>>>>
>>>> I'll give a brief description of my implementation:
>>>> single-threaded:  Arrays.sort(int[])
>>>>
>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at 
>>>> by trial-and-error)
>>>>
>>>> int[] compute()
>>>> {
>>>> if int[].length < THRESHOLD
>>>>    return insertionSort(int[])
>>>> left = new MergeSort(int[] //split in half)
>>>> right = new MergeSort(int[] //other half)
>>>> return merge(right.compute(), left.join())
>>>> }
>>>>
>>>> The insertionSort() and merge() methods are just standard 
>>>> implementations; there is a bit of apples to oranges comparison 
>>>> since Arrays.sort() uses an optimized quicksort, but we're still 
>>>> talking O(nlog(n))
>>>>
>>>> Just ran it on my laptop:
>>>> Windows 7 64-bit
>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> Core 2 Duo==> 2 cores 2GHz
>>>>
>>>> 2M int[]:
>>>> single-threaded: ~330ms
>>>> ForkJoin (2 workers): ~390ms
>>>>
>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>
>>>> -T
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/7fb999c2/attachment-0001.html>

From javamann at cox.net  Tue Feb 12 15:41:47 2013
From: javamann at cox.net (javamann at cox.net)
Date: Tue, 12 Feb 2013 15:41:47 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <76CC10E2-DADB-4341-ADDF-3914BECC8198@kodewerk.com>
Message-ID: <20130212204147.NR54C.197691.imail@fed1rmwml207>

Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.

Thanks

-Pete

    public int remainingCapacity() {
        final ReentrantLock lock = this.lock;
        lock.lock();
        try {
            return items.length - count;
        } finally {
            lock.unlock();
        }
    }


From nathan.reynolds at oracle.com  Tue Feb 12 16:00:13 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 12 Feb 2013 14:00:13 -0700
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <20130212204147.NR54C.197691.imail@fed1rmwml207>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
Message-ID: <511AAD5D.6070902@oracle.com>

this.lock is a memory indirection (i.e. dereferences "this" to get the 
value in "lock") and could incur a cache miss (i.e. loads 
ArrayBlockQueue into the L1 cache) or even worse false sharing.  By 
copying to the local variable, the value is on the stack.  There won't 
be any memory indirection to access the value.  Cache misses would only 
happen if the thread context switched.  False sharing is impossible.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 1:41 PM, javamann at cox.net wrote:
> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>
> Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/180afa0d/attachment.html>

From stanimir at riflexo.com  Tue Feb 12 16:05:32 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 12 Feb 2013 23:05:32 +0200
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <20130212204147.NR54C.197691.imail@fed1rmwml207>
References: <76CC10E2-DADB-4341-ADDF-3914BECC8198@kodewerk.com>
	<20130212204147.NR54C.197691.imail@fed1rmwml207>
Message-ID: <CAEJX8oq+S-NYLdmnkx7Geij5ijH_Q_xd9rGEmbBEUM4PT99a_Q@mail.gmail.com>

In some tests Doug Lea found introducing the local var to have increased
performance.
It's a micro optimization. It's not necessary the optimization to be
profitable across different JVMs, though.

Stanimir


On Tue, Feb 12, 2013 at 10:41 PM, <javamann at cox.net> wrote:

> Stupid question time. While going through the code for an ArrayBlockQueue
> I came across numerous instances where the instance 'lock' is copied to a
> variable in a Method. I know there is a reason for this, I just don't know
> what it is.
>
> Thanks
>
> -Pete
>
>     public int remainingCapacity() {
>         final ReentrantLock lock = this.lock;
>         lock.lock();
>         try {
>             return items.length - count;
>         } finally {
>             lock.unlock();
>         }
>     }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/56a03abb/attachment.html>

From vitalyd at gmail.com  Tue Feb 12 16:10:27 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 12 Feb 2013 16:10:27 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AAD5D.6070902@oracle.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
Message-ID: <CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>

I don't think it's that since "this" is already loaded (or else you can't
call the method).  I believe Doug did this because he found that final
field loads weren't commoned across lock() calls (even though they could
be).

Sent from my phone
On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  this.lock is a memory indirection (i.e. dereferences "this" to get the
> value in "lock") and could incur a cache miss (i.e. loads ArrayBlockQueue
> into the L1 cache) or even worse false sharing.  By copying to the local
> variable, the value is on the stack.  There won't be any memory indirection
> to access the value.  Cache misses would only happen if the thread context
> switched.  False sharing is impossible.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>
> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>
> Thanks
>
> -Pete
>
>     public int remainingCapacity() {
>         final ReentrantLock lock = this.lock;
>         lock.lock();
>         try {
>             return items.length - count;
>         } finally {
>             lock.unlock();
>         }
>     }
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/694dc7b9/attachment.html>

From javamann at cox.net  Tue Feb 12 16:12:59 2013
From: javamann at cox.net (javamann at cox.net)
Date: Tue, 12 Feb 2013 16:12:59 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AAD5D.6070902@oracle.com>
Message-ID: <20130212211259.2CB54.198035.imail@fed1rmwml207>

Thanks, this is why I really like this list.

-Pete

---- Nathan Reynolds <nathan.reynolds at oracle.com> wrote: 

=============
this.lock is a memory indirection (i.e. dereferences "this" to get the 
value in "lock") and could incur a cache miss (i.e. loads 
ArrayBlockQueue into the L1 cache) or even worse false sharing.  By 
copying to the local variable, the value is on the stack.  There won't 
be any memory indirection to access the value.  Cache misses would only 
happen if the thread context switched.  False sharing is impossible.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 1:41 PM, javamann at cox.net wrote:
> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>
> Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


--

1. If a man is standing in the middle of the forest talking, and there is no woman around to hear him, is he still wrong?

2. Behind every great woman... Is a man checking out her ass

3. I am not a member of any organized political party. I am a Democrat.*

4. Diplomacy is the art of saying "Nice doggie" until you can find a rock.*

5. A process is what you need when all your good people have left.


*Will Rogers



From viktor.klang at gmail.com  Tue Feb 12 16:18:29 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 12 Feb 2013 22:18:29 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
Message-ID: <CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>

On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com> wrote:

> >
> > Do you agree that thread pool sizing depends on type of work? (IO bound
> vs CPU bound, bursty vs steady etc etc)
> Yes
> > Do you agree that a JVM Thread is not a unit of parallelism?
> Yes
> > Do you agree that having more JVM Threads than hardware threads is bad
> for CPU-bound workloads?
> No, even with CPU bound workloads I have found that the hardware/OS is
> much better at managing many workloads across many threads than I am. So a
> few more threads is ok, many more threads is bad fast.
>

That's an interesting observation. Have any more data on that? (really
interested)
As I said earlier, for CPU-bound workloads we've seen the best performance
when only loading 60-70% of the cores (other threads exist on the machine
of course).

Cheers,

?


>
> Regards,
> Kirk




-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/492c09a0/attachment-0001.html>

From dl at cs.oswego.edu  Tue Feb 12 16:21:23 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 12 Feb 2013 16:21:23 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <20130212204147.NR54C.197691.imail@fed1rmwml207>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
Message-ID: <511AB253.1060208@cs.oswego.edu>

On 02/12/13 15:41, javamann at cox.net wrote:
> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>

It's ultimately due to the fundamental mismatch between memory models
and OOP :-)

Just about every method in all of j.u.c adopts the policy of
reading fields as locals whenever a value is used more than once.
This way you are sure which value applies when.
This is not often pretty, but is easier to visually verify.

The surprising case is doing this even for "final" fields.
This is because JVMs are not always smart enough to exploit
the fine points of the JMM and not reload read final
values, as they would otherwise need to do across the
volatile accesses entailed in locking. Some JVMs are smarter
than they used to be about this, but still not always
smart enough.

-Doug



> Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From stanimir at riflexo.com  Tue Feb 12 16:22:29 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 12 Feb 2013 23:22:29 +0200
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
Message-ID: <CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>

This should be in some register (ECX on 32bit x86) and of course it still
has to deref./load 'lock'. Calling the method may not need to derefence
this.
However the 2nd load could be avoided (and possible false sharing
prevented) if the value is stored on the stack/kept in register. The
current method body is too short to actually worry about since there would
be likely enough registers available. Yet, the idiom is used everywhere, so
the current case just follows suits.

Stanimir

On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> I don't think it's that since "this" is already loaded (or else you can't
> call the method).  I believe Doug did this because he found that final
> field loads weren't commoned across lock() calls (even though they could
> be).
>
> Sent from my phone
> On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
> wrote:
>
>>  this.lock is a memory indirection (i.e. dereferences "this" to get the
>> value in "lock") and could incur a cache miss (i.e. loads ArrayBlockQueue
>> into the L1 cache) or even worse false sharing.  By copying to the local
>> variable, the value is on the stack.  There won't be any memory indirection
>> to access the value.  Cache misses would only happen if the thread context
>> switched.  False sharing is impossible.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>>
>> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>
>> Thanks
>>
>> -Pete
>>
>>     public int remainingCapacity() {
>>         final ReentrantLock lock = this.lock;
>>         lock.lock();
>>         try {
>>             return items.length - count;
>>         } finally {
>>             lock.unlock();
>>         }
>>     }
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/10256aa2/attachment.html>

From vitalyd at gmail.com  Tue Feb 12 16:24:21 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 12 Feb 2013 16:24:21 -0500
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
Message-ID: <CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>

For pure CPU bound work, I typically add 1 or maybe 2 more threads than #
of hardware threads; this is to account for hardware threads possibly
hitting a hard page fault and getting suspended.  I don't see how having
any more than that threads benefits perf.

Sent from my phone
On Feb 12, 2013 4:21 PM, "?iktor ?lang" <viktor.klang at gmail.com> wrote:

>
>
>
> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>
>> >
>> > Do you agree that thread pool sizing depends on type of work? (IO bound
>> vs CPU bound, bursty vs steady etc etc)
>> Yes
>> > Do you agree that a JVM Thread is not a unit of parallelism?
>> Yes
>> > Do you agree that having more JVM Threads than hardware threads is bad
>> for CPU-bound workloads?
>> No, even with CPU bound workloads I have found that the hardware/OS is
>> much better at managing many workloads across many threads than I am. So a
>> few more threads is ok, many more threads is bad fast.
>>
>
> That's an interesting observation. Have any more data on that? (really
> interested)
> As I said earlier, for CPU-bound workloads we've seen the best performance
> when only loading 60-70% of the cores (other threads exist on the machine
> of course).
>
> Cheers,
>
> ?
>
>
>>
>> Regards,
>> Kirk
>
>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/829c6428/attachment.html>

From aleksey.shipilev at oracle.com  Tue Feb 12 16:24:51 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 13 Feb 2013 01:24:51 +0400
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
Message-ID: <511AB323.90309@oracle.com>

On 02/13/2013 01:18 AM, ?iktor ?lang wrote:
> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com
> <mailto:kirk at kodewerk.com>> wrote:
> 
>     >
>     > Do you agree that thread pool sizing depends on type of work? (IO
>     bound vs CPU bound, bursty vs steady etc etc)
>     Yes
>     > Do you agree that a JVM Thread is not a unit of parallelism?
>     Yes
>     > Do you agree that having more JVM Threads than hardware threads is
>     bad for CPU-bound workloads?
>     No, even with CPU bound workloads I have found that the hardware/OS
>     is much better at managing many workloads across many threads than I
>     am. So a few more threads is ok, many more threads is bad fast.
> 
> 
> That's an interesting observation. Have any more data on that? (really
> interested)
> As I said earlier, for CPU-bound workloads we've seen the best
> performance when only loading 60-70% of the cores (other threads exist
> on the machine of course).

I could relate to this observation if "performance" in Viktor's
statement has the significant "latency" component. "pure throughput"
things indeed have the behavior Kirk mentions.

-Aleksey.


From forax at univ-mlv.fr  Tue Feb 12 16:34:06 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Tue, 12 Feb 2013 22:34:06 +0100
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AB253.1060208@cs.oswego.edu>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
Message-ID: <511AB54E.3060003@univ-mlv.fr>

On 02/12/2013 10:21 PM, Doug Lea wrote:
> On 02/12/13 15:41, javamann at cox.net wrote:
>> Stupid question time. While going through the code for an 
>> ArrayBlockQueue I came across numerous instances where the instance 
>> 'lock' is copied to a variable in a Method. I know there is a reason 
>> for this, I just don't know what it is.
>>
>
> It's ultimately due to the fundamental mismatch between memory models
> and OOP :-)
>
> Just about every method in all of j.u.c adopts the policy of
> reading fields as locals whenever a value is used more than once.
> This way you are sure which value applies when.
> This is not often pretty, but is easier to visually verify.
>
> The surprising case is doing this even for "final" fields.
> This is because JVMs are not always smart enough to exploit
> the fine points of the JMM and not reload read final
> values, as they would otherwise need to do across the
> volatile accesses entailed in locking. Some JVMs are smarter
> than they used to be about this, but still not always
> smart enough.

It's not exactly that the VM is not smart enough, it's that the VM may 
not see enough.
In the example, the VM has to be able to inline the whole graph of 
method behind the call lock.lock() to be sure that no method that is 
part of this graph contains code that may change the value of the field 
lock*. If the VM can prove that, the field doesn't need to be re-loaded 
when calling lock.unlock().

>
> -Doug

R?mi
* and yes, final field can be changed.

>
>
>
>> Thanks
>>
>> -Pete
>>
>>      public int remainingCapacity() {
>>          final ReentrantLock lock = this.lock;
>>          lock.lock();
>>          try {
>>              return items.length - count;
>>          } finally {
>>              lock.unlock();
>>          }
>>      }
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Tue Feb 12 16:37:38 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 12 Feb 2013 16:37:38 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
	<CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
Message-ID: <CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>

Do you mean false sharing due to the lock word sitting on same cache line
as the lock object itself? Sure, but storing a pointer in a register is not
going to help since ultimately you have to fetch the value from memory
address that's stored in the register (some offset from that to find the
lock word), and if cache line is gone, you get hit anyway.  In other words,
you have to deref the value in the register - it's not storing a scalar
value.  Or did I misunderstand your point?

Sent from my phone
On Feb 12, 2013 4:22 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

> This should be in some register (ECX on 32bit x86) and of course it still
> has to deref./load 'lock'. Calling the method may not need to derefence
> this.
> However the 2nd load could be avoided (and possible false sharing
> prevented) if the value is stored on the stack/kept in register. The
> current method body is too short to actually worry about since there would
> be likely enough registers available. Yet, the idiom is used everywhere, so
> the current case just follows suits.
>
> Stanimir
>
> On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> I don't think it's that since "this" is already loaded (or else you can't
>> call the method).  I believe Doug did this because he found that final
>> field loads weren't commoned across lock() calls (even though they could
>> be).
>>
>> Sent from my phone
>> On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>> wrote:
>>
>>>  this.lock is a memory indirection (i.e. dereferences "this" to get the
>>> value in "lock") and could incur a cache miss (i.e. loads ArrayBlockQueue
>>> into the L1 cache) or even worse false sharing.  By copying to the local
>>> variable, the value is on the stack.  There won't be any memory indirection
>>> to access the value.  Cache misses would only happen if the thread context
>>> switched.  False sharing is impossible.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>>>
>>> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>>
>>> Thanks
>>>
>>> -Pete
>>>
>>>     public int remainingCapacity() {
>>>         final ReentrantLock lock = this.lock;
>>>         lock.lock();
>>>         try {
>>>             return items.length - count;
>>>         } finally {
>>>             lock.unlock();
>>>         }
>>>     }
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/6f79222f/attachment-0001.html>

From stanimir at riflexo.com  Tue Feb 12 16:43:36 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 12 Feb 2013 23:43:36 +0200
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
	<CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
	<CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>
Message-ID: <CAEJX8oo_fF1NN-aCKoTV1_YsQ5PN69-YcBwoN53A3ajY1B5=MQ@mail.gmail.com>

On Tue, Feb 12, 2013 at 11:37 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Do you mean false sharing due to the lock word sitting on same cache line
> as the lock object itself?
>
Nope, just the ArrayBlockingQueue.lock being on the same cache line as any
other object. Changing "an int" field on that object would invalidate the
cache line holding the lock.

Stanimir


> Sure, but storing a pointer in a register is not going to help since
> ultimately you have to fetch the value from memory address that's stored in
> the register (some offset from that to find the lock word), and if cache
> line is gone, you get hit anyway.  In other words, you have to deref the
> value in the register - it's not storing a scalar value.  Or did I
> misunderstand your point?
>
> Sent from my phone
> On Feb 12, 2013 4:22 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
> wrote:
>
>> This should be in some register (ECX on 32bit x86) and of course it still
>> has to deref./load 'lock'. Calling the method may not need to derefence
>> this.
>> However the 2nd load could be avoided (and possible false sharing
>> prevented) if the value is stored on the stack/kept in register. The
>> current method body is too short to actually worry about since there would
>> be likely enough registers available. Yet, the idiom is used everywhere, so
>> the current case just follows suits.
>>
>> Stanimir
>>
>> On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> I don't think it's that since "this" is already loaded (or else you
>>> can't call the method).  I believe Doug did this because he found that
>>> final field loads weren't commoned across lock() calls (even though they
>>> could be).
>>>
>>> Sent from my phone
>>> On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>> wrote:
>>>
>>>>  this.lock is a memory indirection (i.e. dereferences "this" to get
>>>> the value in "lock") and could incur a cache miss (i.e. loads
>>>> ArrayBlockQueue into the L1 cache) or even worse false sharing.  By copying
>>>> to the local variable, the value is on the stack.  There won't be any
>>>> memory indirection to access the value.  Cache misses would only happen if
>>>> the thread context switched.  False sharing is impossible.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>>>>
>>>> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>>>
>>>> Thanks
>>>>
>>>> -Pete
>>>>
>>>>     public int remainingCapacity() {
>>>>         final ReentrantLock lock = this.lock;
>>>>         lock.lock();
>>>>         try {
>>>>             return items.length - count;
>>>>         } finally {
>>>>             lock.unlock();
>>>>         }
>>>>     }
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/8fd202ba/attachment.html>

From vitalyd at gmail.com  Tue Feb 12 16:46:32 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 12 Feb 2013 16:46:32 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AB54E.3060003@univ-mlv.fr>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu> <511AB54E.3060003@univ-mlv.fr>
Message-ID: <CAHjP37E6hat842DWKQgwCzPs7cQonP-LMa+rjcKCLrGbshT+eA@mail.gmail.com>

So Cliff Click had a few blogs on this.  Apparently, some popular
frameworks muck around with final fields (I.e. change them via
reflection).  One of his arguments was that enregistering final fields
isn't all that profitable since the 2nd load will most likely hit L1 and be
only marginally slower.  I can see that point, but I'd say it's unfortunate
that JIT writers can't liberally enregister final fields due to someone
doing bizarre things with them.

Sent from my phone
On Feb 12, 2013 4:40 PM, "Remi Forax" <forax at univ-mlv.fr> wrote:

> On 02/12/2013 10:21 PM, Doug Lea wrote:
>
>> On 02/12/13 15:41, javamann at cox.net wrote:
>>
>>> Stupid question time. While going through the code for an
>>> ArrayBlockQueue I came across numerous instances where the instance 'lock'
>>> is copied to a variable in a Method. I know there is a reason for this, I
>>> just don't know what it is.
>>>
>>>
>> It's ultimately due to the fundamental mismatch between memory models
>> and OOP :-)
>>
>> Just about every method in all of j.u.c adopts the policy of
>> reading fields as locals whenever a value is used more than once.
>> This way you are sure which value applies when.
>> This is not often pretty, but is easier to visually verify.
>>
>> The surprising case is doing this even for "final" fields.
>> This is because JVMs are not always smart enough to exploit
>> the fine points of the JMM and not reload read final
>> values, as they would otherwise need to do across the
>> volatile accesses entailed in locking. Some JVMs are smarter
>> than they used to be about this, but still not always
>> smart enough.
>>
>
> It's not exactly that the VM is not smart enough, it's that the VM may not
> see enough.
> In the example, the VM has to be able to inline the whole graph of method
> behind the call lock.lock() to be sure that no method that is part of this
> graph contains code that may change the value of the field lock*. If the VM
> can prove that, the field doesn't need to be re-loaded when calling
> lock.unlock().
>
>
>> -Doug
>>
>
> R?mi
> * and yes, final field can be changed.
>
>
>>
>>
>>  Thanks
>>>
>>> -Pete
>>>
>>>      public int remainingCapacity() {
>>>          final ReentrantLock lock = this.lock;
>>>          lock.lock();
>>>          try {
>>>              return items.length - count;
>>>          } finally {
>>>              lock.unlock();
>>>          }
>>>      }
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/92b4881d/attachment.html>

From nathan.reynolds at oracle.com  Tue Feb 12 16:47:39 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 12 Feb 2013 14:47:39 -0700
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
Message-ID: <511AB87B.1040303@oracle.com>

 > best performance when only loading 60-70% of the cores

What do you mean by performance?  Do you mean you achieve the highest 
throughput?  Do you mean you achieve the lowest response times?  Do you 
mean something else?

The early implementations of hyper-threading on Intel processors 
sometimes ran into trouble depending upon the workload.  Enabling 
hyper-threading actual hurt performance and throughput.  A lot of people 
quickly learned to disable hyper-threading.  They are so entrenched in 
that decision that it is hard to help them see that hyper-threading is 
actually beneficial now.

The Linux thread scheduler is smart enough to put 1 thread on each 
physical core first and then double up on physical cores.  So, I am not 
surprised that loading 60-70% cores yields best performance on the above 
mentioned processors.  This creates a few more threads than physical 
cores which in a way disables hyper-threading.

Later implementations of hyper-threading improved considerably.  I am 
not aware of any workloads which perform worse with hyper-threading 
enabled.  With a modern processor (i.e. Westmere or newer), it would be 
interesting if you ran your workload with hyper-threading enabled and 
disabled.  Then find the optimal thread count for each configuration.  
If hyper-threading disabled performs better, then that definitely would 
be an interesting workload and result.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>
>
>
> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com 
> <mailto:kirk at kodewerk.com>> wrote:
>
>     >
>     > Do you agree that thread pool sizing depends on type of work?
>     (IO bound vs CPU bound, bursty vs steady etc etc)
>     Yes
>     > Do you agree that a JVM Thread is not a unit of parallelism?
>     Yes
>     > Do you agree that having more JVM Threads than hardware threads
>     is bad for CPU-bound workloads?
>     No, even with CPU bound workloads I have found that the
>     hardware/OS is much better at managing many workloads across many
>     threads than I am. So a few more threads is ok, many more threads
>     is bad fast.
>
>
> That's an interesting observation. Have any more data on that? (really 
> interested)
> As I said earlier, for CPU-bound workloads we've seen the best 
> performance when only loading 60-70% of the cores (other threads exist 
> on the machine of course).
>
> Cheers,
>
> ?
>
>
>     Regards,
>     Kirk
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> /
> /
> Typesafe <http://www.typesafe.com/>- The software stack for 
> applications that scale
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/aeb0e062/attachment-0001.html>

From stanimir at riflexo.com  Tue Feb 12 16:49:13 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 12 Feb 2013 23:49:13 +0200
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AB54E.3060003@univ-mlv.fr>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu> <511AB54E.3060003@univ-mlv.fr>
Message-ID: <CAEJX8oqYFcsraMCCe5ip+ueh73-Qr4m79-czDeQUrZMMSNd59A@mail.gmail.com>

R?mi
> * and yes, final field can be changed.
>
> Yeah, Cliff Click had a couple of posts regarding that. It's even used in
java.util.Random (unsafe.putObjectVolatile), albeit before the object
actually is 'published'.
It's hard to optimize and not break code.

Stanimir


>
>
>>
>>
>>  Thanks
>>>
>>> -Pete
>>>
>>>      public int remainingCapacity() {
>>>          final ReentrantLock lock = this.lock;
>>>          lock.lock();
>>>          try {
>>>              return items.length - count;
>>>          } finally {
>>>              lock.unlock();
>>>          }
>>>      }
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/4265f894/attachment.html>

From kirk at kodewerk.com  Tue Feb 12 16:52:52 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 12 Feb 2013 22:52:52 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
Message-ID: <F206B4F2-1126-499C-9ABB-FD6F93BEB4CF@kodewerk.com>


On 2013-02-12, at 10:24 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> For pure CPU bound work, I typically add 1 or maybe 2 more threads than # of hardware threads; this is to account for hardware threads possibly hitting a hard page fault and getting suspended.  I don't see how having any more than that threads benefits perf.
> 
> 

+1,

if you only 60-70% on a core you've got something bad happening. i've always been able to get close to 100% on what should CPU bound jobs when I get rid of bad stuff... that would include a lot of safe-pointing that goes on in the JVM.

Regards,
Kirk

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/04d55c68/attachment.html>

From ariel at weisberg.ws  Tue Feb 12 16:58:16 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Tue, 12 Feb 2013 16:58:16 -0500
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
Message-ID: <1360706296.8191.140661190571697.2B045E82@webmail.messagingengine.com>

Hi,

My experience benchmarking an in memory database that shards down to
the core level on 2x2x4 Nehalem was that the majority of the
performance comes from the first four threads, the next two give you
peak throughput on workloads where transaction size is small. For TPC-C
where transactions are large best performance is with 12 threads
although 8 threads is very close.

We recommend 2/3rds the number of physical cores (sans hyper-threading)
as a starting point to users.

We aren't doing anything NUMA aware although there is no memory shared
between shards on. Asynchronous networking has a dedicated thread for
sending/receiving messages.

I wonder if there is a difference between CPU compute bound tasks and
CPU memory bound tasks. I have noticed the execution time of
transactions increases as you add threads even though there is no state
shared between threads/transactions and there are fewer threads then
cores.

Thanks,
Ariel

On Tue, Feb 12, 2013, at 04:24 PM, Vitaly Davidovich wrote:

  For pure CPU bound work, I typically add 1 or maybe 2 more threads
  than # of hardware threads; this is to account for hardware threads
  possibly hitting a hard page fault and getting suspended.  I don't
  see how having any more than that threads benefits perf.

  Sent from my phone

On Feb 12, 2013 4:21 PM, "?iktor ?lang" <[1]viktor.klang at gmail.com>
wrote:

On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <[2]kirk at kodewerk.com>
wrote:

>
> Do you agree that thread pool sizing depends on type of work? (IO
bound vs CPU bound, bursty vs steady etc etc)

  Yes

> Do you agree that a JVM Thread is not a unit of parallelism?

  Yes

> Do you agree that having more JVM Threads than hardware threads is
bad for CPU-bound workloads?

  No, even with CPU bound workloads I have found that the hardware/OS
  is much better at managing many workloads across many threads than I
  am. So a few more threads is ok, many more threads is bad fast.


That's an interesting observation. Have any more data on that? (really
interested)
As I said earlier, for CPU-bound workloads we've seen the best
performance when only loading 60-70% of the cores (other threads exist
on the machine of course).

Cheers,
?

  Regards,
  Kirk


--
Viktor Klang
Director of Engineering
[3]Typesafe - The software stack for applications that scale
Twitter: @viktorklang

  _______________________________________________
  Concurrency-interest mailing list
  [4]Concurrency-interest at cs.oswego.edu
  [5]http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________

Concurrency-interest mailing list

[6]Concurrency-interest at cs.oswego.edu

[7]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. mailto:viktor.klang at gmail.com
2. mailto:kirk at kodewerk.com
3. http://www.typesafe.com/
4. mailto:Concurrency-interest at cs.oswego.edu
5. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
6. mailto:Concurrency-interest at cs.oswego.edu
7. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/4236eeec/attachment.html>

From oleksandr.otenko at oracle.com  Tue Feb 12 17:01:00 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 12 Feb 2013 22:01:00 +0000
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAHjP37E6hat842DWKQgwCzPs7cQonP-LMa+rjcKCLrGbshT+eA@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu> <511AB54E.3060003@univ-mlv.fr>
	<CAHjP37E6hat842DWKQgwCzPs7cQonP-LMa+rjcKCLrGbshT+eA@mail.gmail.com>
Message-ID: <511ABB9C.8030404@oracle.com>

This is a bizarre explanation.

The final field may not even reload after first read; or its value may 
even be compiled-in as a constant. If a framework modifies final fields, 
it is their problem.


Alex


On 12/02/2013 21:46, Vitaly Davidovich wrote:
>
> So Cliff Click had a few blogs on this.  Apparently, some popular 
> frameworks muck around with final fields (I.e. change them via 
> reflection).  One of his arguments was that enregistering final fields 
> isn't all that profitable since the 2nd load will most likely hit L1 
> and be only marginally slower. I can see that point, but I'd say it's 
> unfortunate that JIT writers can't liberally enregister final fields 
> due to someone doing bizarre things with them.
>
> Sent from my phone
>
> On Feb 12, 2013 4:40 PM, "Remi Forax" <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     On 02/12/2013 10:21 PM, Doug Lea wrote:
>
>         On 02/12/13 15:41, javamann at cox.net <mailto:javamann at cox.net>
>         wrote:
>
>             Stupid question time. While going through the code for an
>             ArrayBlockQueue I came across numerous instances where the
>             instance 'lock' is copied to a variable in a Method. I
>             know there is a reason for this, I just don't know what it is.
>
>
>         It's ultimately due to the fundamental mismatch between memory
>         models
>         and OOP :-)
>
>         Just about every method in all of j.u.c adopts the policy of
>         reading fields as locals whenever a value is used more than once.
>         This way you are sure which value applies when.
>         This is not often pretty, but is easier to visually verify.
>
>         The surprising case is doing this even for "final" fields.
>         This is because JVMs are not always smart enough to exploit
>         the fine points of the JMM and not reload read final
>         values, as they would otherwise need to do across the
>         volatile accesses entailed in locking. Some JVMs are smarter
>         than they used to be about this, but still not always
>         smart enough.
>
>
>     It's not exactly that the VM is not smart enough, it's that the VM
>     may not see enough.
>     In the example, the VM has to be able to inline the whole graph of
>     method behind the call lock.lock() to be sure that no method that
>     is part of this graph contains code that may change the value of
>     the field lock*. If the VM can prove that, the field doesn't need
>     to be re-loaded when calling lock.unlock().
>
>
>         -Doug
>
>
>     R?mi
>     * and yes, final field can be changed.
>
>
>
>
>             Thanks
>
>             -Pete
>
>                  public int remainingCapacity() {
>                      final ReentrantLock lock = this.lock;
>                      lock.lock();
>                      try {
>                          return items.length - count;
>                      } finally {
>                          lock.unlock();
>                      }
>                  }
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/2a8bb8bc/attachment-0001.html>

From hans.boehm at hp.com  Tue Feb 12 17:00:29 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 12 Feb 2013 22:00:29 +0000
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <511AB87B.1040303@oracle.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>

I wouldn?t be surprised if a memory-bandwidth-limited benchmark ran fastest with fewer threads than cores.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nathan Reynolds
Sent: Tuesday, February 12, 2013 1:48 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Some interesting (confusing?) benchmark results

> best performance when only loading 60-70% of the cores

What do you mean by performance?  Do you mean you achieve the highest throughput?  Do you mean you achieve the lowest response times?  Do you mean something else?

The early implementations of hyper-threading on Intel processors sometimes ran into trouble depending upon the workload.  Enabling hyper-threading actual hurt performance and throughput.  A lot of people quickly learned to disable hyper-threading.  They are so entrenched in that decision that it is hard to help them see that hyper-threading is actually beneficial now.

The Linux thread scheduler is smart enough to put 1 thread on each physical core first and then double up on physical cores.  So, I am not surprised that loading 60-70% cores yields best performance on the above mentioned processors.  This creates a few more threads than physical cores which in a way disables hyper-threading.

Later implementations of hyper-threading improved considerably.  I am not aware of any workloads which perform worse with hyper-threading enabled.  With a modern processor (i.e. Westmere or newer), it would be interesting if you ran your workload with hyper-threading enabled and disabled.  Then find the optimal thread count for each configuration.  If hyper-threading disabled performs better, then that definitely would be an interesting workload and result.
Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | Architect | 602.333.9091
Oracle PSR Engineering<http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 2:18 PM, ?iktor ?lang wrote:


On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>> wrote:
>
> Do you agree that thread pool sizing depends on type of work? (IO bound vs CPU bound, bursty vs steady etc etc)
Yes
> Do you agree that a JVM Thread is not a unit of parallelism?
Yes
> Do you agree that having more JVM Threads than hardware threads is bad for CPU-bound workloads?
No, even with CPU bound workloads I have found that the hardware/OS is much better at managing many workloads across many threads than I am. So a few more threads is ok, many more threads is bad fast.

That's an interesting observation. Have any more data on that? (really interested)
As I said earlier, for CPU-bound workloads we've seen the best performance when only loading 60-70% of the cores (other threads exist on the machine of course).

Cheers,

?


Regards,
Kirk



--
Viktor Klang
Director of Engineering


Typesafe<http://www.typesafe.com/> - The software stack for applications that scale
Twitter: @viktorklang




_______________________________________________

Concurrency-interest mailing list

Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>

http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/58b9bb17/attachment.html>

From vitalyd at gmail.com  Tue Feb 12 17:01:46 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 12 Feb 2013 17:01:46 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAEJX8oo_fF1NN-aCKoTV1_YsQ5PN69-YcBwoN53A3ajY1B5=MQ@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
	<CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
	<CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>
	<CAEJX8oo_fF1NN-aCKoTV1_YsQ5PN69-YcBwoN53A3ajY1B5=MQ@mail.gmail.com>
Message-ID: <CAHjP37G08brRZ4m2VY5hQM1kTFtSkshbUzkUq8YR7pdDT0fyuQ@mail.gmail.com>

Wow OK, that's very paranoid :).  Moving the address into a register
doesn't help though as I mentioned earlier; only scalar loads would benefit
from this.

Sent from my phone
On Feb 12, 2013 4:43 PM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

> On Tue, Feb 12, 2013 at 11:37 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> Do you mean false sharing due to the lock word sitting on same cache line
>> as the lock object itself?
>>
> Nope, just the ArrayBlockingQueue.lock being on the same cache line as any
> other object. Changing "an int" field on that object would invalidate the
> cache line holding the lock.
>
> Stanimir
>
>
>> Sure, but storing a pointer in a register is not going to help since
>> ultimately you have to fetch the value from memory address that's stored in
>> the register (some offset from that to find the lock word), and if cache
>> line is gone, you get hit anyway.  In other words, you have to deref the
>> value in the register - it's not storing a scalar value.  Or did I
>> misunderstand your point?
>>
>> Sent from my phone
>> On Feb 12, 2013 4:22 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
>> wrote:
>>
>>> This should be in some register (ECX on 32bit x86) and of course it
>>> still has to deref./load 'lock'. Calling the method may not need to
>>> derefence this.
>>> However the 2nd load could be avoided (and possible false sharing
>>> prevented) if the value is stored on the stack/kept in register. The
>>> current method body is too short to actually worry about since there would
>>> be likely enough registers available. Yet, the idiom is used everywhere, so
>>> the current case just follows suits.
>>>
>>> Stanimir
>>>
>>> On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> I don't think it's that since "this" is already loaded (or else you
>>>> can't call the method).  I believe Doug did this because he found that
>>>> final field loads weren't commoned across lock() calls (even though they
>>>> could be).
>>>>
>>>> Sent from my phone
>>>> On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>> wrote:
>>>>
>>>>>  this.lock is a memory indirection (i.e. dereferences "this" to get
>>>>> the value in "lock") and could incur a cache miss (i.e. loads
>>>>> ArrayBlockQueue into the L1 cache) or even worse false sharing.  By copying
>>>>> to the local variable, the value is on the stack.  There won't be any
>>>>> memory indirection to access the value.  Cache misses would only happen if
>>>>> the thread context switched.  False sharing is impossible.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>>>>>
>>>>> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>>>>
>>>>> Thanks
>>>>>
>>>>> -Pete
>>>>>
>>>>>     public int remainingCapacity() {
>>>>>         final ReentrantLock lock = this.lock;
>>>>>         lock.lock();
>>>>>         try {
>>>>>             return items.length - count;
>>>>>         } finally {
>>>>>             lock.unlock();
>>>>>         }
>>>>>     }
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/edbc61ed/attachment-0001.html>

From thurston at nomagicsoftware.com  Tue Feb 12 17:04:01 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Tue, 12 Feb 2013 14:04:01 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <1360706296.8191.140661190571697.2B045E82@webmail.messagingengine.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
	<1360706296.8191.140661190571697.2B045E82@webmail.messagingengine.com>
Message-ID: <75cd99c3aca85d822acdf04904c0869d@nomagicsoftware.com>

  Even when those additional threads are idle?

> I have noticed the execution time of
> transactions increases as you add threads even though there is no
> state shared between threads/transactions and there are fewer threads
> then cores.
>
>  Thanks,
>  Ariel
>
>  On Tue, Feb 12, 2013, at 04:24 PM, Vitaly Davidovich wrote:
>
>> For pure CPU bound work, I typically add 1 or maybe 2 more threads 
>> than # of hardware threads; this is to account for hardware threads 
>> possibly hitting a hard page fault and getting suspended. I don't see 
>> how having any more than that threads benefits perf.
>>
>> Sent from my phone
>> On Feb 12, 2013 4:21 PM, "?iktor ?lang" <viktor.klang at gmail.com> 
>> wrote:
>>
>>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine 
>>> <kirk at kodewerk.com> wrote:
>>>
>>>>>
>>>> > Do you agree that thread pool sizing depends on type of work? 
>>>> (IO bound vs CPU bound, bursty vs steady etc etc) Yes
>>>>
>>>> > Do you agree that a JVM Thread is not a unit of parallelism? Yes
>>>>
>>>> > Do you agree that having more JVM Threads than hardware threads 
>>>> is bad for CPU-bound workloads? No, even with CPU bound workloads I 
>>>> have found that the hardware/OS is much better at managing many 
>>>> workloads across many threads than I am. So a few more threads is 
>>>> ok, many more threads is bad fast.
>>>
>>> That's an interesting observation. Have any more data on that? 
>>> (really interested)
>>> As I said earlier, for CPU-bound workloads we've seen the best 
>>> performance when only loading 60-70% of the cores (other threads 
>>> exist on the machine of course).
>>>
>>> Cheers,
>>>
>>> ?
>>>
>>>> Regards,
>>>> Kirk
>>>
>>> --
>>>
>>> Viktor Klang
>>> Director of Engineering
>>>
>>> Typesafe [1] - The software stack for applications that scale
>>> Twitter: @viktorklang
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [2]
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [2]
>
>
>
> Links:
> ------
> [1] http://www.typesafe.com/
> [2] http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Tue Feb 12 17:07:07 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 13 Feb 2013 00:07:07 +0200
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAHjP37G08brRZ4m2VY5hQM1kTFtSkshbUzkUq8YR7pdDT0fyuQ@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
	<CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
	<CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>
	<CAEJX8oo_fF1NN-aCKoTV1_YsQ5PN69-YcBwoN53A3ajY1B5=MQ@mail.gmail.com>
	<CAHjP37G08brRZ4m2VY5hQM1kTFtSkshbUzkUq8YR7pdDT0fyuQ@mail.gmail.com>
Message-ID: <CAEJX8oo=9OOA3y_QF0WcL+1Kz0+BjQZk+7ojxeRGHdQQDezFjA@mail.gmail.com>

I am not sure I understand why the ReentrantLock.sync field should be on
the same cache line as ABQ.lock field, it's possible but not necessary true.

Stanimir

On Wed, Feb 13, 2013 at 12:01 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Wow OK, that's very paranoid :).  Moving the address into a register
> doesn't help though as I mentioned earlier; only scalar loads would benefit
> from this.
>
> Sent from my phone
> On Feb 12, 2013 4:43 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
> wrote:
>
>> On Tue, Feb 12, 2013 at 11:37 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>
>>> Do you mean false sharing due to the lock word sitting on same cache
>>> line as the lock object itself?
>>>
>> Nope, just the ArrayBlockingQueue.lock being on the same cache line as
>> any other object. Changing "an int" field on that object would invalidate
>> the cache line holding the lock.
>>
>> Stanimir
>>
>>
>>> Sure, but storing a pointer in a register is not going to help since
>>> ultimately you have to fetch the value from memory address that's stored in
>>> the register (some offset from that to find the lock word), and if cache
>>> line is gone, you get hit anyway.  In other words, you have to deref the
>>> value in the register - it's not storing a scalar value.  Or did I
>>> misunderstand your point?
>>>
>>> Sent from my phone
>>> On Feb 12, 2013 4:22 PM, "Stanimir Simeonoff" <stanimir at riflexo.com>
>>> wrote:
>>>
>>>> This should be in some register (ECX on 32bit x86) and of course it
>>>> still has to deref./load 'lock'. Calling the method may not need to
>>>> derefence this.
>>>> However the 2nd load could be avoided (and possible false sharing
>>>> prevented) if the value is stored on the stack/kept in register. The
>>>> current method body is too short to actually worry about since there would
>>>> be likely enough registers available. Yet, the idiom is used everywhere, so
>>>> the current case just follows suits.
>>>>
>>>> Stanimir
>>>>
>>>> On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>>
>>>>> I don't think it's that since "this" is already loaded (or else you
>>>>> can't call the method).  I believe Doug did this because he found that
>>>>> final field loads weren't commoned across lock() calls (even though they
>>>>> could be).
>>>>>
>>>>> Sent from my phone
>>>>> On Feb 12, 2013 4:04 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
>>>>> wrote:
>>>>>
>>>>>>  this.lock is a memory indirection (i.e. dereferences "this" to get
>>>>>> the value in "lock") and could incur a cache miss (i.e. loads
>>>>>> ArrayBlockQueue into the L1 cache) or even worse false sharing.  By copying
>>>>>> to the local variable, the value is on the stack.  There won't be any
>>>>>> memory indirection to access the value.  Cache misses would only happen if
>>>>>> the thread context switched.  False sharing is impossible.
>>>>>>
>>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>>> 602.333.9091
>>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>>>>> Technology
>>>>>>  On 2/12/2013 1:41 PM, javamann at cox.net wrote:
>>>>>>
>>>>>> Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>>>>>
>>>>>> Thanks
>>>>>>
>>>>>> -Pete
>>>>>>
>>>>>>     public int remainingCapacity() {
>>>>>>         final ReentrantLock lock = this.lock;
>>>>>>         lock.lock();
>>>>>>         try {
>>>>>>             return items.length - count;
>>>>>>         } finally {
>>>>>>             lock.unlock();
>>>>>>         }
>>>>>>     }
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/5f9156c8/attachment.html>

From nathan.reynolds at oracle.com  Tue Feb 12 17:24:12 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 12 Feb 2013 15:24:12 -0700
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CAEJX8oo=9OOA3y_QF0WcL+1Kz0+BjQZk+7ojxeRGHdQQDezFjA@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AAD5D.6070902@oracle.com>
	<CAHjP37Er3v=AMS4pzj7+Z_EPAtAezNnA6Db4=w6YwU4-1n443A@mail.gmail.com>
	<CAEJX8oq7eXLbWQseVRvMfAxHiZRexpT3jtns7ttwvJWR777hRQ@mail.gmail.com>
	<CAHjP37HPp5M4rfxWpMZqcwRahEWWwoX2i7UkvtQM8UPAjGuh-Q@mail.gmail.com>
	<CAEJX8oo_fF1NN-aCKoTV1_YsQ5PN69-YcBwoN53A3ajY1B5=MQ@mail.gmail.com>
	<CAHjP37G08brRZ4m2VY5hQM1kTFtSkshbUzkUq8YR7pdDT0fyuQ@mail.gmail.com>
	<CAEJX8oo=9OOA3y_QF0WcL+1Kz0+BjQZk+7ojxeRGHdQQDezFjA@mail.gmail.com>
Message-ID: <511AC10C.8000200@oracle.com>

In one case where I hit false sharing, it was because 2 (C++) objects 
were next to each other and both were updated very frequently.  All I 
had to do is move them to their separate corners and they behaved much 
better.  :)

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 3:07 PM, Stanimir Simeonoff wrote:
> I am not sure I understand why the ReentrantLock.sync field should be 
> on the same cache line as ABQ.lock field, it's possible but not 
> necessary true.
>
> Stanimir
>
> On Wed, Feb 13, 2013 at 12:01 AM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     Wow OK, that's very paranoid :).  Moving the address into a
>     register doesn't help though as I mentioned earlier; only scalar
>     loads would benefit from this.
>
>     Sent from my phone
>
>     On Feb 12, 2013 4:43 PM, "Stanimir Simeonoff"
>     <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>         On Tue, Feb 12, 2013 at 11:37 PM, Vitaly Davidovich
>         <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>             Do you mean false sharing due to the lock word sitting on
>             same cache line as the lock object itself?
>
>         Nope, just the ArrayBlockingQueue.lock being on the same cache
>         line as any other object. Changing "an int" field on that
>         object would invalidate the cache line holding the lock.
>
>         Stanimir
>
>             Sure, but storing a pointer in a register is not going to
>             help since ultimately you have to fetch the value from
>             memory address that's stored in the register (some offset
>             from that to find the lock word), and if cache line is
>             gone, you get hit anyway.  In other words, you have to
>             deref the value in the register - it's not storing a
>             scalar value.  Or did I misunderstand your point?
>
>             Sent from my phone
>
>             On Feb 12, 2013 4:22 PM, "Stanimir Simeonoff"
>             <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>                 This should be in some register (ECX on 32bit x86) and
>                 of course it still has to deref./load 'lock'. Calling
>                 the method may not need to derefence this.
>                 However the 2nd load could be avoided (and possible
>                 false sharing prevented) if the value is stored on the
>                 stack/kept in register. The current method body is too
>                 short to actually worry about since there would be
>                 likely enough registers available. Yet, the idiom is
>                 used everywhere, so the current case just follows suits.
>
>                 Stanimir
>
>                 On Tue, Feb 12, 2013 at 11:10 PM, Vitaly Davidovich
>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>                     I don't think it's that since "this" is already
>                     loaded (or else you can't call the method).  I
>                     believe Doug did this because he found that final
>                     field loads weren't commoned across lock() calls
>                     (even though they could be).
>
>                     Sent from my phone
>
>                     On Feb 12, 2013 4:04 PM, "Nathan Reynolds"
>                     <nathan.reynolds at oracle.com
>                     <mailto:nathan.reynolds at oracle.com>> wrote:
>
>                         this.lock is a memory indirection (i.e.
>                         dereferences "this" to get the value in
>                         "lock") and could incur a cache miss (i.e.
>                         loads ArrayBlockQueue into the L1 cache) or
>                         even worse false sharing.  By copying to the
>                         local variable, the value is on the stack.
>                         There won't be any memory indirection to
>                         access the value.  Cache misses would only
>                         happen if the thread context switched.  False
>                         sharing is impossible.
>
>                         Nathan Reynolds
>                         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>                         | Architect | 602.333.9091 <tel:602.333.9091>
>                         Oracle PSR Engineering
>                         <http://psr.us.oracle.com/> | Server Technology
>                         On 2/12/2013 1:41 PM, javamann at cox.net
>                         <mailto:javamann at cox.net> wrote:
>>                         Stupid question time. While going through the code for an ArrayBlockQueue I came across numerous instances where the instance 'lock' is copied to a variable in a Method. I know there is a reason for this, I just don't know what it is.
>>
>>                         Thanks
>>
>>                         -Pete
>>
>>                              public int remainingCapacity() {
>>                                  final ReentrantLock lock = this.lock;
>>                                  lock.lock();
>>                                  try {
>>                                      return items.length - count;
>>                                  } finally {
>>                                      lock.unlock();
>>                                  }
>>                              }
>>
>>                         _______________________________________________
>>                         Concurrency-interest mailing list
>>                         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                         _______________________________________________
>                         Concurrency-interest mailing list
>                         Concurrency-interest at cs.oswego.edu
>                         <mailto:Concurrency-interest at cs.oswego.edu>
>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/e884f1f0/attachment-0001.html>

From zhong.j.yu at gmail.com  Tue Feb 12 17:26:52 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Tue, 12 Feb 2013 16:26:52 -0600
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511AB253.1060208@cs.oswego.edu>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
Message-ID: <CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>

On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 02/12/13 15:41, javamann at cox.net wrote:
>>
>> Stupid question time. While going through the code for an ArrayBlockQueue
>> I came across numerous instances where the instance 'lock' is copied to a
>> variable in a Method. I know there is a reason for this, I just don't know
>> what it is.
>>
>
> It's ultimately due to the fundamental mismatch between memory models
> and OOP :-)
>
> Just about every method in all of j.u.c adopts the policy of
> reading fields as locals whenever a value is used more than once.
> This way you are sure which value applies when.
> This is not often pretty, but is easier to visually verify.
>
> The surprising case is doing this even for "final" fields.
> This is because JVMs are not always smart enough to exploit
> the fine points of the JMM and not reload read final
> values, as they would otherwise need to do across the
> volatile accesses entailed in locking. Some JVMs are smarter
> than they used to be about this, but still not always
> smart enough.

(Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
always cache the value of a final field? What if `this` was leaked
before constructor exit? And if the 1st read sees a non-null value,
can that value then be cached safely?

Zhong Yu


>
> -Doug
>
>
>
>
>> Thanks
>>
>> -Pete
>>
>>      public int remainingCapacity() {
>>          final ReentrantLock lock = this.lock;
>>          lock.lock();
>>          try {
>>              return items.length - count;
>>          } finally {
>>              lock.unlock();
>>          }
>>      }
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From ariel at weisberg.ws  Tue Feb 12 17:31:50 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Tue, 12 Feb 2013 17:31:50 -0500
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <75cd99c3aca85d822acdf04904c0869d@nomagicsoftware.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<CAHjP37EuSK7pynR_DX1BF+dSM9ywgcg=XOfB7yH4+viMws5H4Q@mail.gmail.com>
	<1360706296.8191.140661190571697.2B045E82@webmail.messagingengine.com>
	<75cd99c3aca85d822acdf04904c0869d@nomagicsoftware.com>
Message-ID: <1360708310.14670.140661190596901.1995B25F@webmail.messagingengine.com>

Hi,

Sorry, I am talking about active threads since there is a thread per
shard and you can choose how many you want to have. 10s of idle threads
seems to have no negative impact, and I have not experimented with more
then that.

Thanks,
Ariel

On Tue, Feb 12, 2013, at 05:04 PM, thurston at nomagicsoftware.com wrote:
>   Even when those additional threads are idle?
> 
> > I have noticed the execution time of
> > transactions increases as you add threads even though there is no
> > state shared between threads/transactions and there are fewer threads
> > then cores.
> >
> >  Thanks,
> >  Ariel
> >
> >  On Tue, Feb 12, 2013, at 04:24 PM, Vitaly Davidovich wrote:
> >
> >> For pure CPU bound work, I typically add 1 or maybe 2 more threads 
> >> than # of hardware threads; this is to account for hardware threads 
> >> possibly hitting a hard page fault and getting suspended. I don't see 
> >> how having any more than that threads benefits perf.
> >>
> >> Sent from my phone
> >> On Feb 12, 2013 4:21 PM, "?iktor ?lang" <viktor.klang at gmail.com> 
> >> wrote:
> >>
> >>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine 
> >>> <kirk at kodewerk.com> wrote:
> >>>
> >>>>>
> >>>> > Do you agree that thread pool sizing depends on type of work? 
> >>>> (IO bound vs CPU bound, bursty vs steady etc etc) Yes
> >>>>
> >>>> > Do you agree that a JVM Thread is not a unit of parallelism? Yes
> >>>>
> >>>> > Do you agree that having more JVM Threads than hardware threads 
> >>>> is bad for CPU-bound workloads? No, even with CPU bound workloads I 
> >>>> have found that the hardware/OS is much better at managing many 
> >>>> workloads across many threads than I am. So a few more threads is 
> >>>> ok, many more threads is bad fast.
> >>>
> >>> That's an interesting observation. Have any more data on that? 
> >>> (really interested)
> >>> As I said earlier, for CPU-bound workloads we've seen the best 
> >>> performance when only loading 60-70% of the cores (other threads 
> >>> exist on the machine of course).
> >>>
> >>> Cheers,
> >>>
> >>> ?
> >>>
> >>>> Regards,
> >>>> Kirk
> >>>
> >>> --
> >>>
> >>> Viktor Klang
> >>> Director of Engineering
> >>>
> >>> Typesafe [1] - The software stack for applications that scale
> >>> Twitter: @viktorklang
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [2]
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [2]
> >
> >
> >
> > Links:
> > ------
> > [1] http://www.typesafe.com/
> > [2] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From nathan.reynolds at oracle.com  Tue Feb 12 18:09:52 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 12 Feb 2013 16:09:52 -0700
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
Message-ID: <511ACBC0.5080500@oracle.com>

Currently, JIT's visibility is as far as it can inline.  It can't see 
any further than that.  Couldn't JIT keep notes on the methods for 
whether (1) if parameters or the return value escape, (2) which fields 
are modified, (3) anything else useful for cross method optimization?  
With these notes, JIT wouldn't have to inline the entire method tree to 
be able to see that the final lock field isn't modified.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/12/2013 3:26 PM, Zhong Yu wrote:
> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>> On 02/12/13 15:41, javamann at cox.net wrote:
>>> Stupid question time. While going through the code for an ArrayBlockQueue
>>> I came across numerous instances where the instance 'lock' is copied to a
>>> variable in a Method. I know there is a reason for this, I just don't know
>>> what it is.
>>>
>> It's ultimately due to the fundamental mismatch between memory models
>> and OOP :-)
>>
>> Just about every method in all of j.u.c adopts the policy of
>> reading fields as locals whenever a value is used more than once.
>> This way you are sure which value applies when.
>> This is not often pretty, but is easier to visually verify.
>>
>> The surprising case is doing this even for "final" fields.
>> This is because JVMs are not always smart enough to exploit
>> the fine points of the JMM and not reload read final
>> values, as they would otherwise need to do across the
>> volatile accesses entailed in locking. Some JVMs are smarter
>> than they used to be about this, but still not always
>> smart enough.
> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
> always cache the value of a final field? What if `this` was leaked
> before constructor exit? And if the 1st read sees a non-null value,
> can that value then be cached safely?
>
> Zhong Yu
>
>
>> -Doug
>>
>>
>>
>>
>>> Thanks
>>>
>>> -Pete
>>>
>>>       public int remainingCapacity() {
>>>           final ReentrantLock lock = this.lock;
>>>           lock.lock();
>>>           try {
>>>               return items.length - count;
>>>           } finally {
>>>               lock.unlock();
>>>           }
>>>       }
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130212/8e870b08/attachment.html>

From viktor.klang at gmail.com  Wed Feb 13 03:29:21 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 13 Feb 2013 09:29:21 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
Message-ID: <CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>

Yup, that's very true. Being message oriented we really have no "use-case"
which _only_ involves ops on registers.

Cheers,
?


On Tue, Feb 12, 2013 at 11:00 PM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  I wouldn?t be surprised if a memory-bandwidth-limited benchmark ran
> fastest with fewer threads than cores.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Nathan Reynolds
> *Sent:* Tuesday, February 12, 2013 1:48 PM
>
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Some interesting (confusing?)
> benchmark results****
>
>  ** **
>
> > best performance when only loading 60-70% of the cores
>
>
> What do you mean by performance?  Do you mean you achieve the highest
> throughput?  Do you mean you achieve the lowest response times?  Do you
> mean something else?
>
> The early implementations of hyper-threading on Intel processors sometimes
> ran into trouble depending upon the workload.  Enabling hyper-threading
> actual hurt performance and throughput.  A lot of people quickly learned to
> disable hyper-threading.  They are so entrenched in that decision that it
> is hard to help them see that hyper-threading is actually beneficial now.
>
> The Linux thread scheduler is smart enough to put 1 thread on each
> physical core first and then double up on physical cores.  So, I am not
> surprised that loading 60-70% cores yields best performance on the above
> mentioned processors.  This creates a few more threads than physical cores
> which in a way disables hyper-threading.
>
> Later implementations of hyper-threading improved considerably.  I am not
> aware of any workloads which perform worse with hyper-threading enabled.
> With a modern processor (i.e. Westmere or newer), it would be interesting
> if you ran your workload with hyper-threading enabled and disabled.  Then
> find the optimal thread count for each configuration.  If hyper-threading
> disabled performs better, then that definitely would be an interesting
> workload and result.****
>
>  Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology****
>
> On 2/12/2013 2:18 PM, ?iktor ?lang wrote:****
>
>  ** **
>
> ** **
>
> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>
> wrote:****
>
> >
> > Do you agree that thread pool sizing depends on type of work? (IO bound
> vs CPU bound, bursty vs steady etc etc)****
>
> Yes****
>
> > Do you agree that a JVM Thread is not a unit of parallelism?****
>
> Yes****
>
> > Do you agree that having more JVM Threads than hardware threads is bad
> for CPU-bound workloads?****
>
> No, even with CPU bound workloads I have found that the hardware/OS is
> much better at managing many workloads across many threads than I am. So a
> few more threads is ok, many more threads is bad fast.****
>
> ** **
>
> That's an interesting observation. Have any more data on that? (really
> interested)****
>
> As I said earlier, for CPU-bound workloads we've seen the best performance
> when only loading 60-70% of the cores (other threads exist on the machine
> of course).****
>
> ** **
>
> Cheers,****
>
>
> ?****
>
>  ****
>
>
> Regards,
> Kirk****
>
>
>
> ****
>
> ** **
>
> -- ****
>
> *Viktor Klang*
> *Director of Engineering* ****
>
> *
>
> *****
>
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang****
>
>
>
>
> ****
>
> _______________________________________________****
>
> Concurrency-interest mailing list****
>
> Concurrency-interest at cs.oswego.edu****
>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>
>  ** **
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/4f1d1ac9/attachment-0001.html>

From viktor.klang at gmail.com  Wed Feb 13 03:33:59 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 13 Feb 2013 09:33:59 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <511AB87B.1040303@oracle.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
Message-ID: <CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>

On Tue, Feb 12, 2013 at 10:47 PM, Nathan Reynolds <
nathan.reynolds at oracle.com> wrote:

>  > best performance when only loading 60-70% of the cores
>
> What do you mean by performance?  Do you mean you achieve the highest
> throughput?
>
Do you mean you achieve the lowest response times?  Do you mean something
> else?
>

Highest throughput.


>
> The early implementations of hyper-threading on Intel processors sometimes
> ran into trouble depending upon the workload.  Enabling hyper-threading
> actual hurt performance and throughput.  A lot of people quickly learned to
> disable hyper-threading.  They are so entrenched in that decision that it
> is hard to help them see that hyper-threading is actually beneficial now.
>

This resonates very well with my experience.


>
> The Linux thread scheduler is smart enough to put 1 thread on each
> physical core first and then double up on physical cores.  So, I am not
> surprised that loading 60-70% cores yields best performance on the above
> mentioned processors.  This creates a few more threads than physical cores
> which in a way disables hyper-threading.
>

Also, in my experience the workload type is essentially everything when it
comes to what you can get out of hyperthreading.


>
> Later implementations of hyper-threading improved considerably.  I am not
> aware of any workloads which perform worse with hyper-threading enabled.
>

Me neither. But a hyper thread is not equivalent to a "real" core.


> With a modern processor (i.e. Westmere or newer), it would be interesting
> if you ran your workload with hyper-threading enabled and disabled.  Then
> find the optimal thread count for each configuration.  If hyper-threading
> disabled performs better, then that definitely would be an interesting
> workload and result.
>

Is there a way to know how many physical processors are available on the
JVM?

Cheers,
?


>
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>
>
>
>
> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>
>> >
>> > Do you agree that thread pool sizing depends on type of work? (IO bound
>> vs CPU bound, bursty vs steady etc etc)
>>  Yes
>> > Do you agree that a JVM Thread is not a unit of parallelism?
>>  Yes
>> > Do you agree that having more JVM Threads than hardware threads is bad
>> for CPU-bound workloads?
>>  No, even with CPU bound workloads I have found that the hardware/OS is
>> much better at managing many workloads across many threads than I am. So a
>> few more threads is ok, many more threads is bad fast.
>>
>
>  That's an interesting observation. Have any more data on that? (really
> interested)
> As I said earlier, for CPU-bound workloads we've seen the best performance
> when only loading 60-70% of the cores (other threads exist on the machine
> of course).
>
>  Cheers,
>
> ?
>
>
>>
>> Regards,
>> Kirk
>
>
>
>
>  --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/a6ebdaac/attachment.html>

From javamann at cox.net  Wed Feb 13 03:48:18 2013
From: javamann at cox.net (Pete Haidinyak)
Date: Wed, 13 Feb 2013 00:48:18 -0800
Subject: [concurrency-interest] Thread Allocation
In-Reply-To: <CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
	<CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>
Message-ID: <op.wsfr2spn7o0gc3@mail.cox.net>

I have a question on how to allocate Threads. I am creating a SIEM which is a bunch of independent Java Services. The most likely use case is this will run on one 2U box. The box will have two quad core Xeon processors and 32G of RAM. Some of the Services will be I/O bound but some will be CPU bound.
   In one of the latest discussion it was mentions that you should allocate a Thread for each core (plus or minus a couple) for the best throughput. I have the ability to turn the Thread Pools after startup based on the number and types of Services running on the box.

My question is what would be the best way to allocate Threads when you have multiple processes competing for resources?

Thanks

-Pete
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/c1dbdf84/attachment.html>

From mr.chrisvest at gmail.com  Wed Feb 13 04:34:42 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Wed, 13 Feb 2013 10:34:42 +0100
Subject: [concurrency-interest] Thread Allocation
In-Reply-To: <op.wsfr2spn7o0gc3@mail.cox.net>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
	<CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>
	<op.wsfr2spn7o0gc3@mail.cox.net>
Message-ID: <B4F600C5-62CE-427B-A626-366004323701@gmail.com>

If you can tell before hand which tasks (that you submit to the thread pools) are going to be IO bound and which are going to be CPU bound, then you can have to separate thread pools: a big one for the IO bound tasks and a small one for he CPU bound ones.

Otherwise I'd say just set a high upper bound (upwards hundreds, but depends on expected distribution) and let the OS manage things, see how that works and if its performant enough, then you're done.

Note that I have no idea what kind of performance is expected of your SIEM system.

Chris

On 13/02/2013, at 09.48, "Pete Haidinyak" <javamann at cox.net> wrote:

> I have a question on how to allocate Threads. I am creating a SIEM which is a bunch of independent Java Services. The most likely use case is this will run on one 2U box. The box will have two quad core Xeon processors and 32G of RAM. Some of the Services will be I/O bound but some will be CPU bound.
>   In one of the latest discussion it was mentions that you should allocate a Thread for each core (plus or minus a couple) for the best throughput. I have the ability to turn the Thread Pools after startup based on the number and types of Services running on the box.
> 
> My question is what would be the best way to allocate Threads when you have multiple processes competing for resources?
> 
> Thanks
> 
> -Pete
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/e49d5bf3/attachment.html>

From william.louth at jinspired.com  Wed Feb 13 04:39:54 2013
From: william.louth at jinspired.com (william.louth at jinspired.com)
Date: Wed, 13 Feb 2013 09:39:54 +0000
Subject: [concurrency-interest] Thread Allocation
Message-ID: <W4727327463121681360748394@webmail108>


>From my perspective I see allocation as a form of capacity planning (or provision) - global resource limits, a cap on consumption irrespective of the service itself. In this way it is possible to over provision or over subscribe a managed resource.

The actual consumption (or assignment) of the capacity at a service level is a different matter and can be very distinct from what capacity is available in the thread pool. Here we look to control the consumption through resource reservation and its policing based on limits, policies and dynamic expansion/contraction per classified service using rate limiting, quotas, adaptive valves, caps, prioritization,....all of which can introduce delay or suspension of thread execution. 

Google recently published on performance variability which touches on this under the heading "Differentiating service classes and higher-level queuing".

I wrote about this just yesterday 
http://www.jinspired.com/site/google-engineering-on-performance-variability-the-tail-at-scale

For the optimal level the way to go is metered control valves: 
http://www.jinspired.com/research/adaptive-control-in-execution

The key strategy here is to design or instrument the system in such a way that you can influence the system dynamics (which we can't always be sure of) to do the right thing whether that is for better throughput, response time, safety or resource utilization.

http://www.jinspired.com/site/using-system-dynamics-for-effective-concurrency-consumption-control-of-code

William

>-----Original Message-----
>From: Pete Haidinyak [mailto:javamann at cox.net]
>Sent: Wednesday, February 13, 2013 09:48 AM
>To: concurrency-interest at cs.oswego.edu
>Subject: [concurrency-interest] Thread Allocation
>
>I have a question on how to allocate Threads. I am creating a SIEM which is a bunch of independent Java Services. The most likely use case is this will run on one 2U box. The box will have two quad core Xeon processors and 32G of RAM. Some of the Services will be I/O bound but some will be CPU bound.
>   In one of the latest discussion it was mentions that you should allocate a Thread for each core (plus or minus a couple) for the best throughput. I have the ability to turn the Thread Pools after startup based on the number and types of Services running on the box.
>
>My question is what would be the best way to allocate Threads when you have multiple processes competing for resources?
>
>Thanks
>
>-Pete




From stanimir at riflexo.com  Wed Feb 13 05:45:25 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 13 Feb 2013 12:45:25 +0200
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
Message-ID: <CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>

> Is there a way to know how many physical processors are available on the
> JVM?
>
> On linux you can use taskset -p <pid>
(pid you can retrieve form /proc/self )
and you can use /proc/cpuinfo to determine which processors have siblings.

Quite inconvenient overall. So kernels might have /proc/self/affinity which
would make life easier .
And of course there is JNI and syscall.

Stanimir



> Cheers,
> ?
>
>
>>
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>  On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>>
>>
>>
>>
>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>
>>> >
>>> > Do you agree that thread pool sizing depends on type of work? (IO
>>> bound vs CPU bound, bursty vs steady etc etc)
>>>  Yes
>>> > Do you agree that a JVM Thread is not a unit of parallelism?
>>>  Yes
>>> > Do you agree that having more JVM Threads than hardware threads is bad
>>> for CPU-bound workloads?
>>>  No, even with CPU bound workloads I have found that the hardware/OS is
>>> much better at managing many workloads across many threads than I am. So a
>>> few more threads is ok, many more threads is bad fast.
>>>
>>
>>  That's an interesting observation. Have any more data on that? (really
>> interested)
>> As I said earlier, for CPU-bound workloads we've seen the best
>> performance when only loading 60-70% of the cores (other threads exist on
>> the machine of course).
>>
>>  Cheers,
>>
>> ?
>>
>>
>>>
>>> Regards,
>>> Kirk
>>
>>
>>
>>
>>  --
>> *Viktor Klang*
>> *Director of Engineering*
>> *
>> *
>> Typesafe <http://www.typesafe.com/> - The software stack for
>> applications that scale
>> Twitter: @viktorklang
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/f32c8282/attachment.html>

From oleksandr.otenko at oracle.com  Wed Feb 13 05:45:32 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 13 Feb 2013 10:45:32 +0000
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511ACBC0.5080500@oracle.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com>
Message-ID: <511B6ECC.6000501@oracle.com>

I don't get this. Why have a final, if you need to keep track of illegal 
attempts to modify it via Unsafe / JNI / reflection? Is this really how 
finals are implemented, ie don't assume they are final?

I understand when you do it the other way around - assume a unmarked 
field is final, until someone attempts to modify the field.


Alex


On 12/02/2013 23:09, Nathan Reynolds wrote:
> Currently, JIT's visibility is as far as it can inline.  It can't see 
> any further than that.  Couldn't JIT keep notes on the methods for 
> whether (1) if parameters or the return value escape, (2) which fields 
> are modified, (3) anything else useful for cross method optimization?  
> With these notes, JIT wouldn't have to inline the entire method tree 
> to be able to see that the final lock field isn't modified.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea<dl at cs.oswego.edu>  wrote:
>>> On 02/12/13 15:41,javamann at cox.net  wrote:
>>>> Stupid question time. While going through the code for an ArrayBlockQueue
>>>> I came across numerous instances where the instance 'lock' is copied to a
>>>> variable in a Method. I know there is a reason for this, I just don't know
>>>> what it is.
>>>>
>>> It's ultimately due to the fundamental mismatch between memory models
>>> and OOP :-)
>>>
>>> Just about every method in all of j.u.c adopts the policy of
>>> reading fields as locals whenever a value is used more than once.
>>> This way you are sure which value applies when.
>>> This is not often pretty, but is easier to visually verify.
>>>
>>> The surprising case is doing this even for "final" fields.
>>> This is because JVMs are not always smart enough to exploit
>>> the fine points of the JMM and not reload read final
>>> values, as they would otherwise need to do across the
>>> volatile accesses entailed in locking. Some JVMs are smarter
>>> than they used to be about this, but still not always
>>> smart enough.
>> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
>> always cache the value of a final field? What if `this` was leaked
>> before constructor exit? And if the 1st read sees a non-null value,
>> can that value then be cached safely?
>>
>> Zhong Yu
>>
>>
>>> -Doug
>>>
>>>
>>>
>>>
>>>> Thanks
>>>>
>>>> -Pete
>>>>
>>>>       public int remainingCapacity() {
>>>>           final ReentrantLock lock = this.lock;
>>>>           lock.lock();
>>>>           try {
>>>>               return items.length - count;
>>>>           } finally {
>>>>               lock.unlock();
>>>>           }
>>>>       }
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/eb2af89e/attachment.html>

From nitsanw at yahoo.com  Wed Feb 13 06:04:48 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 13 Feb 2013 03:04:48 -0800 (PST)
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511B6ECC.6000501@oracle.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com> <511B6ECC.6000501@oracle.com>
Message-ID: <1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>

See this : http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
And the follow up: http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
Final fields are not treated as final because sometimes (as in: popular use case you can't ignore) they are not... 




________________________________
 From: oleksandr otenko <oleksandr.otenko at oracle.com>
To: concurrency-interest at cs.oswego.edu 
Sent: Wednesday, February 13, 2013 10:45 AM
Subject: Re: [concurrency-interest] Stupid Question
 

I don't get this. Why have a final, if you need to keep track of illegal attempts to modify it via Unsafe / JNI / reflection? Is this really how finals are implemented, ie don't assume they are final?

I understand when you do it the other way around - assume a unmarked
    field is final, until someone attempts to modify the field.


Alex



On 12/02/2013 23:09, Nathan Reynolds wrote:

Currently, JIT's visibility is as far as it can inline.? It can't see any further than that.? Couldn't JIT keep notes on the methods for whether (1) if parameters or the return value escape, (2) which fields are modified, (3) anything else useful for cross method optimization?? With these notes, JIT wouldn't have to inline the entire method tree to be able to see that the final lock field isn't modified.
>
>
>Nathan Reynolds | Architect | 602.333.9091
>Oracle PSR Engineering | Server Technology
>
On 2/12/2013 3:26 PM, Zhong Yu wrote:
>
>On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote: 
>>On 02/12/13 15:41, javamann at cox.net wrote: 
>>>Stupid question time. While going through the code for an ArrayBlockQueue
I came across numerous instances where the instance 'lock' is copied to a
variable in a Method. I know there is a reason for this, I just don't know
what it is. 
>>>It's ultimately due to the fundamental mismatch between memory models
and OOP :-) Just about every method in all of j.u.c adopts the policy of
reading fields as locals whenever a value is used more than once.
This way you are sure which value applies when.
This is not often pretty, but is easier to visually verify. The surprising case is doing this even for "final" fields.
This is because JVMs are not always smart enough to exploit
the fine points of the JMM and not reload read final
values, as they would otherwise need to do across the
volatile accesses entailed in locking. Some JVMs are smarter
than they used to be about this, but still not always
smart enough. 
>>(Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
always cache the value of a final field? What if `this` was leaked
before constructor exit? And if the 1st read sees a non-null value,
can that value then be cached safely? Zhong Yu 
>>-Doug 
>>>Thanks -Pete public int remainingCapacity() { final ReentrantLock lock = this.lock; lock.lock(); try { return items.length - count; } finally { lock.unlock(); } } _______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>>>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 
>
>
>
>_______________________________________________
Concurrency-interest mailing list Concurrency-interest at cs.oswego.edu http://cs.oswego.edu/mailman/listinfo/concurrency-interest 

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/841127a4/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Feb 13 06:28:16 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 13 Feb 2013 11:28:16 +0000
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com> <511B6ECC.6000501@oracle.com>
	<1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>
Message-ID: <511B78D0.4000007@oracle.com>

I look at it simpler.

If the final-modifying code breaks the application's assumption that the 
field is final, then the final-modifying code has the problem.

If the final-modifying code guarantees the application's assumption that 
the field looks final, then there is no need to distinguish the case 
that it may be modified by some external means.


Alex


On 13/02/2013 11:04, Nitsan Wakart wrote:
> See this : 
> http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
> And the follow up: 
> http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
> Final fields are not treated as final because sometimes (as in: 
> popular use case you can't ignore) they are not...
>
>
> ------------------------------------------------------------------------
> *From:* oleksandr otenko <oleksandr.otenko at oracle.com>
> *To:* concurrency-interest at cs.oswego.edu
> *Sent:* Wednesday, February 13, 2013 10:45 AM
> *Subject:* Re: [concurrency-interest] Stupid Question
>
> I don't get this. Why have a final, if you need to keep track of 
> illegal attempts to modify it via Unsafe / JNI / reflection? Is this 
> really how finals are implemented, ie don't assume they are final?
>
> I understand when you do it the other way around - assume a unmarked 
> field is final, until someone attempts to modify the field.
>
>
> Alex
>
>
> On 12/02/2013 23:09, Nathan Reynolds wrote:
>> Currently, JIT's visibility is as far as it can inline.  It can't see 
>> any further than that.  Couldn't JIT keep notes on the methods for 
>> whether (1) if parameters or the return value escape, (2) which 
>> fields are modified, (3) anything else useful for cross method 
>> optimization?  With these notes, JIT wouldn't have to inline the 
>> entire method tree to be able to see that the final lock field isn't 
>> modified.
>>
>> Nathan Reynolds 
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>>> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea<dl at cs.oswego.edu>  <mailto:dl at cs.oswego.edu>  wrote:
>>>> On 02/12/13 15:41,javamann at cox.net  <mailto:javamann at cox.net>  wrote:
>>>>> Stupid question time. While going through the code for an ArrayBlockQueue
>>>>> I came across numerous instances where the instance 'lock' is copied to a
>>>>> variable in a Method. I know there is a reason for this, I just don't know
>>>>> what it is.
>>>>>
>>>> It's ultimately due to the fundamental mismatch between memory models
>>>> and OOP :-)
>>>>
>>>> Just about every method in all of j.u.c adopts the policy of
>>>> reading fields as locals whenever a value is used more than once.
>>>> This way you are sure which value applies when.
>>>> This is not often pretty, but is easier to visually verify.
>>>>
>>>> The surprising case is doing this even for "final" fields.
>>>> This is because JVMs are not always smart enough to exploit
>>>> the fine points of the JMM and not reload read final
>>>> values, as they would otherwise need to do across the
>>>> volatile accesses entailed in locking. Some JVMs are smarter
>>>> than they used to be about this, but still not always
>>>> smart enough.
>>> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
>>> always cache the value of a final field? What if `this` was leaked
>>> before constructor exit? And if the 1st read sees a non-null value,
>>> can that value then be cached safely?
>>>
>>> Zhong Yu
>>>
>>>
>>>> -Doug
>>>>
>>>>
>>>>
>>>>
>>>>> Thanks
>>>>>
>>>>> -Pete
>>>>>
>>>>>       public int remainingCapacity() {
>>>>>           final ReentrantLock lock = this.lock;
>>>>>           lock.lock();
>>>>>           try {
>>>>>               return items.length - count;
>>>>>           } finally {
>>>>>               lock.unlock();
>>>>>           }
>>>>>       }
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/c29dcfc7/attachment.html>

From tomasz.kowalczewski at gmail.com  Wed Feb 13 06:41:28 2013
From: tomasz.kowalczewski at gmail.com (Tomasz Kowalczewski)
Date: Wed, 13 Feb 2013 12:41:28 +0100
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511B78D0.4000007@oracle.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com> <511B6ECC.6000501@oracle.com>
	<1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<511B78D0.4000007@oracle.com>
Message-ID: <CANTKXYnGuUA0tg2brosX=txVfGLyz6hO-Kp4QhJa25d1CXBVrQ@mail.gmail.com>

Hi,

AFAIK it's not about application assumption that is broken. It's JVM
assumption that lead to all sorts of optimizations like constant
propagation. Changing final variable will invalidate these assumptions
and C2 compiled and optimized code will be executing with old value of
the "final" field. This means that all final-modifying code would
break JVM assumptions (while not necessarily application logic).

e.g. System.out is final static and initially null. Yet you can set it
using System::setOut.
Better way to go with "almost" final vlaues is one described by Remi
in http://www.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field

On Wed, Feb 13, 2013 at 12:28 PM, oleksandr otenko
<oleksandr.otenko at oracle.com> wrote:
> I look at it simpler.
>
> If the final-modifying code breaks the application's assumption that the
> field is final, then the final-modifying code has the problem.
>
> If the final-modifying code guarantees the application's assumption that the
> field looks final, then there is no need to distinguish the case that it may
> be modified by some external means.
>
>
> Alex
>
>
>
> On 13/02/2013 11:04, Nitsan Wakart wrote:
>
> See this :
> http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
> And the follow up:
> http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
> Final fields are not treated as final because sometimes (as in: popular use
> case you can't ignore) they are not...
>
>
> ________________________________
> From: oleksandr otenko <oleksandr.otenko at oracle.com>
> To: concurrency-interest at cs.oswego.edu
> Sent: Wednesday, February 13, 2013 10:45 AM
> Subject: Re: [concurrency-interest] Stupid Question
>
> I don't get this. Why have a final, if you need to keep track of illegal
> attempts to modify it via Unsafe / JNI / reflection? Is this really how
> finals are implemented, ie don't assume they are final?
>
> I understand when you do it the other way around - assume a unmarked field
> is final, until someone attempts to modify the field.
>
>
> Alex
>
>
> On 12/02/2013 23:09, Nathan Reynolds wrote:
>
> Currently, JIT's visibility is as far as it can inline.  It can't see any
> further than that.  Couldn't JIT keep notes on the methods for whether (1)
> if parameters or the return value escape, (2) which fields are modified, (3)
> anything else useful for cross method optimization?  With these notes, JIT
> wouldn't have to inline the entire method tree to be able to see that the
> final lock field isn't modified.
>
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>
> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> On 02/12/13 15:41, javamann at cox.net wrote:
>
> Stupid question time. While going through the code for an ArrayBlockQueue
> I came across numerous instances where the instance 'lock' is copied to a
> variable in a Method. I know there is a reason for this, I just don't know
> what it is.
>
> It's ultimately due to the fundamental mismatch between memory models
> and OOP :-)
>
> Just about every method in all of j.u.c adopts the policy of
> reading fields as locals whenever a value is used more than once.
> This way you are sure which value applies when.
> This is not often pretty, but is easier to visually verify.
>
> The surprising case is doing this even for "final" fields.
> This is because JVMs are not always smart enough to exploit
> the fine points of the JMM and not reload read final
> values, as they would otherwise need to do across the
> volatile accesses entailed in locking. Some JVMs are smarter
> than they used to be about this, but still not always
> smart enough.
>
> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
> always cache the value of a final field? What if `this` was leaked
> before constructor exit? And if the 1st read sees a non-null value,
> can that value then be cached safely?
>
> Zhong Yu
>
>
> -Doug
>
>
>
>
> Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



--
Tomasz Kowalczewski

On Wed, Feb 13, 2013 at 12:28 PM, oleksandr otenko
<oleksandr.otenko at oracle.com> wrote:
> I look at it simpler.
>
> If the final-modifying code breaks the application's assumption that the
> field is final, then the final-modifying code has the problem.
>
> If the final-modifying code guarantees the application's assumption that the
> field looks final, then there is no need to distinguish the case that it may
> be modified by some external means.
>
>
> Alex
>
>
>
> On 13/02/2013 11:04, Nitsan Wakart wrote:
>
> See this :
> http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
> And the follow up:
> http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
> Final fields are not treated as final because sometimes (as in: popular use
> case you can't ignore) they are not...
>
>
> ________________________________
> From: oleksandr otenko <oleksandr.otenko at oracle.com>
> To: concurrency-interest at cs.oswego.edu
> Sent: Wednesday, February 13, 2013 10:45 AM
> Subject: Re: [concurrency-interest] Stupid Question
>
> I don't get this. Why have a final, if you need to keep track of illegal
> attempts to modify it via Unsafe / JNI / reflection? Is this really how
> finals are implemented, ie don't assume they are final?
>
> I understand when you do it the other way around - assume a unmarked field
> is final, until someone attempts to modify the field.
>
>
> Alex
>
>
> On 12/02/2013 23:09, Nathan Reynolds wrote:
>
> Currently, JIT's visibility is as far as it can inline.  It can't see any
> further than that.  Couldn't JIT keep notes on the methods for whether (1)
> if parameters or the return value escape, (2) which fields are modified, (3)
> anything else useful for cross method optimization?  With these notes, JIT
> wouldn't have to inline the entire method tree to be able to see that the
> final lock field isn't modified.
>
> Nathan Reynolds | Architect | 602.333.9091
> Oracle PSR Engineering | Server Technology
> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>
> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
> On 02/12/13 15:41, javamann at cox.net wrote:
>
> Stupid question time. While going through the code for an ArrayBlockQueue
> I came across numerous instances where the instance 'lock' is copied to a
> variable in a Method. I know there is a reason for this, I just don't know
> what it is.
>
> It's ultimately due to the fundamental mismatch between memory models
> and OOP :-)
>
> Just about every method in all of j.u.c adopts the policy of
> reading fields as locals whenever a value is used more than once.
> This way you are sure which value applies when.
> This is not often pretty, but is easier to visually verify.
>
> The surprising case is doing this even for "final" fields.
> This is because JVMs are not always smart enough to exploit
> the fine points of the JMM and not reload read final
> values, as they would otherwise need to do across the
> volatile accesses entailed in locking. Some JVMs are smarter
> than they used to be about this, but still not always
> smart enough.
>
> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
> always cache the value of a final field? What if `this` was leaked
> before constructor exit? And if the 1st read sees a non-null value,
> can that value then be cached safely?
>
> Zhong Yu
>
>
> -Doug
>
>
>
>
> Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



--
Tomasz Kowalczewski

From ldm at gmx.at  Wed Feb 13 07:10:04 2013
From: ldm at gmx.at (Markus Krainz)
Date: Wed, 13 Feb 2013 13:10:04 +0100
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <CANTKXYnGuUA0tg2brosX=txVfGLyz6hO-Kp4QhJa25d1CXBVrQ@mail.gmail.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com> <511B6ECC.6000501@oracle.com>
	<1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<511B78D0.4000007@oracle.com>
	<CANTKXYnGuUA0tg2brosX=txVfGLyz6hO-Kp4QhJa25d1CXBVrQ@mail.gmail.com>
Message-ID: <511B829C.7040800@gmx.at>

Hi,

I have read that blog post some time ago. It uses the SwitchPoint class 
to optimize the access.
I am wondering, could this be more efficient than the double-checked 
locking idiom used e.g. for lazy initialization?

Regards,
Markus Krainz

Am 13.02.2013 12:41, schrieb Tomasz Kowalczewski:
> Hi,
>
> AFAIK it's not about application assumption that is broken. It's JVM
> assumption that lead to all sorts of optimizations like constant
> propagation. Changing final variable will invalidate these assumptions
> and C2 compiled and optimized code will be executing with old value of
> the "final" field. This means that all final-modifying code would
> break JVM assumptions (while not necessarily application logic).
>
> e.g. System.out is final static and initially null. Yet you can set it
> using System::setOut.
> Better way to go with "almost" final vlaues is one described by Remi
> in http://www.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field
>
> On Wed, Feb 13, 2013 at 12:28 PM, oleksandr otenko
> <oleksandr.otenko at oracle.com> wrote:
>> I look at it simpler.
>>
>> If the final-modifying code breaks the application's assumption that the
>> field is final, then the final-modifying code has the problem.
>>
>> If the final-modifying code guarantees the application's assumption that the
>> field looks final, then there is no need to distinguish the case that it may
>> be modified by some external means.
>>
>>
>> Alex
>>
>>
>>
>> On 13/02/2013 11:04, Nitsan Wakart wrote:
>>
>> See this :
>> http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
>> And the follow up:
>> http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
>> Final fields are not treated as final because sometimes (as in: popular use
>> case you can't ignore) they are not...
>>
>>
>> ________________________________
>> From: oleksandr otenko <oleksandr.otenko at oracle.com>
>> To: concurrency-interest at cs.oswego.edu
>> Sent: Wednesday, February 13, 2013 10:45 AM
>> Subject: Re: [concurrency-interest] Stupid Question
>>
>> I don't get this. Why have a final, if you need to keep track of illegal
>> attempts to modify it via Unsafe / JNI / reflection? Is this really how
>> finals are implemented, ie don't assume they are final?
>>
>> I understand when you do it the other way around - assume a unmarked field
>> is final, until someone attempts to modify the field.
>>
>>
>> Alex
>>
>>
>> On 12/02/2013 23:09, Nathan Reynolds wrote:
>>
>> Currently, JIT's visibility is as far as it can inline.  It can't see any
>> further than that.  Couldn't JIT keep notes on the methods for whether (1)
>> if parameters or the return value escape, (2) which fields are modified, (3)
>> anything else useful for cross method optimization?  With these notes, JIT
>> wouldn't have to inline the entire method tree to be able to see that the
>> final lock field isn't modified.
>>
>> Nathan Reynolds | Architect | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>>
>> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> On 02/12/13 15:41, javamann at cox.net wrote:
>>
>> Stupid question time. While going through the code for an ArrayBlockQueue
>> I came across numerous instances where the instance 'lock' is copied to a
>> variable in a Method. I know there is a reason for this, I just don't know
>> what it is.
>>
>> It's ultimately due to the fundamental mismatch between memory models
>> and OOP :-)
>>
>> Just about every method in all of j.u.c adopts the policy of
>> reading fields as locals whenever a value is used more than once.
>> This way you are sure which value applies when.
>> This is not often pretty, but is easier to visually verify.
>>
>> The surprising case is doing this even for "final" fields.
>> This is because JVMs are not always smart enough to exploit
>> the fine points of the JMM and not reload read final
>> values, as they would otherwise need to do across the
>> volatile accesses entailed in locking. Some JVMs are smarter
>> than they used to be about this, but still not always
>> smart enough.
>>
>> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
>> always cache the value of a final field? What if `this` was leaked
>> before constructor exit? And if the 1st read sees a non-null value,
>> can that value then be cached safely?
>>
>> Zhong Yu
>>
>>
>> -Doug
>>
>>
>>
>>
>> Thanks
>>
>> -Pete
>>
>>       public int remainingCapacity() {
>>           final ReentrantLock lock = this.lock;
>>           lock.lock();
>>           try {
>>               return items.length - count;
>>           } finally {
>>               lock.unlock();
>>           }
>>       }
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> Tomasz Kowalczewski
>
> On Wed, Feb 13, 2013 at 12:28 PM, oleksandr otenko
> <oleksandr.otenko at oracle.com> wrote:
>> I look at it simpler.
>>
>> If the final-modifying code breaks the application's assumption that the
>> field is final, then the final-modifying code has the problem.
>>
>> If the final-modifying code guarantees the application's assumption that the
>> field looks final, then there is no need to distinguish the case that it may
>> be modified by some external means.
>>
>>
>> Alex
>>
>>
>>
>> On 13/02/2013 11:04, Nitsan Wakart wrote:
>>
>> See this :
>> http://www.azulsystems.com/blog/cliff/2011-10-17-writing-to-final-fields-via-reflection
>> And the follow up:
>> http://www.azulsystems.com/blog/cliff/2011-10-27-final-fields-part-2
>> Final fields are not treated as final because sometimes (as in: popular use
>> case you can't ignore) they are not...
>>
>>
>> ________________________________
>> From: oleksandr otenko <oleksandr.otenko at oracle.com>
>> To: concurrency-interest at cs.oswego.edu
>> Sent: Wednesday, February 13, 2013 10:45 AM
>> Subject: Re: [concurrency-interest] Stupid Question
>>
>> I don't get this. Why have a final, if you need to keep track of illegal
>> attempts to modify it via Unsafe / JNI / reflection? Is this really how
>> finals are implemented, ie don't assume they are final?
>>
>> I understand when you do it the other way around - assume a unmarked field
>> is final, until someone attempts to modify the field.
>>
>>
>> Alex
>>
>>
>> On 12/02/2013 23:09, Nathan Reynolds wrote:
>>
>> Currently, JIT's visibility is as far as it can inline.  It can't see any
>> further than that.  Couldn't JIT keep notes on the methods for whether (1)
>> if parameters or the return value escape, (2) which fields are modified, (3)
>> anything else useful for cross method optimization?  With these notes, JIT
>> wouldn't have to inline the entire method tree to be able to see that the
>> final lock field isn't modified.
>>
>> Nathan Reynolds | Architect | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>> On 2/12/2013 3:26 PM, Zhong Yu wrote:
>>
>> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> On 02/12/13 15:41, javamann at cox.net wrote:
>>
>> Stupid question time. While going through the code for an ArrayBlockQueue
>> I came across numerous instances where the instance 'lock' is copied to a
>> variable in a Method. I know there is a reason for this, I just don't know
>> what it is.
>>
>> It's ultimately due to the fundamental mismatch between memory models
>> and OOP :-)
>>
>> Just about every method in all of j.u.c adopts the policy of
>> reading fields as locals whenever a value is used more than once.
>> This way you are sure which value applies when.
>> This is not often pretty, but is easier to visually verify.
>>
>> The surprising case is doing this even for "final" fields.
>> This is because JVMs are not always smart enough to exploit
>> the fine points of the JMM and not reload read final
>> values, as they would otherwise need to do across the
>> volatile accesses entailed in locking. Some JVMs are smarter
>> than they used to be about this, but still not always
>> smart enough.
>>
>> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
>> always cache the value of a final field? What if `this` was leaked
>> before constructor exit? And if the 1st read sees a non-null value,
>> can that value then be cached safely?
>>
>> Zhong Yu
>>
>>
>> -Doug
>>
>>
>>
>>
>> Thanks
>>
>> -Pete
>>
>>       public int remainingCapacity() {
>>           final ReentrantLock lock = this.lock;
>>           lock.lock();
>>           try {
>>               return items.length - count;
>>           } finally {
>>               lock.unlock();
>>           }
>>       }
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> --
> Tomasz Kowalczewski
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From ldm at gmx.at  Wed Feb 13 07:27:04 2013
From: ldm at gmx.at (Markus Krainz)
Date: Wed, 13 Feb 2013 13:27:04 +0100
Subject: [concurrency-interest] ReadMostlyVector vs CopyOnWriteArrayList
Message-ID: <511B8698.2090403@gmx.at>

Hi,

I have a question regarding ReadMostlyVector from the jsr166e package. I 
had a look at the sourcecode and it uses StampedLock, for which some 
impressive benchmarks results have been reported on this. For it's use 
cases it seems pretty similar to CopyOnWriteArrayList. There are some 
subtile differences from the API docs. The CopyOnWriteArrayList iterator 
does not reflecting changes, while the ReadMostlyVector will reflect 
them on a best effort basis (if snapshotIterator() is not used). 
CopyOnWriteArrayList's cow might have higher memory overhead. When 
should we use ReadMostlyVector in preference to CopyOnWriteArrayList?

Thanks and best regards,
Markus Krainz

[1]: 
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166edocs/jsr166e/extra/ReadMostlyVector.html
[2]: 
http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/CopyOnWriteArrayList.html

From concurrency at kuli.org  Wed Feb 13 08:09:59 2013
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 13 Feb 2013 14:09:59 +0100
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511B829C.7040800@gmx.at>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com> <511B6ECC.6000501@oracle.com>
	<1360753488.66070.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<511B78D0.4000007@oracle.com>
	<CANTKXYnGuUA0tg2brosX=txVfGLyz6hO-Kp4QhJa25d1CXBVrQ@mail.gmail.com>
	<511B829C.7040800@gmx.at>
Message-ID: <511B90A7.5010807@kuli.org>

Am 13.02.2013 13:10, schrieb Markus Krainz:
> Hi,
> 
> I have read that blog post some time ago. It uses the SwitchPoint class
> to optimize the access.
> I am wondering, could this be more efficient than the double-checked
> locking idiom used e.g. for lazy initialization?

This isn't directly comparable. The mentioned code is just too
complicated for sa simply lazy initialization. Better just use an own
static container class for lazy initialization as in
java.lang.Integer$IntegerCache.

But top answer your question, yes. In fact, double checked locking only
works for volatile fields or immutable objects with only final fields.
In either case, JIT can't optimize the read access away since it's not a
static final declaration.

Greetings,
Kuli



From dl at cs.oswego.edu  Wed Feb 13 08:22:42 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 13 Feb 2013 08:22:42 -0500
Subject: [concurrency-interest] ReadMostlyVector vs CopyOnWriteArrayList
In-Reply-To: <511B8698.2090403@gmx.at>
References: <511B8698.2090403@gmx.at>
Message-ID: <511B93A2.9000807@cs.oswego.edu>

On 02/13/13 07:27, Markus Krainz wrote:
> Hi,
>
> I have a question regarding ReadMostlyVector from the jsr166e package. I had a
> look at the sourcecode and it uses StampedLock, for which some impressive
> benchmarks results have been reported on this. For it's use cases it seems
> pretty similar to CopyOnWriteArrayList. There are some subtile differences from
> the API docs. The CopyOnWriteArrayList iterator does not reflecting changes,
> while the ReadMostlyVector will reflect them on a best effort basis (if
> snapshotIterator() is not used). CopyOnWriteArrayList's cow might have higher
> memory overhead. When should we use ReadMostlyVector in preference to
> CopyOnWriteArrayList?
>

These are good questions, that I'd been planning to post something
about when jdk8 j.u.c StampedLock becomes routinely compilable/usable
using early-access jdk8 and/or lambda builds. (It is possible to
do so now only if you are willing to create a custom build
from repositories.) In the mean time, the current jsr166e.StampedLock
does a reasonable emulation of j.u.c version for JDK7+ systems.
So now seems as good a time as ever:

The ReadMostlyVector class was initially conceived as one
application of SequenceLocks. We found though that without
some sort of readLock capability, these kinds of applications
didn't work out very well. Which led to the replacement of
SequenceLock with StampedLock, which looks to be a
good choice for a lock in a lot of contexts. The released
ReadMostlyVector class was minimally adjusted to use it.
But it should be further reworked here and there, and renamed
to ConcurrentlyReadableVector, to reflect its reliance
on combinations of optimistic and pessimistic read-write-style
locking. (In fact, I've done most of this so I that can
continue to use this class for internal testing
of StampedLock, but it remains uncommitted because there is not
yet a place to put it in our repositories that would correctly
reflect dependencies on as-yet-unavailable JDK builds).

OK, finally to the questions: ConcurrentlyReadableVector
should often be a better choice than any of Vector,
CopyOnWriteArrayList, or Collections.synchronizedList(...ArrayList..).
But is not strictly a replacement for any of them:

* Pure read-only operations it's a bit slower than CopyOnWriteArrayList.
Not by much though.

* It's main iterator is NOT a snapshot, and can throw
ConcurrentModificationExceptions unless a read-mode
lock is used surrounding traversal. But we don't want to hand out
that lock. ConcurrentlyReadableVector can support this in
jdk8 using:
   forEachReadLocked(Consumer<E> action);

* Most update operations will run faster than in any of the
other classes. Generally much faster and with less
space/GC overhead than CopyOnWriteArrayList. And faster
than the others unless one is used purely single-threadedly, in
which case it is likely that JVM biased or fast-path locking  used
inside Vector and synchronizedList will perform a bit better.

My only other hesitance about releasing ConcurrentlyReadableVector
is that it might give the false impression than sequential-list
classes are usually a good choice for heavily concurrent
applications. They really aren't. When possible, use
ConcurrentHashMaps and related scalable data structures.

-Doug







From vitalyd at gmail.com  Wed Feb 13 08:28:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 08:28:13 -0500
Subject: [concurrency-interest] Stupid Question
In-Reply-To: <511ACBC0.5080500@oracle.com>
References: <20130212204147.NR54C.197691.imail@fed1rmwml207>
	<511AB253.1060208@cs.oswego.edu>
	<CACuKZqESuwWBhGLnXY=OEnc7KsK6dZDCbCDpCE=ibhs8RdbTLw@mail.gmail.com>
	<511ACBC0.5080500@oracle.com>
Message-ID: <CAHjP37GGzU2T00b_vDhN=TKytXOtHy81P1c=YnqqKcY5uu=F9w@mail.gmail.com>

The additional memory requirements to track all of that are probably non
trivial for large apps.  It would also have track this across unsafe calls,
reflection, class loading (newly loaded classes may modify things), etc.
Seems very expensive (implementation and perf wise) for probably very
little (if any) perf gain.

Sent from my phone
On Feb 12, 2013 6:17 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Currently, JIT's visibility is as far as it can inline.  It can't see
> any further than that.  Couldn't JIT keep notes on the methods for whether
> (1) if parameters or the return value escape, (2) which fields are
> modified, (3) anything else useful for cross method optimization?  With
> these notes, JIT wouldn't have to inline the entire method tree to be able
> to see that the final lock field isn't modified.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/12/2013 3:26 PM, Zhong Yu wrote:
>
> On Tue, Feb 12, 2013 at 3:21 PM, Doug Lea <dl at cs.oswego.edu> <dl at cs.oswego.edu> wrote:
>
>  On 02/12/13 15:41, javamann at cox.net wrote:
>
>
> Stupid question time. While going through the code for an ArrayBlockQueue
> I came across numerous instances where the instance 'lock' is copied to a
> variable in a Method. I know there is a reason for this, I just don't know
> what it is.
>
>
>
> It's ultimately due to the fundamental mismatch between memory models
> and OOP :-)
>
> Just about every method in all of j.u.c adopts the policy of
> reading fields as locals whenever a value is used more than once.
> This way you are sure which value applies when.
> This is not often pretty, but is easier to visually verify.
>
> The surprising case is doing this even for "final" fields.
> This is because JVMs are not always smart enough to exploit
> the fine points of the JMM and not reload read final
> values, as they would otherwise need to do across the
> volatile accesses entailed in locking. Some JVMs are smarter
> than they used to be about this, but still not always
> smart enough.
>
>
> (Forget reflection/Unsafe, only consider typical JMM actions) Can JVM
> always cache the value of a final field? What if `this` was leaked
> before constructor exit? And if the 1st read sees a non-null value,
> can that value then be cached safely?
>
> Zhong Yu
>
>
>
>
> -Doug
>
>
>
>
>
>  Thanks
>
> -Pete
>
>      public int remainingCapacity() {
>          final ReentrantLock lock = this.lock;
>          lock.lock();
>          try {
>              return items.length - count;
>          } finally {
>              lock.unlock();
>          }
>      }
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/ee8a63b9/attachment-0001.html>

From vitalyd at gmail.com  Wed Feb 13 08:32:25 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 08:32:25 -0500
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>
Message-ID: <CAHjP37G8O+Azf_w0Ew_J-kR9Q9qPymzTcQqe_2SNCCvH3vOLpA@mail.gmail.com>

/proc/cpuinfo parsing is the way to go but would be nice to have a "machine
spec/capability" API baked into the JDK.

Sent from my phone
On Feb 13, 2013 5:49 AM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

>
> Is there a way to know how many physical processors are available on the
>> JVM?
>>
>> On linux you can use taskset -p <pid>
> (pid you can retrieve form /proc/self )
> and you can use /proc/cpuinfo to determine which processors have siblings.
>
> Quite inconvenient overall. So kernels might have /proc/self/affinity
> which would make life easier .
> And of course there is JNI and syscall.
>
> Stanimir
>
>
>
>> Cheers,
>> ?
>>
>>
>>>
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>  On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>>>
>>>
>>>
>>>
>>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>>
>>>> >
>>>> > Do you agree that thread pool sizing depends on type of work? (IO
>>>> bound vs CPU bound, bursty vs steady etc etc)
>>>>  Yes
>>>> > Do you agree that a JVM Thread is not a unit of parallelism?
>>>>  Yes
>>>> > Do you agree that having more JVM Threads than hardware threads is
>>>> bad for CPU-bound workloads?
>>>>  No, even with CPU bound workloads I have found that the hardware/OS is
>>>> much better at managing many workloads across many threads than I am. So a
>>>> few more threads is ok, many more threads is bad fast.
>>>>
>>>
>>>  That's an interesting observation. Have any more data on that? (really
>>> interested)
>>> As I said earlier, for CPU-bound workloads we've seen the best
>>> performance when only loading 60-70% of the cores (other threads exist on
>>> the machine of course).
>>>
>>>  Cheers,
>>>
>>> ?
>>>
>>>
>>>>
>>>> Regards,
>>>> Kirk
>>>
>>>
>>>
>>>
>>>  --
>>> *Viktor Klang*
>>> *Director of Engineering*
>>> *
>>> *
>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>> applications that scale
>>> Twitter: @viktorklang
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> *Viktor Klang*
>> *Director of Engineering*
>> *
>> *
>> Typesafe <http://www.typesafe.com/> - The software stack for
>> applications that scale
>> Twitter: @viktorklang
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/de8623ac/attachment.html>

From stanimir at riflexo.com  Wed Feb 13 08:39:22 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 13 Feb 2013 15:39:22 +0200
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CAHjP37G8O+Azf_w0Ew_J-kR9Q9qPymzTcQqe_2SNCCvH3vOLpA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>
	<CAHjP37G8O+Azf_w0Ew_J-kR9Q9qPymzTcQqe_2SNCCvH3vOLpA@mail.gmail.com>
Message-ID: <CAEJX8ooD-a6huNLXF6pNxRgk9gFe3hNos9HCf8Axc8c=MFJqrw@mail.gmail.com>

On Wed, Feb 13, 2013 at 3:32 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> /proc/cpuinfo parsing is the way to go but would be nice to have a
> "machine spec/capability" API baked into the JDK.
>
> But you still to get info on which CPU the process is limited to, i.e.
affinity. Which is the harder task. OTOH, /proc/cpuinfo is quite trivial to
parse.

Stanimir

> Sent from my phone
> On Feb 13, 2013 5:49 AM, "Stanimir Simeonoff" <stanimir at riflexo.com>
> wrote:
>
>>
>> Is there a way to know how many physical processors are available on the
>>> JVM?
>>>
>>> On linux you can use taskset -p <pid>
>> (pid you can retrieve form /proc/self )
>> and you can use /proc/cpuinfo to determine which processors have siblings.
>>
>> Quite inconvenient overall. So kernels might have /proc/self/affinity
>> which would make life easier .
>> And of course there is JNI and syscall.
>>
>> Stanimir
>>
>>
>>
>>> Cheers,
>>> ?
>>>
>>>
>>>>
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>>>>
>>>>
>>>>
>>>>
>>>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>>>
>>>>> >
>>>>> > Do you agree that thread pool sizing depends on type of work? (IO
>>>>> bound vs CPU bound, bursty vs steady etc etc)
>>>>>  Yes
>>>>> > Do you agree that a JVM Thread is not a unit of parallelism?
>>>>>  Yes
>>>>> > Do you agree that having more JVM Threads than hardware threads is
>>>>> bad for CPU-bound workloads?
>>>>>  No, even with CPU bound workloads I have found that the hardware/OS
>>>>> is much better at managing many workloads across many threads than I am. So
>>>>> a few more threads is ok, many more threads is bad fast.
>>>>>
>>>>
>>>>  That's an interesting observation. Have any more data on that?
>>>> (really interested)
>>>> As I said earlier, for CPU-bound workloads we've seen the best
>>>> performance when only loading 60-70% of the cores (other threads exist on
>>>> the machine of course).
>>>>
>>>>  Cheers,
>>>>
>>>> ?
>>>>
>>>>
>>>>>
>>>>> Regards,
>>>>> Kirk
>>>>
>>>>
>>>>
>>>>
>>>>  --
>>>> *Viktor Klang*
>>>> *Director of Engineering*
>>>> *
>>>> *
>>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>>> applications that scale
>>>> Twitter: @viktorklang
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> *Viktor Klang*
>>> *Director of Engineering*
>>> *
>>> *
>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>> applications that scale
>>> Twitter: @viktorklang
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/ae02e407/attachment-0001.html>

From vitalyd at gmail.com  Wed Feb 13 08:47:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 08:47:00 -0500
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <CAEJX8ooD-a6huNLXF6pNxRgk9gFe3hNos9HCf8Axc8c=MFJqrw@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>
	<CAHjP37G8O+Azf_w0Ew_J-kR9Q9qPymzTcQqe_2SNCCvH3vOLpA@mail.gmail.com>
	<CAEJX8ooD-a6huNLXF6pNxRgk9gFe3hNos9HCf8Axc8c=MFJqrw@mail.gmail.com>
Message-ID: <CAHjP37ESWSV_xODMMxGCG6ed50ynSoCwM1kqntQaHGFMQk+8yA@mail.gmail.com>

If you're using an affinity mask, sure.  The advantage of having this in
jdk is cross platform support and not having people write the same code
across their apps.

Sent from my phone
On Feb 13, 2013 8:39 AM, "Stanimir Simeonoff" <stanimir at riflexo.com> wrote:

>
>
> On Wed, Feb 13, 2013 at 3:32 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> /proc/cpuinfo parsing is the way to go but would be nice to have a
>> "machine spec/capability" API baked into the JDK.
>>
>> But you still to get info on which CPU the process is limited to, i.e.
> affinity. Which is the harder task. OTOH, /proc/cpuinfo is quite trivial to
> parse.
>
> Stanimir
>
>> Sent from my phone
>> On Feb 13, 2013 5:49 AM, "Stanimir Simeonoff" <stanimir at riflexo.com>
>> wrote:
>>
>>>
>>> Is there a way to know how many physical processors are available on the
>>>> JVM?
>>>>
>>>> On linux you can use taskset -p <pid>
>>> (pid you can retrieve form /proc/self )
>>> and you can use /proc/cpuinfo to determine which processors have
>>> siblings.
>>>
>>> Quite inconvenient overall. So kernels might have /proc/self/affinity
>>> which would make life easier .
>>> And of course there is JNI and syscall.
>>>
>>> Stanimir
>>>
>>>
>>>
>>>> Cheers,
>>>> ?
>>>>
>>>>
>>>>>
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>  On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>>>>
>>>>>> >
>>>>>> > Do you agree that thread pool sizing depends on type of work? (IO
>>>>>> bound vs CPU bound, bursty vs steady etc etc)
>>>>>>  Yes
>>>>>> > Do you agree that a JVM Thread is not a unit of parallelism?
>>>>>>  Yes
>>>>>> > Do you agree that having more JVM Threads than hardware threads is
>>>>>> bad for CPU-bound workloads?
>>>>>>  No, even with CPU bound workloads I have found that the hardware/OS
>>>>>> is much better at managing many workloads across many threads than I am. So
>>>>>> a few more threads is ok, many more threads is bad fast.
>>>>>>
>>>>>
>>>>>  That's an interesting observation. Have any more data on that?
>>>>> (really interested)
>>>>> As I said earlier, for CPU-bound workloads we've seen the best
>>>>> performance when only loading 60-70% of the cores (other threads exist on
>>>>> the machine of course).
>>>>>
>>>>>  Cheers,
>>>>>
>>>>> ?
>>>>>
>>>>>
>>>>>>
>>>>>> Regards,
>>>>>> Kirk
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>  --
>>>>> *Viktor Klang*
>>>>> *Director of Engineering*
>>>>> *
>>>>> *
>>>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>>>> applications that scale
>>>>> Twitter: @viktorklang
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>> *Viktor Klang*
>>>> *Director of Engineering*
>>>> *
>>>> *
>>>> Typesafe <http://www.typesafe.com/> - The software stack for
>>>> applications that scale
>>>> Twitter: @viktorklang
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/7fc1478a/attachment.html>

From holger.hoffstaette at googlemail.com  Wed Feb 13 08:47:50 2013
From: holger.hoffstaette at googlemail.com (=?UTF-8?B?SG9sZ2VyIEhvZmZzdMOkdHRl?=)
Date: Wed, 13 Feb 2013 14:47:50 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
Message-ID: <511B9986.3000806@googlemail.com>

On Wed, Feb 13, 2013 at 9:33 AM, ?iktor ?lang <viktor.klang at gmail.com>
wrote:

> Is there a way to know how many physical processors are available on the
> JVM?

Not really, and why this is almost impossible to implement in a
simplistic way can be readily seen when you look what
http://www.open-mpi.org/projects/hwloc/ does. Add NUMA and other
heterogenous HW (GPGPUs, weaker cores etc.) to the mix and "the number"
rapidly ceases to have any meaning.

Holger


From viktor.klang at gmail.com  Wed Feb 13 09:21:08 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Wed, 13 Feb 2013 15:21:08 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
	results
In-Reply-To: <511B9986.3000806@googlemail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
	<511B9986.3000806@googlemail.com>
Message-ID: <CANPzfU943SxEvWkoQSe6j9wc6wGs-Bh_cM4pFKBMEkA3ozOgDA@mail.gmail.com>

What's even more interesting is the the available processors can change at
runtime, making it extremely hard to do anything interesting with the info.


On Wed, Feb 13, 2013 at 2:47 PM, Holger Hoffst?tte <
holger.hoffstaette at googlemail.com> wrote:

> On Wed, Feb 13, 2013 at 9:33 AM, ?iktor ?lang <viktor.klang at gmail.com>
> wrote:
>
> > Is there a way to know how many physical processors are available on the
> > JVM?
>
> Not really, and why this is almost impossible to implement in a
> simplistic way can be readily seen when you look what
> http://www.open-mpi.org/projects/hwloc/ does. Add NUMA and other
> heterogenous HW (GPGPUs, weaker cores etc.) to the mix and "the number"
> rapidly ceases to have any meaning.
>
> Holger
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/9b073de6/attachment.html>

From holger.hoffstaette at googlemail.com  Wed Feb 13 09:37:53 2013
From: holger.hoffstaette at googlemail.com (=?UTF-8?B?SG9sZ2VyIEhvZmZzdMOkdHRl?=)
Date: Wed, 13 Feb 2013 15:37:53 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CANPzfU943SxEvWkoQSe6j9wc6wGs-Bh_cM4pFKBMEkA3ozOgDA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
	<511B9986.3000806@googlemail.com>
	<CANPzfU943SxEvWkoQSe6j9wc6wGs-Bh_cM4pFKBMEkA3ozOgDA@mail.gmail.com>
Message-ID: <511BA541.4090800@googlemail.com>

On 13.02.2013 15:21, ?iktor ?lang wrote:
> What's even more interesting is the the available processors can change
> at runtime, making it extremely hard to do anything interesting with the
> info.

Well..see towards the end of [1]. However this approach is obviously
useless for Java, so you'd have to do something internal, like register
inotify callbacks on the /sys list or the like. Unfortunately that
doesn't work either [2]. Things go downhill from there.
All statically wired thread pools (hi FJ) also suffer from this.

..but we're getting off-topic :)

-h

[1] http://www.kernel.org/doc/Documentation/cpu-hotplug.txt
[2] http://www.mail-archive.com/kernelnewbies at nl.linux.org/msg07651.html


From peter.levart at gmail.com  Wed Feb 13 09:40:09 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 13 Feb 2013 15:40:09 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
Message-ID: <511BA5C9.3000003@gmail.com>

On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
> Here is all of the relevant code in a single blob.  I think it's 
> selef-explanatory; sample test is at the end.
>
> http://pastebin.com/WrfBHYSG

Hi Thurston,

You're doing too much copying. I modified tour MergeSorter a little:


public class MergeSorter2 extends RecursiveTask<int[]> {
     static final int THRESHOLD = 250;
     final int[] array, work;
     final int offset, length;

     public MergeSorter2(int[] array) {
         this.array = array;
         this.work = new int[array.length];
         offset = 0;
         length = array.length;
     }

     private MergeSorter2(int[] array, int[] work, int offset, int length) {
         this.array = array;
         this.work = work;
         this.offset = offset;
         this.length = length;
     }

     @Override
     protected int[] compute() {
         if (length < MergeSorter2.THRESHOLD) {
             Arrays.sort(array, offset, offset + length);
             return this.array;
         }

         int halfLength = length >> 1;
         MergeSorter2 left = new MergeSorter2(array, work, offset, 
halfLength);
         left.fork();

         int mid = offset + halfLength;

         MergeSorter2 right = new MergeSorter2(array, work, mid, length 
- halfLength);
         right.compute();
         left.join();

         // copy 1st half from array to work
         System.arraycopy(array, offset, work, offset, halfLength);

         // merge 1st half of work and 2nd half of array into array
         int end = offset + length;
         int i = offset;
         int j = mid;
         int k = offset;
         int x = work[i];
         int y = array[j];
         while (true) {
             if (j >= end || x <= y) {
                 array[k++] = x;
                 i++;
                 // until we drain the 1st half (the rest of array is 
already in place)
                 if (i >= mid) break;
                 x = work[i];
             } else { // j < end
                 array[k++] = y;
                 j++;
                 if (j < end) y = array[j];
             }
         }

         return this.array;
     }
}


... it uses a single parallel "work" array for merging, which is shared 
among tasks. It tries to minimize the copying. Here's the results I get 
on a FJPool with parallelism=2:

  MergeSorter: 1557638090 nanos
  MergeSorter: 1545600414 nanos
  MergeSorter: 1478185502 nanos
MergeSorter2: 1058750062 nanos
MergeSorter2:  978295370 nanos
MergeSorter2:  976838429 nanos
  Arrays.sort: 1494256525 nanos
  Arrays.sort: 1505639161 nanos
  Arrays.sort: 1483640342 nanos


I get even better results with greater THRESHOLD (but not more than 10% 
better). Your MergeSorter is worse with greater THRESHOLD because you 
are using insert-sort for final tasks. The Arrays.sort() delegates to 
insert-sort only when N < 47, else it uses quick-sort...

Regards, Peter



>
>
>
> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>> Yes, I will push the code and my measurements some time this week.
>> I'm really interested in seeing what others' results would be. What I
>> can say is that the timings are surprisingly stable (range 30-40ms)
>> and the input data is randomly generated for each test run which can
>> certainly affect merge(int[], int[])'s performance
>>
>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>> Hi,
>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>> Hi T,
>>>
>>> Can you pub the raw data from your runs? Average tends to hide
>>> effects and it's difficult to say anything with only that measure.
>>>
>>> Regards,
>>> Kirk
>>>
>>>
>>>> Hello,
>>>>
>>>> I made some initial attempts at using ForkJoin framework for some 
>>>> rather obvious recursively parallel use-cases, namely summing the 
>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>
>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>
>>>> I really enjoy using the framework and find it relatively easy to 
>>>> reason about my programs (if not necessarily the internals of the 
>>>> framework).
>>>> But I was disappointed with the results: in both cases they were 
>>>> slower than the corresponding Java serial implementation.
>>>>
>>>> I completely understand the principle of YMMV, and I wasn't 
>>>> expecting 2x speedup, but especially in the case of the sort, but I 
>>>> was expecting that ForkJoin would do at least a little better than 
>>>> the single-threaded version.  I guess what I'm asking is: are these 
>>>> results surprising?  Does it mean I'm doing something wrong?
>>>>
>>>> I'll give a brief description of my implementation:
>>>> single-threaded:  Arrays.sort(int[])
>>>>
>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived at 
>>>> by trial-and-error)
>>>>
>>>> int[] compute()
>>>> {
>>>> if int[].length < THRESHOLD
>>>>    return insertionSort(int[])
>>>> left = new MergeSort(int[] //split in half)
>>>> right = new MergeSort(int[] //other half)
>>>> return merge(right.compute(), left.join())
>>>> }
>>>>
>>>> The insertionSort() and merge() methods are just standard 
>>>> implementations; there is a bit of apples to oranges comparison 
>>>> since Arrays.sort() uses an optimized quicksort, but we're still 
>>>> talking O(nlog(n))
>>>>
>>>> Just ran it on my laptop:
>>>> Windows 7 64-bit
>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>> Core 2 Duo==> 2 cores 2GHz
>>>>
>>>> 2M int[]:
>>>> single-threaded: ~330ms
>>>> ForkJoin (2 workers): ~390ms
>>>>
>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>
>>>> -T
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathan.reynolds at oracle.com  Wed Feb 13 09:41:54 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 07:41:54 -0700
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <CAHjP37ESWSV_xODMMxGCG6ed50ynSoCwM1kqntQaHGFMQk+8yA@mail.gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAEJX8oq7HU9xo_YxvgNrFKvtLsHuYZDxZCyx7jkAD5h86Kpeuw@mail.gmail.com>
	<CAHjP37G8O+Azf_w0Ew_J-kR9Q9qPymzTcQqe_2SNCCvH3vOLpA@mail.gmail.com>
	<CAEJX8ooD-a6huNLXF6pNxRgk9gFe3hNos9HCf8Axc8c=MFJqrw@mail.gmail.com>
	<CAHjP37ESWSV_xODMMxGCG6ed50ynSoCwM1kqntQaHGFMQk+8yA@mail.gmail.com>
Message-ID: <511BA632.3020900@oracle.com>

If we ever get around to adding ProcessorLocal and its implied processor 
id and core-distance computing method, then we should probably consider 
adding APIs which describe the system upon which the JVM is running.  
This should include the number of hardware threads and which physical 
cores they belong to, the number of physical cores and which processor 
socket they belong to and the number of processor sockets.  This model 
comes from a Intel MP system point of view. There are probably other 
machine configurations which need to be accounted for in the API.  After 
this is specified, then affinity masks make sense.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 6:47 AM, Vitaly Davidovich wrote:
>
> If you're using an affinity mask, sure.  The advantage of having this 
> in jdk is cross platform support and not having people write the same 
> code across their apps.
>
> Sent from my phone
>
> On Feb 13, 2013 8:39 AM, "Stanimir Simeonoff" <stanimir at riflexo.com 
> <mailto:stanimir at riflexo.com>> wrote:
>
>
>
>     On Wed, Feb 13, 2013 at 3:32 PM, Vitaly Davidovich
>     <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>         /proc/cpuinfo parsing is the way to go but would be nice to
>         have a "machine spec/capability" API baked into the JDK.
>
>     But you still to get info on which CPU the process is limited to,
>     i.e. affinity. Which is the harder task. OTOH, /proc/cpuinfo is
>     quite trivial to parse.
>
>     Stanimir
>
>         Sent from my phone
>
>         On Feb 13, 2013 5:49 AM, "Stanimir Simeonoff"
>         <stanimir at riflexo.com <mailto:stanimir at riflexo.com>> wrote:
>
>
>                 Is there a way to know how many physical processors
>                 are available on the JVM?
>
>             On linux you can use taskset -p <pid>
>             (pid you can retrieve form /proc/self )
>             and you can use /proc/cpuinfo to determine which
>             processors have siblings.
>
>             Quite inconvenient overall. So kernels might have
>             /proc/self/affinity which would make life easier .
>             And of course there is JNI and syscall.
>
>             Stanimir
>
>
>                 Cheers,
>                 ?
>
>
>
>                     Nathan Reynolds
>                     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>                     | Architect | 602.333.9091 <tel:602.333.9091>
>                     Oracle PSR Engineering <http://psr.us.oracle.com/>
>                     | Server Technology
>                     On 2/12/2013 2:18 PM, ?iktor ?lang wrote:
>>
>>
>>
>>                     On Tue, Feb 12, 2013 at 8:28 PM, Kirk Pepperdine
>>                     <kirk at kodewerk.com <mailto:kirk at kodewerk.com>> wrote:
>>
>>                         >
>>                         > Do you agree that thread pool sizing
>>                         depends on type of work? (IO bound vs CPU
>>                         bound, bursty vs steady etc etc)
>>                         Yes
>>                         > Do you agree that a JVM Thread is not a
>>                         unit of parallelism?
>>                         Yes
>>                         > Do you agree that having more JVM Threads
>>                         than hardware threads is bad for CPU-bound
>>                         workloads?
>>                         No, even with CPU bound workloads I have
>>                         found that the hardware/OS is much better at
>>                         managing many workloads across many threads
>>                         than I am. So a few more threads is ok, many
>>                         more threads is bad fast.
>>
>>
>>                     That's an interesting observation. Have any more
>>                     data on that? (really interested)
>>                     As I said earlier, for CPU-bound workloads we've
>>                     seen the best performance when only loading
>>                     60-70% of the cores (other threads exist on the
>>                     machine of course).
>>
>>                     Cheers,
>>
>>                     ?
>>
>>
>>                         Regards,
>>                         Kirk
>>
>>
>>
>>
>>                     -- 
>>                     *Viktor Klang*
>>                     /Director of Engineering/
>>                     /
>>                     /
>>                     Typesafe <http://www.typesafe.com/>- The software
>>                     stack for applications that scale
>>                     Twitter: @viktorklang
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>                     Concurrency-interest at cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>                 -- 
>                 *Viktor Klang*
>                 /Director of Engineering/
>                 /
>                 /
>                 Typesafe <http://www.typesafe.com/>- The software
>                 stack for applications that scale
>                 Twitter: @viktorklang
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/f6bf2fbb/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Feb 13 09:50:46 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 07:50:46 -0700
Subject: [concurrency-interest] Thread Allocation
In-Reply-To: <B4F600C5-62CE-427B-A626-366004323701@gmail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
	<CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>
	<op.wsfr2spn7o0gc3@mail.cox.net>
	<B4F600C5-62CE-427B-A626-366004323701@gmail.com>
Message-ID: <511BA846.2070803@oracle.com>

I have heard that building your system to deal with asynchronous I/O 
will perform much better.  With synchronous I/O, the thread blocks and 
waits.  This incurs 2 context switches as well as ties up a thread and 
all of its resources.  With asynchronous I/O, the thread submits the I/O 
request and continues to do other processing.  When the I/O completes, a 
thread picks up the result and continues processing. Asynchronous I/O 
allows 1 thread to submit enough I/O requests that the underlying 
storage system can optimize how the requests are stored.  For example, a 
bunch of random writes using synchronous I/O will cause the disk head to 
seek wildly since each write has to go to a different location.  A bunch 
of random writes using asynchronous I/O will give the underlying system 
a chance to sort the writes and have the disk head make a single pass 
over the disk.  Disk performance will greatly improve.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 2:34 AM, Chris Vest wrote:
> If you can tell before hand which tasks (that you submit to the thread 
> pools) are going to be IO bound and which are going to be CPU bound, 
> then you can have to separate thread pools: a big one for the IO bound 
> tasks and a small one for he CPU bound ones.
>
> Otherwise I'd say just set a high upper bound (upwards hundreds, but 
> depends on expected distribution) and let the OS manage things, see 
> how that works and if its performant enough, then you're done.
>
> Note that I have no idea what kind of performance is expected of your 
> SIEM system.
>
> Chris
>
> On 13/02/2013, at 09.48, "Pete Haidinyak" <javamann at cox.net 
> <mailto:javamann at cox.net>> wrote:
>
>> I have a question on how to allocate Threads. I am creating a SIEM 
>> which is a bunch of independent Java Services. The most likely use 
>> case is this will run on one 2U box. The box will have two quad core 
>> Xeon processors and 32G of RAM. Some of the Services will be I/O 
>> bound but some will be CPU bound.
>>   In one of the latest discussion it was mentions that you should 
>> allocate a Thread for each core (plus or minus a couple) for the best 
>> throughput. I have the ability to turn the Thread Pools after startup 
>> based on the number and types of Services running on the box.
>>
>> My question is what would be the best way to allocate Threads when 
>> you have multiple processes competing for resources?
>>
>> Thanks
>>
>> -Pete
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/dfbcb000/attachment.html>

From holger.hoffstaette at googlemail.com  Wed Feb 13 09:52:30 2013
From: holger.hoffstaette at googlemail.com (=?UTF-8?B?SG9sZ2VyIEhvZmZzdMOkdHRl?=)
Date: Wed, 13 Feb 2013 15:52:30 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <511BA541.4090800@googlemail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
	<511B9986.3000806@googlemail.com>
	<CANPzfU943SxEvWkoQSe6j9wc6wGs-Bh_cM4pFKBMEkA3ozOgDA@mail.gmail.com>
	<511BA541.4090800@googlemail.com>
Message-ID: <511BA8AE.5070606@googlemail.com>


I have to eat my words, at least some ;)

On 13.02.2013 15:37, Holger Hoffst?tte wrote:
> On 13.02.2013 15:21, ?iktor ?lang wrote:
>> What's even more interesting is the the available processors can change
>> at runtime, making it extremely hard to do anything interesting with the
>> info.
> 
> Well..see towards the end of [1]. However this approach is obviously
> useless for Java, so you'd have to do something internal, like register
> inotify callbacks on the /sys list or the like. Unfortunately that
> doesn't work either [2]. Things go downhill from there.

So at least (just as the Javadocs say) periodically polling
Runtime#availableProcessors() works fine - I just tried.

> All statically wired thread pools (hi FJ) also suffer from this.

This problem remains. Maybe the Runtime MBean could offer events so that
individual subsystems can subscribe to them, and up/downscale appropriately.

-h


From thurston at nomagicsoftware.com  Wed Feb 13 10:27:21 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Wed, 13 Feb 2013 07:27:21 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <511BA5C9.3000003@gmail.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
Message-ID: <ee400a66923639a90b051d832afb3222@nomagicsoftware.com>

Could you publish your machine specs?
And what was the size of your sample array?  2M or something else?
If it is 2M (and if I'm doing my math right), it's weird that my times 
are less:
basically for 2M/250, I get around 370ms +- 25ms for the MergeSorter
and around 330ms for java's Array.sort

Thanks

On 2013-02-13 06:40, Peter Levart wrote:
> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>> Here is all of the relevant code in a single blob.  I think it's 
>> selef-explanatory; sample test is at the end.
>>
>> http://pastebin.com/WrfBHYSG
>
> Hi Thurston,
>
> You're doing too much copying. I modified tour MergeSorter a little:
>
>
> public class MergeSorter2 extends RecursiveTask<int[]> {
>     static final int THRESHOLD = 250;
>     final int[] array, work;
>     final int offset, length;
>
>     public MergeSorter2(int[] array) {
>         this.array = array;
>         this.work = new int[array.length];
>         offset = 0;
>         length = array.length;
>     }
>
>     private MergeSorter2(int[] array, int[] work, int offset, int 
> length) {
>         this.array = array;
>         this.work = work;
>         this.offset = offset;
>         this.length = length;
>     }
>
>     @Override
>     protected int[] compute() {
>         if (length < MergeSorter2.THRESHOLD) {
>             Arrays.sort(array, offset, offset + length);
>             return this.array;
>         }
>
>         int halfLength = length >> 1;
>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
> halfLength);
>         left.fork();
>
>         int mid = offset + halfLength;
>
>         MergeSorter2 right = new MergeSorter2(array, work, mid,
> length - halfLength);
>         right.compute();
>         left.join();
>
>         // copy 1st half from array to work
>         System.arraycopy(array, offset, work, offset, halfLength);
>
>         // merge 1st half of work and 2nd half of array into array
>         int end = offset + length;
>         int i = offset;
>         int j = mid;
>         int k = offset;
>         int x = work[i];
>         int y = array[j];
>         while (true) {
>             if (j >= end || x <= y) {
>                 array[k++] = x;
>                 i++;
>                 // until we drain the 1st half (the rest of array is
> already in place)
>                 if (i >= mid) break;
>                 x = work[i];
>             } else { // j < end
>                 array[k++] = y;
>                 j++;
>                 if (j < end) y = array[j];
>             }
>         }
>
>         return this.array;
>     }
> }
>
>
> ... it uses a single parallel "work" array for merging, which is
> shared among tasks. It tries to minimize the copying. Here's the
> results I get on a FJPool with parallelism=2:
>
>  MergeSorter: 1557638090 nanos
>  MergeSorter: 1545600414 nanos
>  MergeSorter: 1478185502 nanos
> MergeSorter2: 1058750062 nanos
> MergeSorter2:  978295370 nanos
> MergeSorter2:  976838429 nanos
>  Arrays.sort: 1494256525 nanos
>  Arrays.sort: 1505639161 nanos
>  Arrays.sort: 1483640342 nanos
>
>
> I get even better results with greater THRESHOLD (but not more than
> 10% better). Your MergeSorter is worse with greater THRESHOLD because
> you are using insert-sort for final tasks. The Arrays.sort() 
> delegates
> to insert-sort only when N < 47, else it uses quick-sort...
>
> Regards, Peter
>
>
>
>>
>>
>>
>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>> Yes, I will push the code and my measurements some time this week.
>>> I'm really interested in seeing what others' results would be. What 
>>> I
>>> can say is that the timings are surprisingly stable (range 30-40ms)
>>> and the input data is randomly generated for each test run which 
>>> can
>>> certainly affect merge(int[], int[])'s performance
>>>
>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>> Hi,
>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>> Hi T,
>>>>
>>>> Can you pub the raw data from your runs? Average tends to hide
>>>> effects and it's difficult to say anything with only that measure.
>>>>
>>>> Regards,
>>>> Kirk
>>>>
>>>>
>>>>> Hello,
>>>>>
>>>>> I made some initial attempts at using ForkJoin framework for some 
>>>>> rather obvious recursively parallel use-cases, namely summing the 
>>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>>
>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>
>>>>> I really enjoy using the framework and find it relatively easy to 
>>>>> reason about my programs (if not necessarily the internals of the 
>>>>> framework).
>>>>> But I was disappointed with the results: in both cases they were 
>>>>> slower than the corresponding Java serial implementation.
>>>>>
>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>> expecting 2x speedup, but especially in the case of the sort, but I 
>>>>> was expecting that ForkJoin would do at least a little better than 
>>>>> the single-threaded version.  I guess what I'm asking is: are these 
>>>>> results surprising?  Does it mean I'm doing something wrong?
>>>>>
>>>>> I'll give a brief description of my implementation:
>>>>> single-threaded:  Arrays.sort(int[])
>>>>>
>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>>>>> at by trial-and-error)
>>>>>
>>>>> int[] compute()
>>>>> {
>>>>> if int[].length < THRESHOLD
>>>>>    return insertionSort(int[])
>>>>> left = new MergeSort(int[] //split in half)
>>>>> right = new MergeSort(int[] //other half)
>>>>> return merge(right.compute(), left.join())
>>>>> }
>>>>>
>>>>> The insertionSort() and merge() methods are just standard 
>>>>> implementations; there is a bit of apples to oranges comparison 
>>>>> since Arrays.sort() uses an optimized quicksort, but we're still 
>>>>> talking O(nlog(n))
>>>>>
>>>>> Just ran it on my laptop:
>>>>> Windows 7 64-bit
>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>
>>>>> 2M int[]:
>>>>> single-threaded: ~330ms
>>>>> ForkJoin (2 workers): ~390ms
>>>>>
>>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>>
>>>>> -T
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.levart at gmail.com  Wed Feb 13 10:34:16 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 13 Feb 2013 16:34:16 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
	<ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
Message-ID: <511BB278.3030409@gmail.com>

On 02/13/2013 04:27 PM, thurston at nomagicsoftware.com wrote:
> Could you publish your machine specs?
> And what was the size of your sample array?  2M or something else?
> If it is 2M (and if I'm doing my math right), it's weird that my times 
> are less:
> basically for 2M/250, I get around 370ms +- 25ms for the MergeSorter
> and around 330ms for java's Array.sort

Oh, sorry. My machine is i7 Linux PC, using recent JDK8 build. I had to 
increase the N to 20M to actually make it sweat a little...

Your unchanged MergeSorter is better than Arrays.sort with parallelism = 
4 for example:

MergeSorter: 1134740049 nanos
MergeSorter: 1082783496 nanos
MergeSorter: 1033780556 nanos
MergeSorter2: 720092808 nanos
MergeSorter2: 649416922 nanos
MergeSorter2: 605587079 nanos
Arrays.sort: 1454093373 nanos
Arrays.sort: 1466476368 nanos
Arrays.sort: 1461604292 nanos

...but with only 2 threads it is on-par with Arrays.sort.

Regards, Peter

>
> Thanks
>
> On 2013-02-13 06:40, Peter Levart wrote:
>> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>>> Here is all of the relevant code in a single blob.  I think it's 
>>> selef-explanatory; sample test is at the end.
>>>
>>> http://pastebin.com/WrfBHYSG
>>
>> Hi Thurston,
>>
>> You're doing too much copying. I modified tour MergeSorter a little:
>>
>>
>> public class MergeSorter2 extends RecursiveTask<int[]> {
>>     static final int THRESHOLD = 250;
>>     final int[] array, work;
>>     final int offset, length;
>>
>>     public MergeSorter2(int[] array) {
>>         this.array = array;
>>         this.work = new int[array.length];
>>         offset = 0;
>>         length = array.length;
>>     }
>>
>>     private MergeSorter2(int[] array, int[] work, int offset, int 
>> length) {
>>         this.array = array;
>>         this.work = work;
>>         this.offset = offset;
>>         this.length = length;
>>     }
>>
>>     @Override
>>     protected int[] compute() {
>>         if (length < MergeSorter2.THRESHOLD) {
>>             Arrays.sort(array, offset, offset + length);
>>             return this.array;
>>         }
>>
>>         int halfLength = length >> 1;
>>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
>> halfLength);
>>         left.fork();
>>
>>         int mid = offset + halfLength;
>>
>>         MergeSorter2 right = new MergeSorter2(array, work, mid,
>> length - halfLength);
>>         right.compute();
>>         left.join();
>>
>>         // copy 1st half from array to work
>>         System.arraycopy(array, offset, work, offset, halfLength);
>>
>>         // merge 1st half of work and 2nd half of array into array
>>         int end = offset + length;
>>         int i = offset;
>>         int j = mid;
>>         int k = offset;
>>         int x = work[i];
>>         int y = array[j];
>>         while (true) {
>>             if (j >= end || x <= y) {
>>                 array[k++] = x;
>>                 i++;
>>                 // until we drain the 1st half (the rest of array is
>> already in place)
>>                 if (i >= mid) break;
>>                 x = work[i];
>>             } else { // j < end
>>                 array[k++] = y;
>>                 j++;
>>                 if (j < end) y = array[j];
>>             }
>>         }
>>
>>         return this.array;
>>     }
>> }
>>
>>
>> ... it uses a single parallel "work" array for merging, which is
>> shared among tasks. It tries to minimize the copying. Here's the
>> results I get on a FJPool with parallelism=2:
>>
>>  MergeSorter: 1557638090 nanos
>>  MergeSorter: 1545600414 nanos
>>  MergeSorter: 1478185502 nanos
>> MergeSorter2: 1058750062 nanos
>> MergeSorter2:  978295370 nanos
>> MergeSorter2:  976838429 nanos
>>  Arrays.sort: 1494256525 nanos
>>  Arrays.sort: 1505639161 nanos
>>  Arrays.sort: 1483640342 nanos
>>
>>
>> I get even better results with greater THRESHOLD (but not more than
>> 10% better). Your MergeSorter is worse with greater THRESHOLD because
>> you are using insert-sort for final tasks. The Arrays.sort() delegates
>> to insert-sort only when N < 47, else it uses quick-sort...
>>
>> Regards, Peter
>>
>>
>>
>>>
>>>
>>>
>>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>>> Yes, I will push the code and my measurements some time this week.
>>>> I'm really interested in seeing what others' results would be. What I
>>>> can say is that the timings are surprisingly stable (range 30-40ms)
>>>> and the input data is randomly generated for each test run which can
>>>> certainly affect merge(int[], int[])'s performance
>>>>
>>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>>> Hi,
>>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>>> Hi T,
>>>>>
>>>>> Can you pub the raw data from your runs? Average tends to hide
>>>>> effects and it's difficult to say anything with only that measure.
>>>>>
>>>>> Regards,
>>>>> Kirk
>>>>>
>>>>>
>>>>>> Hello,
>>>>>>
>>>>>> I made some initial attempts at using ForkJoin framework for some 
>>>>>> rather obvious recursively parallel use-cases, namely summing the 
>>>>>> elements of an int[] and also sorting an int[] (randomly filled).
>>>>>>
>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>
>>>>>> I really enjoy using the framework and find it relatively easy to 
>>>>>> reason about my programs (if not necessarily the internals of the 
>>>>>> framework).
>>>>>> But I was disappointed with the results: in both cases they were 
>>>>>> slower than the corresponding Java serial implementation.
>>>>>>
>>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>>> expecting 2x speedup, but especially in the case of the sort, but 
>>>>>> I was expecting that ForkJoin would do at least a little better 
>>>>>> than the single-threaded version.  I guess what I'm asking is: 
>>>>>> are these results surprising?  Does it mean I'm doing something 
>>>>>> wrong?
>>>>>>
>>>>>> I'll give a brief description of my implementation:
>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>
>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>>>>>> at by trial-and-error)
>>>>>>
>>>>>> int[] compute()
>>>>>> {
>>>>>> if int[].length < THRESHOLD
>>>>>>    return insertionSort(int[])
>>>>>> left = new MergeSort(int[] //split in half)
>>>>>> right = new MergeSort(int[] //other half)
>>>>>> return merge(right.compute(), left.join())
>>>>>> }
>>>>>>
>>>>>> The insertionSort() and merge() methods are just standard 
>>>>>> implementations; there is a bit of apples to oranges comparison 
>>>>>> since Arrays.sort() uses an optimized quicksort, but we're still 
>>>>>> talking O(nlog(n))
>>>>>>
>>>>>> Just ran it on my laptop:
>>>>>> Windows 7 64-bit
>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>
>>>>>> 2M int[]:
>>>>>> single-threaded: ~330ms
>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>
>>>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>>>
>>>>>> -T
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From thurston at nomagicsoftware.com  Wed Feb 13 10:59:38 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Wed, 13 Feb 2013 07:59:38 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <511BB278.3030409@gmail.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
	<ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
	<511BB278.3030409@gmail.com>
Message-ID: <b7f1522b4b8abdb3ef28e7b26ca50d8d@nomagicsoftware.com>

so you have 4 cores?
Makes sense that those results correspond to 20M as opposed to 2M
And given that you changed insertSort() -> Arrays.sort, if you change 
threshold to input.length / ForkJoinPool.threads (presumably 4 in your 
case), does that perform better?

Many people were suggesting that the # of tasks created should be 
bounded by the # of threads in the fjpool which should == # of hardware 
threads. (you can accomplish this by setting the threshold to above 
formula).

And with 20M/250 (threshold), you would be creating what 100Ks?, 1Ms? 
(you can comment in the AtomicInteger.increment()) in the constructor to 
save on the math

And of course *I* understand the performance characteristics of 
insertionSort, look at the implementation in SortUtil (which is not 
parallel).  What I did makes perfect sense if you understand insertSort 
vs mergeSort

On 2013-02-13 07:34, Peter Levart wrote:
> On 02/13/2013 04:27 PM, thurston at nomagicsoftware.com wrote:
>> Could you publish your machine specs?
>> And what was the size of your sample array?  2M or something else?
>> If it is 2M (and if I'm doing my math right), it's weird that my 
>> times are less:
>> basically for 2M/250, I get around 370ms +- 25ms for the MergeSorter
>> and around 330ms for java's Array.sort
>
> Oh, sorry. My machine is i7 Linux PC, using recent JDK8 build. I had
> to increase the N to 20M to actually make it sweat a little...
>
> Your unchanged MergeSorter is better than Arrays.sort with
> parallelism = 4 for example:
>
> MergeSorter: 1134740049 nanos
> MergeSorter: 1082783496 nanos
> MergeSorter: 1033780556 nanos
> MergeSorter2: 720092808 nanos
> MergeSorter2: 649416922 nanos
> MergeSorter2: 605587079 nanos
> Arrays.sort: 1454093373 nanos
> Arrays.sort: 1466476368 nanos
> Arrays.sort: 1461604292 nanos
>
> ...but with only 2 threads it is on-par with Arrays.sort.
>
> Regards, Peter
>
>>
>> Thanks
>>
>> On 2013-02-13 06:40, Peter Levart wrote:
>>> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>>>> Here is all of the relevant code in a single blob.  I think it's 
>>>> selef-explanatory; sample test is at the end.
>>>>
>>>> http://pastebin.com/WrfBHYSG
>>>
>>> Hi Thurston,
>>>
>>> You're doing too much copying. I modified tour MergeSorter a 
>>> little:
>>>
>>>
>>> public class MergeSorter2 extends RecursiveTask<int[]> {
>>>     static final int THRESHOLD = 250;
>>>     final int[] array, work;
>>>     final int offset, length;
>>>
>>>     public MergeSorter2(int[] array) {
>>>         this.array = array;
>>>         this.work = new int[array.length];
>>>         offset = 0;
>>>         length = array.length;
>>>     }
>>>
>>>     private MergeSorter2(int[] array, int[] work, int offset, int 
>>> length) {
>>>         this.array = array;
>>>         this.work = work;
>>>         this.offset = offset;
>>>         this.length = length;
>>>     }
>>>
>>>     @Override
>>>     protected int[] compute() {
>>>         if (length < MergeSorter2.THRESHOLD) {
>>>             Arrays.sort(array, offset, offset + length);
>>>             return this.array;
>>>         }
>>>
>>>         int halfLength = length >> 1;
>>>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
>>> halfLength);
>>>         left.fork();
>>>
>>>         int mid = offset + halfLength;
>>>
>>>         MergeSorter2 right = new MergeSorter2(array, work, mid,
>>> length - halfLength);
>>>         right.compute();
>>>         left.join();
>>>
>>>         // copy 1st half from array to work
>>>         System.arraycopy(array, offset, work, offset, halfLength);
>>>
>>>         // merge 1st half of work and 2nd half of array into array
>>>         int end = offset + length;
>>>         int i = offset;
>>>         int j = mid;
>>>         int k = offset;
>>>         int x = work[i];
>>>         int y = array[j];
>>>         while (true) {
>>>             if (j >= end || x <= y) {
>>>                 array[k++] = x;
>>>                 i++;
>>>                 // until we drain the 1st half (the rest of array 
>>> is
>>> already in place)
>>>                 if (i >= mid) break;
>>>                 x = work[i];
>>>             } else { // j < end
>>>                 array[k++] = y;
>>>                 j++;
>>>                 if (j < end) y = array[j];
>>>             }
>>>         }
>>>
>>>         return this.array;
>>>     }
>>> }
>>>
>>>
>>> ... it uses a single parallel "work" array for merging, which is
>>> shared among tasks. It tries to minimize the copying. Here's the
>>> results I get on a FJPool with parallelism=2:
>>>
>>>  MergeSorter: 1557638090 nanos
>>>  MergeSorter: 1545600414 nanos
>>>  MergeSorter: 1478185502 nanos
>>> MergeSorter2: 1058750062 nanos
>>> MergeSorter2:  978295370 nanos
>>> MergeSorter2:  976838429 nanos
>>>  Arrays.sort: 1494256525 nanos
>>>  Arrays.sort: 1505639161 nanos
>>>  Arrays.sort: 1483640342 nanos
>>>
>>>
>>> I get even better results with greater THRESHOLD (but not more than
>>> 10% better). Your MergeSorter is worse with greater THRESHOLD 
>>> because
>>> you are using insert-sort for final tasks. The Arrays.sort() 
>>> delegates
>>> to insert-sort only when N < 47, else it uses quick-sort...
>>>
>>> Regards, Peter
>>>
>>>
>>>
>>>>
>>>>
>>>>
>>>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>>>> Yes, I will push the code and my measurements some time this 
>>>>> week.
>>>>> I'm really interested in seeing what others' results would be. 
>>>>> What I
>>>>> can say is that the timings are surprisingly stable (range 
>>>>> 30-40ms)
>>>>> and the input data is randomly generated for each test run which 
>>>>> can
>>>>> certainly affect merge(int[], int[])'s performance
>>>>>
>>>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>>>> Hi,
>>>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>>>> Hi T,
>>>>>>
>>>>>> Can you pub the raw data from your runs? Average tends to hide
>>>>>> effects and it's difficult to say anything with only that 
>>>>>> measure.
>>>>>>
>>>>>> Regards,
>>>>>> Kirk
>>>>>>
>>>>>>
>>>>>>> Hello,
>>>>>>>
>>>>>>> I made some initial attempts at using ForkJoin framework for 
>>>>>>> some rather obvious recursively parallel use-cases, namely 
>>>>>>> summing the elements of an int[] and also sorting an int[] 
>>>>>>> (randomly filled).
>>>>>>>
>>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>>
>>>>>>> I really enjoy using the framework and find it relatively easy 
>>>>>>> to reason about my programs (if not necessarily the internals of 
>>>>>>> the framework).
>>>>>>> But I was disappointed with the results: in both cases they 
>>>>>>> were slower than the corresponding Java serial implementation.
>>>>>>>
>>>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>>>> expecting 2x speedup, but especially in the case of the sort, but 
>>>>>>> I was expecting that ForkJoin would do at least a little better 
>>>>>>> than the single-threaded version.  I guess what I'm asking is: 
>>>>>>> are these results surprising?  Does it mean I'm doing something 
>>>>>>> wrong?
>>>>>>>
>>>>>>> I'll give a brief description of my implementation:
>>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>>
>>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>>>>>>> at by trial-and-error)
>>>>>>>
>>>>>>> int[] compute()
>>>>>>> {
>>>>>>> if int[].length < THRESHOLD
>>>>>>>    return insertionSort(int[])
>>>>>>> left = new MergeSort(int[] //split in half)
>>>>>>> right = new MergeSort(int[] //other half)
>>>>>>> return merge(right.compute(), left.join())
>>>>>>> }
>>>>>>>
>>>>>>> The insertionSort() and merge() methods are just standard 
>>>>>>> implementations; there is a bit of apples to oranges comparison 
>>>>>>> since Arrays.sort() uses an optimized quicksort, but we're still 
>>>>>>> talking O(nlog(n))
>>>>>>>
>>>>>>> Just ran it on my laptop:
>>>>>>> Windows 7 64-bit
>>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>>
>>>>>>> 2M int[]:
>>>>>>> single-threaded: ~330ms
>>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>>
>>>>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>>>>
>>>>>>> -T
>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>


From nathan.reynolds at oracle.com  Wed Feb 13 11:24:59 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 09:24:59 -0700
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <511BA8AE.5070606@googlemail.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<CANPzfU9SPjsrKYYt9UgxOxbmRdnau3AshS1uAvPu1XM9ffq-nw@mail.gmail.com>
	<CAHji151kmscB8WP01cFsfs9jYfot+AMbRTUZYRtcErY_KGLCbw@mail.gmail.com>
	<511B9986.3000806@googlemail.com>
	<CANPzfU943SxEvWkoQSe6j9wc6wGs-Bh_cM4pFKBMEkA3ozOgDA@mail.gmail.com>
	<511BA541.4090800@googlemail.com> <511BA8AE.5070606@googlemail.com>
Message-ID: <511BBE5B.5050009@oracle.com>

Alex on this list recently bumped into a problem.  If all of the logical 
cores are disabled except 1, then HotSpot will assume a uniprocessor 
machine and enable all sorts of optimizations.

We also noticed some other issues.  If the number of enabled cores 
dynamically changes, then the number of GC threads doesn't adjust and 
the heap layout doesn't account for the new NUMA characteristics.  We 
also wonder if fork-join framework can deal with the change and achieve 
maximum throughput with the new configuration.

There are many layers of code which need to deal with core changes.  
Having N threads do polling doesn't sound like a great idea.  I like the 
MBean event subscription idea.  The JVM already should be checking and 
when it detects a change, then it fires the MBean event.  But, how do we 
make sure all of the subscribers behave.  What if the event thread gets 
stuck inside one subscriber?  The rest of the subscribers don't get the 
notification.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 7:52 AM, Holger Hoffst?tte wrote:
> I have to eat my words, at least some ;)
>
> On 13.02.2013 15:37, Holger Hoffst?tte wrote:
>> On 13.02.2013 15:21, ?iktor ?lang wrote:
>>> What's even more interesting is the the available processors can change
>>> at runtime, making it extremely hard to do anything interesting with the
>>> info.
>> Well..see towards the end of [1]. However this approach is obviously
>> useless for Java, so you'd have to do something internal, like register
>> inotify callbacks on the /sys list or the like. Unfortunately that
>> doesn't work either [2]. Things go downhill from there.
> So at least (just as the Javadocs say) periodically polling
> Runtime#availableProcessors() works fine - I just tried.
>
>> All statically wired thread pools (hi FJ) also suffer from this.
> This problem remains. Maybe the Runtime MBean could offer events so that
> individual subsystems can subscribe to them, and up/downscale appropriately.
>
> -h
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/9b643698/attachment.html>

From peter.levart at gmail.com  Wed Feb 13 11:30:22 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 13 Feb 2013 17:30:22 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <b7f1522b4b8abdb3ef28e7b26ca50d8d@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
	<ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
	<511BB278.3030409@gmail.com>
	<b7f1522b4b8abdb3ef28e7b26ca50d8d@nomagicsoftware.com>
Message-ID: <511BBF9E.9050504@gmail.com>

On 02/13/2013 04:59 PM, thurston at nomagicsoftware.com wrote:
> so you have 4 cores?
> Makes sense that those results correspond to 20M as opposed to 2M
> And given that you changed insertSort() -> Arrays.sort, if you change 
> threshold to input.length / ForkJoinPool.threads (presumably 4 in your 
> case), does that perform better?
It does (N=20M, THRESHOLD=5M, parallelism=4):

Arrays.sort: 1529550064 nanos
Arrays.sort: 1478430550 nanos
Arrays.sort: 1475643365 nanos
Arrays.sort: 1481308646 nanos
Arrays.sort: 1477199836 nanos
MergeSorter2: 500316422 nanos
MergeSorter2: 486144409 nanos
MergeSorter2: 468044083 nanos
MergeSorter2: 624446626 nanos
MergeSorter2: 635152854 nanos


>
> Many people were suggesting that the # of tasks created should be 
> bounded by the # of threads in the fjpool which should == # of 
> hardware threads. (you can accomplish this by setting the threshold to 
> above formula).

But as David Holmes has noted, the THRESHOLD should be further divided 
by 8 as a starting point, to accommodate for unequal execution of 
threads/tasks, so that there is no waiting at the end for final long 
tasks to complete while other threads in pool are IDLE-ing...

For example (N=20M, THRESHOLD=500K, parallelism=4):

MergeSorter2: 531712994 nanos
MergeSorter2: 539048356 nanos
MergeSorter2: 586208993 nanos
MergeSorter2: 544239837 nanos
MergeSorter2: 497440246 nanos

And (N=20M, THRESHOLD=50K, parallelism=4):

MergeSorter2: 548634308 nanos
MergeSorter2: 579119444 nanos
MergeSorter2: 555252699 nanos
MergeSorter2: 505902467 nanos
MergeSorter2: 503907973 nanos

And (N=20M, THRESHOLD=5K, parallelism=4):

MergeSorter2: 547342635 nanos
MergeSorter2: 575145627 nanos
MergeSorter2: 559160837 nanos
MergeSorter2: 530709697 nanos
MergeSorter2: 562272911 nanos

...it does perform a little slower, but in average with lower THRESHOLD, 
the performance should be more stable...


Regards, Peter

>
> And with 20M/250 (threshold), you would be creating what 100Ks?, 1Ms? 
> (you can comment in the AtomicInteger.increment()) in the constructor 
> to save on the math
>
> And of course *I* understand the performance characteristics of 
> insertionSort, look at the implementation in SortUtil (which is not 
> parallel).  What I did makes perfect sense if you understand 
> insertSort vs mergeSort
>
> On 2013-02-13 07:34, Peter Levart wrote:
>> On 02/13/2013 04:27 PM, thurston at nomagicsoftware.com wrote:
>>> Could you publish your machine specs?
>>> And what was the size of your sample array?  2M or something else?
>>> If it is 2M (and if I'm doing my math right), it's weird that my 
>>> times are less:
>>> basically for 2M/250, I get around 370ms +- 25ms for the MergeSorter
>>> and around 330ms for java's Array.sort
>>
>> Oh, sorry. My machine is i7 Linux PC, using recent JDK8 build. I had
>> to increase the N to 20M to actually make it sweat a little...
>>
>> Your unchanged MergeSorter is better than Arrays.sort with
>> parallelism = 4 for example:
>>
>> MergeSorter: 1134740049 nanos
>> MergeSorter: 1082783496 nanos
>> MergeSorter: 1033780556 nanos
>> MergeSorter2: 720092808 nanos
>> MergeSorter2: 649416922 nanos
>> MergeSorter2: 605587079 nanos
>> Arrays.sort: 1454093373 nanos
>> Arrays.sort: 1466476368 nanos
>> Arrays.sort: 1461604292 nanos
>>
>> ...but with only 2 threads it is on-par with Arrays.sort.
>>
>> Regards, Peter
>>
>>>
>>> Thanks
>>>
>>> On 2013-02-13 06:40, Peter Levart wrote:
>>>> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>>>>> Here is all of the relevant code in a single blob.  I think it's 
>>>>> selef-explanatory; sample test is at the end.
>>>>>
>>>>> http://pastebin.com/WrfBHYSG
>>>>
>>>> Hi Thurston,
>>>>
>>>> You're doing too much copying. I modified tour MergeSorter a little:
>>>>
>>>>
>>>> public class MergeSorter2 extends RecursiveTask<int[]> {
>>>>     static final int THRESHOLD = 250;
>>>>     final int[] array, work;
>>>>     final int offset, length;
>>>>
>>>>     public MergeSorter2(int[] array) {
>>>>         this.array = array;
>>>>         this.work = new int[array.length];
>>>>         offset = 0;
>>>>         length = array.length;
>>>>     }
>>>>
>>>>     private MergeSorter2(int[] array, int[] work, int offset, int 
>>>> length) {
>>>>         this.array = array;
>>>>         this.work = work;
>>>>         this.offset = offset;
>>>>         this.length = length;
>>>>     }
>>>>
>>>>     @Override
>>>>     protected int[] compute() {
>>>>         if (length < MergeSorter2.THRESHOLD) {
>>>>             Arrays.sort(array, offset, offset + length);
>>>>             return this.array;
>>>>         }
>>>>
>>>>         int halfLength = length >> 1;
>>>>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
>>>> halfLength);
>>>>         left.fork();
>>>>
>>>>         int mid = offset + halfLength;
>>>>
>>>>         MergeSorter2 right = new MergeSorter2(array, work, mid,
>>>> length - halfLength);
>>>>         right.compute();
>>>>         left.join();
>>>>
>>>>         // copy 1st half from array to work
>>>>         System.arraycopy(array, offset, work, offset, halfLength);
>>>>
>>>>         // merge 1st half of work and 2nd half of array into array
>>>>         int end = offset + length;
>>>>         int i = offset;
>>>>         int j = mid;
>>>>         int k = offset;
>>>>         int x = work[i];
>>>>         int y = array[j];
>>>>         while (true) {
>>>>             if (j >= end || x <= y) {
>>>>                 array[k++] = x;
>>>>                 i++;
>>>>                 // until we drain the 1st half (the rest of array is
>>>> already in place)
>>>>                 if (i >= mid) break;
>>>>                 x = work[i];
>>>>             } else { // j < end
>>>>                 array[k++] = y;
>>>>                 j++;
>>>>                 if (j < end) y = array[j];
>>>>             }
>>>>         }
>>>>
>>>>         return this.array;
>>>>     }
>>>> }
>>>>
>>>>
>>>> ... it uses a single parallel "work" array for merging, which is
>>>> shared among tasks. It tries to minimize the copying. Here's the
>>>> results I get on a FJPool with parallelism=2:
>>>>
>>>>  MergeSorter: 1557638090 nanos
>>>>  MergeSorter: 1545600414 nanos
>>>>  MergeSorter: 1478185502 nanos
>>>> MergeSorter2: 1058750062 nanos
>>>> MergeSorter2:  978295370 nanos
>>>> MergeSorter2:  976838429 nanos
>>>>  Arrays.sort: 1494256525 nanos
>>>>  Arrays.sort: 1505639161 nanos
>>>>  Arrays.sort: 1483640342 nanos
>>>>
>>>>
>>>> I get even better results with greater THRESHOLD (but not more than
>>>> 10% better). Your MergeSorter is worse with greater THRESHOLD because
>>>> you are using insert-sort for final tasks. The Arrays.sort() delegates
>>>> to insert-sort only when N < 47, else it uses quick-sort...
>>>>
>>>> Regards, Peter
>>>>
>>>>
>>>>
>>>>>
>>>>>
>>>>>
>>>>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>>>>> Yes, I will push the code and my measurements some time this week.
>>>>>> I'm really interested in seeing what others' results would be. 
>>>>>> What I
>>>>>> can say is that the timings are surprisingly stable (range 30-40ms)
>>>>>> and the input data is randomly generated for each test run which can
>>>>>> certainly affect merge(int[], int[])'s performance
>>>>>>
>>>>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>>>>> Hi,
>>>>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>>>>> Hi T,
>>>>>>>
>>>>>>> Can you pub the raw data from your runs? Average tends to hide
>>>>>>> effects and it's difficult to say anything with only that measure.
>>>>>>>
>>>>>>> Regards,
>>>>>>> Kirk
>>>>>>>
>>>>>>>
>>>>>>>> Hello,
>>>>>>>>
>>>>>>>> I made some initial attempts at using ForkJoin framework for 
>>>>>>>> some rather obvious recursively parallel use-cases, namely 
>>>>>>>> summing the elements of an int[] and also sorting an int[] 
>>>>>>>> (randomly filled).
>>>>>>>>
>>>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>>>
>>>>>>>> I really enjoy using the framework and find it relatively easy 
>>>>>>>> to reason about my programs (if not necessarily the internals 
>>>>>>>> of the framework).
>>>>>>>> But I was disappointed with the results: in both cases they 
>>>>>>>> were slower than the corresponding Java serial implementation.
>>>>>>>>
>>>>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>>>>> expecting 2x speedup, but especially in the case of the sort, 
>>>>>>>> but I was expecting that ForkJoin would do at least a little 
>>>>>>>> better than the single-threaded version.  I guess what I'm 
>>>>>>>> asking is: are these results surprising?  Does it mean I'm 
>>>>>>>> doing something wrong?
>>>>>>>>
>>>>>>>> I'll give a brief description of my implementation:
>>>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>>>
>>>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 (arrived 
>>>>>>>> at by trial-and-error)
>>>>>>>>
>>>>>>>> int[] compute()
>>>>>>>> {
>>>>>>>> if int[].length < THRESHOLD
>>>>>>>>    return insertionSort(int[])
>>>>>>>> left = new MergeSort(int[] //split in half)
>>>>>>>> right = new MergeSort(int[] //other half)
>>>>>>>> return merge(right.compute(), left.join())
>>>>>>>> }
>>>>>>>>
>>>>>>>> The insertionSort() and merge() methods are just standard 
>>>>>>>> implementations; there is a bit of apples to oranges comparison 
>>>>>>>> since Arrays.sort() uses an optimized quicksort, but we're 
>>>>>>>> still talking O(nlog(n))
>>>>>>>>
>>>>>>>> Just ran it on my laptop:
>>>>>>>> Windows 7 64-bit
>>>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>>>
>>>>>>>> 2M int[]:
>>>>>>>> single-threaded: ~330ms
>>>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>>>
>>>>>>>> Would appreciate any feedback and hopefully the #s are somewhat 
>>>>>>>> helpful (and please no scoffawing at my antiquated machine)
>>>>>>>>
>>>>>>>> -T
>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>


From thurston at nomagicsoftware.com  Wed Feb 13 11:36:14 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Wed, 13 Feb 2013 08:36:14 -0800
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
Message-ID: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>

Hello,

I was wondering what requires more overhead: reading a volatile 
reference or obtaining a lock when there is no contention.

Essentially I have a single-reader scenario, where the common-path 
(99%+) doesn't require either a volatile read or a lock; but if I want 
to support a few very-rarely used cases, I can only make it thread-safe 
by making the 99% case rely on a volatile read or obtain a lock.
Any guidelines?

Thanks


From vitalyd at gmail.com  Wed Feb 13 11:44:48 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 11:44:48 -0500
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
Message-ID: <CAHjP37G-XyMEOPmjtQ+_p1KFjvFgeLyi0=v2TSy6Wsf0ZhaADw@mail.gmail.com>

Lock will have higher overhead.  Depending on CPU, volatile load can be as
cheap as a normal load + a compiler barrier.  If you hit L1 cache, then
perf is going to be almost like normal load that gets enregistered by JIT.

Sent from my phone
On Feb 13, 2013 11:39 AM, <thurston at nomagicsoftware.com> wrote:

> Hello,
>
> I was wondering what requires more overhead: reading a volatile reference
> or obtaining a lock when there is no contention.
>
> Essentially I have a single-reader scenario, where the common-path (99%+)
> doesn't require either a volatile read or a lock; but if I want to support
> a few very-rarely used cases, I can only make it thread-safe by making the
> 99% case rely on a volatile read or obtain a lock.
> Any guidelines?
>
> Thanks
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/bbe17e1a/attachment.html>

From stanimir at riflexo.com  Wed Feb 13 11:54:06 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Wed, 13 Feb 2013 18:54:06 +0200
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
Message-ID: <CAEJX8opYYKKNZykCTQSQR7Ar1=M0BcNgHGBcya+1r8aXk6Bpbw@mail.gmail.com>

Obtaining the lock already requires reading a volatile and it involves an
atomic operation (volatile write at the least) + some state store.
The lock release needs volatile store (or at least ordered write) + queue
checking if anyone has gone asleep waiting for the lock.

Overall volatile reads are cheap and almost free on x86.

Stanimir

On Wed, Feb 13, 2013 at 6:36 PM, <thurston at nomagicsoftware.com> wrote:

> Hello,
>
> I was wondering what requires more overhead: reading a volatile reference
> or obtaining a lock when there is no contention.
>
> Essentially I have a single-reader scenario, where the common-path (99%+)
> doesn't require either a volatile read or a lock; but if I want to support
> a few very-rarely used cases, I can only make it thread-safe by making the
> 99% case rely on a volatile read or obtain a lock.
> Any guidelines?
>
> Thanks
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/95f899d1/attachment.html>

From nathan.reynolds at oracle.com  Wed Feb 13 11:54:09 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 09:54:09 -0700
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
Message-ID: <511BC531.4050709@oracle.com>

Reading a volatile on x86 is translated to a simple "mov" instruction.  
Obtaining a lock is a bit more involved.  If the lock is biased, then 
the thread will simply have to check to make sure it is still the 
owner.  It won't have to execute any atomic instructions.  If the lock 
is not biased, then it will have to execute an atomic instruction.  In 
the latter case, the atomic instruction is going to cost a lot.  If the 
former case, the instructions building up to the check and the check 
itself will cost much more than a "mov" instruction.  So, I would 
recommend reading the volatile... unless your intended platforms have a 
significant overhead for reading a volatile.

An alternative might be AlmostFinalValue 
(http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field). 
I haven't played with this yet.  However, it seems to promise the 
ability to have a field optimized as if it were final yet allow for it 
to be safely mutated.  The penalty is that every time the field is 
updated, the code using the field is deoptimized (i.e. runs as bytecode) 
and then is optimized again later by JIT.  So, the overall performance 
might be better than reading a volatile depending upon how many 9s 
follow the decimal point when you say (99%+).

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 9:36 AM, thurston at nomagicsoftware.com wrote:
> Hello,
>
> I was wondering what requires more overhead: reading a volatile 
> reference or obtaining a lock when there is no contention.
>
> Essentially I have a single-reader scenario, where the common-path 
> (99%+) doesn't require either a volatile read or a lock; but if I want 
> to support a few very-rarely used cases, I can only make it 
> thread-safe by making the 99% case rely on a volatile read or obtain a 
> lock.
> Any guidelines?
>
> Thanks
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/9f1dbeff/attachment.html>

From ariel at weisberg.ws  Wed Feb 13 13:05:06 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 13 Feb 2013 13:05:06 -0500
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <511BC531.4050709@oracle.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<511BC531.4050709@oracle.com>
Message-ID: <1360778706.20190.140661191011237.2C7612EA@webmail.messagingengine.com>

Hi,



Biased locking only comes into play for intrinsic locks right? If you
are using ReentrantLock or a class that uses ReentrantLock you won't
get the benefit of biased locking?



Thanks,

Ariel





On Wed, Feb 13, 2013, at 11:54 AM, Nathan Reynolds wrote:

Reading a volatile on x86 is translated to a simple "mov" instruction.
Obtaining a lock is a bit more involved.  If the lock is biased, then
the thread will simply have to check to make sure it is still the
owner.  It won't have to execute any atomic instructions.  If the lock
is not biased, then it will have to execute an atomic instruction.  In
the latter case, the atomic instruction is going to cost a lot.  If the
former case, the instructions building up to the check and the check
itself will cost much more than a "mov" instruction.  So, I would
recommend reading the volatile... unless your intended platforms have a
significant overhead for reading a volatile.

An alternative might be AlmostFinalValue
([1]http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodn
ess-almost-static-final-field).  I haven't played with this yet.
However, it seems to promise the ability to have a field optimized as
if it were final yet allow for it to be safely mutated.  The penalty is
that every time the field is updated, the code using the field is
deoptimized (i.e. runs as bytecode) and then is optimized again later
by JIT.  So, the overall performance might be better than reading a
volatile depending upon how many 9s follow the decimal point when you
say (99%+).

[2]Nathan Reynolds | Architect | 602.333.9091
Oracle[3]PSR Engineering | Server Technology
On 2/13/2013 9:36 AM, [4]thurston at nomagicsoftware.com wrote:

  Hello,
  I was wondering what requires more overhead: reading a volatile
  reference or obtaining a lock when there is no contention.
  Essentially I have a single-reader scenario, where the common-path
  (99%+) doesn't require either a volatile read or a lock; but if I
  want to support a few very-rarely used cases, I can only make it
  thread-safe by making the 99% case rely on a volatile read or obtain
  a lock.
  Any guidelines?
  Thanks
  _______________________________________________
  Concurrency-interest mailing list
  [5]Concurrency-interest at cs.oswego.edu
  [6]http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________

Concurrency-interest mailing list

[7]Concurrency-interest at cs.oswego.edu

[8]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field
2. http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds
3. http://psr.us.oracle.com/
4. mailto:thurston at nomagicsoftware.com
5. mailto:Concurrency-interest at cs.oswego.edu
6. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
7. mailto:Concurrency-interest at cs.oswego.edu
8. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/78543309/attachment.html>

From nathan.reynolds at oracle.com  Wed Feb 13 13:36:08 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 11:36:08 -0700
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <1360778706.20190.140661191011237.2C7612EA@webmail.messagingengine.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<511BC531.4050709@oracle.com>
	<1360778706.20190.140661191011237.2C7612EA@webmail.messagingengine.com>
Message-ID: <511BDD18.4000504@oracle.com>

Correct.  Biased locking is only for "synchronized" locks and none of 
the java.util.concurrent locks.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 11:05 AM, Ariel Weisberg wrote:
> Hi,
> Biased locking only comes into play for intrinsic locks right? If you 
> are using ReentrantLock or a class that uses ReentrantLock you won't 
> get the benefit of biased locking?
> Thanks,
> Ariel
> On Wed, Feb 13, 2013, at 11:54 AM, Nathan Reynolds wrote:
>> Reading a volatile on x86 is translated to a simple "mov" 
>> instruction.  Obtaining a lock is a bit more involved.  If the lock 
>> is biased, then the thread will simply have to check to make sure it 
>> is still the owner.  It won't have to execute any atomic 
>> instructions.  If the lock is not biased, then it will have to 
>> execute an atomic instruction.  In the latter case, the atomic 
>> instruction is going to cost a lot. If the former case, the 
>> instructions building up to the check and the check itself will cost 
>> much more than a "mov" instruction.  So, I would recommend reading 
>> the volatile... unless your intended platforms have a significant 
>> overhead for reading a volatile.
>> An alternative might be AlmostFinalValue 
>> (http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field). 
>> I haven't played with this yet.  However, it seems to promise the 
>> ability to have a field optimized as if it were final yet allow for 
>> it to be safely mutated.  The penalty is that every time the field is 
>> updated, the code using the field is deoptimized (i.e. runs as 
>> bytecode) and then is optimized again later by JIT.  So, the overall 
>> performance might be better than reading a volatile depending upon 
>> how many 9s follow the decimal point when you say (99%+).
>> Nathan Reynolds 
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
>> Architect | 602.333.9091
>> OraclePSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 2/13/2013 9:36 AM, thurston at nomagicsoftware.com wrote:
>>> Hello,
>>>
>>> I was wondering what requires more overhead: reading a volatile 
>>> reference or obtaining a lock when there is no contention.
>>>
>>> Essentially I have a single-reader scenario, where the common-path 
>>> (99%+) doesn't require either a volatile read or a lock; but if I 
>>> want to support a few very-rarely used cases, I can only make it 
>>> thread-safe by making the 99% case rely on a volatile read or obtain 
>>> a lock.
>>> Any guidelines?
>>>
>>> Thanks
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _________________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/5c5fae53/attachment.html>

From thurston at nomagicsoftware.com  Wed Feb 13 13:38:19 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Wed, 13 Feb 2013 10:38:19 -0800
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <CAEJX8opYYKKNZykCTQSQR7Ar1=M0BcNgHGBcya+1r8aXk6Bpbw@mail.gmail.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<CAEJX8opYYKKNZykCTQSQR7Ar1=M0BcNgHGBcya+1r8aXk6Bpbw@mail.gmail.com>
Message-ID: <c779370904adbf37e082af4c271aa651@nomagicsoftware.com>

Thanks for your responses.
My understanding is that a volatile *write* does entail invalidating 
all CPUs' L1,L2... caches that have cached the volatile reference.  But 
so does a lock/unlock cycle; is that more or less correct?

On 2013-02-13 08:54, Stanimir Simeonoff wrote:
> Obtaining the lock already requires reading a volatile and it
> involves an atomic operation (volatile write at the least) + some
> state store.
> The lock release needs volatile store (or at least ordered write) +
> queue checking if anyone has gone asleep waiting for the lock.
>
> Overall volatile reads are cheap and almost free on x86.
>
> Stanimir
>
> On Wed, Feb 13, 2013 at 6:36 PM, <thurston at nomagicsoftware.com> 
> wrote:
>
>> Hello,
>>
>> I was wondering what requires more overhead: reading a volatile 
>> reference or obtaining a lock when there is no contention.
>>
>> Essentially I have a single-reader scenario, where the common-path 
>> (99%+) doesn't require either a volatile read or a lock; but if I want 
>> to support a few very-rarely used cases, I can only make it 
>> thread-safe by making the 99% case rely on a volatile read or obtain a 
>> lock.
>> Any guidelines?
>>
>> Thanks
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>
>
>
> Links:
> ------
> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Wed Feb 13 13:38:47 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 13:38:47 -0500
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <1360778706.20190.140661191011237.2C7612EA@webmail.messagingengine.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<511BC531.4050709@oracle.com>
	<1360778706.20190.140661191011237.2C7612EA@webmail.messagingengine.com>
Message-ID: <CAHjP37Fi2vx_eJ+Etk4ysfmKGqWtJz0BDH28=5AEcQCqb6KRkg@mail.gmail.com>

That's correct.

Sent from my phone
On Feb 13, 2013 1:26 PM, "Ariel Weisberg" <ariel at weisberg.ws> wrote:

> **
> Hi,
>
> Biased locking only comes into play for intrinsic locks right? If you are
> using ReentrantLock or a class that uses ReentrantLock you won't get the
> benefit of biased locking?
>
> Thanks,
> Ariel
>
>
> On Wed, Feb 13, 2013, at 11:54 AM, Nathan Reynolds wrote:
>
> Reading a volatile on x86 is translated to a simple "mov" instruction.
> Obtaining a lock is a bit more involved.  If the lock is biased, then the
> thread will simply have to check to make sure it is still the owner.  It
> won't have to execute any atomic instructions.  If the lock is not biased,
> then it will have to execute an atomic instruction.  In the latter case,
> the atomic instruction is going to cost a lot.  If the former case, the
> instructions building up to the check and the check itself will cost much
> more than a "mov" instruction.  So, I would recommend reading the
> volatile... unless your intended platforms have a significant overhead for
> reading a volatile.
>
>  An alternative might be AlmostFinalValue (
> http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field).
> I haven't played with this yet.  However, it seems to promise the ability
> to have a field optimized as if it were final yet allow for it to be safely
> mutated.  The penalty is that every time the field is updated, the code
> using the field is deoptimized (i.e. runs as bytecode) and then is
> optimized again later by JIT.  So, the overall performance might be better
> than reading a volatile depending upon how many 9s follow the decimal point
> when you say (99%+).
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> OraclePSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/13/2013 9:36 AM, thurston at nomagicsoftware.com wrote:
>
> Hello,
>
> I was wondering what requires more overhead: reading a volatile reference
> or obtaining a lock when there is no contention.
>
> Essentially I have a single-reader scenario, where the common-path (99%+)
> doesn't require either a volatile read or a lock; but if I want to support
> a few very-rarely used cases, I can only make it thread-safe by making the
> 99% case rely on a volatile read or obtain a lock.
> Any guidelines?
>
> Thanks
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> *_______________________________________________*
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/d642a87b/attachment-0001.html>

From ldm at gmx.at  Wed Feb 13 13:40:26 2013
From: ldm at gmx.at (Markus Krainz)
Date: Wed, 13 Feb 2013 19:40:26 +0100
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <511BC531.4050709@oracle.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<511BC531.4050709@oracle.com>
Message-ID: <511BDE1A.5010502@gmx.at>

Am 13.02.2013 17:54, schrieb Nathan Reynolds:
> Reading a volatile on x86 is translated to a simple "mov" 
> instruction.  Obtaining a lock is a bit more involved.  If the lock is 
> biased, then the thread will simply have to check to make sure it is 
> still the owner.  It won't have to execute any atomic instructions.  
> If the lock is not biased, then it will have to execute an atomic 
> instruction. In the latter case, the atomic instruction is going to 
> cost a lot.  If the former case, the instructions building up to the 
> check and the check itself will cost much more than a "mov" 
> instruction.  So, I would recommend reading the volatile... unless 
> your intended platforms have a significant overhead for reading a 
> volatile.
>
> An alternative might be AlmostFinalValue 
> (http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field). 
> I haven't played with this yet.  However, it seems to promise the 
> ability to have a field optimized as if it were final yet allow for it 
> to be safely mutated.  The penalty is that every time the field is 
> updated, the code using the field is deoptimized (i.e. runs as 
> bytecode) and then is optimized again later by JIT.  So, the overall 
> performance might be better than reading a volatile depending upon how 
> many 9s follow the decimal point when you say (99%+).

This might be a stupid question, but is this AlmostFinalValue hack valid 
for non-static fields?
I was under impression that the thread starter wants to use this not 
just for static members.

>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/13/2013 9:36 AM, thurston at nomagicsoftware.com wrote:
>> Hello,
>>
>> I was wondering what requires more overhead: reading a volatile 
>> reference or obtaining a lock when there is no contention.
>>
>> Essentially I have a single-reader scenario, where the common-path 
>> (99%+) doesn't require either a volatile read or a lock; but if I 
>> want to support a few very-rarely used cases, I can only make it 
>> thread-safe by making the 99% case rely on a volatile read or obtain 
>> a lock.
>> Any guidelines?
>>
>> Thanks
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Regards, Markus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/4f9881bf/attachment.html>

From ariel at weisberg.ws  Wed Feb 13 13:43:57 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 13 Feb 2013 13:43:57 -0500
Subject: [concurrency-interest] Thread Allocation
In-Reply-To: <511BA846.2070803@oracle.com>
References: <62b416d8c12779dcb1a1c66015ef65a2@nomagicsoftware.com>
	<NFBBKALFDCPFIDBNKAPCGEEEJKAA.davidcholmes@aapt.net.au>
	<CANPzfU8YvJnDOhD5wORBuVaqRUXGZ8qVZfN4zaS8nqjHQ8v0+Q@mail.gmail.com>
	<ddd95b8bee8f72daa9154e6a9ee640a0@nomagicsoftware.com>
	<CANPzfU8cjmbJLMX=qEGuPWkYmtk5MWaAZV2iKNzqSV3K17FqMQ@mail.gmail.com>
	<8A593917-7A2B-4428-8942-2180BAB25D32@kodewerk.com>
	<CANPzfU_MXcr24qjfRC7xArr3-5+EGxmApV9y4p2FFVh35MtFEA@mail.gmail.com>
	<511AB87B.1040303@oracle.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236986F62@G9W0725.americas.hpqcorp.net>
	<CANPzfU9maSk4WZNUZ04_rC-GsfLSWCSE3x8YDVP_RNh0OSXkuQ@mail.gmail.com>
	<op.wsfr2spn7o0gc3@mail.cox.net>
	<B4F600C5-62CE-427B-A626-366004323701@gmail.com>
	<511BA846.2070803@oracle.com>
Message-ID: <1360781037.27783.140661191011709.7704676A@webmail.messagingengine.com>

Hi,



On Linux most people are not directly issuing disk writes. They are
modifying pages in the page cache and then Linux is flushing them
asynchronously in the background. Depending on the IO scheduler in play
you will get different scheduling and coalescing behaviors and the
actual disk and/or disk controller will also repeat that process. If
virtualization is in play you get yet another layer of IO indirection
and I hear the noop scheduler is the way to go in guest kernels.



Writes pretty much never block unless you are bursting beyond the
capacity of the page cache to absorb writes or you are being
backpressured because the disk can't keep up. Writes also won't block
on the disk unless there is a write barrier because disks and
controllers also have write caches.



Writes can block if the page you are modifying is not in the page
cache. If you always overwrite entire pages or append to a file you
usually won't have a problem.



Databases like to use O_DIRECT to bypass the page cache and manage this
stuff themselves because they understand that not all disk pages
contain the same kind of data.



I have found that using multiple threads to do blocking reads with NIO
works fine and I have no problem getting hundreds of thousands of reads
when the data is in the page cache. With an SSD I have no problem
getting the advertised number of random IOPs when the dataset is 5x
RAM.



You can run hdparm -I to find out the queue depth of your disk and use
that to size the thread pool doing blocking random reads/writes. For
throughput the exact number doesn't matter much as long as there is a
reasonable amount of parallelism.



If the workload is mostly sequential it is better to break it up into
serial chunks and do your own scheduling. Pull/push 100 megabytes at
time and always have exactly one outstanding task pulling/pushing the
next 100 megabytes to keep the disk(s) busy. If you don't schedule
yourself throughput drops as the execution of the reads is interleaved
at too fine a granularity.



You also need to take into account pre-fetching at the kernel and disk
level, with an SSD this will kill you. If there is prefetching of data
you don't want it will pollute the page cache with irrelevant pages and
with an SSD it will kill the number of small random IOs you can do.



Thanks,

Ariel





On Wed, Feb 13, 2013, at 09:50 AM, Nathan Reynolds wrote:

I have heard that building your system to deal with asynchronous I/O
will perform much better.  With synchronous I/O, the thread blocks and
waits.  This incurs 2 context switches as well as ties up a thread and
all of its resources.  With asynchronous I/O, the thread submits the
I/O request and continues to do other processing.  When the I/O
completes, a thread picks up the result and continues processing.
Asynchronous I/O allows 1 thread to submit enough I/O requests that the
underlying storage system can optimize how the requests are stored.
For example, a bunch of random writes using synchronous I/O will cause
the disk head to seek wildly since each write has to go to a different
location.  A bunch of random writes using asynchronous I/O will give
the underlying system a chance to sort the writes and have the disk
head make a single pass over the disk.  Disk performance will greatly
improve.

[1]Nathan Reynolds | Architect | 602.333.9091
Oracle[2]PSR Engineering | Server Technology
On 2/13/2013 2:34 AM, Chris Vest wrote:

If you can tell before hand which tasks (that you submit to the thread
pools) are going to be IO bound and which are going to be CPU bound,
then you can have to separate thread pools: a big one for the IO bound
tasks and a small one for he CPU bound ones.



Otherwise I'd say just set a high upper bound (upwards hundreds, but
depends on expected distribution) and let the OS manage things, see how
that works and if its performant enough, then you're done.



Note that I have no idea what kind of performance is expected of your
SIEM system.

Chris

On 13/02/2013, at 09.48, "Pete Haidinyak" <[3]javamann at cox.net> wrote:

I have a question on how to allocate Threads. I am creating a SIEM
which is a bunch of independent Java Services. The most likely use case
is this will run on one 2U box. The box will have two quad core Xeon
processors and 32G of RAM. Some of the Services will be I/O bound but
some will be CPU bound.
  In one of the latest discussion it was mentions that you should
allocate a Thread for each core (plus or minus a couple) for the best
throughput. I have the ability to turn the Thread Pools after startup
based on the number and types of Services running on the box.

My question is what would be the best way to allocate Threads when you
have multiple processes competing for resources?

Thanks

-Pete



_______________________________________________

Concurrency-interest mailing list

[4]Concurrency-interest at cs.oswego.edu

[5]http://cs.oswego.edu/mailman/listinfo/concurrency-interest





_______________________________________________
Concurrency-interest mailing list
[6]Concurrency-interest at cs.oswego.edu[7]http://cs.oswego.edu/mailman/listinfo/co
ncurrency-interest



_______________________________________________

Concurrency-interest mailing list

[8]Concurrency-interest at cs.oswego.edu

[9]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds
2. http://psr.us.oracle.com/
3. mailto:javamann at cox.net
4. mailto:Concurrency-interest at cs.oswego.edu
5. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
6. mailto:Concurrency-interest at cs.oswego.edu
7. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
8. mailto:Concurrency-interest at cs.oswego.edu
9. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/44f50daf/attachment.html>

From vitalyd at gmail.com  Wed Feb 13 13:59:39 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 13:59:39 -0500
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <c779370904adbf37e082af4c271aa651@nomagicsoftware.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<CAEJX8opYYKKNZykCTQSQR7Ar1=M0BcNgHGBcya+1r8aXk6Bpbw@mail.gmail.com>
	<c779370904adbf37e082af4c271aa651@nomagicsoftware.com>
Message-ID: <CAHjP37GxKv7=qu2Rwv97CyZ3jtFPQnfLTNoTb8CbZ68jF8PQYw@mail.gmail.com>

That's more or less correct :).  Volatile write will cause store buffer to
drain before CPU can proceed to next instruction.  Cache line
invalidation/request-for-ownership coherence traffic is part of normal
store operations as well (once the store makes its way to L1).

Sent from my phone
On Feb 13, 2013 1:46 PM, <thurston at nomagicsoftware.com> wrote:

> Thanks for your responses.
> My understanding is that a volatile *write* does entail invalidating all
> CPUs' L1,L2... caches that have cached the volatile reference.  But so does
> a lock/unlock cycle; is that more or less correct?
>
> On 2013-02-13 08:54, Stanimir Simeonoff wrote:
>
>> Obtaining the lock already requires reading a volatile and it
>> involves an atomic operation (volatile write at the least) + some
>> state store.
>> The lock release needs volatile store (or at least ordered write) +
>> queue checking if anyone has gone asleep waiting for the lock.
>>
>> Overall volatile reads are cheap and almost free on x86.
>>
>> Stanimir
>>
>> On Wed, Feb 13, 2013 at 6:36 PM, <thurston at nomagicsoftware.com> wrote:
>>
>>  Hello,
>>>
>>> I was wondering what requires more overhead: reading a volatile
>>> reference or obtaining a lock when there is no contention.
>>>
>>> Essentially I have a single-reader scenario, where the common-path
>>> (99%+) doesn't require either a volatile read or a lock; but if I want to
>>> support a few very-rarely used cases, I can only make it thread-safe by
>>> making the 99% case rely on a volatile read or obtain a lock.
>>> Any guidelines?
>>>
>>> Thanks
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>[1]
>>>
>>
>>
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/f14b0ff5/attachment-0001.html>

From nathan.reynolds at oracle.com  Wed Feb 13 14:07:40 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 12:07:40 -0700
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <c779370904adbf37e082af4c271aa651@nomagicsoftware.com>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<CAEJX8opYYKKNZykCTQSQR7Ar1=M0BcNgHGBcya+1r8aXk6Bpbw@mail.gmail.com>
	<c779370904adbf37e082af4c271aa651@nomagicsoftware.com>
Message-ID: <511BE47C.5020700@oracle.com>

On x86, any write (volatile and non-volatile) invalidates all CPUs L1, 
L2 and L3 caches except for the physical core which did the write.  
Volatile writes have a memory fence added to ensure happens-before.  See 
http://g.oswego.edu/dl/jmm/cookbook.html .  The invalidation is required 
for cache coherency and forces the other cores to fetch the cache line 
to read it... hence get the most up to date version.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 11:38 AM, thurston at nomagicsoftware.com wrote:
> Thanks for your responses.
> My understanding is that a volatile *write* does entail invalidating 
> all CPUs' L1,L2... caches that have cached the volatile reference.  
> But so does a lock/unlock cycle; is that more or less correct?
>
> On 2013-02-13 08:54, Stanimir Simeonoff wrote:
>> Obtaining the lock already requires reading a volatile and it
>> involves an atomic operation (volatile write at the least) + some
>> state store.
>> The lock release needs volatile store (or at least ordered write) +
>> queue checking if anyone has gone asleep waiting for the lock.
>>
>> Overall volatile reads are cheap and almost free on x86.
>>
>> Stanimir
>>
>> On Wed, Feb 13, 2013 at 6:36 PM, <thurston at nomagicsoftware.com> wrote:
>>
>>> Hello,
>>>
>>> I was wondering what requires more overhead: reading a volatile 
>>> reference or obtaining a lock when there is no contention.
>>>
>>> Essentially I have a single-reader scenario, where the common-path 
>>> (99%+) doesn't require either a volatile read or a lock; but if I 
>>> want to support a few very-rarely used cases, I can only make it 
>>> thread-safe by making the 99% case rely on a volatile read or obtain 
>>> a lock.
>>> Any guidelines?
>>>
>>> Thanks
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>
>>
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/1d48e9be/attachment.html>

From nathan.reynolds at oracle.com  Wed Feb 13 14:15:35 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 13 Feb 2013 12:15:35 -0700
Subject: [concurrency-interest] Reading a volatile vs uncontented lock
In-Reply-To: <511BDE1A.5010502@gmx.at>
References: <22c8c38a2e713dfbc91a27a00f7b2034@nomagicsoftware.com>
	<511BC531.4050709@oracle.com> <511BDE1A.5010502@gmx.at>
Message-ID: <511BE657.2000602@oracle.com>

I don't see why not.  JIT probably can't do as much optimization work 
though.  Remi should comment since he wrote the code.  He is on this list.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/13/2013 11:40 AM, Markus Krainz wrote:
> Am 13.02.2013 17:54, schrieb Nathan Reynolds:
>> Reading a volatile on x86 is translated to a simple "mov" 
>> instruction.  Obtaining a lock is a bit more involved.  If the lock 
>> is biased, then the thread will simply have to check to make sure it 
>> is still the owner. It won't have to execute any atomic 
>> instructions.  If the lock is not biased, then it will have to 
>> execute an atomic instruction.  In the latter case, the atomic 
>> instruction is going to cost a lot.  If the former case, the 
>> instructions building up to the check and the check itself will cost 
>> much more than a "mov" instruction.  So, I would recommend reading 
>> the volatile... unless your intended platforms have a significant 
>> overhead for reading a volatile.
>>
>> An alternative might be AlmostFinalValue 
>> (http://weblogs.java.net/blog/forax/archive/2011/12/17/jsr-292-goodness-almost-static-final-field). 
>> I haven't played with this yet.  However, it seems to promise the 
>> ability to have a field optimized as if it were final yet allow for 
>> it to be safely mutated.  The penalty is that every time the field is 
>> updated, the code using the field is deoptimized (i.e. runs as 
>> bytecode) and then is optimized again later by JIT.  So, the overall 
>> performance might be better than reading a volatile depending upon 
>> how many 9s follow the decimal point when you say (99%+).
>
> This might be a stupid question, but is this AlmostFinalValue hack 
> valid for non-static fields?
> I was under impression that the thread starter wants to use this not 
> just for static members.
>
>>
>> Nathan Reynolds 
>> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
>> Architect | 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>> On 2/13/2013 9:36 AM, thurston at nomagicsoftware.com wrote:
>>> Hello,
>>>
>>> I was wondering what requires more overhead: reading a volatile 
>>> reference or obtaining a lock when there is no contention.
>>>
>>> Essentially I have a single-reader scenario, where the common-path 
>>> (99%+) doesn't require either a volatile read or a lock; but if I 
>>> want to support a few very-rarely used cases, I can only make it 
>>> thread-safe by making the 99% case rely on a volatile read or obtain 
>>> a lock.
>>> Any guidelines?
>>>
>>> Thanks
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> Regards, Markus 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/1e0d6644/attachment.html>

From ariel at weisberg.ws  Wed Feb 13 15:41:16 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 13 Feb 2013 15:41:16 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
Message-ID: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>

Hi,

Does it make sense to use Atomic*FieldUpdater to remove the indirection
overhead of an AtomicLong and AtomicReference? Similarly, does it make
sense to use Atomic* to create indirection in order to avoid false
sharing?

Thanks,
Ariel

From vitalyd at gmail.com  Wed Feb 13 15:51:33 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 15:51:33 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
Message-ID: <CAHjP37HkNDpP_eS80Vq=co1-9k2Z8TjN9yHGH=3VSuhB=FomGg@mail.gmail.com>

Can you elaborate a bit? Do you mean using AtomicInteger (as an example)
instead of an int field inside a class? If so, I'd personally pad out the
class with filler fields to avoid false sharing - there's no guarantee that
the indirection via AtomicXXX will put memory far apart; padding gives you
a bit more control here.

Sent from my phone
On Feb 13, 2013 3:44 PM, "Ariel Weisberg" <ariel at weisberg.ws> wrote:

> Hi,
>
> Does it make sense to use Atomic*FieldUpdater to remove the indirection
> overhead of an AtomicLong and AtomicReference? Similarly, does it make
> sense to use Atomic* to create indirection in order to avoid false
> sharing?
>
> Thanks,
> Ariel
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/b19cb32b/attachment.html>

From ariel at weisberg.ws  Wed Feb 13 16:45:42 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 13 Feb 2013 16:45:42 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
 indirection
In-Reply-To: <CAHjP37HkNDpP_eS80Vq=co1-9k2Z8TjN9yHGH=3VSuhB=FomGg@mail.gmail.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<CAHjP37HkNDpP_eS80Vq=co1-9k2Z8TjN9yHGH=3VSuhB=FomGg@mail.gmail.com>
Message-ID: <1360791942.31933.140661191100741.4B8529CF@webmail.messagingengine.com>

Hi,



I have two use cases in mind.



I have some COW* data structures that use AtomicReference internally
for example [1]COWNavigableSet. Using AtomicReference means there is an
extra indirection for every lookup.



The other use case is a field that tracks the [2]last time a message
was received from a host. There is a single thread that updates the
field every time a message is delivered and multiple threads read the
field [3]every time they send a message to that host.



In the last message time tracking use case there will always be a
coherence operation for the cache line containing the value tracking
the last message time. I suppose it doesn't really matter if the other
fields in that class are on the same line because the number of
coherence operations remains the same. If there were a get equivalent
of lazySet that would be nice.



Thanks,

Ariel





On Wed, Feb 13, 2013, at 03:51 PM, Vitaly Davidovich wrote:

  Can you elaborate a bit? Do you mean using AtomicInteger (as an
  example) instead of an int field inside a class? If so, I'd
  personally pad out the class with filler fields to avoid false
  sharing - there's no guarantee that the indirection via AtomicXXX
  will put memory far apart; padding gives you a bit more control
  here.

  Sent from my phone

On Feb 13, 2013 3:44 PM, "Ariel Weisberg" <[4]ariel at weisberg.ws> wrote:

  Hi,
  Does it make sense to use Atomic*FieldUpdater to remove the
  indirection
  overhead of an AtomicLong and AtomicReference? Similarly, does it
  make
  sense to use Atomic* to create indirection in order to avoid false
  sharing?
  Thanks,
  Ariel
  _______________________________________________
  Concurrency-interest mailing list
  [5]Concurrency-interest at cs.oswego.edu
  [6]http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. https://github.com/VoltDB/voltdb/blob/master/src/frontend/org/voltcore/utils/COWNavigableSet.java#L30
2. https://github.com/VoltDB/voltdb/blob/cdc1eb42d6d8708cb5371af5cb1fd747372d8be0/src/frontend/org/voltcore/messaging/ForeignHost.java#L300
3. https://github.com/VoltDB/voltdb/blob/cdc1eb42d6d8708cb5371af5cb1fd747372d8be0/src/frontend/org/voltcore/messaging/ForeignHost.java#L213
4. mailto:ariel at weisberg.ws
5. mailto:Concurrency-interest at cs.oswego.edu
6. http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/9aeb61d6/attachment-0001.html>

From vitalyd at gmail.com  Wed Feb 13 17:15:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 13 Feb 2013 17:15:13 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <1360791942.31933.140661191100741.4B8529CF@webmail.messagingengine.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<CAHjP37HkNDpP_eS80Vq=co1-9k2Z8TjN9yHGH=3VSuhB=FomGg@mail.gmail.com>
	<1360791942.31933.140661191100741.4B8529CF@webmail.messagingengine.com>
Message-ID: <CAHjP37GffGN-GW=WO79BVr08gU_LKDoYXF+OJ9cWm1gy+hmrvg@mail.gmail.com>

I'd be more concerned with heap overhead of using AtomicXXX if it's
embedded in lots of objects (or embedded in classes with high allocation
rates).  For example, AtomicInteger will be 24 bytes for a payload of 4
bytes (nevermind requiring more GC time to mark and sweep these objects if
they get tenured).  The memory indirection cost may be minimal depending on
whether you hit cache or not.  Anyway, you can use Unsafe to replicate
AtomicXXX to remove overhead.

The second use case sounds OK - you have single writer multiple readers.
Your only possible concern there is whether other fields on the same cache
line are read independent of message time.  In other words, if message time
changes more frequently than the other data on the line, you'll get
unnecessary coherence hits for reading that other data.

Vitaly

Sent from my phone
On Feb 13, 2013 4:45 PM, "Ariel Weisberg" <ariel at weisberg.ws> wrote:

> **
> Hi,
>
> I have two use cases in mind.
>
> I have some COW* data structures that use AtomicReference internally for
> example COWNavigableSet<https://github.com/VoltDB/voltdb/blob/master/src/frontend/org/voltcore/utils/COWNavigableSet.java#L30>.
> Using AtomicReference means there is an extra indirection for every lookup.
>
> The other use case is a field that tracks the last time a message<https://github.com/VoltDB/voltdb/blob/cdc1eb42d6d8708cb5371af5cb1fd747372d8be0/src/frontend/org/voltcore/messaging/ForeignHost.java#L300>was received from a host. There is a single thread that updates the field
> every time a message is delivered and multiple threads read the field every
> time they send a message<https://github.com/VoltDB/voltdb/blob/cdc1eb42d6d8708cb5371af5cb1fd747372d8be0/src/frontend/org/voltcore/messaging/ForeignHost.java#L213>to that host.
>
> In the last message time tracking use case there will always be a
> coherence operation for the cache line containing the value tracking the
> last message time. I suppose it doesn't really matter if the other fields
> in that class are on the same line because the number of coherence
> operations remains the same. If there were a get equivalent of lazySet that
> would be nice.
>
> Thanks,
> Ariel
>
>
> On Wed, Feb 13, 2013, at 03:51 PM, Vitaly Davidovich wrote:
>
> Can you elaborate a bit? Do you mean using AtomicInteger (as an example)
> instead of an int field inside a class? If so, I'd personally pad out the
> class with filler fields to avoid false sharing - there's no guarantee that
> the indirection via AtomicXXX will put memory far apart; padding gives you
> a bit more control here.
>
> Sent from my phone
> On Feb 13, 2013 3:44 PM, "Ariel Weisberg" <ariel at weisberg.ws> wrote:
>
>> Hi,
>>
>> Does it make sense to use Atomic*FieldUpdater to remove the indirection
>> overhead of an AtomicLong and AtomicReference? Similarly, does it make
>> sense to use Atomic* to create indirection in order to avoid false
>> sharing?
>>
>> Thanks,
>> Ariel
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130213/ccf68e81/attachment.html>

From aaron.grunthal at infinite-source.de  Wed Feb 13 22:37:38 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Thu, 14 Feb 2013 04:37:38 +0100
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
Message-ID: <511C5C02.9070604@infinite-source.de>

Btw, Atomic*Updaters are not entirely equivalent to raw volatile/CAS 
accesses due to a bunch of security checking they do. Especially when 
security domains are used or the Updater's target class has subclasses 
then it has to do some checks that the JVM cannot optimize away. So if 
every ounce of performance counts you probably should do volatile reads 
and writes to the field handled by the updater directly and only use it 
for CAS. And as last resort there's also Unsafe, but there usually are 
bigger fish to fry before you get to that point.

On 13.02.2013 21:41, Ariel Weisberg wrote:
> Hi,
>
> Does it make sense to use Atomic*FieldUpdater to remove the indirection
> overhead of an AtomicLong and AtomicReference? Similarly, does it make
> sense to use Atomic* to create indirection in order to avoid false
> sharing?
>
> Thanks,
> Ariel
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From peter.levart at gmail.com  Thu Feb 14 10:56:45 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 14 Feb 2013 16:56:45 +0100
Subject: [concurrency-interest] Numerical Stream code
In-Reply-To: <511CF876.1000003@oracle.com>
References: <CACR_FB41DwO6HBohqAMYdH3DMnUhS=PoqKZjb0tp-8WWw1qQtQ@mail.gmail.com>
	<511CF6C3.8000709@oracle.com> <511CF876.1000003@oracle.com>
Message-ID: <511D093D.5010608@gmail.com>

On 02/14/2013 03:45 PM, Brian Goetz wrote:
>> The parallel version is almost certainly suffering false cache line
>> sharing when adjacent tasks are writing to the shared arrays u0, etc.
>> Nothing to do with streams, just a standard parallelism gotcha.
> Cure: don't write to shared arrays from parallel tasks.
>
>
Hi,

I would like to discuss this a little bit (hence the cc: 
concurrency-interest - the conversation can continue on this list only).

Is it really important to avoid writing to shared arrays from multiple 
threads (of course without synchronization, not even volatile 
writes/reads) when indexes are not shared (each thread writes/reads it's 
own disjunct subset).

Do element sizes matter (byte vs. short vs. int  vs. long)?

I had a (false?) feeling that cache lines are not invalidated when 
writes are performed without fences.

Also I don't know how short (byte, char) writes are combined into memory 
words on the hardware when they come from different cores and whether 
this is connected to any performance issues.

Thanks,

Peter


From cdennis at terracottatech.com  Thu Feb 14 11:15:16 2013
From: cdennis at terracottatech.com (Chris Dennis)
Date: Thu, 14 Feb 2013 11:15:16 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <511C5C02.9070604@infinite-source.de>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<511C5C02.9070604@infinite-source.de>
Message-ID: <80C20C16-8835-4ABE-86AF-5D1C7AAF7D2D@terracottatech.com>

My understanding is that doing that is not technically (per the JDK spec) safe.  The updaters don't guarantee atomicity of their changes with respect to other mutations.  On Hotspot I think you can only see this behavior for AtomicLongFieldUpdater when the underlying hardware lacks an 8 byte compare-and-swap (which obviously makes sense).

On Feb 13, 2013, at 10:37 PM, Aaron Grunthal wrote:

> Btw, Atomic*Updaters are not entirely equivalent to raw volatile/CAS accesses due to a bunch of security checking they do. Especially when security domains are used or the Updater's target class has subclasses then it has to do some checks that the JVM cannot optimize away. So if every ounce of performance counts you probably should do volatile reads and writes to the field handled by the updater directly and only use it for CAS. And as last resort there's also Unsafe, but there usually are bigger fish to fry before you get to that point.
> 
> On 13.02.2013 21:41, Ariel Weisberg wrote:
>> Hi,
>> 
>> Does it make sense to use Atomic*FieldUpdater to remove the indirection
>> overhead of an AtomicLong and AtomicReference? Similarly, does it make
>> sense to use Atomic* to create indirection in order to avoid false
>> sharing?
>> 
>> Thanks,
>> Ariel
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From thurston at nomagicsoftware.com  Thu Feb 14 11:48:32 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Thu, 14 Feb 2013 08:48:32 -0800
Subject: [concurrency-interest] CPU cache-coherency's effects on visibility
Message-ID: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>

Given that all (?) modern CPUs provide cache-coherency, in the 
following (admittedly superficial example):

Thread 1:
while (true)
{
     log(this.shared)
     Thread.sleep(1000L)
}

Thread 2:
    this.shared = new Foo();

with Thread 2's code only invoked once and sometime significantly after 
(in a wall-clock sense)
Thread 1; and there are no operations performed by either thread 
forming a happens-before relationship (in the JMM sense).

Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
And that that guarantee is provided not by the JMM, but by the 
cache-coherency of the CPUs?

From ariel at weisberg.ws  Thu Feb 14 11:56:15 2013
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Thu, 14 Feb 2013 11:56:15 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
 indirection
In-Reply-To: <80C20C16-8835-4ABE-86AF-5D1C7AAF7D2D@terracottatech.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<511C5C02.9070604@infinite-source.de>
	<80C20C16-8835-4ABE-86AF-5D1C7AAF7D2D@terracottatech.com>
Message-ID: <1360860975.32542.140661191501041.212BCD86@webmail.messagingengine.com>

Hi,



I only need this to work on x86_64 so I should be fine with volatile
reads of references. The only path that has to be fast is reading the
field, updating can be slow (and is).



Will storing the [1]volatile field onto a stack variable give me a
snapshot of the value?



Another question I have is about the padding suggestion. I looked at
how it is done in Exchanger with the 15 long fields. What guarantees
are there for field ordering and alignment of the allocated object WRT
to cache lines? It seems like you might need to pad both before and
after the field that is contended.



Is there a difference between storing a reference to a properly padded
object for contended fields vs adding the padding directly to the
object containing the contended field? What about array fields
(primitives, and objects), are they ever stored inside the object or
are they always separate allocations?



Thanks,

Ariel



On Thu, Feb 14, 2013, at 11:15 AM, Chris Dennis wrote:

> My understanding is that doing that is not technically (per the JDK
spec)

> safe. The updaters don't guarantee atomicity of their changes with

> respect to other mutations. On Hotspot I think you can only see this

> behavior for AtomicLongFieldUpdater when the underlying hardware
lacks an

> 8 byte compare-and-swap (which obviously makes sense).

>

> On Feb 13, 2013, at 10:37 PM, Aaron Grunthal wrote:

>

> > Btw, Atomic*Updaters are not entirely equivalent to raw
volatile/CAS accesses due to a bunch of security checking they do.
Especially when security domains are used or the Updater's target class
has subclasses then it has to do some checks that the JVM cannot
optimize away. So if every ounce of performance counts you probably
should do volatile reads and writes to the field handled by the updater
directly and only use it for CAS. And as last resort there's also
Unsafe, but there usually are bigger fish to fry before you get to that
point.

> >

> > On 13.02.2013 21:41, Ariel Weisberg wrote:

> >> Hi,

> >>

> >> Does it make sense to use Atomic*FieldUpdater to remove the
indirection

> >> overhead of an AtomicLong and AtomicReference? Similarly, does it
make

> >> sense to use Atomic* to create indirection in order to avoid false

> >> sharing?

> >>

> >> Thanks,

> >> Ariel

> >> _______________________________________________

> >> Concurrency-interest mailing list

> >> Concurrency-interest at cs.oswego.edu

> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

> >>

> >

> > _______________________________________________

> > Concurrency-interest mailing list

> > Concurrency-interest at cs.oswego.edu

> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

>

>

> _______________________________________________

> Concurrency-interest mailing list

> Concurrency-interest at cs.oswego.edu

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

References

1. https://github.com/VoltDB/voltdb/blob/cow_indirection/src/frontend/org/voltcore/utils/COWMap.java#L52
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/24b266d5/attachment-0001.html>

From nitsanw at yahoo.com  Thu Feb 14 12:09:01 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 14 Feb 2013 09:09:01 -0800 (PST)
Subject: [concurrency-interest] Numerical Stream code
In-Reply-To: <511D093D.5010608@gmail.com>
References: <CACR_FB41DwO6HBohqAMYdH3DMnUhS=PoqKZjb0tp-8WWw1qQtQ@mail.gmail.com>
	<511CF6C3.8000709@oracle.com> <511CF876.1000003@oracle.com>
	<511D093D.5010608@gmail.com>
Message-ID: <1360861741.83154.YahooMailNeo@web120703.mail.ne1.yahoo.com>

There's no issue with writing to disjoint areas in same arrays, you'll need some correct publication mechanism to make sure all the updates happened before reads do if you are worried about that.
Writing in larger chunks is faster, so writing longs directly is better than writing individual bytes. If you are using HeapByteBuffer you should know that the writes will translate to byte writes. On a DirectByteBuffer each type write ends up as an Unsafe.putType, so you don't get that effect. Also with DirectByteBuffers you can control alignment(see this blog post: http://psy-lob-saw.blogspot.com/2013/01/direct-memory-alignment-in-java.html and the follow up here:http://psy-lob-saw.blogspot.com/2013/02/alignment-concurrency-and-torture-x86.html) and make better choices with regards to splitting chunks between threads such that no false sharing ever happens.


________________________________
 From: Peter Levart <peter.levart at gmail.com>
To: lambda-dev at openjdk.java.net 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Thursday, February 14, 2013 3:56 PM
Subject: Re: [concurrency-interest] Numerical Stream code
 
On 02/14/2013 03:45 PM, Brian Goetz wrote:
>> The parallel version is almost certainly suffering false cache line
>> sharing when adjacent tasks are writing to the shared arrays u0, etc.
>> Nothing to do with streams, just a standard parallelism gotcha.
> Cure: don't write to shared arrays from parallel tasks.
> 
> 
Hi,

I would like to discuss this a little bit (hence the cc: concurrency-interest - the conversation can continue on this list only).

Is it really important to avoid writing to shared arrays from multiple threads (of course without synchronization, not even volatile writes/reads) when indexes are not shared (each thread writes/reads it's own disjunct subset).

Do element sizes matter (byte vs. short vs. int? vs. long)?

I had a (false?) feeling that cache lines are not invalidated when writes are performed without fences.

Also I don't know how short (byte, char) writes are combined into memory words on the hardware when they come from different cores and whether this is connected to any performance issues.

Thanks,

Peter

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/aac8de3b/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 14 12:25:56 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 14 Feb 2013 10:25:56 -0700
Subject: [concurrency-interest] Numerical Stream code
In-Reply-To: <511D093D.5010608@gmail.com>
References: <CACR_FB41DwO6HBohqAMYdH3DMnUhS=PoqKZjb0tp-8WWw1qQtQ@mail.gmail.com>
	<511CF6C3.8000709@oracle.com> <511CF876.1000003@oracle.com>
	<511D093D.5010608@gmail.com>
Message-ID: <511D1E24.7070700@oracle.com>

 > Is it really important to avoid writing to shared arrays from 
multiple threads (of course without synchronization, not even volatile 
writes/reads) when indexes are not shared (each thread writes/reads it's 
own disjunct subset)?

I realize that this is a Java message board, but I have most of my 
knowledge on Intel x86 processors.  Keep that in mind when reading my 
response.  Other processors might behave differently.

Since the threads are reading/writing to their own disjunct subset, the 
data will never be corrupted.  No fences or locks are required.  Right 
before a core writes, the cache line is invalidated in all other cores 
and the update happens in the only copy of the cache line.  If another 
core needs to read or write to that cache line, it will have to get a 
copy from the core which has it.  This is because the Intel x86 
processor is a cache coherent architecture.

This is what is called false sharing.  Threads are making updates to the 
same cache line but at different locations.  Depending upon the 
concurrency hitting each cache line, cache misses are going to be high.  
The cores are going to stall waiting to get a hold of the cache line.  
The inter-core and inter-processor network is going to be flooded or 
even bottlenecked with cache invalidation and cache line fetching 
operations.

Here's the parallelism gotcha.  In a worse case scenario, every thread 
is updating its own byte in a single cache line in a very tight loop.  
You have now just serialized all of the cores on 1 cache line.  You will 
get much worse performance than if a single thread did all of the work.  
This is because with a single thread, the cache line will stay in L1 
cache.  The memory accesses will be extremely fast (i.e. 4 cycles).  
With multiple threads, once the core has the cache line in L1, the 
memory access will still be 4 cycles; however, the cache line must spend 
a lot of its time inaccessible as it travels among the cores.

 > Do element sizes matter (byte vs. short vs. int  vs. long)?

I don't think so.  All of this assumes that the proper instruction is 
used.  For example, if 2 threads are writing to adjacent bytes, then the 
"mov" instruction has to only write the byte.  If the compiler, decides 
to read 32-bits, mask in the 8-bits and write 32-bits then the data will 
be corrupted.  I believe that HotSpot will only generate the write byte 
mov instruction.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/14/2013 8:56 AM, Peter Levart wrote:
> On 02/14/2013 03:45 PM, Brian Goetz wrote:
>>> The parallel version is almost certainly suffering false cache line
>>> sharing when adjacent tasks are writing to the shared arrays u0, etc.
>>> Nothing to do with streams, just a standard parallelism gotcha.
>> Cure: don't write to shared arrays from parallel tasks.
>>
>>
> Hi,
>
> I would like to discuss this a little bit (hence the cc: 
> concurrency-interest - the conversation can continue on this list only).
>
> Is it really important to avoid writing to shared arrays from multiple 
> threads (of course without synchronization, not even volatile 
> writes/reads) when indexes are not shared (each thread writes/reads 
> it's own disjunct subset).
>
> Do element sizes matter (byte vs. short vs. int  vs. long)?
>
> I had a (false?) feeling that cache lines are not invalidated when 
> writes are performed without fences.
>
> Also I don't know how short (byte, char) writes are combined into 
> memory words on the hardware when they come from different cores and 
> whether this is connected to any performance issues.
>
> Thanks,
>
> Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/1789034c/attachment.html>

From vitalyd at gmail.com  Thu Feb 14 12:27:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 14 Feb 2013 12:27:00 -0500
Subject: [concurrency-interest] CPU cache-coherency's effects on
	visibility
In-Reply-To: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
References: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
Message-ID: <CAHjP37F4+Shb4wL=V32K4Cs1JATnuBckYHTAf2JfEoOuT-76bw@mail.gmail.com>

It's not guaranteed because compiler can hoist the read out of the loop (if
the field is non volatile).  If it didn't do that, then for all practical
purposes (I.e. exotic hypotheticals aside) yes you'll see it once the store
buffer drains to L1 (assuming it even went there in the first place and
again assuming no exotic stuff where buffer never drains).

Sent from my phone
On Feb 14, 2013 11:56 AM, <thurston at nomagicsoftware.com> wrote:

> Given that all (?) modern CPUs provide cache-coherency, in the following
> (admittedly superficial example):
>
> Thread 1:
> while (true)
> {
>     log(this.shared)
>     Thread.sleep(1000L)
> }
>
> Thread 2:
>    this.shared = new Foo();
>
> with Thread 2's code only invoked once and sometime significantly after
> (in a wall-clock sense)
> Thread 1; and there are no operations performed by either thread forming a
> happens-before relationship (in the JMM sense).
>
> Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
> And that that guarantee is provided not by the JMM, but by the
> cache-coherency of the CPUs?
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/7842e991/attachment.html>

From vitalyd at gmail.com  Thu Feb 14 12:31:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 14 Feb 2013 12:31:13 -0500
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <1360860975.32542.140661191501041.212BCD86@webmail.messagingengine.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<511C5C02.9070604@infinite-source.de>
	<80C20C16-8835-4ABE-86AF-5D1C7AAF7D2D@terracottatech.com>
	<1360860975.32542.140661191501041.212BCD86@webmail.messagingengine.com>
Message-ID: <CAHjP37EyLe72upX8yy=TsxyBMx=0N_4MyQx_04Ra47wYZYxrbw@mail.gmail.com>

Yes reading volatile into a temp will get you a register/stack slot.

To be safe, you should pad before and after.  No guarantees on object
layout but you can take advantage of knowing hotspot's default layout
(fields laid out in increasing representation size, references last I
believe).

Arrays are contiguous blobs of memory but are heap allocated separately
(they're not embedded in the object itself but referenced).

Sent from my phone
On Feb 14, 2013 11:59 AM, "Ariel Weisberg" <ariel at weisberg.ws> wrote:

> **
> Hi,
>
> I only need this to work on x86_64 so I should be fine with volatile reads
> of references. The only path that has to be fast is reading the field,
> updating can be slow (and is).
>
> Will storing the volatile field onto a stack variable<https://github.com/VoltDB/voltdb/blob/cow_indirection/src/frontend/org/voltcore/utils/COWMap.java#L52> give
> me a snapshot of the value?
>
> Another question I have is about the padding suggestion. I looked at how
> it is done in Exchanger with the 15 long fields. What guarantees are there
> for field ordering and alignment of the allocated object WRT to cache
> lines? It seems like you might need to pad both before and after the field
> that is contended.
>
> Is there a difference between storing a reference to a properly padded
> object for contended fields vs adding the padding directly to the object
> containing the contended field? What about array fields (primitives, and
> objects), are they ever stored inside the object or are they always
> separate allocations?
>
> Thanks,
> Ariel
>
> On Thu, Feb 14, 2013, at 11:15 AM, Chris Dennis wrote:
> > My understanding is that doing that is not technically (per the JDK spec)
> > safe. The updaters don't guarantee atomicity of their changes with
> > respect to other mutations. On Hotspot I think you can only see this
> > behavior for AtomicLongFieldUpdater when the underlying hardware lacks an
> > 8 byte compare-and-swap (which obviously makes sense).
> >
> > On Feb 13, 2013, at 10:37 PM, Aaron Grunthal wrote:
> >
> > > Btw, Atomic*Updaters are not entirely equivalent to raw volatile/CAS
> accesses due to a bunch of security checking they do. Especially when
> security domains are used or the Updater's target class has subclasses then
> it has to do some checks that the JVM cannot optimize away. So if every
> ounce of performance counts you probably should do volatile reads and
> writes to the field handled by the updater directly and only use it for
> CAS. And as last resort there's also Unsafe, but there usually are bigger
> fish to fry before you get to that point.
> > >
> > > On 13.02.2013 21:41, Ariel Weisberg wrote:
> > >> Hi,
> > >>
> > >> Does it make sense to use Atomic*FieldUpdater to remove the
> indirection
> > >> overhead of an AtomicLong and AtomicReference? Similarly, does it make
> > >> sense to use Atomic* to create indirection in order to avoid false
> > >> sharing?
> > >>
> > >> Thanks,
> > >> Ariel
> > >> _______________________________________________
> > >> Concurrency-interest mailing list
> > >> Concurrency-interest at cs.oswego.edu
> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >>
> > >
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at cs.oswego.edu
> > > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/70bca2a8/attachment-0001.html>

From stanimir at riflexo.com  Thu Feb 14 12:38:55 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 14 Feb 2013 19:38:55 +0200
Subject: [concurrency-interest] CPU cache-coherency's effects on
	visibility
In-Reply-To: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
References: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
Message-ID: <CAEJX8ortdHD_wB22Bc4PB3W1GTTuADB55fA3vGshCLre8kPncQ@mail.gmail.com>

this.shared can be hoisted by JVM unless it's volatile. So Thread1 always
sees the initial value, e.g. null.

The code can be x-formed into:
Thread 1:
Foo shared = this.shared
while (true){
    log(shared)
    Thread.sleep(1000L)
}


On Thu, Feb 14, 2013 at 6:48 PM, <thurston at nomagicsoftware.com> wrote:

> Given that all (?) modern CPUs provide cache-coherency, in the following
> (admittedly superficial example):
>
> Thread 1:
> while (true)
> {
>     log(this.shared)
>     Thread.sleep(1000L)
> }
>
> Thread 2:
>    this.shared = new Foo();
>
> with Thread 2's code only invoked once and sometime significantly after
> (in a wall-clock sense)
> Thread 1; and there are no operations performed by either thread forming a
> happens-before relationship (in the JMM sense).
>
> Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
> And that that guarantee is provided not by the JMM, but by the
> cache-coherency of the CPUs?
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/d2510145/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 14 12:41:18 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 14 Feb 2013 10:41:18 -0700
Subject: [concurrency-interest] Using Atomic*FieldUpdater to remove
	indirection
In-Reply-To: <1360860975.32542.140661191501041.212BCD86@webmail.messagingengine.com>
References: <1360788076.19208.140661191082617.5B0F6D6C@webmail.messagingengine.com>
	<511C5C02.9070604@infinite-source.de>
	<80C20C16-8835-4ABE-86AF-5D1C7AAF7D2D@terracottatech.com>
	<1360860975.32542.140661191501041.212BCD86@webmail.messagingengine.com>
Message-ID: <511D21BE.20600@oracle.com>

 > Will storing the volatile field onto a stack variable give me a 
snapshot of the value?

Yes.  The typical strategy is to copy the field into a stack variable, 
check the state using the stack variable, calculate the updates based 
off of the stack variable and apply the update to the volatile field 
using 1 atomic instruction.

 > Another question I have is about the padding suggestion. I looked at 
how it is done in Exchanger with the 15 long fields. What guarantees are 
there for field ordering and alignment of the allocated object WRT to 
cache lines? It seems like you might need to pad both before and after 
the field that is contended.

There are no guarantees for field ordering.  The JVM will rearrange the 
fields for its needs.  For example, it will sort the primitive fields by 
width and put the wider ones first so that no padding is necessary 
between fields.  It will then put all of the references last to help GC 
performance.  So, if you are looking to avoid false sharing among fields 
in the class you are out of luck until @Contended is available.  I think 
it is/will be available in HotSpot 8.

15 * sizeof(long) = 120 bytes.  Add 8 bytes for the object header (i.e. 
32-bit JVMs) and the total is 128 bytes.  This is exactly 2 cache 
lines.  If multiple Exchangers are next to each, then the fields can 
never be on the same cache line no matter where in the cache line the 
initial byte of the object is placed.  If the protected field ends up at 
the end of the object, then I don't see how this would protect it from 
false sharing with a different type of object which has a heavily 
written field at the beginning.  I figured to achieve ultimate false 
sharing protection, then I have to put the protected field in the middle 
of an array.  The array must have 64 bytes on both sides of the 
protected field.  In other words, the array is 128 bytes long + the size 
of the protected field.  For example, an int[33] would protect an int 
stored at position 16.  This doesn't account for the 12 byte object 
header (32-bit JVM).  So, the actual array length could be 3 elements 
shorter with the protected int stored at position 13.

 > Is there a difference between storing a reference to a properly 
padded object for contended fields vs adding the padding directly to the 
object containing the contended field?

You have to put the padding around the contended field in order to 
protect it from false sharing.

 > What about array fields (primitives, and objects), are they ever 
stored inside the object or are they always separate allocations?

Arrays are always individual objects and can be placed anywhere in the heap.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/14/2013 9:56 AM, Ariel Weisberg wrote:
> Hi,
> I only need this to work on x86_64 so I should be fine with volatile 
> reads of references. The only path that has to be fast is reading the 
> field, updating can be slow (and is).
> Will storing the volatile field onto a stack variable 
> <https://github.com/VoltDB/voltdb/blob/cow_indirection/src/frontend/org/voltcore/utils/COWMap.java#L52> give 
> me a snapshot of the value?
> Another question I have is about the padding suggestion. I looked at 
> how it is done in Exchanger with the 15 long fields. What guarantees 
> are there for field ordering and alignment of the allocated object WRT 
> to cache lines? It seems like you might need to pad both before and 
> after the field that is contended.
> Is there a difference between storing a reference to a properly padded 
> object for contended fields vs adding the padding directly to the 
> object containing the contended field? What about array fields 
> (primitives, and objects), are they ever stored inside the object or 
> are they always separate allocations?
> Thanks,
> Ariel
> On Thu, Feb 14, 2013, at 11:15 AM, Chris Dennis wrote:
> > My understanding is that doing that is not technically (per the JDK 
> spec)
> > safe. The updaters don't guarantee atomicity of their changes with
> > respect to other mutations. On Hotspot I think you can only see this
> > behavior for AtomicLongFieldUpdater when the underlying hardware 
> lacks an
> > 8 byte compare-and-swap (which obviously makes sense).
> >
> > On Feb 13, 2013, at 10:37 PM, Aaron Grunthal wrote:
> >
> > > Btw, Atomic*Updaters are not entirely equivalent to raw 
> volatile/CAS accesses due to a bunch of security checking they do. 
> Especially when security domains are used or the Updater's target 
> class has subclasses then it has to do some checks that the JVM cannot 
> optimize away. So if every ounce of performance counts you probably 
> should do volatile reads and writes to the field handled by the 
> updater directly and only use it for CAS. And as last resort there's 
> also Unsafe, but there usually are bigger fish to fry before you get 
> to that point.
> > >
> > > On 13.02.2013 21:41, Ariel Weisberg wrote:
> > >> Hi,
> > >>
> > >> Does it make sense to use Atomic*FieldUpdater to remove the 
> indirection
> > >> overhead of an AtomicLong and AtomicReference? Similarly, does it 
> make
> > >> sense to use Atomic* to create indirection in order to avoid false
> > >> sharing?
> > >>
> > >> Thanks,
> > >> Ariel
> > >> _______________________________________________
> > >> Concurrency-interest mailing list
> > >> Concurrency-interest at cs.oswego.edu
> > >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> > >>
> > >
> > > _______________________________________________
> > > Concurrency-interest mailing list
> > > Concurrency-interest at cs.oswego.edu
> > > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/fd3aba5b/attachment.html>

From thurston at nomagicsoftware.com  Thu Feb 14 12:48:34 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Thu, 14 Feb 2013 09:48:34 -0800
Subject: [concurrency-interest] CPU cache-coherency's effects on
 visibility
In-Reply-To: <CAEJX8ortdHD_wB22Bc4PB3W1GTTuADB55fA3vGshCLre8kPncQ@mail.gmail.com>
References: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
	<CAEJX8ortdHD_wB22Bc4PB3W1GTTuADB55fA3vGshCLre8kPncQ@mail.gmail.com>
Message-ID: <7f74353e0d7c9af9a57778df9b934152@nomagicsoftware.com>

Ahh, the problem with superficial examples is well, they're 
superificial.
Let me rewrite the example a bit:
Thread 1:
run()
{
     log();
     Thread.sleep();
}

void log()
{
    log(this.shared)
}

and let me further stipulate that the JIT doesn't inline the log 
method.

I guess what I'm really asking is does CPU cache-coherency extend to 
registers?  Say, shared was a primitive (int,boolean), then 
theoretically shared could be cached in a register.  Would 
cache-coherency somehow reach down to another CPU's registers and 
'invalidate' the registers?


On 2013-02-14 09:38, Stanimir Simeonoff wrote:
> this.shared can be hoisted by JVM unless it's volatile. So Thread1
> always sees the initial value, e.g. null.
>
> The code can be x-formed into:
>  Thread 1:
> Foo shared = this.shared
>  while (true){
>  ? ? log(shared)
>  ? ? Thread.sleep(1000L)
>  }
>
> On Thu, Feb 14, 2013 at 6:48 PM, <thurston at nomagicsoftware.com> 
> wrote:
>
>> Given that all (?) modern CPUs provide cache-coherency, in the 
>> following (admittedly superficial example):
>>
>> Thread 1:
>> while (true)
>> {
>> ? ? log(this.shared)
>> ? ? Thread.sleep(1000L)
>> }
>>
>> Thread 2:
>> ? ?this.shared = new Foo();
>>
>> with Thread 2's code only invoked once and sometime significantly 
>> after (in a wall-clock sense)
>> Thread 1; and there are no operations performed by either thread 
>> forming a happens-before relationship (in the JMM sense).
>>
>> Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
>> And that that guarantee is provided not by the JMM, but by the 
>> cache-coherency of the CPUs?
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>
>
>
> Links:
> ------
> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Thu Feb 14 12:48:38 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 14 Feb 2013 19:48:38 +0200
Subject: [concurrency-interest] Numerical Stream code
In-Reply-To: <511D1E24.7070700@oracle.com>
References: <CACR_FB41DwO6HBohqAMYdH3DMnUhS=PoqKZjb0tp-8WWw1qQtQ@mail.gmail.com>
	<511CF6C3.8000709@oracle.com> <511CF876.1000003@oracle.com>
	<511D093D.5010608@gmail.com> <511D1E24.7070700@oracle.com>
Message-ID: <CAEJX8oozMKoTiE_-rwF_d48cc9ySwK4RUqa8itO2OeFTivHkuQ@mail.gmail.com>

> > Do element sizes matter (byte vs. short vs. int  vs. long)?
>
> I don't think so.  All of this assumes that the proper instruction is
> used.  For example, if 2 threads are writing to adjacent bytes, then the
> "mov" instruction has to only write the byte.  If the compiler, decides to
> read 32-bits, mask in the 8-bits and write 32-bits then the data will be
> corrupted.
>
JLS mandates no corruption for neighbor writes.


> I believe that HotSpot will only generate the write byte mov instruction.
>
> That would be the correct one. The case affects only
boolean[]/byte[]/short[]/char[]  as simple primitive fields are always at
least 32bits.

Stanimir



> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/14/2013 8:56 AM, Peter Levart wrote:
>
> On 02/14/2013 03:45 PM, Brian Goetz wrote:
>
> The parallel version is almost certainly suffering false cache line
> sharing when adjacent tasks are writing to the shared arrays u0, etc.
> Nothing to do with streams, just a standard parallelism gotcha.
>
> Cure: don't write to shared arrays from parallel tasks.
>
>
>  Hi,
>
> I would like to discuss this a little bit (hence the cc:
> concurrency-interest - the conversation can continue on this list only).
>
> Is it really important to avoid writing to shared arrays from multiple
> threads (of course without synchronization, not even volatile writes/reads)
> when indexes are not shared (each thread writes/reads it's own disjunct
> subset).
>
> Do element sizes matter (byte vs. short vs. int  vs. long)?
>
> I had a (false?) feeling that cache lines are not invalidated when writes
> are performed without fences.
>
> Also I don't know how short (byte, char) writes are combined into memory
> words on the hardware when they come from different cores and whether this
> is connected to any performance issues.
>
> Thanks,
>
> Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/be5d1db0/attachment-0001.html>

From stanimir at riflexo.com  Thu Feb 14 12:52:07 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 14 Feb 2013 19:52:07 +0200
Subject: [concurrency-interest] CPU cache-coherency's effects on
	visibility
In-Reply-To: <7f74353e0d7c9af9a57778df9b934152@nomagicsoftware.com>
References: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
	<CAEJX8ortdHD_wB22Bc4PB3W1GTTuADB55fA3vGshCLre8kPncQ@mail.gmail.com>
	<7f74353e0d7c9af9a57778df9b934152@nomagicsoftware.com>
Message-ID: <CAEJX8oriL5piQj8PKpzP0b0fr_-2E6YgJhp_=b-AQjpP8udz5A@mail.gmail.com>

I guess what I'm really asking is does CPU cache-coherency extend to
> registers?

Cache coherency affects memory, registers are just that and very private.
They are unaffected.


> Say, shared was a primitive (int,boolean), then theoretically shared could
> be cached in a register.  Would cache-coherency somehow reach down to
> another CPU's registers and 'invalidate' the registers?

No.

Stanimir

>
>
>
> On 2013-02-14 09:38, Stanimir Simeonoff wrote:
>
>> this.shared can be hoisted by JVM unless it's volatile. So Thread1
>> always sees the initial value, e.g. null.
>>
>> The code can be x-formed into:
>>  Thread 1:
>> Foo shared = this.shared
>>  while (true){
>>      log(shared)
>>      Thread.sleep(1000L)
>>  }
>>
>> On Thu, Feb 14, 2013 at 6:48 PM, <thurston at nomagicsoftware.com> wrote:
>>
>>  Given that all (?) modern CPUs provide cache-coherency, in the following
>>> (admittedly superficial example):
>>>
>>> Thread 1:
>>> while (true)
>>> {
>>>     log(this.shared)
>>>     Thread.sleep(1000L)
>>> }
>>>
>>> Thread 2:
>>>    this.shared = new Foo();
>>>
>>> with Thread 2's code only invoked once and sometime significantly after
>>> (in a wall-clock sense)
>>> Thread 1; and there are no operations performed by either thread forming
>>> a happens-before relationship (in the JMM sense).
>>>
>>> Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
>>> And that that guarantee is provided not by the JMM, but by the
>>> cache-coherency of the CPUs?
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>>
>>
>>
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/fbf0af5e/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 14 12:59:49 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 14 Feb 2013 10:59:49 -0700
Subject: [concurrency-interest] CPU cache-coherency's effects on
	visibility
In-Reply-To: <7f74353e0d7c9af9a57778df9b934152@nomagicsoftware.com>
References: <ced5e6e9a7cd31ab0f1baaf202dc134c@nomagicsoftware.com>
	<CAEJX8ortdHD_wB22Bc4PB3W1GTTuADB55fA3vGshCLre8kPncQ@mail.gmail.com>
	<7f74353e0d7c9af9a57778df9b934152@nomagicsoftware.com>
Message-ID: <511D2615.70606@oracle.com>

 > let me further stipulate that the JIT doesn't inline the log method.

Today it doesn't.  In the future, the JIT may do all sorts of tricks to 
enhance the log method.  So, you are essentially sitting on a ticking 
time bomb.  When it goes off, is a very difficult question.  If it does 
go off, it will be very difficult to reproduce and even if it were very 
reproducible, it would be very difficult to figure out.  I highly 
recommend not doing that.

 > Would cache-coherency somehow reach down to another CPU's registers 
and 'invalidate' the registers?

Cache coherency only affects L1, L2 and L3 caches.  It doesn't have any 
impact on the registers, the load/store buffer or any intermediate 
values moving about inside the core.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/14/2013 10:48 AM, thurston at nomagicsoftware.com wrote:
> Ahh, the problem with superficial examples is well, they're superificial.
> Let me rewrite the example a bit:
> Thread 1:
> run()
> {
>     log();
>     Thread.sleep();
> }
>
> void log()
> {
>    log(this.shared)
> }
>
> and let me further stipulate that the JIT doesn't inline the log method.
>
> I guess what I'm really asking is does CPU cache-coherency extend to 
> registers?  Say, shared was a primitive (int,boolean), then 
> theoretically shared could be cached in a register.  Would 
> cache-coherency somehow reach down to another CPU's registers and 
> 'invalidate' the registers?
>
>
> On 2013-02-14 09:38, Stanimir Simeonoff wrote:
>> this.shared can be hoisted by JVM unless it's volatile. So Thread1
>> always sees the initial value, e.g. null.
>>
>> The code can be x-formed into:
>>  Thread 1:
>> Foo shared = this.shared
>>  while (true){
>>      log(shared)
>>      Thread.sleep(1000L)
>>  }
>>
>> On Thu, Feb 14, 2013 at 6:48 PM, <thurston at nomagicsoftware.com> wrote:
>>
>>> Given that all (?) modern CPUs provide cache-coherency, in the 
>>> following (admittedly superficial example):
>>>
>>> Thread 1:
>>> while (true)
>>> {
>>>     log(this.shared)
>>>     Thread.sleep(1000L)
>>> }
>>>
>>> Thread 2:
>>>    this.shared = new Foo();
>>>
>>> with Thread 2's code only invoked once and sometime significantly 
>>> after (in a wall-clock sense)
>>> Thread 1; and there are no operations performed by either thread 
>>> forming a happens-before relationship (in the JMM sense).
>>>
>>> Is Thread 1 *guaranteed* to eventually see the write by Thread 2?
>>> And that that guarantee is provided not by the JMM, but by the 
>>> cache-coherency of the CPUs?
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest [1]
>>
>>
>>
>> Links:
>> ------
>> [1] http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130214/c56df949/attachment.html>

From thurston at nomagicsoftware.com  Thu Feb 14 14:40:42 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Thu, 14 Feb 2013 11:40:42 -0800
Subject: [concurrency-interest]
 =?utf-8?q?Some_interesting_=28confusing=3F?=
 =?utf-8?q?=29_benchmark_results?=
In-Reply-To: <511BBF9E.9050504@gmail.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
	<ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
	<511BB278.3030409@gmail.com>
	<b7f1522b4b8abdb3ef28e7b26ca50d8d@nomagicsoftware.com>
	<511BBF9E.9050504@gmail.com>
Message-ID: <c8b5af783cb975073bae61b27b2f9b6f@nomagicsoftware.com>

So my results are different in that the MergeSorter version (which 
creates many subtasks, 16K+) performs better than the tasks == workers 
(i.e. FJPThreads) == 2.  And not by a few percentage points - between 
30-40% on average.
It's difficult for me to make sense of these conflicting results.

As far as performing sorts in-place on an array; my question: is that 
multi-thread-safe?
(although my code faces the same issue since insertionSort() is done 
in-place).

There is nothing in the javadocs that specifically addresses this 
issue:
    "Tasks should also not perform blocking IO, and should ideally 
access variables that   are completely independent of those accessed by 
other running tasks" is the closest thing I could find.

Is a child returning a value from its compute method *happen-before* 
its parent's reading the return value from join(), i.e.

C(return from compute) < P(read return from join)

because P can occur on another thread than C
I guess that's a question for Dr. Lea



On 2013-02-13 08:30, Peter Levart wrote:
> On 02/13/2013 04:59 PM, thurston at nomagicsoftware.com wrote:
>> so you have 4 cores?
>> Makes sense that those results correspond to 20M as opposed to 2M
>> And given that you changed insertSort() -> Arrays.sort, if you 
>> change threshold to input.length / ForkJoinPool.threads (presumably 4 
>> in your case), does that perform better?
> It does (N=20M, THRESHOLD=5M, parallelism=4):
>
> Arrays.sort: 1529550064 nanos
> Arrays.sort: 1478430550 nanos
> Arrays.sort: 1475643365 nanos
> Arrays.sort: 1481308646 nanos
> Arrays.sort: 1477199836 nanos
> MergeSorter2: 500316422 nanos
> MergeSorter2: 486144409 nanos
> MergeSorter2: 468044083 nanos
> MergeSorter2: 624446626 nanos
> MergeSorter2: 635152854 nanos
>
>
>>
>> Many people were suggesting that the # of tasks created should be 
>> bounded by the # of threads in the fjpool which should == # of 
>> hardware threads. (you can accomplish this by setting the threshold to 
>> above formula).
>
> But as David Holmes has noted, the THRESHOLD should be further
> divided by 8 as a starting point, to accommodate for unequal 
> execution
> of threads/tasks, so that there is no waiting at the end for final
> long tasks to complete while other threads in pool are IDLE-ing...
>
> For example (N=20M, THRESHOLD=500K, parallelism=4):
>
> MergeSorter2: 531712994 nanos
> MergeSorter2: 539048356 nanos
> MergeSorter2: 586208993 nanos
> MergeSorter2: 544239837 nanos
> MergeSorter2: 497440246 nanos
>
> And (N=20M, THRESHOLD=50K, parallelism=4):
>
> MergeSorter2: 548634308 nanos
> MergeSorter2: 579119444 nanos
> MergeSorter2: 555252699 nanos
> MergeSorter2: 505902467 nanos
> MergeSorter2: 503907973 nanos
>
> And (N=20M, THRESHOLD=5K, parallelism=4):
>
> MergeSorter2: 547342635 nanos
> MergeSorter2: 575145627 nanos
> MergeSorter2: 559160837 nanos
> MergeSorter2: 530709697 nanos
> MergeSorter2: 562272911 nanos
>
> ...it does perform a little slower, but in average with lower
> THRESHOLD, the performance should be more stable...
>
>
> Regards, Peter
>
>>
>> And with 20M/250 (threshold), you would be creating what 100Ks?, 
>> 1Ms? (you can comment in the AtomicInteger.increment()) in the 
>> constructor to save on the math
>>
>> And of course *I* understand the performance characteristics of 
>> insertionSort, look at the implementation in SortUtil (which is not 
>> parallel).  What I did makes perfect sense if you understand 
>> insertSort vs mergeSort
>>
>> On 2013-02-13 07:34, Peter Levart wrote:
>>> On 02/13/2013 04:27 PM, thurston at nomagicsoftware.com wrote:
>>>> Could you publish your machine specs?
>>>> And what was the size of your sample array?  2M or something else?
>>>> If it is 2M (and if I'm doing my math right), it's weird that my 
>>>> times are less:
>>>> basically for 2M/250, I get around 370ms +- 25ms for the 
>>>> MergeSorter
>>>> and around 330ms for java's Array.sort
>>>
>>> Oh, sorry. My machine is i7 Linux PC, using recent JDK8 build. I 
>>> had
>>> to increase the N to 20M to actually make it sweat a little...
>>>
>>> Your unchanged MergeSorter is better than Arrays.sort with
>>> parallelism = 4 for example:
>>>
>>> MergeSorter: 1134740049 nanos
>>> MergeSorter: 1082783496 nanos
>>> MergeSorter: 1033780556 nanos
>>> MergeSorter2: 720092808 nanos
>>> MergeSorter2: 649416922 nanos
>>> MergeSorter2: 605587079 nanos
>>> Arrays.sort: 1454093373 nanos
>>> Arrays.sort: 1466476368 nanos
>>> Arrays.sort: 1461604292 nanos
>>>
>>> ...but with only 2 threads it is on-par with Arrays.sort.
>>>
>>> Regards, Peter
>>>
>>>>
>>>> Thanks
>>>>
>>>> On 2013-02-13 06:40, Peter Levart wrote:
>>>>> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>>>>>> Here is all of the relevant code in a single blob.  I think it's 
>>>>>> selef-explanatory; sample test is at the end.
>>>>>>
>>>>>> http://pastebin.com/WrfBHYSG
>>>>>
>>>>> Hi Thurston,
>>>>>
>>>>> You're doing too much copying. I modified tour MergeSorter a 
>>>>> little:
>>>>>
>>>>>
>>>>> public class MergeSorter2 extends RecursiveTask<int[]> {
>>>>>     static final int THRESHOLD = 250;
>>>>>     final int[] array, work;
>>>>>     final int offset, length;
>>>>>
>>>>>     public MergeSorter2(int[] array) {
>>>>>         this.array = array;
>>>>>         this.work = new int[array.length];
>>>>>         offset = 0;
>>>>>         length = array.length;
>>>>>     }
>>>>>
>>>>>     private MergeSorter2(int[] array, int[] work, int offset, int 
>>>>> length) {
>>>>>         this.array = array;
>>>>>         this.work = work;
>>>>>         this.offset = offset;
>>>>>         this.length = length;
>>>>>     }
>>>>>
>>>>>     @Override
>>>>>     protected int[] compute() {
>>>>>         if (length < MergeSorter2.THRESHOLD) {
>>>>>             Arrays.sort(array, offset, offset + length);
>>>>>             return this.array;
>>>>>         }
>>>>>
>>>>>         int halfLength = length >> 1;
>>>>>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
>>>>> halfLength);
>>>>>         left.fork();
>>>>>
>>>>>         int mid = offset + halfLength;
>>>>>
>>>>>         MergeSorter2 right = new MergeSorter2(array, work, mid,
>>>>> length - halfLength);
>>>>>         right.compute();
>>>>>         left.join();
>>>>>
>>>>>         // copy 1st half from array to work
>>>>>         System.arraycopy(array, offset, work, offset, 
>>>>> halfLength);
>>>>>
>>>>>         // merge 1st half of work and 2nd half of array into 
>>>>> array
>>>>>         int end = offset + length;
>>>>>         int i = offset;
>>>>>         int j = mid;
>>>>>         int k = offset;
>>>>>         int x = work[i];
>>>>>         int y = array[j];
>>>>>         while (true) {
>>>>>             if (j >= end || x <= y) {
>>>>>                 array[k++] = x;
>>>>>                 i++;
>>>>>                 // until we drain the 1st half (the rest of array 
>>>>> is
>>>>> already in place)
>>>>>                 if (i >= mid) break;
>>>>>                 x = work[i];
>>>>>             } else { // j < end
>>>>>                 array[k++] = y;
>>>>>                 j++;
>>>>>                 if (j < end) y = array[j];
>>>>>             }
>>>>>         }
>>>>>
>>>>>         return this.array;
>>>>>     }
>>>>> }
>>>>>
>>>>>
>>>>> ... it uses a single parallel "work" array for merging, which is
>>>>> shared among tasks. It tries to minimize the copying. Here's the
>>>>> results I get on a FJPool with parallelism=2:
>>>>>
>>>>>  MergeSorter: 1557638090 nanos
>>>>>  MergeSorter: 1545600414 nanos
>>>>>  MergeSorter: 1478185502 nanos
>>>>> MergeSorter2: 1058750062 nanos
>>>>> MergeSorter2:  978295370 nanos
>>>>> MergeSorter2:  976838429 nanos
>>>>>  Arrays.sort: 1494256525 nanos
>>>>>  Arrays.sort: 1505639161 nanos
>>>>>  Arrays.sort: 1483640342 nanos
>>>>>
>>>>>
>>>>> I get even better results with greater THRESHOLD (but not more 
>>>>> than
>>>>> 10% better). Your MergeSorter is worse with greater THRESHOLD 
>>>>> because
>>>>> you are using insert-sort for final tasks. The Arrays.sort() 
>>>>> delegates
>>>>> to insert-sort only when N < 47, else it uses quick-sort...
>>>>>
>>>>> Regards, Peter
>>>>>
>>>>>
>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>>>>>> Yes, I will push the code and my measurements some time this 
>>>>>>> week.
>>>>>>> I'm really interested in seeing what others' results would be. 
>>>>>>> What I
>>>>>>> can say is that the timings are surprisingly stable (range 
>>>>>>> 30-40ms)
>>>>>>> and the input data is randomly generated for each test run 
>>>>>>> which can
>>>>>>> certainly affect merge(int[], int[])'s performance
>>>>>>>
>>>>>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>>>>>> Hi,
>>>>>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>>>>>> Hi T,
>>>>>>>>
>>>>>>>> Can you pub the raw data from your runs? Average tends to hide
>>>>>>>> effects and it's difficult to say anything with only that 
>>>>>>>> measure.
>>>>>>>>
>>>>>>>> Regards,
>>>>>>>> Kirk
>>>>>>>>
>>>>>>>>
>>>>>>>>> Hello,
>>>>>>>>>
>>>>>>>>> I made some initial attempts at using ForkJoin framework for 
>>>>>>>>> some rather obvious recursively parallel use-cases, namely 
>>>>>>>>> summing the elements of an int[] and also sorting an int[] 
>>>>>>>>> (randomly filled).
>>>>>>>>>
>>>>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>>>>
>>>>>>>>> I really enjoy using the framework and find it relatively 
>>>>>>>>> easy to reason about my programs (if not necessarily the 
>>>>>>>>> internals of the framework).
>>>>>>>>> But I was disappointed with the results: in both cases they 
>>>>>>>>> were slower than the corresponding Java serial implementation.
>>>>>>>>>
>>>>>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>>>>>> expecting 2x speedup, but especially in the case of the sort, 
>>>>>>>>> but I was expecting that ForkJoin would do at least a little 
>>>>>>>>> better than the single-threaded version.  I guess what I'm 
>>>>>>>>> asking is: are these results surprising?  Does it mean I'm 
>>>>>>>>> doing something wrong?
>>>>>>>>>
>>>>>>>>> I'll give a brief description of my implementation:
>>>>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>>>>
>>>>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 
>>>>>>>>> (arrived at by trial-and-error)
>>>>>>>>>
>>>>>>>>> int[] compute()
>>>>>>>>> {
>>>>>>>>> if int[].length < THRESHOLD
>>>>>>>>>    return insertionSort(int[])
>>>>>>>>> left = new MergeSort(int[] //split in half)
>>>>>>>>> right = new MergeSort(int[] //other half)
>>>>>>>>> return merge(right.compute(), left.join())
>>>>>>>>> }
>>>>>>>>>
>>>>>>>>> The insertionSort() and merge() methods are just standard 
>>>>>>>>> implementations; there is a bit of apples to oranges comparison 
>>>>>>>>> since Arrays.sort() uses an optimized quicksort, but we're 
>>>>>>>>> still talking O(nlog(n))
>>>>>>>>>
>>>>>>>>> Just ran it on my laptop:
>>>>>>>>> Windows 7 64-bit
>>>>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>>>>
>>>>>>>>> 2M int[]:
>>>>>>>>> single-threaded: ~330ms
>>>>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>>>>
>>>>>>>>> Would appreciate any feedback and hopefully the #s are 
>>>>>>>>> somewhat helpful (and please no scoffawing at my antiquated 
>>>>>>>>> machine)
>>>>>>>>>
>>>>>>>>> -T
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>


From ach at quartetfs.com  Fri Feb 15 04:48:45 2013
From: ach at quartetfs.com (Antoine Chambille)
Date: Fri, 15 Feb 2013 10:48:45 +0100
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory databases
Message-ID: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>

I think this community is the right place to start a conversation about
NUMA (aren't NUMA nodes to memory what multiprocessors are to processing?
;). I apologize if this is considered off-topic.


We are developing a Java in-memory analytical database (it's called
"ActivePivot") that our customers deploy on ever larger datasets. Some
ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
trend, and we are researching solutions to improve our performance on NUMA
configurations.


We understand that in the current state of things (and including JDK8) the
support for NUMA in hotspot is the following:
* The young generation heap layout can be NUMA-Aware (partitioned per NUMA
node, objects allocated in the same node than the running thread)
* The old generation heap layout is not optimized for NUMA (at best the old
generation is interleaved among nodes which at least makes memory accesses
somewhat uniform)
* The parallel garbage collector is NUMA optimized, the GC threads focusing
on objects in their node.


Yet activating -XX:+UseNUMA option has almost no impact on the performance
of our in-memory database. It is not surprising, the pattern for a database
is to load the data in the memory and then make queries on it. The data
goes and stays in the old generation, and it is read from there by queries.
Most memory accesses are in the old gen and most of those are not local.

I guess there is a reason hotspot does not yet optimize the old generation
for NUMA. It must be very difficult to do it in the general case, when you
have no idea what thread from what node will read data and interleaving is.
But for an in-memory database this is frustrating because we know very well
which threads will access which piece of data. At least in ActivePivot data
structures are partitioned, partitions are each assigned a thread pool so
the threads that allocated the data in a partition are also the threads
that perform sub-queries on that partition. We are a few lines of code away
from binding thread pools to NUMA nodes, and if the garbage collector would
leave objects promoted to the old generation on their original NUMA node
memory accesses would be close to optimal.

We have not been able to do that. But that being said I read an inspiring
2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth that did
experiment on NUMA layouts for the old generation. ("NUMA-aware Java heaps
for server applications"
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf).
That motivated me to ask the following questions:


* Are there hidden or experimental hotspot options that allow NUMA-Aware
partitioning of the old generation?
* Do you know why there isn't much (visible, generally available) research
on NUMA optimizations for the old gen? Is the Java in-memory database use
case considered a rare one?
* Maybe we should experiment and even contribute new heap layouts to the
open-jdk project. Can some of you guys comment on the difficulty of that?


Thanks for reading,

--
Antoine CHAMBILLE
Director Research & Development
Quartet FS
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130215/d9236740/attachment.html>

From peter.levart at gmail.com  Fri Feb 15 04:54:59 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 15 Feb 2013 10:54:59 +0100
Subject: [concurrency-interest] Some interesting (confusing?) benchmark
 results
In-Reply-To: <c8b5af783cb975073bae61b27b2f9b6f@nomagicsoftware.com>
References: <531d336576d417ea3cf5c8acbc6aa8fe@nomagicsoftware.com>
	<24C91203-37C8-41B9-B37B-C5624A504BC6@kodewerk.com>
	<c2c7eef3bb1c6bff01b4a0786e9c445d@nomagicsoftware.com>
	<cf9c8a1cf997d387ef77e345fd99cda6@nomagicsoftware.com>
	<511BA5C9.3000003@gmail.com>
	<ee400a66923639a90b051d832afb3222@nomagicsoftware.com>
	<511BB278.3030409@gmail.com>
	<b7f1522b4b8abdb3ef28e7b26ca50d8d@nomagicsoftware.com>
	<511BBF9E.9050504@gmail.com>
	<c8b5af783cb975073bae61b27b2f9b6f@nomagicsoftware.com>
Message-ID: <511E05F3.5030905@gmail.com>

On 02/14/2013 08:40 PM, thurston at nomagicsoftware.com wrote:
> So my results are different in that the MergeSorter version (which 
> creates many subtasks, 16K+) performs better than the tasks == workers 
> (i.e. FJPThreads) == 2.  And not by a few percentage points - between 
> 30-40% on average.
> It's difficult for me to make sense of these conflicting results.

Your MergeSorter uses an inferior algorithm for sorting in the leaf 
tasks (insert-sort). It has a time complexity of O(N^2) for random 
inputs, but it is as fast or even faster than better algorithms (quick, 
heap, merge-sort) for very small Ns (Arrays.sort delegates to 
insert-sort when N < 47), so when the THRESHOLD is big, the N^2 hits you.

>
> As far as performing sorts in-place on an array; my question: is that 
> multi-thread-safe?
> (although my code faces the same issue since insertionSort() is done 
> in-place).

As long as the algorithm guarantees that segments of array that are used 
(read or written) by different threads (tasks) that execute concurrently 
are disjunct, you are fine. When you do a SubTask.join(), the part of 
the array that this subtask was responsible for, is ready to be used by 
the task that joined the subtask - that's all the synchronization you need.

>
> There is nothing in the javadocs that specifically addresses this issue:
>    "Tasks should also not perform blocking IO, and should ideally 
> access variables that   are completely independent of those accessed 
> by other running tasks" is the closest thing I could find.

Individual elements of an array *are* independent variables.

>
> Is a child returning a value from its compute method *happen-before* 
> its parent's reading the return value from join(), i.e.
>
> C(return from compute) < P(read return from join)
>
> because P can occur on another thread than C
> I guess that's a question for Dr. Lea

Although there's no explicit mention of that in the javadoc (or I just 
can't spot it), this should be self-evident. I don't know how FJPool 
would be useful otherwise.

Regards, Peter

>
>
>
> On 2013-02-13 08:30, Peter Levart wrote:
>> On 02/13/2013 04:59 PM, thurston at nomagicsoftware.com wrote:
>>> so you have 4 cores?
>>> Makes sense that those results correspond to 20M as opposed to 2M
>>> And given that you changed insertSort() -> Arrays.sort, if you 
>>> change threshold to input.length / ForkJoinPool.threads (presumably 
>>> 4 in your case), does that perform better?
>> It does (N=20M, THRESHOLD=5M, parallelism=4):
>>
>> Arrays.sort: 1529550064 nanos
>> Arrays.sort: 1478430550 nanos
>> Arrays.sort: 1475643365 nanos
>> Arrays.sort: 1481308646 nanos
>> Arrays.sort: 1477199836 nanos
>> MergeSorter2: 500316422 nanos
>> MergeSorter2: 486144409 nanos
>> MergeSorter2: 468044083 nanos
>> MergeSorter2: 624446626 nanos
>> MergeSorter2: 635152854 nanos
>>
>>
>>>
>>> Many people were suggesting that the # of tasks created should be 
>>> bounded by the # of threads in the fjpool which should == # of 
>>> hardware threads. (you can accomplish this by setting the threshold 
>>> to above formula).
>>
>> But as David Holmes has noted, the THRESHOLD should be further
>> divided by 8 as a starting point, to accommodate for unequal execution
>> of threads/tasks, so that there is no waiting at the end for final
>> long tasks to complete while other threads in pool are IDLE-ing...
>>
>> For example (N=20M, THRESHOLD=500K, parallelism=4):
>>
>> MergeSorter2: 531712994 nanos
>> MergeSorter2: 539048356 nanos
>> MergeSorter2: 586208993 nanos
>> MergeSorter2: 544239837 nanos
>> MergeSorter2: 497440246 nanos
>>
>> And (N=20M, THRESHOLD=50K, parallelism=4):
>>
>> MergeSorter2: 548634308 nanos
>> MergeSorter2: 579119444 nanos
>> MergeSorter2: 555252699 nanos
>> MergeSorter2: 505902467 nanos
>> MergeSorter2: 503907973 nanos
>>
>> And (N=20M, THRESHOLD=5K, parallelism=4):
>>
>> MergeSorter2: 547342635 nanos
>> MergeSorter2: 575145627 nanos
>> MergeSorter2: 559160837 nanos
>> MergeSorter2: 530709697 nanos
>> MergeSorter2: 562272911 nanos
>>
>> ...it does perform a little slower, but in average with lower
>> THRESHOLD, the performance should be more stable...
>>
>>
>> Regards, Peter
>>
>>>
>>> And with 20M/250 (threshold), you would be creating what 100Ks?, 
>>> 1Ms? (you can comment in the AtomicInteger.increment()) in the 
>>> constructor to save on the math
>>>
>>> And of course *I* understand the performance characteristics of 
>>> insertionSort, look at the implementation in SortUtil (which is not 
>>> parallel).  What I did makes perfect sense if you understand 
>>> insertSort vs mergeSort
>>>
>>> On 2013-02-13 07:34, Peter Levart wrote:
>>>> On 02/13/2013 04:27 PM, thurston at nomagicsoftware.com wrote:
>>>>> Could you publish your machine specs?
>>>>> And what was the size of your sample array?  2M or something else?
>>>>> If it is 2M (and if I'm doing my math right), it's weird that my 
>>>>> times are less:
>>>>> basically for 2M/250, I get around 370ms +- 25ms for the MergeSorter
>>>>> and around 330ms for java's Array.sort
>>>>
>>>> Oh, sorry. My machine is i7 Linux PC, using recent JDK8 build. I had
>>>> to increase the N to 20M to actually make it sweat a little...
>>>>
>>>> Your unchanged MergeSorter is better than Arrays.sort with
>>>> parallelism = 4 for example:
>>>>
>>>> MergeSorter: 1134740049 nanos
>>>> MergeSorter: 1082783496 nanos
>>>> MergeSorter: 1033780556 nanos
>>>> MergeSorter2: 720092808 nanos
>>>> MergeSorter2: 649416922 nanos
>>>> MergeSorter2: 605587079 nanos
>>>> Arrays.sort: 1454093373 nanos
>>>> Arrays.sort: 1466476368 nanos
>>>> Arrays.sort: 1461604292 nanos
>>>>
>>>> ...but with only 2 threads it is on-par with Arrays.sort.
>>>>
>>>> Regards, Peter
>>>>
>>>>>
>>>>> Thanks
>>>>>
>>>>> On 2013-02-13 06:40, Peter Levart wrote:
>>>>>> On 02/12/2013 08:20 PM, thurston at nomagicsoftware.com wrote:
>>>>>>> Here is all of the relevant code in a single blob.  I think it's 
>>>>>>> selef-explanatory; sample test is at the end.
>>>>>>>
>>>>>>> http://pastebin.com/WrfBHYSG
>>>>>>
>>>>>> Hi Thurston,
>>>>>>
>>>>>> You're doing too much copying. I modified tour MergeSorter a little:
>>>>>>
>>>>>>
>>>>>> public class MergeSorter2 extends RecursiveTask<int[]> {
>>>>>>     static final int THRESHOLD = 250;
>>>>>>     final int[] array, work;
>>>>>>     final int offset, length;
>>>>>>
>>>>>>     public MergeSorter2(int[] array) {
>>>>>>         this.array = array;
>>>>>>         this.work = new int[array.length];
>>>>>>         offset = 0;
>>>>>>         length = array.length;
>>>>>>     }
>>>>>>
>>>>>>     private MergeSorter2(int[] array, int[] work, int offset, int 
>>>>>> length) {
>>>>>>         this.array = array;
>>>>>>         this.work = work;
>>>>>>         this.offset = offset;
>>>>>>         this.length = length;
>>>>>>     }
>>>>>>
>>>>>>     @Override
>>>>>>     protected int[] compute() {
>>>>>>         if (length < MergeSorter2.THRESHOLD) {
>>>>>>             Arrays.sort(array, offset, offset + length);
>>>>>>             return this.array;
>>>>>>         }
>>>>>>
>>>>>>         int halfLength = length >> 1;
>>>>>>         MergeSorter2 left = new MergeSorter2(array, work, offset, 
>>>>>> halfLength);
>>>>>>         left.fork();
>>>>>>
>>>>>>         int mid = offset + halfLength;
>>>>>>
>>>>>>         MergeSorter2 right = new MergeSorter2(array, work, mid,
>>>>>> length - halfLength);
>>>>>>         right.compute();
>>>>>>         left.join();
>>>>>>
>>>>>>         // copy 1st half from array to work
>>>>>>         System.arraycopy(array, offset, work, offset, halfLength);
>>>>>>
>>>>>>         // merge 1st half of work and 2nd half of array into array
>>>>>>         int end = offset + length;
>>>>>>         int i = offset;
>>>>>>         int j = mid;
>>>>>>         int k = offset;
>>>>>>         int x = work[i];
>>>>>>         int y = array[j];
>>>>>>         while (true) {
>>>>>>             if (j >= end || x <= y) {
>>>>>>                 array[k++] = x;
>>>>>>                 i++;
>>>>>>                 // until we drain the 1st half (the rest of array is
>>>>>> already in place)
>>>>>>                 if (i >= mid) break;
>>>>>>                 x = work[i];
>>>>>>             } else { // j < end
>>>>>>                 array[k++] = y;
>>>>>>                 j++;
>>>>>>                 if (j < end) y = array[j];
>>>>>>             }
>>>>>>         }
>>>>>>
>>>>>>         return this.array;
>>>>>>     }
>>>>>> }
>>>>>>
>>>>>>
>>>>>> ... it uses a single parallel "work" array for merging, which is
>>>>>> shared among tasks. It tries to minimize the copying. Here's the
>>>>>> results I get on a FJPool with parallelism=2:
>>>>>>
>>>>>>  MergeSorter: 1557638090 nanos
>>>>>>  MergeSorter: 1545600414 nanos
>>>>>>  MergeSorter: 1478185502 nanos
>>>>>> MergeSorter2: 1058750062 nanos
>>>>>> MergeSorter2:  978295370 nanos
>>>>>> MergeSorter2:  976838429 nanos
>>>>>>  Arrays.sort: 1494256525 nanos
>>>>>>  Arrays.sort: 1505639161 nanos
>>>>>>  Arrays.sort: 1483640342 nanos
>>>>>>
>>>>>>
>>>>>> I get even better results with greater THRESHOLD (but not more than
>>>>>> 10% better). Your MergeSorter is worse with greater THRESHOLD 
>>>>>> because
>>>>>> you are using insert-sort for final tasks. The Arrays.sort() 
>>>>>> delegates
>>>>>> to insert-sort only when N < 47, else it uses quick-sort...
>>>>>>
>>>>>> Regards, Peter
>>>>>>
>>>>>>
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On 2013-02-12 01:55, thurston at nomagicsoftware.com wrote:
>>>>>>>> Yes, I will push the code and my measurements some time this week.
>>>>>>>> I'm really interested in seeing what others' results would be. 
>>>>>>>> What I
>>>>>>>> can say is that the timings are surprisingly stable (range 
>>>>>>>> 30-40ms)
>>>>>>>> and the input data is randomly generated for each test run 
>>>>>>>> which can
>>>>>>>> certainly affect merge(int[], int[])'s performance
>>>>>>>>
>>>>>>>> On 2013-02-12 01:50, Kirk Pepperdine wrote:
>>>>>>>>> Hi,
>>>>>>>>> On 2013-02-11, at 9:52 PM, thurston at nomagicsoftware.com wrote:
>>>>>>>>> Hi T,
>>>>>>>>>
>>>>>>>>> Can you pub the raw data from your runs? Average tends to hide
>>>>>>>>> effects and it's difficult to say anything with only that 
>>>>>>>>> measure.
>>>>>>>>>
>>>>>>>>> Regards,
>>>>>>>>> Kirk
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>> Hello,
>>>>>>>>>>
>>>>>>>>>> I made some initial attempts at using ForkJoin framework for 
>>>>>>>>>> some rather obvious recursively parallel use-cases, namely 
>>>>>>>>>> summing the elements of an int[] and also sorting an int[] 
>>>>>>>>>> (randomly filled).
>>>>>>>>>>
>>>>>>>>>> The problem-sizes I used ranged from 1M - 10M.
>>>>>>>>>>
>>>>>>>>>> I really enjoy using the framework and find it relatively 
>>>>>>>>>> easy to reason about my programs (if not necessarily the 
>>>>>>>>>> internals of the framework).
>>>>>>>>>> But I was disappointed with the results: in both cases they 
>>>>>>>>>> were slower than the corresponding Java serial implementation.
>>>>>>>>>>
>>>>>>>>>> I completely understand the principle of YMMV, and I wasn't 
>>>>>>>>>> expecting 2x speedup, but especially in the case of the sort, 
>>>>>>>>>> but I was expecting that ForkJoin would do at least a little 
>>>>>>>>>> better than the single-threaded version. I guess what I'm 
>>>>>>>>>> asking is: are these results surprising?  Does it mean I'm 
>>>>>>>>>> doing something wrong?
>>>>>>>>>>
>>>>>>>>>> I'll give a brief description of my implementation:
>>>>>>>>>> single-threaded:  Arrays.sort(int[])
>>>>>>>>>>
>>>>>>>>>> ForkJoin: I'm using a merge-sort, with THRESHOLD = 250 
>>>>>>>>>> (arrived at by trial-and-error)
>>>>>>>>>>
>>>>>>>>>> int[] compute()
>>>>>>>>>> {
>>>>>>>>>> if int[].length < THRESHOLD
>>>>>>>>>>    return insertionSort(int[])
>>>>>>>>>> left = new MergeSort(int[] //split in half)
>>>>>>>>>> right = new MergeSort(int[] //other half)
>>>>>>>>>> return merge(right.compute(), left.join())
>>>>>>>>>> }
>>>>>>>>>>
>>>>>>>>>> The insertionSort() and merge() methods are just standard 
>>>>>>>>>> implementations; there is a bit of apples to oranges 
>>>>>>>>>> comparison since Arrays.sort() uses an optimized quicksort, 
>>>>>>>>>> but we're still talking O(nlog(n))
>>>>>>>>>>
>>>>>>>>>> Just ran it on my laptop:
>>>>>>>>>> Windows 7 64-bit
>>>>>>>>>> 1.7.0_03; Java HotSpot(TM) 64-Bit Server VM 22.1-b02
>>>>>>>>>> Core 2 Duo==> 2 cores 2GHz
>>>>>>>>>>
>>>>>>>>>> 2M int[]:
>>>>>>>>>> single-threaded: ~330ms
>>>>>>>>>> ForkJoin (2 workers): ~390ms
>>>>>>>>>>
>>>>>>>>>> Would appreciate any feedback and hopefully the #s are 
>>>>>>>>>> somewhat helpful (and please no scoffawing at my antiquated 
>>>>>>>>>> machine)
>>>>>>>>>>
>>>>>>>>>> -T
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>
>


From stanimir at riflexo.com  Fri Feb 15 05:35:31 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 15 Feb 2013 12:35:31 +0200
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
Message-ID: <CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>

Just out of curiosity: would not DirectBuffers and managing the data
yourself would be both easier and more efficient?
Technically you can ship the data w/o even copying it straight to the
sockets (or disks).
I don't know how you store the data itself but I can think only of tuples
i.e. Object[].

Stanimir

On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:

> I think this community is the right place to start a conversation about
> NUMA (aren't NUMA nodes to memory what multiprocessors are to processing?
> ;). I apologize if this is considered off-topic.
>
>
> We are developing a Java in-memory analytical database (it's called
> "ActivePivot") that our customers deploy on ever larger datasets. Some
> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
> trend, and we are researching solutions to improve our performance on NUMA
> configurations.
>
>
> We understand that in the current state of things (and including JDK8) the
> support for NUMA in hotspot is the following:
> * The young generation heap layout can be NUMA-Aware (partitioned per NUMA
> node, objects allocated in the same node than the running thread)
> * The old generation heap layout is not optimized for NUMA (at best the
> old generation is interleaved among nodes which at least makes memory
> accesses somewhat uniform)
> * The parallel garbage collector is NUMA optimized, the GC threads
> focusing on objects in their node.
>
>
> Yet activating -XX:+UseNUMA option has almost no impact on the performance
> of our in-memory database. It is not surprising, the pattern for a database
> is to load the data in the memory and then make queries on it. The data
> goes and stays in the old generation, and it is read from there by queries.
> Most memory accesses are in the old gen and most of those are not local.
>
> I guess there is a reason hotspot does not yet optimize the old generation
> for NUMA. It must be very difficult to do it in the general case, when you
> have no idea what thread from what node will read data and interleaving is.
> But for an in-memory database this is frustrating because we know very well
> which threads will access which piece of data. At least in ActivePivot data
> structures are partitioned, partitions are each assigned a thread pool so
> the threads that allocated the data in a partition are also the threads
> that perform sub-queries on that partition. We are a few lines of code away
> from binding thread pools to NUMA nodes, and if the garbage collector would
> leave objects promoted to the old generation on their original NUMA node
> memory accesses would be close to optimal.
>
> We have not been able to do that. But that being said I read an inspiring
> 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth that did
> experiment on NUMA layouts for the old generation. ("NUMA-aware Java heaps
> for server applications"
> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>
>
> * Are there hidden or experimental hotspot options that allow NUMA-Aware
> partitioning of the old generation?
> * Do you know why there isn't much (visible, generally available) research
> on NUMA optimizations for the old gen? Is the Java in-memory database use
> case considered a rare one?
> * Maybe we should experiment and even contribute new heap layouts to the
> open-jdk project. Can some of you guys comment on the difficulty of that?
>
>
> Thanks for reading,
>
> --
> Antoine CHAMBILLE
> Director Research & Development
> Quartet FS
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130215/075323f7/attachment-0001.html>

From ach at quartetfs.com  Fri Feb 15 06:18:58 2013
From: ach at quartetfs.com (Antoine Chambille)
Date: Fri, 15 Feb 2013 12:18:58 +0100
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
	<CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
Message-ID: <CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>

Data is stored in columns, to maximize the performance of analytical
queries that commonly scan billions of rows but only for a subset of the
columns. We support a mix of primitive data and object oriented data ( some
columns look like double[], some other look like Object[] ).

Using direct buffers would open a door to NUMA-Aware memory placement
(provided that the direct allocation itself can be made on the right node).
That's probably more a Pandora box than a door though ;) Anyway it implies
serializing data into byte arrays, and deserializing at each query. That's
a serious performance penalty for primitive data, and that's absolutely
prohibitive when you do that with plain objects, even Externizable ones.

-Antoine


On 15 February 2013 11:35, Stanimir Simeonoff <stanimir at riflexo.com> wrote:

> Just out of curiosity: would not DirectBuffers and managing the data
> yourself would be both easier and more efficient?
> Technically you can ship the data w/o even copying it straight to the
> sockets (or disks).
> I don't know how you store the data itself but I can think only of tuples
> i.e. Object[].
>
> Stanimir
>
> On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:
>
>> I think this community is the right place to start a conversation about
>> NUMA (aren't NUMA nodes to memory what multiprocessors are to processing?
>> ;). I apologize if this is considered off-topic.
>>
>>
>> We are developing a Java in-memory analytical database (it's called
>> "ActivePivot") that our customers deploy on ever larger datasets. Some
>> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
>> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
>> trend, and we are researching solutions to improve our performance on NUMA
>> configurations.
>>
>>
>> We understand that in the current state of things (and including JDK8)
>> the support for NUMA in hotspot is the following:
>> * The young generation heap layout can be NUMA-Aware (partitioned per
>> NUMA node, objects allocated in the same node than the running thread)
>> * The old generation heap layout is not optimized for NUMA (at best the
>> old generation is interleaved among nodes which at least makes memory
>> accesses somewhat uniform)
>> * The parallel garbage collector is NUMA optimized, the GC threads
>> focusing on objects in their node.
>>
>>
>> Yet activating -XX:+UseNUMA option has almost no impact on the
>> performance of our in-memory database. It is not surprising, the pattern
>> for a database is to load the data in the memory and then make queries on
>> it. The data goes and stays in the old generation, and it is read from
>> there by queries. Most memory accesses are in the old gen and most of those
>> are not local.
>>
>> I guess there is a reason hotspot does not yet optimize the old
>> generation for NUMA. It must be very difficult to do it in the general
>> case, when you have no idea what thread from what node will read data and
>> interleaving is. But for an in-memory database this is frustrating because
>> we know very well which threads will access which piece of data. At least
>> in ActivePivot data structures are partitioned, partitions are each
>> assigned a thread pool so the threads that allocated the data in a
>> partition are also the threads that perform sub-queries on that partition.
>> We are a few lines of code away from binding thread pools to NUMA nodes,
>> and if the garbage collector would leave objects promoted to the old
>> generation on their original NUMA node memory accesses would be close to
>> optimal.
>>
>> We have not been able to do that. But that being said I read an inspiring
>> 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth that did
>> experiment on NUMA layouts for the old generation. ("NUMA-aware Java heaps
>> for server applications"
>> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>>
>>
>> * Are there hidden or experimental hotspot options that allow NUMA-Aware
>> partitioning of the old generation?
>> * Do you know why there isn't much (visible, generally available)
>> research on NUMA optimizations for the old gen? Is the Java in-memory
>> database use case considered a rare one?
>> * Maybe we should experiment and even contribute new heap layouts to the
>> open-jdk project. Can some of you guys comment on the difficulty of that?
>>
>>
>> Thanks for reading,
>>
>> --
>> Antoine CHAMBILLE
>> Director Research & Development
>> Quartet FS
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
Antoine CHAMBILLE
Director Research & Development
Quartet FS
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130215/f72f3f7e/attachment.html>

From michal.warecki at gmail.com  Fri Feb 15 10:24:51 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Fri, 15 Feb 2013 16:24:51 +0100
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
	<CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
	<CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>
Message-ID: <CAJ_mLLorEBpZxUONUE8Lj1K+qrQMTJH0hA_X5Sq5bEL40kX1wQ@mail.gmail.com>

Hi!

I may be wrong but:
If you know in which NUMA node the data are stored and which thread will
read this data, you can use pthead_setaffinity_np() function.
This will direct particular thread to particular CPU with faster access to
particular NUMA node.
That's not a Pandora box but you have to use JNI.

Cheers,
Micha?

2013/2/15 Antoine Chambille <ach at quartetfs.com>

> Data is stored in columns, to maximize the performance of analytical
> queries that commonly scan billions of rows but only for a subset of the
> columns. We support a mix of primitive data and object oriented data ( some
> columns look like double[], some other look like Object[] ).
>
> Using direct buffers would open a door to NUMA-Aware memory placement
> (provided that the direct allocation itself can be made on the right node).
> That's probably more a Pandora box than a door though ;) Anyway it implies
> serializing data into byte arrays, and deserializing at each query. That's
> a serious performance penalty for primitive data, and that's absolutely
> prohibitive when you do that with plain objects, even Externizable ones.
>
> -Antoine
>
>
> On 15 February 2013 11:35, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>> Just out of curiosity: would not DirectBuffers and managing the data
>> yourself would be both easier and more efficient?
>> Technically you can ship the data w/o even copying it straight to the
>> sockets (or disks).
>> I don't know how you store the data itself but I can think only of tuples
>> i.e. Object[].
>>
>> Stanimir
>>
>> On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:
>>
>>> I think this community is the right place to start a conversation about
>>> NUMA (aren't NUMA nodes to memory what multiprocessors are to processing?
>>> ;). I apologize if this is considered off-topic.
>>>
>>>
>>> We are developing a Java in-memory analytical database (it's called
>>> "ActivePivot") that our customers deploy on ever larger datasets. Some
>>> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
>>> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
>>> trend, and we are researching solutions to improve our performance on NUMA
>>> configurations.
>>>
>>>
>>> We understand that in the current state of things (and including JDK8)
>>> the support for NUMA in hotspot is the following:
>>> * The young generation heap layout can be NUMA-Aware (partitioned per
>>> NUMA node, objects allocated in the same node than the running thread)
>>> * The old generation heap layout is not optimized for NUMA (at best the
>>> old generation is interleaved among nodes which at least makes memory
>>> accesses somewhat uniform)
>>> * The parallel garbage collector is NUMA optimized, the GC threads
>>> focusing on objects in their node.
>>>
>>>
>>> Yet activating -XX:+UseNUMA option has almost no impact on the
>>> performance of our in-memory database. It is not surprising, the pattern
>>> for a database is to load the data in the memory and then make queries on
>>> it. The data goes and stays in the old generation, and it is read from
>>> there by queries. Most memory accesses are in the old gen and most of those
>>> are not local.
>>>
>>> I guess there is a reason hotspot does not yet optimize the old
>>> generation for NUMA. It must be very difficult to do it in the general
>>> case, when you have no idea what thread from what node will read data and
>>> interleaving is. But for an in-memory database this is frustrating because
>>> we know very well which threads will access which piece of data. At least
>>> in ActivePivot data structures are partitioned, partitions are each
>>> assigned a thread pool so the threads that allocated the data in a
>>> partition are also the threads that perform sub-queries on that partition.
>>> We are a few lines of code away from binding thread pools to NUMA nodes,
>>> and if the garbage collector would leave objects promoted to the old
>>> generation on their original NUMA node memory accesses would be close to
>>> optimal.
>>>
>>> We have not been able to do that. But that being said I read an
>>> inspiring 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth
>>> that did experiment on NUMA layouts for the old generation. ("NUMA-aware
>>> Java heaps for server applications"
>>> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>>>
>>>
>>> * Are there hidden or experimental hotspot options that allow NUMA-Aware
>>> partitioning of the old generation?
>>> * Do you know why there isn't much (visible, generally available)
>>> research on NUMA optimizations for the old gen? Is the Java in-memory
>>> database use case considered a rare one?
>>> * Maybe we should experiment and even contribute new heap layouts to the
>>> open-jdk project. Can some of you guys comment on the difficulty of that?
>>>
>>>
>>> Thanks for reading,
>>>
>>> --
>>> Antoine CHAMBILLE
>>> Director Research & Development
>>> Quartet FS
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
> --
> Antoine CHAMBILLE
> Director Research & Development
> Quartet FS
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130215/b1b98f3a/attachment.html>

From ach at quartetfs.com  Fri Feb 15 10:58:23 2013
From: ach at quartetfs.com (Antoine Chambille)
Date: Fri, 15 Feb 2013 16:58:23 +0100
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAJ_mLLorEBpZxUONUE8Lj1K+qrQMTJH0hA_X5Sq5bEL40kX1wQ@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
	<CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
	<CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>
	<CAJ_mLLorEBpZxUONUE8Lj1K+qrQMTJH0hA_X5Sq5bEL40kX1wQ@mail.gmail.com>
Message-ID: <CAJGQDw=wgQqn52RusBudUJNsFShGeLtmXZSu6XiQfy62gSuMrg@mail.gmail.com>

> Michal

That was my initial hope. Binding threads to a NUMA node, and crossing my
fingers that the objects instantiated by a thread would be allocated on its
home NUMA node. And stay there. So that if later the same thread reads the
same data it will read memory from its home NUMA node.

But that is not how it works. Even with NUMA options activated, the hotspot
JVM will move objects when they get promoted into the old generation,
removing them from their home NUMA node and copying them in some big shared
NUMA-oblivious memory area...


I agree that binding threads to NUMA nodes is actually an easy trick when
you use the JNA library. There is even an open source project by Peter
Lawrey that does it quite elegantly and cross platform.
https://github.com/peter-lawrey/Java-Thread-Affinity/

-Antoine





On 15 February 2013 16:24, Micha? Warecki <michal.warecki at gmail.com> wrote:

> Hi!
>
> I may be wrong but:
> If you know in which NUMA node the data are stored and which thread will
> read this data, you can use pthead_setaffinity_np() function.
> This will direct particular thread to particular CPU with faster access to
> particular NUMA node.
> That's not a Pandora box but you have to use JNI.
>
> Cheers,
> Micha?
>
>
> 2013/2/15 Antoine Chambille <ach at quartetfs.com>
>
>> Data is stored in columns, to maximize the performance of analytical
>> queries that commonly scan billions of rows but only for a subset of the
>> columns. We support a mix of primitive data and object oriented data ( some
>> columns look like double[], some other look like Object[] ).
>>
>> Using direct buffers would open a door to NUMA-Aware memory placement
>> (provided that the direct allocation itself can be made on the right node).
>> That's probably more a Pandora box than a door though ;) Anyway it implies
>> serializing data into byte arrays, and deserializing at each query. That's
>> a serious performance penalty for primitive data, and that's absolutely
>> prohibitive when you do that with plain objects, even Externizable ones.
>>
>> -Antoine
>>
>>
>> On 15 February 2013 11:35, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>>
>>> Just out of curiosity: would not DirectBuffers and managing the data
>>> yourself would be both easier and more efficient?
>>> Technically you can ship the data w/o even copying it straight to the
>>> sockets (or disks).
>>> I don't know how you store the data itself but I can think only of
>>> tuples i.e. Object[].
>>>
>>> Stanimir
>>>
>>> On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:
>>>
>>>> I think this community is the right place to start a conversation about
>>>> NUMA (aren't NUMA nodes to memory what multiprocessors are to processing?
>>>> ;). I apologize if this is considered off-topic.
>>>>
>>>>
>>>> We are developing a Java in-memory analytical database (it's called
>>>> "ActivePivot") that our customers deploy on ever larger datasets. Some
>>>> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
>>>> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
>>>> trend, and we are researching solutions to improve our performance on NUMA
>>>> configurations.
>>>>
>>>>
>>>> We understand that in the current state of things (and including JDK8)
>>>> the support for NUMA in hotspot is the following:
>>>> * The young generation heap layout can be NUMA-Aware (partitioned per
>>>> NUMA node, objects allocated in the same node than the running thread)
>>>> * The old generation heap layout is not optimized for NUMA (at best the
>>>> old generation is interleaved among nodes which at least makes memory
>>>> accesses somewhat uniform)
>>>> * The parallel garbage collector is NUMA optimized, the GC threads
>>>> focusing on objects in their node.
>>>>
>>>>
>>>> Yet activating -XX:+UseNUMA option has almost no impact on the
>>>> performance of our in-memory database. It is not surprising, the pattern
>>>> for a database is to load the data in the memory and then make queries on
>>>> it. The data goes and stays in the old generation, and it is read from
>>>> there by queries. Most memory accesses are in the old gen and most of those
>>>> are not local.
>>>>
>>>> I guess there is a reason hotspot does not yet optimize the old
>>>> generation for NUMA. It must be very difficult to do it in the general
>>>> case, when you have no idea what thread from what node will read data and
>>>> interleaving is. But for an in-memory database this is frustrating because
>>>> we know very well which threads will access which piece of data. At least
>>>> in ActivePivot data structures are partitioned, partitions are each
>>>> assigned a thread pool so the threads that allocated the data in a
>>>> partition are also the threads that perform sub-queries on that partition.
>>>> We are a few lines of code away from binding thread pools to NUMA nodes,
>>>> and if the garbage collector would leave objects promoted to the old
>>>> generation on their original NUMA node memory accesses would be close to
>>>> optimal.
>>>>
>>>> We have not been able to do that. But that being said I read an
>>>> inspiring 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth
>>>> that did experiment on NUMA layouts for the old generation. ("NUMA-aware
>>>> Java heaps for server applications"
>>>> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>>>>
>>>>
>>>> * Are there hidden or experimental hotspot options that allow
>>>> NUMA-Aware partitioning of the old generation?
>>>> * Do you know why there isn't much (visible, generally available)
>>>> research on NUMA optimizations for the old gen? Is the Java in-memory
>>>> database use case considered a rare one?
>>>> * Maybe we should experiment and even contribute new heap layouts to
>>>> the open-jdk project. Can some of you guys comment on the difficulty of
>>>> that?
>>>>
>>>>
>>>> Thanks for reading,
>>>>
>>>> --
>>>> Antoine CHAMBILLE
>>>> Director Research & Development
>>>> Quartet FS
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>>
>> --
>> Antoine CHAMBILLE
>> Director Research & Development
>> Quartet FS
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
Antoine CHAMBILLE
Director Research & Development
Quartet FS
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130215/2cc1a409/attachment-0001.html>

From howard.lovatt at gmail.com  Fri Feb 15 15:26:09 2013
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Sat, 16 Feb 2013 06:26:09 +1000
Subject: [concurrency-interest] Numerical Stream code
In-Reply-To: <CAEJX8oozMKoTiE_-rwF_d48cc9ySwK4RUqa8itO2OeFTivHkuQ@mail.gmail.com>
References: <CACR_FB41DwO6HBohqAMYdH3DMnUhS=PoqKZjb0tp-8WWw1qQtQ@mail.gmail.com>
	<511CF6C3.8000709@oracle.com> <511CF876.1000003@oracle.com>
	<511D093D.5010608@gmail.com> <511D1E24.7070700@oracle.com>
	<CAEJX8oozMKoTiE_-rwF_d48cc9ySwK4RUqa8itO2OeFTivHkuQ@mail.gmail.com>
Message-ID: <7F8A8604-06CA-44A3-A190-1F4C814D2164@gmail.com>

Hi,

Thanks for all the replies. This is largely a holding email. I am travelling with work and don't have my laptop. When get home I will post some more code. 

@Jin: I did warm up the code, but I do agree that benchmarks are tricky. As I said I was expecting some overhead but was surprised at how much. 

@Brian: The reason I factored t0 and tg0 out into methods is that they are common between the serial and parallel versions and I thought the code read better. I don't think it makes any difference, but I will check. 

@Others: To avoid writing over an old array I will have to allocate each time round the t loop. I will give this a try and see if it helps. The discussion about the parallel problems is interesting, but how come the serial version is so slow? Could a problem with the Stream code in general be the underlying problem with the parallel version?

Sent from my iPad

On 15/02/2013, at 3:48 AM, Stanimir Simeonoff <stanimir at riflexo.com> wrote:

> 
>> > Do element sizes matter (byte vs. short vs. int  vs. long)? 
>> 
>> I don't think so.  All of this assumes that the proper instruction is used.  For example, if 2 threads are writing to adjacent bytes, then the "mov" instruction has to only write the byte.  If the compiler, decides to read 32-bits, mask in the 8-bits and write 32-bits then the data will be corrupted. 
> JLS mandates no corruption for neighbor writes.
>  
>> I believe that HotSpot will only generate the write byte mov instruction.
> That would be the correct one. The case affects only boolean[]/byte[]/short[]/char[]  as simple primitive fields are always at least 32bits.
> 
> Stanimir
> 
>  
>> Nathan Reynolds | Architect | 602.333.9091
>> Oracle PSR Engineering | Server Technology
>> On 2/14/2013 8:56 AM, Peter Levart wrote:
>>> On 02/14/2013 03:45 PM, Brian Goetz wrote: 
>>>>> The parallel version is almost certainly suffering false cache line 
>>>>> sharing when adjacent tasks are writing to the shared arrays u0, etc. 
>>>>> Nothing to do with streams, just a standard parallelism gotcha.
>>>> Cure: don't write to shared arrays from parallel tasks.
>>> Hi, 
>>> 
>>> I would like to discuss this a little bit (hence the cc: concurrency-interest - the conversation can continue on this list only). 
>>> 
>>> Is it really important to avoid writing to shared arrays from multiple threads (of course without synchronization, not even volatile writes/reads) when indexes are not shared (each thread writes/reads it's own disjunct subset). 
>>> 
>>> Do element sizes matter (byte vs. short vs. int  vs. long)? 
>>> 
>>> I had a (false?) feeling that cache lines are not invalidated when writes are performed without fences. 
>>> 
>>> Also I don't know how short (byte, char) writes are combined into memory words on the hardware when they come from different cores and whether this is connected to any performance issues. 
>>> 
>>> Thanks, 
>>> 
>>> Peter 
>>> 
>>> _______________________________________________ 
>>> Concurrency-interest mailing list 
>>> Concurrency-interest at cs.oswego.edu 
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130216/6e1085e7/attachment.html>

From aleksey.shipilev at oracle.com  Sat Feb 16 05:42:42 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 16 Feb 2013 14:42:42 +0400
Subject: [concurrency-interest] Racy lazy initialization: blush test
Message-ID: <511F62A2.1020404@oracle.com>

Hi there,

I think it's time to have another argument about racy stuff! :)

I want to assign the grading for some of the new
java-concurrency-torture tests, and wanted to cross-check the reasoning.
Given this test [1], what are the plausible outcomes?

In pseudo-code:

   class X {
       int f;
   }

   X x = new X();

          T1      |     T2
   ---------------+---------------
   if (x.f == 0)  | if (x.f == 0)
       x.f = 1    |     x.f = 1
   r1 = x.f       | r2 = x.f

Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
(It appears there are valid executions under JMM, mostly because there
is the data race on x.f; Jeremy had a post [2] about the similar issue
before, and reaching to the same conclusion.)

In that spirit,

          T1        |      T2
   -----------------+---------------
   rX1 = x.f        | rX2 = x.f
   if (rX1 == 0) {  | if (rX2 == 0) {
       rX1 = 1      |     rX2 = 1
       x.f = rX1    |     x.f = rX2
   }                | }
   r1 = rX1         | r2 = rX2

...should always yield (r1, r2) = (1, 1).

Thanks,
Aleksey.

[1]
https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/causality/lazyinit/plain/IntLazyTest.java
[2] http://jeremymanson.blogspot.ru/2008/12/benign-data-races-in-java.html

From stanimir at riflexo.com  Sat Feb 16 07:05:36 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 16 Feb 2013 14:05:36 +0200
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <511F62A2.1020404@oracle.com>
References: <511F62A2.1020404@oracle.com>
Message-ID: <CAEJX8ooaWzHHEBKYiO6+8N+DwVzaH4_VeRLEd7zom6R9PAjM_w@mail.gmail.com>

There are valid transformations to result into (0,1)/(1, 0)  The 2nd test
obviously should be 1,1 always as x.f is practically ignored for the final
outcome .
Recently there was discussion about that stuff.

Stanimir

On Sat, Feb 16, 2013 at 12:42 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> Hi there,
>
> I think it's time to have another argument about racy stuff! :)
>
> I want to assign the grading for some of the new
> java-concurrency-torture tests, and wanted to cross-check the reasoning.
> Given this test [1], what are the plausible outcomes?
>
> In pseudo-code:
>
>    class X {
>        int f;
>    }
>
>    X x = new X();
>
>           T1      |     T2
>    ---------------+---------------
>    if (x.f == 0)  | if (x.f == 0)
>        x.f = 1    |     x.f = 1
>    r1 = x.f       | r2 = x.f
>
> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
> (It appears there are valid executions under JMM, mostly because there
> is the data race on x.f; Jeremy had a post [2] about the similar issue
> before, and reaching to the same conclusion.)
>
> In that spirit,
>
>           T1        |      T2
>    -----------------+---------------
>    rX1 = x.f        | rX2 = x.f
>    if (rX1 == 0) {  | if (rX2 == 0) {
>        rX1 = 1      |     rX2 = 1
>        x.f = rX1    |     x.f = rX2
>    }                | }
>    r1 = rX1         | r2 = rX2
>
> ...should always yield (r1, r2) = (1, 1).
>
> Thanks,
> Aleksey.
>
> [1]
>
> https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/java/org/openjdk/concurrent/torture/tests/causality/lazyinit/plain/IntLazyTest.java
> [2] http://jeremymanson.blogspot.ru/2008/12/benign-data-races-in-java.html
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130216/05cfab97/attachment.html>

From michal.warecki at gmail.com  Sat Feb 16 12:51:44 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Sat, 16 Feb 2013 18:51:44 +0100
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAJGQDw=wgQqn52RusBudUJNsFShGeLtmXZSu6XiQfy62gSuMrg@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
	<CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
	<CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>
	<CAJ_mLLorEBpZxUONUE8Lj1K+qrQMTJH0hA_X5Sq5bEL40kX1wQ@mail.gmail.com>
	<CAJGQDw=wgQqn52RusBudUJNsFShGeLtmXZSu6XiQfy62gSuMrg@mail.gmail.com>
Message-ID: <CAJ_mLLpD0CS3+Gu2s3LiSs3u7GFESAf3GvOuMszAu12FXuxGSw@mail.gmail.com>

Yes, of course. I didn't mean just a simple CPU binding. After evacuation
(from young to old) and compaction, threads have to be rebinded.
As you wrote, problem is with old gen heap and therefore JVM modifications
are needed. I don't know if you can deliver custom OpenJDK HotSpot.
Probably you have to experiment with NUMA-aware old gen and NUMA-aware
objects reordering during compaction (you have to take care also about CPU
cache and TLB). Which GC are you using? With CMS there is another issues
with fragmentation after a few collections. I believe, "bump the pointer"
allocation is better in this case.
If you have time on such issues in the work you're lucky :-)

These are just my thoughts, but I'm no expert.

Micha?

2013/2/15 Antoine Chambille <ach at quartetfs.com>

> > Michal
>
> That was my initial hope. Binding threads to a NUMA node, and crossing my
> fingers that the objects instantiated by a thread would be allocated on its
> home NUMA node. And stay there. So that if later the same thread reads the
> same data it will read memory from its home NUMA node.
>
> But that is not how it works. Even with NUMA options activated, the
> hotspot JVM will move objects when they get promoted into the old
> generation, removing them from their home NUMA node and copying them in
> some big shared NUMA-oblivious memory area...
>
>
> I agree that binding threads to NUMA nodes is actually an easy trick when
> you use the JNA library. There is even an open source project by Peter
> Lawrey that does it quite elegantly and cross platform.
> https://github.com/peter-lawrey/Java-Thread-Affinity/
>
> -Antoine
>
>
>
>
>
> On 15 February 2013 16:24, Micha? Warecki <michal.warecki at gmail.com>wrote:
>
>> Hi!
>>
>> I may be wrong but:
>> If you know in which NUMA node the data are stored and which thread will
>> read this data, you can use pthead_setaffinity_np() function.
>> This will direct particular thread to particular CPU with faster access
>> to particular NUMA node.
>> That's not a Pandora box but you have to use JNI.
>>
>> Cheers,
>> Micha?
>>
>>
>> 2013/2/15 Antoine Chambille <ach at quartetfs.com>
>>
>>> Data is stored in columns, to maximize the performance of analytical
>>> queries that commonly scan billions of rows but only for a subset of the
>>> columns. We support a mix of primitive data and object oriented data ( some
>>> columns look like double[], some other look like Object[] ).
>>>
>>> Using direct buffers would open a door to NUMA-Aware memory placement
>>> (provided that the direct allocation itself can be made on the right node).
>>> That's probably more a Pandora box than a door though ;) Anyway it implies
>>> serializing data into byte arrays, and deserializing at each query. That's
>>> a serious performance penalty for primitive data, and that's absolutely
>>> prohibitive when you do that with plain objects, even Externizable ones.
>>>
>>> -Antoine
>>>
>>>
>>> On 15 February 2013 11:35, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>>>
>>>> Just out of curiosity: would not DirectBuffers and managing the data
>>>> yourself would be both easier and more efficient?
>>>> Technically you can ship the data w/o even copying it straight to the
>>>> sockets (or disks).
>>>> I don't know how you store the data itself but I can think only of
>>>> tuples i.e. Object[].
>>>>
>>>> Stanimir
>>>>
>>>> On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:
>>>>
>>>>> I think this community is the right place to start a conversation
>>>>> about NUMA (aren't NUMA nodes to memory what multiprocessors are to
>>>>> processing? ;). I apologize if this is considered off-topic.
>>>>>
>>>>>
>>>>> We are developing a Java in-memory analytical database (it's called
>>>>> "ActivePivot") that our customers deploy on ever larger datasets. Some
>>>>> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
>>>>> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
>>>>> trend, and we are researching solutions to improve our performance on NUMA
>>>>> configurations.
>>>>>
>>>>>
>>>>> We understand that in the current state of things (and including JDK8)
>>>>> the support for NUMA in hotspot is the following:
>>>>> * The young generation heap layout can be NUMA-Aware (partitioned per
>>>>> NUMA node, objects allocated in the same node than the running thread)
>>>>> * The old generation heap layout is not optimized for NUMA (at best
>>>>> the old generation is interleaved among nodes which at least makes memory
>>>>> accesses somewhat uniform)
>>>>> * The parallel garbage collector is NUMA optimized, the GC threads
>>>>> focusing on objects in their node.
>>>>>
>>>>>
>>>>> Yet activating -XX:+UseNUMA option has almost no impact on the
>>>>> performance of our in-memory database. It is not surprising, the pattern
>>>>> for a database is to load the data in the memory and then make queries on
>>>>> it. The data goes and stays in the old generation, and it is read from
>>>>> there by queries. Most memory accesses are in the old gen and most of those
>>>>> are not local.
>>>>>
>>>>> I guess there is a reason hotspot does not yet optimize the old
>>>>> generation for NUMA. It must be very difficult to do it in the general
>>>>> case, when you have no idea what thread from what node will read data and
>>>>> interleaving is. But for an in-memory database this is frustrating because
>>>>> we know very well which threads will access which piece of data. At least
>>>>> in ActivePivot data structures are partitioned, partitions are each
>>>>> assigned a thread pool so the threads that allocated the data in a
>>>>> partition are also the threads that perform sub-queries on that partition.
>>>>> We are a few lines of code away from binding thread pools to NUMA nodes,
>>>>> and if the garbage collector would leave objects promoted to the old
>>>>> generation on their original NUMA node memory accesses would be close to
>>>>> optimal.
>>>>>
>>>>> We have not been able to do that. But that being said I read an
>>>>> inspiring 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth
>>>>> that did experiment on NUMA layouts for the old generation. ("NUMA-aware
>>>>> Java heaps for server applications"
>>>>> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>>>>>
>>>>>
>>>>> * Are there hidden or experimental hotspot options that allow
>>>>> NUMA-Aware partitioning of the old generation?
>>>>> * Do you know why there isn't much (visible, generally available)
>>>>> research on NUMA optimizations for the old gen? Is the Java in-memory
>>>>> database use case considered a rare one?
>>>>> * Maybe we should experiment and even contribute new heap layouts to
>>>>> the open-jdk project. Can some of you guys comment on the difficulty of
>>>>> that?
>>>>>
>>>>>
>>>>> Thanks for reading,
>>>>>
>>>>> --
>>>>> Antoine CHAMBILLE
>>>>> Director Research & Development
>>>>> Quartet FS
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>>
>>> --
>>> Antoine CHAMBILLE
>>> Director Research & Development
>>> Quartet FS
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
> --
> Antoine CHAMBILLE
> Director Research & Development
> Quartet FS
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130216/fbfdad05/attachment.html>

From stanimir at riflexo.com  Sat Feb 16 16:47:53 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Sat, 16 Feb 2013 23:47:53 +0200
Subject: [concurrency-interest] NUMA-Aware Java Heaps for in-memory
	databases
In-Reply-To: <CAJGQDw=QsCKksdt9cmkCSuOFHG+K-J8U+YcF2snzCCw3z=szuQ@mail.gmail.com>
References: <CAJGQDwk58GEaWUZBZandR2hJjw--K2ufEbyexGiOOYY4EiLkdw@mail.gmail.com>
	<CAEJX8oqOn56OTpu=GGNK9XwHeMraarq694N96V99ZmPhc7Epxw@mail.gmail.com>
	<CAJGQDwmQ09SUyJKb=_JnhjjN9o2XUe6kvseM9x=V+5HZQycwNQ@mail.gmail.com>
	<CAEJX8oqDtncMsy0juZVn-Cv02w8_n8XKKhDufEp0cvyGpKrcPA@mail.gmail.com>
	<CAJGQDw=QsCKksdt9cmkCSuOFHG+K-J8U+YcF2snzCCw3z=szuQ@mail.gmail.com>
Message-ID: <CAEJX8opqu8iqGc6fUqfxhYZpbH4PqaSmW=6RhWqnvYmCu9DJWQ@mail.gmail.com>

On Fri, Feb 15, 2013 at 5:48 PM, Antoine Chambille <ach at quartetfs.com>wrote:

> You are right, for primitive arrays I may be overestimating the overhead.
> Nevertheless I have no idea where (regarding NUMA layout) *
> UNSAFE.allocateMemory()* allocates the memory for direct byte buffers...
> I should find some time to write a prototype.
>
> Unsafe.allocareMemory calls os:malloc that's a thin wrapper around just
malloc. In explicit numa_ allocation the OS should(?) allocate the pages on
the 1st write touch, i.e. page fault. At least that should be the default
behavior.
Hence if you bind a few the threads to numa nodes and use them to allocate
and pre-touch the DirectBuffers you are good to go - the raw data should
stay unmoved..
Actually I thought in-mem DB and DirectBuffers should be a perfect match.



> But the serialization price is definitely to steep for objects. Of course
> the results of queries to the database must be serialized before sent to
> clients, but the size of a result cell set is incommensurate with the size
> of the data scanned to produce it. A typical analytical query on
> ActivePivot scans and aggregates one billion lines to produce 100 cells.
>
> If you use bound threads and split the dataset amongst the NUMA nodes via
the DirectBuffers and the query is split accordingly it should be possible
to have few cross-node loads.

Stanimir



> -Antoine
>
>
> On 15 February 2013 16:01, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>
>>
>>
>> On Fri, Feb 15, 2013 at 1:18 PM, Antoine Chambille <ach at quartetfs.com>wrote:
>>
>>> Data is stored in columns, to maximize the performance of analytical
>>> queries that commonly scan billions of rows but only for a subset of the
>>> columns. We support a mix of primitive data and object oriented data ( some
>>> columns look like double[], some other look like Object[] ).
>>>
>>> The point is to replace all primitive arrays (+String) w/ ByteBuffer (or
>> Double/LongBuffer), instead double[]  you  can use DoubleBuffer in native
>> byteorder( the latter is quite important). Also you use
>> ByteBuffer.asDoubleBuffer() and have individual access to each byte.
>>
>>
>> Using direct buffers would open a door to NUMA-Aware memory placement
>>> (provided that the direct allocation itself can be made on the right node).
>>> That's probably more a Pandora box than a door though ;) Anyway it implies
>>> serializing data into byte arrays, and deserializing at each query. That's
>>> a serious performance penalty for primitive data, and that's absolutely
>>> prohibitive when you do that with plain objects, even Externizable ones.
>>>
>> Unless the database runs in the process, there would be always need to
>> serialize java objects into network format. I assumed the database is
>> standalone and not embedded.
>>
>> Stanimir
>>
>>
>>
>>>
>>> On 15 February 2013 11:35, Stanimir Simeonoff <stanimir at riflexo.com>wrote:
>>>
>>>> Just out of curiosity: would not DirectBuffers and managing the data
>>>> yourself would be both easier and more efficient?
>>>> Technically you can ship the data w/o even copying it straight to the
>>>> sockets (or disks).
>>>> I don't know how you store the data itself but I can think only of
>>>> tuples i.e. Object[].
>>>>
>>>> Stanimir
>>>>
>>>> On Fri, Feb 15, 2013 at 11:48 AM, Antoine Chambille <ach at quartetfs.com>wrote:
>>>>
>>>>> I think this community is the right place to start a conversation
>>>>> about NUMA (aren't NUMA nodes to memory what multiprocessors are to
>>>>> processing? ;). I apologize if this is considered off-topic.
>>>>>
>>>>>
>>>>> We are developing a Java in-memory analytical database (it's called
>>>>> "ActivePivot") that our customers deploy on ever larger datasets. Some
>>>>> ActivePivot instances are deployed on java heaps close to 1TB, on NUMA
>>>>> servers (typically 4 Xeon processors and 4 NUMA nodes). This is becoming a
>>>>> trend, and we are researching solutions to improve our performance on NUMA
>>>>> configurations.
>>>>>
>>>>>
>>>>> We understand that in the current state of things (and including JDK8)
>>>>> the support for NUMA in hotspot is the following:
>>>>> * The young generation heap layout can be NUMA-Aware (partitioned per
>>>>> NUMA node, objects allocated in the same node than the running thread)
>>>>> * The old generation heap layout is not optimized for NUMA (at best
>>>>> the old generation is interleaved among nodes which at least makes memory
>>>>> accesses somewhat uniform)
>>>>> * The parallel garbage collector is NUMA optimized, the GC threads
>>>>> focusing on objects in their node.
>>>>>
>>>>>
>>>>> Yet activating -XX:+UseNUMA option has almost no impact on the
>>>>> performance of our in-memory database. It is not surprising, the pattern
>>>>> for a database is to load the data in the memory and then make queries on
>>>>> it. The data goes and stays in the old generation, and it is read from
>>>>> there by queries. Most memory accesses are in the old gen and most of those
>>>>> are not local.
>>>>>
>>>>> I guess there is a reason hotspot does not yet optimize the old
>>>>> generation for NUMA. It must be very difficult to do it in the general
>>>>> case, when you have no idea what thread from what node will read data and
>>>>> interleaving is. But for an in-memory database this is frustrating because
>>>>> we know very well which threads will access which piece of data. At least
>>>>> in ActivePivot data structures are partitioned, partitions are each
>>>>> assigned a thread pool so the threads that allocated the data in a
>>>>> partition are also the threads that perform sub-queries on that partition.
>>>>> We are a few lines of code away from binding thread pools to NUMA nodes,
>>>>> and if the garbage collector would leave objects promoted to the old
>>>>> generation on their original NUMA node memory accesses would be close to
>>>>> optimal.
>>>>>
>>>>> We have not been able to do that. But that being said I read an
>>>>> inspiring 2005 article from Mustafa M. Tikir and Jeffrey K. Hollingsworth
>>>>> that did experiment on NUMA layouts for the old generation. ("NUMA-aware
>>>>> Java heaps for server applications"
>>>>> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6587&rep=rep1&type=pdf). That motivated me to ask the following questions:
>>>>>
>>>>>
>>>>> * Are there hidden or experimental hotspot options that allow
>>>>> NUMA-Aware partitioning of the old generation?
>>>>> * Do you know why there isn't much (visible, generally available)
>>>>> research on NUMA optimizations for the old gen? Is the Java in-memory
>>>>> database use case considered a rare one?
>>>>> * Maybe we should experiment and even contribute new heap layouts to
>>>>> the open-jdk project. Can some of you guys comment on the difficulty of
>>>>> that?
>>>>>
>>>>>
>>>>> Thanks for reading,
>>>>>
>>>>> --
>>>>> Antoine CHAMBILLE
>>>>> Director Research & Development
>>>>> Quartet FS
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>>
>>> --
>>> Antoine CHAMBILLE
>>> Director Research & Development
>>> Quartet FS
>>>
>>
>>
>
>
> --
> Antoine CHAMBILLE
> Director Research & Development
> Quartet FS
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130216/a636e979/attachment-0001.html>

From fshen at avos.com  Sun Feb 17 04:44:40 2013
From: fshen at avos.com (Feng Shen)
Date: Sun, 17 Feb 2013 17:44:40 +0800
Subject: [concurrency-interest] Help on design a thread pool,
	some tasks need to executed in order
Message-ID: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>

Hello,

I am developing a HTTP Server with WebSocket support.  The thread model:
One IO thread to accept connection, read bytes, decoding request, hand the
request to a threadpool.

HTTP has no ordering requirement, But websocket has:  a client can sent
several messages to server, these messages should be handled in order.

So I need a thread pool with some ordering guarantee. For example:
T1,
T2, *
T3,
T4, *
T5

T4 can be executed only after T2 complete. But the other tasks has no
ordering requirement, thus can be executed in parallel.

Would appreciate for any thought about how to design such a thread pool?


- Feng
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130217/01d59911/attachment.html>

From aleksey.shipilev at oracle.com  Sun Feb 17 06:31:42 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sun, 17 Feb 2013 15:31:42 +0400
Subject: [concurrency-interest] Help on design a thread pool,
 some tasks need to executed in order
In-Reply-To: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
Message-ID: <5120BF9E.8010304@oracle.com>

On 02/17/2013 01:44 PM, Feng Shen wrote:
> Would appreciate for any thought about how to design such a thread pool?

http://www.javaspecialists.eu/archive/Issue206.html

-Aleksey.


From jeffhain at rocketmail.com  Sun Feb 17 16:35:59 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sun, 17 Feb 2013 21:35:59 +0000 (GMT)
Subject: [concurrency-interest] Help on design a thread pool,
	some tasks need to executed in order
In-Reply-To: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
Message-ID: <1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>

Hi.



Feng Shen wrote:
>Would appreciate for any thought about how to design such a thread pool?



It seems to me you don't need a particular thread pool to have some of your
tasks ordered, if you can keep a reference to last submitted runnable for each
ordered sequence of runnables (for example using a map in IO thread).


I'm thinking about using a specific "LinkingRunnable", containing
an "impl" Runnable and an AtomicReference to a "next" Runnable.


When LinkingRunnable.run() is called, it first runs impl.run(), and then
does CAS(null,this) (using "this" as a tombstone): if CAS fails, that means
someone added a runnable to run next, and it runs it.


When subsequent task arrives, you try to "enqueue" it doing CAS(null,next):
If CAS succeeds, that means it'll be executed just after previous task
(and in same thread), and if it fails that means that the previous task
already completed, so you can just submit your next task to the pool.


You can also test "get() == null" before doing CASes, which should
make them rare, and reduce the usual overhead to a volatile read.



-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130217/70c29bdf/attachment.html>

From viktor.klang at gmail.com  Sun Feb 17 17:21:09 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 17 Feb 2013 23:21:09 +0100
Subject: [concurrency-interest] Help on design a thread pool,
 some tasks need to executed in order
In-Reply-To: <1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
	<1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>
Message-ID: <CANPzfU8bOS9AaQKz_wS-PG7eGiXaFLc02Fz4JjA_8qB3S480aw@mail.gmail.com>

Here you have it:
https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/util/SerializedSuspendableExecutionContext.scala

If you're only on plain Java, search-and-replace ExecutionContext with
j.u.c.Executor

(It is also susspendable and resumable)

Cheers,
?


On Sun, Feb 17, 2013 at 10:35 PM, Jeff Hain <jeffhain at rocketmail.com> wrote:

> Hi.
>
>
>
>
> Feng Shen wrote:
> >Would appreciate for any thought about how to design such a thread pool?
>
>
>
> It seems to me you don't need a particular thread pool to have some of your
> tasks ordered, if you can keep a reference to last submitted runnable for
> each
> ordered sequence of runnables (for example using a map in IO thread).
>
>
> I'm thinking about using a specific "LinkingRunnable", containing
> an "impl" Runnable and an AtomicReference to a "next" Runnable.
>
>
> When LinkingRunnable.run() is called, it first runs impl.run(), and then
> does CAS(null,this) (using "this" as a tombstone): if CAS fails, that means
> someone added a runnable to run next, and it runs it.
>
>
> When subsequent task arrives, you try to "enqueue" it doing CAS(null,next):
> If CAS succeeds, that means it'll be executed just after previous task
> (and in same thread), and if it fails that means that the previous task
> already completed, so you can just submit your next task to the pool.
>
>
> You can also test "get() == null" before doing CASes, which should
> make them rare, and reduce the usual overhead to a volatile read.
>
>
>
> -Jeff
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130217/2d36bfa2/attachment.html>

From jeffhain at rocketmail.com  Sun Feb 17 17:42:42 2013
From: jeffhain at rocketmail.com (Jeff Hain)
Date: Sun, 17 Feb 2013 22:42:42 +0000 (GMT)
Subject: [concurrency-interest] Help on design a thread pool,
	some tasks need to executed in order
In-Reply-To: <CANPzfU8bOS9AaQKz_wS-PG7eGiXaFLc02Fz4JjA_8qB3S480aw@mail.gmail.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
	<1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>
	<CANPzfU8bOS9AaQKz_wS-PG7eGiXaFLc02Fz4JjA_8qB3S480aw@mail.gmail.com>
Message-ID: <1361140962.52667.YahooMailNeo@web132103.mail.ird.yahoo.com>



?iktor ?lang wrote:

>Here you have it:?https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/util/SerializedSuspendableExecutionContext.scala 




Handy.
I should read more of the nice code that's out there.

(and I also shoudn't have said that usual overhead could be reduced to a volatile read)



-Jeff
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130217/6521b8b2/attachment.html>

From viktor.klang at gmail.com  Sun Feb 17 17:57:38 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Sun, 17 Feb 2013 23:57:38 +0100
Subject: [concurrency-interest] Help on design a thread pool,
 some tasks need to executed in order
In-Reply-To: <1361140962.52667.YahooMailNeo@web132103.mail.ird.yahoo.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
	<1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>
	<CANPzfU8bOS9AaQKz_wS-PG7eGiXaFLc02Fz4JjA_8qB3S480aw@mail.gmail.com>
	<1361140962.52667.YahooMailNeo@web132103.mail.ird.yahoo.com>
Message-ID: <CANPzfU84jCzPOmOhiemuOCXQT3NhbrWr4DV7aq_9gqM48-qxQw@mail.gmail.com>

It's a really useful construct; I just used it to reimplement Akka Agents.

Cheers,
?


On Sun, Feb 17, 2013 at 11:42 PM, Jeff Hain <jeffhain at rocketmail.com> wrote:

>
> ?iktor ?lang wrote:
> >Here you have it:
> https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/util/SerializedSuspendableExecutionContext.scala
>
>
>
> Handy.
> I should read more of the nice code that's out there.
> (and I also shoudn't have said that usual overhead could be reduced to a
> volatile read)
>
>
>
> -Jeff
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130217/5fdb6ed8/attachment-0001.html>

From fshen at avos.com  Sun Feb 17 22:12:24 2013
From: fshen at avos.com (Feng Shen)
Date: Mon, 18 Feb 2013 11:12:24 +0800
Subject: [concurrency-interest] Help on design a thread pool,
 some tasks need to executed in order
In-Reply-To: <CANPzfU84jCzPOmOhiemuOCXQT3NhbrWr4DV7aq_9gqM48-qxQw@mail.gmail.com>
References: <CAO5geq2LON2WF6szCPZEUW9k8bvJZPbK16AtqbR_fTpkq89Qcg@mail.gmail.com>
	<1361136959.15425.YahooMailNeo@web132102.mail.ird.yahoo.com>
	<CANPzfU8bOS9AaQKz_wS-PG7eGiXaFLc02Fz4JjA_8qB3S480aw@mail.gmail.com>
	<1361140962.52667.YahooMailNeo@web132103.mail.ird.yahoo.com>
	<CANPzfU84jCzPOmOhiemuOCXQT3NhbrWr4DV7aq_9gqM48-qxQw@mail.gmail.com>
Message-ID: <CAO5geq0jMowQebogFPfPRD67w9fob_f5t84MqAY8DNb=pzR5Yw@mail.gmail.com>

The idea of LinkingRunnable is very clever!
Thanks




On Mon, Feb 18, 2013 at 6:57 AM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

> It's a really useful construct; I just used it to reimplement Akka Agents.
>
> Cheers,
> ?
>
>
> On Sun, Feb 17, 2013 at 11:42 PM, Jeff Hain <jeffhain at rocketmail.com>wrote:
>
>>
>> ?iktor ?lang wrote:
>> >Here you have it:
>> https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/util/SerializedSuspendableExecutionContext.scala
>>
>>
>>
>> Handy.
>> I should read more of the nice code that's out there.
>> (and I also shoudn't have said that usual overhead could be reduced to a
>> volatile read)
>>
>>
>>
>> -Jeff
>>
>>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @viktorklang
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/21a0cc5b/attachment.html>

From cit at cit.spb.ru  Mon Feb 18 05:39:46 2013
From: cit at cit.spb.ru (Dmitry Tsitelov)
Date: Mon, 18 Feb 2013 14:39:46 +0400
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <511F62A2.1020404@oracle.com>
Message-ID: <CD47EC31.354CB%cit@cit.spb.ru>


On 16.02.13 14:42 , "Aleksey Shipilev" <aleksey.shipilev at oracle.com> wrote:

>Hi there,
>
>I think it's time to have another argument about racy stuff! :)
>
>I want to assign the grading for some of the new
>java-concurrency-torture tests, and wanted to cross-check the reasoning.
>Given this test [1], what are the plausible outcomes?
>
>In pseudo-code:
>
>   class X {
>       int f;
>   }
>
>   X x = new X();
>
>          T1      |     T2
>   ---------------+---------------
>   if (x.f == 0)  | if (x.f == 0)
>       x.f = 1    |     x.f = 1
>   r1 = x.f       | r2 = x.f
>
>Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>(It appears there are valid executions under JMM, mostly because there
>is the data race on x.f; Jeremy had a post [2] about the similar issue
>before, and reaching to the same conclusion.)
>

Could someone point me to a sequentially-consistent execution of this
scenario emitting (1,0)/(0,1) result, please?

>
>[1]
>https://github.com/shipilev/java-concurrency-torture/blob/master/src/main/
>java/org/openjdk/concurrent/torture/tests/causality/lazyinit/plain/IntLazy
>Test.java
>[2] http://jeremymanson.blogspot.ru/2008/12/benign-data-races-in-java.html



From aleksey.shipilev at oracle.com  Mon Feb 18 05:48:06 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Mon, 18 Feb 2013 14:48:06 +0400
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <CD47EC31.354CB%cit@cit.spb.ru>
References: <CD47EC31.354CB%cit@cit.spb.ru>
Message-ID: <512206E6.9040407@oracle.com>

On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>   class X {
>>       int f;
>>   }
>>
>>   X x = new X();
>>
>>          T1      |     T2
>>   ---------------+---------------
>>   if (x.f == 0)  | if (x.f == 0)
>>       x.f = 1    |     x.f = 1
>>   r1 = x.f       | r2 = x.f
>>
>> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> (It appears there are valid executions under JMM, mostly because there
>> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> before, and reaching to the same conclusion.)
>>
> 
> Could someone point me to a sequentially-consistent execution of this
> scenario emitting (1,0)/(0,1) result, please?

I'm afraid there are no SC executions that yield this result. We have
the data race on x.f, DRF guarantee is out of the window, it's useless
to seek SC execution to justify this behavior.

-Aleksey.


From nitsanw at yahoo.com  Mon Feb 18 09:32:13 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Mon, 18 Feb 2013 06:32:13 -0800 (PST)
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <512206E6.9040407@oracle.com>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
Message-ID: <1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>

My understanding is that the program is open to SC equivalent interpretation given no hints are given with regards to ordering/concurrency.
Consider the code:
r2 = x.f;
if(x.f == 0){
x.f = 1;
r2 = 1;
}
Is SC equivalent to the original. The fact that x.f is written to concurrently is not conveyed to the compiler, so as far as it's concerned it's fine. Why would the above interpretation be a good idea I'm not sure, but I think it's allowed.

________________________________
 From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
To: Dmitry Tsitelov <cit at cit.spb.ru> 
Cc: "Concurrency-interest at cs.oswego.edu" <Concurrency-interest at cs.oswego.edu> 
Sent: Monday, February 18, 2013 10:48 AM
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test
 
On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>?  class X {
>>? ? ?  int f;
>>?  }
>>
>>?  X x = new X();
>>
>>? ? ? ? ? T1? ? ? |? ?  T2
>>?  ---------------+---------------
>>?  if (x.f == 0)? | if (x.f == 0)
>>? ? ?  x.f = 1? ? |? ?  x.f = 1
>>?  r1 = x.f? ? ?  | r2 = x.f
>>
>> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> (It appears there are valid executions under JMM, mostly because there
>> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> before, and reaching to the same conclusion.)
>>
> 
> Could someone point me to a sequentially-consistent execution of this
> scenario emitting (1,0)/(0,1) result, please?

I'm afraid there are no SC executions that yield this result. We have
the data race on x.f, DRF guarantee is out of the window, it's useless
to seek SC execution to justify this behavior.

-Aleksey.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/ff56a0f6/attachment.html>

From cit at cit.spb.ru  Mon Feb 18 12:01:23 2013
From: cit at cit.spb.ru (Dmitry Tsitelov)
Date: Mon, 18 Feb 2013 21:01:23 +0400
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <CD4846C5.3553D%cit@cit.spb.ru>

Thank you, I've missed this kind of transformation.

From:  Nitsan Wakart <nitsanw at yahoo.com>
Reply-To:  Nitsan Wakart <nitsanw at yahoo.com>
Date:  Monday, February 18, 2013 18:32
To:  Aleksey Shipilev <aleksey.shipilev at oracle.com>, Dmitry Tsitelov
<cit at cit.spb.ru>
Cc:  "Concurrency-interest at cs.oswego.edu"
<Concurrency-interest at cs.oswego.edu>
Subject:  Re: [concurrency-interest] Racy lazy initialization: blush test

My understanding is that the program is open to SC equivalent interpretation
given no hints are given with regards to ordering/concurrency.
Consider the code:
r2 = x.f;
if(x.f == 0){
x.f = 1;
r2 = 1;
}
Is SC equivalent to the original. The fact that x.f is written to
concurrently is not conveyed to the compiler, so as far as it's concerned
it's fine. Why would the above interpretation be a good idea I'm not sure,
but I think it's allowed.
  
 
 
  

  From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
 To: Dmitry Tsitelov <cit at cit.spb.ru>
Cc: "Concurrency-interest at cs.oswego.edu"
<Concurrency-interest at cs.oswego.edu>
 Sent: Monday, February 18, 2013 10:48 AM
 Subject: Re: [concurrency-interest] Racy lazy initialization: blush test
  
 
On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>   class X {
>>       int f;
>>   }
>>
>>   X x = new X();
>>
>>          T1      |     T2
>>   ---------------+---------------
>>   if (x.f == 0)  | if (x.f == 0)
>>       x.f = 1    |     x.f = 1
>>   r1 = x.f       | r2 = x.f
>>
>> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> (It appears there are valid executions under JMM, mostly because there
>> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> before, and reaching to the same conclusion.)
>>
> 
> Could someone point me to a sequentially-consistent execution of this
> scenario emitting (1,0)/(0,1) result, please?

I'm afraid there are no SC executions that yield this result. We have
the data race on x.f, DRF guarantee is out of the window, it's useless
to seek SC execution to justify this behavior.

-Aleksey.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


 
 
  


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/f8111a80/attachment.html>

From hans.boehm at hp.com  Mon Feb 18 12:32:35 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 18 Feb 2013 17:32:35 +0000
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>

I think some things got confused here, including the rules for different languages.

The basic rules are:

In basically all modern languages: IF THERE ARE NO DATA RACES and no uses of atomics/volatiles, you get sequential consistency.

In Java, C, C++: If there are no data races, and accesses to atomics/volatiles don't say otherwise, you still get sequential consistency.  (This doesn't hold in C# or OpenMP, for example.)

In the presence of data races, these languages instead provide really weak or no guarantees.  The examples under discussion here have data races, so none of this applies.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitsan Wakart
Sent: Monday, February 18, 2013 6:32 AM
To: Aleksey Shipilev; Dmitry Tsitelov
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test

My understanding is that the program is open to SC equivalent interpretation given no hints are given with regards to ordering/concurrency.
Consider the code:
r2 = x.f;
if(x.f == 0){
          x.f = 1;
          r2 = 1;
}
Is SC equivalent to the original. The fact that x.f is written to concurrently is not conveyed to the compiler, so as far as it's concerned it's fine. Why would the above interpretation be a good idea I'm not sure, but I think it's allowed.
________________________________
From: Aleksey Shipilev <aleksey.shipilev at oracle.com<mailto:aleksey.shipilev at oracle.com>>
To: Dmitry Tsitelov <cit at cit.spb.ru<mailto:cit at cit.spb.ru>>
Cc: "Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>" <Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>>
Sent: Monday, February 18, 2013 10:48 AM
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test

On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>  class X {
>>      int f;
>>  }
>>
>>  X x = new X();
>>
>>          T1      |    T2
>>  ---------------+---------------
>>  if (x.f == 0)  | if (x.f == 0)
>>      x.f = 1    |    x.f = 1
>>  r1 = x.f      | r2 = x.f
>>
>> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> (It appears there are valid executions under JMM, mostly because there
>> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> before, and reaching to the same conclusion.)
>>
>
> Could someone point me to a sequentially-consistent execution of this
> scenario emitting (1,0)/(0,1) result, please?

I'm afraid there are no SC executions that yield this result. We have
the data race on x.f, DRF guarantee is out of the window, it's useless
to seek SC execution to justify this behavior.

-Aleksey.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/9f473de5/attachment.html>

From hans.boehm at hp.com  Mon Feb 18 14:59:48 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Mon, 18 Feb 2013 19:59:48 +0000
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>

On rereading, I think I misunderstood the context here. When Nitsan said "SC equivalent", I think he meant "sequentially equivalent", in which case I now understand the original posting.  Sorry about that.

I agree that this is a valid transformation.

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Boehm, Hans
Sent: Monday, February 18, 2013 9:33 AM
To: Nitsan Wakart; Aleksey Shipilev; Dmitry Tsitelov
Cc: Concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test

I think some things got confused here, including the rules for different languages.

The basic rules are:

In basically all modern languages: IF THERE ARE NO DATA RACES and no uses of atomics/volatiles, you get sequential consistency.

In Java, C, C++: If there are no data races, and accesses to atomics/volatiles don't say otherwise, you still get sequential consistency.  (This doesn't hold in C# or OpenMP, for example.)

In the presence of data races, these languages instead provide really weak or no guarantees.  The examples under discussion here have data races, so none of this applies.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitsan Wakart
Sent: Monday, February 18, 2013 6:32 AM
To: Aleksey Shipilev; Dmitry Tsitelov
Cc: Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test

My understanding is that the program is open to SC equivalent interpretation given no hints are given with regards to ordering/concurrency.
Consider the code:
r2 = x.f;
if(x.f == 0){
          x.f = 1;
          r2 = 1;
}
Is SC equivalent to the original. The fact that x.f is written to concurrently is not conveyed to the compiler, so as far as it's concerned it's fine. Why would the above interpretation be a good idea I'm not sure, but I think it's allowed.
________________________________
From: Aleksey Shipilev <aleksey.shipilev at oracle.com<mailto:aleksey.shipilev at oracle.com>>
To: Dmitry Tsitelov <cit at cit.spb.ru<mailto:cit at cit.spb.ru>>
Cc: "Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>" <Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>>
Sent: Monday, February 18, 2013 10:48 AM
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test

On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>  class X {
>>      int f;
>>  }
>>
>>  X x = new X();
>>
>>          T1      |    T2
>>  ---------------+---------------
>>  if (x.f == 0)  | if (x.f == 0)
>>      x.f = 1    |    x.f = 1
>>  r1 = x.f      | r2 = x.f
>>
>> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> (It appears there are valid executions under JMM, mostly because there
>> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> before, and reaching to the same conclusion.)
>>
>
> Could someone point me to a sequentially-consistent execution of this
> scenario emitting (1,0)/(0,1) result, please?

I'm afraid there are no SC executions that yield this result. We have
the data race on x.f, DRF guarantee is out of the window, it's useless
to seek SC execution to justify this behavior.

-Aleksey.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/bedeba8f/attachment-0001.html>

From tnachen at gmail.com  Mon Feb 18 16:39:32 2013
From: tnachen at gmail.com (Timothy Chen)
Date: Mon, 18 Feb 2013 13:39:32 -0800
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
Message-ID: <CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>

Just curious, you said C# doesn't give you sequential consistency in no
data races and no atomics/volatiles usage, then what does C# do?

Is the behavior documented somewhere?

Thanks,

Tim



On Mon, Feb 18, 2013 at 11:59 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  On rereading, I think I misunderstood the context here. When Nitsan said
> ?SC equivalent?, I think he meant ?sequentially equivalent?, in which case
> I now understand the original posting.  Sorry about that.****
>
> ** **
>
> I agree that this is a valid transformation.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Boehm, Hans
> *Sent:* Monday, February 18, 2013 9:33 AM
> *To:* Nitsan Wakart; Aleksey Shipilev; Dmitry Tsitelov
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush test
> ****
>
> ** **
>
> I think some things got confused here, including the rules for different
> languages.****
>
> ** **
>
> The basic rules are:****
>
> ** **
>
> In basically all modern languages: IF THERE ARE NO DATA RACES and no uses
> of atomics/volatiles, you get sequential consistency.****
>
> ** **
>
> In Java, C, C++: If there are no data races, and accesses to
> atomics/volatiles don?t say otherwise, you still get sequential
> consistency.  (This doesn?t hold in C# or OpenMP, for example.)****
>
> ** **
>
> In the presence of data races, these languages instead provide really weak
> or no guarantees.  The examples under discussion here have data races, so
> none of this applies.****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [
> mailto:concurrency-interest-bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]
> *On Behalf Of *Nitsan Wakart
> *Sent:* Monday, February 18, 2013 6:32 AM
> *To:* Aleksey Shipilev; Dmitry Tsitelov
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush test
> ****
>
> ** **
>
> My understanding is that the program is open to SC equivalent
> interpretation given no hints are given with regards to
> ordering/concurrency.****
>
> Consider the code:****
>
> r2 = x.f;****
>
> if(x.f == 0){****
>
>           x.f = 1;****
>
>           r2 = 1;****
>
> }****
>
> Is SC equivalent to the original. The fact that x.f is written to
> concurrently is not conveyed to the compiler, so as far as it's concerned
> it's fine. Why would the above interpretation be a good idea I'm not sure,
> but I think it's allowed.****
>    ------------------------------
>
> *From:* Aleksey Shipilev <aleksey.shipilev at oracle.com>
> *To:* Dmitry Tsitelov <cit at cit.spb.ru>
> *Cc:* "Concurrency-interest at cs.oswego.edu" <
> Concurrency-interest at cs.oswego.edu>
> *Sent:* Monday, February 18, 2013 10:48 AM
> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush test
> ****
>
>
> On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
> >>  class X {
> >>      int f;
> >>  }
> >>
> >>  X x = new X();
> >>
> >>          T1      |    T2
> >>  ---------------+---------------
> >>  if (x.f == 0)  | if (x.f == 0)
> >>      x.f = 1    |    x.f = 1
> >>  r1 = x.f      | r2 = x.f
> >>
> >> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
> >> (It appears there are valid executions under JMM, mostly because there
> >> is the data race on x.f; Jeremy had a post [2] about the similar issue
> >> before, and reaching to the same conclusion.)
> >>
> >
> > Could someone point me to a sequentially-consistent execution of this
> > scenario emitting (1,0)/(0,1) result, please?
>
> I'm afraid there are no SC executions that yield this result. We have
> the data race on x.f, DRF guarantee is out of the window, it's useless
> to seek SC execution to justify this behavior.
>
> -Aleksey.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130218/87966817/attachment.html>

From dmitry.zaslavsky at gmail.com  Wed Feb 20 15:51:16 2013
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Wed, 20 Feb 2013 15:51:16 -0500
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
	<CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
Message-ID: <023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>

My understanding of what Hans is saying:
 
1. No races, no atomics = sc
I am not even adding anything to what Hans said
 
2. No races but volatiles (Java) or atomics (c++) you get SC.
 
And this is where C# differs or at leastthings become interesting.
In Dekkers algorithm even if you make the2 variables volatile you still have a broken code in c#.
In .net volatile read has acquire semantics as in no reads pass it. Using language from
http://g.oswego.edu/dl/jmm/cookbook.htmlon StoreLoad java would do a fence of sorts on x86 and .net (at least v4) doesn?t.
 
From I.12.6.7 of the .net spec volatile read has ?acquire semantics? meaning that the read is guaranteed to occur prior to any
references to memory that occur after the read instruction in the CIL instruction sequence. A
volatile write has ?release semantics? meaning that the write is guaranteed to happen after any
memory references prior to the write instruction in the CIL instruction sequence.
 
What that rule allows is for things to move inside of the acquire/release pair (from above acquire and from below release).
.NET spec is silent on the following issue: can acquire move above release (2 pairs to cross, or exactly what happens in Dekker?s algorithm)
Java as you can see from the reference above on StoreLoad (the case here) requires fences and don?t allow for such reordering.
 
 
What?s a bit strange in .net is thatThread.ReadVolatile fixes the problem even though the documentation says that it does exactly reading volatile field would do. But that could be extra implementation detail of ReadVolatile.

Sent from mobile device

On Feb 18, 2013, at 4:39 PM, Timothy Chen <tnachen at gmail.com> wrote:

> Just curious, you said C# doesn't give you sequential consistency in no data races and no atomics/volatiles usage, then what does C# do?
> 
> Is the behavior documented somewhere?
> 
> Thanks,
> 
> Tim
> 
> 
> 
> On Mon, Feb 18, 2013 at 11:59 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>> On rereading, I think I misunderstood the context here. When Nitsan said ?SC equivalent?, I think he meant ?sequentially equivalent?, in which case I now understand the original posting.  Sorry about that.
>> 
>>  
>> 
>> I agree that this is a valid transformation.
>> 
>>  
>> 
>> Hans
>> 
>>  
>> 
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Boehm, Hans
>> Sent: Monday, February 18, 2013 9:33 AM
>> To: Nitsan Wakart; Aleksey Shipilev; Dmitry Tsitelov
>> Cc: Concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Racy lazy initialization: blush test
>> 
>>  
>> 
>> I think some things got confused here, including the rules for different languages.
>> 
>>  
>> 
>> The basic rules are:
>> 
>>  
>> 
>> In basically all modern languages: IF THERE ARE NO DATA RACES and no uses of atomics/volatiles, you get sequential consistency.
>> 
>>  
>> 
>> In Java, C, C++: If there are no data races, and accesses to atomics/volatiles don?t say otherwise, you still get sequential consistency.  (This doesn?t hold in C# or OpenMP, for example.)
>> 
>>  
>> 
>> In the presence of data races, these languages instead provide really weak or no guarantees.  The examples under discussion here have data races, so none of this applies.
>> 
>>  
>> 
>> Hans
>> 
>>  
>> 
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitsan Wakart
>> Sent: Monday, February 18, 2013 6:32 AM
>> To: Aleksey Shipilev; Dmitry Tsitelov
>> Cc: Concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] Racy lazy initialization: blush test
>> 
>>  
>> 
>> My understanding is that the program is open to SC equivalent interpretation given no hints are given with regards to ordering/concurrency.
>> 
>> Consider the code:
>> 
>> r2 = x.f;
>> 
>> if(x.f == 0){
>> 
>>           x.f = 1;
>> 
>>           r2 = 1;
>> 
>> }
>> 
>> Is SC equivalent to the original. The fact that x.f is written to concurrently is not conveyed to the compiler, so as far as it's concerned it's fine. Why would the above interpretation be a good idea I'm not sure, but I think it's allowed.
>> 
>> From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
>> To: Dmitry Tsitelov <cit at cit.spb.ru> 
>> Cc: "Concurrency-interest at cs.oswego.edu" <Concurrency-interest at cs.oswego.edu> 
>> Sent: Monday, February 18, 2013 10:48 AM
>> Subject: Re: [concurrency-interest] Racy lazy initialization: blush test
>> 
>> 
>> On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>> >>  class X {
>> >>      int f;
>> >>  }
>> >>
>> >>  X x = new X();
>> >>
>> >>          T1      |    T2
>> >>  ---------------+---------------
>> >>  if (x.f == 0)  | if (x.f == 0)
>> >>      x.f = 1    |    x.f = 1
>> >>  r1 = x.f      | r2 = x.f
>> >>
>> >> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> >> (It appears there are valid executions under JMM, mostly because there
>> >> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> >> before, and reaching to the same conclusion.)
>> >>
>> > 
>> > Could someone point me to a sequentially-consistent execution of this
>> > scenario emitting (1,0)/(0,1) result, please?
>> 
>> I'm afraid there are no SC executions that yield this result. We have
>> the data race on x.f, DRF guarantee is out of the window, it's useless
>> to seek SC execution to justify this behavior.
>> 
>> -Aleksey.
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130220/6adda25d/attachment.html>

From vitalyd at gmail.com  Wed Feb 20 18:07:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 20 Feb 2013 18:07:13 -0500
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
	<CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
	<023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>
Message-ID: <CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>

.net docs are clearly wrong.  Thread.VolatileRead() invokes
Thread.MemoryBarrier() after the read, thus preventing the reordering.  In
turn, Thread.VolatileWrite() issues MemoryBarrier() before the write,
preventing reordering in the other direction.  For some reason, they don't
mention this important bit.

Write to a volatile on x86/64 in .net means nothing in terms of codegen -
it's a plain mov.  In java, there's a fenced instruction (lock add rsp 0 or
the like) for volatile writes, and that's where the "disconnect" is.

Sent from my phone
On Feb 20, 2013 3:57 PM, "Dmitry Zaslavsky" <dmitry.zaslavsky at gmail.com>
wrote:

> My understanding of what Hans is saying:****
>
>
>
> 1. No races, no atomics = sc****
>
> I am not even adding anything to what Hans said****
>
>
>
> 2. No races but volatiles (Java) or atomics (c++) you get SC.****
>
>
>
> And this is where C# differs or at leastthings become interesting.****
>
> In Dekkers algorithm even if you make the2 variables volatile you still
> have a broken code in c#.****
>
> In .net volatile read has acquire semantics as in no reads pass it. Using
> language from****
>
> http://g.oswego.edu/dl/jmm/cookbook.htmlon StoreLoad java would do a
> fence of sorts on x86 and .net (at least v4) doesn?t.****
>
>
>
> From I.12.6.7 of the .net spec volatile read has ?acquire semantics?
> meaning that the read is guaranteed to occur *prior* to any****
>
> references to memory that occur after the read instruction in the CIL
> instruction sequence. A****
>
> volatile write has ?release semantics? meaning that the write is
> guaranteed to happen after any****
>
> memory references *prior* to the write instruction in the CIL instruction
> sequence.****
>
>
>
> What that rule allows is for things to move inside of the acquire/release
> pair (from above acquire and from below release).****
>
> .NET spec is silent on the following issue: can acquire move above release
> (2 pairs to cross, or exactly what happens in Dekker?s algorithm)****
>
> Java as you can see from the reference above on StoreLoad (the case here)
> requires fences and don?t allow for such reordering.****
>
>
>
>
>
> What?s a bit strange in .net is thatThread.ReadVolatile fixes the problem
> even though the documentation says that it does exactly reading volatile
> field would do. But that could be extra implementation detail of
> ReadVolatile.
>
> Sent from mobile device
>
> On Feb 18, 2013, at 4:39 PM, Timothy Chen <tnachen at gmail.com> wrote:
>
> Just curious, you said C# doesn't give you sequential consistency in no
> data races and no atomics/volatiles usage, then what does C# do?
>
> Is the behavior documented somewhere?
>
> Thanks,
>
> Tim
>
>
>
> On Mon, Feb 18, 2013 at 11:59 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>
>>  On rereading, I think I misunderstood the context here. When Nitsan
>> said ?SC equivalent?, I think he meant ?sequentially equivalent?, in which
>> case I now understand the original posting.  Sorry about that.****
>>
>> ** **
>>
>> I agree that this is a valid transformation.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Boehm, Hans
>> *Sent:* Monday, February 18, 2013 9:33 AM
>> *To:* Nitsan Wakart; Aleksey Shipilev; Dmitry Tsitelov
>> *Cc:* Concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>> test****
>>
>> ** **
>>
>> I think some things got confused here, including the rules for different
>> languages.****
>>
>> ** **
>>
>> The basic rules are:****
>>
>> ** **
>>
>> In basically all modern languages: IF THERE ARE NO DATA RACES and no uses
>> of atomics/volatiles, you get sequential consistency.****
>>
>> ** **
>>
>> In Java, C, C++: If there are no data races, and accesses to
>> atomics/volatiles don?t say otherwise, you still get sequential
>> consistency.  (This doesn?t hold in C# or OpenMP, for example.)****
>>
>> ** **
>>
>> In the presence of data races, these languages instead provide really
>> weak or no guarantees.  The examples under discussion here have data races,
>> so none of this applies.****
>>
>> ** **
>>
>> Hans****
>>
>> ** **
>>
>> *From:* concurrency-interest-bounces at cs.oswego.edu [
>> mailto:concurrency-interest-bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]
>> *On Behalf Of *Nitsan Wakart
>> *Sent:* Monday, February 18, 2013 6:32 AM
>> *To:* Aleksey Shipilev; Dmitry Tsitelov
>> *Cc:* Concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>> test****
>>
>> ** **
>>
>> My understanding is that the program is open to SC equivalent
>> interpretation given no hints are given with regards to
>> ordering/concurrency.****
>>
>> Consider the code:****
>>
>> r2 = x.f;****
>>
>> if(x.f == 0){****
>>
>>           x.f = 1;****
>>
>>           r2 = 1;****
>>
>> }****
>>
>> Is SC equivalent to the original. The fact that x.f is written to
>> concurrently is not conveyed to the compiler, so as far as it's concerned
>> it's fine. Why would the above interpretation be a good idea I'm not sure,
>> but I think it's allowed.****
>>    ------------------------------
>>
>> *From:* Aleksey Shipilev <aleksey.shipilev at oracle.com>
>> *To:* Dmitry Tsitelov <cit at cit.spb.ru>
>> *Cc:* "Concurrency-interest at cs.oswego.edu" <
>> Concurrency-interest at cs.oswego.edu>
>> *Sent:* Monday, February 18, 2013 10:48 AM
>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>> test****
>>
>>
>> On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>> >>  class X {
>> >>      int f;
>> >>  }
>> >>
>> >>  X x = new X();
>> >>
>> >>          T1      |    T2
>> >>  ---------------+---------------
>> >>  if (x.f == 0)  | if (x.f == 0)
>> >>      x.f = 1    |    x.f = 1
>> >>  r1 = x.f      | r2 = x.f
>> >>
>> >> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>> >> (It appears there are valid executions under JMM, mostly because there
>> >> is the data race on x.f; Jeremy had a post [2] about the similar issue
>> >> before, and reaching to the same conclusion.)
>> >>
>> >
>> > Could someone point me to a sequentially-consistent execution of this
>> > scenario emitting (1,0)/(0,1) result, please?
>>
>> I'm afraid there are no SC executions that yield this result. We have
>> the data race on x.f, DRF guarantee is out of the window, it's useless
>> to seek SC execution to justify this behavior.
>>
>> -Aleksey.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130220/ffdce8f7/attachment-0001.html>

From vitalyd at gmail.com  Wed Feb 20 18:10:19 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 20 Feb 2013 18:10:19 -0500
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
	<CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
	<023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>
	<CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>
Message-ID: <CAHjP37Hj3n-u=hL1u=tipdxbenGSB1oV7Mgs9b1HMpg-z4HkUQ@mail.gmail.com>

AtomicXXX.lazySet is basically what .net does for volatile writes - that's
the closest parallel.

Sent from my phone
On Feb 20, 2013 6:07 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> .net docs are clearly wrong.  Thread.VolatileRead() invokes
> Thread.MemoryBarrier() after the read, thus preventing the reordering.  In
> turn, Thread.VolatileWrite() issues MemoryBarrier() before the write,
> preventing reordering in the other direction.  For some reason, they don't
> mention this important bit.
>
> Write to a volatile on x86/64 in .net means nothing in terms of codegen -
> it's a plain mov.  In java, there's a fenced instruction (lock add rsp 0 or
> the like) for volatile writes, and that's where the "disconnect" is.
>
> Sent from my phone
> On Feb 20, 2013 3:57 PM, "Dmitry Zaslavsky" <dmitry.zaslavsky at gmail.com>
> wrote:
>
>> My understanding of what Hans is saying:****
>>
>>
>>
>> 1. No races, no atomics = sc****
>>
>> I am not even adding anything to what Hans said****
>>
>>
>>
>> 2. No races but volatiles (Java) or atomics (c++) you get SC.****
>>
>>
>>
>> And this is where C# differs or at leastthings become interesting.****
>>
>> In Dekkers algorithm even if you make the2 variables volatile you still
>> have a broken code in c#.****
>>
>> In .net volatile read has acquire semantics as in no reads pass it. Using
>> language from****
>>
>> http://g.oswego.edu/dl/jmm/cookbook.htmlon StoreLoad java would do a
>> fence of sorts on x86 and .net (at least v4) doesn?t.****
>>
>>
>>
>> From I.12.6.7 of the .net spec volatile read has ?acquire semantics?
>> meaning that the read is guaranteed to occur *prior* to any****
>>
>> references to memory that occur after the read instruction in the CIL
>> instruction sequence. A****
>>
>> volatile write has ?release semantics? meaning that the write is
>> guaranteed to happen after any****
>>
>> memory references *prior* to the write instruction in the CIL
>> instruction sequence.****
>>
>>
>>
>> What that rule allows is for things to move inside of the acquire/release
>> pair (from above acquire and from below release).****
>>
>> .NET spec is silent on the following issue: can acquire move above
>> release (2 pairs to cross, or exactly what happens in Dekker?s algorithm)
>> ****
>>
>> Java as you can see from the reference above on StoreLoad (the case here)
>> requires fences and don?t allow for such reordering.****
>>
>>
>>
>>
>>
>> What?s a bit strange in .net is thatThread.ReadVolatile fixes the problem
>> even though the documentation says that it does exactly reading volatile
>> field would do. But that could be extra implementation detail of
>> ReadVolatile.
>>
>> Sent from mobile device
>>
>> On Feb 18, 2013, at 4:39 PM, Timothy Chen <tnachen at gmail.com> wrote:
>>
>> Just curious, you said C# doesn't give you sequential consistency in no
>> data races and no atomics/volatiles usage, then what does C# do?
>>
>> Is the behavior documented somewhere?
>>
>> Thanks,
>>
>> Tim
>>
>>
>>
>> On Mon, Feb 18, 2013 at 11:59 AM, Boehm, Hans <hans.boehm at hp.com> wrote:
>>
>>>  On rereading, I think I misunderstood the context here. When Nitsan
>>> said ?SC equivalent?, I think he meant ?sequentially equivalent?, in which
>>> case I now understand the original posting.  Sorry about that.****
>>>
>>> ** **
>>>
>>> I agree that this is a valid transformation.****
>>>
>>> ** **
>>>
>>> Hans****
>>>
>>> ** **
>>>
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Boehm, Hans
>>> *Sent:* Monday, February 18, 2013 9:33 AM
>>> *To:* Nitsan Wakart; Aleksey Shipilev; Dmitry Tsitelov
>>> *Cc:* Concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>>> test****
>>>
>>> ** **
>>>
>>> I think some things got confused here, including the rules for different
>>> languages.****
>>>
>>> ** **
>>>
>>> The basic rules are:****
>>>
>>> ** **
>>>
>>> In basically all modern languages: IF THERE ARE NO DATA RACES and no
>>> uses of atomics/volatiles, you get sequential consistency.****
>>>
>>> ** **
>>>
>>> In Java, C, C++: If there are no data races, and accesses to
>>> atomics/volatiles don?t say otherwise, you still get sequential
>>> consistency.  (This doesn?t hold in C# or OpenMP, for example.)****
>>>
>>> ** **
>>>
>>> In the presence of data races, these languages instead provide really
>>> weak or no guarantees.  The examples under discussion here have data races,
>>> so none of this applies.****
>>>
>>> ** **
>>>
>>> Hans****
>>>
>>> ** **
>>>
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [
>>> mailto:concurrency-interest-bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]
>>> *On Behalf Of *Nitsan Wakart
>>> *Sent:* Monday, February 18, 2013 6:32 AM
>>> *To:* Aleksey Shipilev; Dmitry Tsitelov
>>> *Cc:* Concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>>> test****
>>>
>>> ** **
>>>
>>> My understanding is that the program is open to SC equivalent
>>> interpretation given no hints are given with regards to
>>> ordering/concurrency.****
>>>
>>> Consider the code:****
>>>
>>> r2 = x.f;****
>>>
>>> if(x.f == 0){****
>>>
>>>           x.f = 1;****
>>>
>>>           r2 = 1;****
>>>
>>> }****
>>>
>>> Is SC equivalent to the original. The fact that x.f is written to
>>> concurrently is not conveyed to the compiler, so as far as it's concerned
>>> it's fine. Why would the above interpretation be a good idea I'm not sure,
>>> but I think it's allowed.****
>>>    ------------------------------
>>>
>>> *From:* Aleksey Shipilev <aleksey.shipilev at oracle.com>
>>> *To:* Dmitry Tsitelov <cit at cit.spb.ru>
>>> *Cc:* "Concurrency-interest at cs.oswego.edu" <
>>> Concurrency-interest at cs.oswego.edu>
>>> *Sent:* Monday, February 18, 2013 10:48 AM
>>> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush
>>> test****
>>>
>>>
>>> On 02/18/2013 02:39 PM, Dmitry Tsitelov wrote:
>>> >>  class X {
>>> >>      int f;
>>> >>  }
>>> >>
>>> >>  X x = new X();
>>> >>
>>> >>          T1      |    T2
>>> >>  ---------------+---------------
>>> >>  if (x.f == 0)  | if (x.f == 0)
>>> >>      x.f = 1    |    x.f = 1
>>> >>  r1 = x.f      | r2 = x.f
>>> >>
>>> >> Is there a valid execution which yields (r1, r2) as (1, 0) or (0, 1)?
>>> >> (It appears there are valid executions under JMM, mostly because there
>>> >> is the data race on x.f; Jeremy had a post [2] about the similar issue
>>> >> before, and reaching to the same conclusion.)
>>> >>
>>> >
>>> > Could someone point me to a sequentially-consistent execution of this
>>> > scenario emitting (1,0)/(0,1) result, please?
>>>
>>> I'm afraid there are no SC executions that yield this result. We have
>>> the data race on x.f, DRF guarantee is out of the window, it's useless
>>> to seek SC execution to justify this behavior.
>>>
>>> -Aleksey.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130220/6bda12c2/attachment-0001.html>

From hans.boehm at hp.com  Wed Feb 20 18:28:54 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 20 Feb 2013 23:28:54 +0000
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>
References: <CD47EC31.354CB%cit@cit.spb.ru>	<512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
	<CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
	<023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>
	<CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD2369A4304@G4W3214.americas.hpqcorp.net>

I'm not sure what reordering you mean.  It doesn't prevent the unexpected Dekker's result.

x = 1; r1 = y;

turns into memory_barrier; x = 1; r1 = y; memory_barrier;

which still allows the store and the load to be reordered.

Hans

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Wednesday, February 20, 2013 3:07 PM
To: Dmitry Zaslavsky
Cc: concurrency-interest at cs.oswego.edu; Boehm, Hans; Timothy Chen
Subject: Re: [concurrency-interest] Racy lazy initialization: blush test


.net docs are clearly wrong.  Thread.VolatileRead() invokes Thread.MemoryBarrier() after the read, thus preventing the reordering.  In turn, Thread.VolatileWrite() issues MemoryBarrier() before the write, preventing reordering in the other direction.  For some reason, they don't mention this important bit.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130220/16906aff/attachment.html>

From vitalyd at gmail.com  Wed Feb 20 19:31:30 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 20 Feb 2013 19:31:30 -0500
Subject: [concurrency-interest] Racy lazy initialization: blush test
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD2369A4304@G4W3214.americas.hpqcorp.net>
References: <CD47EC31.354CB%cit@cit.spb.ru> <512206E6.9040407@oracle.com>
	<1361197933.88322.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369933D0@G9W0725.americas.hpqcorp.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23699344E@G9W0725.americas.hpqcorp.net>
	<CAFx0iW_zZk8qZwJib1kHWyBRFHFkpyxqSBMuF8j4Vi14QjeArw@mail.gmail.com>
	<023E80C2-4FE2-4F35-BAA5-ACAE739E4376@gmail.com>
	<CAHjP37HJLrzQiSQiY=gezqx_1i5P+GK4a03KcZDbO0BKO8ucfA@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD2369A4304@G4W3214.americas.hpqcorp.net>
Message-ID: <CAHjP37ERHyX_1jitbUBR53xhW00rGa+ZM7fY5-+qxrSgv-MMbg@mail.gmail.com>

Right, for this scenario it still doesn't work.  I only mentioned where I
see mb's placed in their code for VolatileRead/Write.  The crucial one
missing is between the write and read.  The IKVM developer actually ran
into this: http://weblog.ikvm.net/default.aspx?date=2010-10-25

Sent from my phone
On Feb 20, 2013 6:30 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

>  I?m not sure what reordering you mean.  It doesn?t prevent the
> unexpected Dekker?s result.****
>
> ** **
>
> x = 1; r1 = y;****
>
> ** **
>
> turns into memory_barrier; x = 1; r1 = y; memory_barrier;****
>
> ** **
>
> which still allows the store and the load to be reordered.****
>
> ** **
>
> Hans ****
>
> ** **
>
> *From:* Vitaly Davidovich [mailto:vitalyd at gmail.com]
> *Sent:* Wednesday, February 20, 2013 3:07 PM
> *To:* Dmitry Zaslavsky
> *Cc:* concurrency-interest at cs.oswego.edu; Boehm, Hans; Timothy Chen
> *Subject:* Re: [concurrency-interest] Racy lazy initialization: blush test
> ****
>
> ** **
>
> .net docs are clearly wrong.  Thread.VolatileRead() invokes
> Thread.MemoryBarrier() after the read, thus preventing the reordering.  In
> turn, Thread.VolatileWrite() issues MemoryBarrier() before the write,
> preventing reordering in the other direction.  For some reason, they don't
> mention this important bit.****
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130220/a5176d14/attachment.html>

From thurston at nomagicsoftware.com  Fri Feb 22 13:57:22 2013
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Fri, 22 Feb 2013 10:57:22 -0800
Subject: [concurrency-interest]  Racy lazy initialization: blush test
Message-ID: <5d0b12ea6d042f28ae168110f26a58e3@nomagicsoftware.com>

Changing the example somewhat (just excluding registers):

if (0 == x.f)
     x.f = 1

To my mind, this code contains a data race even though the "result" 
(not precisely sure what that means frankly) is the same, i.e. it is 
"racy" even if not "buggy"


From hallerp at gmail.com  Wed Feb 27 06:27:31 2013
From: hallerp at gmail.com (Philipp Haller)
Date: Wed, 27 Feb 2013 12:27:31 +0100
Subject: [concurrency-interest] CFP Scala 2013
Message-ID: <49481771-6644-4BF9-A0E7-6161B3856D09@gmail.com>

========================================================================
                             "Scala 2013"

                   the Fourth Annual Scala Workshop
                      co-located with ECOOP 2013
                         Montpellier, France
                            July 2nd, 2013

                            CALL FOR PAPERS

                 http://lamp.epfl.ch/~hmiller/scala2013
========================================================================

Scala is a general purpose programming language designed to express common
programming patterns in a concise, elegant, and type-safe way. It smoothly
integrates features of object-oriented and functional languages.

This workshop is a forum for researchers and practitioners to share new ideas
and results of interest to the Scala community.

We seek papers on topics related to Scala, including (but not limited to):

- Language design and implementation ? language extensions, optimization, and
  performance evaluation.
- Library design and implementation patterns for extending Scala ? embedded
  domain-specific languages, combining language features, generic and meta-programming.
- Formal techniques for Scala-like programs ? formalizations of the language,
  type system, and semantics, formalizing proposed language extensions and
  variants, dependent object types, type and effect systems.
- Concurrent and distributed programming ? libraries, frameworks, language
  extensions, programming paradigms: (Actors, STM, ...), performance
  evaluation, experimental results.
- Safety and reliability ? pluggable type systems, contracts, static analysis
  and verification, runtime monitoring.
- Tools ? development environments, debuggers, refactoring tools, testing
  frameworks.
- Case studies, experience reports, and pearls.

Submitted papers should describe new ideas, experimental results, or projects
related to Scala. In order to encourage lively discussion, submitted papers
may describe work in progress. All papers will be judged on a combination of
correctness, significance, novelty, clarity, and interest to the community.

In general, papers should explain their original contributions,
identifying what has been accomplished, explaining why it is
significant, and relating it to previous work (also for other
languages where appropriate). Papers in the last category of the list
above need not necessarily report original research results; they may
instead, for example, report practical experience that will be useful
to others, new Scala idioms, or programming pearls. In all cases, such
a paper must make a contribution which is of interest to the Scala
community, or from which other members of the Scala community can
benefit.

Publications at the Scala Workshop represent works-in-progress and are
not intended to preclude later publication at any of the main
conferences. Though, follow-up submissions do need to conform to the
publication policies of the targeted conference, which typically
equates to significant extension or refinement of the workshop
publication.

KEYWORDS: Library Design and Implementation, Language Design and
Implementation, Applications, Formal Techniques, Parallelism and
Concurrency, Distributed Programming, Tools, Experience Reports,
Empirical Studies


## Student Talks ##

In addition to regular papers and tool demos, we also solicit short
student talks by PhD students. A student talk is not accompanied by
a paper (it is sufficient to submit a short abstract of the talk in
plain text). Student talks are about 5 minutes long, presenting
ongoing or completed research related to Scala, or announcing a
project that would be of interest to the Scala community.


## Proceedings ##

It is planned to publish accepted papers in the ACM Digital Library, unless
the authors choose not to. In case of publication in the ACM Digital Library,
authors must transfer copyright to ACM upon acceptance (for government work,
to the extent transferable), but retain various rights (see ACM Copyright
Policy. Authors are encouraged to publish auxiliary material with their paper
(source code, test data, etc.); they retain copyright of auxiliary material.


## Submission Details ##

* Abstract Submission: April 12, 2013
* Paper Submission   : April 19, 2013
* Author Notification: May 17, 2013
* Final Papers Due   : June 1, 2013 (to be confirmed)
* Early Registration : May 31, 2013

Submitted papers should be in portable document format (PDF), formatted using
the standard ACM SIGPLAN two-column conference style (10pt format). Regular
research papers must not exceed 10 pages, tool demonstration papers and short
papers must not exceed 4 pages. "Tool Demos" and "Short Papers" should be
marked as such with those words in the title at time of submission.

Student talks are not accompanied by papers. Therefore, it is sufficient to
only submit a plain-text abstract. "Student Talks" should be marked as such
with those words in the title at time of submission.

Submission is via EasyChair: https://www.easychair.org/conferences/?conf=scala2013


## Program Committee ##

* Marius Eriksen, Twitter
* Viktor Kuncak, EPFL
* Mira Mezini, TU Darmstadt
* Matt Might, University of Utah
* Nate Nystrom, University of Lugano
* Bruno Oliveira, National University of Singapore
* Kunle Olukotun, Stanford University
* Aleksandar Prokopec, EPFL
* David Van Horn, Northeastern University
* Tobias Wrigstad, Uppsala University


## Organizing Committee ##

* Philipp Haller (Chair), Typesafe
* Martin Odersky, EPFL
* Doug Lea, SUNY Oswego
* Heather Miller (Co-Chair), EPFL
* Vojin Jovanovic, EPFL


## Links ##

* The Scala Workshop 2013 web site: http://lamp.epfl.ch/~hmiller/scala2013
* The ECOOP/ECSA/ECMFA 2013 web site: http://www.lirmm.fr/ec-montpellier-2013



From yechielf at gigaspaces.com  Thu Feb 28 06:41:43 2013
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Thu, 28 Feb 2013 11:41:43 +0000
Subject: [concurrency-interest] a question regarding LockSupport
Message-ID: <DD7C17376B6B7E4BA23276B29408B73524E484BD@AMSPRD0410MB373.eurprd04.prod.outlook.com>

All its methods are static. Also, UNSAFE which is used by it is a static instance. So- when PARK()  commands are issued by different threads within different implementations of locks which use LockSupport , or several instances of locks (like the FifoMutex in the code example of LockSupport) - are they all competing for a single Park() permit ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/25fc8a72/attachment.html>

From davidcholmes at aapt.net.au  Thu Feb 28 06:46:30 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 28 Feb 2013 21:46:30 +1000
Subject: [concurrency-interest] a question regarding LockSupport
In-Reply-To: <DD7C17376B6B7E4BA23276B29408B73524E484BD@AMSPRD0410MB373.eurprd04.prod.outlook.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEENCJKAA.davidcholmes@aapt.net.au>

No the permit is per-thread and maintained in the VM.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Yechiel
Feffer
  Sent: Thursday, 28 February 2013 9:42 PM
  To: Concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] a question regarding LockSupport


  All its methods are static. Also, UNSAFE which is used by it is a static
instance. So- when PARK()  commands are issued by different threads within
different implementations of locks which use LockSupport , or several
instances of locks (like the FifoMutex in the code example of LockSupport) -
are they all competing for a single Park() permit ?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/cf7d3bed/attachment.html>

From ylt at letallec.org  Thu Feb 28 16:29:34 2013
From: ylt at letallec.org (Yann Le Tallec)
Date: Thu, 28 Feb 2013 21:29:34 +0000
Subject: [concurrency-interest] AtomicInteger implementation
Message-ID: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>

Hello,

all the code of incrementAndGet (in JDK 7) but one line is identical to
addAndGet.
What is the reason why the code has been duplicated instead of implementing
incrementAndGet as:

public final int incrementAndGet() {
    return addAndGet(1);}

Regards,
Yann

-----

For reference, the two methods:

public final int addAndGet(int delta) {
    for (;;) {
        int current = get();
        int next = current + delta;         // Only difference
        if (compareAndSet(current, next))
            return next;
    }}
public final int incrementAndGet() {
    for (;;) {
        int current = get();
        int next = current + 1;             // Only difference
        if (compareAndSet(current, next))
            return next;
    }}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/2cc8ce28/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 28 16:53:18 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 28 Feb 2013 14:53:18 -0700
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
Message-ID: <512FD1CE.5090005@oracle.com>

Good question.

For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for these 
methods.  So, on x86 they compile down to a single atomic instruction.

For weaker JITs, increment by 1 can be much faster since it is a 
constant and there can be an instruction on the processor for 
incrementing.  This statement assumes that addAndGet(1) wouldn't be inlined.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
> Hello,
>
> all the code of |incrementAndGet| (in JDK 7) but one line is identical 
> to|addAndGet|.
> What is the reason why the code has been duplicated instead of 
> implementing |incrementAndGet| as:
> |public  final  int  incrementAndGet()  {
>      return  addAndGet(1);
> }
> |
> |R|egards,
> Yann
> |-----
>
>
> |
> |For reference, the two methods:
>
> |
> |public  final  int  addAndGet(int  delta)  {
>      for  (;;)  {
>          int  current=  get();
>          int  next=  current+  delta;          // Only difference
>          if  (compareAndSet(current,  next))
>              return  next;
>      }
> }
>
> public  final  int  incrementAndGet()  {
>      for  (;;)  {
>          int  current=  get();
>          int  next=  current+  1;              // Only difference
>          if  (compareAndSet(current,  next))
>              return  next;
>      }
> }|
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/2b22f14a/attachment.html>

From viktor.klang at gmail.com  Thu Feb 28 17:00:29 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 28 Feb 2013 23:00:29 +0100
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <512FD1CE.5090005@oracle.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
Message-ID: <CANPzfU_38CzsE24cwXdsaJsiyUVi5-TQXaAYRf25KO8OJvsszA@mail.gmail.com>

On Thu, Feb 28, 2013 at 10:53 PM, Nathan Reynolds <
nathan.reynolds at oracle.com> wrote:

>  Good question.
>
> For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for these
> methods.  So, on x86 they compile down to a single atomic instruction.
>

Please do correct me if I'm wrong, but that was intrinsified like 6 months
ago, right?

Cheers,
?


>
> For weaker JITs, increment by 1 can be much faster since it is a constant
> and there can be an instruction on the processor for incrementing.  This
> statement assumes that addAndGet(1) wouldn't be inlined.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>
> Hello,
>
> all the code of incrementAndGet (in JDK 7) but one line is identical to
> addAndGet.
> What is the reason why the code has been duplicated instead of
> implementing incrementAndGet as:
>
> public final int incrementAndGet() {
>     return addAndGet(1);}
>
> Regards,
> Yann
>
> -----
>
>  For reference, the two methods:
>
> public final int addAndGet(int delta) {
>     for (;;) {
>         int current = get();
>         int next = current + delta;         // Only difference
>         if (compareAndSet(current, next))
>             return next;
>     }}
> public final int incrementAndGet() {
>     for (;;) {
>         int current = get();
>         int next = current + 1;             // Only difference
>         if (compareAndSet(current, next))
>             return next;
>     }}
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/7152bc90/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 28 17:05:31 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 28 Feb 2013 15:05:31 -0700
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <CANPzfU_38CzsE24cwXdsaJsiyUVi5-TQXaAYRf25KO8OJvsszA@mail.gmail.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
	<CANPzfU_38CzsE24cwXdsaJsiyUVi5-TQXaAYRf25KO8OJvsszA@mail.gmail.com>
Message-ID: <512FD4AB.2000504@oracle.com>

Correct.  It was "resolved" on 9/21/2012.  The backport to 7uX was 
resolved on 11/7/2012.  I am not sure which update it got into, though.

https://jbs.oracle.com/bugs/browse/JDK-7023898

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/28/2013 3:00 PM, ?iktor ?lang wrote:
>
>
>
> On Thu, Feb 28, 2013 at 10:53 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Good question.
>
>     For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for
>     these methods.  So, on x86 they compile down to a single atomic
>     instruction.
>
>
> Please do correct me if I'm wrong, but that was intrinsified like 6 
> months ago, right?
>
> Cheers,
> ?
>
>
>     For weaker JITs, increment by 1 can be much faster since it is a
>     constant and there can be an instruction on the processor for
>     incrementing.  This statement assumes that addAndGet(1) wouldn't
>     be inlined.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>>     Hello,
>>
>>     all the code of |incrementAndGet| (in JDK 7) but one line is
>>     identical to|addAndGet|.
>>     What is the reason why the code has been duplicated instead of
>>     implementing |incrementAndGet| as:
>>     |public  final  int  incrementAndGet()  {
>>          return  addAndGet(1);
>>     }
>>     |
>>     |R|egards,
>>     Yann
>>     |-----
>>
>>
>>     |
>>     |For reference, the two methods:
>>
>>     |
>>     |public  final  int  addAndGet(int  delta)  {
>>          for  (;;)  {
>>              int  current=  get();
>>              int  next=  current+  delta;          // Only difference
>>              if  (compareAndSet(current,  next))
>>                  return  next;
>>          }
>>     }
>>
>>     public  final  int  incrementAndGet()  {
>>          for  (;;)  {
>>              int  current=  get();
>>              int  next=  current+  1;              // Only difference
>>              if  (compareAndSet(current,  next))
>>                  return  next;
>>          }
>>     }|
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> /
> /
> Typesafe <http://www.typesafe.com/>- The software stack for 
> applications that scale
> Twitter: @viktorklang

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/60aa5b29/attachment-0001.html>

From ylt at letallec.org  Thu Feb 28 17:15:24 2013
From: ylt at letallec.org (Yann Le Tallec)
Date: Thu, 28 Feb 2013 22:15:24 +0000
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <512FD1CE.5090005@oracle.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
Message-ID: <CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>

I see. On JDK 7u11, running with PrintAssembly on, it looks like the JIT
does compile the code as is (possibly not treated as an intrinsic method in
that version?). As you explained:
- incementAndGet translates into an INC
- addAndGet translates into an ADD,1.

However, according to Intel optimisation guidelines for x86, ADD,1 can be
faster than INC. A quick and dirty micro benchmark actually shows that
addAndGet(1) is fairly consistently 2-5% faster than incrementAndGet (intel
i5 - 64bit) post JIT compilation.

Hence the question ;-)


On 28 February 2013 21:53, Nathan Reynolds <nathan.reynolds at oracle.com>wrote:

>  Good question.
>
> For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for these
> methods.  So, on x86 they compile down to a single atomic instruction.
>
> For weaker JITs, increment by 1 can be much faster since it is a constant
> and there can be an instruction on the processor for incrementing.  This
> statement assumes that addAndGet(1) wouldn't be inlined.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>
> Hello,
>
> all the code of incrementAndGet (in JDK 7) but one line is identical to
> addAndGet.
> What is the reason why the code has been duplicated instead of
> implementing incrementAndGet as:
>
> public final int incrementAndGet() {
>     return addAndGet(1);}
>
> Regards,
> Yann
>
> -----
>
>  For reference, the two methods:
>
> public final int addAndGet(int delta) {
>     for (;;) {
>         int current = get();
>         int next = current + delta;         // Only difference
>         if (compareAndSet(current, next))
>             return next;
>     }}
> public final int incrementAndGet() {
>     for (;;) {
>         int current = get();
>         int next = current + 1;             // Only difference
>         if (compareAndSet(current, next))
>             return next;
>     }}
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/d1a6a7eb/attachment.html>

From nathan.reynolds at oracle.com  Thu Feb 28 17:23:16 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 28 Feb 2013 15:23:16 -0700
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
	<CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>
Message-ID: <512FD8D4.6060306@oracle.com>

Given that the backport was resolved on 11/7/2012, I kind of doubt it 
was done in time for HotSpot 7u10.  7u11, 13 and 15 are security 
releases and hence won't have this fix.  7u12 and 14 were skipped. I 
hope this means that the next non-security HotSpot release (7u16?) will 
have the backport.

It seems strange that ADD 1 would be any faster or slower than INC on 
Intel x86.  Each instruction is decoded into one or more ?ops.  I would 
think both of these instructions would translate to the same ?ops.  In 
fact, ADD 1 will generate more code bytes than "inc".  So, from a cache 
performance perspective, "inc" should be faster.  I wonder what the 
explanation is for this.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 2/28/2013 3:15 PM, Yann Le Tallec wrote:
> I see. On JDK 7u11, running with PrintAssembly on, it looks like the 
> JIT does compile the code as is (possibly not treated as an intrinsic 
> method in that version?). As you explained:
> - incementAndGet translates into an INC
> - addAndGet translates into an ADD,1.
>
> However, according to Intel optimisation guidelines for x86, ADD,1 can 
> be faster than INC. A quick and dirty micro benchmark actually shows 
> that addAndGet(1) is fairly consistently 2-5% faster than 
> incrementAndGet (intel i5 - 64bit) post JIT compilation.
>
> Hence the question ;-)
>
>
> On 28 February 2013 21:53, Nathan Reynolds <nathan.reynolds at oracle.com 
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Good question.
>
>     For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for
>     these methods.  So, on x86 they compile down to a single atomic
>     instruction.
>
>     For weaker JITs, increment by 1 can be much faster since it is a
>     constant and there can be an instruction on the processor for
>     incrementing.  This statement assumes that addAndGet(1) wouldn't
>     be inlined.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>>     Hello,
>>
>>     all the code of |incrementAndGet| (in JDK 7) but one line is
>>     identical to|addAndGet|.
>>     What is the reason why the code has been duplicated instead of
>>     implementing |incrementAndGet| as:
>>     |public  final  int  incrementAndGet()  {
>>          return  addAndGet(1);
>>     }
>>     |
>>     |R|egards,
>>     Yann
>>     |-----
>>
>>
>>     |
>>     |For reference, the two methods:
>>
>>     |
>>     |public  final  int  addAndGet(int  delta)  {
>>          for  (;;)  {
>>              int  current=  get();
>>              int  next=  current+  delta;          // Only difference
>>              if  (compareAndSet(current,  next))
>>                  return  next;
>>          }
>>     }
>>
>>     public  final  int  incrementAndGet()  {
>>          for  (;;)  {
>>              int  current=  get();
>>              int  next=  current+  1;              // Only difference
>>              if  (compareAndSet(current,  next))
>>                  return  next;
>>          }
>>     }|
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/a61b038e/attachment.html>

From ylt at letallec.org  Thu Feb 28 17:36:41 2013
From: ylt at letallec.org (Yann Le Tallec)
Date: Thu, 28 Feb 2013 22:36:41 +0000
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <512FD8D4.6060306@oracle.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
	<CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>
	<512FD8D4.6060306@oracle.com>
Message-ID: <CAOPu7Eh=VQraM6-rUZ_+2LmZzCvQY1q6vHbwKrzHJ4oU1eYZqQ@mail.gmail.com>

That makes sense. Thank you for your answer.

Regarding ADD vs. INC, it is in section 3.5.1.1 page 3-30 of the pdf
version of the optimisation manual [1]: "INC and DEC instructions should be
replaced with ADD or SUB instructions, because ADD and SUB overwrite all
flags, whereas INC and DEC do not, therefore creating false dependencies on
earlier instructions that set the flags."

If that is the case, it is strange that the JIT uses INC.

 [1]:
http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html


On 28 February 2013 22:23, Nathan Reynolds <nathan.reynolds at oracle.com>wrote:

>  Given that the backport was resolved on 11/7/2012, I kind of doubt it was
> done in time for HotSpot 7u10.  7u11, 13 and 15 are security releases and
> hence won't have this fix.  7u12 and 14 were skipped.  I hope this means
> that the next non-security HotSpot release (7u16?) will have the backport.
>
> It seems strange that ADD 1 would be any faster or slower than INC on
> Intel x86.  Each instruction is decoded into one or more ?ops.  I would
> think both of these instructions would translate to the same ?ops.  In
> fact, ADD 1 will generate more code bytes than "inc".  So, from a cache
> performance perspective, "inc" should be faster.  I wonder what the
> explanation is for this.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/28/2013 3:15 PM, Yann Le Tallec wrote:
>
>  I see. On JDK 7u11, running with PrintAssembly on, it looks like the JIT
> does compile the code as is (possibly not treated as an intrinsic method in
> that version?). As you explained:
> - incementAndGet translates into an INC
> - addAndGet translates into an ADD,1.
>
> However, according to Intel optimisation guidelines for x86, ADD,1 can be
> faster than INC. A quick and dirty micro benchmark actually shows that
> addAndGet(1) is fairly consistently 2-5% faster than incrementAndGet (intel
> i5 - 64bit) post JIT compilation.
>
>  Hence the question ;-)
>
>
> On 28 February 2013 21:53, Nathan Reynolds <nathan.reynolds at oracle.com>wrote:
>
>>  Good question.
>>
>> For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for these
>> methods.  So, on x86 they compile down to a single atomic instruction.
>>
>> For weaker JITs, increment by 1 can be much faster since it is a constant
>> and there can be an instruction on the processor for incrementing.  This
>> statement assumes that addAndGet(1) wouldn't be inlined.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>   On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>>
>>  Hello,
>>
>> all the code of incrementAndGet (in JDK 7) but one line is identical to
>> addAndGet.
>> What is the reason why the code has been duplicated instead of
>> implementing incrementAndGet as:
>>
>> public final int incrementAndGet() {
>>     return addAndGet(1);}
>>
>> Regards,
>> Yann
>>
>> -----
>>
>>  For reference, the two methods:
>>
>> public final int addAndGet(int delta) {
>>     for (;;) {
>>         int current = get();
>>         int next = current + delta;         // Only difference
>>         if (compareAndSet(current, next))
>>             return next;
>>     }}
>> public final int incrementAndGet() {
>>     for (;;) {
>>         int current = get();
>>         int next = current + 1;             // Only difference
>>         if (compareAndSet(current, next))
>>             return next;
>>     }}
>>
>>
>>
>>  _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/696030da/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Feb 28 17:36:06 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 28 Feb 2013 22:36:06 +0000
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <512FD8D4.6060306@oracle.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
	<CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>
	<512FD8D4.6060306@oracle.com>
Message-ID: <512FDBD6.2000205@oracle.com>

They won't translate into exactly the same u-ops. INC doesn't touch 
carry flag.

Alex


On 28/02/2013 22:23, Nathan Reynolds wrote:
> Given that the backport was resolved on 11/7/2012, I kind of doubt it 
> was done in time for HotSpot 7u10.  7u11, 13 and 15 are security 
> releases and hence won't have this fix.  7u12 and 14 were skipped.  I 
> hope this means that the next non-security HotSpot release (7u16?) 
> will have the backport.
>
> It seems strange that ADD 1 would be any faster or slower than INC on 
> Intel x86.  Each instruction is decoded into one or more ?ops. I would 
> think both of these instructions would translate to the same ?ops.  In 
> fact, ADD 1 will generate more code bytes than "inc".  So, from a 
> cache performance perspective, "inc" should be faster.  I wonder what 
> the explanation is for this.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 2/28/2013 3:15 PM, Yann Le Tallec wrote:
>> I see. On JDK 7u11, running with PrintAssembly on, it looks like the 
>> JIT does compile the code as is (possibly not treated as an intrinsic 
>> method in that version?). As you explained:
>> - incementAndGet translates into an INC
>> - addAndGet translates into an ADD,1.
>>
>> However, according to Intel optimisation guidelines for x86, ADD,1 
>> can be faster than INC. A quick and dirty micro benchmark actually 
>> shows that addAndGet(1) is fairly consistently 2-5% faster than 
>> incrementAndGet (intel i5 - 64bit) post JIT compilation.
>>
>> Hence the question ;-)
>>
>>
>> On 28 February 2013 21:53, Nathan Reynolds 
>> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>>
>>     Good question.
>>
>>     For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for
>>     these methods.  So, on x86 they compile down to a single atomic
>>     instruction.
>>
>>     For weaker JITs, increment by 1 can be much faster since it is a
>>     constant and there can be an instruction on the processor for
>>     incrementing.  This statement assumes that addAndGet(1) wouldn't
>>     be inlined.
>>
>>     Nathan Reynolds
>>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>     Architect | 602.333.9091 <tel:602.333.9091>
>>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>     Technology
>>     On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>>>     Hello,
>>>
>>>     all the code of |incrementAndGet| (in JDK 7) but one line is
>>>     identical to|addAndGet|.
>>>     What is the reason why the code has been duplicated instead of
>>>     implementing |incrementAndGet| as:
>>>     |public  final  int  incrementAndGet()  {
>>>          return  addAndGet(1);
>>>     }
>>>     |
>>>     |R|egards,
>>>     Yann
>>>     |-----
>>>
>>>
>>>     |
>>>     |For reference, the two methods:
>>>
>>>     |
>>>     |public  final  int  addAndGet(int  delta)  {
>>>          for  (;;)  {
>>>              int  current=  get();
>>>              int  next=  current+  delta;          // Only difference
>>>              if  (compareAndSet(current,  next))
>>>                  return  next;
>>>          }
>>>     }
>>>
>>>     public  final  int  incrementAndGet()  {
>>>          for  (;;)  {
>>>              int  current=  get();
>>>              int  next=  current+  1;              // Only difference
>>>              if  (compareAndSet(current,  next))
>>>                  return  next;
>>>          }
>>>     }|
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130228/cfb21046/attachment.html>

From stanimir at riflexo.com  Thu Feb 28 17:38:53 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 1 Mar 2013 00:38:53 +0200
Subject: [concurrency-interest] AtomicInteger implementation
In-Reply-To: <512FD8D4.6060306@oracle.com>
References: <CAOPu7Ej=SoN1FcPrDQcHkjWRnR0y_C1aU9YzorpmibgQHXJNBQ@mail.gmail.com>
	<512FD1CE.5090005@oracle.com>
	<CAOPu7EiyoFzwNSsHCTvpbEWbcQLNDcarL5sQShk-iX=qe1CmOQ@mail.gmail.com>
	<512FD8D4.6060306@oracle.com>
Message-ID: <CAEJX8oqFQJuC8TExmbvgf3zjpZFpWRFy5UyE_f_9JUBc6MWkZw@mail.gmail.com>

The INC doesn't update the carry flag and can cause partial (un)updated
stalls: "Partial flags stalls" -
http://www.agner.org/optimize/microarchitecture.pdf (page58)
INC is shorter than ADD and can be of use, though. Since it doesn't affect
carry flag in can be hidden between adds/subs and consecutive carry flag
checks.

Stanimir


On Fri, Mar 1, 2013 at 12:23 AM, Nathan Reynolds <nathan.reynolds at oracle.com
> wrote:

>  Given that the backport was resolved on 11/7/2012, I kind of doubt it was
> done in time for HotSpot 7u10.  7u11, 13 and 15 are security releases and
> hence won't have this fix.  7u12 and 14 were skipped.  I hope this means
> that the next non-security HotSpot release (7u16?) will have the backport.
>
> It seems strange that ADD 1 would be any faster or slower than INC on
> Intel x86.  Each instruction is decoded into one or more ?ops.  I would
> think both of these instructions would translate to the same ?ops.  In
> fact, ADD 1 will generate more code bytes than "inc".  So, from a cache
> performance perspective, "inc" should be faster.  I wonder what the
> explanation is for this.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 2/28/2013 3:15 PM, Yann Le Tallec wrote:
>
>  I see. On JDK 7u11, running with PrintAssembly on, it looks like the JIT
> does compile the code as is (possibly not treated as an intrinsic method in
> that version?). As you explained:
> - incementAndGet translates into an INC
> - addAndGet translates into an ADD,1.
>
> However, according to Intel optimisation guidelines for x86, ADD,1 can be
> faster than INC. A quick and dirty micro benchmark actually shows that
> addAndGet(1) is fairly consistently 2-5% faster than incrementAndGet (intel
> i5 - 64bit) post JIT compilation.
>
>  Hence the question ;-)
>
>
> On 28 February 2013 21:53, Nathan Reynolds <nathan.reynolds at oracle.com>wrote:
>
>>  Good question.
>>
>> For Hotspot x86, it doesn't matter.  HotSpot has intrinsics for these
>> methods.  So, on x86 they compile down to a single atomic instruction.
>>
>> For weaker JITs, increment by 1 can be much faster since it is a constant
>> and there can be an instruction on the processor for incrementing.  This
>> statement assumes that addAndGet(1) wouldn't be inlined.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>   On 2/28/2013 2:29 PM, Yann Le Tallec wrote:
>>
>>  Hello,
>>
>> all the code of incrementAndGet (in JDK 7) but one line is identical to
>> addAndGet.
>> What is the reason why the code has been duplicated instead of
>> implementing incrementAndGet as:
>>
>> public final int incrementAndGet() {
>>     return addAndGet(1);}
>>
>> Regards,
>> Yann
>>
>> -----
>>
>>  For reference, the two methods:
>>
>> public final int addAndGet(int delta) {
>>     for (;;) {
>>         int current = get();
>>         int next = current + delta;         // Only difference
>>         if (compareAndSet(current, next))
>>             return next;
>>     }}
>> public final int incrementAndGet() {
>>     for (;;) {
>>         int current = get();
>>         int next = current + 1;             // Only difference
>>         if (compareAndSet(current, next))
>>             return next;
>>     }}
>>
>>
>>
>>  _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130301/3313d88a/attachment-0001.html>

