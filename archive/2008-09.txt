From dawid.kurzyniec at gmail.com  Tue Sep  2 03:39:42 2008
From: dawid.kurzyniec at gmail.com (Dawid Kurzyniec)
Date: Tue, 2 Sep 2008 09:39:42 +0200
Subject: [concurrency-interest] backport of java.util.concurrent: Bug in
	Arrays.deepEquals()
In-Reply-To: <48B7C1CA.2010800@buluschek.com>
References: <48B7C1CA.2010800@buluschek.com>
Message-ID: <3cbaca580809020039o7d402dfbg69ff132e255e1fa0@mail.gmail.com>

+ concurrency-interest

Thanks! I'll look into it.

On Fri, Aug 29, 2008 at 11:30 AM, Philipp Buluschek
<philipp at buluschek.com> wrote:
> Dear Sir,
> I am using your backport to the java.util.concurrent. (version 3.1 for java
> 1.2)
>
> In the package you distribute, there is also a backport for the
> java.util.Arrays class, which is very handy. Unfortunately, I think that the
> implementation of the deepEquals() method contains a bug. The following test
> case should return true, while in the present implementation it returns
> false. The present implementation in Sun's SDK 6.0 is correct in this
> regard. Maybe you could update the packages to the implementation from Sun?
>
> Thank you again for your work in backporting the j.u.concurrent classes.
> Best regards
> Philipp Buluschek
>
> --- test case ---
> import edu.emory.mathcs.backport.java.util.Arrays;
> public class ArraysEqualsTest {
>   public static void main(String[] args) {
>       float[][] f1 = {{1,2},{3.1415f, 2.7183f}};
>       float[][] f2 = {{1,2},{3.1415f, 2.7183f}};
>             System.out.println(Arrays.deepEquals(f1, f2)); // should return
> true, but returns false
>   }
> }
>
> --
> ________________________________________
>
> Philipp Buluschek, Dr-Ing. Dipl. EPFL
> Software Design & Implementation
> Schellingstr. 16
> D-80799 M?nchen, Germany
> Phone: +49 89 2875 5533
> Mobile: +49 157 7580 8218
> E-mail: philipp at buluschek.com
> ________________________________________
>
>


From dannydig at csail.mit.edu  Tue Sep  2 22:58:15 2008
From: dannydig at csail.mit.edu (Danny Dig)
Date: Tue, 2 Sep 2008 22:58:15 -0400
Subject: [concurrency-interest] tool for converting sequential code to use
	j.u.c.
Message-ID: <9868b4570809021958r261fef34t13021fab018846d6@mail.gmail.com>

Hello j.u.c. enthusiasts,

We've been working on a tool for reengineering Java sequential code for
concurrency by using j.u.c. utilities.

We are announcing the first release of our tool, Concurrencer. It converts
int fields to AtomicIntegers, and HashMap fields to ConcurrentHashMap.
Concurrencer is implemented as an extension to Eclipse's refactoring engine.
We've tried it on a few open-source projects and we found that (i) Concurrencer
saves the programmer from changing many lines of code,
(ii) is not error-prone like manual conversion, and (iii) is less
omission-prone then
the human.

We would love to get your feedback on the tool. Please use the tool and let us
know if it is useful. We are looking for constructive criticism on how we can
improve it.

Please visit the webpage below for instructions on how to get the tool. The
webpage also contains a white paper (currently under review to a workshop)
describing the features of Concurrencer. Due to lack of space, the paper
does not contain many details like the data flow analysis for
ConvertToConcurrentHashMap. It turns out that converting usages of put() to
putIfAbsent() is not trivial, and has very interesting cases. Please try some
variations yourself and let us know.

Here comes the webpage:
http://people.csail.mit.edu/dannydig/CR/

Currently we are implementing another transformation. It converts a recursive
divide-and-conquer method into one which uses the FJTask framework and the
RecursiveAction. We've tried it on a few textbook algorithms (e.g., fibonacci,
quicksort, mergesort, matrix multiplication). We have not released yet this
refactoring since it is not fully finished. Do you have some examples of real
code which was converted to use RecursiveAction (other than the examples
coming with forkjoin Javadoc)?

looking forward to receiving your feedback,
Danny

-- 
Danny Dig's homepage: http://netfiles.uiuc.edu/dig/www

Motto: "Success is not for the chosen few but for the few who choose"

From dl at cs.oswego.edu  Sat Sep  6 10:08:31 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 06 Sep 2008 10:08:31 -0400
Subject: [concurrency-interest] forkjoin updates
Message-ID: <48C28EDF.2040003@cs.oswego.edu>

An updated jsr166y package is available. The changes
are mainly focussed on internal reducing overhead and improving
scalability. There are some visible changes though:

* Phasers now support tiering -- you can create trees
of them to reduce contention. (Initial plans were
to do this internally/adaptively, but exposing
it leaving it up to users works out better.)

* AsyncAction is now a base class for (existing) class
LinkedAsyncAction, along with new classes BasicAsyncAction
(a minimal implementation) and BinraryAsyncAction. Async
actions take some attitude adjustment to get used to, but
are more flexible than Recursive{Action,Task}.

* CyclicAction has disappeared, probably to be replaced
by better support for mixing long-lived looping threads
running in ExecutorServices with data-parallel tasks running
in ForkJoinPools. (Which is possible even now, but not
always easy to set up.) Similarly for Phaser integration.

* A few small utility methods that probably no
one has used have gone away, and a few better
documented ones introduced.

Some of the main streamlining improvements rely on
Java6 features. (It also runs of course on
early-access versions of openjdk/jdk7.)
There is no longer even internal guidance on
how to adapt for Java5. If you must run on Java5, you
probably want to keep using older snaphots.

Comments welcome as always. Things are in their usual places:
API specs:
   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
jar file:
   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar
Browsable CVS sources:
   http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/

-Doug




From timmyc at hotmail.com  Sat Sep  6 13:37:50 2008
From: timmyc at hotmail.com (Tim Clendenen)
Date: Sat, 6 Sep 2008 17:37:50 +0000
Subject: [concurrency-interest] forkjoin updates
Message-ID: <BAY116-W21896B7B00245FB0701F06C85B0@phx.gbl>


The BasicAsyncAction class doesn't appear to be in CVS.

Doug Lea wrote:
> An updated jsr166y package is available. The changes
> are mainly focussed on internal reducing overhead and improving
> scalability. There are some visible changes though:
>
> * Phasers now support tiering -- you can create trees
> of them to reduce contention. (Initial plans were
> to do this internally/adaptively, but exposing
> it leaving it up to users works out better.)
>
> * AsyncAction is now a base class for (existing) class
> LinkedAsyncAction, along with new classes BasicAsyncAction
> (a minimal implementation) and BinraryAsyncAction. Async
> actions take some attitude adjustment to get used to, but
> are more flexible than Recursive{Action,Task}.
>
> * CyclicAction has disappeared, probably to be replaced
> by better support for mixing long-lived looping threads
> running in ExecutorServices with data-parallel tasks running
> in ForkJoinPools. (Which is possible even now, but not
> always easy to set up.) Similarly for Phaser integration.
>
> * A few small utility methods that probably no
> one has used have gone away, and a few better
> documented ones introduced.
>
> Some of the main streamlining improvements rely on
> Java6 features. (It also runs of course on
> early-access versions of openjdk/jdk7.)
> There is no longer even internal guidance on
> how to adapt for Java5. If you must run on Java5, you
> probably want to keep using older snaphots.
>
> Comments welcome as always. Things are in their usual places:
> API specs:
>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
> jar file:
>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar
> Browsable CVS sources:
>   http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>



_________________________________________________________________
Get more out of the Web. Learn 10 hidden secrets of Windows Live.
http://windowslive.com/connect/post/jamiethomson.spaces.live.com-Blog-cns!550F681DAD532637!5295.entry?ocid=TXT_TAGLM_WL_domore_092008
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080906/7e3aca71/attachment.html>

From dl at cs.oswego.edu  Sat Sep  6 14:43:01 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 06 Sep 2008 14:43:01 -0400
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <484844FC.5040305@fh-landshut.de>
References: <484844FC.5040305@fh-landshut.de>
Message-ID: <48C2CF35.1060009@cs.oswego.edu>

Sorry for the long delay replying to this!

Michael Bien wrote:
> -made ForkJoinTask Serializeable

We can't mandate that all ForkJoinTasks are serializable.
In a non-JDK release maybe we could, because then we
could just tell people creating subclasses (which is the main
way you use FJ) to ignore the serializablility
if they don't need it, because nothing actually relies on it.
An alternative would be to introduce SerializableForkJoinTask,
but then we'd need the basic exec internals fleshed out for
the various main flavors (SerializableRecursiveAction etc)
because we don't/can't allow people to implement these themselves.

Further, all you really want here is to
ensure that arguments and results are serializable. Sadly
you can't write this as a conjunctive type like:
    void foo([ForkJoinTask & Serializable] task)
So my best suggestion, sadly enough, is to rely on
dynamic typing, as in:
    void foo(ForkJoinTask task) {
       if (!(task implements Serializable)) throw...

Which would then entail javadoc @param specs etc that
spell out the otherwise unstated type requirements.

(Perhaps someone has already created an annotation tag
along these lines?)

How bad would that be?

> -added popQuedTask() to ForkJoinPool which returns a not-yet executed 
> task and removes it from the pool (i am pretty sure this will not work 
> under all conditions)

You mean a submission to a pool, right?
That is now possible via ForkJoinWorkerThread.pollSubmission.
It seems somewhat dangerous to expose the ForkJoinPool version
so that non-FJ code can remove tasks, but maybe there is
a good argument for it?

> -added getTask() to Submission

The Submission class is not even public, so this
by itself wouldn't do much good. For plumbing-level
manipulation though, perhaps you could use privileged
reflection to directly access the task field?

-Doug

From dl at cs.oswego.edu  Sun Sep  7 06:36:58 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 07 Sep 2008 06:36:58 -0400
Subject: [concurrency-interest] Phasers (were: TaskBarriers)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEMJHMAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCCEMJHMAA.dcholmes@optusnet.com.au>
Message-ID: <48C3AECA.4090007@cs.oswego.edu>

Slowly catching up ...

David Holmes wrote:
> There are seven combinations to cover here (even if not all are obviously 
> useful), covering arrival, waiting and deregistration - combined with 
> interruptible/timed versions of the blocking ones.
> 
> arriveAndAwaitAdvanceAndDeregister() etc ...

Backing up further, the underlying issue was that
arriveAndAwaitAdvance() was not equivalent to
awaitAdvance(arrive()), and similarly for
(nonexistent) methods like
arriveAndAwaitAdvanceInterruptiblyAndDeregister()
etc.

The reason they were not equivalent was that the
return value of arriveAndAwaitAdvance() was not the
phase, but the arrival number. However, with tiering
support, it is not useful to return the arrival number;
in fact there is no such unique number in general. The
only useful value to report is the new phase number.

So, now, arriveAndAwaitAdvance() == awaitAdvance(arrive()).
So the method doesn't really need to exist. But is
kept because it is convenient in the most common usages.
However, there's no good reason to define the other six.

-Doug

From dl at cs.oswego.edu  Sun Sep  7 07:22:39 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 07 Sep 2008 07:22:39 -0400
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C2CF35.1060009@cs.oswego.edu>
References: <484844FC.5040305@fh-landshut.de> <48C2CF35.1060009@cs.oswego.edu>
Message-ID: <48C3B97F.3060609@cs.oswego.edu>

Doug Lea wrote:
>> -added popQuedTask() to ForkJoinPool which returns a not-yet executed 
>> task and removes it from the pool (i am pretty sure this will not work 
>> under all conditions)
> 
> You mean a submission to a pool, right?
> That is now possible via ForkJoinWorkerThread.pollSubmission.
> ...
>> -added getTask() to Submission
> 

It occurs to me that there are other uses for this
sort of functionality (in exception recovery etc).
Here are the revised ForkJoinWorkerThread methods
that should also serve your purposes:


pollSubmission

public static ForkJoinTask<?> pollSubmission(ForkJoinPool pool)

     Removes and returns, without executing, the next task submitted for 
execution to the given pool, if one is available. To access a submission from 
the current worker's pool, use pollSubmission(getPool()). This method may be 
useful for draining tasks during exception recovery.

     Parameters:
         pool - the pool
     Returns:
         the next submission, or null if none

asSubmitted

public static <V> ForkJoinTask<V> asSubmitted(ForkJoinTask<V> t)

     If the given task t represents a submission to a ForkJoinPool (typically, 
one returned by pollSubmission), returns the actual task submitted to the pool, 
otherwise returning t itself. This method may be useful for alternate handling 
of drained tasks.

     Parameters:
         t - the task
     Returns:
         the underlying task if one exists, else t.



From dl at cs.oswego.edu  Sun Sep  7 09:29:51 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 07 Sep 2008 09:29:51 -0400
Subject: [concurrency-interest] lock acquire/release order in CHM.size()
In-Reply-To: <C9AA20BF37F14CF0AD7B37A6CECCAF97@PIGGY2>
References: <C9AA20BF37F14CF0AD7B37A6CECCAF97@PIGGY2>
Message-ID: <48C3D74F.4030903@cs.oswego.edu>

Another long delayed reply (sorry!)

Tim Eck wrote:
> Just a mild curiosity of mine that folks on this list might have insight
> into...
> 
> When operations like size() on j.u.c.ConcurrentHashMap have to resort to
> locking, the locks are acquired and released like so:
> 
>   for (int i = 0; i < segments.length; ++i) 
>       segments[i].lock();
>   /* ... */ 
>   for (int i = 0; i < segments.length; ++i)
>       segments[i].unlock();
> 
> The curious bit is that the last lock obtained is not the first lock
> released. I'm not suggesting there is anything wrong with that, but I was
> wondering if there is a specific rationale for that? I suppose two threads
> going through those loops would be less serialized since one thread could
> start acquiring in lock step with the other thread doing the unlocks. 
> 

You are right on all counts. Normally you must unlock in reverse order,
but in the particular case here, there are no possible exceptions
so no need for stack-based unwinding, so you might as well improve
pipelining.

-Doug

From jim.andreou at gmail.com  Sun Sep  7 09:34:24 2008
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Sun, 07 Sep 2008 16:34:24 +0300
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C2CF35.1060009@cs.oswego.edu>
References: <484844FC.5040305@fh-landshut.de> <48C2CF35.1060009@cs.oswego.edu>
Message-ID: <48C3D860.1060104@gmail.com>

O/H Doug Lea ??????:
>
> Further, all you really want here is to
> ensure that arguments and results are serializable. Sadly
> you can't write this as a conjunctive type like:
>    void foo([ForkJoinTask & Serializable] task)
But you can:

<T extends ForkJoinTask & Serializable> void foo(T task)

Do I miss something?

Dimitris Andreou

From dl at cs.oswego.edu  Sun Sep  7 09:56:17 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 07 Sep 2008 09:56:17 -0400
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C3D860.1060104@gmail.com>
References: <484844FC.5040305@fh-landshut.de> <48C2CF35.1060009@cs.oswego.edu>
	<48C3D860.1060104@gmail.com>
Message-ID: <48C3DD81.1030709@cs.oswego.edu>

Dimitris Andreou wrote:
>> Further, all you really want here is to ensure that arguments and results
>> are serializable.
> ... But you can:
> 
> <T extends ForkJoinTask & Serializable> void foo(T task)
> 
> Do I miss something?
> 

Thanks! I was mis-thinking that Serializable was an abstract
class (in which case you couldn't do this).
But you are right that this works fine.

(It is a little painful to write things this way if you
use them a lot though.)

-Doug



From mbien at fh-landshut.de  Sun Sep  7 12:27:04 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Sun, 07 Sep 2008 18:27:04 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C2CF35.1060009@cs.oswego.edu>
References: <484844FC.5040305@fh-landshut.de> <48C2CF35.1060009@cs.oswego.edu>
Message-ID: <48C400D8.8050602@fh-landshut.de>

Doug Lea wrote:
> Sorry for the long delay replying to this!
No problem, better late then never ;)

comments inline...

>
> Michael Bien wrote:
>> -made ForkJoinTask Serializeable
>
> We can't mandate that all ForkJoinTasks are serializable.
> In a non-JDK release maybe we could, because then we
> could just tell people creating subclasses (which is the main
> way you use FJ) to ignore the serializablility
> if they don't need it, because nothing actually relies on it.
> An alternative would be to introduce SerializableForkJoinTask,
> but then we'd need the basic exec internals fleshed out for
> the various main flavors (SerializableRecursiveAction etc)
> because we don't/can't allow people to implement these themselves.
I understand your concern but if i look at other jdk classes like JFrame
or String, i also can't tell that the marker interface contributes
anything to the functionality (who serializes JFrames?...). But on the
other hand making e.g String not serializable would make many XML
libraries harder.
Anyway I will try to workaround that.

>
> Further, all you really want here is to
> ensure that arguments and results are serializable. Sadly
> you can't write this as a conjunctive type like:
>    void foo([ForkJoinTask & Serializable] task)
> So my best suggestion, sadly enough, is to rely on
> dynamic typing, as in:
>    void foo(ForkJoinTask task) {
>       if (!(task implements Serializable)) throw...
>
> Which would then entail javadoc @param specs etc that
> spell out the otherwise unstated type requirements.
>
> (Perhaps someone has already created an annotation tag
> along these lines?)
>
> How bad would that be?
This wouldn't be bad but if the ForkJoinTask would be Serializable by
default FishFarm could distribute the ForkJoinPool with almost no code
changes.  But for more complex tasks you have to think about
serialization anyway... (but its a nice to have for presentations with
the new DistributedForkJoinPool as only code change ;-))

>
>> -added popQuedTask() to ForkJoinPool which returns a not-yet executed 
>> task and removes it from the pool (i am pretty sure this will not 
>> work under all conditions)
> buf
> You mean a submission to a pool, right?
> That is now possible via ForkJoinWorkerThread.pollSubmission.
> It seems somewhat dangerous to expose the ForkJoinPool version
> so that non-FJ code can remove tasks, but maybe there is
> a good argument for it?
well in the case of FishFarm I remove tasks from local pool before
distributing them over the network to prevent computing them multiple
times. A copy of the task is stored locally before distribution, to
restore (resubmit) the task if something went wrong (or local machine
went idle).

When a result arrives from other nodes I only have to rewire the future
object to point to the correct result because users of the FJ framework
almost certain block in isDone() or similar methods.

Thats the main reason why I require a pollTask() but I think it is in
general good to have a popX if you have a putX.

>
>> -added getTask() to Submission
>
> The Submission class is not even public, so this
> by itself wouldn't do much good. For plumbing-level
> manipulation though, perhaps you could use privileged
> reflection to directly access the task field?
Submission is Future and a wrapper for the submitted task. In theory I
don't have to deal with Submission directly, Future and the task itself
are enough. I only have to ask the submission and the task in one point
if they are done because a Submission never notices when its task was
finished with results computed somewhere else.

>
> -Doug
>
>

Thank you for integrating the changes!


best regards,

Michael



From joe.bowbeer at gmail.com  Sun Sep  7 13:35:47 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sun, 7 Sep 2008 10:35:47 -0700
Subject: [concurrency-interest] Project FishFarm;
	making a ForkJoinPool distributeable
In-Reply-To: <48C400D8.8050602@fh-landshut.de>
References: <484844FC.5040305@fh-landshut.de> <48C2CF35.1060009@cs.oswego.edu>
	<48C400D8.8050602@fh-landshut.de>
Message-ID: <31f2a7bd0809071035j4624f620u34ff48941150513e@mail.gmail.com>

On Sun, Sep 7, 2008 at 9:27 AM, Michael Bien wrote:
> Doug Lea wrote:
>>
>> Sorry for the long delay replying to this!
>
> No problem, better late then never ;)
>
> comments inline...
>
>>
>> Michael Bien wrote:
>>>
>>> -made ForkJoinTask Serializeable
>>
>> We can't mandate that all ForkJoinTasks are serializable.
>> In a non-JDK release maybe we could, because then we
>> could just tell people creating subclasses (which is the main
>> way you use FJ) to ignore the serializablility
>> if they don't need it, because nothing actually relies on it.
>> An alternative would be to introduce SerializableForkJoinTask,
>> but then we'd need the basic exec internals fleshed out for
>> the various main flavors (SerializableRecursiveAction etc)
>> because we don't/can't allow people to implement these themselves.
>
> I understand your concern but if i look at other jdk classes like JFrame
> or String, i also can't tell that the marker interface contributes
> anything to the functionality (who serializes JFrames?...). But on the
> other hand making e.g String not serializable would make many XML
> libraries harder.

FutureTask is the most analogous class and it follows the convention
that Thread-like classes are not generally Serializable.

Is there enough difference between ForkJoinTask and FutureTask to
warrant an exception in the case of ForkJoinTask?

--Joe

PS - In case you haven't seen it, check out the item "Implement
Serializable Judiciously" in Effective Java 2nd Edition.  It makes
some mention of a non-arg constructor.  I wonder if that's applicable
here.

From mbien at fh-landshut.de  Sun Sep  7 14:13:05 2008
From: mbien at fh-landshut.de (Michael Bien)
Date: Sun, 07 Sep 2008 20:13:05 +0200
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C3DD81.1030709@cs.oswego.edu>
References: <484844FC.5040305@fh-landshut.de>
	<48C2CF35.1060009@cs.oswego.edu>	<48C3D860.1060104@gmail.com>
	<48C3DD81.1030709@cs.oswego.edu>
Message-ID: <48C419B1.9080704@fh-landshut.de>


Doug Lea wrote:
> Dimitris Andreou wrote:
>>> Further, all you really want here is to ensure that arguments and 
>>> results
>>> are serializable.
>> ... But you can:
>>
>> <T extends ForkJoinTask & Serializable> void foo(T task)
>>
>> Do I miss something?
>>
>
> Thanks! I was mis-thinking that Serializable was an abstract
> class (in which case you couldn't do this).
> But you are right that this works fine.
>
> (It is a little painful to write things this way if you
> use them a lot though.)
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

I think this won't work. I can't just overwrite the T type just by 
overriding a method.

the method is defined like this:

public interface ForkJoinExecutor {
    public <T> Future<T> submit(ForkJoinTask<T> task);
...
}

to force serializable results I could do this:
@Override
public <Serializable> Future<Serializable> 
submit(ForkJoinTask<Serializable> task) {
...
}

but only by overriding the method I can't force the caller to insert 
serializable tasks (except inctanceof of course).
I am no expert in generics but AFAIK the interface which would allow 
that would look like this:

//R means result; T means task
public interface ForkJoinExecutor<T extends ForkJoinTask<?>> {
    public <R> Future<R> submit(T task);
}

then I could do something like that:

public class DistributedForkJoinPool<S extends 
ForkJoinTask<Serializable> & Serializable> extends ForkJoinPool<S> {

    public <Serializable> void execute(ForkJoinTask<Serializable> task) 
{   }
    public <Serializable> Serializable invoke(ForkJoinTask<Serializable> 
task) {    }
    public <Serializable> Future<Serializable> submit(S task) {    }
    public int getParallelismLevel() {    }

}

caller/user:

    public class MySerializableTask extends ForkJoinTask<Serializable> 
implements Serializable {  ...  }

... new DistributedForkJoinPool<MySerializableTask>()
or:
... new ForkJoinPool()


(precondition is that ForkJoinPool passes all generified types to the 
executor interface)


 it looks pretty ugly from the api point of view but I guess this is the 
nature of generics... easy use but very verbose internal api code.

regards,

-michael


From wangbing.nudt at gmail.com  Mon Sep  8 05:14:24 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Mon, 8 Sep 2008 11:14:24 +0200
Subject: [concurrency-interest] question on the might-be deadlock
Message-ID: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>

Dear All:
         I am trying to use the Findbugs to collect some advice on my
multi thread program ,and they give me a very interesting might-be bug
that I don't understand:
which is below :
The code synchronizes on a boxed primitive constant, such as an Integer.
private static Integer count = 0;
...
  synchronized(count) {
     count++;
     }
...

Since Integer objects can be cached and shared, this code could be
synchronizing on the same object as other, unrelated code, leading to
unresponsiveness and possible deadlock

what is the meaing of it ,in my case , I use a
 public Integer logicalProcessState = 1;// 1 2 3
and use it like
 synchronized (this.logicalProcessState) {

 ......

    this.logicalProcessState = 2;
      }

could that lead to dead lock ,or I just add a Volatile  in front of it?
thanks !
-- 
Wang Bing
bing.wang at uni-rostock.de
bw107 at uni-rostock.de
QQ:14757617

From matthias at mernst.org  Mon Sep  8 05:29:43 2008
From: matthias at mernst.org (Matthias Ernst)
Date: Mon, 8 Sep 2008 11:29:43 +0200
Subject: [concurrency-interest] question on the might-be deadlock
In-Reply-To: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
References: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
Message-ID: <22ec15240809080229g64d173fdg477c48fc5cab064a@mail.gmail.com>

Wang Bing,

you should never synchronize this way:

var = something;
synchronized(var) {
  var = something_else;
}

since var is changing its value, one thread might sync on the old
value, another one on the new value and they both execute. You must
synchronize an action on something that is invariable with respect to
that action.

What are you exactly trying to achieve? If you want a thread to
execute something depending on the logical process state (the three
dots) and then switch the process state to a new value, you'd probably
want to synchronize on a "private final Object lock = new Object();"
which is also a field of your process state. Other threads will have
to wait while this happens.

Matthias


On Mon, Sep 8, 2008 at 11:14 AM, bing wang <wangbing.nudt at gmail.com> wrote:
> Dear All:
>         I am trying to use the Findbugs to collect some advice on my
> multi thread program ,and they give me a very interesting might-be bug
> that I don't understand:
> which is below :
> The code synchronizes on a boxed primitive constant, such as an Integer.
> private static Integer count = 0;
> ...
>  synchronized(count) {
>     count++;
>     }
> ...
>
> Since Integer objects can be cached and shared, this code could be
> synchronizing on the same object as other, unrelated code, leading to
> unresponsiveness and possible deadlock
>
> what is the meaing of it ,in my case , I use a
>  public Integer logicalProcessState = 1;// 1 2 3
> and use it like
>  synchronized (this.logicalProcessState) {
>
>  ......
>
>    this.logicalProcessState = 2;
>      }
>
> could that lead to dead lock ,or I just add a Volatile  in front of it?
> thanks !
> --
> Wang Bing
> bing.wang at uni-rostock.de
> bw107 at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From mallikap at deshaw.com  Mon Sep  8 05:43:37 2008
From: mallikap at deshaw.com (Mallikarjunaiah, Praveena)
Date: Mon, 8 Sep 2008 15:13:37 +0530
Subject: [concurrency-interest] question on the might-be deadlock
In-Reply-To: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
References: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
Message-ID: <B911686319FBAC4BA4671AF36F8DC30903C81DF0@mailhyd2.hyd.deshaw.com>

You need to worry about other issues than just caching of integer
objects.

Your code doesn't gurantee mutual exclusivity. 

Assume, the current state is 1 and two threads A & B are trying to enter
critical secion, Thread A succeeds whereas B is waiting on integer
object
Representing 1. Now, thread-A goes on modifies logical state to 2 and
gives up the lock on Integer 1. Now, thread to acquires the lock on
Integer 1
and continues with rest of critial section. Meanwhile, if a new thread
comes and asks for lock on state, it will get the lock because it is
trying to lock on a different object itself.

So, you should always synchronize only on final fields.

-Praveen

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of bing
wang
Sent: Monday, September 08, 2008 2:44 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] question on the might-be deadlock

Dear All:
         I am trying to use the Findbugs to collect some advice on my
multi thread program ,and they give me a very interesting might-be bug
that I don't understand:
which is below :
The code synchronizes on a boxed primitive constant, such as an Integer.
private static Integer count = 0;
...
  synchronized(count) {
     count++;
     }
...

Since Integer objects can be cached and shared, this code could be
synchronizing on the same object as other, unrelated code, leading to
unresponsiveness and possible deadlock

what is the meaing of it ,in my case , I use a
 public Integer logicalProcessState = 1;// 1 2 3
and use it like
 synchronized (this.logicalProcessState) {

 ......

    this.logicalProcessState = 2;
      }

could that lead to dead lock ,or I just add a Volatile  in front of it?
thanks !
-- 
Wang Bing
bing.wang at uni-rostock.de
bw107 at uni-rostock.de
QQ:14757617
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Mon Sep  8 07:28:10 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 08 Sep 2008 07:28:10 -0400
Subject: [concurrency-interest] Project FishFarm;
 making a ForkJoinPool distributeable
In-Reply-To: <48C419B1.9080704@fh-landshut.de>
References: <484844FC.5040305@fh-landshut.de>
	<48C2CF35.1060009@cs.oswego.edu>	<48C3D860.1060104@gmail.com>
	<48C3DD81.1030709@cs.oswego.edu> <48C419B1.9080704@fh-landshut.de>
Message-ID: <48C50C4A.5040502@cs.oswego.edu>

Michael Bien wrote:
> 
> I think this won't work. I can't just overwrite the T type just by 
> overriding a method.
> 

Yes, I now agree that all solutions along these lines are much
worse than declaring ForkJoinTask Serializable and making
a few other adjustments.

And on further thought, it is in keeping with many JDK usages
to declare such classes Serializable and leave responsibility
for breakage in subclasses to users. We made a similar
decision about j.u.c Lock classes.

Joe: One reason ForkJoinTasks are amenable to remote execution
is that they do not include non-serializable state: They access
schedule control etc by relying on the fact they are executing
in ForkJoinWorkerThreads so cast Thread.currentThread() to access.
So long as you only serialize before or after execution (which
are the only sensible things to do anyway) all is well.

The only problematic case is the internal Submission class
that must simultaneously act as a Future for external usages
and as a ForkJoinTask for internal ones. It cannot be
Serializable (it intrinsically needs a reference to its
ForkJoinPool, which is intrinsically non-serializable).
Because this is not a public class, this arises only
if someone type-probes the object returned by submit:
   future = pool.submit(task);
   if (future instanceof ForkJoinTask)
     stream.writeObject((ForkJoinTask)future); // blows up
But I think we can add a disclaimer somewhere stating that
this is not a supported usage. Which it isn't, even ignoring
serialization -- calling, say, ((ForkJoinTask)future).fork()
is equally problematic.

(Avoiding this is why Michael needs the utility to
to extract the actual task. But under above scheme should
have a Future, not ForkJoinTask param.)

Unless I hear objections, I'll incorporate the various
minor changes needed to put this into place.

-Doug


From wangbing.nudt at gmail.com  Mon Sep  8 08:41:02 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Mon, 8 Sep 2008 14:41:02 +0200
Subject: [concurrency-interest] question on the might-be deadlock
In-Reply-To: <B911686319FBAC4BA4671AF36F8DC30903C81DF0@mailhyd2.hyd.deshaw.com>
References: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
	<B911686319FBAC4BA4671AF36F8DC30903C81DF0@mailhyd2.hyd.deshaw.com>
Message-ID: <3e257e530809080541l3eb62588hc0b85467774e8869@mail.gmail.com>

Thanks for the help from our maillist:)
In order to prevent the autoboxing problem ,I use the method below ,it
is a little bit ugly.

  protected class LogicalProcessState{
    public int innerNumber;
}


synchronized (this.logicalProcessState) {
        this.logicalProcessState.innerNumber = 1;
      }




On Mon, Sep 8, 2008 at 11:43 AM, Mallikarjunaiah, Praveena
<mallikap at deshaw.com> wrote:
> You need to worry about other issues than just caching of integer
> objects.
>
> Your code doesn't gurantee mutual exclusivity.
>
> Assume, the current state is 1 and two threads A & B are trying to enter
> critical secion, Thread A succeeds whereas B is waiting on integer
> object
> Representing 1. Now, thread-A goes on modifies logical state to 2 and
> gives up the lock on Integer 1. Now, thread to acquires the lock on
> Integer 1
> and continues with rest of critial section. Meanwhile, if a new thread
> comes and asks for lock on state, it will get the lock because it is
> trying to lock on a different object itself.
>
> So, you should always synchronize only on final fields.
>
> -Praveen
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of bing
> wang
> Sent: Monday, September 08, 2008 2:44 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] question on the might-be deadlock
>
> Dear All:
>         I am trying to use the Findbugs to collect some advice on my
> multi thread program ,and they give me a very interesting might-be bug
> that I don't understand:
> which is below :
> The code synchronizes on a boxed primitive constant, such as an Integer.
> private static Integer count = 0;
> ...
>  synchronized(count) {
>     count++;
>     }
> ...
>
> Since Integer objects can be cached and shared, this code could be
> synchronizing on the same object as other, unrelated code, leading to
> unresponsiveness and possible deadlock
>
> what is the meaing of it ,in my case , I use a
>  public Integer logicalProcessState = 1;// 1 2 3
> and use it like
>  synchronized (this.logicalProcessState) {
>
>  ......
>
>    this.logicalProcessState = 2;
>      }
>
> could that lead to dead lock ,or I just add a Volatile  in front of it?
> thanks !
> --
> Wang Bing
> bing.wang at uni-rostock.de
> bw107 at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Wang Bing
bing.wang at uni-rostock.de
bw107 at uni-rostock.de
QQ:14757617

From Thomas.Hawtin at Sun.COM  Mon Sep  8 10:37:00 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Mon, 08 Sep 2008 15:37:00 +0100
Subject: [concurrency-interest] question on the might-be deadlock
In-Reply-To: <3e257e530809080541l3eb62588hc0b85467774e8869@mail.gmail.com>
References: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
	<B911686319FBAC4BA4671AF36F8DC30903C81DF0@mailhyd2.hyd.deshaw.com>
	<3e257e530809080541l3eb62588hc0b85467774e8869@mail.gmail.com>
Message-ID: <48C5388C.6010001@sun.com>

bing wang wrote:
> In order to prevent the autoboxing problem ,I use the method below ,it
> is a little bit ugly.
> 
>   protected class LogicalProcessState{
>     public int innerNumber;
> }
> 
> 
> synchronized (this.logicalProcessState) {
>         this.logicalProcessState.innerNumber = 1;
>       }

Less ugly would be:

    private static class LogicalProcessState {
        private int innerNumber; // private!
        public synchronized void reset() {
            innerNumber = 1;
        }
        ...
    }
    private final LogicalProcessState logicalProcessState =
        new LogicalProcessState();

Alternatively, you might well find that 
java.util.concurrent.AtomicInteger does what you want (faster).

Tom Hawtin

From wangbing.nudt at gmail.com  Tue Sep  9 03:01:28 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Tue, 9 Sep 2008 09:01:28 +0200
Subject: [concurrency-interest] serialization or migeration precondition
Message-ID: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>

Dear All:
I would like to have one of my class A totally Sierializible so I can
migerate it to another JVM when there r too many thread in one JVM.
but it seems somethingwrong when A have a reference to another class B:

There are 2 class ,A and B

Class  A implements Sierializible{
private static final long serialVersionUID = -783680237161881281L;
  private  B varB1;
  protected B varB2;
  public B varB3;
}

Then I got the warning on both three field in FindBugs Below:
[Se] Non-transient non-serializable instance field in serializable
class [SE_BAD_FIELD]
This Serializable class defines a non-primitive instance field which
is neither transient, Serializable, or java.lang.Object, and does not
appear to implement the Externalizable interface or the readObject()
and writeObject() methods.  Objects of this class will not be
deserialized correctly if a non-Serializable object is stored in this
field.

How could I change my code
Must I let Class B also implements Serializable if I would like to use
the varBx also after a
deserialized ?

Danke!

-- 
Wang Bing
bing.wang at uni-rostock.de
bw107 at uni-rostock.de
QQ:14757617

From dcholmes at optusnet.com.au  Tue Sep  9 03:42:22 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 9 Sep 2008 17:42:22 +1000
Subject: [concurrency-interest] serialization or migeration precondition
In-Reply-To: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEJKHNAA.dcholmes@optusnet.com.au>

This is off-topic for this mailing list but as you're new here ...

It sounds like you need to do a bit of reading on serialization. It's
covered quite well in "The Java Programming Language" and I think Josh
probably has an item or two in "Effective Java" - but any decent Java
book/tutorial will cover this.

You either need to make class B Serializable (and all instances it
transitively holds) so that the entire object graph can be saved and
restored; or you declare your B fields as transient (so they don't get
serialized) and you then use writeObject and readObject to save/restore the
state needed to reconstruct suitable B instances when an A is deserialized.
Only you can determine what needs to be done to save/restore a B instance as
it depends on what B really is.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of bing
> wang
> Sent: Tuesday, 9 September 2008 5:01 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] serialization or migeration precondition
>
>
> Dear All:
> I would like to have one of my class A totally Sierializible so I can
> migerate it to another JVM when there r too many thread in one JVM.
> but it seems somethingwrong when A have a reference to another class B:
>
> There are 2 class ,A and B
>
> Class  A implements Sierializible{
> private static final long serialVersionUID = -783680237161881281L;
>   private  B varB1;
>   protected B varB2;
>   public B varB3;
> }
>
> Then I got the warning on both three field in FindBugs Below:
> [Se] Non-transient non-serializable instance field in serializable
> class [SE_BAD_FIELD]
> This Serializable class defines a non-primitive instance field which
> is neither transient, Serializable, or java.lang.Object, and does not
> appear to implement the Externalizable interface or the readObject()
> and writeObject() methods.  Objects of this class will not be
> deserialized correctly if a non-Serializable object is stored in this
> field.
>
> How could I change my code
> Must I let Class B also implements Serializable if I would like to use
> the varBx also after a
> deserialized ?
>
> Danke!
>
> --
> Wang Bing
> bing.wang at uni-rostock.de
> bw107 at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From joe.bowbeer at gmail.com  Tue Sep  9 03:53:20 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 9 Sep 2008 00:53:20 -0700
Subject: [concurrency-interest] serialization or migeration precondition
In-Reply-To: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>
References: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>
Message-ID: <31f2a7bd0809090053t62f108i69b966ac4f25cdf1@mail.gmail.com>

Can you rephrase your question in terms of concurrent programming? :)

As the warning hints, you can forget about serializing B if you declare your
variables to be "transient".

 private transient B varB1;
 // etc.

But if you need these B instances after deserialization, then they'll need
to be serialized.

For more details, I recommend you turn to the excellent chapter on
serialization in Effective Java 2nd Ed.

http://java.sun.com/docs/books/effective/

--Joe

On Tue, Sep 9, 2008 at 12:01 AM, bing wang wrote:

> Dear All:
> I would like to have one of my class A totally Sierializible so I can
> migerate it to another JVM when there r too many thread in one JVM.
> but it seems somethingwrong when A have a reference to another class B:
>
> There are 2 class ,A and B
>
> Class  A implements Sierializible{
> private static final long serialVersionUID = -783680237161881281L;
>  private  B varB1;
>  protected B varB2;
>  public B varB3;
> }
>
> Then I got the warning on both three field in FindBugs Below:
> [Se] Non-transient non-serializable instance field in serializable
> class [SE_BAD_FIELD]
> This Serializable class defines a non-primitive instance field which
> is neither transient, Serializable, or java.lang.Object, and does not
> appear to implement the Externalizable interface or the readObject()
> and writeObject() methods.  Objects of this class will not be
> deserialized correctly if a non-Serializable object is stored in this
> field.
>
> How could I change my code
> Must I let Class B also implements Serializable if I would like to use
> the varBx also after a
> deserialized ?
>
> Danke!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080909/8ccbae76/attachment.html>

From mbien at fh-landshut.de  Tue Sep  9 04:01:48 2008
From: mbien at fh-landshut.de (mbien at fh-landshut.de)
Date: Tue, 09 Sep 2008 10:01:48 +0200
Subject: [concurrency-interest] serialization or migeration	precondition
In-Reply-To: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>
References: <3e257e530809090001h7a913956td7c5a687ef21c61c@mail.gmail.com>
Message-ID: <20080909100148.4550r96vgg4sg0w8@webmail.fh-landshut.de>

Zitat von bing wang <wangbing.nudt at gmail.com>:

> Dear All:
> I would like to have one of my class A totally Sierializible so I can
> migerate it to another JVM when there r too many thread in one JVM.
> but it seems somethingwrong when A have a reference to another class B:
>
> There are 2 class ,A and B
>
> Class  A implements Sierializible{
> private static final long serialVersionUID = -783680237161881281L;
>   private  B varB1;
>   protected B varB2;
>   public B varB3;
> }
>
> Then I got the warning on both three field in FindBugs Below:
> [Se] Non-transient non-serializable instance field in serializable
> class [SE_BAD_FIELD]
> This Serializable class defines a non-primitive instance field which
> is neither transient, Serializable, or java.lang.Object, and does not
> appear to implement the Externalizable interface or the readObject()
> and writeObject() methods.  Objects of this class will not be
> deserialized correctly if a non-Serializable object is stored in this
> field.
>
> How could I change my code
> Must I let Class B also implements Serializable if I would like to use
> the varBx also after a
> deserialized ?
yes, everything (dependencies included) you want to  
serialize/deserialize must implement Serializable.

if you don't want to serialize a specific object just use transient  
(static has also the same side effect) in the declaration, e.g:

//don't persist temp
private transient B temp;

btw: this is probably not the right mailing list for general questions  
on serialization ;-)

>
> Danke!
Bitte!

-michael

>
> --
> Wang Bing
> bing.wang at uni-rostock.de
> bw107 at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>





From ben_manes at yahoo.com  Tue Sep  9 04:41:16 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 9 Sep 2008 01:41:16 -0700 (PDT)
Subject: [concurrency-interest] serialization or migeration precondition
Message-ID: <92502.62127.qm@web38808.mail.mud.yahoo.com>

Yes.  The entire graph must be serializable.  If it is transient or static, then it does not need to be serialized as it will not be part of the byte stream.  You can recreate transient fields during deserialization, though.

That said, serialization is a fairly nasty approach for distributed systems due to versioning and class dependencies.  If at all possible it is much nicer to have a externalized form that is not class dependent, such as an XML representation.  If you do rolling updates or hot-patches, you may find yourself stuck having to update the entire cluster.

Component Development for the Java Platform (free download) has the best section on serialization that I have come across.  Effective Java does a good job of warning you of the implications when using it.



----- Original Message ----
From: bing wang <wangbing.nudt at gmail.com>
To: concurrency-interest at cs.oswego.edu
Sent: Tuesday, September 9, 2008 12:01:28 AM
Subject: [concurrency-interest] serialization or migeration precondition

Dear All:
I would like to have one of my class A totally Sierializible so I can
migerate it to another JVM when there r too many thread in one JVM.
but it seems somethingwrong when A have a reference to another class B:

There are 2 class ,A and B

Class  A implements Sierializible{
private static final long serialVersionUID = -783680237161881281L;
  private  B varB1;
  protected B varB2;
  public B varB3;
}

Then I got the warning on both three field in FindBugs Below:
[Se] Non-transient non-serializable instance field in serializable
class [SE_BAD_FIELD]
This Serializable class defines a non-primitive instance field which
is neither transient, Serializable, or java.lang.Object, and does not
appear to implement the Externalizable interface or the readObject()
and writeObject() methods.  Objects of this class will not be
deserialized correctly if a non-Serializable object is stored in this
field.

How could I change my code
Must I let Class B also implements Serializable if I would like to use
the varBx also after a
deserialized ?

Danke!

-- 
Wang Bing
bing.wang at uni-rostock.de
bw107 at uni-rostock.de
QQ:14757617
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080909/22a9723d/attachment.html>

From markus.kohler at gmail.com  Tue Sep  9 05:05:43 2008
From: markus.kohler at gmail.com (Markus Kohler)
Date: Tue, 9 Sep 2008 11:05:43 +0200
Subject: [concurrency-interest] question on the might-be deadlock
In-Reply-To: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
References: <3e257e530809080214y4e3f8aa9y72ad139f5659856d@mail.gmail.com>
Message-ID: <771905290809090205y82d0da4yb1d4ce0cf247cef3@mail.gmail.com>

Hi, As others have pointed out, the code is not correct for other reasons.
But still the message that findbugs generated makes sense.

the Integer class caches small numbers. Just look at the code:

static final Integer cache[] = new Integer[-(-128) + 127 + 1];

static {
    for(int i = 0; i < cache.length; i++)
cache[i] = new Integer(i - 128);
}
    }

Therefore your lock could suddenly become a "global" lock, which can cause
performance problems

Regards,
Markus ( http://kohlerm.blogspot.com/)

On Mon, Sep 8, 2008 at 11:14 AM, bing wang <wangbing.nudt at gmail.com> wrote:

> Dear All:
>         I am trying to use the Findbugs to collect some advice on my
> multi thread program ,and they give me a very interesting might-be bug
> that I don't understand:
> which is below :
> The code synchronizes on a boxed primitive constant, such as an Integer.
> private static Integer count = 0;
> ...
>  synchronized(count) {
>     count++;
>     }
> ...
>
> Since Integer objects can be cached and shared, this code could be
> synchronizing on the same object as other, unrelated code, leading to
> unresponsiveness and possible deadlock
>
> what is the meaing of it ,in my case , I use a
>  public Integer logicalProcessState = 1;// 1 2 3
> and use it like
>  synchronized (this.logicalProcessState) {
>
>  ......
>
>    this.logicalProcessState = 2;
>      }
>
> could that lead to dead lock ,or I just add a Volatile  in front of it?
> thanks !
> --
> Wang Bing
> bing.wang at uni-rostock.de
> bw107 at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080909/02afb67a/attachment-0001.html>

From hallorant at gmail.com  Wed Sep 10 16:40:52 2008
From: hallorant at gmail.com (Tim Halloran)
Date: Wed, 10 Sep 2008 16:40:52 -0400
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <48063B71.3010004@redhat.com>
References: <48063B71.3010004@redhat.com>
Message-ID: <a36ab4bc0809101340g24a6d9f7jd5f9a6e80c335286@mail.gmail.com>

Jason & Doug,

We are still using this collection and we uncovered an unusual behavior.  If
values placed in the map are of a subclass of
java.lang.ref.Reference<T>then the
get() method of the collection always calls value.get() rather than passing
back the value object the collection was given.  This is because the
implementation assumes it wrapped the value in the reference type, but, as
in our case, this could have come from client code.

Here is my declaration:

 private static final ConcurrentMap<Class,ClassPhantomReference>
f_classToPhantom =
    new ConcurrentReferenceHashMap<Class,ClassPhantomReference>(
      ReferenceType.WEAK, ReferenceType.STRONG, true);

where ClassPhantomReference is a subclass of PhantomReference.  As you know
calling the get() method on any instance of PhantomReference returns a
null.  This took hours for Edwin Chan, whom I work with, to track down so I
wanted to pass it along as it is mighty confusing and probably should be
changed.

Best Regards,
Tim Halloran


On Wed, Apr 16, 2008 at 1:46 PM, Jason T. Greene <jason.greene at redhat.com>wrote:

> We are currently considering the inclusion of a ConcurrentReferenceMap
> into a JSR-166 update. This map supports any combination of weak, soft,
> or strong keys and values with similar performance characteristics to
> ConcurrentHashMap (non-blocking reads, striped writes etc). This map can
> also be configured to use identity checking instead of standard equality
> when locating keys (similar to IdentityHashMap).
>
> At this point, we are looking for feedback, in particular on the API.
>
> Some of the open questions are:
>
> 1) Should the IDENTITY_COMPARISONS option apply to values and keys, or
> just to keys as it does now? If not, should there be separate options
> for both?
>
> 2) Should the key reference type, and the value reference type be merged
> into the option enum, instead of providing separate parameters? This
> reduces the number of overloaded constructors, but introduces the
> problem of having a combination of mutually exclusive options (WEAK_KEYS
> + SOFT_KEYS).
>
> 3) Should the configuration values be exposed via get methods so that
> calling code can introspect the map configuration? Currently none of the
> standard collections allow you to do this (load factor, etc).
>
> 4) Should behavioral options be exposed as booleans instead? With only
> one option this makes since; however, depending on the answer to
> question number 1, which would add another option, and some expected GC
> changes to the JVM that would add yet another one, this would introduce
> way too many overloaded constructors.
>
> The javadoc for the initial version is available here:
> http://www.stacksmash.com/jsr166y
>
> The source code (public domain) is available here:
>
> http://viewvc.jboss.org/cgi-bin/viewvc.cgi/jbosscache/experimental/jsr166/src/jsr166y/ConcurrentReferenceHashMap.java?view=markup
>
> It should be noted that the API and implementation are extremely subject
> to change, and are also of pre-release quality, so it is not recommended
> for production usage. Also, these links are just temporary.
>
> Thanks!
>
> --
> Jason T. Greene
> JBoss, a division of Red Hat
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080910/30de1353/attachment.html>

From jason.greene at redhat.com  Wed Sep 10 18:19:44 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Wed, 10 Sep 2008 17:19:44 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a36ab4bc0809101340g24a6d9f7jd5f9a6e80c335286@mail.gmail.com>
References: <48063B71.3010004@redhat.com>
	<a36ab4bc0809101340g24a6d9f7jd5f9a6e80c335286@mail.gmail.com>
Message-ID: <48C84800.6030807@redhat.com>

Thanks. I corrected the assumption in the implementation.

Tim Halloran wrote:
> Jason & Doug,
> 
> We are still using this collection and we uncovered an unusual 
> behavior.  If values placed in the map are of a subclass of 
> java.lang.ref.Reference<T> then the get() method of the collection 
> always calls value.get() rather than passing back the value object the 
> collection was given.  This is because the implementation assumes it 
> wrapped the value in the reference type, but, as in our case, this could 
> have come from client code.
> 
> Here is my declaration:
> 
>  private static final ConcurrentMap<Class,ClassPhantomReference> 
> f_classToPhantom =
>     new ConcurrentReferenceHashMap<Class,ClassPhantomReference>(
>       ReferenceType.WEAK, ReferenceType.STRONG, true);
> 
> where ClassPhantomReference is a subclass of PhantomReference.  As you 
> know calling the get() method on any instance of PhantomReference 
> returns a null.  This took hours for Edwin Chan, whom I work with, to 
> track down so I wanted to pass it along as it is mighty confusing and 
> probably should be changed.
> 
> Best Regards,
> Tim Halloran
> 
> 
> On Wed, Apr 16, 2008 at 1:46 PM, Jason T. Greene 
> <jason.greene at redhat.com <mailto:jason.greene at redhat.com>> wrote:
> 
>     We are currently considering the inclusion of a ConcurrentReferenceMap
>     into a JSR-166 update. This map supports any combination of weak, soft,
>     or strong keys and values with similar performance characteristics to
>     ConcurrentHashMap (non-blocking reads, striped writes etc). This map can
>     also be configured to use identity checking instead of standard equality
>     when locating keys (similar to IdentityHashMap).
> 
>     At this point, we are looking for feedback, in particular on the API.
> 
>     Some of the open questions are:
> 
>     1) Should the IDENTITY_COMPARISONS option apply to values and keys, or
>     just to keys as it does now? If not, should there be separate options
>     for both?
> 
>     2) Should the key reference type, and the value reference type be merged
>     into the option enum, instead of providing separate parameters? This
>     reduces the number of overloaded constructors, but introduces the
>     problem of having a combination of mutually exclusive options (WEAK_KEYS
>     + SOFT_KEYS).
> 
>     3) Should the configuration values be exposed via get methods so that
>     calling code can introspect the map configuration? Currently none of the
>     standard collections allow you to do this (load factor, etc).
> 
>     4) Should behavioral options be exposed as booleans instead? With only
>     one option this makes since; however, depending on the answer to
>     question number 1, which would add another option, and some expected GC
>     changes to the JVM that would add yet another one, this would introduce
>     way too many overloaded constructors.
> 
>     The javadoc for the initial version is available here:
>     http://www.stacksmash.com/jsr166y
> 
>     The source code (public domain) is available here:
>     http://viewvc.jboss.org/cgi-bin/viewvc.cgi/jbosscache/experimental/jsr166/src/jsr166y/ConcurrentReferenceHashMap.java?view=markup
> 
>     It should be noted that the API and implementation are extremely subject
>     to change, and are also of pre-release quality, so it is not recommended
>     for production usage. Also, these links are just temporary.
> 
>     Thanks!
> 
>     --
>     Jason T. Greene
>     JBoss, a division of Red Hat
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at altair.cs.oswego.edu
>     <mailto:Concurrency-interest at altair.cs.oswego.edu>
>     http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


-- 
Jason T. Greene
JBoss, a division of Red Hat

From bmeike at speakeasy.net  Wed Sep 10 22:09:45 2008
From: bmeike at speakeasy.net (Blake Meike)
Date: Wed, 10 Sep 2008 19:09:45 -0700
Subject: [concurrency-interest] Barriers
Message-ID: <70C4AD7A-DAC2-4B0A-BAD8-6D9F632D9F30@speakeasy.net>

Hey there.

I was discussing the implementation of a latch with a colleague, the  
other day.  The idea is that, on a machine with n processors, we would  
like to start m <= n threads running in some section of code at  
exactly the same time.  We could both come up with implementations  
that come pretty close, something like:

synchronised (this) {
     waiting++;
     while (waiting < registered) {
         try { this.wait(); }
        catch (InterruptedException) {}
     }
}
this.notifyAll();

One problem with this, that neither of us could see a way to avoid, is  
that, as each thread exits the wait() method, as we understand it, has  
seized the monitor on 'this', something that only one processor at a  
time can do.

I actually did try to follow the implementation of CyclicBarrier and  
got as far as a class called Unsafe, which seems to be deep magic.

Would any of you take a minute to enlighten me?

Thanks!
   -blake

From dcholmes at optusnet.com.au  Wed Sep 10 23:21:27 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 11 Sep 2008 13:21:27 +1000
Subject: [concurrency-interest] Barriers
In-Reply-To: <70C4AD7A-DAC2-4B0A-BAD8-6D9F632D9F30@speakeasy.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKEHNAA.dcholmes@optusnet.com.au>

Mike,

If the key phrase is "exactly the same time" then no traditional
synchronization mechanism will do this, as there will always be a "convoy"
through the mutual exclusion mechanism at some level. However you could then
use a secondary busy-wait "barrier" to make all the threads pile up after
the blocking synchronization:


AtomicInteger readyThreads = new AtomicInteger(0);
volatile boolean ready = false;

...

blockingBarrier.await(); // CyclicBarrier will do

if (readyThreads.getAndIncrement() != registered) {
  while (!ready) ; // spin
}
else {
  ready = true;
}
// all threads can proceed now

Of course busy-waits must be used with due care and consideration. ;-)

By the way, Unsafe() is just a way to write Java code that is turned into
known machine code, for executing atomic instructions for example.

Minor nit: your notifyAll() needs to be in the synchronizede block.

Hope this helps.
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Blake
> Meike
> Sent: Thursday, 11 September 2008 12:10 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Barriers
>
>
> Hey there.
>
> I was discussing the implementation of a latch with a colleague, the
> other day.  The idea is that, on a machine with n processors, we would
> like to start m <= n threads running in some section of code at
> exactly the same time.  We could both come up with implementations
> that come pretty close, something like:
>
> synchronised (this) {
>      waiting++;
>      while (waiting < registered) {
>          try { this.wait(); }
>         catch (InterruptedException) {}
>      }
> }
> this.notifyAll();
>
> One problem with this, that neither of us could see a way to avoid, is
> that, as each thread exits the wait() method, as we understand it, has
> seized the monitor on 'this', something that only one processor at a
> time can do.
>
> I actually did try to follow the implementation of CyclicBarrier and
> got as far as a class called Unsafe, which seems to be deep magic.
>
> Would any of you take a minute to enlighten me?
>
> Thanks!
>    -blake
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From hallorant at gmail.com  Thu Sep 11 10:56:32 2008
From: hallorant at gmail.com (Tim Halloran)
Date: Thu, 11 Sep 2008 10:56:32 -0400
Subject: [concurrency-interest] Barriers
In-Reply-To: <70C4AD7A-DAC2-4B0A-BAD8-6D9F632D9F30@speakeasy.net>
References: <70C4AD7A-DAC2-4B0A-BAD8-6D9F632D9F30@speakeasy.net>
Message-ID: <a36ab4bc0809110756i2ac9f98x5457f4e114361b44@mail.gmail.com>

Blake,

Not sure what you are up to, but I don't think "exactly" is possible.  That
said, you might look at page 96 of Goetz's Java Concurrency In Practice.
This shows code for how to use a CountDownLatch to have test code start n
threads running as close together as possible.  The book is using this for
testing but this might fit your requirements as well.

Regards,
Tim Halloran

On Wed, Sep 10, 2008 at 10:09 PM, Blake Meike <bmeike at speakeasy.net> wrote:

> Hey there.
>
> I was discussing the implementation of a latch with a colleague, the other
> day.  The idea is that, on a machine with n processors, we would like to
> start m <= n threads running in some section of code at exactly the same
> time.  We could both come up with implementations that come pretty close,
> something like:
>
> synchronised (this) {
>    waiting++;
>    while (waiting < registered) {
>        try { this.wait(); }
>       catch (InterruptedException) {}
>    }
> }
> this.notifyAll();
>
> One problem with this, that neither of us could see a way to avoid, is
> that, as each thread exits the wait() method, as we understand it, has
> seized the monitor on 'this', something that only one processor at a time
> can do.
>
> I actually did try to follow the implementation of CyclicBarrier and got as
> far as a class called Unsafe, which seems to be deep magic.
>
> Would any of you take a minute to enlighten me?
>
> Thanks!
>  -blake
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080911/9e8b3292/attachment-0001.html>

From bomba at cboe.com  Thu Sep 11 12:29:11 2008
From: bomba at cboe.com (Bomba, Craig)
Date: Thu, 11 Sep 2008 11:29:11 -0500
Subject: [concurrency-interest] Reusable Latch
Message-ID: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>

Other than a CyclicBarrier, what recommendations are out there for a reusable (i.e. get around the one-shot phenomenon) CountDownLatch?

Thanks




From jnguyen at automotive.com  Thu Sep 11 12:43:33 2008
From: jnguyen at automotive.com (Joe Nguyen)
Date: Thu, 11 Sep 2008 09:43:33 -0700
Subject: [concurrency-interest] Shutdown idle SingleThreadExecutor
In-Reply-To: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
References: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
Message-ID: <A7487BEC58632D46B8C7E8BC69B3EB120BE9125A@mail-001.corp.automotive.com>

Hi,
    Is there a way to config SingelThreadExecutor so that it will
shutdown itself after being idle for over 10 minutes? 
Thanks


From ed.bayudan at prudential.com  Thu Sep 11 13:48:11 2008
From: ed.bayudan at prudential.com (ed.bayudan at prudential.com)
Date: Thu, 11 Sep 2008 13:48:11 -0400
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
Message-ID: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a blocking
queue and I'd like to be able to refill the queue from the database once I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not want
it to block and wait; rather I'd like it to notify the producer.

Thanks.




From jnguyen at automotive.com  Thu Sep 11 14:07:09 2008
From: jnguyen at automotive.com (Joe Nguyen)
Date: Thu, 11 Sep 2008 11:07:09 -0700
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
References: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
	<OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
Message-ID: <A7487BEC58632D46B8C7E8BC69B3EB120BE9125D@mail-001.corp.automotive.com>

If you have a single producer, the producer could produce a special task
at the end.  Once the consumer sees the special task, it will notify the
producer.  If you have multiple producers, the consumer could poll the
queue for a specific time interval, if it receives null, it notify the
producers.

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of
ed.bayudan at prudential.com
Sent: Thursday, September 11, 2008 10:48 Joe
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Sending a Re: Reusable Latch

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a
blocking
queue and I'd like to be able to refill the queue from the database once
I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces
refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not
want
it to block and wait; rather I'd like it to notify the producer.

Thanks.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From chris.winters at healthcare.vocollect.com  Thu Sep 11 14:13:08 2008
From: chris.winters at healthcare.vocollect.com (Chris Winters)
Date: Thu, 11 Sep 2008 14:13:08 -0400
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
References: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
Message-ID: <48C95FB4.7010203@healthcare.vocollect.com>


ed.bayudan at prudential.com wrote:
> Is there a way to notify another process to refill a blocking queue when
> the queue becomes empty? I have a process that pulls data from a blocking
> queue and I'd like to be able to refill the queue from the database once I
> exhaust its contents.
> 
> This is a sort of a producer-consumer pattern, where the produces refills
> the queue from the database at specified intervals (usually 5 minutes).
> However when the consumer is ready to process more records, I do not want
> it to block and wait; rather I'd like it to notify the producer.

queue.poll( timeout, timeUnit ) won't block, so if that doesn't return anything 
   you can do your refill. I'd think creating a wrapper around a BlockingQueue 
would be pretty simple, such that the equivalent of take() will do a much less 
naive version of:

   public E myTake()
   {
       E item = delegatedQueue.poll( 1, TimeUnit.MILLISECONDS );
       if ( item != null ) {
           return item;
       }
       producer.refill( delegatedQueue );
       return myTake();
   }

Chris

From jnguyen at automotive.com  Thu Sep 11 14:44:41 2008
From: jnguyen at automotive.com (Joe Nguyen)
Date: Thu, 11 Sep 2008 11:44:41 -0700
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <48C95FB4.7010203@healthcare.vocollect.com>
References: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
	<48C95FB4.7010203@healthcare.vocollect.com>
Message-ID: <A7487BEC58632D46B8C7E8BC69B3EB120BE9125E@mail-001.corp.automotive.com>

producer.refill( delegatedQueue );  would this be an async call to avoid
blocking the consumer?  If it is, then producer.refill() would be
invoked several times?

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Chris
Winters
Sent: Thursday, September 11, 2008 11:13 Joe
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Sending a Re: Reusable Latch


ed.bayudan at prudential.com wrote:
> Is there a way to notify another process to refill a blocking queue
when
> the queue becomes empty? I have a process that pulls data from a
blocking
> queue and I'd like to be able to refill the queue from the database
once I
> exhaust its contents.
> 
> This is a sort of a producer-consumer pattern, where the produces
refills
> the queue from the database at specified intervals (usually 5
minutes).
> However when the consumer is ready to process more records, I do not
want
> it to block and wait; rather I'd like it to notify the producer.

queue.poll( timeout, timeUnit ) won't block, so if that doesn't return
anything 
   you can do your refill. I'd think creating a wrapper around a
BlockingQueue 
would be pretty simple, such that the equivalent of take() will do a
much less 
naive version of:

   public E myTake()
   {
       E item = delegatedQueue.poll( 1, TimeUnit.MILLISECONDS );
       if ( item != null ) {
           return item;
       }
       producer.refill( delegatedQueue );
       return myTake();
   }

Chris
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From ben_manes at yahoo.com  Thu Sep 11 14:55:23 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 11 Sep 2008 11:55:23 -0700 (PDT)
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
Message-ID: <897517.49504.qm@web38807.mail.mud.yahoo.com>

I did something like this when rewriting how we cached sequence values (so that hibernate didn't make an extra call to generate an id field).  Each sequence cache was a blocking queue and on every retrieval the consumer would check to see if it took the last entry.  If it did, then it would call the warming method before returning.  To prevent multiple consumers trying to warm, the database call was guarded with a try-lock and, if lock became acquired, a check that the queue was still empty.

If you do not want the consumers to share in the production work then you could instead have it unblock the producer thread.



----- Original Message ----
From: "ed.bayudan at prudential.com" <ed.bayudan at prudential.com>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, September 11, 2008 10:48:11 AM
Subject: [concurrency-interest] Sending a Re:  Reusable Latch

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a blocking
queue and I'd like to be able to refill the queue from the database once I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not want
it to block and wait; rather I'd like it to notify the producer.

Thanks.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080911/596bafe5/attachment.html>

From jnewsham at referentia.com  Thu Sep 11 15:36:20 2008
From: jnewsham at referentia.com (Jim Newsham)
Date: Thu, 11 Sep 2008 09:36:20 -1000
Subject: [concurrency-interest] Reusable Latch; AQS questions
In-Reply-To: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
References: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
Message-ID: <004a01c91445$ab0b1920$4400a8c0@referentia.com>



> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] On Behalf Of Bomba, Craig
> Sent: Thursday, September 11, 2008 6:29 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Reusable Latch
> 
> Other than a CyclicBarrier, what recommendations are out there for a
> reusable (i.e. get around the one-shot phenomenon) CountDownLatch?


Here is a "CountingLatch" I wrote not long ago.  It's quite similar to
CountDownLatch (I would have called it CountdownLatch), but it supports
increment as well as decrement.  I was thinking about asking for comments on
this list but never did... might as well take this opportunity to do so. 

It's actually simple enough, and I believe it is "correct" (but please prove
me wrong).  However I had lingering questions about
AbstractQueuedSynchronizer and efficiency.  I understand AQS in the examples
that I've seen, where all state is managed as a single integer value, but
what if I have additional information that I also need to synchronize?  

In this case, I have the latch count, and a version.  The version is used to
ensure we don't have a missed signal; in the case where we decrement to
zero, and then immediately increment again, some waiters may not awake in
time to see the 0, so they also need to inspect the version when deciding
whether to unblock.

I know an optimization would be to partition the integer state into two
values, the count and the version, but I'm not in the habit of optimizing on
the first pass.  So, how much of the benefit of AQS do I lose by having
additional state that I must synchronize on?  

Also, I use an atomic integer in this case, but are there any risks
associated with using a lock instead?

I also notice another (seemingly very minor) inefficiency in my
implementation.  Please refer to tryAcquireShared().  When a thread first
calls await(), the version will be identical, but we are forced to check it
anyway (because tryAcquireShared() is called in this case as well as the
case where the thread is notified of a potential change).  I don't see any
way around this other than to hand-write a synchronizer instead of using
AQS.

Any comments appreciated.  By the way, would be nice for something like this
to exist in Java out-of-the-box.

Thanks,

Jim


/**
 * A synchronization aid that allows one or more threads to wait until a set
of
 * operations being performed in other threads completes.  This latch
maintains
 * a count, and blocks threads which invoke {@link #await} while the count 
 * remains positive (that is, greater than zero).  When the count decrements

 * below a positive value (that is, when it decrements below 1), all waiting

 * threads are released.  When the count increments above a zero value,
threads 
 * which subsequently invoke {@link #await} will block once again.  
 * <p>
 * Note that when a thread returns from {@link #await}, there is no
guarantee 
 * that the count will still be non-positive, as the count could have been 
 * subsequently incremented.  Use cases which depend on the released state 
 * persisting might be better served with CountDownLatch or some other 
 * synchronizer.
 * <p>
 * This latch is similar to {@link java.util.concurrent.CountDownLatch}, but

 * supports increment as well as decrement.  Unlike CountDownLatch, this
latch 
 * is not a one-shot phenomenon; after the latch is released by decrementing

 * below 1, the latch may be reset again by incrementing above 0 again.
 * @author Jim Newsham
 */
@ThreadSafe
public final class CountingLatch {
  
  private final Sync sync;
  /**
   * A version number which changes every time a "release event" occurs.  A 
   * release event occurs when the count is decremented from a positive
value 
   * to a non-positive value, indicating that all currently waiting threads 
   * should be released.
   */
  private final AtomicInteger releaseVersion = new AtomicInteger();
  
  /**
   * Creates a new instance of CountingLatch, with an initial count value of
0.
   */
  public CountingLatch() {
    this(0);
  }
  
  /**
   * Creates a new instance of CountingLatch.
   * @param initialCount the initial count value
   */
  public CountingLatch(int initialCount) {
    sync = new Sync(initialCount);
  }
  
  /**
   * Increments the count.  This may cause threads which subsequently call
   * {@link #await} to block, if the count becomes positive (greater than
zero).
   */
  public void increment() {
    sync.releaseShared(1);
  }
  
  /**
   * Decrements the count.  This may release waiting threads if the count is
   * decremented below 1.
   */
  public void decrement() {
    sync.releaseShared(-1);
  }
  
  /**
   * Gets the current count.
   * @return the current count
   */
  public int getCount() {
    return sync.getCount();
  }
  
  /**
   * Causes the current thread to wait until the latch count is less than 1,

   * unless the thread is interrupted.  
   * <p>
   * If the current count is less than 1, then this method returns
immediately.  
   * Otherwise, the current thread bgecomes disabled for thread-scheduling
   * purposes and lies dormant until either (a) the count reaches zero due
to
   * invocations of the {@link #decrement} method, or (b) some other thread
   * interrupts the current thread.
   * @throws InterruptedException if the current thread is interrupted while
waiting
   */
  public void await() throws InterruptedException {
    sync.acquireSharedInterruptibly(releaseVersion.get());
  }
  
  /**
   * Causes the current thread to wait until the latch count is less than 1.

   * <p>
   * If the current count is less than 1, then this method returns
immediately.  
   * Otherwise, the current thread bgecomes disabled for thread-scheduling
   * purposes and lies dormant until the count reaches zero due to
invocations 
   * of the {@link #decrement} method.
   */
  public void awaitUninterruptibly() {
    sync.acquireShared(releaseVersion.get());
  }
  
  /**
   * Causes the current thread to wait until the latch count is less than 1,

   * unless the thread is interrupted, or the specified waiting time
elapses.  
   * <p>
   * If the current count is less than 1, then this method returns
immediately.  
   * Otherwise, the current thread bgecomes disabled for thread-scheduling
   * purposes and lies dormant until either (a) the count reaches zero due
to
   * invocations of the {@link #decrement} method, or (b) some other thread
   * interrupts the current thread, or (c) the specified waiting time
elapses.
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument
   * @return whether this method returned as a result of a latch count
reaching 
   *         a value below 1 (as opposed to a timeout)
   * @throws InterruptedException if the current thread is interrupted while
waiting
   */
  public boolean await(long timeout, TimeUnit unit) throws
InterruptedException {
    return sync.tryAcquireSharedNanos(releaseVersion.get(),
unit.toNanos(timeout));
  }
  

  /**
   * An internal synchronizer which implements the semantics of this
counting 
   * latch.
   */
  private class Sync extends AbstractQueuedSynchronizer {
    
    /**
     * Creates a new instance of Sync.
     * @param initialCount
     */
    public Sync(int initialCount) {
      setState(initialCount);
    }
    
    /**
     * Gets the count.
     * @return the count
     */
    public int getCount() {
      return getState();
    }
    
    /**
     * {@inheritDoc}
     */
    @Override
    public int tryAcquireShared(int version) {
      boolean acquired = getState() <= 0 || 
        CountingLatch.this.releaseVersion.get() != version;
      return acquired ? 1 : -1;
    }
    
    /**
     * {@inheritDoc}
     * @param increment the amount to increment or decrement by
     */
    @Override
    public boolean tryReleaseShared(int increment) {
      while (true) {
        int fromState = getState();
        int toState = fromState + increment;
        if (compareAndSetState(fromState, toState)) {
          // release waiting threads if decremented below 1
          boolean release = fromState > 0 && toState <= 0;
          if (release) {
            releaseVersion.incrementAndGet();
          }
          return release;
        }
      }
    }
    
  }

}



From dcholmes at optusnet.com.au  Thu Sep 11 19:09:59 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 12 Sep 2008 09:09:59 +1000
Subject: [concurrency-interest] Reusable Latch; AQS questions
In-Reply-To: <004a01c91445$ab0b1920$4400a8c0@referentia.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKHHNAA.dcholmes@optusnet.com.au>

Jim,

I can't go into your code in detail but the problem with maintaining
additional state is that there is no way to make changes to that additional
state atomically with respect to the inherent state - without synchronizing
everything which defeats the purpose of AQS. In your code for example it
isn't clear to me if it is correct for an acquirer to see the state set by a
release, but before the release has incremented the version - this might be
an inconsistent view of your latch.

Encoding multiple bits of state in the integer (or long if you're on a JDK
version that supports that) is not an "optimization" in AQS it is the
fundamental way to get correct synchronization.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jim
> Newsham
> Sent: Friday, 12 September 2008 5:36 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Reusable Latch; AQS questions
>
>
>
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> > interest-bounces at cs.oswego.edu] On Behalf Of Bomba, Craig
> > Sent: Thursday, September 11, 2008 6:29 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: [concurrency-interest] Reusable Latch
> >
> > Other than a CyclicBarrier, what recommendations are out there for a
> > reusable (i.e. get around the one-shot phenomenon) CountDownLatch?
>
>
> Here is a "CountingLatch" I wrote not long ago.  It's quite similar to
> CountDownLatch (I would have called it CountdownLatch), but it supports
> increment as well as decrement.  I was thinking about asking for
> comments on
> this list but never did... might as well take this opportunity to do so.
>
> It's actually simple enough, and I believe it is "correct" (but
> please prove
> me wrong).  However I had lingering questions about
> AbstractQueuedSynchronizer and efficiency.  I understand AQS in
> the examples
> that I've seen, where all state is managed as a single integer value, but
> what if I have additional information that I also need to synchronize?
>
> In this case, I have the latch count, and a version.  The version
> is used to
> ensure we don't have a missed signal; in the case where we decrement to
> zero, and then immediately increment again, some waiters may not awake in
> time to see the 0, so they also need to inspect the version when deciding
> whether to unblock.
>
> I know an optimization would be to partition the integer state into two
> values, the count and the version, but I'm not in the habit of
> optimizing on
> the first pass.  So, how much of the benefit of AQS do I lose by having
> additional state that I must synchronize on?
>
> Also, I use an atomic integer in this case, but are there any risks
> associated with using a lock instead?
>
> I also notice another (seemingly very minor) inefficiency in my
> implementation.  Please refer to tryAcquireShared().  When a thread first
> calls await(), the version will be identical, but we are forced
> to check it
> anyway (because tryAcquireShared() is called in this case as well as the
> case where the thread is notified of a potential change).  I don't see any
> way around this other than to hand-write a synchronizer instead of using
> AQS.
>
> Any comments appreciated.  By the way, would be nice for
> something like this
> to exist in Java out-of-the-box.
>
> Thanks,
>
> Jim
>
>
> /**
>  * A synchronization aid that allows one or more threads to wait
> until a set
> of
>  * operations being performed in other threads completes.  This latch
> maintains
>  * a count, and blocks threads which invoke {@link #await} while
> the count
>  * remains positive (that is, greater than zero).  When the count
> decrements
>
>  * below a positive value (that is, when it decrements below 1),
> all waiting
>
>  * threads are released.  When the count increments above a zero value,
> threads
>  * which subsequently invoke {@link #await} will block once again.
>  * <p>
>  * Note that when a thread returns from {@link #await}, there is no
> guarantee
>  * that the count will still be non-positive, as the count could
> have been
>  * subsequently incremented.  Use cases which depend on the
> released state
>  * persisting might be better served with CountDownLatch or some other
>  * synchronizer.
>  * <p>
>  * This latch is similar to {@link
> java.util.concurrent.CountDownLatch}, but
>
>  * supports increment as well as decrement.  Unlike CountDownLatch, this
> latch
>  * is not a one-shot phenomenon; after the latch is released by
> decrementing
>
>  * below 1, the latch may be reset again by incrementing above 0 again.
>  * @author Jim Newsham
>  */
> @ThreadSafe
> public final class CountingLatch {
>
>   private final Sync sync;
>   /**
>    * A version number which changes every time a "release event"
> occurs.  A
>    * release event occurs when the count is decremented from a positive
> value
>    * to a non-positive value, indicating that all currently
> waiting threads
>    * should be released.
>    */
>   private final AtomicInteger releaseVersion = new AtomicInteger();
>
>   /**
>    * Creates a new instance of CountingLatch, with an initial
> count value of
> 0.
>    */
>   public CountingLatch() {
>     this(0);
>   }
>
>   /**
>    * Creates a new instance of CountingLatch.
>    * @param initialCount the initial count value
>    */
>   public CountingLatch(int initialCount) {
>     sync = new Sync(initialCount);
>   }
>
>   /**
>    * Increments the count.  This may cause threads which subsequently call
>    * {@link #await} to block, if the count becomes positive (greater than
> zero).
>    */
>   public void increment() {
>     sync.releaseShared(1);
>   }
>
>   /**
>    * Decrements the count.  This may release waiting threads if
> the count is
>    * decremented below 1.
>    */
>   public void decrement() {
>     sync.releaseShared(-1);
>   }
>
>   /**
>    * Gets the current count.
>    * @return the current count
>    */
>   public int getCount() {
>     return sync.getCount();
>   }
>
>   /**
>    * Causes the current thread to wait until the latch count is
> less than 1,
>
>    * unless the thread is interrupted.
>    * <p>
>    * If the current count is less than 1, then this method returns
> immediately.
>    * Otherwise, the current thread bgecomes disabled for thread-scheduling
>    * purposes and lies dormant until either (a) the count reaches zero due
> to
>    * invocations of the {@link #decrement} method, or (b) some
> other thread
>    * interrupts the current thread.
>    * @throws InterruptedException if the current thread is
> interrupted while
> waiting
>    */
>   public void await() throws InterruptedException {
>     sync.acquireSharedInterruptibly(releaseVersion.get());
>   }
>
>   /**
>    * Causes the current thread to wait until the latch count is
> less than 1.
>
>    * <p>
>    * If the current count is less than 1, then this method returns
> immediately.
>    * Otherwise, the current thread bgecomes disabled for thread-scheduling
>    * purposes and lies dormant until the count reaches zero due to
> invocations
>    * of the {@link #decrement} method.
>    */
>   public void awaitUninterruptibly() {
>     sync.acquireShared(releaseVersion.get());
>   }
>
>   /**
>    * Causes the current thread to wait until the latch count is
> less than 1,
>
>    * unless the thread is interrupted, or the specified waiting time
> elapses.
>    * <p>
>    * If the current count is less than 1, then this method returns
> immediately.
>    * Otherwise, the current thread bgecomes disabled for thread-scheduling
>    * purposes and lies dormant until either (a) the count reaches zero due
> to
>    * invocations of the {@link #decrement} method, or (b) some
> other thread
>    * interrupts the current thread, or (c) the specified waiting time
> elapses.
>    * @param timeout the maximum time to wait
>    * @param unit the time unit of the timeout argument
>    * @return whether this method returned as a result of a latch count
> reaching
>    *         a value below 1 (as opposed to a timeout)
>    * @throws InterruptedException if the current thread is
> interrupted while
> waiting
>    */
>   public boolean await(long timeout, TimeUnit unit) throws
> InterruptedException {
>     return sync.tryAcquireSharedNanos(releaseVersion.get(),
> unit.toNanos(timeout));
>   }
>
>
>   /**
>    * An internal synchronizer which implements the semantics of this
> counting
>    * latch.
>    */
>   private class Sync extends AbstractQueuedSynchronizer {
>
>     /**
>      * Creates a new instance of Sync.
>      * @param initialCount
>      */
>     public Sync(int initialCount) {
>       setState(initialCount);
>     }
>
>     /**
>      * Gets the count.
>      * @return the count
>      */
>     public int getCount() {
>       return getState();
>     }
>
>     /**
>      * {@inheritDoc}
>      */
>     @Override
>     public int tryAcquireShared(int version) {
>       boolean acquired = getState() <= 0 ||
>         CountingLatch.this.releaseVersion.get() != version;
>       return acquired ? 1 : -1;
>     }
>
>     /**
>      * {@inheritDoc}
>      * @param increment the amount to increment or decrement by
>      */
>     @Override
>     public boolean tryReleaseShared(int increment) {
>       while (true) {
>         int fromState = getState();
>         int toState = fromState + increment;
>         if (compareAndSetState(fromState, toState)) {
>           // release waiting threads if decremented below 1
>           boolean release = fromState > 0 && toState <= 0;
>           if (release) {
>             releaseVersion.incrementAndGet();
>           }
>           return release;
>         }
>       }
>     }
>
>   }
>
> }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Thu Sep 11 19:28:19 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 12 Sep 2008 09:28:19 +1000
Subject: [concurrency-interest] Shutdown idle SingleThreadExecutor
In-Reply-To: <A7487BEC58632D46B8C7E8BC69B3EB120BE9125A@mail-001.corp.automotive.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEKIHNAA.dcholmes@optusnet.com.au>

Joe Nguyen writes:
>     Is there a way to config SingelThreadExecutor so that it will
> shutdown itself after being idle for over 10 minutes?

One possible trick - in JDK 6 which supports core timeouts - would be to
create a pool with a core and max size of 1 and to set a core timeout of 10
minutes. Then provide your own ThreadFactory to define a custom Thread that
upon returning from the Runnable's run() method (which indicates the thread
has been idle for 10 minutes or else got an exception) can shutdown the
executor.

Of course any code that might later try to use the pool has to be prepared
for it being shutdown. If you expect it to only be idle because it is in
fact unused and unreferenced then look into the
FinalizableDelegatedExecutorService in Executors.java and see whether that
pattern suits your needs - shutdown the pool if the wrapper gets finalized.

Cheers,
David Holmes


From jnewsham at referentia.com  Thu Sep 11 22:53:54 2008
From: jnewsham at referentia.com (Jim Newsham)
Date: Thu, 11 Sep 2008 16:53:54 -1000
Subject: [concurrency-interest] Reusable Latch; AQS questions
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEKHHNAA.dcholmes@optusnet.com.au>
References: <004a01c91445$ab0b1920$4400a8c0@referentia.com>
	<NFBBKALFDCPFIDBNKAPCAEKHHNAA.dcholmes@optusnet.com.au>
Message-ID: <006401c91482$cb604dc0$4400a8c0@referentia.com>


Hi David,

I understand that storing all of the state in the int is required for
atomicity.  In my case, I don't think that strict atomicity of both fields
is required for correctness.  The javadoc for AQS sanctions this use:
"Subclasses can maintain other state fields, but only the atomically updated
int  value manipulated using methods getState(), setState(int) and
compareAndSetState(int, int) is tracked with respect to synchronization."  

So assuming my code is correct despite not updating all state atomically,
then moving the remaining state into the int would be considered an
optimization.

Jim

> -----Original Message-----
> From: David Holmes [mailto:dcholmes at optusnet.com.au]
> Sent: Thursday, September 11, 2008 1:10 PM
> To: Jim Newsham; concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] Reusable Latch; AQS questions
> 
> Jim,
> 
> I can't go into your code in detail but the problem with maintaining
> additional state is that there is no way to make changes to that
> additional
> state atomically with respect to the inherent state - without
> synchronizing
> everything which defeats the purpose of AQS. In your code for example it
> isn't clear to me if it is correct for an acquirer to see the state set by
> a
> release, but before the release has incremented the version - this might
> be
> an inconsistent view of your latch.
> 
> Encoding multiple bits of state in the integer (or long if you're on a JDK
> version that supports that) is not an "optimization" in AQS it is the
> fundamental way to get correct synchronization.
> 
> Cheers,
> David Holmes
> 
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jim
> > Newsham
> > Sent: Friday, 12 September 2008 5:36 AM
> > To: concurrency-interest at cs.oswego.edu
> > Subject: Re: [concurrency-interest] Reusable Latch; AQS questions
> >
> >
> >
> >
> > > -----Original Message-----
> > > From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> > > interest-bounces at cs.oswego.edu] On Behalf Of Bomba, Craig
> > > Sent: Thursday, September 11, 2008 6:29 AM
> > > To: concurrency-interest at cs.oswego.edu
> > > Subject: [concurrency-interest] Reusable Latch
> > >
> > > Other than a CyclicBarrier, what recommendations are out there for a
> > > reusable (i.e. get around the one-shot phenomenon) CountDownLatch?
> >
> >
> > Here is a "CountingLatch" I wrote not long ago.  It's quite similar to
> > CountDownLatch (I would have called it CountdownLatch), but it supports
> > increment as well as decrement.  I was thinking about asking for
> > comments on
> > this list but never did... might as well take this opportunity to do so.
> >
> > It's actually simple enough, and I believe it is "correct" (but
> > please prove
> > me wrong).  However I had lingering questions about
> > AbstractQueuedSynchronizer and efficiency.  I understand AQS in
> > the examples
> > that I've seen, where all state is managed as a single integer value,
> but
> > what if I have additional information that I also need to synchronize?
> >
> > In this case, I have the latch count, and a version.  The version
> > is used to
> > ensure we don't have a missed signal; in the case where we decrement to
> > zero, and then immediately increment again, some waiters may not awake
> in
> > time to see the 0, so they also need to inspect the version when
> deciding
> > whether to unblock.
> >
> > I know an optimization would be to partition the integer state into two
> > values, the count and the version, but I'm not in the habit of
> > optimizing on
> > the first pass.  So, how much of the benefit of AQS do I lose by having
> > additional state that I must synchronize on?
> >
> > Also, I use an atomic integer in this case, but are there any risks
> > associated with using a lock instead?
> >
> > I also notice another (seemingly very minor) inefficiency in my
> > implementation.  Please refer to tryAcquireShared().  When a thread
> first
> > calls await(), the version will be identical, but we are forced
> > to check it
> > anyway (because tryAcquireShared() is called in this case as well as the
> > case where the thread is notified of a potential change).  I don't see
> any
> > way around this other than to hand-write a synchronizer instead of using
> > AQS.
> >
> > Any comments appreciated.  By the way, would be nice for
> > something like this
> > to exist in Java out-of-the-box.
> >
> > Thanks,
> >
> > Jim
> >
> >
> > /**
> >  * A synchronization aid that allows one or more threads to wait
> > until a set
> > of
> >  * operations being performed in other threads completes.  This latch
> > maintains
> >  * a count, and blocks threads which invoke {@link #await} while
> > the count
> >  * remains positive (that is, greater than zero).  When the count
> > decrements
> >
> >  * below a positive value (that is, when it decrements below 1),
> > all waiting
> >
> >  * threads are released.  When the count increments above a zero value,
> > threads
> >  * which subsequently invoke {@link #await} will block once again.
> >  * <p>
> >  * Note that when a thread returns from {@link #await}, there is no
> > guarantee
> >  * that the count will still be non-positive, as the count could
> > have been
> >  * subsequently incremented.  Use cases which depend on the
> > released state
> >  * persisting might be better served with CountDownLatch or some other
> >  * synchronizer.
> >  * <p>
> >  * This latch is similar to {@link
> > java.util.concurrent.CountDownLatch}, but
> >
> >  * supports increment as well as decrement.  Unlike CountDownLatch, this
> > latch
> >  * is not a one-shot phenomenon; after the latch is released by
> > decrementing
> >
> >  * below 1, the latch may be reset again by incrementing above 0 again.
> >  * @author Jim Newsham
> >  */
> > @ThreadSafe
> > public final class CountingLatch {
> >
> >   private final Sync sync;
> >   /**
> >    * A version number which changes every time a "release event"
> > occurs.  A
> >    * release event occurs when the count is decremented from a positive
> > value
> >    * to a non-positive value, indicating that all currently
> > waiting threads
> >    * should be released.
> >    */
> >   private final AtomicInteger releaseVersion = new AtomicInteger();
> >
> >   /**
> >    * Creates a new instance of CountingLatch, with an initial
> > count value of
> > 0.
> >    */
> >   public CountingLatch() {
> >     this(0);
> >   }
> >
> >   /**
> >    * Creates a new instance of CountingLatch.
> >    * @param initialCount the initial count value
> >    */
> >   public CountingLatch(int initialCount) {
> >     sync = new Sync(initialCount);
> >   }
> >
> >   /**
> >    * Increments the count.  This may cause threads which subsequently
> call
> >    * {@link #await} to block, if the count becomes positive (greater
> than
> > zero).
> >    */
> >   public void increment() {
> >     sync.releaseShared(1);
> >   }
> >
> >   /**
> >    * Decrements the count.  This may release waiting threads if
> > the count is
> >    * decremented below 1.
> >    */
> >   public void decrement() {
> >     sync.releaseShared(-1);
> >   }
> >
> >   /**
> >    * Gets the current count.
> >    * @return the current count
> >    */
> >   public int getCount() {
> >     return sync.getCount();
> >   }
> >
> >   /**
> >    * Causes the current thread to wait until the latch count is
> > less than 1,
> >
> >    * unless the thread is interrupted.
> >    * <p>
> >    * If the current count is less than 1, then this method returns
> > immediately.
> >    * Otherwise, the current thread bgecomes disabled for thread-
> scheduling
> >    * purposes and lies dormant until either (a) the count reaches zero
> due
> > to
> >    * invocations of the {@link #decrement} method, or (b) some
> > other thread
> >    * interrupts the current thread.
> >    * @throws InterruptedException if the current thread is
> > interrupted while
> > waiting
> >    */
> >   public void await() throws InterruptedException {
> >     sync.acquireSharedInterruptibly(releaseVersion.get());
> >   }
> >
> >   /**
> >    * Causes the current thread to wait until the latch count is
> > less than 1.
> >
> >    * <p>
> >    * If the current count is less than 1, then this method returns
> > immediately.
> >    * Otherwise, the current thread bgecomes disabled for thread-
> scheduling
> >    * purposes and lies dormant until the count reaches zero due to
> > invocations
> >    * of the {@link #decrement} method.
> >    */
> >   public void awaitUninterruptibly() {
> >     sync.acquireShared(releaseVersion.get());
> >   }
> >
> >   /**
> >    * Causes the current thread to wait until the latch count is
> > less than 1,
> >
> >    * unless the thread is interrupted, or the specified waiting time
> > elapses.
> >    * <p>
> >    * If the current count is less than 1, then this method returns
> > immediately.
> >    * Otherwise, the current thread bgecomes disabled for thread-
> scheduling
> >    * purposes and lies dormant until either (a) the count reaches zero
> due
> > to
> >    * invocations of the {@link #decrement} method, or (b) some
> > other thread
> >    * interrupts the current thread, or (c) the specified waiting time
> > elapses.
> >    * @param timeout the maximum time to wait
> >    * @param unit the time unit of the timeout argument
> >    * @return whether this method returned as a result of a latch count
> > reaching
> >    *         a value below 1 (as opposed to a timeout)
> >    * @throws InterruptedException if the current thread is
> > interrupted while
> > waiting
> >    */
> >   public boolean await(long timeout, TimeUnit unit) throws
> > InterruptedException {
> >     return sync.tryAcquireSharedNanos(releaseVersion.get(),
> > unit.toNanos(timeout));
> >   }
> >
> >
> >   /**
> >    * An internal synchronizer which implements the semantics of this
> > counting
> >    * latch.
> >    */
> >   private class Sync extends AbstractQueuedSynchronizer {
> >
> >     /**
> >      * Creates a new instance of Sync.
> >      * @param initialCount
> >      */
> >     public Sync(int initialCount) {
> >       setState(initialCount);
> >     }
> >
> >     /**
> >      * Gets the count.
> >      * @return the count
> >      */
> >     public int getCount() {
> >       return getState();
> >     }
> >
> >     /**
> >      * {@inheritDoc}
> >      */
> >     @Override
> >     public int tryAcquireShared(int version) {
> >       boolean acquired = getState() <= 0 ||
> >         CountingLatch.this.releaseVersion.get() != version;
> >       return acquired ? 1 : -1;
> >     }
> >
> >     /**
> >      * {@inheritDoc}
> >      * @param increment the amount to increment or decrement by
> >      */
> >     @Override
> >     public boolean tryReleaseShared(int increment) {
> >       while (true) {
> >         int fromState = getState();
> >         int toState = fromState + increment;
> >         if (compareAndSetState(fromState, toState)) {
> >           // release waiting threads if decremented below 1
> >           boolean release = fromState > 0 && toState <= 0;
> >           if (release) {
> >             releaseVersion.incrementAndGet();
> >           }
> >           return release;
> >         }
> >       }
> >     }
> >
> >   }
> >
> > }
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 




From chris.winters at healthcare.vocollect.com  Fri Sep 12 08:32:20 2008
From: chris.winters at healthcare.vocollect.com (Chris Winters)
Date: Fri, 12 Sep 2008 08:32:20 -0400
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <A7487BEC58632D46B8C7E8BC69B3EB120BE9125E@mail-001.corp.automotive.com>
References: <OF9E22F4AE.7184AF05-ON852574C1.006062DD-852574C1.0061CBDE@prudential.com>
	<48C95FB4.7010203@healthcare.vocollect.com>
	<A7487BEC58632D46B8C7E8BC69B3EB120BE9125E@mail-001.corp.automotive.com>
Message-ID: <48CA6154.70402@healthcare.vocollect.com>


I think it would be best to block since the queue is empty anyway and the 
consumer would be waiting on a new item either way.

Chris

Joe Nguyen wrote:
> producer.refill( delegatedQueue );  would this be an async call to avoid
> blocking the consumer?  If it is, then producer.refill() would be
> invoked several times?
> 
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Chris
> Winters
> Sent: Thursday, September 11, 2008 11:13 Joe
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Sending a Re: Reusable Latch
> 
> 
> ed.bayudan at prudential.com wrote:
>> Is there a way to notify another process to refill a blocking queue
> when
>> the queue becomes empty? I have a process that pulls data from a
> blocking
>> queue and I'd like to be able to refill the queue from the database
> once I
>> exhaust its contents.
>>
>> This is a sort of a producer-consumer pattern, where the produces
> refills
>> the queue from the database at specified intervals (usually 5
> minutes).
>> However when the consumer is ready to process more records, I do not
> want
>> it to block and wait; rather I'd like it to notify the producer.
> 
> queue.poll( timeout, timeUnit ) won't block, so if that doesn't return
> anything 
>    you can do your refill. I'd think creating a wrapper around a
> BlockingQueue 
> would be pretty simple, such that the equivalent of take() will do a
> much less 
> naive version of:
> 
>    public E myTake()
>    {
>        E item = delegatedQueue.poll( 1, TimeUnit.MILLISECONDS );
>        if ( item != null ) {
>            return item;
>        }
>        producer.refill( delegatedQueue );
>        return myTake();
>    }
> 
> Chris
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From ed.bayudan at prudential.com  Fri Sep 12 09:34:08 2008
From: ed.bayudan at prudential.com (ed.bayudan at prudential.com)
Date: Fri, 12 Sep 2008 09:34:08 -0400
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
In-Reply-To: <897517.49504.qm@web38807.mail.mud.yahoo.com>
Message-ID: <OF11B600B1.0C234A37-ON852574C2.0048AECE-852574C2.004A894F@prudential.com>

I like this. I was thinking of not using blocked queues, but your approach
is better.
Can you explain the purpose of checking the queue a second time? In my
case, I really don't care if the queue at that point is empty or not; I
just want the producer to add more records to it.
Thanks.



                                                                           
             Ben Manes                                                     
             <ben_manes at yahoo.                                             
             com>                                                       To 
                                       ed.bayudan at prudential.com,          
                                       concurrency-interest at cs.oswego.edu  
             Thu 09/11/2008                                             cc 
             02:55 PM                                                      
                                                                   Subject 
                                       Re: [concurrency-interest] Sending  
                                       a Re:  Reusable Latch               
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           



I did something like this when rewriting how we cached sequence values (so
that hibernate didn't make an extra call to generate an id field).  Each
sequence cache was a blocking queue and on every retrieval the consumer
would check to see if it took the last entry.  If it did, then it would
call the warming method before returning.  To prevent multiple consumers
trying to warm, the database call was guarded with a try-lock and, if lock
became acquired, a check that the queue was still empty.

If you do not want the consumers to share in the production work then you
could instead have it unblock the producer thread.



----- Original Message ----
From: "ed.bayudan at prudential.com" <ed.bayudan at prudential.com>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, September 11, 2008 10:48:11 AM
Subject: [concurrency-interest] Sending a Re:  Reusable Latch

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a blocking
queue and I'd like to be able to refill the queue from the database once I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not want
it to block and wait; rather I'd like it to notify the producer.

Thanks.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




(See attached file: C.htm)
                                                                                 
                                                                                 
                                                                                 
                                                                                 
                                                                                 
                                                                                 
                                                                                 
                                                                                 
                                                                                 


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080912/c45d317d/attachment.htm>

From ben_manes at yahoo.com  Fri Sep 12 17:26:02 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Fri, 12 Sep 2008 14:26:02 -0700 (PDT)
Subject: [concurrency-interest] Sending a Re:  Reusable Latch
Message-ID: <413650.40793.qm@web38807.mail.mud.yahoo.com>

Actually, its not needed so I was wrong. :-)

I remember that I was concerned of a race condition that executed as (batchSize=100):
1. Consumer #1 polls last element from queue, realizes its empty, and enters refill lock.
2. Consumer #2-#98 poll off elements while #1 populates.  Passes through try-lock.
3. Consumer #99 - polls off element, sees its empty
                        #1 - adds last element, exits refill lock
                        #99 - acquires try-lock, retrieves 100 elements, and puts them 1-99 into the queue
                                - can only put 99/100 in, as it wasn't really empty, so blocks on last put until a consumer arrives

The solution was not to limit the blocking queue's capacity to the batchSize, so on this race you may end up with 101 elements in
the queue, which is perfectly acceptable.

I dug up the code.  Note that the timeout is simply for safety purposes (in case I screwed up ;-)).

    /**
     * Gets the next value for the current sequence
     *
     * @return The next value for that sequence
     */
    public long getNextValue() {
        Long result;
        do {
            try {
                result = sequenceQueue.poll(1, TimeUnit.SECONDS);
                if (sequenceQueue.isEmpty()) {
                    loadFromDB();
                }
            } catch (InterruptedException e) {
                throw new SystemException(e, "Thread interrupted while waiting to take from DBSequence queue.");
            }
        } while (result == null);
        return result;
    }

    /**
     * Loads sequence values from the database
     */
    private void loadFromDB() {
        if (lock.tryLock()) {
            try {
               ...
            } finally {
                lock.unlock();
            }
        }
    }


----- Original Message ----
From: "ed.bayudan at prudential.com" <ed.bayudan at prudential.com>
To: Ben Manes <ben_manes at yahoo.com>
Cc: concurrency-interest at cs.oswego.edu
Sent: Friday, September 12, 2008 6:34:08 AM
Subject: Re: [concurrency-interest] Sending a Re:  Reusable Latch

I like this. I was thinking of not using blocked queues, but your approach
is better.
Can you explain the purpose of checking the queue a second time? In my
case, I really don't care if the queue at that point is empty or not; I
just want the producer to add more records to it.
Thanks.



                                                                          
             Ben Manes                                                    
             <ben_manes at yahoo.                                            
             com>                                                       To 
                                      ed.bayudan at prudential.com,          
                                      concurrency-interest at cs.oswego.edu  
             Thu 09/11/2008                                             cc 
             02:55 PM                                                      
                                                                   Subject 
                                       Re: [concurrency-interest] Sending  
                                       a Re:  Reusable Latch              
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          
                                                                          



I did something like this when rewriting how we cached sequence values (so
that hibernate didn't make an extra call to generate an id field).  Each
sequence cache was a blocking queue and on every retrieval the consumer
would check to see if it took the last entry.  If it did, then it would
call the warming method before returning.  To prevent multiple consumers
trying to warm, the database call was guarded with a try-lock and, if lock
became acquired, a check that the queue was still empty.

If you do not want the consumers to share in the production work then you
could instead have it unblock the producer thread.



----- Original Message ----
From: "ed.bayudan at prudential.com" <ed.bayudan at prudential.com>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, September 11, 2008 10:48:11 AM
Subject: [concurrency-interest] Sending a Re:  Reusable Latch

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a blocking
queue and I'd like to be able to refill the queue from the database once I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not want
it to block and wait; rather I'd like it to notify the producer.

Thanks.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




(See attached file: C.htm)
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                
                                                                                





----- Original Message ----
From: "ed.bayudan at prudential.com" <ed.bayudan at prudential.com>
To: concurrency-interest at cs.oswego.edu
Sent: Thursday, September 11, 2008 10:48:11 AM
Subject: [concurrency-interest] Sending a Re:  Reusable Latch

Is there a way to notify another process to refill a blocking queue when
the queue becomes empty? I have a process that pulls data from a blocking
queue and I'd like to be able to refill the queue from the database once I
exhaust its contents.

This is a sort of a producer-consumer pattern, where the produces refills
the queue from the database at specified intervals (usually 5 minutes).
However when the consumer is ready to process more records, I do not want
it to block and wait; rather I'd like it to notify the producer.

Thanks.



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080912/e54ec907/attachment.html>

From wangbing.nudt at gmail.com  Sun Sep 14 05:21:41 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Sun, 14 Sep 2008 11:21:41 +0200
Subject: [concurrency-interest] Question on Threads and Locks in Java
	language specification 3.0
Message-ID: <3e257e530809140221x2065638akd8e6dd32cf75b9c2@mail.gmail.com>

Questions on Threads and Locks in Java language specification 3.0.
Dear All:

 I am reading the chapter of Threads and Locks in Java language
specification 3.0. 2000 edition,and I was puzzled by the following
point in it ,I would like to get your advise :
first of all ,
What is the difference between  Program  order,Synchronization
Order,Happens-before Order and Sequential consistency in the spec?

First , in 17.4 Memory Model : it was mentioned that
Intra-thread semantics are the semantics for single
threaded programs, and allow the complete prediction of the behavior of a thread
based on the values seen by read actions within the thread.

To determine if the
actions of thread t in an execution are legal, we simply evaluate the
implementation
of thread t as it would be performed in a single threaded context, as defined in
the rest of this specification.

My question is :when there r many threads ,and there maybe so many
combination when we force the multi thread to be single threaded run
,so how could this kind of "single threaded context" come into
realization?

And
Each time the evaluation of thread t generates an inter-thread action, it must
match the inter-thread action a of t that comes next in program order.
If a is a read,
then further evaluation of t uses the value seen by a as determined by
the memory
model

How to do this kind fo match?  I assume this is done by the JVM ,not
the programmer?.

Second ,
In the first discussion of Happens-before Order 17.4.5
For example, the write of a default value to every field of an object
constructed by a thread
need not happen before the beginning of that thread, as long as no
read ever observes that
fact.
More specifically, if two actions share a happens-before relationship,
they do not necessarily have to appear to have happened in that order
to any code with which they do not
share a happens-before relationship. Writes in one thread that are in
a data race with
reads in another thread may, for example, appear to occur out of order
to those reads.


How to interpret "
if two actions share a happens-before relationship, they do not
necessarily have to appear to have happened in that order to any code
with which they do not
share a happens-before relationship
"

Third
When a program contains two conflicting accesses (?17.4.1) that are not ordered
by a happens-before relationship, it is said to contain a data race.
A program is correctly synchronized if and only if all sequentially
consistent executions
are free of data races.
So who ,the programmer or the JVM should take care of These order?
Is that mean we need to give out the relationship to the JVM inorder
to prevent the Racing condition?

However, its use does
allow a programmer to reason about the possible behaviors of a program
in a simple way;
the behavior of a correctly synchronized program is much less
dependent on possible reorderings.
Without correct synchronization, very strange, confusing and counterintuitive
behaviors are possible.

Is that mean that the programmer should give out the order ,and the
JVM would follow the order and maintain it during running?
More over ,is happens-before consistent  is a characteristic of  a
program ,or of JAVA language??

Forth ,I am totally confused by the exampe given out by Trace 17.5:
Trace 17.5 Behavior allowed by happens-before consistency, but not sequential
consistency. May observe r2 ==0, r1 == 0
Thread 1| Thread 2
B = 1;   |A = 2;
r2 = A; |r1 = B;

Initially, A == B == 0. In this case, since there is no
synchronization, each read can see
either the write of the initial value or the write by the other
thread. One such execution order
is
1: B = 1;
3: A = 2;
2: r2 = A; // sees initial write of 0
4: r1 = B; // sees initial write of 0
Similarly, the behavior shown in Trace 17.5 is happens-before
consistent, since there is
an execution order that allows each read to see the appropriate write.
An execution order
that displays that behavior is:
1: r2 = A; // sees write of A = 2
3: r1 = B; // sees write of B = 1
2: B = 1;
4: A = 2;
In this execution, the reads see writes that occur later in the
execution order. This may
seem counterintuitive, but is allowed by happens-before consistency.
Allowing reads to see
later writes can sometimes produce unacceptable behaviors.
Is it some thing provided to the Compiler optimization or have other
words to say in here


How can we expect  the same effect when the programs are excecutes  in
revert order?
         Thank you very much!
        Cheers
         Bing

-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617


From dcholmes at optusnet.com.au  Sun Sep 14 07:54:41 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sun, 14 Sep 2008 21:54:41 +1000
Subject: [concurrency-interest] Question on Threads and Locks in
	Javalanguage specification 3.0
In-Reply-To: <3e257e530809140221x2065638akd8e6dd32cf75b9c2@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEKPHNAA.dcholmes@optusnet.com.au>

Bing,

Trying to answer your questions requires explaining the whole Java memory model, and while there are a number of people who could attempt that, it is likely to lead to even more questions. :) Chapter 17 was the end result of the memory model work, but there has been an awful lot more written about. To get a better understanding I recommend looking at that additional material:

http://www.cs.umd.edu/~pugh/java/memoryModel/

Hope this helps.

David Holmes


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of bing
> wang
> Sent: Sunday, 14 September 2008 7:22 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Question on Threads and Locks in
> Javalanguage specification 3.0
> 
> 
> Questions on Threads and Locks in Java language specification 3.0.
> Dear All:
> 
>  I am reading the chapter of Threads and Locks in Java language
> specification 3.0. 2000 edition,and I was puzzled by the following
> point in it ,I would like to get your advise :
> first of all ,
> What is the difference between  Program  order,Synchronization
> Order,Happens-before Order and Sequential consistency in the spec?
> 
> First , in 17.4 Memory Model : it was mentioned that
> Intra-thread semantics are the semantics for single
> threaded programs, and allow the complete prediction of the 
> behavior of a thread
> based on the values seen by read actions within the thread.
> 
> To determine if the
> actions of thread t in an execution are legal, we simply evaluate the
> implementation
> of thread t as it would be performed in a single threaded 
> context, as defined in
> the rest of this specification.
> 
> My question is :when there r many threads ,and there maybe so many
> combination when we force the multi thread to be single threaded run
> ,so how could this kind of "single threaded context" come into
> realization?
> 
> And
> Each time the evaluation of thread t generates an inter-thread 
> action, it must
> match the inter-thread action a of t that comes next in program order.
> If a is a read,
> then further evaluation of t uses the value seen by a as determined by
> the memory
> model
> 
> How to do this kind fo match?  I assume this is done by the JVM ,not
> the programmer?.
> 
> Second ,
> In the first discussion of Happens-before Order 17.4.5
> For example, the write of a default value to every field of an object
> constructed by a thread
> need not happen before the beginning of that thread, as long as no
> read ever observes that
> fact.
> More specifically, if two actions share a happens-before relationship,
> they do not necessarily have to appear to have happened in that order
> to any code with which they do not
> share a happens-before relationship. Writes in one thread that are in
> a data race with
> reads in another thread may, for example, appear to occur out of order
> to those reads.
> 
> 
> How to interpret "
> if two actions share a happens-before relationship, they do not
> necessarily have to appear to have happened in that order to any code
> with which they do not
> share a happens-before relationship
> "
> 
> Third
> When a program contains two conflicting accesses (?17.4.1) that 
> are not ordered
> by a happens-before relationship, it is said to contain a data race.
> A program is correctly synchronized if and only if all sequentially
> consistent executions
> are free of data races.
> So who ,the programmer or the JVM should take care of These order?
> Is that mean we need to give out the relationship to the JVM inorder
> to prevent the Racing condition?
> 
> However, its use does
> allow a programmer to reason about the possible behaviors of a program
> in a simple way;
> the behavior of a correctly synchronized program is much less
> dependent on possible reorderings.
> Without correct synchronization, very strange, confusing and 
> counterintuitive
> behaviors are possible.
> 
> Is that mean that the programmer should give out the order ,and the
> JVM would follow the order and maintain it during running?
> More over ,is happens-before consistent  is a characteristic of  a
> program ,or of JAVA language??
> 
> Forth ,I am totally confused by the exampe given out by Trace 17.5:
> Trace 17.5 Behavior allowed by happens-before consistency, but 
> not sequential
> consistency. May observe r2 ==0, r1 == 0
> Thread 1| Thread 2
> B = 1;   |A = 2;
> r2 = A; |r1 = B;
> 
> Initially, A == B == 0. In this case, since there is no
> synchronization, each read can see
> either the write of the initial value or the write by the other
> thread. One such execution order
> is
> 1: B = 1;
> 3: A = 2;
> 2: r2 = A; // sees initial write of 0
> 4: r1 = B; // sees initial write of 0
> Similarly, the behavior shown in Trace 17.5 is happens-before
> consistent, since there is
> an execution order that allows each read to see the appropriate write.
> An execution order
> that displays that behavior is:
> 1: r2 = A; // sees write of A = 2
> 3: r1 = B; // sees write of B = 1
> 2: B = 1;
> 4: A = 2;
> In this execution, the reads see writes that occur later in the
> execution order. This may
> seem counterintuitive, but is allowed by happens-before consistency.
> Allowing reads to see
> later writes can sometimes produce unacceptable behaviors.
> Is it some thing provided to the Compiler optimization or have other
> words to say in here
> 
> 
> How can we expect  the same effect when the programs are excecutes  in
> revert order?
>          Thank you very much!
>         Cheers
>          Bing
> 
> -- 
> Wang Bing
> bing.wang at uni-rostock.de
> QQ:14757617
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From jed at atlassian.com  Mon Sep 15 02:41:14 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 15 Sep 2008 16:41:14 +1000
Subject: [concurrency-interest] Reusable Latch
In-Reply-To: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
References: <4BD174404CF4A34C98322DC926CF862B064B36A3@MSMAIL.cboent.cboe.com>
Message-ID: <48CE038A.6080207@atlassian.com>

I wrote a simple BooleanLatch that is useful for telling a single 
waiting thread to continue (or pass through if it hasn't reached the 
latch yet). The implementation was posted in this forum previously.

My testing showed it is significantly quicker than Condition.await() or 
ConcurrentQueue.take() on MacOS Java6, and limited testing on Linux Sun 
Java 5 and 6 backed this up. There was anecdotal evidence that it wasn't 
quicker for JRockit though.

It is really only suitable for SRSW or SRMW situations though, as in MR 
situations it behaves semantically the same as Condition.notify() (only 
allowing a single arbitrary reader though, with no fairness).

I also have a PhasedLatch which is a nod to the new Phaser stuff Doug 
has (nowhere near as powerful, but it does allow multi-readers to block 
until a particular phase has been reached and then all be allowed through).

Source is here:
https://maven.atlassian.com/public/com/atlassian/util/concurrent/atlassian-util-concurrent/0.0.2/atlassian-util-concurrent-0.0.2-sources.jar

Haven't got anonymous SVN access yet: 
https://studio.atlassian.com/browse/JST-999

BTW. All code is Apache 2.0 licensed.

cheers,
jed.

Bomba, Craig wrote:
> Other than a CyclicBarrier, what recommendations are out there for a reusable (i.e. get around the one-shot phenomenon) CountDownLatch?

From wangbing.nudt at gmail.com  Tue Sep 16 09:40:13 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Tue, 16 Sep 2008 15:40:13 +0200
Subject: [concurrency-interest] which unlock belongs to which lock in
	"hand-over-hand"
Message-ID: <3e257e530809160640k2569e0b2n68cb59e9222216ae@mail.gmail.com>

Dear all:
         I am puzzled by an example in the Java in a NutShell

the author clain that there 's one superior of
util.concurrent.locks.Lock over Synchronized() ,which is
"hand-over-hand" linked list traversal.:

a Lock object that you explicitly lock and unlock. Lock objects are
not automatically block-scoped and you must be careful to use
TRy/finally constructs to ensure that locks are always released. On
the other hand, Lock enables algorithms that are simply not possible
with block-scoped locks, such as the following "hand-over-hand" linked
list traversal:


but in the code ,there r 2 unlock() to the node .lock.lock () ,but no
unlock for next.lock.lock();
so .... am I mentally disturbed?

thanks

import java.util.concurrent.locks.*;  // New in Java 5.0

/**
 * A partial implementation of a linked list of values of type E.
 * It demonstrates hand-over-hand locking with Lock
 */
public class LinkList<E> {
    E value;                // The value of this node of the list
    LinkList<E> rest;       // The rest of the list
    Lock lock;              // A lock for this node

    public LinkList(E value) {  // Constructor for a list
        this.value = value;          // Node value
        rest = null;                 // This is the only node in the list
        lock = new ReentrantLock();  // We can lock this node
    }

    /**
     * Append a node to the end of the list, traversing the list using
     * hand-over-hand locking. This method is threadsafe: multiple threads
     * may traverse different portions of the list at the same time.
     **/
    public void append(E value) {
        LinkList<E> node = this;  // Start at this node
        node.lock.lock();         // Lock it.

        // Loop 'till we find the last node in the list
        while(node.rest != null) {
            LinkList<E> next = node.rest;

            // This is the hand-over-hand part.  Lock the next node and then
            // unlock the current node.  We use a try/finally construct so
            // that the current node is unlocked even if the lock on the
            // next node fails with an exception.
            try { next.lock.lock(); }  // lock the next node
            finally { node.lock.unlock(); } // unlock the current node
            node = next;
        }

        // At this point, node is the final node in the list, and we have
        // a lock on it.  Use a try/finally to ensure that we unlock it.
        try {
            node.rest = new LinkList<E>(value); // Append new node
        }
        finally { node.lock.unlock(); }
    }
}


-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617

From mallikap at deshaw.com  Tue Sep 16 10:04:33 2008
From: mallikap at deshaw.com (Mallikarjunaiah, Praveena)
Date: Tue, 16 Sep 2008 19:34:33 +0530
Subject: [concurrency-interest] which unlock belongs to which lock
	in"hand-over-hand"
In-Reply-To: <3e257e530809160640k2569e0b2n68cb59e9222216ae@mail.gmail.com>
References: <3e257e530809160640k2569e0b2n68cb59e9222216ae@mail.gmail.com>
Message-ID: <B911686319FBAC4BA4671AF36F8DC30903C81E5F@mailhyd2.hyd.deshaw.com>

I think, you missing the fact the node is getting assigned with next
inside the loop.

So, it is something like you get the lock for next node and unlock for
the curr node.

Thanks
Praveena 

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of bing
wang
Sent: Tuesday, September 16, 2008 7:10 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] which unlock belongs to which lock
in"hand-over-hand"

Dear all:
         I am puzzled by an example in the Java in a NutShell

the author clain that there 's one superior of
util.concurrent.locks.Lock over Synchronized() ,which is
"hand-over-hand" linked list traversal.:

a Lock object that you explicitly lock and unlock. Lock objects are
not automatically block-scoped and you must be careful to use
TRy/finally constructs to ensure that locks are always released. On
the other hand, Lock enables algorithms that are simply not possible
with block-scoped locks, such as the following "hand-over-hand" linked
list traversal:


but in the code ,there r 2 unlock() to the node .lock.lock () ,but no
unlock for next.lock.lock();
so .... am I mentally disturbed?

thanks

import java.util.concurrent.locks.*;  // New in Java 5.0

/**
 * A partial implementation of a linked list of values of type E.
 * It demonstrates hand-over-hand locking with Lock
 */
public class LinkList<E> {
    E value;                // The value of this node of the list
    LinkList<E> rest;       // The rest of the list
    Lock lock;              // A lock for this node

    public LinkList(E value) {  // Constructor for a list
        this.value = value;          // Node value
        rest = null;                 // This is the only node in the
list
        lock = new ReentrantLock();  // We can lock this node
    }

    /**
     * Append a node to the end of the list, traversing the list using
     * hand-over-hand locking. This method is threadsafe: multiple
threads
     * may traverse different portions of the list at the same time.
     **/
    public void append(E value) {
        LinkList<E> node = this;  // Start at this node
        node.lock.lock();         // Lock it.

        // Loop 'till we find the last node in the list
        while(node.rest != null) {
            LinkList<E> next = node.rest;

            // This is the hand-over-hand part.  Lock the next node and
then
            // unlock the current node.  We use a try/finally construct
so
            // that the current node is unlocked even if the lock on the
            // next node fails with an exception.
            try { next.lock.lock(); }  // lock the next node
            finally { node.lock.unlock(); } // unlock the current node
            node = next;
        }

        // At this point, node is the final node in the list, and we
have
        // a lock on it.  Use a try/finally to ensure that we unlock it.
        try {
            node.rest = new LinkList<E>(value); // Append new node
        }
        finally { node.lock.unlock(); }
    }
}


-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From wangbing.nudt at gmail.com  Tue Sep 16 10:18:38 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Tue, 16 Sep 2008 16:18:38 +0200
Subject: [concurrency-interest] which unlock belongs to which lock
	in"hand-over-hand"
In-Reply-To: <B911686319FBAC4BA4671AF36F8DC30903C81E5F@mailhyd2.hyd.deshaw.com>
References: <3e257e530809160640k2569e0b2n68cb59e9222216ae@mail.gmail.com>
	<B911686319FBAC4BA4671AF36F8DC30903C81E5F@mailhyd2.hyd.deshaw.com>
Message-ID: <3e257e530809160718p3b0ab42ftaa3c1a062faac4e8@mail.gmail.com>

thanks :)

On Tue, Sep 16, 2008 at 4:04 PM, Mallikarjunaiah, Praveena
<mallikap at deshaw.com> wrote:
> I think, you missing the fact the node is getting assigned with next
> inside the loop.
What if we add somethingelse   to the try ,
and if the somethingelse   fails ,the next .lock would miss a final....
so ,the central puzzle for me is how to appricate the beauty of this
"hand-over-hand " and what is its special purpose......

>            // that the current node is unlocked even if the lock on the
>            // next node fails with an exception.
>            try {
                              next.lock.lock(); // lock the next node
                               .....do somethingelse
                     }
 >            finally { node.lock.unlock(); } // unlock the current node
>            node = next;
>        }
> So, it is something like you get the lock for next node and unlock for
> the curr node.
>
> Thanks
> Praveena
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of bing
> wang
> Sent: Tuesday, September 16, 2008 7:10 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] which unlock belongs to which lock
> in"hand-over-hand"
>
> Dear all:
>         I am puzzled by an example in the Java in a NutShell
>
> the author clain that there 's one superior of
> util.concurrent.locks.Lock over Synchronized() ,which is
> "hand-over-hand" linked list traversal.:
>
> a Lock object that you explicitly lock and unlock. Lock objects are
> not automatically block-scoped and you must be careful to use
> TRy/finally constructs to ensure that locks are always released. On
> the other hand, Lock enables algorithms that are simply not possible
> with block-scoped locks, such as the following "hand-over-hand" linked
> list traversal:
>
>
> but in the code ,there r 2 unlock() to the node .lock.lock () ,but no
> unlock for next.lock.lock();
> so .... am I mentally disturbed?
>
> thanks
>
> import java.util.concurrent.locks.*;  // New in Java 5.0
>
> /**
>  * A partial implementation of a linked list of values of type E.
>  * It demonstrates hand-over-hand locking with Lock
>  */
> public class LinkList<E> {
>    E value;                // The value of this node of the list
>    LinkList<E> rest;       // The rest of the list
>    Lock lock;              // A lock for this node
>
>    public LinkList(E value) {  // Constructor for a list
>        this.value = value;          // Node value
>        rest = null;                 // This is the only node in the
> list
>        lock = new ReentrantLock();  // We can lock this node
>    }
>
>    /**
>     * Append a node to the end of the list, traversing the list using
>     * hand-over-hand locking. This method is threadsafe: multiple
> threads
>     * may traverse different portions of the list at the same time.
>     **/
>    public void append(E value) {
>        LinkList<E> node = this;  // Start at this node
>        node.lock.lock();         // Lock it.
>
>        // Loop 'till we find the last node in the list
>        while(node.rest != null) {
>            LinkList<E> next = node.rest;
>
>            // This is the hand-over-hand part.  Lock the next node and
> then
>            // unlock the current node.  We use a try/finally construct
> so
>            // that the current node is unlocked even if the lock on the
>            // next node fails with an exception.
>            try { next.lock.lock(); }  // lock the next node
>            finally { node.lock.unlock(); } // unlock the current node
>            node = next;
>        }
>
>        // At this point, node is the final node in the list, and we
> have
>        // a lock on it.  Use a try/finally to ensure that we unlock it.
>        try {
>            node.rest = new LinkList<E>(value); // Append new node
>        }
>        finally { node.lock.unlock(); }
>    }
> }
>
>
> --
> Wang Bing
> bing.wang at uni-rostock.de
> QQ:14757617
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617

From ben_manes at yahoo.com  Mon Sep 22 16:18:07 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Mon, 22 Sep 2008 13:18:07 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentLinkedHashMap prototype
Message-ID: <903231.46752.qm@web38805.mail.mud.yahoo.com>

Hi,


I have implemented a concurrent version of LinkedHashMap by using a lock-free doubly linked list that cross-cuts a ConcurrentHashMap.  I would greatly appreciate any critiques on the implementation, as well as tips on better techniques on how to unit and performance test the data structure.  My current testing strategy is a single threaded unit test of every function, a multi-threaded read/write test (based on JCiP's TestHarness), and validations of the eviction algorithms.  I plan on using retesting with a Zipfian distribution, since I had used an exponential as I was unable to create an adequate Poisson distribution.

The algorithm's is based on the following insights:
 - The CAS'ing strategy uses a state machine, which was an approach that Cliff Click suggested during his JavaOne talk.
 - A removal deletes the entry from the map, makes the list's node "dead", and allows the node to be lazily evicted when it reaches the head.  This is a simpler than backtracking or a 3-node locking strategy.
 - An adaption of the UNIX pageout daemon's second-chance algorithm is used to enhance a FIFO with basic frequency information.  This allows for LRU-level hit rates with FIFO's concurrency characteristics.

In terms of limitations:
 - I did not see value in allowing ordered traversal of keys for a concurrent version of LinkedHashMap, so that is not  provided.  This could be done but it seemed unnecessary.
 - An LRU policy proliferates dead nodes on the list and, since the capacity is based on the list's length, this takes up slots in the cache.  This artifact must be compensated for by increasing the capacity.

In the future I'll probably abandon the dead node removal approach and use an explicit removal, which I adopted since I didn't fully trust my backtracking algorithm.  I'd also like to experiment with different caching algorithms (e.g. CAR), though more for fun than for any practical purposes.  I'm sure that there are many ways to improve this implementation so any suggestions are appreciated.

project: http://code.google.com/p/concurrentlinkedhashmap/
source: http://code.google.com/p/concurrentlinkedhashmap/source/browse/trunk/src/java/com/rc/util/concurrent/ConcurrentLinkedHashMap.java

Thanks,
Ben


      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080922/37a75c23/attachment.html>

From holger at wizards.de  Tue Sep 23 08:36:07 2008
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 23 Sep 2008 14:36:07 +0200
Subject: [concurrency-interest] ConcurrentLinkedHashMap prototype
In-Reply-To: <903231.46752.qm@web38805.mail.mud.yahoo.com>
References: <903231.46752.qm@web38805.mail.mud.yahoo.com>
Message-ID: <48D8E2B7.3080905@wizards.de>

Ben Manes wrote:
> I have implemented a concurrent version of LinkedHashMap by using a
> lock-free doubly linked list that cross-cuts a ConcurrentHashMap. [..]

This looks very nice. The pluggable eviction strategies are a great idea,
though if there's one thing I'd like to see is an atomic getOrCreate()
facility that would prevent the need for multiple concurrent CAS value
creations by the client (as in Memoizer). Think this would be possible?
Seems to me that ehCache or any other clients could benefit greatly from
this to provide a transparent read-through to a value factory.
I've been contemplating a mostly-nonblocking/concurrent rewrite of
commons-pool and the ability to implement pool/value factory semantics on
top of your CLHM would be really cool.

Thanks for making this public!
Holger


From ben_manes at yahoo.com  Tue Sep 23 15:57:04 2008
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 23 Sep 2008 12:57:04 -0700 (PDT)
Subject: [concurrency-interest] ConcurrentLinkedHashMap prototype
Message-ID: <672582.17268.qm@web38805.mail.mud.yahoo.com>

I would prefer if you borrowed the future approach from JCiP (http://jcip.net/listings/Memoizer.java).  This has a number of nice properties, such as allowing you to pivot the backing data store.  I use it to support my map, ehcache (in-memory + disk), and a dynamic cache (Google's ReferenceMap).  This is similar to Ehcache's pre-future approach which provides a construct that uses an array of locks.  I wrote my own for my caching layer, instead of using Ehcache's, to efficiently support a bulk get() operation so that I can make one remote call rather than N sequential ones (my local caches are backed by memcached).  My approach was to insert single entry futures that proxied to a bulk future to retrieve its value, thus allowing subsequent calls to block.  So adding a decorator on top of the map would be a more flexible (and extremely simple) approach, imho.



----- Original Message ----
From: Holger Hoffst?tte <holger at wizards.de>
To: concurrency-interest at cs.oswego.edu
Sent: Tuesday, September 23, 2008 5:36:07 AM
Subject: Re: [concurrency-interest] ConcurrentLinkedHashMap prototype

Ben Manes wrote:
> I have implemented a concurrent version of LinkedHashMap by using a
> lock-free doubly linked list that cross-cuts a ConcurrentHashMap. [..]

This looks very nice. The pluggable eviction strategies are a great idea,
though if there's one thing I'd like to see is an atomic getOrCreate()
facility that would prevent the need for multiple concurrent CAS value
creations by the client (as in Memoizer). Think this would be possible?
Seems to me that ehCache or any other clients could benefit greatly from
this to provide a transparent read-through to a value factory.
I've been contemplating a mostly-nonblocking/concurrent rewrite of
commons-pool and the ability to implement pool/value factory semantics on
top of your CLHM would be really cool.

Thanks for making this public!
Holger

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080923/c0f18535/attachment.html>

From hanson.char at gmail.com  Tue Sep 23 20:33:25 2008
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 23 Sep 2008 17:33:25 -0700
Subject: [concurrency-interest] Interruptible I/O (was RE: enable /
	disable interrupt
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEBHHLAA.dcholmes@optusnet.com.au>
References: <ca53c8f80803292042r6832cd7au97a523ab35a4699@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEBHHLAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80809231733m4e382accy7cc4381b52af013e@mail.gmail.com>

A solution I found recently that helps resolve this problem typical on
Socket that others may find useful:


http://hansonchar.blogspot.com/2008/09/no-more-infinitely-blocking-socket.html

Hanson

On Sat, Mar 29, 2008 at 9:15 PM, David Holmes <dcholmes at optusnet.com.au>wrote:

>
> Also as you note, nothing else gets you out of a blocked I/O operation, not
> even Thread.stop - another reason the deprecation of Thread.stop was no
> great loss.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080923/5e5b30af/attachment.html>

From peter.kovacs.1.0rc at gmail.com  Wed Sep 24 08:31:32 2008
From: peter.kovacs.1.0rc at gmail.com (Peter Kovacs)
Date: Wed, 24 Sep 2008 14:31:32 +0200
Subject: [concurrency-interest] Interruptible I/O (was RE: enable /
	disable interrupt
In-Reply-To: <ca53c8f80809231733m4e382accy7cc4381b52af013e@mail.gmail.com>
References: <ca53c8f80803292042r6832cd7au97a523ab35a4699@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEBHHLAA.dcholmes@optusnet.com.au>
	<ca53c8f80809231733m4e382accy7cc4381b52af013e@mail.gmail.com>
Message-ID: <b6e8f2e80809240531j7fb3dbcen21d9782c0b97df15@mail.gmail.com>

An implementation for a more specific domain (RMI):
https://interruptiblermi.dev.java.net/

Peter

On Wed, Sep 24, 2008 at 2:33 AM, Hanson Char <hanson.char at gmail.com> wrote:
> A solution I found recently that helps resolve this problem typical on
> Socket that others may find useful:
>
>
> http://hansonchar.blogspot.com/2008/09/no-more-infinitely-blocking-socket.html
>
> Hanson
>
> On Sat, Mar 29, 2008 at 9:15 PM, David Holmes <dcholmes at optusnet.com.au>
> wrote:
>>
>>
>> Also as you note, nothing else gets you out of a blocked I/O operation,
>> not even Thread.stop - another reason the deprecation of Thread.stop was no
>> great loss.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From kschneider at gmail.com  Wed Sep 24 17:23:44 2008
From: kschneider at gmail.com (Kris Schneider)
Date: Wed, 24 Sep 2008 17:23:44 -0400
Subject: [concurrency-interest] JTaP article on stateful web apps
Message-ID: <3d242a3d0809241423o3677df8ey3510ba6e2ebacb36@mail.gmail.com>

The latest Java Theory and Practice article:

http://www.ibm.com/developerworks/library/j-jtp09238.html

exposes some of the concurrency issues that arise from using
HttpSession and ServletContext attributes. At the end of the "Possible
solutions" section, there's a reference to using SpringMVC's
synchronizeOnSession property of AbstractController to serialize
access to the session. After looking at the related code:

http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/servlet/mvc/AbstractController.java?view=markup
http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/WebUtils.java?view=markup
http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/HttpSessionMutexListener.java?view=markup

It seems like it's hiding its own dark corners. For example, in
AbstractController:

public abstract class AbstractController extends WebContentGenerator
implements Controller {
    private boolean synchronizeOnSession = false;

    public final void setSynchronizeOnSession(boolean synchronizeOnSession) {
        this.synchronizeOnSession = synchronizeOnSession;
    }

    public final boolean isSynchronizeOnSession() {
        return this.synchronizeOnSession;
    }

    public ModelAndView handleRequest(HttpServletRequest request,
HttpServletResponse response) throws Exception {
        // Delegate to WebContentGenerator for checking and preparing.
        checkAndPrepare(request, response, this instanceof LastModified);

        // Execute handleRequestInternal in synchronized block if required.
        if (this.synchronizeOnSession) {
            HttpSession session = request.getSession(false);
            if (session != null) {
                Object mutex = WebUtils.getSessionMutex(session);
                synchronized (mutex) {
                    return handleRequestInternal(request, response);
                }
            }
        }
		
        return handleRequestInternal(request, response);
    }
...

If an instance of that class is shared between request threads, isn't
there a visibility (and publication?) issue with synchronizeOnSession?

The handleRequest method uses WebUtils.getSessionMutex to obtain the
lock object. Here's what that method looks like:

public static Object getSessionMutex(HttpSession session) {
    Assert.notNull(session, "Session must not be null");
    Object mutex = session.getAttribute(SESSION_MUTEX_ATTRIBUTE);
    if (mutex == null) {
        mutex = session;
    }
    return mutex;
}

So, if the mutex attribute doesn't exist, the session object itself is
returned. Whether or not that will actually work is completely up to
the servlet container. There are no guarantees from the servlet spec
that the same instance is always returned from calls to
HttpServletRequest.getSession (let alone all the other ways that a
session object can be obtained). To be fair, the documentation for
getSessionMutex alludes to that fact, but then why not do something
like throw IllegalStateException in the case where the attribute
doesn't exist?

-- 
Kris Schneider <mailto:kschneider at gmail.com>

From alarmnummer at gmail.com  Wed Sep 24 18:29:57 2008
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Thu, 25 Sep 2008 00:29:57 +0200
Subject: [concurrency-interest] JTaP article on stateful web apps
In-Reply-To: <3d242a3d0809241423o3677df8ey3510ba6e2ebacb36@mail.gmail.com>
References: <3d242a3d0809241423o3677df8ey3510ba6e2ebacb36@mail.gmail.com>
Message-ID: <1466c1d60809241529k6a43f35bud2e75e116e351dcb@mail.gmail.com>

As long as the setter is called when the bean is constructed
everything will be alright because there (was) is a guaranteed happens
before relation between bean creation (inclusive setters) and usage of
the bean (not documented btw.. so no guarantee)

But once the setter is called later (for example by JMX) you are
completely right. Shit could hit the fan.

This blogpost contains some info about Spring and visibility problems.
http://blog.xebia.com/2007/03/01/spring-and-visibility-problems/

I'm currently working on a tool that does runtime analysis on objects
to figure out which objects are touched by multiple threads. I have
executed it on a few projects and also on Spring batch (one of the
spring modules) and there are some visibility issues here as well. It
appears that the JMM has not a high priority within Spring Source.

ps:
Tomcat also suffers from those issues

On Wed, Sep 24, 2008 at 11:23 PM, Kris Schneider <kschneider at gmail.com> wrote:
> The latest Java Theory and Practice article:
>
> http://www.ibm.com/developerworks/library/j-jtp09238.html
>
> exposes some of the concurrency issues that arise from using
> HttpSession and ServletContext attributes. At the end of the "Possible
> solutions" section, there's a reference to using SpringMVC's
> synchronizeOnSession property of AbstractController to serialize
> access to the session. After looking at the related code:
>
> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/servlet/mvc/AbstractController.java?view=markup
> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/WebUtils.java?view=markup
> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/HttpSessionMutexListener.java?view=markup
>
> It seems like it's hiding its own dark corners. For example, in
> AbstractController:
>
> public abstract class AbstractController extends WebContentGenerator
> implements Controller {
>    private boolean synchronizeOnSession = false;
>
>    public final void setSynchronizeOnSession(boolean synchronizeOnSession) {
>        this.synchronizeOnSession = synchronizeOnSession;
>    }
>
>    public final boolean isSynchronizeOnSession() {
>        return this.synchronizeOnSession;
>    }
>
>    public ModelAndView handleRequest(HttpServletRequest request,
> HttpServletResponse response) throws Exception {
>        // Delegate to WebContentGenerator for checking and preparing.
>        checkAndPrepare(request, response, this instanceof LastModified);
>
>        // Execute handleRequestInternal in synchronized block if required.
>        if (this.synchronizeOnSession) {
>            HttpSession session = request.getSession(false);
>            if (session != null) {
>                Object mutex = WebUtils.getSessionMutex(session);
>                synchronized (mutex) {
>                    return handleRequestInternal(request, response);
>                }
>            }
>        }
>
>        return handleRequestInternal(request, response);
>    }
> ...
>
> If an instance of that class is shared between request threads, isn't
> there a visibility (and publication?) issue with synchronizeOnSession?
>
> The handleRequest method uses WebUtils.getSessionMutex to obtain the
> lock object. Here's what that method looks like:
>
> public static Object getSessionMutex(HttpSession session) {
>    Assert.notNull(session, "Session must not be null");
>    Object mutex = session.getAttribute(SESSION_MUTEX_ATTRIBUTE);
>    if (mutex == null) {
>        mutex = session;
>    }
>    return mutex;
> }
>
> So, if the mutex attribute doesn't exist, the session object itself is
> returned. Whether or not that will actually work is completely up to
> the servlet container. There are no guarantees from the servlet spec
> that the same instance is always returned from calls to
> HttpServletRequest.getSession (let alone all the other ways that a
> session object can be obtained). To be fair, the documentation for
> getSessionMutex alludes to that fact, but then why not do something
> like throw IllegalStateException in the case where the attribute
> doesn't exist?
>
> --
> Kris Schneider <mailto:kschneider at gmail.com>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From kschneider at gmail.com  Wed Sep 24 19:20:56 2008
From: kschneider at gmail.com (Kris Schneider)
Date: Wed, 24 Sep 2008 19:20:56 -0400
Subject: [concurrency-interest] JTaP article on stateful web apps
In-Reply-To: <1466c1d60809241529k6a43f35bud2e75e116e351dcb@mail.gmail.com>
References: <3d242a3d0809241423o3677df8ey3510ba6e2ebacb36@mail.gmail.com>
	<1466c1d60809241529k6a43f35bud2e75e116e351dcb@mail.gmail.com>
Message-ID: <3d242a3d0809241620l441dced8j28771e5a5ea88aa6@mail.gmail.com>

Is that happens-before relationship between bean creation and usage
something that's intrinsic to Spring?

On Wed, Sep 24, 2008 at 6:29 PM, Peter Veentjer <alarmnummer at gmail.com> wrote:
> As long as the setter is called when the bean is constructed
> everything will be alright because there (was) is a guaranteed happens
> before relation between bean creation (inclusive setters) and usage of
> the bean (not documented btw.. so no guarantee)
>
> But once the setter is called later (for example by JMX) you are
> completely right. Shit could hit the fan.
>
> This blogpost contains some info about Spring and visibility problems.
> http://blog.xebia.com/2007/03/01/spring-and-visibility-problems/
>
> I'm currently working on a tool that does runtime analysis on objects
> to figure out which objects are touched by multiple threads. I have
> executed it on a few projects and also on Spring batch (one of the
> spring modules) and there are some visibility issues here as well. It
> appears that the JMM has not a high priority within Spring Source.
>
> ps:
> Tomcat also suffers from those issues
>
> On Wed, Sep 24, 2008 at 11:23 PM, Kris Schneider <kschneider at gmail.com> wrote:
>> The latest Java Theory and Practice article:
>>
>> http://www.ibm.com/developerworks/library/j-jtp09238.html
>>
>> exposes some of the concurrency issues that arise from using
>> HttpSession and ServletContext attributes. At the end of the "Possible
>> solutions" section, there's a reference to using SpringMVC's
>> synchronizeOnSession property of AbstractController to serialize
>> access to the session. After looking at the related code:
>>
>> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/servlet/mvc/AbstractController.java?view=markup
>> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/WebUtils.java?view=markup
>> http://springframework.cvs.sourceforge.net/springframework/spring/src/org/springframework/web/util/HttpSessionMutexListener.java?view=markup
>>
>> It seems like it's hiding its own dark corners. For example, in
>> AbstractController:
>>
>> public abstract class AbstractController extends WebContentGenerator
>> implements Controller {
>>    private boolean synchronizeOnSession = false;
>>
>>    public final void setSynchronizeOnSession(boolean synchronizeOnSession) {
>>        this.synchronizeOnSession = synchronizeOnSession;
>>    }
>>
>>    public final boolean isSynchronizeOnSession() {
>>        return this.synchronizeOnSession;
>>    }
>>
>>    public ModelAndView handleRequest(HttpServletRequest request,
>> HttpServletResponse response) throws Exception {
>>        // Delegate to WebContentGenerator for checking and preparing.
>>        checkAndPrepare(request, response, this instanceof LastModified);
>>
>>        // Execute handleRequestInternal in synchronized block if required.
>>        if (this.synchronizeOnSession) {
>>            HttpSession session = request.getSession(false);
>>            if (session != null) {
>>                Object mutex = WebUtils.getSessionMutex(session);
>>                synchronized (mutex) {
>>                    return handleRequestInternal(request, response);
>>                }
>>            }
>>        }
>>
>>        return handleRequestInternal(request, response);
>>    }
>> ...
>>
>> If an instance of that class is shared between request threads, isn't
>> there a visibility (and publication?) issue with synchronizeOnSession?
>>
>> The handleRequest method uses WebUtils.getSessionMutex to obtain the
>> lock object. Here's what that method looks like:
>>
>> public static Object getSessionMutex(HttpSession session) {
>>    Assert.notNull(session, "Session must not be null");
>>    Object mutex = session.getAttribute(SESSION_MUTEX_ATTRIBUTE);
>>    if (mutex == null) {
>>        mutex = session;
>>    }
>>    return mutex;
>> }
>>
>> So, if the mutex attribute doesn't exist, the session object itself is
>> returned. Whether or not that will actually work is completely up to
>> the servlet container. There are no guarantees from the servlet spec
>> that the same instance is always returned from calls to
>> HttpServletRequest.getSession (let alone all the other ways that a
>> session object can be obtained). To be fair, the documentation for
>> getSessionMutex alludes to that fact, but then why not do something
>> like throw IllegalStateException in the case where the attribute
>> doesn't exist?
>>
>> --
>> Kris Schneider <mailto:kschneider at gmail.com>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>



-- 
Kris Schneider <mailto:kschneider at gmail.com>

From wangbing.nudt at gmail.com  Tue Sep 30 04:48:36 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Tue, 30 Sep 2008 10:48:36 +0200
Subject: [concurrency-interest] is java Memory model sequentially consistent
Message-ID: <3e257e530809300148r506df2a3ye592079030ab79b0@mail.gmail.com>

Dear all :
     recently I am trying to implement a Time management algorithm for
parallel dicrete event simulation using Java concurrent Packege,but I
have to make a judgement on the Java memory model . in the first
glance  the Inter-thread communication of Java thread should be
different from message-driven communication .
      As far as I know ,the shared-memory multiprocessors machines
normally guarantee that no two processors will observe a set of memory
operations as occurring in different orders. and this feature is very
differnt from what we have to face in message based commnunication in
ditributed system .
     there r a certain formal assumption behind the clain above:
is that shared-memory machines typically provide sequentially
consistent memory that guarantees different processors
will not observe different orderings of memory references. but as a
Virtual machine run on a single core or multicore processor ,would JVM
provide such assumption to us ? ,the reading of the JAVA language
speicification makes me more doubt about this .
     if yes ,any precondition?in not ,any  restriction that turn it to "yes"?
     Danke!
      Bing


-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617

From mthornton at optrak.co.uk  Tue Sep 30 05:12:08 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 30 Sep 2008 10:12:08 +0100
Subject: [concurrency-interest] is java Memory model sequentially
	consistent
In-Reply-To: <3e257e530809300148r506df2a3ye592079030ab79b0@mail.gmail.com>
References: <3e257e530809300148r506df2a3ye592079030ab79b0@mail.gmail.com>
Message-ID: <48E1ED68.5070203@optrak.co.uk>

bing wang wrote:
>       As far as I know ,the shared-memory multiprocessors machines
> normally guarantee that no two processors will observe a set of memory
> operations as occurring in different orders.
>   
This is a popular assumption but not one guaranteed by the Java memory 
model. I believe there exist processors which can violate it. Some 
versions of the (former) DEC Alpha are usually cited in this context.

Mark Thornton



From wangbing.nudt at gmail.com  Tue Sep 30 05:16:58 2008
From: wangbing.nudt at gmail.com (bing wang)
Date: Tue, 30 Sep 2008 11:16:58 +0200
Subject: [concurrency-interest] is java Memory model sequentially
	consistent
In-Reply-To: <48E1ED68.5070203@optrak.co.uk>
References: <3e257e530809300148r506df2a3ye592079030ab79b0@mail.gmail.com>
	<48E1ED68.5070203@optrak.co.uk>
Message-ID: <3e257e530809300216l445b5efbxd4ea0cda1297527e@mail.gmail.com>

Dear Mark :
       Thank you very much for your reply ,
the  key observation is that all processors perceive the same total
ordering of the memory references.
Not all shared memory machines provide sequentially consistent
memory (as you have mentioned ,and  e.g., Gharachorloo et al. [1988]).
However, machines using weaker
memory models may emulate sequential consistency by inserting synchronization
primitives at suitable locations in the program.
 Lamport defines sequential consistency
as "the result of any execution [on the multiprocessor] is the same as if the
 operations of all processors were executed in some sequential order, and
the operations of each individual processor appear in this sequence in the
order specified by its program" [Lamport 1979]. For example, if processor 1
issues memory references M1, M2, M3, and M4 (in that order), and
processor 2 similarly issues references Ma, Mb, Mc, and Md, then M1, Ma,
Mb, M2, Mc, M3, M4, Md is a sequentially consistent total ordering, but
M1, Ma, Mb, M3, Mc, M2, M4, Md is not.

On Tue, Sep 30, 2008 at 11:12 AM, Mark Thornton <mthornton at optrak.co.uk> wrote:
> bing wang wrote:
>>
>>      As far as I know ,the shared-memory multiprocessors machines
>> normally guarantee that no two processors will observe a set of memory
>> operations as occurring in different orders.
>>
>
> This is a popular assumption but not one guaranteed by the Java memory
> model. I believe there exist processors which can violate it. Some versions
> of the (former) DEC Alpha are usually cited in this context.
>
> Mark Thornton
>
>
>



-- 
Wang Bing
bing.wang at uni-rostock.de
QQ:14757617

From holger at wizards.de  Tue Sep 30 07:24:03 2008
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 30 Sep 2008 13:24:03 +0200
Subject: [concurrency-interest] is java Memory model
	sequentially	consistent
In-Reply-To: <3e257e530809300216l445b5efbxd4ea0cda1297527e@mail.gmail.com>
References: <3e257e530809300148r506df2a3ye592079030ab79b0@mail.gmail.com>	<48E1ED68.5070203@optrak.co.uk>
	<3e257e530809300216l445b5efbxd4ea0cda1297527e@mail.gmail.com>
Message-ID: <48E20C53.4030106@wizards.de>

bing wang wrote:
> Dear Mark :
>        Thank you very much for your reply ,
> the  key observation is that all processors perceive the same total
> ordering of the memory references.
> Not all shared memory machines provide sequentially consistent
> memory (as you have mentioned ,and  e.g., Gharachorloo et al. [1988]).
> [..]

Everything you need to know can be found here:
http://www.cs.umd.edu/~pugh/java/memoryModel/

-h

From markus.kohler at gmail.com  Tue Sep 30 10:50:51 2008
From: markus.kohler at gmail.com (Markus Kohler)
Date: Tue, 30 Sep 2008 16:50:51 +0200
Subject: [concurrency-interest] Concurrent BitSet?
Message-ID: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>

Hi all,
boolean[] uses one byte per Array, but allows concurrent access (to
different entries), whereas BitSet uses one bit (approximately) per
"boolean" but doesn't allow concurrent access.
Does anyone here know whether there's an implementation of BitSet that would
allow concurrent access but still would (approximately) use one bit?

I'm inclined to believe that such a thing should be possible, because one
could use AtomIntegerArray as a backup and use striping to allow efficient
concurrent access.



Regards,
Markus
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20080930/c53116d9/attachment.html>

From sberlin at gmail.com  Tue Sep 30 13:05:19 2008
From: sberlin at gmail.com (Sam Berlin)
Date: Tue, 30 Sep 2008 13:05:19 -0400
Subject: [concurrency-interest] Concurrent BitSet?
In-Reply-To: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
References: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
Message-ID: <19196d860809301005r2e53bf60tcd3462b060fdf172@mail.gmail.com>

In addition to that, is anyone aware of a good SparseBitSet (that
doesn't allocate the leading long[]'s when only some trailing bits are
present, yet still has optimizations of BitSet) ?

Sam

On Tue, Sep 30, 2008 at 10:50 AM, Markus Kohler <markus.kohler at gmail.com> wrote:
> Hi all,
> boolean[] uses one byte per Array, but allows concurrent access (to
> different entries), whereas BitSet uses one bit (approximately) per
> "boolean" but doesn't allow concurrent access.
> Does anyone here know whether there's an implementation of BitSet that would
> allow concurrent access but still would (approximately) use one bit?
>
> I'm inclined to believe that such a thing should be possible, because one
> could use AtomIntegerArray as a backup and use striping to allow efficient
> concurrent access.
>
>
>
> Regards,
> Markus
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From martinrb at google.com  Tue Sep 30 14:46:44 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 30 Sep 2008 11:46:44 -0700
Subject: [concurrency-interest] Concurrent BitSet?
In-Reply-To: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
References: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
Message-ID: <1ccfd1c10809301146q34e29691p818ddf63fdc9ba01@mail.gmail.com>

On Tue, Sep 30, 2008 at 07:50, Markus Kohler <markus.kohler at gmail.com> wrote:
> Hi all,
> boolean[] uses one byte per Array, but allows concurrent access (to
> different entries), whereas BitSet uses one bit (approximately) per
> "boolean" but doesn't allow concurrent access.
> Does anyone here know whether there's an implementation of BitSet that would
> allow concurrent access but still would (approximately) use one bit?
>
> I'm inclined to believe that such a thing should be possible, because one
> could use AtomIntegerArray as a backup and use striping to allow efficient
> concurrent access.

I agree that
one should be able to straightforwardly translate BitSet
replacing the long[] with AtomicIntegerArray
or AtomicLongArray and CAS the array elements.

Martin

From martinrb at google.com  Tue Sep 30 15:56:46 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 30 Sep 2008 12:56:46 -0700
Subject: [concurrency-interest] Concurrent BitSet?
In-Reply-To: <19196d860809301005r2e53bf60tcd3462b060fdf172@mail.gmail.com>
References: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
	<19196d860809301005r2e53bf60tcd3462b060fdf172@mail.gmail.com>
Message-ID: <1ccfd1c10809301256n3e2fb2efoc1d28a80e30d00c2@mail.gmail.com>

I think a number of student-quality projects have tried to tackle the problem
of creating an IntSet that is both space- and time-efficient.
But I don't know of any that is suitable for import into Google.
It's especially tricky if you also want it to be concurrent.

Here's an effort that unfortunately has an unsuitable license,
but is nevertheless a good start for further investigation:
http://www.iis.uni-stuttgart.de/intset/

Martin

On Tue, Sep 30, 2008 at 10:05, Sam Berlin <sberlin at gmail.com> wrote:
> In addition to that, is anyone aware of a good SparseBitSet (that
> doesn't allocate the leading long[]'s when only some trailing bits are
> present, yet still has optimizations of BitSet) ?

From Thomas.Hawtin at Sun.COM  Tue Sep 30 16:19:06 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Tue, 30 Sep 2008 21:19:06 +0100
Subject: [concurrency-interest] Concurrent BitSet?
In-Reply-To: <1ccfd1c10809301146q34e29691p818ddf63fdc9ba01@mail.gmail.com>
References: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
	<1ccfd1c10809301146q34e29691p818ddf63fdc9ba01@mail.gmail.com>
Message-ID: <48E289BA.1020900@sun.com>

Martin Buchholz wrote:
> 
> I agree that
> one should be able to straightforwardly translate BitSet
> replacing the long[] with AtomicIntegerArray
> or AtomicLongArray and CAS the array elements.

Expanding the array makes it a little more difficult.

Tom Hawtin

From martinrb at google.com  Tue Sep 30 16:28:40 2008
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 30 Sep 2008 13:28:40 -0700
Subject: [concurrency-interest] Concurrent BitSet?
In-Reply-To: <1ccfd1c10809301256n3e2fb2efoc1d28a80e30d00c2@mail.gmail.com>
References: <771905290809300750g3a104630ib34d8354ca72c779@mail.gmail.com>
	<19196d860809301005r2e53bf60tcd3462b060fdf172@mail.gmail.com>
	<1ccfd1c10809301256n3e2fb2efoc1d28a80e30d00c2@mail.gmail.com>
Message-ID: <1ccfd1c10809301328i1bf13700lb414f9b09698eaaf@mail.gmail.com>

Oops....sorry....I thought I was sending to a Google-internal mailing list.

Martin

On Tue, Sep 30, 2008 at 12:56, Martin Buchholz <martinrb at google.com> wrote:
> But I don't know of any that is suitable for import into Google.

