From boehm at acm.org  Thu Jan  1 02:38:50 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 31 Dec 2014 23:38:50 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe0-+78XNH84MMXvxUex6kRu6JhENd2WVGXbSktUj_MGd8w@mail.gmail.com>
References: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>
	<CA+kOe08CtsoDNzcMEyRo91XR1DO+CqDggwha4gW1VR0LhsAaKQ@mail.gmail.com>
	<5481108A.3030605@oracle.com>
	<CA+kOe08OC57hEgGR-3CSfsJUMVSk-_mCmZK2Gd=3tpkE9=6j_w@mail.gmail.com>
	<5488B4EF.6060103@redhat.com> <5488EB09.9000501@oracle.com>
	<54895EFB.3060204@redhat.com>
	<CA+kOe0_QOXi=mvLdXvYn4EF4J54HKDJVo=S4VBDUjujtpzgYzw@mail.gmail.com>
	<5490EA49.2050406@oracle.com> <54914CBE.2000201@gmail.com>
	<CA+kOe0-+78XNH84MMXvxUex6kRu6JhENd2WVGXbSktUj_MGd8w@mail.gmail.com>
Message-ID: <CAPUmR1aDm+pz0GKQ5YAVbj2c5UkuoabBS9gEsVOfRJQiANCyhQ@mail.gmail.com>

If we look at using purely store fences and purely load fences in the
"initialized flag" example as in this discussion, I think it's worth
distinguishing too possible scenarios:

1) We guarantee some form of dependency-based ordering, as most real
computer architectures do.  This probably invalidates the example from my
committee paper that's under discussion here.  The problem is, as always,
that we don't know how to make this precise at the programming language
level.  It's the compiler's job to break certain dependencies, like the
dependency of the store to x on the load of y in x = 0 * y.  Many people
are thinking about this problem, both to deal with "out-of-thin-air" issues
correctly in various memory models, and to design a version of C++'s
memory_order_consume that's more usable.  If we had a way to guarantee some
well-defined notion of dependency-based ordering, then at least some of the
examples here would need to be revisited.

2) We don't guarantee that dependencies imply any sort of ordering.  Then I
think the weird example under discussion here stands.  There is officially
nothing to prevent the load of x.a in thread 1 from being reordered with
the store to x_init.

But there may actually be better examples as to why the store-store
ordering in the initializing thread is not always enough.  Consider:

Thread 1:
x.a = 1;
if (x.a != 1) world_is_broken = true;
StoreStore fence;
x_init = true;
...
if (world_is_broken) die();

Thread 2:
if (x_init) {
    full fence;
    x.a++;
}

I think there is nothing to prevent the read of x.a in Thread 1 from seeing
the incremented value, at least if (1) the compiler promotes
world_is_broken to a register, and  (2) at the assembly level the store to
x_init is not dependent on the load of x.a.  (1) seems quite plausible, and
(2) seems very reasonable if the architecture has a conditional move
instruction or the like.  (For Itanium, (2) holds even for the naive
compilation.)

This is not a particularly likely scenario, but I have no idea how would
concoct programming rules that would guarantee to prevent this kind of
weirdness.  The first two statements of Thread 1 might appear inside an
"initialize a" library routine that knows nothing about concurrency.

Hans


On Wed, Dec 17, 2014 at 10:54 AM, Martin Buchholz <martinrb at google.com>
wrote:

> On Wed, Dec 17, 2014 at 1:28 AM, Peter Levart <peter.levart at gmail.com>
> wrote:
> > On 12/17/2014 03:28 AM, David Holmes wrote:
> >>
> >> On 17/12/2014 10:06 AM, Martin Buchholz wrote:
> >> Hans allows for the nonsensical, in my view, possibility that the load
> of
> >> x.a can happen after the x_init=true store and yet somehow be subject
> to the
> >> ++ and the ensuing store that has to come before the x_init = true.
> >
> > Perhaps, he is speaking about why it is dangerous to replace BOTH release
> > with just store-store AND acquire with just load-load?
>
> I'm pretty sure he's talking about weakening EITHER.
>
> """Clearly, and unsurprisingly, it is unsafe to replace the
> load_acquire with a version that restricts only load ordering in this
> case. That would allow the store to x in thread 2 to become visible
> before the initialization of x by thread 1 is complete, possibly
> losing the update, or corrupting the state of x during initialization.
>
> More interestingly, it is also generally unsafe to restrict the
> release ordering constraint in thread 1 to only stores."""
>
> (What's "clear and unsurprising" to Hans may not be to the rest of us)
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141231/9dbb9d3b/attachment-0001.html>

From dl at cs.oswego.edu  Mon Jan  5 07:03:03 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 05 Jan 2015 07:03:03 -0500
Subject: [concurrency-interest] jdk9 preview packaging
Message-ID: <54AA7D77.3060505@cs.oswego.edu>


For the past three JDK releases, we've packaged up preliminary
versions of new classes separately (jsr166{x,y,e}) to allow
use with current releases before integrating into OpenJDK.
But there's less motivation for doing this these days.

So our current plans are to update them directly in our
main repository, that can be used in JDK8/9 by getting
   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
and then running with -Xbootclasspath/p:jsr166.jar

We also plan to more frequently integrate into OpenJDK9.

As mentioned before, the main planned j.u.c additions for JDK9
are Async utilities for CompletableFutures, CompletionStages,
and tie-ins to async-io, timers, and reactive-stream APIs.
More about these soon.

-Doug




From daniel.mitterdorfer at gmail.com  Tue Jan  6 13:54:15 2015
From: daniel.mitterdorfer at gmail.com (Daniel Mitterdorfer)
Date: Tue, 6 Jan 2015 19:54:15 +0100
Subject: [concurrency-interest] instructions reordering in java -
 practical check tools/principals/papers
In-Reply-To: <54A369FA.6090901@flyingtroika.com>
References: <54A369FA.6090901@flyingtroika.com>
Message-ID: <CAJmnTFjMWigxhsr-Yx-4ZhVEJfWCX2ATzQh9gShx8MteKMpF3g@mail.gmail.com>

Hi,

I am not sure what you want to do but you might want to look into JITWatch
[1] written by Chris Newland. It presents Java code, along with byte code
instructions and generated assembly instructions. This might be a starting
point for your analysis.

Bye,

Daniel

[1] JITWatch: https://github.com/AdoptOpenJDK/jitwatch

2014-12-31 4:14 GMT+01:00 DT <dt at flyingtroika.com>:

> What would be a good way or tools to check of the reordering of
> instructions in java starting from java code-> compiler ->jvm reordering
> ->processor reordering?
>
>
> Thanks,
> DT
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150106/3256cc16/attachment.html>

From dt at flyingtroika.com  Wed Jan  7 00:21:06 2015
From: dt at flyingtroika.com (DT)
Date: Tue, 06 Jan 2015 21:21:06 -0800
Subject: [concurrency-interest] instructions reordering in java -
 practical check tools/principals/papers
In-Reply-To: <CAJmnTFjMWigxhsr-Yx-4ZhVEJfWCX2ATzQh9gShx8MteKMpF3g@mail.gmail.com>
References: <54A369FA.6090901@flyingtroika.com>
	<CAJmnTFjMWigxhsr-Yx-4ZhVEJfWCX2ATzQh9gShx8MteKMpF3g@mail.gmail.com>
Message-ID: <54ACC242.4060005@flyingtroika.com>

I am not sure if this is the right way to explore this question, but 
thought it would be interesting to see  how byte code execution 
(opcodes) can be checked for reordering through entire cycle up to the 
processor.
For instance, there are two opcodes, monitorEnter and monitorExist, but 
in terms of synchronization constructions we have  semaphores, 
reentrantlocks, synchronized stmts, etc. so we should analyze different 
opcode combinations based on the synchronization construct and influence 
on the jvm execution of those opcodes to be able to understand if there 
are any issues.
Then, once we see how jvm is performing, the next step would be to 
compare whether  there is any difference in program behavior based on 
the specific garbage collector and cpu architecture.


On 1/6/2015 10:54 AM, Daniel Mitterdorfer wrote:
> Hi,
>
> I am not sure what you want to do but you might want to look into 
> JITWatch [1] written by Chris Newland. It presents Java code, along 
> with byte code instructions and generated assembly instructions. This 
> might be a starting point for your analysis.
>
> Bye,
>
> Daniel
>
> [1] JITWatch: https://github.com/AdoptOpenJDK/jitwatch
>
> 2014-12-31 4:14 GMT+01:00 DT <dt at flyingtroika.com 
> <mailto:dt at flyingtroika.com>>:
>
>     What would be a good way or tools to check of the reordering of
>     instructions in java starting from java code-> compiler ->jvm
>     reordering ->processor reordering?
>
>
>     Thanks,
>     DT
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150106/7011351f/attachment.html>

From daniel.mitterdorfer at gmail.com  Thu Jan  8 08:53:00 2015
From: daniel.mitterdorfer at gmail.com (Daniel Mitterdorfer)
Date: Thu, 8 Jan 2015 14:53:00 +0100
Subject: [concurrency-interest] instructions reordering in java -
 practical check tools/principals/papers
In-Reply-To: <54ACC242.4060005@flyingtroika.com>
References: <54A369FA.6090901@flyingtroika.com>
	<CAJmnTFjMWigxhsr-Yx-4ZhVEJfWCX2ATzQh9gShx8MteKMpF3g@mail.gmail.com>
	<54ACC242.4060005@flyingtroika.com>
Message-ID: <CAJmnTFjiMroWHpOifzC23e+a+eiOpeQiizGewj9TCmciygiVDA@mail.gmail.com>

Hi,

you can follow such reorderings down to assembly level with JITWatch as its
assembly view is based on HotSpot's assembly log output (see also [1]).
Beyond that level it can get a bit tricky. :)

Regarding your concrete example with synchronization: As far as I am aware,
the high level abstractions in j.u.c are mostly built on top of methods in
sun.misc.Unsafe so you need to look for calls to sun.misc.Unsafe instead of
monitorenter and monitorexit. As HotSpot implements some optimizations
regarding locks (see [2]) you should expect that synchronization code might
be eliminated alltogether after the JIT compiler is done (see [3]).

If you are interested in implementation guidance of synchronization
primitives on JVM level you can look into Doug Lea's "JSR-133 Cookbook for
Compiler Writers" [4] and want to continue with some articles on specifics
like "NOOP Memory Barriers on x86 are NOT FREE " by Nitsan Wakart [5]. Last
but not least you can also look into the OpenJDK source code [6].

Bye,

Daniel

[1] https://wikis.oracle.com/display/HotSpotInternals/PrintAssembly
[2] https://wikis.oracle.com/display/HotSpotInternals/PerformanceTacticIndex
[3] http://www.ibm.com/developerworks/library/j-jtp10185/index.html
[4] http://gee.cs.oswego.edu/dl/jmm/cookbook.html
[5] http://psy-lob-saw.blogspot.de/2013/08/memory-barriers-are-not-free.html
[6] http://download.java.net/openjdk/jdk8/

2015-01-07 6:21 GMT+01:00 DT <dt at flyingtroika.com>:

>  I am not sure if this is the right way to explore this question, but
> thought it would be interesting to see  how byte code execution (opcodes)
> can be checked for reordering through entire cycle up to the processor.
> For instance, there are two opcodes, monitorEnter and monitorExist, but in
> terms of synchronization constructions we have  semaphores, reentrantlocks,
> synchronized stmts, etc. so we should analyze different opcode combinations
> based on the synchronization construct and influence on the jvm execution
> of those opcodes to be able to understand if there are any issues.
> Then, once we see how jvm is performing, the next step would be to compare
> whether  there is any difference in program behavior based on the specific
> garbage collector and cpu architecture.
>
>
> On 1/6/2015 10:54 AM, Daniel Mitterdorfer wrote:
>
>   Hi,
>
>  I am not sure what you want to do but you might want to look into
> JITWatch [1] written by Chris Newland. It presents Java code, along with
> byte code instructions and generated assembly instructions. This might be a
> starting point for your analysis.
>
>  Bye,
>
>  Daniel
>
> [1] JITWatch: https://github.com/AdoptOpenJDK/jitwatch
>
> 2014-12-31 4:14 GMT+01:00 DT <dt at flyingtroika.com>:
>
>> What would be a good way or tools to check of the reordering of
>> instructions in java starting from java code-> compiler ->jvm reordering
>> ->processor reordering?
>>
>>
>> Thanks,
>> DT
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150108/3115589f/attachment.html>

From thurston at nomagicsoftware.com  Thu Jan  8 13:59:03 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 8 Jan 2015 11:59:03 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
Message-ID: <1420743543652-11812.post@n7.nabble.com>

Hello,

Does an unpark(...)/(return from) park() pair constitute an happens-before
edge?

Looking at the javadoc there isn't the expected "has memory visibility
effects of . . ." comment, but I guess I just always assumed it does.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Thu Jan  8 14:31:45 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 08 Jan 2015 19:31:45 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420743543652-11812.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
Message-ID: <54AEDB21.8070207@oracle.com>

No. You may consider them no-ops with no loss of generality.

In practice, though, unpark will have a full barrier, and both will 
behave like full compiler barriers.

Alex

On 08/01/2015 18:59, thurstonn wrote:
> Hello,
>
> Does an unpark(...)/(return from) park() pair constitute an happens-before
> edge?
>
> Looking at the javadoc there isn't the expected "has memory visibility
> effects of . . ." comment, but I guess I just always assumed it does.
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From mr.chrisvest at gmail.com  Thu Jan  8 16:31:36 2015
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Thu, 8 Jan 2015 22:31:36 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54AEDB21.8070207@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
Message-ID: <5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>

In different terms, park() is allowed to return spuriously, that is, for no apparent reason. So you should always check that your expected condition for unparking was met. This check usually implies the ordering guarantees one needs, though the onus is on the programmer to ensure this.

Chris

> On 08/01/2015, at 20.31, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
> 
> No. You may consider them no-ops with no loss of generality.
> 
> In practice, though, unpark will have a full barrier, and both will behave like full compiler barriers.
> 
> Alex
> 
>> On 08/01/2015 18:59, thurstonn wrote:
>> Hello,
>> 
>> Does an unpark(...)/(return from) park() pair constitute an happens-before
>> edge?
>> 
>> Looking at the javadoc there isn't the expected "has memory visibility
>> effects of . . ." comment, but I guess I just always assumed it does.
>> 
>> 
>> 
>> --
>> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Thu Jan  8 17:20:45 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 08 Jan 2015 22:20:45 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>
Message-ID: <54AF02BD.3090407@oracle.com>

Yes. Let's also take this opportunity to remind ourselves that wait() 
can also return spuriously, so park() is no different in this respect.

Alex

On 08/01/2015 21:31, Chris Vest wrote:
> In different terms, park() is allowed to return spuriously, that is, for no apparent reason. So you should always check that your expected condition for unparking was met. This check usually implies the ordering guarantees one needs, though the onus is on the programmer to ensure this.
>
> Chris
>
>> On 08/01/2015, at 20.31, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>
>> No. You may consider them no-ops with no loss of generality.
>>
>> In practice, though, unpark will have a full barrier, and both will behave like full compiler barriers.
>>
>> Alex
>>
>>> On 08/01/2015 18:59, thurstonn wrote:
>>> Hello,
>>>
>>> Does an unpark(...)/(return from) park() pair constitute an happens-before
>>> edge?
>>>
>>> Looking at the javadoc there isn't the expected "has memory visibility
>>> effects of . . ." comment, but I guess I just always assumed it does.
>>>
>>>
>>>
>>> --
>>> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From yechielf at gigaspaces.com  Fri Jan  9 03:31:23 2015
From: yechielf at gigaspaces.com (Yechiel Feffer)
Date: Fri, 9 Jan 2015 08:31:23 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54AF02BD.3090407@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>,
	<54AF02BD.3090407@oracle.com>
Message-ID: <6AB258A8-879A-4DCF-98AD-047F06A396AE@gigaspaces.com>

Can someone please elaborate: why Apis like wait and park are "allowed to return spuriously"?
More over- aren't those basic java services (certainly wait) that have to provide a minimal QoS like tcp/ip and mask the caller from recheck loop stuff that should be done on the jvm level?
Yechiel



> On 9 ???? 2015, at 00:30, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:
> 
> Yes. Let's also take this opportunity to remind ourselves that wait() can also return spuriously, so park() is no different in this respect.
> 
> Alex
> 
>> On 08/01/2015 21:31, Chris Vest wrote:
>> In different terms, park() is allowed to return spuriously, that is, for no apparent reason. So you should always check that your expected condition for unparking was met. This check usually implies the ordering guarantees one needs, though the onus is on the programmer to ensure this.
>> 
>> Chris
>> 
>>> On 08/01/2015, at 20.31, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>> 
>>> No. You may consider them no-ops with no loss of generality.
>>> 
>>> In practice, though, unpark will have a full barrier, and both will behave like full compiler barriers.
>>> 
>>> Alex
>>> 
>>>> On 08/01/2015 18:59, thurstonn wrote:
>>>> Hello,
>>>> 
>>>> Does an unpark(...)/(return from) park() pair constitute an happens-before
>>>> edge?
>>>> 
>>>> Looking at the javadoc there isn't the expected "has memory visibility
>>>> effects of . . ." comment, but I guess I just always assumed it does.
>>>> 
>>>> 
>>>> 
>>>> --
>>>> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Fri Jan  9 03:54:25 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 9 Jan 2015 18:54:25 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <6AB258A8-879A-4DCF-98AD-047F06A396AE@gigaspaces.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEFCKMAA.davidcholmes@aapt.net.au>

The primitives are allowed to return spuriously to allow for more efficient
implementation techniques - and it helps reinforce that they must generally
be used in loops that re-check what is actually being waited for. The
"retries" are built into higher-level concurrency constructs like Semaphore,
CountdownLatch, AbstractQueuedSynchronizer, etc.

This question get raised and answered periodically so you should be able to
find discussion in the archives.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> Yechiel Feffer
> Sent: Friday, 9 January 2015 6:31 PM
> To: Oleksandr Otenko
> Cc: thurstonn; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> Can someone please elaborate: why Apis like wait and park are
> "allowed to return spuriously"?
> More over- aren't those basic java services (certainly wait) that
> have to provide a minimal QoS like tcp/ip and mask the caller
> from recheck loop stuff that should be done on the jvm level?
> Yechiel
>
>
>
> > On 9 ???? 2015, at 00:30, "Oleksandr Otenko"
> <oleksandr.otenko at oracle.com> wrote:
> >
> > Yes. Let's also take this opportunity to remind ourselves that
> wait() can also return spuriously, so park() is no different in
> this respect.
> >
> > Alex
> >
> >> On 08/01/2015 21:31, Chris Vest wrote:
> >> In different terms, park() is allowed to return spuriously,
> that is, for no apparent reason. So you should always check that
> your expected condition for unparking was met. This check usually
> implies the ordering guarantees one needs, though the onus is on
> the programmer to ensure this.
> >>
> >> Chris
> >>
> >>> On 08/01/2015, at 20.31, Oleksandr Otenko
> <oleksandr.otenko at oracle.com> wrote:
> >>>
> >>> No. You may consider them no-ops with no loss of generality.
> >>>
> >>> In practice, though, unpark will have a full barrier, and
> both will behave like full compiler barriers.
> >>>
> >>> Alex
> >>>
> >>>> On 08/01/2015 18:59, thurstonn wrote:
> >>>> Hello,
> >>>>
> >>>> Does an unpark(...)/(return from) park() pair constitute an
> happens-before
> >>>> edge?
> >>>>
> >>>> Looking at the javadoc there isn't the expected "has memory
> visibility
> >>>> effects of . . ." comment, but I guess I just always assumed it does.
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812.html
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Fri Jan  9 07:14:06 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 09 Jan 2015 12:14:06 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <6AB258A8-879A-4DCF-98AD-047F06A396AE@gigaspaces.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54AEDB21.8070207@oracle.com>	<5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>,
	<54AF02BD.3090407@oracle.com>
	<6AB258A8-879A-4DCF-98AD-047F06A396AE@gigaspaces.com>
Message-ID: <54AFC60E.3090802@oracle.com>

Efficiency of implementation aside, how do you define the notify() that 
the wait() should wake up for? All you can tell is that the notify() 
preceding the return from wait() wakes it up. But how do you define the 
condition that if the wait() you were thinking of has already been 
notified(), then don't notify(), don't trigger the wake up of any wait() 
that may occur subsequently? Once you realize this problem with the API, 
you will see that the wait() has to be relaxed - it may return without 
you knowing why. Once it is that relaxed, it doesn't matter any more 
whether the notify() is the sole reason the wait() may return, and we 
see the formulation "wait() may return spuriously".


Alex


On 09/01/2015 08:31, Yechiel Feffer wrote:
> Can someone please elaborate: why Apis like wait and park are "allowed to return spuriously"?
> More over- aren't those basic java services (certainly wait) that have to provide a minimal QoS like tcp/ip and mask the caller from recheck loop stuff that should be done on the jvm level?
> Yechiel
>
>
>
>> On 9 ???? 2015, at 00:30, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:
>>
>> Yes. Let's also take this opportunity to remind ourselves that wait() can also return spuriously, so park() is no different in this respect.
>>
>> Alex
>>
>>> On 08/01/2015 21:31, Chris Vest wrote:
>>> In different terms, park() is allowed to return spuriously, that is, for no apparent reason. So you should always check that your expected condition for unparking was met. This check usually implies the ordering guarantees one needs, though the onus is on the programmer to ensure this.
>>>
>>> Chris
>>>
>>>> On 08/01/2015, at 20.31, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>>>
>>>> No. You may consider them no-ops with no loss of generality.
>>>>
>>>> In practice, though, unpark will have a full barrier, and both will behave like full compiler barriers.
>>>>
>>>> Alex
>>>>
>>>>> On 08/01/2015 18:59, thurstonn wrote:
>>>>> Hello,
>>>>>
>>>>> Does an unpark(...)/(return from) park() pair constitute an happens-before
>>>>> edge?
>>>>>
>>>>> Looking at the javadoc there isn't the expected "has memory visibility
>>>>> effects of . . ." comment, but I guess I just always assumed it does.
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Fri Jan  9 13:05:53 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 9 Jan 2015 11:05:53 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54AEDB21.8070207@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
Message-ID: <1420826753248-11819.post@n7.nabble.com>

smacks a bit like no means yes.

It does bring up an interesting question:  why is the "intrinsic"
happens-before rule list as it is?
Other than monitors and volatiles, of course.
Is the logic something like: any known/possible/reasonable implementation is
necessarily going to provide the memory model guarantees, or is it the
semantics that drive it?

Take thread interruption - largely analogous to (un)park (except for the
spurious de-parks), and I would guess largely similar implementations (?). 
So why would interrupts provide hb, and not park?
I wonder if AQS, e.g., would have been implemented slightly differently if
the hb guarantee was there



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11819.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Fri Jan  9 16:57:23 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 9 Jan 2015 14:57:23 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
Message-ID: <1420840643988-11820.post@n7.nabble.com>

Hello,

I was curious as to why there isn't an AtomicReference CAS:

T cas(T expectedValue, T newValue)

where the returned T represents the oldValue (if it == expectedValue, then
CAS succeeded).

(LOCK) CMPXCHG on x86* platforms provides this behavior directly.

It's more flexible than the present
boolean compareAndSet(T expected, T new), which is fine as it goes, but is
not equivalent

Is it because other JVM platforms don't natively provide the equivalent
instruction?





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Fri Jan  9 18:04:53 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 9 Jan 2015 18:04:53 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420840643988-11820.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
Message-ID: <CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>

So the hardware is going to return the unexpected value - that's going to
be an oop address/pointer.  Are you suggesting the JVM should go and load
that object as well?

Sent from my phone
On Jan 9, 2015 5:22 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Hello,
>
> I was curious as to why there isn't an AtomicReference CAS:
>
> T cas(T expectedValue, T newValue)
>
> where the returned T represents the oldValue (if it == expectedValue, then
> CAS succeeded).
>
> (LOCK) CMPXCHG on x86* platforms provides this behavior directly.
>
> It's more flexible than the present
> boolean compareAndSet(T expected, T new), which is fine as it goes, but is
> not equivalent
>
> Is it because other JVM platforms don't natively provide the equivalent
> instruction?
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150109/21562781/attachment.html>

From davidcholmes at aapt.net.au  Fri Jan  9 18:27:23 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 10 Jan 2015 09:27:23 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54AFC60E.3090802@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEFEKMAA.davidcholmes@aapt.net.au>

If you wait for different conditions then you can't use notify() in the
first place, but must use notifyAll(). It is entirely possible that wait()
be implemented such that a thread only gets woken when a notification
occurs, but that doesn't tell the thread anything about what it was waiting
for - hence the need for the loop regardless of spurious wakeups.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> Oleksandr Otenko
> Sent: Friday, 9 January 2015 10:14 PM
> To: Yechiel Feffer
> Cc: thurstonn; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> Efficiency of implementation aside, how do you define the notify() that
> the wait() should wake up for? All you can tell is that the notify()
> preceding the return from wait() wakes it up. But how do you define the
> condition that if the wait() you were thinking of has already been
> notified(), then don't notify(), don't trigger the wake up of any wait()
> that may occur subsequently? Once you realize this problem with the API,
> you will see that the wait() has to be relaxed - it may return without
> you knowing why. Once it is that relaxed, it doesn't matter any more
> whether the notify() is the sole reason the wait() may return, and we
> see the formulation "wait() may return spuriously".
>
>
> Alex
>
>
> On 09/01/2015 08:31, Yechiel Feffer wrote:
> > Can someone please elaborate: why Apis like wait and park are
> "allowed to return spuriously"?
> > More over- aren't those basic java services (certainly wait)
> that have to provide a minimal QoS like tcp/ip and mask the
> caller from recheck loop stuff that should be done on the jvm level?
> > Yechiel
> >
> >
> >
> >> On 9 ???? 2015, at 00:30, "Oleksandr Otenko"
> <oleksandr.otenko at oracle.com> wrote:
> >>
> >> Yes. Let's also take this opportunity to remind ourselves that
> wait() can also return spuriously, so park() is no different in
> this respect.
> >>
> >> Alex
> >>
> >>> On 08/01/2015 21:31, Chris Vest wrote:
> >>> In different terms, park() is allowed to return spuriously,
> that is, for no apparent reason. So you should always check that
> your expected condition for unparking was met. This check usually
> implies the ordering guarantees one needs, though the onus is on
> the programmer to ensure this.
> >>>
> >>> Chris
> >>>
> >>>> On 08/01/2015, at 20.31, Oleksandr Otenko
> <oleksandr.otenko at oracle.com> wrote:
> >>>>
> >>>> No. You may consider them no-ops with no loss of generality.
> >>>>
> >>>> In practice, though, unpark will have a full barrier, and
> both will behave like full compiler barriers.
> >>>>
> >>>> Alex
> >>>>
> >>>>> On 08/01/2015 18:59, thurstonn wrote:
> >>>>> Hello,
> >>>>>
> >>>>> Does an unpark(...)/(return from) park() pair constitute an
> happens-before
> >>>>> edge?
> >>>>>
> >>>>> Looking at the javadoc there isn't the expected "has memory
> visibility
> >>>>> effects of . . ." comment, but I guess I just always
> assumed it does.
> >>>>>
> >>>>>
> >>>>>
> >>>>> --
> >>>>> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Fri Jan  9 18:30:51 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 10 Jan 2015 09:30:51 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420826753248-11819.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>

Interruption is a top-level Thread API so it was decided not unreasonable to
have it behave as-if there were a "volatile boolean interrupted" field in
Thread. Hence the HB relation.

Park/unpark is the lowest-level of API and in 99.99% of cases will be used
in conjunction with volatile state variables, which provide the HB
relations. It would have been an excessive burden to require HB for the
unpark/park themselves.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> thurstonn
> Sent: Saturday, 10 January 2015 4:06 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> smacks a bit like no means yes.
>
> It does bring up an interesting question:  why is the "intrinsic"
> happens-before rule list as it is?
> Other than monitors and volatiles, of course.
> Is the logic something like: any known/possible/reasonable
> implementation is
> necessarily going to provide the memory model guarantees, or is it the
> semantics that drive it?
>
> Take thread interruption - largely analogous to (un)park (except for the
> spurious de-parks), and I would guess largely similar
> implementations (?).
> So why would interrupts provide hb, and not park?
> I wonder if AQS, e.g., would have been implemented slightly differently if
> the hb guarantee was there
>
>
>
> --
> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11819.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Fri Jan  9 18:40:40 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 9 Jan 2015 23:40:40 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81801@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> So the hardware is going to return the unexpected value - that's
> going to be an oop address/pointer. Are you suggesting the JVM
> should go and load that object as well?

Are you saying that the value sitting in the accumulator register
isn't just the reference value to be returned? The referenced object
doesn't have to be loaded before returning it. Or is there some kind
of additional pointer indirection going on? Thurston's question is
still valid looking at AtomicInteger instead of AtomicReference, if
that helps clarify the issue. I'm very curious about the answer
myself. :)

Thurston: There's also getAndSet and (in Java 8) getAndUpdate, which
do return the old value. They have loops calling compareAndSet
internally. Presumably if they could be implemented better with a
proper compare-and-swap the implementers could make that change and
preserve the API, though they would still require loops; they would
just eliminate an extra volatile load per retry in the presence of
contention.

Cheers,
Justin


From vitalyd at gmail.com  Fri Jan  9 19:11:07 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 9 Jan 2015 19:11:07 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81801@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81801@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37HUenZU0BxR=Ev2PwjRoTk6EhsAsztQ2K=+A61g=HSO4Q@mail.gmail.com>

Sorry, I didn't really phrase my reply/question properly.  If you were to
return the current value, isn't it the case that by the time the caller
sees it that it may no longer be the actual value/object there? For
AtomicInteger I think that's fine because it's just a value type, but I'm
not sure that's very useful for references.  Also, the caller presumably
would have to do their own reference identity check to figure out whether
the cas succeeded or not, making the code longer for an (I think?) uncommon
use case.

To answer your indirection question, no, I don't think there's any extra
indirection; the register will contain the address (raw ptr or encoded
object ptr if compressed oops are used) of the object, but that's all
that's needed to return T.  Again, the confusion is my fault as I see my
original reply insinuated that.  My real question is what is one going to
do with that T considering it's possibly stale as soon as you get it back.

Sent from my phone
Vitaly Davidovich wrote:

> So the hardware is going to return the unexpected value - that's
> going to be an oop address/pointer. Are you suggesting the JVM
> should go and load that object as well?

Are you saying that the value sitting in the accumulator register
isn't just the reference value to be returned? The referenced object
doesn't have to be loaded before returning it. Or is there some kind
of additional pointer indirection going on? Thurston's question is
still valid looking at AtomicInteger instead of AtomicReference, if
that helps clarify the issue. I'm very curious about the answer
myself. :)

Thurston: There's also getAndSet and (in Java 8) getAndUpdate, which
do return the old value. They have loops calling compareAndSet
internally. Presumably if they could be implemented better with a
proper compare-and-swap the implementers could make that change and
preserve the API, though they would still require loops; they would
just eliminate an extra volatile load per retry in the presence of
contention.

Cheers,
Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150109/54287006/attachment-0001.html>

From dl at cs.oswego.edu  Fri Jan  9 19:20:55 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 Jan 2015 19:20:55 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420840643988-11820.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
Message-ID: <54B07067.30103@cs.oswego.edu>

On 01/09/2015 04:57 PM, thurstonn wrote:
> Hello,
>
> I was curious as to why there isn't an AtomicReference CAS:
>
> T cas(T expectedValue, T newValue)
>
> where the returned T represents the oldValue (if it == expectedValue, then
> CAS succeeded).

Because there is no equivalent on processors (including ARM, POWER)
implementing compareAndSet using load-linked/store-conditional.
(C++11/C11 make the same choice for the same reason.)

-Doug



>
> (LOCK) CMPXCHG on x86* platforms provides this behavior directly.
>
> It's more flexible than the present
> boolean compareAndSet(T expected, T new), which is fine as it goes, but is
> not equivalent
>
> Is it because other JVM platforms don't natively provide the equivalent
> instruction?
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Fri Jan  9 19:27:04 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 9 Jan 2015 19:27:04 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HUenZU0BxR=Ev2PwjRoTk6EhsAsztQ2K=+A61g=HSO4Q@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81801@sm-ex-01-vm.guidewire.com>
	<CAHjP37HUenZU0BxR=Ev2PwjRoTk6EhsAsztQ2K=+A61g=HSO4Q@mail.gmail.com>
Message-ID: <CAHjP37HJ-8XhaJaZujuC8S7ytmTZpUSr27L_D-i2H_GjnXyr6g@mail.gmail.com>

Actually, thinking about it a bit more, if you write (pseudocode):

Foo old = AtomicReference<Foo>.cas (expected, new);

Wouldn't the JVM have to do a type check to verify it's a Foo? Perhaps it
won't if you don't dereference it though (I.e. ref check only).

Sent from my phone
On Jan 9, 2015 7:11 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Sorry, I didn't really phrase my reply/question properly.  If you were to
> return the current value, isn't it the case that by the time the caller
> sees it that it may no longer be the actual value/object there? For
> AtomicInteger I think that's fine because it's just a value type, but I'm
> not sure that's very useful for references.  Also, the caller presumably
> would have to do their own reference identity check to figure out whether
> the cas succeeded or not, making the code longer for an (I think?) uncommon
> use case.
>
> To answer your indirection question, no, I don't think there's any extra
> indirection; the register will contain the address (raw ptr or encoded
> object ptr if compressed oops are used) of the object, but that's all
> that's needed to return T.  Again, the confusion is my fault as I see my
> original reply insinuated that.  My real question is what is one going to
> do with that T considering it's possibly stale as soon as you get it back.
>
> Sent from my phone
> Vitaly Davidovich wrote:
>
> > So the hardware is going to return the unexpected value - that's
> > going to be an oop address/pointer. Are you suggesting the JVM
> > should go and load that object as well?
>
> Are you saying that the value sitting in the accumulator register
> isn't just the reference value to be returned? The referenced object
> doesn't have to be loaded before returning it. Or is there some kind
> of additional pointer indirection going on? Thurston's question is
> still valid looking at AtomicInteger instead of AtomicReference, if
> that helps clarify the issue. I'm very curious about the answer
> myself. :)
>
> Thurston: There's also getAndSet and (in Java 8) getAndUpdate, which
> do return the old value. They have loops calling compareAndSet
> internally. Presumably if they could be implemented better with a
> proper compare-and-swap the implementers could make that change and
> preserve the API, though they would still require loops; they would
> just eliminate an extra volatile load per retry in the presence of
> contention.
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150109/ca7c3f57/attachment.html>

From jsampson at guidewire.com  Fri Jan  9 23:26:34 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 10 Jan 2015 04:26:34 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HJ-8XhaJaZujuC8S7ytmTZpUSr27L_D-i2H_GjnXyr6g@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<CAHjP37HFNfhMUFN7R_vOBEteZyhHSjXZ7n=54awHSr6QX7v2NA@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81801@sm-ex-01-vm.guidewire.com>
	<CAHjP37HUenZU0BxR=Ev2PwjRoTk6EhsAsztQ2K=+A61g=HSO4Q@mail.gmail.com>
	<CAHjP37HJ-8XhaJaZujuC8S7ytmTZpUSr27L_D-i2H_GjnXyr6g@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D818B9@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Actually, thinking about it a bit more, if you write (pseudocode):
>
> Foo old = AtomicReference<Foo>.cas (expected, new);
>
> Wouldn't the JVM have to do a type check to verify it's a Foo?
> Perhaps it won't if you don't dereference it though (I.e. ref
> check only).

Yep! Generic type erasure means there's an implicit cast inserted
there, which I think is required at runtime even if it's never
dereferenced. So the object does get loaded! Surprise, surprise.

That doesn't apply to, say, an AtomicInteger, or even to an
AtomicReference<Object>. But, yeah, good catch. :)

Cheers,
Justin


From jsampson at guidewire.com  Fri Jan  9 23:54:42 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 10 Jan 2015 04:54:42 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B07067.30103@cs.oswego.edu>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> On 01/09/2015 04:57 PM, thurstonn wrote:
> > Hello,
> >
> > I was curious as to why there isn't an AtomicReference CAS:
> >
> > T cas(T expectedValue, T newValue)
> >
> > where the returned T represents the oldValue (if it ==
> > expectedValue, then CAS succeeded).
>
> Because there is no equivalent on processors (including ARM, POWER)
> implementing compareAndSet using load-linked/store-conditional.
> (C++11/C11 make the same choice for the same reason.)
>
> > (LOCK) CMPXCHG on x86* platforms provides this behavior directly.

Also, the x86 CMPXCHG instruction sets the ZF flag on success, so it can
serve equally well as a compareAndSet or a compareAndSwap without any
extra instructions.

I'm just mentioning that fact because I personally found it interesting
when I read about it while pondering similar questions just a week or
two ago. :)

In the case of LL/SC, I was briefly puzzled when I first read about it
because it would seem at first glance that we have the old value right
there in the result of the LL. But of course the SC can still fail even
if the LL returns the value we're looking for, in which case we don't
know what the now-current value actually is.

So all processors support compareAndSet directly, but only some support
compareAndSwap. Add to that the fact that compareAndSet is easier to
program against anyway, and it's clearly the only viable choice for an
intrinsic operation in the JVM.

That said, a compareAndSwap implemented on top of the existing
compareAndSet would be easy enough, and very similar to the other
high-level methods available on the AtomicXYZ classes. It's just a
matter of justifying some use cases.

Cheers,
Justin


From davidcholmes at aapt.net.au  Sat Jan 10 04:40:11 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 10 Jan 2015 19:40:11 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEFGKMAA.davidcholmes@aapt.net.au>

Justin Sampson wrote:
> Doug Lea wrote:
>
> > On 01/09/2015 04:57 PM, thurstonn wrote:
> > > Hello,
> > >
> > > I was curious as to why there isn't an AtomicReference CAS:
> > >
> > > T cas(T expectedValue, T newValue)
> > >
> > > where the returned T represents the oldValue (if it ==
> > > expectedValue, then CAS succeeded).
> >
> > Because there is no equivalent on processors (including ARM, POWER)
> > implementing compareAndSet using load-linked/store-conditional.
> > (C++11/C11 make the same choice for the same reason.)
> >
> > > (LOCK) CMPXCHG on x86* platforms provides this behavior directly.
>
> Also, the x86 CMPXCHG instruction sets the ZF flag on success, so it can
> serve equally well as a compareAndSet or a compareAndSwap without any
> extra instructions.
>
> I'm just mentioning that fact because I personally found it interesting
> when I read about it while pondering similar questions just a week or
> two ago. :)
>
> In the case of LL/SC, I was briefly puzzled when I first read about it
> because it would seem at first glance that we have the old value right
> there in the result of the LL. But of course the SC can still fail even
> if the LL returns the value we're looking for, in which case we don't
> know what the now-current value actually is.

That would be true for a weak CAS, where we can fail spuriously, but for a
strong CAS we only fail if the compare fails, not because the SC fails.

> So all processors support compareAndSet directly, but only some support
> compareAndSwap. Add to that the fact that compareAndSet is easier to
> program against anyway, and it's clearly the only viable choice for an
> intrinsic operation in the JVM.

There's not much value in returning the value that was found in general,
because as already stated it can be stale a moment after you've read it.

> That said, a compareAndSwap implemented on top of the existing
> compareAndSet would be easy enough, and very similar to the other
> high-level methods available on the AtomicXYZ classes. It's just a
> matter of justifying some use cases.

I don't think that is true - if the value is not returned by the failing CAS
then any attempt to return whatever the value is now, may not return the
value that caused the CAS to fail - it could even return the expected value.

David
----

>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Sat Jan 10 05:14:01 2015
From: aph at redhat.com (Andrew Haley)
Date: Sat, 10 Jan 2015 10:14:01 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B07067.30103@cs.oswego.edu>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
Message-ID: <54B0FB69.7080009@redhat.com>

On 10/01/15 00:20, Doug Lea wrote:
> On 01/09/2015 04:57 PM, thurstonn wrote:
>>
>> I was curious as to why there isn't an AtomicReference CAS:
>>
>> T cas(T expectedValue, T newValue)
>>
>> where the returned T represents the oldValue (if it == expectedValue, then
>> CAS succeeded).
> 
> Because there is no equivalent on processors (including ARM, POWER)
> implementing compareAndSet using load-linked/store-conditional.
> (C++11/C11 make the same choice for the same reason.)

I don't understand what you mean.  Writing a CAS with this signature
would seem to me to be pretty straightforward on ARM.  I'm wondering
if you're referring to some specific atomicity or ordering guarantees,
but I'm not aware of any problem with that either.

Andrew.

From boehm at acm.org  Sat Jan 10 10:08:38 2015
From: boehm at acm.org (Hans Boehm)
Date: Sat, 10 Jan 2015 07:08:38 -0800
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B0FB69.7080009@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
Message-ID: <CAPUmR1a1g+OrbQG9AiPNLNswnwFS9jJ+=q3nSm4NuhwLoNpf7w@mail.gmail.com>

On Saturday, January 10, 2015, Andrew Haley <aph at redhat.com> wrote:

> On 10/01/15 00:20, Doug Lea wrote:
> > On 01/09/2015 04:57 PM, thurstonn wrote:
> >>
> >> I was curious as to why there isn't an AtomicReference CAS:
> >>
> >> T cas(T expectedValue, T newValue)
> >>
> >> where the returned T represents the oldValue (if it == expectedValue,
> then
> >> CAS succeeded).
> >
> > Because there is no equivalent on processors (including ARM, POWER)
> > implementing compareAndSet using load-linked/store-conditional.
> > (C++11/C11 make the same choice for the same reason.)
>
> I don't understand what you mean.  Writing a CAS with this signature
> would seem to me to be pretty straightforward on ARM.  I'm wondering
> if you're referring to some specific atomicity or ordering guarantees,
> but I'm not aware of any problem with that either.
>
> Andrew.
>

I second Andrew's puzzlement.

The C++ versions do give you back the old value.  expectedValue in the C++
version is a reference, which is updated on failure.  That API doesn't work
in Java, and I have mixed feelings about it anyway.  But it can be very
convenient, and it's exactly what you want if you just want to replace x by
f(x) atomically.  It becomes essentially

expected = x;
while (!x.CAS(expected, f(expected));

Unfortunately, it seems to be a bit of a bug magnet, and may have been too
clever a design.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150110/67b6453d/attachment-0001.html>

From thurston at nomagicsoftware.com  Sat Jan 10 10:18:48 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 08:18:48 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B07067.30103@cs.oswego.edu>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
Message-ID: <1420903128005-11833.post@n7.nabble.com>

Thanks, as my original post feared.

Why couldn't the JVMs for LL/SC platforms do a compare-and-swap
implementation:

load-*locked*

. . .
store-*unlocked*


instead?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11833.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From dl at cs.oswego.edu  Sat Jan 10 10:31:37 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 10 Jan 2015 10:31:37 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAPUmR1a1g+OrbQG9AiPNLNswnwFS9jJ+=q3nSm4NuhwLoNpf7w@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com>
	<CAPUmR1a1g+OrbQG9AiPNLNswnwFS9jJ+=q3nSm4NuhwLoNpf7w@mail.gmail.com>
Message-ID: <54B145D9.5040803@cs.oswego.edu>

On 01/10/2015 10:08 AM, Hans Boehm wrote:
>     I don't understand what you mean.  Writing a CAS with this signature
>     would seem to me to be pretty straightforward on ARM.  I'm wondering
>     if you're referring to some specific atomicity or ordering guarantees,
>     but I'm not aware of any problem with that either.
>
> I second Andrew's puzzlement.

As Justin pointed out, the semantics are not quite identical ...

> In the case of LL/SC, I was briefly puzzled when I first read about it
> because it would seem at first glance that we have the old value right
> there in the result of the LL. But of course the SC can still fail even
> if the LL returns the value we're looking for, in which case we don't
> know what the now-current value actually is.


... although as it turned out in the JSR133 (and C11) memory models
(even JSR133 finalized after intrinisics introduction), it would have
been have been OK spec-wise. So I suppose it is worth re-contemplating.

> The C++ versions do give you back the old value.

Oops. I forgot that this was done while I wasn't paying attention.

-Doug


From thurston at nomagicsoftware.com  Sat Jan 10 10:48:10 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 08:48:10 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
Message-ID: <1420904890038-11835.post@n7.nabble.com>

So this conversation digressed in unexpected ways . . . .

Perhaps, because I wasn't clear.  
I'm not suggesting that the extant 
boolean compareAndSet(T expected, T new) and
T getAndSet(T new)

be changed/removed anything.
If you don't want to use the new proposed:
T cas(T expected, T new)
then you can just ignore it.


But the proposed cas would be faster, simpler, and **has no equivalent** in
the currrent AtomicReference (because it would be **atomic**).

And I really don't understand the "wouldn't be useful" comments - there are
many circumstances (state machines, code/runtime visibility) in which it
comes in handy (I was looking for it and was surprised it wasn't there while
writing YACQ)

Take the simplest case:
you can use it as a sort of surrogate runtime assert;
you're at a place in your code where you expect some shared state to be X,
but you're lost and want to make sure:

T problem;
if ((problem = ar.cas(X, Y)) != X)
    log.error("BUG - expected X but was $problem")

Is any other justification needed (and there are many more potential
justifications)?

Now, if it's truly not possible/practical to implement cross-platform,
that's one thing . . .






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11835.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Sat Jan 10 11:27:40 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 10 Jan 2015 16:27:40 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEFGKMAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCOEFGKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8196E@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> Justin Sampson wrote:
>
> > In the case of LL/SC, I was briefly puzzled when I first read
> > about it because it would seem at first glance that we have the
> > old value right there in the result of the LL. But of course the
> > SC can still fail even if the LL returns the value we're looking
> > for, in which case we don't know what the now-current value
> > actually is.
>
> That would be true for a weak CAS, where we can fail spuriously,
> but for a strong CAS we only fail if the compare fails, not
> because the SC fails.

Hmm, does that mean our LL/SC implementation has to loop anyway?

boolean compareAndSet(T expected, T newval) {
  while (true) {
    T oldval = loadLinked(); // pseudo-code :)
    if (oldval != expected) return false;
    if (storeConditional(newval)) return true;
  }
}

In that case we _do_ have the actual old value, which can be
returned by either of those returns instead of the boolean!

> There's not much value in returning the value that was found in
> general, because as already stated it can be stale a moment after
> you've read it.

That's true for any volatile read, but volatile reads are still
pretty useful. :)

> > That said, a compareAndSwap implemented on top of the existing
> > compareAndSet would be easy enough, and very similar to the
> > other high-level methods available on the AtomicXYZ classes.
> > It's just a matter of justifying some use cases.
>
> I don't think that is true - if the value is not returned by the
> failing CAS then any attempt to return whatever the value is now,
> may not return the value that caused the CAS to fail - it could
> even return the expected value.

What about this?

T compareAndSwap(T expected, T newval) {
  while (true) {
    T oldval = get();
    if (oldval != expected || compareAndSet(oldval, newval)) {
      return oldval;
    }
  }
}

Cheers,
Justin


From nathan.reynolds at oracle.com  Sat Jan 10 13:03:51 2015
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Sat, 10 Jan 2015 11:03:51 -0700
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54AFC60E.3090802@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54AEDB21.8070207@oracle.com>	<5CEAE02D-78C5-4F8A-8898-C316C8E30271@gmail.com>,
	<54AF02BD.3090407@oracle.com>	<6AB258A8-879A-4DCF-98AD-047F06A396AE@gigaspaces.com>
	<54AFC60E.3090802@oracle.com>
Message-ID: <54B01711.2090704@oracle.com>

Another problem is that anyone can call LockSupport.unpark() on any 
thread.  Ignoring security manager, any thread can get a list of all the 
threads running.  The same can be done for Object.notify() and 
Object.notifyAll().  Getting a list of instances is a bit harder.

The chance that such code is running in your program is very small.  
However, bugs and timing issues could cause a first thread to call 
LockSupport.unpark() on a second thread after the second thread has 
already unparked and gone to park in some other unrelated piece of 
code.  The second thread will wake up unexpectedly and the code must 
handle that case.

-Nathan

On 1/9/2015 5:14 AM, Oleksandr Otenko wrote:
> Efficiency of implementation aside, how do you define the notify() 
> that the wait() should wake up for? All you can tell is that the 
> notify() preceding the return from wait() wakes it up. But how do you 
> define the condition that if the wait() you were thinking of has 
> already been notified(), then don't notify(), don't trigger the wake 
> up of any wait() that may occur subsequently? Once you realize this 
> problem with the API, you will see that the wait() has to be relaxed - 
> it may return without you knowing why. Once it is that relaxed, it 
> doesn't matter any more whether the notify() is the sole reason the 
> wait() may return, and we see the formulation "wait() may return 
> spuriously".
>
>
> Alex
>
>
> On 09/01/2015 08:31, Yechiel Feffer wrote:
>> Can someone please elaborate: why Apis like wait and park are 
>> "allowed to return spuriously"?
>> More over- aren't those basic java services (certainly wait) that 
>> have to provide a minimal QoS like tcp/ip and mask the caller from 
>> recheck loop stuff that should be done on the jvm level?
>> Yechiel
>>
>>
>>
>>> On 9 ???? 2015, at 00:30, "Oleksandr Otenko" 
>>> <oleksandr.otenko at oracle.com> wrote:
>>>
>>> Yes. Let's also take this opportunity to remind ourselves that 
>>> wait() can also return spuriously, so park() is no different in this 
>>> respect.
>>>
>>> Alex
>>>
>>>> On 08/01/2015 21:31, Chris Vest wrote:
>>>> In different terms, park() is allowed to return spuriously, that 
>>>> is, for no apparent reason. So you should always check that your 
>>>> expected condition for unparking was met. This check usually 
>>>> implies the ordering guarantees one needs, though the onus is on 
>>>> the programmer to ensure this.
>>>>
>>>> Chris
>>>>
>>>>> On 08/01/2015, at 20.31, Oleksandr Otenko 
>>>>> <oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>> No. You may consider them no-ops with no loss of generality.
>>>>>
>>>>> In practice, though, unpark will have a full barrier, and both 
>>>>> will behave like full compiler barriers.
>>>>>
>>>>> Alex
>>>>>
>>>>>> On 08/01/2015 18:59, thurstonn wrote:
>>>>>> Hello,
>>>>>>
>>>>>> Does an unpark(...)/(return from) park() pair constitute an 
>>>>>> happens-before
>>>>>> edge?
>>>>>>
>>>>>> Looking at the javadoc there isn't the expected "has memory 
>>>>>> visibility
>>>>>> effects of . . ." comment, but I guess I just always assumed it 
>>>>>> does.
>>>>>>
>>>>>>
>>>>>>
>>>>>> -- 
>>>>>> View this message in context: 
>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812.html
>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150110/09bcf0d5/attachment.html>

From thurston at nomagicsoftware.com  Sat Jan 10 13:19:31 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 11:19:31 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8196E@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCOEFGKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D8196E@sm-ex-01-vm.guidewire.com>
Message-ID: <1420913971093-11838.post@n7.nabble.com>

Justin Sampson wrote
> Hmm, does that mean our LL/SC implementation has to loop anyway?
> 
> boolean compareAndSet(T expected, T newval) {
>   while (true) {
>     T oldval = loadLinked(); // pseudo-code :)
>     if (oldval != expected) return false;
>     if (storeConditional(newval)) return true;
>   }
> }
> 
> In that case we _do_ have the actual old value, which can be
> returned by either of those returns instead of the boolean!

Yes, I would suspect this is how getAndSet(T) is implemented on LL/SC
platforms:
while (i++ < SOME_CONSTANT)
   T tmp:= loadLinked()
   if (storeConditional(newVal)
      return tmp;

//promote to load-lock after SOME_CONSTANT failures:
T tmp = loadLocked()
storeUnlock(newVal)
return tmp

But it's important to emphasize this is **assembler** pseudocode







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11838.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Sat Jan 10 14:12:08 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 12:12:08 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B0FB69.7080009@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
Message-ID: <1420917128899-11839.post@n7.nabble.com>

Right.
The C++11 version provides the same behavior as CMPXCHG, essentially
providing 2 return values



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11839.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Sat Jan 10 15:01:22 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 10 Jan 2015 15:01:22 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420904890038-11835.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<1420904890038-11835.post@n7.nabble.com>
Message-ID: <CAHjP37FBeqHWdL4rMaU5Ob_mj4=1xhKqUbL4Gz_GJ++q+bASaQ@mail.gmail.com>

Using your assertion example, you're basically saying "at this point in the
program I'm expecting the current value to be X and nobody else should he
modifying it concurrently".  Well, in that case your error case is a
slow/unlikely path, meaning can't you effectively write it as:

if (!ar.cas (x, y))
      log.error ("bug: " + ar.get ());

Yes, this isn't atomic.  But neither is your version because the actual
current value may be different by the time you log it.  You *got* the
current value atomically, but it doesn't mean that's what's in there by the
time you do something with it.

But sure, if you want to have that as *another* version of cas and it's
portably implementable and those semantics are adequate, I don't see an
issue.  I just wouldn't want that to be the only variant, especially if it
causes extra instructions to run or memory load to occur (e.g. checkcast
that needs to load the object header to verify type) even if only reference
comparison follows.

Sent from my phone
On Jan 10, 2015 11:11 AM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> So this conversation digressed in unexpected ways . . . .
>
> Perhaps, because I wasn't clear.
> I'm not suggesting that the extant
> boolean compareAndSet(T expected, T new) and
> T getAndSet(T new)
>
> be changed/removed anything.
> If you don't want to use the new proposed:
> T cas(T expected, T new)
> then you can just ignore it.
>
>
> But the proposed cas would be faster, simpler, and **has no equivalent** in
> the currrent AtomicReference (because it would be **atomic**).
>
> And I really don't understand the "wouldn't be useful" comments - there are
> many circumstances (state machines, code/runtime visibility) in which it
> comes in handy (I was looking for it and was surprised it wasn't there
> while
> writing YACQ)
>
> Take the simplest case:
> you can use it as a sort of surrogate runtime assert;
> you're at a place in your code where you expect some shared state to be X,
> but you're lost and want to make sure:
>
> T problem;
> if ((problem = ar.cas(X, Y)) != X)
>     log.error("BUG - expected X but was $problem")
>
> Is any other justification needed (and there are many more potential
> justifications)?
>
> Now, if it's truly not possible/practical to implement cross-platform,
> that's one thing . . .
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11835.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150110/af9daba3/attachment.html>

From vitalyd at gmail.com  Sat Jan 10 15:11:20 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 10 Jan 2015 15:11:20 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37FBeqHWdL4rMaU5Ob_mj4=1xhKqUbL4Gz_GJ++q+bASaQ@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<1420904890038-11835.post@n7.nabble.com>
	<CAHjP37FBeqHWdL4rMaU5Ob_mj4=1xhKqUbL4Gz_GJ++q+bASaQ@mail.gmail.com>
Message-ID: <CAHjP37Ey=PCRbsnuArf6CRfUQ_R29vhX1cceV3PScE-9wLJCBA@mail.gmail.com>

I just double checked what .NET does (just to see another comparison
besides c++) and they return the old value(reference).  They do, however,
have sane (I.e. reified) generics so I don't think a checkcast is a concern
there.

Sent from my phone
On Jan 10, 2015 3:01 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Using your assertion example, you're basically saying "at this point in
> the program I'm expecting the current value to be X and nobody else should
> he modifying it concurrently".  Well, in that case your error case is a
> slow/unlikely path, meaning can't you effectively write it as:
>
> if (!ar.cas (x, y))
>       log.error ("bug: " + ar.get ());
>
> Yes, this isn't atomic.  But neither is your version because the actual
> current value may be different by the time you log it.  You *got* the
> current value atomically, but it doesn't mean that's what's in there by the
> time you do something with it.
>
> But sure, if you want to have that as *another* version of cas and it's
> portably implementable and those semantics are adequate, I don't see an
> issue.  I just wouldn't want that to be the only variant, especially if it
> causes extra instructions to run or memory load to occur (e.g. checkcast
> that needs to load the object header to verify type) even if only reference
> comparison follows.
>
> Sent from my phone
> On Jan 10, 2015 11:11 AM, "thurstonn" <thurston at nomagicsoftware.com>
> wrote:
>
>> So this conversation digressed in unexpected ways . . . .
>>
>> Perhaps, because I wasn't clear.
>> I'm not suggesting that the extant
>> boolean compareAndSet(T expected, T new) and
>> T getAndSet(T new)
>>
>> be changed/removed anything.
>> If you don't want to use the new proposed:
>> T cas(T expected, T new)
>> then you can just ignore it.
>>
>>
>> But the proposed cas would be faster, simpler, and **has no equivalent**
>> in
>> the currrent AtomicReference (because it would be **atomic**).
>>
>> And I really don't understand the "wouldn't be useful" comments - there
>> are
>> many circumstances (state machines, code/runtime visibility) in which it
>> comes in handy (I was looking for it and was surprised it wasn't there
>> while
>> writing YACQ)
>>
>> Take the simplest case:
>> you can use it as a sort of surrogate runtime assert;
>> you're at a place in your code where you expect some shared state to be X,
>> but you're lost and want to make sure:
>>
>> T problem;
>> if ((problem = ar.cas(X, Y)) != X)
>>     log.error("BUG - expected X but was $problem")
>>
>> Is any other justification needed (and there are many more potential
>> justifications)?
>>
>> Now, if it's truly not possible/practical to implement cross-platform,
>> that's one thing . . .
>>
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11835.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150110/a25795fa/attachment-0001.html>

From thurston at nomagicsoftware.com  Sat Jan 10 16:24:25 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 14:24:25 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
Message-ID: <1420925065709-11842.post@n7.nabble.com>

David Holmes-6 wrote
> Interruption is a top-level Thread API so it was decided not unreasonable
> to
> have it behave as-if there were a "volatile boolean interrupted" field in
> Thread. Hence the HB relation.
> 
> Park/unpark is the lowest-level of API and in 
*
> 99.99% of cases will be used
> in conjunction with volatile state variables
*
> , which provide the HB
> relations. It would have been an excessive burden to require HB for the
> unpark/park themselves.
> 
> David

Thanks.
Just so I understand, you mean **user-level code** will pair unparks/parks
with volatile variables, right?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11842.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From davidcholmes at aapt.net.au  Sat Jan 10 16:43:28 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 11 Jan 2015 07:43:28 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420925065709-11842.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>

Not sure what you mean by "user-level" - it's all Java code. park/unpark are
low-level API's only normally used via higher-level concurrency abstractions
which also contain the volatile state variable(s) - eg
AbstractQueuedSynchronizer etc.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> thurstonn
> Sent: Sunday, 11 January 2015 7:24 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> David Holmes-6 wrote
> > Interruption is a top-level Thread API so it was decided not
> unreasonable
> > to
> > have it behave as-if there were a "volatile boolean
> interrupted" field in
> > Thread. Hence the HB relation.
> >
> > Park/unpark is the lowest-level of API and in
> *
> > 99.99% of cases will be used
> > in conjunction with volatile state variables
> *
> > , which provide the HB
> > relations. It would have been an excessive burden to require HB for the
> > unpark/park themselves.
> >
> > David
>
> Thanks.
> Just so I understand, you mean **user-level code** will pair unparks/parks
> with volatile variables, right?
>
>
>
> --
> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11842.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Sat Jan 10 18:29:05 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 10 Jan 2015 16:29:05 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
	<1420925065709-11842.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
Message-ID: <1420932545850-11844.post@n7.nabble.com>

So I think we're talking about the same thing.
I was using 'User-level' (AQS is userlevel) in contradistinction to the
native implementation of Unsafe.(un)park ('JVM-level').

Here's an (very artificial) example that appears problematical:

Original source:
volatile boolean signal = false //global
T1:
for (int loops = 0; loops < 1 and ! signal; loops++)
{
        park();

}

T2:
signal = true;
unpark(T1);

now, without any guarantees of unpark/depark hb (or knowledge of its
implementation), the following hypothetical history would be valid under the
JMM:

T1: r1 = 0; //loops = 0
T1: r2 = signal (false)
T1: r1 < 1 && ! r2
T1: park() //blocks

T2: unpark(T1) // can be reordered before volatile write
T1: depark // return from park()
T1: r1 = r1 + 1 //++ 
T1: exit loop // r1 >= 1
T2: signal = true

The original source is in line with (un)park recommended use (paired with a
volatile read/write), but it allows an unexpected/unreasonable history.

If unpark/depark were hb, such a history would be impossible and #signal
need not even be volatile (piggybacking on the hb) - although I'm not sure
that would have any efficiency benefit








--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11844.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From davidcholmes at aapt.net.au  Sun Jan 11 02:18:21 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 11 Jan 2015 17:18:21 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420932545850-11844.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFKKMAA.davidcholmes@aapt.net.au>

Happens-before establishes inter-thread orderings, what you are concerned
about here is the intra-thread ordering: can a volatile write followed by
unpark() be reordered? Intra-thread semantics are governed by program order:
everything is supposed to execute in program order as per the semantics of
the instructions being executed - unless the compiler can determine that a
reordering can be done without affecting any semantics. When it comes to
function calls compilers generally don't know if they can reorder so they
don't. But in general if you had a very smart compiler there would have to
be something about the call to unpark that told it that it could not be
reordered with the volatile write (or put another way there will always be a
limit on what the compiler can determine about the semantics of the call).

David


> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> thurstonn
> Sent: Sunday, 11 January 2015 9:29 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> So I think we're talking about the same thing.
> I was using 'User-level' (AQS is userlevel) in contradistinction to the
> native implementation of Unsafe.(un)park ('JVM-level').
>
> Here's an (very artificial) example that appears problematical:
>
> Original source:
> volatile boolean signal = false //global
> T1:
> for (int loops = 0; loops < 1 and ! signal; loops++)
> {
>         park();
>
> }
>
> T2:
> signal = true;
> unpark(T1);
>
> now, without any guarantees of unpark/depark hb (or knowledge of its
> implementation), the following hypothetical history would be
> valid under the
> JMM:
>
> T1: r1 = 0; //loops = 0
> T1: r2 = signal (false)
> T1: r1 < 1 && ! r2
> T1: park() //blocks
>
> T2: unpark(T1) // can be reordered before volatile write
> T1: depark // return from park()
> T1: r1 = r1 + 1 //++
> T1: exit loop // r1 >= 1
> T2: signal = true
>
> The original source is in line with (un)park recommended use
> (paired with a
> volatile read/write), but it allows an unexpected/unreasonable history.
>
> If unpark/depark were hb, such a history would be impossible and #signal
> need not even be volatile (piggybacking on the hb) - although I'm not sure
> that would have any efficiency benefit
>
>
>
>
>
>
>
>
> --
> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11844.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Sun Jan 11 06:11:28 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 11 Jan 2015 11:11:28 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420932545850-11844.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>	<1420826753248-11819.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
	<1420925065709-11842.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
	<1420932545850-11844.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D819D2@sm-ex-01-vm.guidewire.com>

Thurston wrote:

> Here's an (very artificial) example that appears problematical:
>
> Original source:
> volatile boolean signal = false //global
> T1:
> for (int loops = 0; loops < 1 and ! signal; loops++)
> {
>         park();
>
> }
>
> T2:
> signal = true;
> unpark(T1);

I don't understand the significance of the for-loop here. Isn't that
equivalent to this?

T1: if (!signal) park();
T2: signal = true; unpark(T1);

Remember that park() can return spuriously. If the compiler reorders
the unpark(T1) before the signal = true, that will just appear to be
a spurious wakeup, which is in fact legal. So this example isn't
really much help in resolving the broader question.

Due to spurious wakeups, we really have to make the example more
realistic by making the loop continue indefinitely. That gives us:

T1: while (!signal) park();
T2: signal = true; unpark(T1);

(In real code we always have to check the interrupt status after
returning from park(), but let's agree to ignore that for this
analysis.)

Now it's more interesting... If nothing is reordered, T1 is
guaranteed to terminate. But if T2 is reordered, T1 could park
once, get unparked by T2, go right back around the loop to park
again, and then never be unparked.

So it really does seem like there has to be some kind of ordering
guarantee going on here for the typical park/unpark idiom to work
reliably. The javadoc for unpark() says, in part, "If the thread was
blocked on park then it will unblock. Otherwise, its next call to
park is guaranteed not to block." The very notion that there is an
identifiable "next call" seems to imply that there's some kind of
inherent ordering. Let's call it "parking order". :) So the question
is, what constraints does parking order impose on reordering of
reads and writes?

Let's say, for the sake of argument, that parking order is as strong
as happens-before order, as if unpark() were writing a volatile
field that park() is reading. Well, it turns out practically that we
can't piggyback on that happens-before edge anyway! Sticking with
the example code above, if the first time through the loop, T1 reads
the true value that T2 has written to the signal variable, T1 never
calls park() at all. If there's no park(), there's no happens-before
edge from unpark() to park(). So T1 goes on its merry way based on
having seen signal == true, but if we've declared signal to be
non-volatile, then none of T2's prior writes are guaranteed to be
visible to T1.

Therefore parking order does need to give us some guarantees about
reordering, but can actually be weaker than happens-before order
since we do always use it in conjunction with volatile state
variables. If I'm on the right track, is there a spec-worthy way to
describe those guarantees in such terms?

Cheers,
Justin


From davidcholmes at aapt.net.au  Sun Jan 11 17:17:05 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Jan 2015 08:17:05 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D819D2@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEFPKMAA.davidcholmes@aapt.net.au>

Justin writes below:
> The javadoc for unpark() says, in part, "If the thread was
> blocked on park then it will unblock. Otherwise, its next call to
> park is guaranteed not to block." The very notion that there is an
> identifiable "next call" seems to imply that there's some kind of
> inherent ordering.

It is simply referring to the fact that unpark is "sticky" - it sets the
token and unparks the thread. If the thread was not parked the token remains
sets and so next time park() is called it claims the token and returns
immediately.

David
-----

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Justin
> Sampson
> Sent: Sunday, 11 January 2015 9:11 PM
> To: thurstonn; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] unpark/park memory visibility
>
>
> Thurston wrote:
>
> > Here's an (very artificial) example that appears problematical:
> >
> > Original source:
> > volatile boolean signal = false //global
> > T1:
> > for (int loops = 0; loops < 1 and ! signal; loops++)
> > {
> >         park();
> >
> > }
> >
> > T2:
> > signal = true;
> > unpark(T1);
>
> I don't understand the significance of the for-loop here. Isn't that
> equivalent to this?
>
> T1: if (!signal) park();
> T2: signal = true; unpark(T1);
>
> Remember that park() can return spuriously. If the compiler reorders
> the unpark(T1) before the signal = true, that will just appear to be
> a spurious wakeup, which is in fact legal. So this example isn't
> really much help in resolving the broader question.
>
> Due to spurious wakeups, we really have to make the example more
> realistic by making the loop continue indefinitely. That gives us:
>
> T1: while (!signal) park();
> T2: signal = true; unpark(T1);
>
> (In real code we always have to check the interrupt status after
> returning from park(), but let's agree to ignore that for this
> analysis.)
>
> Now it's more interesting... If nothing is reordered, T1 is
> guaranteed to terminate. But if T2 is reordered, T1 could park
> once, get unparked by T2, go right back around the loop to park
> again, and then never be unparked.
>
> So it really does seem like there has to be some kind of ordering
> guarantee going on here for the typical park/unpark idiom to work
> reliably. The javadoc for unpark() says, in part, "If the thread was
> blocked on park then it will unblock. Otherwise, its next call to
> park is guaranteed not to block." The very notion that there is an
> identifiable "next call" seems to imply that there's some kind of
> inherent ordering. Let's call it "parking order". :) So the question
> is, what constraints does parking order impose on reordering of
> reads and writes?
>
> Let's say, for the sake of argument, that parking order is as strong
> as happens-before order, as if unpark() were writing a volatile
> field that park() is reading. Well, it turns out practically that we
> can't piggyback on that happens-before edge anyway! Sticking with
> the example code above, if the first time through the loop, T1 reads
> the true value that T2 has written to the signal variable, T1 never
> calls park() at all. If there's no park(), there's no happens-before
> edge from unpark() to park(). So T1 goes on its merry way based on
> having seen signal == true, but if we've declared signal to be
> non-volatile, then none of T2's prior writes are guaranteed to be
> visible to T1.
>
> Therefore parking order does need to give us some guarantees about
> reordering, but can actually be weaker than happens-before order
> since we do always use it in conjunction with volatile state
> variables. If I'm on the right track, is there a spec-worthy way to
> describe those guarantees in such terms?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Sun Jan 11 17:57:33 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Jan 2015 08:57:33 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8196E@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEGBKMAA.davidcholmes@aapt.net.au>

Justin Sampson writes:
> David Holmes wrote:
>
> > Justin Sampson wrote:
> >
> > > In the case of LL/SC, I was briefly puzzled when I first read
> > > about it because it would seem at first glance that we have the
> > > old value right there in the result of the LL. But of course the
> > > SC can still fail even if the LL returns the value we're looking
> > > for, in which case we don't know what the now-current value
> > > actually is.
> >
> > That would be true for a weak CAS, where we can fail spuriously,
> > but for a strong CAS we only fail if the compare fails, not
> > because the SC fails.
>
> Hmm, does that mean our LL/SC implementation has to loop anyway?
>
> boolean compareAndSet(T expected, T newval) {
>   while (true) {
>     T oldval = loadLinked(); // pseudo-code :)
>     if (oldval != expected) return false;
>     if (storeConditional(newval)) return true;
>   }
> }
>
> In that case we _do_ have the actual old value, which can be
> returned by either of those returns instead of the boolean!

Yes it has to loop; and yes it has access to the value that caused failure.

> > There's not much value in returning the value that was found in
> > general, because as already stated it can be stale a moment after
> > you've read it.
>
> That's true for any volatile read, but volatile reads are still
> pretty useful. :)

Different usage contexts.

> > > That said, a compareAndSwap implemented on top of the existing
> > > compareAndSet would be easy enough, and very similar to the
> > > other high-level methods available on the AtomicXYZ classes.
> > > It's just a matter of justifying some use cases.
> >
> > I don't think that is true - if the value is not returned by the
> > failing CAS then any attempt to return whatever the value is now,
> > may not return the value that caused the CAS to fail - it could
> > even return the expected value.
>
> What about this?
>
> T compareAndSwap(T expected, T newval) {
>   while (true) {
>     T oldval = get();
>     if (oldval != expected || compareAndSet(oldval, newval)) {
>       return oldval;
>     }
>   }
> }

Ok but all this says is that it either returns the expected value on success
or some past value that was once stored, on failure. Doesn't seem to give me
much over a boolean. Typically compareAndSwap code just looks like:

if (cas(expected, newval) == expected) ...

which is more typing than needed if cas just returns boolean. :)

Though it wasn't the original reason for using boolean, weakCompareAndSet
requires it. It would have looked odd to have:

T compareAndSet(T expected, T newVal)
boolean weakCompareAndSet(T expected, T newVal)

I don't see a strong enough justification for adding a T returning CAS
method but won't fight it either.

Cheers,
David

> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Sun Jan 11 18:38:11 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sun, 11 Jan 2015 16:38:11 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFKKMAA.davidcholmes@aapt.net.au>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
	<1420925065709-11842.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
	<1420932545850-11844.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFKKMAA.davidcholmes@aapt.net.au>
Message-ID: <1421019491714-11849.post@n7.nabble.com>

Multi-thread histories are all about inter-thread semantics.
In the example history (which is valid according to the JMM), T1 can "see"
the unpark() before it sees the update to signal.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11849.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From davidcholmes at aapt.net.au  Sun Jan 11 18:55:04 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Jan 2015 09:55:04 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421019491714-11849.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>

thurstonn writes:
>
> Multi-thread histories are all about inter-thread semantics.
> In the example history (which is valid according to the JMM), T1 can "see"
> the unpark() before it sees the update to signal.

It is only valid according to the JMM if the intra-thread reordering of the
volatile write and unpark is allowed. You can't just treat the unpark as a
simple memory access.

David

>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-v
> isibility-tp11812p11849.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Sun Jan 11 19:40:16 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sun, 11 Jan 2015 17:40:16 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCKEFEKMAA.davidcholmes@aapt.net.au>
	<1420925065709-11842.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
	<1420932545850-11844.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCCEFKKMAA.davidcholmes@aapt.net.au>
	<1421019491714-11849.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>
Message-ID: <1421023216623-11851.post@n7.nabble.com>

David Holmes-6 wrote
> thurstonn writes:
>>
>> Multi-thread histories are all about inter-thread semantics.
>> In the example history (which is valid according to the JMM), T1 can
>> "see"
>> the unpark() before it sees the update to signal.
> 
> It is only valid according to the JMM if the intra-thread reordering of
> the
> volatile write and unpark is allowed. You can't just treat the unpark as a
> simple memory access.
> 
> David
> 
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-v
>> isibility-tp11812p11849.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> 

> Concurrency-interest at .oswego

>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Exactly my point; without a requirement that there is a hb(unpark, depark),
a JVM implementation **theoretically** could implement (un)park in such a
way, that it entails nothing but a simple memory write/read (or even no
shared memory mutation at all). didn't alex describe (un) park as
"semantically a no-op"?  No-ops can be reordered with anything.

I'm not saying that is practical (in fact I can't imagine how that could be
done, but I don't know how real JVM's implement it now).  But the first rule
of Java concurrency, is don't rely on that kind of thinking - unless there
are explicit synchronization actions - intuition, etc. are *not reliable*

And so I'm just stating that the hb should be made explicit (I had assumed
it implicitly all along);
If you think it through, it's very hard to imagine how developers could
write bullet-proof correct concurrent code that relies on park/unpark
without there being a hb(unpark, depark), which of course would invalidate
the kind of histories above.

There's already an explicit "interrupt hb rule" (and a thread death rule,
etc) - park fits the same pattern.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11851.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From ben_manes at yahoo.com  Sun Jan 11 19:51:32 2015
From: ben_manes at yahoo.com (Ben Manes)
Date: Mon, 12 Jan 2015 00:51:32 +0000 (UTC)
Subject: [concurrency-interest] CHM: removing during a computeIfAbsent
Message-ID: <1113916303.390025.1421023892657.JavaMail.yahoo@jws100122.mail.ne1.yahoo.com>

When an absent value is being computed, this operation blocks a removal by the same key and clearing the map. This is interesting because a get(), contains(), and iterators do not block, so it could be argued that the values are not considered present yet. If that is agreed to then by the wording of the JavaDoc on remove() could return false immediately and clear() could skip those entries. Was this considered and, if so, why was that optimization rejected?
In the case of a cache, an invalidation doesn't require returning the previous value(s) and the caller may not expect a long delay waiting for every entry to materialize for removal. This could be resolved by a pre-screening phase to only discard entries that are present to minimize this hit, but I'd like to know arguments for why ConcurrentHashMap blocks first in case there is a scenario that I'm missing.
Thanks-Ben
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/50803efc/attachment.html>

From davidcholmes at aapt.net.au  Sun Jan 11 20:28:29 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Mon, 12 Jan 2015 11:28:29 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421023216623-11851.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>

thurston writes:
> David Holmes-6 wrote
> > thurstonn writes:
> >>
> >> Multi-thread histories are all about inter-thread semantics.
> >> In the example history (which is valid according to the JMM), T1 can
> >> "see" the unpark() before it sees the update to signal.
> >
> > It is only valid according to the JMM if the intra-thread reordering of
> > the volatile write and unpark is allowed. You can't just treat the
> > unpark as a simple memory access.
> >
>
> Exactly my point; without a requirement that there is a
> hb(unpark, depark),
> a JVM implementation **theoretically** could implement (un)park in such a
> way, that it entails nothing but a simple memory write/read (or even no
> shared memory mutation at all). didn't alex describe (un) park as
> "semantically a no-op"?  No-ops can be reordered with anything.

A JVM has to implement a park/unpark that functions in the context of that
JVM. You could implement park and unpark as no-ops and as such they could be
"reordered" with anything and it would all work exactly the same as-if they
had not been re-ordered.

> I'm not saying that is practical (in fact I can't imagine how
> that could be done, but I don't know how real JVM's implement it now).
But the
> first rule of Java concurrency, is don't rely on that kind of thinking -
unless there
> are explicit synchronization actions - intuition, etc. are *not reliable*
>
> And so I'm just stating that the hb should be made explicit (I had assumed
> it implicitly all along);
> If you think it through, it's very hard to imagine how developers could
> write bullet-proof correct concurrent code that relies on park/unpark
> without there being a hb(unpark, depark), which of course would invalidate
> the kind of histories above.

I understand what you are saying but it is up to an implementation to ensure
everything happens in the correct order for this API to work, we don't need
the JMM to say that park/unpark establish a specific ordering as it is not a
property of the programming model that needs to be exported to the
programmer. HB is something the implementor has to provide to the
programmer, not something that the implementor gets to use.

> There's already an explicit "interrupt hb rule" (and a thread death rule,
> etc) - park fits the same pattern.

It certainly could be defined that way, but it doesn't need to be defined
that way.

David
-----

>
>
>
>
> --
> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11851.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Mon Jan 12 06:21:01 2015
From: aph at redhat.com (Andrew Haley)
Date: Mon, 12 Jan 2015 11:21:01 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B145D9.5040803@cs.oswego.edu>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<CAPUmR1a1g+OrbQG9AiPNLNswnwFS9jJ+=q3nSm4NuhwLoNpf7w@mail.gmail.com>
	<54B145D9.5040803@cs.oswego.edu>
Message-ID: <54B3AE1D.5070700@redhat.com>

On 10/01/15 15:31, Doug Lea wrote:
> On 01/10/2015 10:08 AM, Hans Boehm wrote:
>>     I don't understand what you mean.  Writing a CAS with this signature
>>     would seem to me to be pretty straightforward on ARM.  I'm wondering
>>     if you're referring to some specific atomicity or ordering guarantees,
>>     but I'm not aware of any problem with that either.
>>
>> I second Andrew's puzzlement.
> 
> As Justin pointed out, the semantics are not quite identical ...

That's really not true: there is no problem implementing a CAS
like this.  Sure, you have to spin until the store conditional
succeeds rather than returning early if it fails.

You wrote

> Because there is no equivalent on processors (including ARM, POWER)
> implementing compareAndSet using load-linked/store-conditional.

Andrew.


From oleksandr.otenko at oracle.com  Mon Jan 12 08:10:59 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 13:10:59 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEFEKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGEFEKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B3C7E3.4050208@oracle.com>

Yes, of course. I meant a different scenario - suppose, we even know the 
monitor has only one thread waiting, and we even know what that thread 
is. Still, this is not enough.

Alex

On 09/01/2015 23:27, David Holmes wrote:
> If you wait for different conditions then you can't use notify() in the
> first place, but must use notifyAll(). It is entirely possible that wait()
> be implemented such that a thread only gets woken when a notification
> occurs, but that doesn't tell the thread anything about what it was waiting
> for - hence the need for the loop regardless of spurious wakeups.
>
> David
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> Oleksandr Otenko
>> Sent: Friday, 9 January 2015 10:14 PM
>> To: Yechiel Feffer
>> Cc: thurstonn; concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] unpark/park memory visibility
>>
>>
>> Efficiency of implementation aside, how do you define the notify() that
>> the wait() should wake up for? All you can tell is that the notify()
>> preceding the return from wait() wakes it up. But how do you define the
>> condition that if the wait() you were thinking of has already been
>> notified(), then don't notify(), don't trigger the wake up of any wait()
>> that may occur subsequently? Once you realize this problem with the API,
>> you will see that the wait() has to be relaxed - it may return without
>> you knowing why. Once it is that relaxed, it doesn't matter any more
>> whether the notify() is the sole reason the wait() may return, and we
>> see the formulation "wait() may return spuriously".
>>
>>
>> Alex
>>
>>
>> On 09/01/2015 08:31, Yechiel Feffer wrote:
>>> Can someone please elaborate: why Apis like wait and park are
>> "allowed to return spuriously"?
>>> More over- aren't those basic java services (certainly wait)
>> that have to provide a minimal QoS like tcp/ip and mask the
>> caller from recheck loop stuff that should be done on the jvm level?
>>> Yechiel
>>>
>>>
>>>
>>>> On 9 ???? 2015, at 00:30, "Oleksandr Otenko"
>> <oleksandr.otenko at oracle.com> wrote:
>>>> Yes. Let's also take this opportunity to remind ourselves that
>> wait() can also return spuriously, so park() is no different in
>> this respect.
>>>> Alex
>>>>
>>>>> On 08/01/2015 21:31, Chris Vest wrote:
>>>>> In different terms, park() is allowed to return spuriously,
>> that is, for no apparent reason. So you should always check that
>> your expected condition for unparking was met. This check usually
>> implies the ordering guarantees one needs, though the onus is on
>> the programmer to ensure this.
>>>>> Chris
>>>>>
>>>>>> On 08/01/2015, at 20.31, Oleksandr Otenko
>> <oleksandr.otenko at oracle.com> wrote:
>>>>>> No. You may consider them no-ops with no loss of generality.
>>>>>>
>>>>>> In practice, though, unpark will have a full barrier, and
>> both will behave like full compiler barriers.
>>>>>> Alex
>>>>>>
>>>>>>> On 08/01/2015 18:59, thurstonn wrote:
>>>>>>> Hello,
>>>>>>>
>>>>>>> Does an unpark(...)/(return from) park() pair constitute an
>> happens-before
>>>>>>> edge?
>>>>>>>
>>>>>>> Looking at the javadoc there isn't the expected "has memory
>> visibility
>>>>>>> effects of . . ." comment, but I guess I just always
>> assumed it does.
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
> tp11812.html
>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From oleksandr.otenko at oracle.com  Mon Jan 12 08:28:58 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 13:28:58 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420826753248-11819.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
Message-ID: <54B3CC1A.5080807@oracle.com>

Mind you, the interrupts are a wider concept - they are just as 
applicable for wait/notify.

The interruption, though, synchronizes-with the checking of interruption 
status "in Java land". park, on the other hand, does not check anything 
"in Java land", so does not necessarily enforce a hb.

There is a clear difference. The park/unpark pair will have the code 
checking something meaningful anyway, creating enough hbs where 
necessary, so you don't want an additional hb. The interruption, on the 
other hand, is the meaningful thing that doesn't depend on whatever the 
thread is doing, so manipulating the state must have the hb edge.


Alex


On 09/01/2015 18:05, thurstonn wrote:
> smacks a bit like no means yes.
>
> It does bring up an interesting question:  why is the "intrinsic"
> happens-before rule list as it is?
> Other than monitors and volatiles, of course.
> Is the logic something like: any known/possible/reasonable implementation is
> necessarily going to provide the memory model guarantees, or is it the
> semantics that drive it?
>
> Take thread interruption - largely analogous to (un)park (except for the
> spurious de-parks), and I would guess largely similar implementations (?).
> So why would interrupts provide hb, and not park?
> I wonder if AQS, e.g., would have been implemented slightly differently if
> the hb guarantee was there
>
>


From oleksandr.otenko at oracle.com  Mon Jan 12 10:01:13 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 15:01:13 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B3E1B9.1040603@oracle.com>

I agree with thurstonn and Justin. It is underspecified.

park() waking up spuriously is not enough. park() waking up in response 
to a unpark() is not enough. Specifying that they should be executed in 
program order is not enough. What is important, is that if park() woke 
up /*in response*/ to unpark() (actually observed the unpark), then the 
volatile reads following the park() in program order *must 
synchronize-with* the volatile writes preceding the unpark().

The difficulty with the specification is the presence of spurious 
wake-ups. This means the implementation is allowed to completely consist 
of spurious wake-ups. But when it doesn't always wake up spurously, 
sometimes it won't wake up on its own - so not allowing the loop to go 
back to park() is important, since there may be no more unparks. This 
means that the wake-up event, if it is implemented, must become a 
synchronization event placed exactly between the volatile write and 
volatile read in the total order.


Alex


On 12/01/2015 01:28, David Holmes wrote:
> thurston writes:
>> David Holmes-6 wrote
>>> thurstonn writes:
>>>> Multi-thread histories are all about inter-thread semantics.
>>>> In the example history (which is valid according to the JMM), T1 can
>>>> "see" the unpark() before it sees the update to signal.
>>> It is only valid according to the JMM if the intra-thread reordering of
>>> the volatile write and unpark is allowed. You can't just treat the
>>> unpark as a simple memory access.
>>>
>> Exactly my point; without a requirement that there is a
>> hb(unpark, depark),
>> a JVM implementation **theoretically** could implement (un)park in such a
>> way, that it entails nothing but a simple memory write/read (or even no
>> shared memory mutation at all). didn't alex describe (un) park as
>> "semantically a no-op"?  No-ops can be reordered with anything.
> A JVM has to implement a park/unpark that functions in the context of that
> JVM. You could implement park and unpark as no-ops and as such they could be
> "reordered" with anything and it would all work exactly the same as-if they
> had not been re-ordered.
>
>> I'm not saying that is practical (in fact I can't imagine how
>> that could be done, but I don't know how real JVM's implement it now).
> But the
>> first rule of Java concurrency, is don't rely on that kind of thinking -
> unless there
>> are explicit synchronization actions - intuition, etc. are *not reliable*
>>
>> And so I'm just stating that the hb should be made explicit (I had assumed
>> it implicitly all along);
>> If you think it through, it's very hard to imagine how developers could
>> write bullet-proof correct concurrent code that relies on park/unpark
>> without there being a hb(unpark, depark), which of course would invalidate
>> the kind of histories above.
> I understand what you are saying but it is up to an implementation to ensure
> everything happens in the correct order for this API to work, we don't need
> the JMM to say that park/unpark establish a specific ordering as it is not a
> property of the programming model that needs to be exported to the
> programmer. HB is something the implementor has to provide to the
> programmer, not something that the implementor gets to use.
>
>> There's already an explicit "interrupt hb rule" (and a thread death rule,
>> etc) - park fits the same pattern.
> It certainly could be defined that way, but it doesn't need to be defined
> that way.
>
> David
> -----
>
>>
>>
>>
>> --
>> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
> tp11812p11851.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/f753e51b/attachment.html>

From thurston at nomagicsoftware.com  Mon Jan 12 10:22:25 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 12 Jan 2015 08:22:25 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B3CC1A.5080807@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54AEDB21.8070207@oracle.com>
	<1420826753248-11819.post@n7.nabble.com>
	<54B3CC1A.5080807@oracle.com>
Message-ID: <1421076145114-11858.post@n7.nabble.com>

Hello Alex,


Let me try this:
This should be a yes/no question


Here's a program consisting of 2 threads:
volatile boolean signal = false //global

Thread 1:
park()
assert signal : "Unfortunately possible given current spec, signal can be
false"

Thread 2:
signal = true
unpark(Thread1)


Now, I need to add in a special caveat, that let's assume (just for the
purposes of this discussion), that no spurious deparks are possible, or if
you prefer, in practice, occur

Can this program result in an AssertionException?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11858.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Mon Jan 12 10:59:55 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 15:59:55 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B3E1B9.1040603@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
Message-ID: <54B3EF7B.9000501@oracle.com>

Another reason:

since there are patterns of using park/unpark that are allowed to 
deadlock, those patterns must be specified.

Eg if there is exactly one unpark after a boolean variable set, the loop 
checking the volatile variable with park() in the body is not allowed to 
end with a indefinitely blocked park(); the loop is finite. This latter 
requirement is a very strong requirement on how park must be implemented.

For example, it can be roughly modelled like so:

unpark(): thread.lock.release();
park(): while( rnd.nextBoolean() && !thread.lock.tryAcquire() && 
!thread.isInterrupted());

Then you know that park *may* synchronize-with unpark, but as a user you 
don't know - so you have to devise other synchronization means.

And also you know that park *may* block until unpark, so you can't just 
plug park() anywhere (must use sparingly; it isn't really a no-op).

And also you know that if it does get blocked, it will be *at least* 
unblocked by lock.release or interruption.


Something like this is already in the documentation, but without 
mentioning synchronizes-with there is no relation to the total order of 
synchronization events - that is, that the volatile read *must* 
synchronize-with the store, it *must* appear in the total order after 
the store.

Alex

On 12/01/2015 15:01, Oleksandr Otenko wrote:
> I agree with thurstonn and Justin. It is underspecified.
>
> park() waking up spuriously is not enough. park() waking up in 
> response to a unpark() is not enough. Specifying that they should be 
> executed in program order is not enough. What is important, is that if 
> park() woke up /*in response*/ to unpark() (actually observed the 
> unpark), then the volatile reads following the park() in program order 
> *must synchronize-with* the volatile writes preceding the unpark().
>
> The difficulty with the specification is the presence of spurious 
> wake-ups. This means the implementation is allowed to completely 
> consist of spurious wake-ups. But when it doesn't always wake up 
> spurously, sometimes it won't wake up on its own - so not allowing the 
> loop to go back to park() is important, since there may be no more 
> unparks. This means that the wake-up event, if it is implemented, must 
> become a synchronization event placed exactly between the volatile 
> write and volatile read in the total order.
>
>
> Alex
>
>
> On 12/01/2015 01:28, David Holmes wrote:
>> thurston writes:
>>> David Holmes-6 wrote
>>>> thurstonn writes:
>>>>> Multi-thread histories are all about inter-thread semantics.
>>>>> In the example history (which is valid according to the JMM), T1 can
>>>>> "see" the unpark() before it sees the update to signal.
>>>> It is only valid according to the JMM if the intra-thread reordering of
>>>> the volatile write and unpark is allowed. You can't just treat the
>>>> unpark as a simple memory access.
>>>>
>>> Exactly my point; without a requirement that there is a
>>> hb(unpark, depark),
>>> a JVM implementation **theoretically** could implement (un)park in such a
>>> way, that it entails nothing but a simple memory write/read (or even no
>>> shared memory mutation at all). didn't alex describe (un) park as
>>> "semantically a no-op"?  No-ops can be reordered with anything.
>> A JVM has to implement a park/unpark that functions in the context of that
>> JVM. You could implement park and unpark as no-ops and as such they could be
>> "reordered" with anything and it would all work exactly the same as-if they
>> had not been re-ordered.
>>
>>> I'm not saying that is practical (in fact I can't imagine how
>>> that could be done, but I don't know how real JVM's implement it now).
>> But the
>>> first rule of Java concurrency, is don't rely on that kind of thinking -
>> unless there
>>> are explicit synchronization actions - intuition, etc. are *not reliable*
>>>
>>> And so I'm just stating that the hb should be made explicit (I had assumed
>>> it implicitly all along);
>>> If you think it through, it's very hard to imagine how developers could
>>> write bullet-proof correct concurrent code that relies on park/unpark
>>> without there being a hb(unpark, depark), which of course would invalidate
>>> the kind of histories above.
>> I understand what you are saying but it is up to an implementation to ensure
>> everything happens in the correct order for this API to work, we don't need
>> the JMM to say that park/unpark establish a specific ordering as it is not a
>> property of the programming model that needs to be exported to the
>> programmer. HB is something the implementor has to provide to the
>> programmer, not something that the implementor gets to use.
>>
>>> There's already an explicit "interrupt hb rule" (and a thread death rule,
>>> etc) - park fits the same pattern.
>> It certainly could be defined that way, but it doesn't need to be defined
>> that way.
>>
>> David
>> -----
>>
>>>
>>>
>>> --
>>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
>> tp11812p11851.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/62905c15/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Jan 12 11:18:58 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 16:18:58 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421076145114-11858.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54AEDB21.8070207@oracle.com>	<1420826753248-11819.post@n7.nabble.com>	<54B3CC1A.5080807@oracle.com>
	<1421076145114-11858.post@n7.nabble.com>
Message-ID: <54B3F3F2.3050107@oracle.com>

No, it cannot.

I actually agree with you this is underspecified, because there is no 
written guarantee that reading signal *must* synchronize-with writing 
signal.

The same thing can be expressed differently:

Thread 1:
while(!signal) park();

Thread 2:
signal=true;
unpark();

will eventually terminate. Ie that from park() observing the unpark() it 
must follow that the following read of signal appears after the write to 
signal in the total order.

Alex

On 12/01/2015 15:22, thurstonn wrote:
> Hello Alex,
>
>
> Let me try this:
> This should be a yes/no question
>
>
> Here's a program consisting of 2 threads:
> volatile boolean signal = false //global
>
> Thread 1:
> park()
> assert signal : "Unfortunately possible given current spec, signal can be
> false"
>
> Thread 2:
> signal = true
> unpark(Thread1)
>
>
> Now, I need to add in a special caveat, that let's assume (just for the
> purposes of this discussion), that no spurious deparks are possible, or if
> you prefer, in practice, occur
>
> Can this program result in an AssertionException?
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11858.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/2a7de6cf/attachment.html>

From jsampson at guidewire.com  Mon Jan 12 12:31:02 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 12 Jan 2015 17:31:02 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B3E1B9.1040603@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> I agree with thurstonn and Justin. It is underspecified.

Thanks. :)

> park() waking up spuriously is not enough. park() waking up in
> response to a unpark() is not enough. Specifying that they should
> be executed in program order is not enough. What is important, is
> that if park() woke up in response to unpark() (actually observed
> the unpark), then the volatile reads following the park() in
> program order must synchronize-with the volatile writes preceding
> the unpark().

Right -- it doesn't need to be as strong as synchronizes-with
between the unpark and park themselves, but it still needs to be
tied into the synchronization order. How about adding something
like this to the LockSupport class documentation:

An unpark doesn't actually synchronize-with the park that it
unblocks, but each park and unpark does take a definite position in
the overall synchronization order. This is the order used in the
rules for permit handling, as described in the park and unpark
method documentation. As with other synchronization actions, the
position of any park or unpark in the synchronization order is
consistent with its position in the program order for the thread
that performs it. Therefore any synchronization actions preceding an
unpark in one thread are guaranteed to precede, in the overall
synchronization order, any synchronization actions following a park
that it unblocks in another thread. In particular, any write to a
volatile variable before an unpark in one thread synchronizes-with
any read of that volatile variable after a park that it unblocks in
another thread.

Cheers,
Justin


From oleksandr.otenko at oracle.com  Mon Jan 12 14:32:33 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 19:32:33 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
Message-ID: <54B42151.2060509@oracle.com>

> Therefore any synchronization actions preceding an
> unpark in one thread are guaranteed to precede, in the overall
> synchronization order, any synchronization actions following a park
> that it unblocks in another thread.

This is almost the definition of synchronizes-with. Synchronizes-with 
will join any two program orders into a happens-before, but you only 
require the program order of synchronization events to behave like that.

Perhaps, it is possible to specify park/unpark as synchronization events 
such that they appear in the total order of the events - and JVMs would 
be obliged to enforce program order consistent with that total order. 
Yet, we don't need to specify whether park/unpark create the hb edge. 
How and whether they communicate, and how that affects thread's 
progress, is a matter separate from visibility.

Alex


On 12/01/2015 17:31, Justin Sampson wrote:
> Oleksandr Otenko wrote:
>
>> I agree with thurstonn and Justin. It is underspecified.
> Thanks. :)
>
>> park() waking up spuriously is not enough. park() waking up in
>> response to a unpark() is not enough. Specifying that they should
>> be executed in program order is not enough. What is important, is
>> that if park() woke up in response to unpark() (actually observed
>> the unpark), then the volatile reads following the park() in
>> program order must synchronize-with the volatile writes preceding
>> the unpark().
> Right -- it doesn't need to be as strong as synchronizes-with
> between the unpark and park themselves, but it still needs to be
> tied into the synchronization order. How about adding something
> like this to the LockSupport class documentation:
>
> An unpark doesn't actually synchronize-with the park that it
> unblocks, but each park and unpark does take a definite position in
> the overall synchronization order. This is the order used in the
> rules for permit handling, as described in the park and unpark
> method documentation. As with other synchronization actions, the
> position of any park or unpark in the synchronization order is
> consistent with its position in the program order for the thread
> that performs it. Therefore any synchronization actions preceding an
> unpark in one thread are guaranteed to precede, in the overall
> synchronization order, any synchronization actions following a park
> that it unblocks in another thread. In particular, any write to a
> volatile variable before an unpark in one thread synchronizes-with
> any read of that volatile variable after a park that it unblocks in
> another thread.
>
> Cheers,
> Justin


From jsampson at guidewire.com  Mon Jan 12 15:17:18 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 12 Jan 2015 20:17:18 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420840643988-11820.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>

So, to summarize this discussion and add a bit more commentary...

* The proposal on the table is to add a new 'T compareAndSwap(T, T)'
  method to AtomicReference (and presumably all other AtomicXXX
  classes for consistency). No one is challenging the existing API
  methods.

* The main objections to adding compareAndSwap seem to be that it's
  just not as useful as the existing compareAndSet and that the old
  value is stale as soon as it's returned.

* If the caller of compareAndSwap assigns the returned value to a
  variable that erases to something more specific than Object, it
  incurs the overhead of a type check due to the implicit cast. But
  that's also already true for the existing getAndSet and friends.

* It turns out that both compare-and-set and compare-and-swap are
  equally easy and efficient to implement at the machine level on
  all architectures, so it's somewhat arbitrary that Unsafe only has
  compare-and-set operations (and somewhat confusing that they're
  named compareAndSwapXXX).

* C++ and C# both offer low-level compare-and-swap rather than
  merely compare-and-set operations.

* It's easy to implement either compare-and-set or compare-and-swap
  in Java given the other:

boolean compareAndSet(T expect, T update) {
    return compareAndSwap(expect, update) == expect;
}

T compareAndSwap(T expect, T update) {
    for (;;) {
        T current = get();
        if (current != expect || compareAndSet(expect, update))
            return current;
    }
}

* The weakCompareAndSet method must return boolean, so there's a
  consistency benefit to compareAndSet also returning boolean. But
  that reasoning doesn't apply to a new compareAndSwap method.

* The return value will still be confusing between compareAndSet and
  compareAndSwap for AtomicBoolean, though, since they will return
  different boolean values! On the other hand, there's no benefit to
  having both operations on AtomicBoolean anyway, since the return
  value from compareAndSet uniquely indicates what the old value
  must have been (!return^expect).

Cheers,
Justin


From TEREKHOV at de.ibm.com  Mon Jan 12 15:37:32 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Mon, 12 Jan 2015 21:37:32 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
Message-ID: <OF9B051ABD.19FCD20F-ONC1257DCB.00711870-C1257DCB.00714D16@de.ibm.com>

I suppose that in terms of pthreads mutexes and condvars it can be defined
'as if':

park: mutex.lock(), condvar.wait(mutex);
unpark: mutex.lock(), condvar.broadcast(), mutex.unlock();

('stickiness' of unpark aside for a moment)

regards,
alexander.

Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 12.01.2015
18:31:02

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Oleksandr Otenko <oleksandr.otenko at oracle.com>, "dholmes at ieee.org"
       <dholmes at ieee.org>, thurstonn <thurston at nomagicsoftware.com>,
       "concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
cc:
Subject:	Re: [concurrency-interest] unpark/park memory visibility


Oleksandr Otenko wrote:

> I agree with thurstonn and Justin. It is underspecified.

Thanks. :)

> park() waking up spuriously is not enough. park() waking up in
> response to a unpark() is not enough. Specifying that they should
> be executed in program order is not enough. What is important, is
> that if park() woke up in response to unpark() (actually observed
> the unpark), then the volatile reads following the park() in
> program order must synchronize-with the volatile writes preceding
> the unpark().

Right -- it doesn't need to be as strong as synchronizes-with
between the unpark and park themselves, but it still needs to be
tied into the synchronization order. How about adding something
like this to the LockSupport class documentation:

An unpark doesn't actually synchronize-with the park that it
unblocks, but each park and unpark does take a definite position in
the overall synchronization order. This is the order used in the
rules for permit handling, as described in the park and unpark
method documentation. As with other synchronization actions, the
position of any park or unpark in the synchronization order is
consistent with its position in the program order for the thread
that performs it. Therefore any synchronization actions preceding an
unpark in one thread are guaranteed to precede, in the overall
synchronization order, any synchronization actions following a park
that it unblocks in another thread. In particular, any write to a
volatile variable before an unpark in one thread synchronizes-with
any read of that volatile variable after a park that it unblocks in
another thread.

Cheers,
Justin

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From jsampson at guidewire.com  Mon Jan 12 15:43:58 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 12 Jan 2015 20:43:58 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B42151.2060509@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
	<54B42151.2060509@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81C94@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> > Therefore any synchronization actions preceding an unpark in one
> > thread are guaranteed to precede, in the overall synchronization
> > order, any synchronization actions following a park that it
> > unblocks in another thread.
>
> This is almost the definition of synchronizes-with. Synchronizes-with
> will join any two program orders into a happens-before, but you only
> require the program order of synchronization events to behave like
> that.

Well, the particular passage you quoted is just an example something
implied by the earlier bits _together with_ the definition of
synchronizes-with. The meat of my proposed wording is just this: "each
park and unpark does take a definite position in the overall
synchronization order. This is the order used in the rules for permit
handling". Everything else is implied by that.

> Perhaps, it is possible to specify park/unpark as synchronization
> events such that they appear in the total order of the events - and
> JVMs would be obliged to enforce program order consistent with that
> total order. Yet, we don't need to specify whether park/unpark create
> the hb edge. How and whether they communicate, and how that affects
> thread's progress, is a matter separate from visibility.

There's a distinction in the JLS between "synchronization order" and
"synchronizes-with." Merely being after something in the synchronization
order is not enough to create a synchronizes-with relationship. That's
precisely the distinction I'm drawing on in my proposed wording: Let
park() and unpark() take positions in the synchronization order without
actually inducing a synchronizes-with edge. I think that's all we need
in order to get the desired ordering semantics discussed in this email
thread, while allowing implementers sufficient flexibility to implement
the operations efficiently.

Cheers,
Justin


From thurston at nomagicsoftware.com  Mon Jan 12 17:11:49 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 12 Jan 2015 15:11:49 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
Message-ID: <1421100709820-11866.post@n7.nabble.com>

Well, everything is correct except for the extraneous part about "easy to
implement in Java"

What I am proposing is that there be a new **native** method added to Unsafe
(or whatever the Java 9 equivalent would be).


native T compareAndSwap(T expect, T update)


and similarly to AtomicReference, which would just delegate to the native
version; this is very important.


And I see no reason to remove the native current boolean version from Unsafe



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11866.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Mon Jan 12 18:00:12 2015
From: thurston at nomagicsoftware.com (thurston at nomagicsoftware.com)
Date: Mon, 12 Jan 2015 15:00:12 -0800
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <OF9B051ABD.19FCD20F-ONC1257DCB.00711870-C1257DCB.00714D16@de.ibm.com>
References: " <NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>"
	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>
	<OF9B051ABD.19FCD20F-ONC1257DCB.00711870-C1257DCB.00714D16@de.ibm.com>
Message-ID: <a943d97c66caf8fa42607315705463f0@nomagicsoftware.com>

Yes, that captures the semantics and provides the necessary ordering 
guarantees;
but I think it's at least implicit, that unpark() should never block



On 2015-01-12 12:37, Alexander Terekhov wrote:
> I suppose that in terms of pthreads mutexes and condvars it can be 
> defined
> 'as if':
> 
> park: mutex.lock(), condvar.wait(mutex);
> unpark: mutex.lock(), condvar.broadcast(), mutex.unlock();
> 
> ('stickiness' of unpark aside for a moment)
> 
> regards,
> alexander.
> 
> Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 12.01.2015
> 18:31:02
> 
> Sent by:	concurrency-interest-bounces at cs.oswego.edu
> 
> 
> To:	Oleksandr Otenko <oleksandr.otenko at oracle.com>, "dholmes at ieee.org"
>        <dholmes at ieee.org>, thurstonn <thurston at nomagicsoftware.com>,
>        "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> cc:
> Subject:	Re: [concurrency-interest] unpark/park memory visibility
> 
> 
> Oleksandr Otenko wrote:
> 
>> I agree with thurstonn and Justin. It is underspecified.
> 
> Thanks. :)
> 
>> park() waking up spuriously is not enough. park() waking up in
>> response to a unpark() is not enough. Specifying that they should
>> be executed in program order is not enough. What is important, is
>> that if park() woke up in response to unpark() (actually observed
>> the unpark), then the volatile reads following the park() in
>> program order must synchronize-with the volatile writes preceding
>> the unpark().
> 
> Right -- it doesn't need to be as strong as synchronizes-with
> between the unpark and park themselves, but it still needs to be
> tied into the synchronization order. How about adding something
> like this to the LockSupport class documentation:
> 
> An unpark doesn't actually synchronize-with the park that it
> unblocks, but each park and unpark does take a definite position in
> the overall synchronization order. This is the order used in the
> rules for permit handling, as described in the park and unpark
> method documentation. As with other synchronization actions, the
> position of any park or unpark in the synchronization order is
> consistent with its position in the program order for the thread
> that performs it. Therefore any synchronization actions preceding an
> unpark in one thread are guaranteed to precede, in the overall
> synchronization order, any synchronization actions following a park
> that it unblocks in another thread. In particular, any write to a
> volatile variable before an unpark in one thread synchronizes-with
> any read of that volatile variable after a park that it unblocks in
> another thread.
> 
> Cheers,
> Justin
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From davidcholmes at aapt.net.au  Mon Jan 12 18:13:04 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Jan 2015 09:13:04 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B3E1B9.1040603@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>

> What is important, is that if park() woke up in response to unpark()
(actually observed the unpark), then the volatile
> reads following the park() in program order must synchronize-with the
volatile writes preceding the unpark().

I don't see your point here. Normal volatile semantics apply to the volatile
accesses. If the park() returns due to the unpark() then that returning is
subsequent to the unpark(), which in turn is subsequent to the volatile
write before the unpark. And the volatile read is subsequent to the park()
returning. hence:

write -> unpark -> park-returns -> read

establishes the time order, and as the read is subsequent to the write, the
write happens-before the read. As per normal volatile semantics.

If the park returns spuriously it may or may not be subsequent to the write,
hence the read may or may not be subsequent to the write.

There is nothing special about park()/unpark() in terms of the memory model.
We specified the other thread-related synchronization-actions (start, join,
interrupt) to give the programmer a more usable programming model (where
communication between threads is not constrained to using volatile data).

David
-------
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
  Sent: Tuesday, 13 January 2015 1:01 AM
  To: dholmes at ieee.org; thurstonn; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] unpark/park memory visibility


  I agree with thurstonn and Justin. It is underspecified.

  park() waking up spuriously is not enough. park() waking up in response to
a unpark() is not enough. Specifying that they should be executed in program
order is not enough. What is important, is that if park() woke up in
response to unpark() (actually observed the unpark), then the volatile reads
following the park() in program order must synchronize-with the volatile
writes preceding the unpark().

  The difficulty with the specification is the presence of spurious
wake-ups. This means the implementation is allowed to completely consist of
spurious wake-ups. But when it doesn't always wake up spurously, sometimes
it won't wake up on its own - so not allowing the loop to go back to park()
is important, since there may be no more unparks. This means that the
wake-up event, if it is implemented, must become a synchronization event
placed exactly between the volatile write and volatile read in the total
order.


  Alex



  On 12/01/2015 01:28, David Holmes wrote:

thurston writes:
David Holmes-6 wrote
thurstonn writes:
Multi-thread histories are all about inter-thread semantics.
In the example history (which is valid according to the JMM), T1 can
"see" the unpark() before it sees the update to signal.
It is only valid according to the JMM if the intra-thread reordering of
the volatile write and unpark is allowed. You can't just treat the
unpark as a simple memory access.

Exactly my point; without a requirement that there is a
hb(unpark, depark),
a JVM implementation **theoretically** could implement (un)park in such a
way, that it entails nothing but a simple memory write/read (or even no
shared memory mutation at all). didn't alex describe (un) park as
"semantically a no-op"?  No-ops can be reordered with anything.
A JVM has to implement a park/unpark that functions in the context of that
JVM. You could implement park and unpark as no-ops and as such they could be
"reordered" with anything and it would all work exactly the same as-if they
had not been re-ordered.

I'm not saying that is practical (in fact I can't imagine how
that could be done, but I don't know how real JVM's implement it now).
But the
first rule of Java concurrency, is don't rely on that kind of thinking -
unless there
are explicit synchronization actions - intuition, etc. are *not reliable*

And so I'm just stating that the hb should be made explicit (I had assumed
it implicitly all along);
If you think it through, it's very hard to imagine how developers could
write bullet-proof correct concurrent code that relies on park/unpark
without there being a hb(unpark, depark), which of course would invalidate
the kind of histories above.
I understand what you are saying but it is up to an implementation to ensure
everything happens in the correct order for this API to work, we don't need
the JMM to say that park/unpark establish a specific ordering as it is not a
property of the programming model that needs to be exported to the
programmer. HB is something the implementor has to provide to the
programmer, not something that the implementor gets to use.

There's already an explicit "interrupt hb rule" (and a thread death rule,
etc) - park fits the same pattern.
It certainly could be defined that way, but it doesn't need to be defined
that way.

David
-----




--
View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11851.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/eeeaa3a1/attachment.html>

From oleksandr.otenko at oracle.com  Mon Jan 12 18:30:25 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 23:30:25 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B45911.1090904@oracle.com>

Well, the difference really is that start/join/interrupt are discussed 
in JMM, and park/unpark isn't. Then the statement about park seeing 
unpark is not enough - normal reads may see normal writes just as well, 
but that doesn't force the program orders to be observed consistently 
across the threads.

So the point really is about specifying that park/unpark are part of 
synchronization order.

Alex


On 12/01/2015 23:13, David Holmes wrote:
> >What is important, is that if park() woke up /*in response*/to 
> unpark() (actually observed the unpark), then the volatile
> > reads following the park() in program order *must synchronize-with*the volatile writes preceding 
> the unpark().
> I don't see your point here. Normal volatile semantics apply to the 
> volatile accesses. If the park() returns due to the unpark() then that 
> returning is subsequent to the unpark(), which in turn is subsequent 
> to the volatile write before the unpark. And the volatile read is 
> subsequent to the park() returning. hence:
> write -> unpark -> park-returns -> read
> establishes the time order, and as the read is subsequent to the 
> write, the write happens-before the read. As per normal volatile 
> semantics.
> If the park returns spuriously it may or may not be subsequent to the 
> write, hence the read may or may not be subsequent to the write.
> There is nothing special about park()/unpark() in terms of the memory 
> model. We specified the other thread-related synchronization-actions 
> (start, join, interrupt) to give the programmer a more usable 
> programming model (where communication between threads is not 
> constrained to using volatile data).
> David
> -------
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Tuesday, 13 January 2015 1:01 AM
>     *To:* dholmes at ieee.org; thurstonn; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] unpark/park memory visibility
>
>     I agree with thurstonn and Justin. It is underspecified.
>
>     park() waking up spuriously is not enough. park() waking up in
>     response to a unpark() is not enough. Specifying that they should
>     be executed in program order is not enough. What is important, is
>     that if park() woke up /*in response*/ to unpark() (actually
>     observed the unpark), then the volatile reads following the park()
>     in program order *must synchronize-with* the volatile writes
>     preceding the unpark().
>
>     The difficulty with the specification is the presence of spurious
>     wake-ups. This means the implementation is allowed to completely
>     consist of spurious wake-ups. But when it doesn't always wake up
>     spurously, sometimes it won't wake up on its own - so not allowing
>     the loop to go back to park() is important, since there may be no
>     more unparks. This means that the wake-up event, if it is
>     implemented, must become a synchronization event placed exactly
>     between the volatile write and volatile read in the total order.
>
>
>     Alex
>
>
>     On 12/01/2015 01:28, David Holmes wrote:
>>     thurston writes:
>>>     David Holmes-6 wrote
>>>>     thurstonn writes:
>>>>>     Multi-thread histories are all about inter-thread semantics.
>>>>>     In the example history (which is valid according to the JMM), T1 can
>>>>>     "see" the unpark() before it sees the update to signal.
>>>>     It is only valid according to the JMM if the intra-thread reordering of
>>>>     the volatile write and unpark is allowed. You can't just treat the
>>>>     unpark as a simple memory access.
>>>>
>>>     Exactly my point; without a requirement that there is a
>>>     hb(unpark, depark),
>>>     a JVM implementation **theoretically** could implement (un)park in such a
>>>     way, that it entails nothing but a simple memory write/read (or even no
>>>     shared memory mutation at all). didn't alex describe (un) park as
>>>     "semantically a no-op"?  No-ops can be reordered with anything.
>>     A JVM has to implement a park/unpark that functions in the context of that
>>     JVM. You could implement park and unpark as no-ops and as such they could be
>>     "reordered" with anything and it would all work exactly the same as-if they
>>     had not been re-ordered.
>>
>>>     I'm not saying that is practical (in fact I can't imagine how
>>>     that could be done, but I don't know how real JVM's implement it now).
>>     But the
>>>     first rule of Java concurrency, is don't rely on that kind of thinking -
>>     unless there
>>>     are explicit synchronization actions - intuition, etc. are *not reliable*
>>>
>>>     And so I'm just stating that the hb should be made explicit (I had assumed
>>>     it implicitly all along);
>>>     If you think it through, it's very hard to imagine how developers could
>>>     write bullet-proof correct concurrent code that relies on park/unpark
>>>     without there being a hb(unpark, depark), which of course would invalidate
>>>     the kind of histories above.
>>     I understand what you are saying but it is up to an implementation to ensure
>>     everything happens in the correct order for this API to work, we don't need
>>     the JMM to say that park/unpark establish a specific ordering as it is not a
>>     property of the programming model that needs to be exported to the
>>     programmer. HB is something the implementor has to provide to the
>>     programmer, not something that the implementor gets to use.
>>
>>>     There's already an explicit "interrupt hb rule" (and a thread death rule,
>>>     etc) - park fits the same pattern.
>>     It certainly could be defined that way, but it doesn't need to be defined
>>     that way.
>>
>>     David
>>     -----
>>
>>>
>>>     --
>>>     View this message in context:
>>     http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
>>     tp11812p11851.html
>>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/e98baff1/attachment-0001.html>

From jsampson at guidewire.com  Mon Jan 12 18:32:40 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 12 Jan 2015 23:32:40 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
References: <54B3E1B9.1040603@oracle.com>
	<NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> write -> unpark -> park-returns -> read
>
> establishes the time order, and as the read is subsequent to the
> write, the write happens-before the read. As per normal volatile
> semantics.

The concern being raised in this thread is that there's nothing in
the spec to prevent the write being reordered later:

unpark -> park-returns -> read -> write

You've suggested that there's something about the nature of the park
and unpark methods that necessarily prevents the compiler and CPU
from reordering them with reads and writes, but it's still not clear
to some of us what that nature might be.

Cheers,
Justin


From davidcholmes at aapt.net.au  Mon Jan 12 18:36:09 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Jan 2015 09:36:09 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B45911.1090904@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEGKKMAA.davidcholmes@aapt.net.au>

They don't need to be specified as part of the synchronization order!

David
  -----Original Message-----
  From: Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
  Sent: Tuesday, 13 January 2015 9:30 AM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] unpark/park memory visibility


  Well, the difference really is that start/join/interrupt are discussed in
JMM, and park/unpark isn't. Then the statement about park seeing unpark is
not enough - normal reads may see normal writes just as well, but that
doesn't force the program orders to be observed consistently across the
threads.

  So the point really is about specifying that park/unpark are part of
synchronization order.

  Alex



  On 12/01/2015 23:13, David Holmes wrote:

    > What is important, is that if park() woke up in response to unpark()
(actually observed the unpark), then the volatile
    > reads following the park() in program order must synchronize-with the
volatile writes preceding the unpark().

    I don't see your point here. Normal volatile semantics apply to the
volatile accesses. If the park() returns due to the unpark() then that
returning is subsequent to the unpark(), which in turn is subsequent to the
volatile write before the unpark. And the volatile read is subsequent to the
park() returning. hence:

    write -> unpark -> park-returns -> read

    establishes the time order, and as the read is subsequent to the write,
the write happens-before the read. As per normal volatile semantics.

    If the park returns spuriously it may or may not be subsequent to the
write, hence the read may or may not be subsequent to the write.

    There is nothing special about park()/unpark() in terms of the memory
model. We specified the other thread-related synchronization-actions (start,
join, interrupt) to give the programmer a more usable programming model
(where communication between threads is not constrained to using volatile
data).

    David
    -------
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr
Otenko
      Sent: Tuesday, 13 January 2015 1:01 AM
      To: dholmes at ieee.org; thurstonn; concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] unpark/park memory visibility


      I agree with thurstonn and Justin. It is underspecified.

      park() waking up spuriously is not enough. park() waking up in
response to a unpark() is not enough. Specifying that they should be
executed in program order is not enough. What is important, is that if
park() woke up in response to unpark() (actually observed the unpark), then
the volatile reads following the park() in program order must
synchronize-with the volatile writes preceding the unpark().

      The difficulty with the specification is the presence of spurious
wake-ups. This means the implementation is allowed to completely consist of
spurious wake-ups. But when it doesn't always wake up spurously, sometimes
it won't wake up on its own - so not allowing the loop to go back to park()
is important, since there may be no more unparks. This means that the
wake-up event, if it is implemented, must become a synchronization event
placed exactly between the volatile write and volatile read in the total
order.


      Alex



      On 12/01/2015 01:28, David Holmes wrote:

thurston writes:
David Holmes-6 wrote
thurstonn writes:
Multi-thread histories are all about inter-thread semantics.
In the example history (which is valid according to the JMM), T1 can
"see" the unpark() before it sees the update to signal.
It is only valid according to the JMM if the intra-thread reordering of
the volatile write and unpark is allowed. You can't just treat the
unpark as a simple memory access.

Exactly my point; without a requirement that there is a
hb(unpark, depark),
a JVM implementation **theoretically** could implement (un)park in such a
way, that it entails nothing but a simple memory write/read (or even no
shared memory mutation at all). didn't alex describe (un) park as
"semantically a no-op"?  No-ops can be reordered with anything.
A JVM has to implement a park/unpark that functions in the context of that
JVM. You could implement park and unpark as no-ops and as such they could be
"reordered" with anything and it would all work exactly the same as-if they
had not been re-ordered.

I'm not saying that is practical (in fact I can't imagine how
that could be done, but I don't know how real JVM's implement it now).
But the
first rule of Java concurrency, is don't rely on that kind of thinking -
unless there
are explicit synchronization actions - intuition, etc. are *not reliable*

And so I'm just stating that the hb should be made explicit (I had assumed
it implicitly all along);
If you think it through, it's very hard to imagine how developers could
write bullet-proof correct concurrent code that relies on park/unpark
without there being a hb(unpark, depark), which of course would invalidate
the kind of histories above.
I understand what you are saying but it is up to an implementation to ensure
everything happens in the correct order for this API to work, we don't need
the JMM to say that park/unpark establish a specific ordering as it is not a
property of the programming model that needs to be exported to the
programmer. HB is something the implementor has to provide to the
programmer, not something that the implementor gets to use.

There's already an explicit "interrupt hb rule" (and a thread death rule,
etc) - park fits the same pattern.
It certainly could be defined that way, but it doesn't need to be defined
that way.

David
-----


--
View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
tp11812p11851.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/83e80640/attachment.html>

From davidcholmes at aapt.net.au  Mon Jan 12 18:47:04 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Jan 2015 09:47:04 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>

Justin writes:
>
> David Holmes wrote:
>
> > write -> unpark -> park-returns -> read
> >
> > establishes the time order, and as the read is subsequent to the
> > write, the write happens-before the read. As per normal volatile
> > semantics.
>
> The concern being raised in this thread is that there's nothing in
> the spec to prevent the write being reordered later:
>
> unpark -> park-returns -> read -> write
>
> You've suggested that there's something about the nature of the park
> and unpark methods that necessarily prevents the compiler and CPU
> from reordering them with reads and writes, but it's still not clear
> to some of us what that nature might be.

Are you saying that you think any pair of statements can be reordered unless
there is something in the JMM that says they can't? E.g.

canvas.setFillRegion(x1,x2, y1, y2);
canvas.fillRegion(Color.BLUE);

might be reordered to:

canvas.fillRegion(Color.BLUE);
canvas.setFillRegion(x1,x2, y1, y2);

or:

database.commit();
data_commited=true; // volatile

might be reordered to:

data_commited=true; // volatile
database.commit();

??

David
-----


> Cheers,
> Justin


From oleksandr.otenko at oracle.com  Mon Jan 12 18:48:59 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Jan 2015 23:48:59 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEGKKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEGKKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B45D6B.9040504@oracle.com>

Ok, which statement says that park/unpark do not behave like normal 
read/write?

Alex


On 12/01/2015 23:36, David Holmes wrote:
> They don't need to be specified as part of the synchronization order!
> David
>
>     -----Original Message-----
>     *From:* Oleksandr Otenko [mailto:oleksandr.otenko at oracle.com]
>     *Sent:* Tuesday, 13 January 2015 9:30 AM
>     *To:* dholmes at ieee.org; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] unpark/park memory visibility
>
>     Well, the difference really is that start/join/interrupt are
>     discussed in JMM, and park/unpark isn't. Then the statement about
>     park seeing unpark is not enough - normal reads may see normal
>     writes just as well, but that doesn't force the program orders to
>     be observed consistently across the threads.
>
>     So the point really is about specifying that park/unpark are part
>     of synchronization order.
>
>     Alex
>
>
>     On 12/01/2015 23:13, David Holmes wrote:
>>     >What is important, is that if park() woke up /*in response*/to
>>     unpark() (actually observed the unpark), then the volatile
>>     > reads following the park() in program order *must synchronize-with*the volatile
>>     writes preceding the unpark().
>>     I don't see your point here. Normal volatile semantics apply to
>>     the volatile accesses. If the park() returns due to the unpark()
>>     then that returning is subsequent to the unpark(), which in turn
>>     is subsequent to the volatile write before the unpark. And the
>>     volatile read is subsequent to the park() returning. hence:
>>     write -> unpark -> park-returns -> read
>>     establishes the time order, and as the read is subsequent to the
>>     write, the write happens-before the read. As per normal volatile
>>     semantics.
>>     If the park returns spuriously it may or may not be subsequent to
>>     the write, hence the read may or may not be subsequent to the write.
>>     There is nothing special about park()/unpark() in terms of the
>>     memory model. We specified the other thread-related
>>     synchronization-actions (start, join, interrupt) to give the
>>     programmer a more usable programming model (where communication
>>     between threads is not constrained to using volatile data).
>>     David
>>     -------
>>
>>         -----Original Message-----
>>         *From:* concurrency-interest-bounces at cs.oswego.edu
>>         [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf
>>         Of *Oleksandr Otenko
>>         *Sent:* Tuesday, 13 January 2015 1:01 AM
>>         *To:* dholmes at ieee.org; thurstonn;
>>         concurrency-interest at cs.oswego.edu
>>         *Subject:* Re: [concurrency-interest] unpark/park memory
>>         visibility
>>
>>         I agree with thurstonn and Justin. It is underspecified.
>>
>>         park() waking up spuriously is not enough. park() waking up
>>         in response to a unpark() is not enough. Specifying that they
>>         should be executed in program order is not enough. What is
>>         important, is that if park() woke up /*in response*/ to
>>         unpark() (actually observed the unpark), then the volatile
>>         reads following the park() in program order *must
>>         synchronize-with* the volatile writes preceding the unpark().
>>
>>         The difficulty with the specification is the presence of
>>         spurious wake-ups. This means the implementation is allowed
>>         to completely consist of spurious wake-ups. But when it
>>         doesn't always wake up spurously, sometimes it won't wake up
>>         on its own - so not allowing the loop to go back to park() is
>>         important, since there may be no more unparks. This means
>>         that the wake-up event, if it is implemented, must become a
>>         synchronization event placed exactly between the volatile
>>         write and volatile read in the total order.
>>
>>
>>         Alex
>>
>>
>>         On 12/01/2015 01:28, David Holmes wrote:
>>>         thurston writes:
>>>>         David Holmes-6 wrote
>>>>>         thurstonn writes:
>>>>>>         Multi-thread histories are all about inter-thread semantics.
>>>>>>         In the example history (which is valid according to the JMM), T1 can
>>>>>>         "see" the unpark() before it sees the update to signal.
>>>>>         It is only valid according to the JMM if the intra-thread reordering of
>>>>>         the volatile write and unpark is allowed. You can't just treat the
>>>>>         unpark as a simple memory access.
>>>>>
>>>>         Exactly my point; without a requirement that there is a
>>>>         hb(unpark, depark),
>>>>         a JVM implementation **theoretically** could implement (un)park in such a
>>>>         way, that it entails nothing but a simple memory write/read (or even no
>>>>         shared memory mutation at all). didn't alex describe (un) park as
>>>>         "semantically a no-op"?  No-ops can be reordered with anything.
>>>         A JVM has to implement a park/unpark that functions in the context of that
>>>         JVM. You could implement park and unpark as no-ops and as such they could be
>>>         "reordered" with anything and it would all work exactly the same as-if they
>>>         had not been re-ordered.
>>>
>>>>         I'm not saying that is practical (in fact I can't imagine how
>>>>         that could be done, but I don't know how real JVM's implement it now).
>>>         But the
>>>>         first rule of Java concurrency, is don't rely on that kind of thinking -
>>>         unless there
>>>>         are explicit synchronization actions - intuition, etc. are *not reliable*
>>>>
>>>>         And so I'm just stating that the hb should be made explicit (I had assumed
>>>>         it implicitly all along);
>>>>         If you think it through, it's very hard to imagine how developers could
>>>>         write bullet-proof correct concurrent code that relies on park/unpark
>>>>         without there being a hb(unpark, depark), which of course would invalidate
>>>>         the kind of histories above.
>>>         I understand what you are saying but it is up to an implementation to ensure
>>>         everything happens in the correct order for this API to work, we don't need
>>>         the JMM to say that park/unpark establish a specific ordering as it is not a
>>>         property of the programming model that needs to be exported to the
>>>         programmer. HB is something the implementor has to provide to the
>>>         programmer, not something that the implementor gets to use.
>>>
>>>>         There's already an explicit "interrupt hb rule" (and a thread death rule,
>>>>         etc) - park fits the same pattern.
>>>         It certainly could be defined that way, but it doesn't need to be defined
>>>         that way.
>>>
>>>         David
>>>         -----
>>>
>>>>         --
>>>>         View this message in context:
>>>         http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
>>>         tp11812p11851.html
>>>         Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/03e67820/attachment-0001.html>

From jsampson at guidewire.com  Mon Jan 12 18:50:01 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 12 Jan 2015 23:50:01 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1421100709820-11866.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> Well, everything is correct except for the extraneous part about
> "easy to implement in Java"

The easy implementation was right there in my email:

T compareAndSwap(T expect, T update) {
    for (;;) {
        T current = get();
        if (current != expect || compareAndSet(expect, update))
            return current;
    }
}

That's very much in line with other methods that exist on the
various AtomicXXX classes. My point was that it would be very easy
to add such a method to every AtomicXXX class without adding
anything to Unsafe.

> What I am proposing is that there be a new **native** method added
> to Unsafe (or whatever the Java 9 equivalent would be).
>
> native T compareAndSwap(T expect, T update)
>
> and similarly to AtomicReference, which would just delegate to the
> native version; this is very important.

Why is it "very important" that it be implemented as a native method
in Unsafe that AtomicReference delegates to?

I just realized that several methods (including getAndSet) were
implemented in Java back in 7 but are native now in 8. And some new
methods (such as getAndUpdate) are yet again implemented in Java.
And oddly AtomicBoolean's getAndSet is still implemented in Java as
well. So your use case could be met by adding a Java implementation
now, with a native implementation as a possible future enhancement,
couldn't it?

Cheers,
Justin


From jsampson at guidewire.com  Mon Jan 12 19:07:33 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 13 Jan 2015 00:07:33 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> Are you saying that you think any pair of statements can be
> reordered unless there is something in the JMM that says they
> can't?

Yes, I was under the impression that was a good way to think about
reordering!

> E.g.
>
> canvas.setFillRegion(x1,x2, y1, y2);
> canvas.fillRegion(Color.BLUE);
>
> might be reordered to:
>
> canvas.fillRegion(Color.BLUE);
> canvas.setFillRegion(x1,x2, y1, y2);

Doesn't that depend on what those methods actually do? Presumably
the fillRegion method reads some memory locations that were set by
the setFillRegion method, such that the JMM requires that they
_appear_ to execute in that order _to the current thread_, but not
necessarily to any other threads without proper synchronization.

> or:
>
> database.commit();
> data_commited=true; // volatile
>
> might be reordered to:
>
> data_commited=true; // volatile
> database.commit();

If database.commit() performs any memory writes whatsoever, doesn't
the JMM forbid reordering those writes after the volatile write?

> ??

Ditto!

Thanks,
Justin


From TEREKHOV at de.ibm.com  Mon Jan 12 19:25:12 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 13 Jan 2015 01:25:12 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <a943d97c66caf8fa42607315705463f0@nomagicsoftware.com>
References: "
	<NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>"	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>	<OF9B051ABD.19FCD20F-ONC1257DCB.00711870-C1257DCB.00714D16@de.ibm.com>
	<a943d97c66caf8fa42607315705463f0@nomagicsoftware.com>
Message-ID: <OF4A950622.73A6B5B5-ONC1257DCC.0001BBC0-C1257DCC.00024EFA@de.ibm.com>

with pthreads mutexes and condvars... realtime, cancellation aside, it is
possible to get rid of condvars completely:

condvar.wait(mutex): mutex.unlock();
condvar.signal/broadcast: /* nop */;

as such, it follows:

park: mutex.lock(), mutex.unlock();
unpark: mutex.lock(), mutex.unlock();

(again 'stickiness' aside for a moment)

why would you care about 'blocking' in this case?

regards,
alexander.

thurston at nomagicsoftware.com@cs.oswego.edu on 13.01.2015 00:00:12

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	concurrency-interest at cs.oswego.edu, dholmes at ieee.org
Subject:	Re: [concurrency-interest] unpark/park memory visibility


Yes, that captures the semantics and provides the necessary ordering
guarantees;
but I think it's at least implicit, that unpark() should never block



On 2015-01-12 12:37, Alexander Terekhov wrote:
> I suppose that in terms of pthreads mutexes and condvars it can be
> defined
> 'as if':
>
> park: mutex.lock(), condvar.wait(mutex);
> unpark: mutex.lock(), condvar.broadcast(), mutex.unlock();
>
> ('stickiness' of unpark aside for a moment)
>
> regards,
> alexander.
>
> Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 12.01.2015
> 18:31:02
>
> Sent by:		 concurrency-interest-bounces at cs.oswego.edu
>
>
> To:		 Oleksandr Otenko <oleksandr.otenko at oracle.com>,
"dholmes at ieee.org"
>        <dholmes at ieee.org>, thurstonn <thurston at nomagicsoftware.com>,
>        "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> cc:
> Subject:		 Re: [concurrency-interest] unpark/park memory visibility
>
>
> Oleksandr Otenko wrote:
>
>> I agree with thurstonn and Justin. It is underspecified.
>
> Thanks. :)
>
>> park() waking up spuriously is not enough. park() waking up in
>> response to a unpark() is not enough. Specifying that they should
>> be executed in program order is not enough. What is important, is
>> that if park() woke up in response to unpark() (actually observed
>> the unpark), then the volatile reads following the park() in
>> program order must synchronize-with the volatile writes preceding
>> the unpark().
>
> Right -- it doesn't need to be as strong as synchronizes-with
> between the unpark and park themselves, but it still needs to be
> tied into the synchronization order. How about adding something
> like this to the LockSupport class documentation:
>
> An unpark doesn't actually synchronize-with the park that it
> unblocks, but each park and unpark does take a definite position in
> the overall synchronization order. This is the order used in the
> rules for permit handling, as described in the park and unpark
> method documentation. As with other synchronization actions, the
> position of any park or unpark in the synchronization order is
> consistent with its position in the program order for the thread
> that performs it. Therefore any synchronization actions preceding an
> unpark in one thread are guaranteed to precede, in the overall
> synchronization order, any synchronization actions following a park
> that it unblocks in another thread. In particular, any write to a
> volatile variable before an unpark in one thread synchronizes-with
> any read of that volatile variable after a park that it unblocks in
> another thread.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From vitalyd at gmail.com  Mon Jan 12 19:48:42 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 12 Jan 2015 19:48:42 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37GJuadpMThmOU4m9cK22K2EM4hUTUr==Ja0YTzmbSGUCQ@mail.gmail.com>

Presumably he wants it in unsafe so that jit intrinsics pick it up and you
don't have to do the loop dance where xchg is available (e.g x86).

Sent from my phone
On Jan 12, 2015 7:30 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> thurstonn wrote:
>
> > Well, everything is correct except for the extraneous part about
> > "easy to implement in Java"
>
> The easy implementation was right there in my email:
>
> T compareAndSwap(T expect, T update) {
>     for (;;) {
>         T current = get();
>         if (current != expect || compareAndSet(expect, update))
>             return current;
>     }
> }
>
> That's very much in line with other methods that exist on the
> various AtomicXXX classes. My point was that it would be very easy
> to add such a method to every AtomicXXX class without adding
> anything to Unsafe.
>
> > What I am proposing is that there be a new **native** method added
> > to Unsafe (or whatever the Java 9 equivalent would be).
> >
> > native T compareAndSwap(T expect, T update)
> >
> > and similarly to AtomicReference, which would just delegate to the
> > native version; this is very important.
>
> Why is it "very important" that it be implemented as a native method
> in Unsafe that AtomicReference delegates to?
>
> I just realized that several methods (including getAndSet) were
> implemented in Java back in 7 but are native now in 8. And some new
> methods (such as getAndUpdate) are yet again implemented in Java.
> And oddly AtomicBoolean's getAndSet is still implemented in Java as
> well. So your use case could be met by adding a Java implementation
> now, with a native implementation as a possible future enhancement,
> couldn't it?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/6739f661/attachment.html>

From TEREKHOV at de.ibm.com  Mon Jan 12 19:51:49 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 13 Jan 2015 01:51:49 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <OF4A950622.73A6B5B5-ONC1257DCC.0001BBC0-C1257DCC.00024EFA@de.ibm.com>
References: "	<NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>"	<54B3E1B9.1040603@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81A7B@sm-ex-01-vm.guidewire.com>	<OF9B051ABD.19FCD20F-ONC1257DCB.00711870-C1257DCB.00714D16@de.ibm.com>
	<a943d97c66caf8fa42607315705463f0@nomagicsoftware.com>
	<OF4A950622.73A6B5B5-ONC1257DCC.0001BBC0-C1257DCC.00024EFA@de.ibm.com>
Message-ID: <OFD37F6BCC.30B41262-ONC1257DCC.00045692-C1257DCC.0004BEDB@de.ibm.com>

correction:

condvar.wait(mutex): mutex.unlock(), mutex.lock();
condvar.signal/broadcast: /* nop */;

as such, it follows:

park: mutex.lock(), mutex.unlock(), mutex.lock();
unpark: mutex.lock(), mutex.unlock();


Alexander Terekhov/Germany/IBM at IBMDE@cs.oswego.edu on 13.01.2015 01:25:12

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	thurston at nomagicsoftware.com
cc:	concurrency-interest at cs.oswego.edu, dholmes at ieee.org
Subject:	Re: [concurrency-interest] unpark/park memory visibility


with pthreads mutexes and condvars... realtime, cancellation aside, it is
possible to get rid of condvars completely:

condvar.wait(mutex): mutex.unlock();
condvar.signal/broadcast: /* nop */;

as such, it follows:

park: mutex.lock(), mutex.unlock();
unpark: mutex.lock(), mutex.unlock();

(again 'stickiness' aside for a moment)

why would you care about 'blocking' in this case?

regards,
alexander.

thurston at nomagicsoftware.com@cs.oswego.edu on 13.01.2015 00:00:12

Sent by:		 concurrency-interest-bounces at cs.oswego.edu


To:		 Alexander Terekhov/Germany/IBM at IBMDE
cc:		 concurrency-interest at cs.oswego.edu, dholmes at ieee.org
Subject:		 Re: [concurrency-interest] unpark/park memory visibility


Yes, that captures the semantics and provides the necessary ordering
guarantees;
but I think it's at least implicit, that unpark() should never block



On 2015-01-12 12:37, Alexander Terekhov wrote:
> I suppose that in terms of pthreads mutexes and condvars it can be
> defined
> 'as if':
>
> park: mutex.lock(), condvar.wait(mutex);
> unpark: mutex.lock(), condvar.broadcast(), mutex.unlock();
>
> ('stickiness' of unpark aside for a moment)
>
> regards,
> alexander.
>
> Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 12.01.2015
> 18:31:02
>
> Sent by:		 		  concurrency-interest-bounces at cs.oswego.edu
>
>
> To:		 		  Oleksandr Otenko <oleksandr.otenko at oracle.com>,
"dholmes at ieee.org"
>        <dholmes at ieee.org>, thurstonn <thurston at nomagicsoftware.com>,
>        "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> cc:
> Subject:		 		  Re: [concurrency-interest] unpark/park
memory visibility
>
>
> Oleksandr Otenko wrote:
>
>> I agree with thurstonn and Justin. It is underspecified.
>
> Thanks. :)
>
>> park() waking up spuriously is not enough. park() waking up in
>> response to a unpark() is not enough. Specifying that they should
>> be executed in program order is not enough. What is important, is
>> that if park() woke up in response to unpark() (actually observed
>> the unpark), then the volatile reads following the park() in
>> program order must synchronize-with the volatile writes preceding
>> the unpark().
>
> Right -- it doesn't need to be as strong as synchronizes-with
> between the unpark and park themselves, but it still needs to be
> tied into the synchronization order. How about adding something
> like this to the LockSupport class documentation:
>
> An unpark doesn't actually synchronize-with the park that it
> unblocks, but each park and unpark does take a definite position in
> the overall synchronization order. This is the order used in the
> rules for permit handling, as described in the park and unpark
> method documentation. As with other synchronization actions, the
> position of any park or unpark in the synchronization order is
> consistent with its position in the program order for the thread
> that performs it. Therefore any synchronization actions preceding an
> unpark in one thread are guaranteed to precede, in the overall
> synchronization order, any synchronization actions following a park
> that it unblocks in another thread. In particular, any write to a
> volatile variable before an unpark in one thread synchronizes-with
> any read of that volatile variable after a park that it unblocks in
> another thread.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From vitalyd at gmail.com  Mon Jan 12 19:56:38 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 12 Jan 2015 19:56:38 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37Gua5oj1f=uukWNrv71AuChkCyGRJQEJ_xHE-1MGOV4tw@mail.gmail.com>

I think the gist of the question is: can unpark () move ahead of the
volatile store in the example given earlier:

signal = true;
unpark (thread);

I think the answer is yes from JMM standpoint given that unpark isn't
specifically called out.  In practice perhaps that's not true because
unpark () is a JNI call and treated like a black box by the optimizer (at
least I didn't find an intrinsic for it at a cursory glance).

Sent from my phone
On Jan 12, 2015 7:37 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> David Holmes wrote:
>
> > Are you saying that you think any pair of statements can be
> > reordered unless there is something in the JMM that says they
> > can't?
>
> Yes, I was under the impression that was a good way to think about
> reordering!
>
> > E.g.
> >
> > canvas.setFillRegion(x1,x2, y1, y2);
> > canvas.fillRegion(Color.BLUE);
> >
> > might be reordered to:
> >
> > canvas.fillRegion(Color.BLUE);
> > canvas.setFillRegion(x1,x2, y1, y2);
>
> Doesn't that depend on what those methods actually do? Presumably
> the fillRegion method reads some memory locations that were set by
> the setFillRegion method, such that the JMM requires that they
> _appear_ to execute in that order _to the current thread_, but not
> necessarily to any other threads without proper synchronization.
>
> > or:
> >
> > database.commit();
> > data_commited=true; // volatile
> >
> > might be reordered to:
> >
> > data_commited=true; // volatile
> > database.commit();
>
> If database.commit() performs any memory writes whatsoever, doesn't
> the JMM forbid reordering those writes after the volatile write?
>
> > ??
>
> Ditto!
>
> Thanks,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150112/e2529f9a/attachment-0001.html>

From dl at cs.oswego.edu  Mon Jan 12 20:17:36 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 12 Jan 2015 20:17:36 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B47230.5090405@cs.oswego.edu>

On 01/12/2015 06:47 PM, David Holmes wrote:
> Justin writes:

>>
>> You've suggested that there's something about the nature of the park
>> and unpark methods that necessarily prevents the compiler and CPU
>> from reordering them with reads and writes, but it's still not clear
>> to some of us what that nature might be.
>
> Are you saying that you think any pair of statements can be reordered unless
> there is something in the JMM that says they can't? E.g.

I think what Justin means is that In principle, for Java-level code,
people could look inside methods to see if they impose any ordering
constraints. But that doesn't apply to intrinsics.
Fair enough. Not that it seems to enhance anyone's understanding,
but I suppose we could add:

park: This method has the memory semantics of a volatile read and
write to a variable also accessed by any caller of unpark
for the current thread.

unpark: This method has the memory semantics of a volatile write
to a variable also accessed by the caller of park for the given
thread.

-Doug



From thurston at nomagicsoftware.com  Mon Jan 12 20:28:46 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 12 Jan 2015 18:28:46 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
Message-ID: <1421112526622-11881.post@n7.nabble.com>

Justin Sampson wrote
> thurstonn wrote:
> 
> 
> The easy implementation was right there in my email:
> 
> T compareAndSwap(T expect, T update) {
>     for (;;) {
>         T current = get();
>         if (current != expect || compareAndSet(expect, update))
>             return current;
>     }
> }

There are two large problems with the above:
1.  First and foremost, it is unnecessarily slow - in the "success" case it
does a minimum 2 loads, while at least on some hardware platforms (x86), the
native implementation need only do a single load for both success and
failure; that's unacceptable.
2.  In worst-case, the above is O(infinity) - it can loop forever.


And as Andrew and you have pointed out, the assembly code for the extant
native boolean compareAndSet(...) can be entirely duplicated for the
proposed new CAS *on all platforms, including LL/SC based ones*, i.e. it
does the exact same work (therefore with the same performance
characteristics), except that one returns a boolean and the other returns a
reference/pointer.


Obviously, anyone can write the Java implementation above (although I would
probably prefer to give up the "atomicity", for a bounded worst-case), i.e.

T compareAndSwap(T expect, T update) 
{
    
        
        if (compareAndSet(expect, update))
            return expect;
        else
            return get()
}


but regardless, that's a poor substitute for the native version (which does
not require any such compromise).

You did an excellent job summarizing the discussion (and hopefully the
consensus?), except I didn't want the conclusion to be that the native T
cas(T, T) could be ignored, which I don't even think you were advocating,
but muddied the waters unnecessarily.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11881.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Mon Jan 12 20:46:57 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Mon, 12 Jan 2015 18:46:57 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B47230.5090405@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCCEFKKMAA.davidcholmes@aapt.net.au>
	<1421019491714-11849.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>
	<1421023216623-11851.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
	<NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<54B47230.5090405@cs.oswego.edu>
Message-ID: <1421113617093-11882.post@n7.nabble.com>

Hello Doug,

Doesn't AQS depend on those very same (currently unspecified) memory
semantics?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11882.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Tue Jan 13 01:01:07 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 13 Jan 2015 06:01:07 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B47230.5090405@cs.oswego.edu>
References: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<54B47230.5090405@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D822BE@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> I think what Justin means is that In principle, for Java-level
> code, people could look inside methods to see if they impose any
> ordering constraints. But that doesn't apply to intrinsics.

Yeah -- I wouldn't assume in general that every native method is
surrounded by fences. And the docs for LockSupport don't say
whether it's based on intrinsics, regular native calls, or
Java-level code anyway.

> Fair enough. Not that it seems to enhance anyone's understanding,
> but I suppose we could add:
>
> park: This method has the memory semantics of a volatile read and
> write to a variable also accessed by any caller of unpark
> for the current thread.
>
> unpark: This method has the memory semantics of a volatile write
> to a variable also accessed by the caller of park for the given
> thread.

Ah! So you're willing to say it's a happens-before after all. David
said that would be "an excessive burden," which is what led me to
try my hand at defining weaker-but-still-useful ordering semantics.

Actually, looking again at the class documentation for LockSupport,
I see in the very first sentence that the "permit" is described as
being "in the sense of the Semaphore class." The only difference
mentioned is that "permits do not accumulate." Was that intended to
imply the same memory consistency effects as Semaphore?

Cheers,
Justin


From davidcholmes at aapt.net.au  Tue Jan 13 01:27:28 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Jan 2015 16:27:28 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>

Justin Sampson writes:
> David Holmes wrote:
>
> > Are you saying that you think any pair of statements can be
> > reordered unless there is something in the JMM that says they
> > can't?
>
> Yes, I was under the impression that was a good way to think about
> reordering!

For memory accesses perhaps, but not for arbitrary code! How can you
possibly know what can and can't be reordered if everything is up for grabs
???

> > E.g.
> >
> > canvas.setFillRegion(x1,x2, y1, y2);
> > canvas.fillRegion(Color.BLUE);
> >
> > might be reordered to:
> >
> > canvas.fillRegion(Color.BLUE);
> > canvas.setFillRegion(x1,x2, y1, y2);
>
> Doesn't that depend on what those methods actually do? Presumably
> the fillRegion method reads some memory locations that were set by
> the setFillRegion method, such that the JMM requires that they
> _appear_ to execute in that order _to the current thread_, but not
> necessarily to any other threads without proper synchronization.

So you're happy to rely on program order to prevent reordering.

> > or:
> >
> > database.commit();
> > data_commited=true; // volatile
> >
> > might be reordered to:
> >
> > data_commited=true; // volatile
> > database.commit();
>
> If database.commit() performs any memory writes whatsoever, doesn't
> the JMM forbid reordering those writes after the volatile write?

Let's assume that is true. So you are happy to rely on the above code being
kept in the order specified as long as commit() contains at least one
write** - no need for the JMM to mention the commit() method. Okay. Now
replace commit() with unpark() - what's the difference?

** I'll ignore how you would know in general if indeed commit() did contain
a write!

David
-----

> > ??
>
> Ditto!
>
> Thanks,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Tue Jan 13 01:27:16 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 13 Jan 2015 06:27:16 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1421112526622-11881.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
	<1421112526622-11881.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> There are two large problems with the above:
> 1. First and foremost, it is unnecessarily slow - in the "success"
> case it does a minimum 2 loads, while at least on some hardware
> platforms (x86), the native implementation need only do a single
> load for both success and failure; that's unacceptable.
> 2. In worst-case, the above is O(infinity) - it can loop forever.

This idiom is so common in j.u.c that I figured it would be
uncontroversial. I keep hearing that volatile reads aren't a big
deal, and due to the strong semantics of compareAndSet it's sure to
succeed eventually.

I suppose if the successful path is expected to be more common it
could be optimized:

T compareAndSwap(T expect, T update) {
    if (compareAndSet(expect, update))
        return expect;
    for (;;) {
        T current = get();
        if (current != expect || compareAndSet(expect, update))
            return current;
    }
}

That way in the uncontended success case you just have the intrinsic
CAS and in the uncontended failure case you just have a single
volatile read after the intrinsic CAS. It's only when the CAS
repeatedly fails even though the read keeps seeing the expected
value that the code continues looping.

I have no opinion on whether it's worthwhile to implement a proper
intrinstic compare-and-swap. For most of the thread you were focused
on AtomicReference rather than Unsafe, so it seemed worth clarifying
that it would be easy, and not unprecedented, to implement it
directly in AtomicReference.java, especially since David actually
said it would be impossible. :) Doing it in Java would also be, as I
understand things, directly within the scope of this particular
working group, whereas getting the intrinsics changed would require
more work by different groups of people. I don't know that for sure,
though, being not much more than a lurker in these parts.

Cheers,
Justin


From davidcholmes at aapt.net.au  Tue Jan 13 01:53:00 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 13 Jan 2015 16:53:00 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1421112526622-11881.post@n7.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEHBKMAA.davidcholmes@aapt.net.au>

thurstonn writes:
> Sent: Tuesday, 13 January 2015 11:29 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] AtomicReference CAS signatures
>
>
> Justin Sampson wrote
> > thurstonn wrote:
> >
> >
> > The easy implementation was right there in my email:
> >
> > T compareAndSwap(T expect, T update) {
> >     for (;;) {
> >         T current = get();
> >         if (current != expect || compareAndSet(expect, update))
> >             return current;
> >     }
> > }
>
> There are two large problems with the above:
> 1.  First and foremost, it is unnecessarily slow - in the
> "success" case it
> does a minimum 2 loads, while at least on some hardware platforms
> (x86), the
> native implementation need only do a single load for both success and
> failure; that's unacceptable.
> 2.  In worst-case, the above is O(infinity) - it can loop forever.
>
>
> And as Andrew and you have pointed out, the assembly code for the extant
> native boolean compareAndSet(...) can be entirely duplicated for the
> proposed new CAS *on all platforms, including LL/SC based ones*, i.e. it
> does the exact same work (therefore with the same performance
> characteristics), except that one returns a boolean and the other
> returns a
> reference/pointer.
>
>
> Obviously, anyone can write the Java implementation above
> (although I would
> probably prefer to give up the "atomicity", for a bounded
> worst-case), i.e.
>
> T compareAndSwap(T expect, T update)
> {
>
>
>         if (compareAndSet(expect, update))
>             return expect;
>         else
>             return get()
> }

The above implementation is broken because it could return "expected" after
a failure, and that would indicate success even though the value was not
updated. Justin's version avoids that bug.

David
-----


>
> but regardless, that's a poor substitute for the native version
> (which does
> not require any such compromise).
>
> You did an excellent job summarizing the discussion (and hopefully the
> consensus?), except I didn't want the conclusion to be that the native T
> cas(T, T) could be ignored, which I don't even think you were advocating,
> but muddied the waters unnecessarily.
>
>
>
>
>
> --
> View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures
-tp11820p11881.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Tue Jan 13 02:16:55 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 13 Jan 2015 00:16:55 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
	<1421112526622-11881.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
Message-ID: <1421133415458-11887.post@n7.nabble.com>

AtomicReference getAndSet and compareAndSet (among many other methods) are
just adapters around Unsafe native cas-methods.


To get the optimal performance out of the JVM and hardware, compareAndSwap
needs to be native, just like getAndSet and compareAndSet already are.




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11887.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From kirk at kodewerk.com  Tue Jan 13 03:28:57 2015
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 13 Jan 2015 09:28:57 +0100
Subject: [concurrency-interest] Unexplained ForkJoinTask behavior
Message-ID: <8CDE0178-F43F-4375-A7F4-F283C6CF8CE6@kodewerk.com>

Hi,

I?ve been instrumenting ForkJoinPool along with ForkJoinTask. I have two different workloads that are being run sequentially. Lets call them W1 and W2.

If I run in the order W1,W2 what I?ve noticed is that the call to private int setCompletion(int completion) is being made twice by two different ForkJoinWorkerThread for W1 and once for W2. When I say two calls, what I really mean is the status word is reset to the same NORMAL completion value.

The casual stack for both W1 and W2 is;
setCompletion():284, ForkJoinTask, returns success of the call
quietlyComplete():999, ForkJoinTalk, returns void
tryComplete():579, CountedCompleter, returns void
compute():317, AbstractTask, returns void
exec():731, CountedCompleter, always returns false
doExec():309, ForkJoinTask, return status word
runTask():902, ForkJoinTask$WorkQueue, 
scan():1690, ForkJoinPool
runWorker():1645, ForkJoinPool
run():157 ForkJoinWorkerThread

The second call always originates from doExec() as follows

setCompletion():284
doExec():314, ForkJoinTask, return status word
runTask():902, ForkJoinTask$WorkQueue, 
scan():1690, ForkJoinPool
runWorker():1645, ForkJoinPool
run():157 ForkJoinWorkerThread

So I ran a few experiments.

1) run order -> W1,W1,W2. The first call to W1 resulted in the two calls to setCompletion whereas the second call resulted in only a single call. W2 results in a single call.

2) run order ->W2,W1. W2 makes two calls to setCompletion sometimes.

So, I can?t say that I fully understand what is happening but the whole things smells a bit racy. I would have expected the second thread to see the status word and abort it?s call to setCompletion. That it does set the status word doesn?t appear to be harmful in any meaningful makes me wonder if this race (assuming it is a race) is known and intentional. Maybe someone with a better understanding can comment?

Output from the first run.

setCompletion(11110000000000000000000000000000)
called by: java.util.concurrent.ForkJoinWorkerThread::ForkJoinPool.commonPool-worker-7
Status word    : 10000000000000000
setCompletion(11110000000000000000000000000000)
Called by: java.util.concurrent.ForkJoinWorkerThread::ForkJoinPool.commonPool-worker-1
Status word    : 10000000000000000
Results?..
Concurrent Stats             : DoubleSummaryStatistics{count=6836890, sum=25379822.900800, min=0.000020, average=3.712188, max=48.941529}
Submitted Tasks             : 1
number of tasks retired   : 2
Run time (server)            : 12508.4665035 ms
Concurrent Time (client) : 12555.359548 ms
----------------------------------------------------------------

setCompletion(11110000000000000000000000000000)
called by: java.util.concurrent.ForkJoinWorkerThread::ForkJoinPool.commonPool-worker-7
Status word    : 10000000000000000
Results?..
Stopped Stats                : DoubleSummaryStatistics{count=6834790, sum=50802.064404, min=0.000048, average=0.007433, max=17.487807}
Tasks                             : 1
number of tasks retired : 1
Run time (server)          : 7410.314671 ms
Stopped Time (client)    : 7412.75453 ms
????????????????????????????????

Regards,
Kirk
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/c01e4730/attachment-0001.bin>

From dl at cs.oswego.edu  Tue Jan 13 07:39:23 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 13 Jan 2015 07:39:23 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D822BE@sm-ex-01-vm.guidewire.com>
References: <NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<54B47230.5090405@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D822BE@sm-ex-01-vm.guidewire.com>
Message-ID: <54B511FB.9080103@cs.oswego.edu>

On 01/13/2015 01:01 AM, Justin Sampson wrote:
> Actually, looking again at the class documentation for LockSupport,
> I see in the very first sentence that the "permit" is described as
> being "in the sense of the Semaphore class." The only difference
> mentioned is that "permits do not accumulate." Was that intended to
> imply the same memory consistency effects as Semaphore?

Yes, which is why I thought that adding memory-semantics
clauses seemed unlikely to enhance understanding. But apparently
it helped at least a little, so it's worth adding a sentence:


  * <p>This class associates, with each thread that uses it, a permit
  * (in the sense of the {@link java.util.concurrent.Semaphore
  * Semaphore} class). A call to {@code park} will return immediately
  * if the permit is available, consuming it in the process; otherwise
  * it <em>may</em> block.  A call to {@code unpark} makes the permit
  * available, if it was not already available. (Unlike with Semaphores
  * though, permits do not accumulate. There is at most one.)  The
  * memory ordering semantics of {@code park} are the same as those of
  * a volatile read and write to a variable also written by any caller
!  * though, permits do not accumulate. There is at most one.)  The
!  * memory ordering semantics of {@code park} are the same as those of
!  * a volatile read and write to a variable also written by any caller
!  * of {@code unpark} for that thread.
    *



From dl at cs.oswego.edu  Tue Jan 13 07:56:46 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 13 Jan 2015 07:56:46 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1421133415458-11887.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>	<1421100709820-11866.post@n7.nabble.com>	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>	<1421112526622-11881.post@n7.nabble.com>	<0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
	<1421133415458-11887.post@n7.nabble.com>
Message-ID: <54B5160E.4020009@cs.oswego.edu>

On 01/13/2015 02:16 AM, thurstonn wrote:
> AtomicReference getAndSet and compareAndSet (among many other methods) are
> just adapters around Unsafe native cas-methods.
> To get the optimal performance out of the JVM and hardware, compareAndSwap
> needs to be native, just like getAndSet and compareAndSet already are.

Adding an atomic intrinsic that must be mapped to each supported
processor is not easy. I encourage you to do this by creating an
OpenJDK branch, and if successful, proposing to integrate
(which will need approval because it obligates all ports to
all processors  to support it as well). My offhand estimate
is that this will take about 1000 hours if you haven't done this
kind of thing before. If there were a more compelling case for
adding it, you might be able to enlist some help.

-Doug



From dl at cs.oswego.edu  Tue Jan 13 08:10:05 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 13 Jan 2015 08:10:05 -0500
Subject: [concurrency-interest] Unexplained ForkJoinTask behavior
In-Reply-To: <8CDE0178-F43F-4375-A7F4-F283C6CF8CE6@kodewerk.com>
References: <8CDE0178-F43F-4375-A7F4-F283C6CF8CE6@kodewerk.com>
Message-ID: <54B5192D.8090704@cs.oswego.edu>

On 01/13/2015 03:28 AM, Kirk Pepperdine wrote:
> Hi,
>
> I?ve been instrumenting ForkJoinPool along with ForkJoinTask. I have two
> different workloads that are being run sequentially. Lets call them W1 and
> W2.
>
> If I run in the order W1,W2 what I?ve noticed is that the call to private int
> setCompletion(int completion) is being made twice by two different
> ForkJoinWorkerThread for W1 and once for W2. When I say two calls, what I
> really mean is the status word is reset to the same NORMAL completion value.

Yes, multiple attempts to normally complete are allowed to write.
(It avoids a needless CAS.) Thanks for pointing out that this
doesn't seem to be stated currently in internal documentation,
that will be improved. (I think that a mention of this was
inadvertently omitted during an update.)

-Doug



From dl at cs.oswego.edu  Tue Jan 13 08:26:52 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 13 Jan 2015 08:26:52 -0500
Subject: [concurrency-interest] CHM: removing during a computeIfAbsent
In-Reply-To: <1113916303.390025.1421023892657.JavaMail.yahoo@jws100122.mail.ne1.yahoo.com>
References: <1113916303.390025.1421023892657.JavaMail.yahoo@jws100122.mail.ne1.yahoo.com>
Message-ID: <54B51D1C.4020301@cs.oswego.edu>

On 01/11/2015 07:51 PM, Ben Manes wrote:
> When an absent value is being computed, this operation blocks a removal by the
> same key and clearing the map. This is interesting because a get(), contains(),
> and iterators do not block, so it could be argued that the values are not
> considered present yet. If that is agreed to then by the wording of the JavaDoc
> on remove() could return false immediately and clear() could skip those entries.
> Was this considered and, if so, why was that optimization rejected?

Basically, it is another consequence of not having a fast tryLock for
builtin sync. Without this, the most common cases would require
more overhead. (And even with it, adding extra checks and code-paths
is an uncertain tradeoff.)


-Doug




From ben_manes at yahoo.com  Tue Jan 13 09:06:36 2015
From: ben_manes at yahoo.com (Ben Manes)
Date: Tue, 13 Jan 2015 14:06:36 +0000 (UTC)
Subject: [concurrency-interest] CHM: removing during a computeIfAbsent
In-Reply-To: <54B51D1C.4020301@cs.oswego.edu>
References: <54B51D1C.4020301@cs.oswego.edu>
Message-ID: <724579153.229912.1421157996470.JavaMail.yahoo@jws10053.mail.ne1.yahoo.com>

I was thinking that it could be done using a `if (node instanceOf ReservationNode) ...` check and probably a flag to replaceNode indicating that a fast fail is acceptable. I thought something similar might work on clear(). If that's valid then the overhead is small, but I agree it may not be that common.
In the caching case, I found an open bug in Guava indicating that some users wanted the blocking style (vs the clobbering approach implemented), so which approach is desired seems open for debate anyway. 

     On Tuesday, January 13, 2015 5:51 AM, Doug Lea <dl at cs.oswego.edu> wrote:
   

 On 01/11/2015 07:51 PM, Ben Manes wrote:
> When an absent value is being computed, this operation blocks a removal by the
> same key and clearing the map. This is interesting because a get(), contains(),
> and iterators do not block, so it could be argued that the values are not
> considered present yet. If that is agreed to then by the wording of the JavaDoc
> on remove() could return false immediately and clear() could skip those entries.
> Was this considered and, if so, why was that optimization rejected?

Basically, it is another consequence of not having a fast tryLock for
builtin sync. Without this, the most common cases would require
more overhead. (And even with it, adding extra checks and code-paths
is an uncertain tradeoff.)


-Doug



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


    
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/172813e7/attachment.html>

From vitalyd at gmail.com  Tue Jan 13 09:53:22 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 13 Jan 2015 09:53:22 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37GbksRBQDrDJh5PzKQv63v_fLYJuzWNOcFpmsKbX63CvQ@mail.gmail.com>

Everything is up for grabs pretty much, unless you have data dependence or
something else to prevent reordering of data independent operations (e.g.
JMM constructs).  Unpark () is effectively a signaling construct, and you
want to ensure, e.g., that all stores you made prior to unparking a thread
will be visible to that thread.

Sent from my phone
On Jan 13, 2015 1:50 AM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> Justin Sampson writes:
> > David Holmes wrote:
> >
> > > Are you saying that you think any pair of statements can be
> > > reordered unless there is something in the JMM that says they
> > > can't?
> >
> > Yes, I was under the impression that was a good way to think about
> > reordering!
>
> For memory accesses perhaps, but not for arbitrary code! How can you
> possibly know what can and can't be reordered if everything is up for grabs
> ???
>
> > > E.g.
> > >
> > > canvas.setFillRegion(x1,x2, y1, y2);
> > > canvas.fillRegion(Color.BLUE);
> > >
> > > might be reordered to:
> > >
> > > canvas.fillRegion(Color.BLUE);
> > > canvas.setFillRegion(x1,x2, y1, y2);
> >
> > Doesn't that depend on what those methods actually do? Presumably
> > the fillRegion method reads some memory locations that were set by
> > the setFillRegion method, such that the JMM requires that they
> > _appear_ to execute in that order _to the current thread_, but not
> > necessarily to any other threads without proper synchronization.
>
> So you're happy to rely on program order to prevent reordering.
>
> > > or:
> > >
> > > database.commit();
> > > data_commited=true; // volatile
> > >
> > > might be reordered to:
> > >
> > > data_commited=true; // volatile
> > > database.commit();
> >
> > If database.commit() performs any memory writes whatsoever, doesn't
> > the JMM forbid reordering those writes after the volatile write?
>
> Let's assume that is true. So you are happy to rely on the above code being
> kept in the order specified as long as commit() contains at least one
> write** - no need for the JMM to mention the commit() method. Okay. Now
> replace commit() with unpark() - what's the difference?
>
> ** I'll ignore how you would know in general if indeed commit() did contain
> a write!
>
> David
> -----
>
> > > ??
> >
> > Ditto!
> >
> > Thanks,
> > Justin
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/66e3c5c5/attachment.html>

From thurston at nomagicsoftware.com  Tue Jan 13 09:53:20 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 13 Jan 2015 07:53:20 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B5160E.4020009@cs.oswego.edu>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
	<1421112526622-11881.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
	<1421133415458-11887.post@n7.nabble.com>
	<54B5160E.4020009@cs.oswego.edu>
Message-ID: <1421160800828-11894.post@n7.nabble.com>

Well, that's obviously not practicable, so is an argument for status quo.

I guess I have to hope that it finds its way into the jdk9 refactoring of
Unsafe into a more accessible class;
are the CAS methods part of that?

At least as far as the pure implementation part of it (which is only a tiny
% of it when it comes to the jdk), I would think that this is as low
overhead as could be possible, only because the mind work has already been
done in implementing compareAndSet.






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11894.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Tue Jan 13 10:20:25 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 13 Jan 2015 08:20:25 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37GbksRBQDrDJh5PzKQv63v_fLYJuzWNOcFpmsKbX63CvQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>
	<1421023216623-11851.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
	<NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
	<CAHjP37GbksRBQDrDJh5PzKQv63v_fLYJuzWNOcFpmsKbX63CvQ@mail.gmail.com>
Message-ID: <1421162425107-11896.post@n7.nabble.com>

Right.

Off topic,
but David's example does bring up an interesting question, that doesn't seem
to be addressed directly by the JMM.


Say you have something like:


webservice.send(request);
sent = true; //volatile write


and the jvm can determine that webservice.send doesn't do any writes or
reads of shared memory (just writes local data to a socket).

Could webservice.send be reordered after the write to #sent?  I always think
of it as nothing can be reordered after a volatile write (the jsr cookbook
is all NOs in its table), but does 'nothing' include like computing a
fibonacci sequence?


And if there was a 2nd thread doing:

while (! sent)
.. .

Gotta believe the JVM would be very conservative here.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11896.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Tue Jan 13 10:53:10 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 13 Jan 2015 10:53:10 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421162425107-11896.post@n7.nabble.com>
References: <NFBBKALFDCPFIDBNKAPCEEGCKMAA.davidcholmes@aapt.net.au>
	<1421023216623-11851.post@n7.nabble.com>
	<NFBBKALFDCPFIDBNKAPCEEGDKMAA.davidcholmes@aapt.net.au>
	<54B3E1B9.1040603@oracle.com>
	<NFBBKALFDCPFIDBNKAPCIEGJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
	<CAHjP37GbksRBQDrDJh5PzKQv63v_fLYJuzWNOcFpmsKbX63CvQ@mail.gmail.com>
	<1421162425107-11896.post@n7.nabble.com>
Message-ID: <CAHjP37EfGK-H3rMzjAVqySJpLfke1oLx2nxNZyGf1LSQMBBSaA@mail.gmail.com>

Compilers routinely move code around when they can prove that it doesn't
change intra-thread semantics (i.e. single thread of execution cannot tell
the difference) as a matter of optimization.  The JMM/JSR cookbook talk
about reordering with respect to memory accesses (and the type, store/load,
volatile/plain).  In your theoretical example where the JVM can prove that
send() doesn't touch anything (which is practically impossible), then it
could move the code around.  Your 2nd thread is spinning on "sent", but if
send() didn't actually write any shared memory, then effectively those two
calls are independent and there's no problem.  The issue only comes up when
you have independent memory being used for inter-thread communication;
compiler/cpu cannot see that these things have some semantic meaning, and
so thus may reorder unless told otherwise.

On Tue, Jan 13, 2015 at 10:20 AM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> Right.
>
> Off topic,
> but David's example does bring up an interesting question, that doesn't
> seem
> to be addressed directly by the JMM.
>
>
> Say you have something like:
>
>
> webservice.send(request);
> sent = true; //volatile write
>
>
> and the jvm can determine that webservice.send doesn't do any writes or
> reads of shared memory (just writes local data to a socket).
>
> Could webservice.send be reordered after the write to #sent?  I always
> think
> of it as nothing can be reordered after a volatile write (the jsr cookbook
> is all NOs in its table), but does 'nothing' include like computing a
> fibonacci sequence?
>
>
> And if there was a 2nd thread doing:
>
> while (! sent)
> .. .
>
> Gotta believe the JVM would be very conservative here.
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11896.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150113/a064e622/attachment.html>

From jsampson at guidewire.com  Tue Jan 13 12:17:28 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 13 Jan 2015 17:17:28 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81C3D@sm-ex-01-vm.guidewire.com>
	<1421100709820-11866.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F3B@sm-ex-01-vm.guidewire.com>
	<1421112526622-11881.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D822F9@sm-ex-01-vm.guidewire.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D82453@sm-ex-01-vm.guidewire.com>

I wrote:

> I suppose if the successful path is expected to be more common it
> could be optimized:
>
> T compareAndSwap(T expect, T update) {
>     if (compareAndSet(expect, update))
>         return expect;
>     for (;;) {
>         T current = get();
>         if (current != expect || compareAndSet(expect, update))
>             return current;
>     }
> }
>
> That way in the uncontended success case you just have the
> intrinsic CAS and in the uncontended failure case you just have a
> single volatile read after the intrinsic CAS. It's only when the
> CAS repeatedly fails even though the read keeps seeing the
> expected value that the code continues looping.

After sleeping on it, I realize that this optimized version can be
written much more clearly:

T compareAndSwap(T expect, T update) {
    T current = expect;
    while (current == expect && !compareAndSet(expect, update))
        current = get();
    return current;
}

I'm tempted to prepare a patch, but if everyone else thinks that
either (a) compare-and-swap isn't actually useful or (b) it's only
useful if intrinsic, then I suppose it's moot.

Cheers,
Justin


From jsampson at guidewire.com  Tue Jan 13 12:44:35 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 13 Jan 2015 17:44:35 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>

David, I'm curious if you at least agree with Doug that there is, in
fact, a happens-before edge from unpark() to park() in all current
implementations, even if you still disagree that it's necessary.

Responses continue below:

> So you're happy to rely on program order to prevent reordering.

For single-threaded code, of course! Not for multi-threaded code.

> > If database.commit() performs any memory writes whatsoever,
> > doesn't the JMM forbid reordering those writes after the
> > volatile write?
>
> Let's assume that is true. So you are happy to rely on the above
> code being kept in the order specified as long as commit()
> contains at least one write** - no need for the JMM to mention the
> commit() method. Okay. Now replace commit() with unpark() - what's
> the difference?

The difference isn't commit() vs. unpark(), it's the original order.
For most of this thread we've been debating a program snippet that
has a volatile write _before_ the unpark() call (in program order).
The example you just raised has a volatile write _after_ the
commit() call. My admittedly-novice understanding is that volatile
writes generally prevent moving stuff from before them to after
them, but allow moving stuff from after them to before them (where
I'm being intentionally vague about "stuff" so as not to look up the
exact definition -- but it definitely includes regular memory
writes).

> ** I'll ignore how you would know in general if indeed commit()
> did contain a write!

If it doesn't contain any writes, it can't affect any other threads,
and reordering is irrelevant!

Cheers,
Justin


From oleksandr.otenko at oracle.com  Tue Jan 13 12:55:47 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 13 Jan 2015 17:55:47 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B55C23.5050705@oracle.com>

Yes. If you want to use commit() to control visibility of other writes, 
then you do need to know whether commit() contains a volatile write 
inside, and what it will synchronize-with. Other than that the guarantee 
of program order for one thread is lax enough.

Alex

On 13/01/2015 06:27, David Holmes wrote:
> Justin Sampson writes:
>> David Holmes wrote:
>>
>>> Are you saying that you think any pair of statements can be
>>> reordered unless there is something in the JMM that says they
>>> can't?
>> Yes, I was under the impression that was a good way to think about
>> reordering!
> For memory accesses perhaps, but not for arbitrary code! How can you
> possibly know what can and can't be reordered if everything is up for grabs
> ???
>
>>> E.g.
>>>
>>> canvas.setFillRegion(x1,x2, y1, y2);
>>> canvas.fillRegion(Color.BLUE);
>>>
>>> might be reordered to:
>>>
>>> canvas.fillRegion(Color.BLUE);
>>> canvas.setFillRegion(x1,x2, y1, y2);
>> Doesn't that depend on what those methods actually do? Presumably
>> the fillRegion method reads some memory locations that were set by
>> the setFillRegion method, such that the JMM requires that they
>> _appear_ to execute in that order _to the current thread_, but not
>> necessarily to any other threads without proper synchronization.
> So you're happy to rely on program order to prevent reordering.
>
>>> or:
>>>
>>> database.commit();
>>> data_commited=true; // volatile
>>>
>>> might be reordered to:
>>>
>>> data_commited=true; // volatile
>>> database.commit();
>> If database.commit() performs any memory writes whatsoever, doesn't
>> the JMM forbid reordering those writes after the volatile write?
> Let's assume that is true. So you are happy to rely on the above code being
> kept in the order specified as long as commit() contains at least one
> write** - no need for the JMM to mention the commit() method. Okay. Now
> replace commit() with unpark() - what's the difference?
>
> ** I'll ignore how you would know in general if indeed commit() did contain
> a write!
>
> David
> -----
>
>>> ??
>> Ditto!
>>
>> Thanks,
>> Justin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dt at flyingtroika.com  Tue Jan 13 13:37:57 2015
From: dt at flyingtroika.com (DT)
Date: Tue, 13 Jan 2015 10:37:57 -0800
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
Message-ID: <B35F834F-8079-4BCC-B2BA-8D0931CE08EB@flyingtroika.com>

I am not sure if I can compare LockSupport.park / unpark and ReentrankLock.lock/ unlock semantics. Though is there any reason why LockSupport does not have tryPark ? I am trying to get some common background rules where LockSupport should be used based on current thread.

Sent from my iPhone

> On Jan 10, 2015, at 1:43 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
> 
> Not sure what you mean by "user-level" - it's all Java code. park/unpark are
> low-level API's only normally used via higher-level concurrency abstractions
> which also contain the volatile state variable(s) - eg
> AbstractQueuedSynchronizer etc.
> 
> David
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>> thurstonn
>> Sent: Sunday, 11 January 2015 7:24 AM
>> To: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] unpark/park memory visibility
>> 
>> 
>> David Holmes-6 wrote
>>> Interruption is a top-level Thread API so it was decided not
>> unreasonable
>>> to
>>> have it behave as-if there were a "volatile boolean
>> interrupted" field in
>>> Thread. Hence the HB relation.
>>> 
>>> Park/unpark is the lowest-level of API and in
>> *
>>> 99.99% of cases will be used
>>> in conjunction with volatile state variables
>> *
>>> , which provide the HB
>>> relations. It would have been an excessive burden to require HB for the
>>> unpark/park themselves.
>>> 
>>> David
>> 
>> Thanks.
>> Just so I understand, you mean **user-level code** will pair unparks/parks
>> with volatile variables, right?
>> 
>> 
>> 
>> --
>> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
> tp11812p11842.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Tue Jan 13 17:01:28 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 13 Jan 2015 22:01:28 +0000
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
In-Reply-To: <0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>	<5473166A.7030904@gmail.com>	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>	<5474AC98.5020106@gmail.com>	<CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>
	<0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>
Message-ID: <54B595B8.90601@oracle.com>

I guess the important thing in "looking under the hood" is to find a 
higher-order construct that things correspond to.

Wouldn't it be cool to have a higher-order concurrency primitive, which 
represents a particular synchronization protocol - a particular graph of 
synchronizes-with - which one would only need to populate with 
program-order bits.

It doesn't mean it's what "the users" would get. But even for the 
developer of "lower-order" primitives it would be helpful not having to 
prove the correctness.

Alex


On 25/11/2014 21:38, Gil Tene wrote:
> Peter, Pedro,
>
> I could draw a similar equivalence between a ReadWriteLock and a counting Semaphore. After all, you can implement the ReadWriteLock-equivalent "pattern" directly using the APIs of a counting semaphore that supports a P(n) operation (e.g. Java's Semaphore). I could draw a translation table for that too. So maybe that means ReadWriteLock is not a separate synchronization primitive? After all, if we can map the calls in some way, it is simply equivalent to and a variant of a semaphore, isn't it?
>
> The reason that's not the case is that ReadWriteLock describes expected use assumptions and provides guarantees that are met when those use assumptions are followed, and those are useful for users that do not want to reason about generic semaphores when what they need is a ReadWriteLock. It is also not the case because ReadWriteLock can be implemented using something other than a semaphore, and still meet it's required behaviors.
>
> The same is true for WriterReaderPhaser and Left-Right. While WriterReaderPhaser *could* be implemented using parts of Left-Right, it does not present to it's user as Left-Right any more than it presents as a common phased epoch pair (which would be simpler). It describes expected use assumptions and provides guarantees that are met when those use assumptions are followed. And those are useful for users that do not want to reason about generic phased epochs or inside-out Left-Right parts. Furthermore, using Left-Right under the hood is not the preferred implementation of WriterReaderPhaser - it doesn't need the extra complexity and levels of internal abstraction. It certainly doesn't need the internally tracked data structure or the required double-write rules that come with the prescribed use of Left-Right.
>
> The equivalence that you try to draw between Left-Right and WriterReaderPhaser is based on looking under the hood of specific implementations and trying to deduce (unstated) guarantees from each, and new valid ways of using the construct that contradict it's declared and prescribed use (but may still be "ok" if you know what the implementation actually does). A user that tries to do that may as well make direct use of epoch counters and skip the higher level abstractions. Mapping the calls with a translation table does not map the guarantees and use rules. It also doesn't erase the "now we are doing something the instructions tell us to never do" problem either. You'd have to list a new set of expected use assumptions and guarantees provided by the calls (when only those new use assumptions are followed) to make the mapping useful. And by the time you've added that as an alternative valid use Left-right under certain rules (which would contradict it's currently stated use rules, like the requirement to write the same data twice on both sides of a toggle), you'll probably find that you've documented a new and orthogonal use case that is basically WriterReaderPhaser.
>
> For an example of how far you have to stretch to isolate Left-Right's desired parts from it's required use rules, take a look at how convoluted the lambda expression (in your getCounters() example, the one that is passed into LeftRight.modify) ends up being, just so that it can break the rules "just right". The lambda-friendly Left-Right API clearly expects you to perform the same modification operation twice (once on each or the left and right data structures). That's what the lambda expression is for: to avoid having you actually write the modification code twice. But your lambda expression carefully checks to see which side it was asked to work on, and performs a very different operation on each of the "sides". That's quite clever, but is exactly what Left-Right tells you not to do. It works because you've followed the code down to the bottom, and you know how it's being used, and in what order. This extra logic also shows you why thinking of this as "Left-Right protecting the active pointer" doesn't work in your example. Had that been the use case, you would have been able to validly update the "active pointer" to the same value in both calls into the lambda expression (which would obviously break the required WriterReaderPhaser behavior).
>
> Rather than go for such a convoluted application of Left-Right, below is simpler way to implement the same double buffered counts thing. This one is neither Left-Right nor WriterReaderPhaser. It's just direct use of phased epochs with no other primitives involved (unless someone wants to claim that any use of phased epochs is a "variant" of Left-Right, which would be amusing). This direct use of epochs works. It is correct. I've used this or variants of it many times myself. But it requires the author to reason about why this stuff actually works each time, and to carefully examine and protect their logic against concurrency effects on the epochs. It is missing a basic primitive that would provide well stated guarantees and would save the work (and bugs) involved in reasoning through this each time. WriterReaderPhaser captures the API and associated behavior expectations and guarantees. That's what the primitive is all about.
>
> -------------------------------------------
> Direct use of phased epochs:
>
> public class DoubleBufferedCountsUsingEpochs {
>      private AtomicLong startEpoch = new AtomicLong(0);
>      private AtomicLong evenEndEpoch = new AtomicLong(0);
>      private AtomicLong oddEndEpoch = new AtomicLong(Long.MIN_VALUE);
>
>      private long oddCounts[];
>      private long evenCounts[];
>
>      private final long accumulatedCounts[];
>
>      public DoubleBufferedCountsUsingEpochs(int size) {
>          oddCounts = new long[size];
>          evenCounts = new long[size];
>          accumulatedCounts = new long[size];
>      }
>
>      public void incrementCount(int iTh) {
>          boolean phaseIsOdd = (startEpoch.getAndIncrement() < 0);
>          if (phaseIsOdd) {
>              oddCounts[iTh]++;
>              oddEndEpoch.getAndIncrement();
>          } else {
>              evenCounts[iTh]++;
>              evenEndEpoch.getAndIncrement();
>          }
>      }
>
>      public synchronized long[] getCounts() {
>          long sourceArray[];
>          long startValueAtFlip;
>
>          // Clear currently unused [next] phase end epoch and set new startEpoch value:
>          boolean nextPhaseIsEven = (startEpoch.get() < 0); // Current phase is odd...
>          if (nextPhaseIsEven) {
>              evenEndEpoch.set(0);
>              startValueAtFlip = startEpoch.getAndSet(0);
>              sourceArray = oddCounts;
>          } else {
>              oddEndEpoch.set(Long.MIN_VALUE);
>              startValueAtFlip = startEpoch.getAndSet(Long.MIN_VALUE);
>              sourceArray = evenCounts;
>          }
>
>          // Spin until previous phase end epoch value catches up with start value at flip:
>          while ((nextPhaseIsEven && (oddEndEpoch.get() != startValueAtFlip)) ||
>                  (!nextPhaseIsEven && (evenEndEpoch.get() != startValueAtFlip))) {
>              Thread.yield();
>          }
>
>          // sourceArray is stable. Use it:
>          for (int i = 0; i < sourceArray.length; i++) {
>              accumulatedCounts[i] += sourceArray[i];
>              sourceArray[i] = 0;
>          }
>
>          return accumulatedCounts.clone();
>      }
> }
>
>
> Yes. You can use Left-Right per your description below (for LRCounters) with the strange rule-breaking lambda expression, and you can use this simple phased epoch approach above (or many other variants). But *do you want to*? Does using it make it easier or harder to reason about? To me (and to most people, I think) the direct use of epochs is much more readable and easier to reason about for double buffered case than using Left-Right while standing on your head (using it to protect an internal fields, where all roles are reversed from their documented meanings). But neither one of them relieves me of the need to figure out and derive the concurrency behavior for myself, leaving me with unneeded effort and lots of potential bug tails. Much like a ReaderWriterLock saves unneeded effort, pain and bugs when compared to direct use of counting semaphores for the same purpose, WriterReaderPhaser save this effort and pain for double buffered uses looking for wait free writers.
>
> The code below is not only easier to read, it is much easier to *write*, design, and reason about. The author simply follows the recipe spelled out in the WriterReaderPhaser JavaDoc. By carefully following the rules (and without needing to understand why and how) the author knows that the access patterns are guaranteed to be safe.
>
> ----------------------------------
> Direct use of WriterReaderPhaser:
>
> public class DoubleBufferedCountsUsingWRP {
>      WriterReaderPhaser phaser = new WriterReaderPhaser();
>      private long activeCounts[];
>      private long inactiveCounts[];
>
>      private final long accumulatedCounts[];
>
>      public DoubleBufferedCountsUsingWRP(int size) {
>          activeCounts = new long[size];
>          inactiveCounts = new long[size];
>          accumulatedCounts = new long[size];
>      }
>
>      public void incrementCount(int iTh) {
>          long criticalValue = phaser.writerCriticalSectionEnter();
>          try {
>              activeCounts[iTh]++;
>          } finally {
>              phaser.writerCriticalSectionExit(criticalValue);
>          }
>      }
>
>      public synchronized long[] getCounts() {
>          try {
>              phaser.readerLock();
>
>              // switch active and inactive data structures:
>              long tmp[] = activeCounts;
>              activeCounts = inactiveCounts;
>              inactiveCounts = tmp;
>
>              // flip WriterReaderPhaser phase:
>              phaser.flipPhase();
>
>              // use stable (newly inactive) data:
>              for (int i = 0; i < inactiveCounts.length; i++) {
>                  accumulatedCounts[i] += inactiveCounts[i];
>                  inactiveCounts[i] = 0;
>              }
>          } finally {
>              phaser.readerUnlock();
>          }
>          return accumulatedCounts.clone();
>      }
> }
>
>> On Nov 25, 2014, at 8:52 AM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
>>
>> Yes Peter, it makes absolute sense!
>> To make it completely clear just "how equivalent" these two methods are, let me add a "translation table" for the APIs, from WriterReaderPhaser to Left-Right:
>>          ? writerCriticalSectionEnter   -> readIndicator.arrive()
>>          ? writerCriticalSectionExit      -> readIndicator.depart()
>>          ? readerLock                          -> writersMutex.lock()
>>          ? readerUnlock                      -> writersMutex.unlock()
>>          ? flipPhase                             -> toggleVersionAndScan()
>>
>> Thanks,
>> Pedro
>>
>> On Tue, Nov 25, 2014 at 5:21 PM, Peter Levart <peter.levart at gmail.com> wrote:
>> Hi Gil,
>>
>> I think Pedro is right in claiming that WriteReaderPhaser is a kind of Left-Right, but he's wrong in explaining that it is a Left-Right used backwards. In Left-Right terminology, WriteReaderPhaser is not protecting the mutable structure (mutated from multiple threads), but just coordinating access to the "active pointer" to the structure". From Left-Right perspective, multiple threads are just readers of the "active pointer" (what they do with the underlying structure is not relevant here - the underlying structure has it's own synchronization). The single thread (at a time) that wants to get access to the snapshot is the sole writer  (or "swapper") of the "active pointer". Of course, the sole writer or "swapper" of the active pointer also exploits the fact that the "inactive pointer" is not being accessed by any current readers of the "active pointer" and so, the underlying structure is not touched by the "readers".
>>
>> I agree with all your statements below about WriteReaderPhaser and I can see how WriteReaderPhaser API is more suited to the pointer flipping scenarios, but WriteReaderPhaser and Left-Right can be used interchangeably. They are, in a sense equivalent.
>>
>> To illustrate, I'll use the lambda-enabled Left-Right API:
>>
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LeftRight.java
>>
>> ...and try to re-create your example from below:
>>
>> public class LRCounters {
>>
>>      static class ArrayRef {
>>          long[] array;
>>
>>          ArrayRef(long[] array) {
>>              this.array = array;
>>          }
>>      }
>>
>>      private final LeftRight<ArrayRef> counts;
>>      private long intervalCounts[];
>>      private final long accumulatedCounts[];
>>
>>      public LRCounters(int size) {
>>          intervalCounts = new long[size];
>>          accumulatedCounts = new long[size];
>>          counts = new LeftRight<>(
>>              // this is the initially active ArrayRef
>>              new ArrayRef(new long[size]),
>>              // and this is the initially inactive one
>>              new ArrayRef(null)
>>          );
>>      }
>>
>>      public void incrementCount(int iTh) {
>>          counts.read(iTh, (i, arrayRef) -> {
>>              long a[] = arrayRef.array; // this is the read operation
>>              return ++a[i]; // never mind the racy increment (should do it with atomics)
>>          });
>>      }
>>
>>      public long[] getCounts() {
>>          long[][] result = new long[1][];
>>
>>          counts.modify((arrayRef) -> {
>>              if (arrayRef.array == null) {
>>                  // we've got the previously inactive ArrayRef
>>                  arrayRef.array = intervalCounts; // this is the 1st write operation
>>              } else {
>>                  // we've got the previously active ArrayRef
>>                  // that has just been deactivated
>>                  intervalCounts = arrayRef.array;
>>                  arrayRef.array = null; // this is the "mirror" write operation
>>                  // add interval counts to accumulatedCounts
>>                  for (int i = 0; i < intervalCounts.length; i++) {
>>                      accumulatedCounts[i] += intervalCounts[i];
>>                      intervalCounts[i] = 0;
>>                  }
>>                  // return result
>>                  result[0] = accumulatedCounts.clone();
>>              }
>>          });
>>
>>          return result[0];
>>      }
>> }
>>
>>
>> Likewise, let's take an example that is more suited to LeftRight API:
>>
>> public class LRMap<K, V> {
>>
>>      private final LeftRight<Map<K, V>> lrMap = new LeftRight<>(new HashMap<>(), new HashMap<>());
>>
>>      public V get(K key) {
>>          return lrMap.read(m -> m.get(key));
>>      }
>>
>>      public void put(K key, V value) {
>>          lrMap.modify(m -> m.put(key, value));
>>      }
>> }
>>
>> ...and try to implement is using WriteReaderPhaser:
>>
>> public class WRPMap<K, V> {
>>
>>      private final WriterReaderPhaser wrp = new WriterReaderPhaser();
>>      private volatile Map<K, V> activeMap = new HashMap<>();
>>      private volatile Map<K, V> inactiveMap = new HashMap<>();
>>
>>      public V get(K key) {
>>          long stamp = wrp.writerCriticalSectionEnter();
>>          try {
>>              return activeMap.get(key);
>>          } finally {
>>              wrp.writerCriticalSectionExit(stamp);
>>          }
>>      }
>>
>>      public void put(K key, V value) {
>>          wrp.readerLock();
>>          try {
>>              Map<K, V> m1 = inactiveMap;
>>              Map<K, V> m2 = activeMap;
>>              m1.put(key, value); // 1st write to inactive
>>              // swap active <-> inactive
>>              activeMap = m1;
>>              inactiveMap = m2;
>>
>>              wrp.flipPhase();
>>
>>              m2.put(key, value); // mirror write to just deactivated
>>          } finally {
>>              wrp.readerUnlock();
>>          }
>>      }
>> }
>>
>> Does this make any sense?
>>
>> Regards, Peter
>>
>> On 11/25/2014 08:39 AM, Gil Tene wrote:
>> Pedro, I think you are confusing specific under-the-hood implementation choices (which are similar) with what the primitive is. I'm flattered at your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT example) is a Left-Right variant with a different underlying (attributed to me) arrive-depart technique. It is not a WriterReaderPhaser.
>>
>> WriterReaderPhaser captures (in a clean synchronization primitive API form) a pattern I've had to use and re-invent myself several times, and I'm pretty sure many others that have faced the "I want to periodically report/analyze an actively updating data set" have too. The key here is capturing the guarantees and prescribed use such that end-users can use the primitive without needing to understand the underlying logic of an implementation. I do that in my blog entry (and include a definition and use example below).
>>
>> A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used backwards. It's also not Left-Right, or Left-Right used backwards. The qualities and guarantees a WriterReaderPhaser provides are not provided by reversing the meaning of "writer" and "reader" in those primitives. Even if you ignore the confusion that such upside-down use may cause the user, there are specific guarantees that the other primitives do not provide, and that a write-heavy double-buffered use case needs and gets from this primitive.
>>
>> And yes, there are several possible ways to implement a well behaving WriterReaderPhaser, one of which is listed in the links I provided. We can implement it with three atomic words and some clever combos of CAS and GetAndAdd ops, or in other ways. The implementation is not what makes the primitive what it is to it's users. It's the API and the behavior guarantees that do that. And I'm pretty sure these behavior guarantees are not spelled out or provided (even backwards) in Left-Right and variants. Some of them (like the data stability guarantee for readers even in the presence of wait-free write activity) would be un-natural to provide in reverse for writers (since readers are generally not expected to change stuff).
>>
>> Left-Right is cool (really cool), but it focuses purely on wait-free readers and blocking writers. While that use case may appear to be "the opposite" of wait-free writers with blocking readers, there are specific non-mirroring qualities that make that duality invalid. Here are specific differences between the two mechanisms that make "backwards" use inapplicable::
>>
>> - WriterReaderPhaser provides specific data stability guarantees to readers (after a flip while under a readLock), even in the presence of concurrent writer activity. Left-Right does not provide such a guarantee to writers "backwards". E.g. if Left-Right readers happened to write into the Left-Right protected data structure (as they would need to in a "backwards use" attempt like this), Left-Right says nothing about what writers can expect from that data structure in terms of data consistency or stability. Note that I'm not saying that no Left-Right implementation could accidentally provide this behavior without stating it. I'm saying that the Left-Right mechanism, as described and documented in Left-Right paper and the various APIs for it's existing variants makes no such guarantee to the caller, and that a valid Left-Right implementation may or may not provide this behavior. As such, the user cannot rely on it. And this is the main guarantee a typical WriterReaderPhaser user will be looking for.
>>
>> - Left-Right specifically prohibits readers from writing. E.g. "...To access in read-only mode do something like this:..." is stated in the documentation for a LeftRightGuard variants.  In contrast, WriterReaderPhaser allows it's writers (which would be the readers in a backwards mapping attempt) to, um, write...
>>
>> - Writers that use Left-Right are required to to write their updates twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g. "...The exact same operations must be done on the instance before and after guard.writeToggle()." is stated in the documentation for a LeftRightGuard variants. In contrast, WriterReaderPhaser does not require reading twice or writing twice. The two APIs do not mirror each other in this critical aspect.
>>
>> - Left-Right (even when used to replace a Reader-Writer lock) manages the active and inactive data structures internally (leftInstance and rightInstance, or firstInstance and secondInstance), and users of Left-right [must] operate on data structures returned from Left-Right operations. In contrast, WriterReaderPhaser does not manage the active and inactive data structures in any way, leaving that job to readers and writers that operate directly on the shared data structures.
>>
>> To be specific, let me detail what a WriterReaderPhaser is (taken from an updated blog entry that now includes a definition):
>>
>> -----------------------------------------------------------------
>> Definition of WriterReaderPhaser:
>>
>> A WriterReaderPhaser provides a means for wait free writers to coordinate with blocking (but guaranteed forward progress) readers sharing a set of data structures.
>>
>> A WriterReaderPhaser instance provides the following 5 operations:
>>
>>          ? writerCriticalSectionEnter
>>          ? writerCriticalSectionExit
>>          ? readerLock
>>          ? readerUnlock
>>          ? flipPhase
>>
>> When a WriterReaderPhaser  instance is used to protect an active [set of or single] data structure involving [potentially multiple] writers and [potentially multiple] readers , the assumptions on how readers and writers act are:
>>
>>          ? There are two sets of data structures (an "active" set and an "inactive" set)
>>          ? Writing is done to the perceived active version (as perceived by the writer), and only within critical sections delineated by writerCriticalSectionEnter and writerCriticalSectionExit operations.
>>          ? Only readers switch the perceived roles of the active and inactive data structures. They do so only while holding the readerLock, and only before execution a flipPhase.
>>
>>          ? Readers do not hold onto readerLock indefinitely. Only readers perform readerLock and readerUnlock.
>>          ? Writers do not remain in their critical sections indefinitely. Only writers perform writerCriticalSectionEnter and writerCriticalSectionExit.
>>          ? Only readers perform flipPhase operations, and only while holding the readerLock.
>>
>> When the above assumptions are met, WriterReaderPhaser guarantees that the inactive data structures are not being modified by any writers while being read while under readerLock protection after a flipPhase operation.
>>
>> The following progress guarantees are provided to writers and readers that adhere to the above stated assumptions:
>>          ? Writers operations (writerCriticalSectionEnter and  writerCriticalSectionExit) are wait free (on architectures that support wait-free atomic increment operations).
>>          ? flipPhase operations are guaranteed to make forward progress, and will only be blocked by writers whose critical sections were entered prior to the start of the reader's flipPhase operation, and have not yet exited their critical sections.
>>          ? readerLock only block for other readers that are holding the readerLock.
>>
>> -----------------------------------------------------------------
>> Example use:
>>
>> Imagine a simple use case where a large set of rapidly updated counter is being modified by writers, and a reader needs to gain access to stable interval samples of those counters for reporting and other analysis purposes.
>>
>> The counters are represented in a volatile array of values (it is the array reference that is volatile, not the value cells within it):
>>
>> volatile long counts[];
>> ...
>>
>> A writer updates a specific count (n) in the set of counters:
>>
>> writerCriticalSectionEnter
>>      counts[n]++;
>> writerCriticalSectionExit
>>
>> A reader gain access to a stable set of counts collected during an interval, reports on it, and accumulates it:
>>
>> long intervalCounts[];
>> long accumulated_counts[];
>>
>> ...
>> readerLock
>>      reset(interval_counts);
>>      long tmp[] = counts;
>>      counts = interval_counts;
>>      interval_counts = tmp;
>> flipPhase
>>      report_interval_counts(interval_counts);
>>      accumulated_counts.add(interval_counts);
>> readerUnlock
>> -----------------------------------------------------------------
>>
>>
>> Bottom line: the above is defines what a WriterReaderPhaser primitive is, and shows a simple example of using it. While I provide an example implementation, many are possible, and I'm sure another 17 will pop up. To avoid wrongly taking credit for this as a new primitive, I'm looking to see if there have been previously described primitives that explicitly provide these (or equivalent) qualities to their users. "Explicitly" being a key word (since no sane user would rely on an accidental implicit behavior of a specific implementation of a primitive that does not actually guarantee the given behavior).
>>
>> -- Gil.
>>
>> On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
>> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain to Gil, the WriterReaderPhaser uses the same concurrency control algorithm as the Left-Right, and as such it is a variant of the Left-Right (used "backwards") that uses a (new) ReadIndicator with a single ingress combined with versionIndex. This variant is not as good for scalability under high contention as the one you yourself have implemented some time ago, with the ReadIndicator of ingress/egress with LongAdders. You're better off using your own implementation, and just do the mutations in lrSet.read() and the read-only operation in lrSet.modify(), but of course, feel free to try both and let us your results ;)
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
>> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers, Pedro
>>
>> On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
>>
>> Hi Gil,
>>
>> What a coincidence. I was thinking of writing something like that myself in past couple of days for a similar purpose. It comes as a gift that you posted this here, thanks!
>>
>> My application is an asynchronous cache store implementation. A distributed cache (Coherence in my case) emits synchronous events when cache is updated from multiple threads. I want to batch updates and do asynchronous persistence in a background thread. Coherence already supports this by itself, but is rather limited in features, so I have to re-create this functionality and add missing features.
>>
>> Regards, Peter
>>
>> On 11/24/2014 06:54 AM, Gil Tene wrote:
>> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think this may actually be a new, generically useful primitive. Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial. See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers). See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home. And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know. ? Gil.
>> _______________________________________________ Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From thurston at nomagicsoftware.com  Tue Jan 13 17:39:19 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 13 Jan 2015 15:39:19 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420840643988-11820.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
Message-ID: <1421188759498-11903.post@n7.nabble.com>

Probably a lost cause, but a more "compelling" example:

refactor of AtomicReference#getAndUpdate

        T tmp, prev = get();
        do
        {
            tmp = prev;
        } while ((prev = compareSwap(prev, f.apply(prev))) != tmp);    
        return prev;

presumably this pattern is common enough to warrant an additional method in
AR (was that a 1K hours too?);


a native compareSwap would presumably shine here,obviously absolute speedup
is a function of # of cas failures



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11903.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From dt at flyingtroika.com  Tue Jan 13 16:37:20 2015
From: dt at flyingtroika.com (DT)
Date: Tue, 13 Jan 2015 13:37:20 -0800
Subject: [concurrency-interest] Happens-Before relation
Message-ID: <AB2D8F2F-3BCA-4BF3-A075-6098FBEF2819@flyingtroika.com>


In the original paper "how to make a correct multiprocess program execute correctly on a multiprocessor" by Leslie Lamport an operation described by relations when every or some event in A precedes every or some event in B. Does it hold for java happens-before relations for volatile and not volatile reads/ writes? 

Thanks.
Dmitry



From davidcholmes at aapt.net.au  Tue Jan 13 19:47:59 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Jan 2015 10:47:59 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>

Justin writes:
>
> David, I'm curious if you at least agree with Doug that there is, in
> fact, a happens-before edge from unpark() to park() in all current
> implementations, even if you still disagree that it's necessary.

This is confusing two different things. The implementation of park/unpark
must work correctly and all current implementations will have actions that
cause happens-before edges, and these are necessary for correctness. What I
have been unsuccessfully trying to make clear is that the implementation
requirements of park/unpark don't have to be expressed in the JMM as a
property that forms part of the programming model.

> Responses continue below:
>
> > So you're happy to rely on program order to prevent reordering.
>
> For single-threaded code, of course! Not for multi-threaded code.
>
> > > If database.commit() performs any memory writes whatsoever,
> > > doesn't the JMM forbid reordering those writes after the
> > > volatile write?
> >
> > Let's assume that is true. So you are happy to rely on the above
> > code being kept in the order specified as long as commit()
> > contains at least one write** - no need for the JMM to mention the
> > commit() method. Okay. Now replace commit() with unpark() - what's
> > the difference?
>
> The difference isn't commit() vs. unpark(), it's the original order.
> For most of this thread we've been debating a program snippet that
> has a volatile write _before_ the unpark() call (in program order).
> The example you just raised has a volatile write _after_ the
> commit() call.

Yes my bad - I realized this morning this example was the wrong way around
to use as analogy with the unpark case.

> My admittedly-novice understanding is that volatile
> writes generally prevent moving stuff from before them to after
> them, but allow moving stuff from after them to before them (where
> I'm being intentionally vague about "stuff" so as not to look up the
> exact definition -- but it definitely includes regular memory
> writes).

Yes that is right.

> > ** I'll ignore how you would know in general if indeed commit()
> > did contain a write!
>
> If it doesn't contain any writes, it can't affect any other threads,
> and reordering is irrelevant!

Well there's more to computing than just memory accesses - the commit could
be sending a packet to a remote database. The point I was trying to make is
that neither you nor the compiler know whats inside these
methods )especially if there are native ones!)

The compiler can reorder things, or in general do semantic preserving
transformations, but it has to know that the transformation is semantic
preserving. For a method like unpark, which is a native call, it knows
nothing and so can't touch it. If unpark were pure Java and could be inlined
then the compiler can know a lot more, and the implementation of unpark
would have to ensure no unintended reordering were possible. But again the
point I was trying to make is that the requirements for a correct
implementation of unpark do not need to be defined as part of the
programming model exported by the JMM.

Cheers,
David

> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Tue Jan 13 19:53:55 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Jan 2015 10:53:55 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <B35F834F-8079-4BCC-B2BA-8D0931CE08EB@flyingtroika.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEHJKMAA.davidcholmes@aapt.net.au>

DT writes:
>
> I am not sure if I can compare LockSupport.park / unpark and
> ReentrankLock.lock/ unlock semantics. Though is there any reason
> why LockSupport does not have tryPark ? I am trying to get some
> common background rules where LockSupport should be used based on
> current thread.

park is to use when you know you want to block the thread because some
higher-level "resource" (lock, semaphore-permit, latch-count etc etc) is not
available to you. It is intended for use as the lowest-level mechanism
available to block and unblock a thread, for use in higher-level
synchronization constructs. The "try" aspect is normally built into to those
higher level constructs.

David

> Sent from my iPhone
>
> > On Jan 10, 2015, at 1:43 PM, "David Holmes"
> <davidcholmes at aapt.net.au> wrote:
> >
> > Not sure what you mean by "user-level" - it's all Java code.
> park/unpark are
> > low-level API's only normally used via higher-level concurrency
> abstractions
> > which also contain the volatile state variable(s) - eg
> > AbstractQueuedSynchronizer etc.
> >
> > David
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
> >> thurstonn
> >> Sent: Sunday, 11 January 2015 7:24 AM
> >> To: concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] unpark/park memory visibility
> >>
> >>
> >> David Holmes-6 wrote
> >>> Interruption is a top-level Thread API so it was decided not
> >> unreasonable
> >>> to
> >>> have it behave as-if there were a "volatile boolean
> >> interrupted" field in
> >>> Thread. Hence the HB relation.
> >>>
> >>> Park/unpark is the lowest-level of API and in
> >> *
> >>> 99.99% of cases will be used
> >>> in conjunction with volatile state variables
> >> *
> >>> , which provide the HB
> >>> relations. It would have been an excessive burden to require
> HB for the
> >>> unpark/park themselves.
> >>>
> >>> David
> >>
> >> Thanks.
> >> Just so I understand, you mean **user-level code** will pair
> unparks/parks
> >> with volatile variables, right?
> >>
> >>
> >>
> >> --
> >> View this message in context:
> >
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
> tp11812p11842.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From david.lloyd at redhat.com  Tue Jan 13 21:00:43 2015
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Tue, 13 Jan 2015 20:00:43 -0600
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1421188759498-11903.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<1421188759498-11903.post@n7.nabble.com>
Message-ID: <54B5CDCB.4040003@redhat.com>

On 01/13/2015 04:39 PM, thurstonn wrote:
> Probably a lost cause, but a more "compelling" example:
>
> refactor of AtomicReference#getAndUpdate
>
>          T tmp, prev = get();
>          do
>          {
>              tmp = prev;
>          } while ((prev = compareSwap(prev, f.apply(prev))) != tmp);
>          return prev;
>
> presumably this pattern is common enough to warrant an additional method in
> AR (was that a 1K hours too?);
>
>
> a native compareSwap would presumably shine here,obviously absolute speedup
> is a function of # of cas failures

Given that compare+swap is so very similar to compare+set, is it really 
outside of the realm of possibility to try and patch this yourself into 
OpenJDK?  Though I feel I should disclaim that my knowledge of the (JIT) 
compiler internals is pretty minimal (really just limited to 
periodically looking in various files to figure out what intrinsics are 
in there), it should largely be a matter of searching for things 
referencing compare+set and replicating it as compare+swap in 
(hopefully) obvious ways, and maybe implementing a Java rendition of 
compare+swap implemented in terms of compare+set and get as a fallback.

I'm sure if you had a working patch, someone would be willing to at 
least talk about sponsoring the change.
-- 
- DML

From jsampson at guidewire.com  Tue Jan 13 21:47:22 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 14 Jan 2015 02:47:22 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> This is confusing two different things. The implementation of
> park/unpark must work correctly and all current implementations
> will have actions that cause happens-before edges, and these are
> necessary for correctness. What I have been unsuccessfully trying
> to make clear is that the implementation requirements of
> park/unpark don't have to be expressed in the JMM as a property
> that forms part of the programming model.

Well, we weren't asking to update the JLS or anything, just to
clarify the javadoc for LockSupport, which Doug has done. Success!

> > > ** I'll ignore how you would know in general if indeed
> > > commit() did contain a write!
> >
> > If it doesn't contain any writes, it can't affect any other
> > threads, and reordering is irrelevant!
>
> Well there's more to computing than just memory accesses - the
> commit could be sending a packet to a remote database. The point I
> was trying to make is that neither you nor the compiler know whats
> inside these methods )especially if there are native ones!)

And the point I was trying to make is that the compiler knows more
than I do. The commit() method _might_ contain memory writes and it
_might_ contain external actions and it _might_ contain explicit
synchronization. It also _might_ be an in-memory database with a
no-op for commit(). That means that the compiler _might_ inline it
and reorder its inter-thread actions in various ways, and I don't
know one way or another unless the interface is documented with
proper reference to the terminology of the JMM.

In the case of park() and unpark(), _you_ know they're native and
the _compiler_ knows they're native, but the docs don't say that, so
_I_ don't know they're native, and I especially don't know that
they'll _always_ be native, and the JMM doesn't have any special
cases for native code anyway. Which all means that _for all I know_,
the compiler _might_ reorder parks and unparks in some way that I
don't expect. All I need is the simple statement that "an unpark()
happens-before all subsequent returns from park() by the target
thread" to know that they can't _possibly_ be reordered in a way
that violates that relationship.

Thanks!
Justin


From gergg at cox.net  Tue Jan 13 16:51:00 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 13 Jan 2015 15:51:00 -0600
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <fWos1p00902hR0p01WotVD>
References: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>
	<fWos1p00902hR0p01WotVD>
Message-ID: <861813C6-31E6-4228-961C-0216D143B1D3@cox.net>

What continues to amaze me, is the fact that this conversation is about 100% Java code.  The majority of Java applications utilize I/O of some sort to relate to things happening outside of the JVM.  None of those methods seem to be identified by the JIT as ?writing? to anything ?volatile? in general.  How can the JIT ever, really know how functions interact with data, 100% correctly when there are not controls available to the developer to absolutely manage that?

What matters the most, is that Java and the JVM has always been a program order based language with only multi-threaded code representing the opportunity for ?ordering? to need control between threads.  Yes, there are things like hyper-threading and other multi-ALU processors in general, that can ?benefit? from instruction scheduling to some degree.  But, practically, it?s been difficult to see that happening without the use of threads to fully load that hardware up with stuff to do.

If we are going to continue to break program ordering premises that most developers feel are important to them, (yes, I am still livid about the loop test hoist in Java 5.0), why do we even think about threads any longer?  Why have we not decided to just create a new language specification which is 100% data-flow driven with complete ordering 100% controlled by references and modification of data read from those references; i.e. if we really want a data-flow language, why not just design one?   All of the work on AT&T?s original EMSP (http://www.google.com/search?q=AT%26T+EMSP) was about data-flow design and absolute, highest speed possible with the hardware.  Continuing to cause developers in a ?program order? based language, to worry about what actually happens in a thread, based on their own hap-hazard placement of keywords that are optional, but very important, seems amazingly wrong.

Gregg Wonderly

> On Jan 13, 2015, at 12:37 PM, DT <dt at flyingtroika.com> wrote:
> 
> I am not sure if I can compare LockSupport.park / unpark and ReentrankLock.lock/ unlock semantics. Though is there any reason why LockSupport does not have tryPark ? I am trying to get some common background rules where LockSupport should be used based on current thread.
> 
> Sent from my iPhone
> 
>> On Jan 10, 2015, at 1:43 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>> 
>> Not sure what you mean by "user-level" - it's all Java code. park/unpark are
>> low-level API's only normally used via higher-level concurrency abstractions
>> which also contain the volatile state variable(s) - eg
>> AbstractQueuedSynchronizer etc.
>> 
>> David
>> 
>>> -----Original Message-----
>>> From: concurrency-interest-bounces at cs.oswego.edu
>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>> thurstonn
>>> Sent: Sunday, 11 January 2015 7:24 AM
>>> To: concurrency-interest at cs.oswego.edu
>>> Subject: Re: [concurrency-interest] unpark/park memory visibility
>>> 
>>> 
>>> David Holmes-6 wrote
>>>> Interruption is a top-level Thread API so it was decided not
>>> unreasonable
>>>> to
>>>> have it behave as-if there were a "volatile boolean
>>> interrupted" field in
>>>> Thread. Hence the HB relation.
>>>> 
>>>> Park/unpark is the lowest-level of API and in
>>> *
>>>> 99.99% of cases will be used
>>>> in conjunction with volatile state variables
>>> *
>>>> , which provide the HB
>>>> relations. It would have been an excessive burden to require HB for the
>>>> unpark/park themselves.
>>>> 
>>>> David
>>> 
>>> Thanks.
>>> Just so I understand, you mean **user-level code** will pair unparks/parks
>>> with volatile variables, right?
>>> 
>>> 
>>> 
>>> --
>>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
>> tp11812p11842.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From aph at redhat.com  Wed Jan 14 05:19:24 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 14 Jan 2015 10:19:24 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
Message-ID: <54B642AC.3050602@redhat.com>

On 14/01/15 02:47, Justin Sampson wrote:
> In the case of park() and unpark(), _you_ know they're native and
> the _compiler_ knows they're native, but the docs don't say that, so
> _I_ don't know they're native, and I especially don't know that
> they'll _always_ be native, and the JMM doesn't have any special
> cases for native code anyway. Which all means that _for all I know_,
> the compiler _might_ reorder parks and unparks in some way that I
> don't expect. All I need is the simple statement that "an unpark()
> happens-before all subsequent returns from park() by the target
> thread" to know that they can't _possibly_ be reordered in a way
> that violates that relationship.

But we have known from the start that an unpark() may be reordered
because

1.  It may return "spuriously".
2.  park() and unpark() may be no-ops.

So I claim you can't say anything about an ordering relationship.

Andrew.


From davidcholmes at aapt.net.au  Wed Jan 14 05:48:05 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 14 Jan 2015 20:48:05 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B642AC.3050602@redhat.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>

Andrew Haley writes:
>
> On 14/01/15 02:47, Justin Sampson wrote:
> > In the case of park() and unpark(), _you_ know they're native and
> > the _compiler_ knows they're native, but the docs don't say that, so
> > _I_ don't know they're native, and I especially don't know that
> > they'll _always_ be native, and the JMM doesn't have any special
> > cases for native code anyway. Which all means that _for all I know_,
> > the compiler _might_ reorder parks and unparks in some way that I
> > don't expect. All I need is the simple statement that "an unpark()
> > happens-before all subsequent returns from park() by the target
> > thread" to know that they can't _possibly_ be reordered in a way
> > that violates that relationship.
>
> But we have known from the start that an unpark() may be reordered
> because
>
> 1.  It may return "spuriously".
> 2.  park() and unpark() may be no-ops.
>
> So I claim you can't say anything about an ordering relationship.

You'd need to express it a bit more precisely:

An unpark() happens-before a return from park that was trigerred by the
unpark.

David

> Andrew.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Wed Jan 14 07:13:59 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 14 Jan 2015 12:13:59 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B65D87.9010400@redhat.com>

On 01/14/2015 10:48 AM, David Holmes wrote:
> Andrew Haley writes:
>>
>> On 14/01/15 02:47, Justin Sampson wrote:
>>> In the case of park() and unpark(), _you_ know they're native and
>>> the _compiler_ knows they're native, but the docs don't say that, so
>>> _I_ don't know they're native, and I especially don't know that
>>> they'll _always_ be native, and the JMM doesn't have any special
>>> cases for native code anyway. Which all means that _for all I know_,
>>> the compiler _might_ reorder parks and unparks in some way that I
>>> don't expect. All I need is the simple statement that "an unpark()
>>> happens-before all subsequent returns from park() by the target
>>> thread" to know that they can't _possibly_ be reordered in a way
>>> that violates that relationship.
>>
>> But we have known from the start that an unpark() may be reordered
>> because
>>
>> 1.  It may return "spuriously".
>> 2.  park() and unpark() may be no-ops.
>>
>> So I claim you can't say anything about an ordering relationship.
> 
> You'd need to express it a bit more precisely:
> 
> An unpark() happens-before a return from park that was trigerred by the
> unpark.

But, given that items 1 and 2 above are true, what can you do with
this information?  You have to communicate using volatiles anyway, and
this gives you the happens-before guarantees you need.  Allowing
people to use park() and unpark() without such volatiles is surely a
non-goal of this API.

Andrew.

From dl at cs.oswego.edu  Wed Jan 14 07:15:14 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 07:15:14 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B642AC.3050602@redhat.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
	<54B642AC.3050602@redhat.com>
Message-ID: <54B65DD2.5040404@cs.oswego.edu>

On 01/14/2015 05:19 AM, Andrew Haley wrote:
> But we have known from the start that an unpark() may be reordered
> because
>
> 1.  It may return "spuriously".

That's unrelated to the added spec of:

  * The
  * memory ordering semantics of {@code park} are the same as those of
  * a volatile read and write to a variable also written by any caller
  * of {@code unpark} for that thread.

> 2.  park() and unpark() may be no-ops.

Yes, in which case they are non-reorderable no-ops!

Which is a strange concept, but no different than
compiler optimizations that kill unneeded volatile
reads but still maintain their ordering effects.

-Doug


From pramalhe at gmail.com  Wed Jan 14 07:17:27 2015
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Wed, 14 Jan 2015 13:17:27 +0100
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
In-Reply-To: <54B595B8.90601@oracle.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
	<5473166A.7030904@gmail.com>
	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
	<5474AC98.5020106@gmail.com>
	<CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>
	<0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>
	<54B595B8.90601@oracle.com>
Message-ID: <CAAApjO1TpB=C5P78EoFaJdfDcRq-Qm2EyeHhvCqUdjzNRirwSw@mail.gmail.com>

Hi Alex,

Just, to make sure there are no misunderstandings, the paper on the
Left-Right
_has_ a proof of correctness for the Classic algorithm. This means that
other
developers and researchers can use the Classic variant as a building block
for
other synchronization mechanisms, with the guarantee of its correctness.
http://concurrencyfreaks.com/2013/12/left-right-concurrency-control.html

Thanks,
Pedro


On Tue, Jan 13, 2015 at 11:01 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> I guess the important thing in "looking under the hood" is to find a
> higher-order construct that things correspond to.
>
> Wouldn't it be cool to have a higher-order concurrency primitive, which
> represents a particular synchronization protocol - a particular graph of
> synchronizes-with - which one would only need to populate with
> program-order bits.
>
> It doesn't mean it's what "the users" would get. But even for the
> developer of "lower-order" primitives it would be helpful not having to
> prove the correctness.
>
> Alex
>
>
>
> On 25/11/2014 21:38, Gil Tene wrote:
>
>> Peter, Pedro,
>>
>> I could draw a similar equivalence between a ReadWriteLock and a counting
>> Semaphore. After all, you can implement the ReadWriteLock-equivalent
>> "pattern" directly using the APIs of a counting semaphore that supports a
>> P(n) operation (e.g. Java's Semaphore). I could draw a translation table
>> for that too. So maybe that means ReadWriteLock is not a separate
>> synchronization primitive? After all, if we can map the calls in some way,
>> it is simply equivalent to and a variant of a semaphore, isn't it?
>>
>> The reason that's not the case is that ReadWriteLock describes expected
>> use assumptions and provides guarantees that are met when those use
>> assumptions are followed, and those are useful for users that do not want
>> to reason about generic semaphores when what they need is a ReadWriteLock.
>> It is also not the case because ReadWriteLock can be implemented using
>> something other than a semaphore, and still meet it's required behaviors.
>>
>> The same is true for WriterReaderPhaser and Left-Right. While
>> WriterReaderPhaser *could* be implemented using parts of Left-Right, it
>> does not present to it's user as Left-Right any more than it presents as a
>> common phased epoch pair (which would be simpler). It describes expected
>> use assumptions and provides guarantees that are met when those use
>> assumptions are followed. And those are useful for users that do not want
>> to reason about generic phased epochs or inside-out Left-Right parts.
>> Furthermore, using Left-Right under the hood is not the preferred
>> implementation of WriterReaderPhaser - it doesn't need the extra complexity
>> and levels of internal abstraction. It certainly doesn't need the
>> internally tracked data structure or the required double-write rules that
>> come with the prescribed use of Left-Right.
>>
>> The equivalence that you try to draw between Left-Right and
>> WriterReaderPhaser is based on looking under the hood of specific
>> implementations and trying to deduce (unstated) guarantees from each, and
>> new valid ways of using the construct that contradict it's declared and
>> prescribed use (but may still be "ok" if you know what the implementation
>> actually does). A user that tries to do that may as well make direct use of
>> epoch counters and skip the higher level abstractions. Mapping the calls
>> with a translation table does not map the guarantees and use rules. It also
>> doesn't erase the "now we are doing something the instructions tell us to
>> never do" problem either. You'd have to list a new set of expected use
>> assumptions and guarantees provided by the calls (when only those new use
>> assumptions are followed) to make the mapping useful. And by the time
>> you've added that as an alternative valid use Left-right under certain
>> rules (which would contradict it's currently stated use rules, like the
>> requirement to write the same data twice on both sides of a toggle), you'll
>> probably find that you've documented a new and orthogonal use case that is
>> basically WriterReaderPhaser.
>>
>> For an example of how far you have to stretch to isolate Left-Right's
>> desired parts from it's required use rules, take a look at how convoluted
>> the lambda expression (in your getCounters() example, the one that is
>> passed into LeftRight.modify) ends up being, just so that it can break the
>> rules "just right". The lambda-friendly Left-Right API clearly expects you
>> to perform the same modification operation twice (once on each or the left
>> and right data structures). That's what the lambda expression is for: to
>> avoid having you actually write the modification code twice. But your
>> lambda expression carefully checks to see which side it was asked to work
>> on, and performs a very different operation on each of the "sides". That's
>> quite clever, but is exactly what Left-Right tells you not to do. It works
>> because you've followed the code down to the bottom, and you know how it's
>> being used, and in what order. This extra logic also shows you why thinking
>> of this as "Left-Right protecting the active pointer" doesn't work in your
>> example. Had that been the use case, you would have been able to validly
>> update the "active pointer" to the same value in both calls into the lambda
>> expression (which would obviously break the required WriterReaderPhaser
>> behavior).
>>
>> Rather than go for such a convoluted application of Left-Right, below is
>> simpler way to implement the same double buffered counts thing. This one is
>> neither Left-Right nor WriterReaderPhaser. It's just direct use of phased
>> epochs with no other primitives involved (unless someone wants to claim
>> that any use of phased epochs is a "variant" of Left-Right, which would be
>> amusing). This direct use of epochs works. It is correct. I've used this or
>> variants of it many times myself. But it requires the author to reason
>> about why this stuff actually works each time, and to carefully examine and
>> protect their logic against concurrency effects on the epochs. It is
>> missing a basic primitive that would provide well stated guarantees and
>> would save the work (and bugs) involved in reasoning through this each
>> time. WriterReaderPhaser captures the API and associated behavior
>> expectations and guarantees. That's what the primitive is all about.
>>
>> -------------------------------------------
>> Direct use of phased epochs:
>>
>> public class DoubleBufferedCountsUsingEpochs {
>>      private AtomicLong startEpoch = new AtomicLong(0);
>>      private AtomicLong evenEndEpoch = new AtomicLong(0);
>>      private AtomicLong oddEndEpoch = new AtomicLong(Long.MIN_VALUE);
>>
>>      private long oddCounts[];
>>      private long evenCounts[];
>>
>>      private final long accumulatedCounts[];
>>
>>      public DoubleBufferedCountsUsingEpochs(int size) {
>>          oddCounts = new long[size];
>>          evenCounts = new long[size];
>>          accumulatedCounts = new long[size];
>>      }
>>
>>      public void incrementCount(int iTh) {
>>          boolean phaseIsOdd = (startEpoch.getAndIncrement() < 0);
>>          if (phaseIsOdd) {
>>              oddCounts[iTh]++;
>>              oddEndEpoch.getAndIncrement();
>>          } else {
>>              evenCounts[iTh]++;
>>              evenEndEpoch.getAndIncrement();
>>          }
>>      }
>>
>>      public synchronized long[] getCounts() {
>>          long sourceArray[];
>>          long startValueAtFlip;
>>
>>          // Clear currently unused [next] phase end epoch and set new
>> startEpoch value:
>>          boolean nextPhaseIsEven = (startEpoch.get() < 0); // Current
>> phase is odd...
>>          if (nextPhaseIsEven) {
>>              evenEndEpoch.set(0);
>>              startValueAtFlip = startEpoch.getAndSet(0);
>>              sourceArray = oddCounts;
>>          } else {
>>              oddEndEpoch.set(Long.MIN_VALUE);
>>              startValueAtFlip = startEpoch.getAndSet(Long.MIN_VALUE);
>>              sourceArray = evenCounts;
>>          }
>>
>>          // Spin until previous phase end epoch value catches up with
>> start value at flip:
>>          while ((nextPhaseIsEven && (oddEndEpoch.get() !=
>> startValueAtFlip)) ||
>>                  (!nextPhaseIsEven && (evenEndEpoch.get() !=
>> startValueAtFlip))) {
>>              Thread.yield();
>>          }
>>
>>          // sourceArray is stable. Use it:
>>          for (int i = 0; i < sourceArray.length; i++) {
>>              accumulatedCounts[i] += sourceArray[i];
>>              sourceArray[i] = 0;
>>          }
>>
>>          return accumulatedCounts.clone();
>>      }
>> }
>>
>>
>> Yes. You can use Left-Right per your description below (for LRCounters)
>> with the strange rule-breaking lambda expression, and you can use this
>> simple phased epoch approach above (or many other variants). But *do you
>> want to*? Does using it make it easier or harder to reason about? To me
>> (and to most people, I think) the direct use of epochs is much more
>> readable and easier to reason about for double buffered case than using
>> Left-Right while standing on your head (using it to protect an internal
>> fields, where all roles are reversed from their documented meanings). But
>> neither one of them relieves me of the need to figure out and derive the
>> concurrency behavior for myself, leaving me with unneeded effort and lots
>> of potential bug tails. Much like a ReaderWriterLock saves unneeded effort,
>> pain and bugs when compared to direct use of counting semaphores for the
>> same purpose, WriterReaderPhaser save this effort and pain for double
>> buffered uses looking for wait free writers.
>>
>> The code below is not only easier to read, it is much easier to *write*,
>> design, and reason about. The author simply follows the recipe spelled out
>> in the WriterReaderPhaser JavaDoc. By carefully following the rules (and
>> without needing to understand why and how) the author knows that the access
>> patterns are guaranteed to be safe.
>>
>> ----------------------------------
>> Direct use of WriterReaderPhaser:
>>
>> public class DoubleBufferedCountsUsingWRP {
>>      WriterReaderPhaser phaser = new WriterReaderPhaser();
>>      private long activeCounts[];
>>      private long inactiveCounts[];
>>
>>      private final long accumulatedCounts[];
>>
>>      public DoubleBufferedCountsUsingWRP(int size) {
>>          activeCounts = new long[size];
>>          inactiveCounts = new long[size];
>>          accumulatedCounts = new long[size];
>>      }
>>
>>      public void incrementCount(int iTh) {
>>          long criticalValue = phaser.writerCriticalSectionEnter();
>>          try {
>>              activeCounts[iTh]++;
>>          } finally {
>>              phaser.writerCriticalSectionExit(criticalValue);
>>          }
>>      }
>>
>>      public synchronized long[] getCounts() {
>>          try {
>>              phaser.readerLock();
>>
>>              // switch active and inactive data structures:
>>              long tmp[] = activeCounts;
>>              activeCounts = inactiveCounts;
>>              inactiveCounts = tmp;
>>
>>              // flip WriterReaderPhaser phase:
>>              phaser.flipPhase();
>>
>>              // use stable (newly inactive) data:
>>              for (int i = 0; i < inactiveCounts.length; i++) {
>>                  accumulatedCounts[i] += inactiveCounts[i];
>>                  inactiveCounts[i] = 0;
>>              }
>>          } finally {
>>              phaser.readerUnlock();
>>          }
>>          return accumulatedCounts.clone();
>>      }
>> }
>>
>>  On Nov 25, 2014, at 8:52 AM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
>>>
>>> Yes Peter, it makes absolute sense!
>>> To make it completely clear just "how equivalent" these two methods are,
>>> let me add a "translation table" for the APIs, from WriterReaderPhaser to
>>> Left-Right:
>>>          ? writerCriticalSectionEnter   -> readIndicator.arrive()
>>>          ? writerCriticalSectionExit      -> readIndicator.depart()
>>>          ? readerLock                          -> writersMutex.lock()
>>>          ? readerUnlock                      -> writersMutex.unlock()
>>>          ? flipPhase                             ->
>>> toggleVersionAndScan()
>>>
>>> Thanks,
>>> Pedro
>>>
>>> On Tue, Nov 25, 2014 at 5:21 PM, Peter Levart <peter.levart at gmail.com>
>>> wrote:
>>> Hi Gil,
>>>
>>> I think Pedro is right in claiming that WriteReaderPhaser is a kind of
>>> Left-Right, but he's wrong in explaining that it is a Left-Right used
>>> backwards. In Left-Right terminology, WriteReaderPhaser is not protecting
>>> the mutable structure (mutated from multiple threads), but just
>>> coordinating access to the "active pointer" to the structure". From
>>> Left-Right perspective, multiple threads are just readers of the "active
>>> pointer" (what they do with the underlying structure is not relevant here -
>>> the underlying structure has it's own synchronization). The single thread
>>> (at a time) that wants to get access to the snapshot is the sole writer
>>> (or "swapper") of the "active pointer". Of course, the sole writer or
>>> "swapper" of the active pointer also exploits the fact that the "inactive
>>> pointer" is not being accessed by any current readers of the "active
>>> pointer" and so, the underlying structure is not touched by the "readers".
>>>
>>> I agree with all your statements below about WriteReaderPhaser and I can
>>> see how WriteReaderPhaser API is more suited to the pointer flipping
>>> scenarios, but WriteReaderPhaser and Left-Right can be used
>>> interchangeably. They are, in a sense equivalent.
>>>
>>> To illustrate, I'll use the lambda-enabled Left-Right API:
>>>
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LeftRight.java
>>>
>>> ...and try to re-create your example from below:
>>>
>>> public class LRCounters {
>>>
>>>      static class ArrayRef {
>>>          long[] array;
>>>
>>>          ArrayRef(long[] array) {
>>>              this.array = array;
>>>          }
>>>      }
>>>
>>>      private final LeftRight<ArrayRef> counts;
>>>      private long intervalCounts[];
>>>      private final long accumulatedCounts[];
>>>
>>>      public LRCounters(int size) {
>>>          intervalCounts = new long[size];
>>>          accumulatedCounts = new long[size];
>>>          counts = new LeftRight<>(
>>>              // this is the initially active ArrayRef
>>>              new ArrayRef(new long[size]),
>>>              // and this is the initially inactive one
>>>              new ArrayRef(null)
>>>          );
>>>      }
>>>
>>>      public void incrementCount(int iTh) {
>>>          counts.read(iTh, (i, arrayRef) -> {
>>>              long a[] = arrayRef.array; // this is the read operation
>>>              return ++a[i]; // never mind the racy increment (should do
>>> it with atomics)
>>>          });
>>>      }
>>>
>>>      public long[] getCounts() {
>>>          long[][] result = new long[1][];
>>>
>>>          counts.modify((arrayRef) -> {
>>>              if (arrayRef.array == null) {
>>>                  // we've got the previously inactive ArrayRef
>>>                  arrayRef.array = intervalCounts; // this is the 1st
>>> write operation
>>>              } else {
>>>                  // we've got the previously active ArrayRef
>>>                  // that has just been deactivated
>>>                  intervalCounts = arrayRef.array;
>>>                  arrayRef.array = null; // this is the "mirror" write
>>> operation
>>>                  // add interval counts to accumulatedCounts
>>>                  for (int i = 0; i < intervalCounts.length; i++) {
>>>                      accumulatedCounts[i] += intervalCounts[i];
>>>                      intervalCounts[i] = 0;
>>>                  }
>>>                  // return result
>>>                  result[0] = accumulatedCounts.clone();
>>>              }
>>>          });
>>>
>>>          return result[0];
>>>      }
>>> }
>>>
>>>
>>> Likewise, let's take an example that is more suited to LeftRight API:
>>>
>>> public class LRMap<K, V> {
>>>
>>>      private final LeftRight<Map<K, V>> lrMap = new LeftRight<>(new
>>> HashMap<>(), new HashMap<>());
>>>
>>>      public V get(K key) {
>>>          return lrMap.read(m -> m.get(key));
>>>      }
>>>
>>>      public void put(K key, V value) {
>>>          lrMap.modify(m -> m.put(key, value));
>>>      }
>>> }
>>>
>>> ...and try to implement is using WriteReaderPhaser:
>>>
>>> public class WRPMap<K, V> {
>>>
>>>      private final WriterReaderPhaser wrp = new WriterReaderPhaser();
>>>      private volatile Map<K, V> activeMap = new HashMap<>();
>>>      private volatile Map<K, V> inactiveMap = new HashMap<>();
>>>
>>>      public V get(K key) {
>>>          long stamp = wrp.writerCriticalSectionEnter();
>>>          try {
>>>              return activeMap.get(key);
>>>          } finally {
>>>              wrp.writerCriticalSectionExit(stamp);
>>>          }
>>>      }
>>>
>>>      public void put(K key, V value) {
>>>          wrp.readerLock();
>>>          try {
>>>              Map<K, V> m1 = inactiveMap;
>>>              Map<K, V> m2 = activeMap;
>>>              m1.put(key, value); // 1st write to inactive
>>>              // swap active <-> inactive
>>>              activeMap = m1;
>>>              inactiveMap = m2;
>>>
>>>              wrp.flipPhase();
>>>
>>>              m2.put(key, value); // mirror write to just deactivated
>>>          } finally {
>>>              wrp.readerUnlock();
>>>          }
>>>      }
>>> }
>>>
>>> Does this make any sense?
>>>
>>> Regards, Peter
>>>
>>> On 11/25/2014 08:39 AM, Gil Tene wrote:
>>> Pedro, I think you are confusing specific under-the-hood implementation
>>> choices (which are similar) with what the primitive is. I'm flattered at
>>> your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT
>>> example) is a Left-Right variant with a different underlying (attributed to
>>> me) arrive-depart technique. It is not a WriterReaderPhaser.
>>>
>>> WriterReaderPhaser captures (in a clean synchronization primitive API
>>> form) a pattern I've had to use and re-invent myself several times, and I'm
>>> pretty sure many others that have faced the "I want to periodically
>>> report/analyze an actively updating data set" have too. The key here is
>>> capturing the guarantees and prescribed use such that end-users can use the
>>> primitive without needing to understand the underlying logic of an
>>> implementation. I do that in my blog entry (and include a definition and
>>> use example below).
>>>
>>> A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used
>>> backwards. It's also not Left-Right, or Left-Right used backwards. The
>>> qualities and guarantees a WriterReaderPhaser provides are not provided by
>>> reversing the meaning of "writer" and "reader" in those primitives. Even if
>>> you ignore the confusion that such upside-down use may cause the user,
>>> there are specific guarantees that the other primitives do not provide, and
>>> that a write-heavy double-buffered use case needs and gets from this
>>> primitive.
>>>
>>> And yes, there are several possible ways to implement a well behaving
>>> WriterReaderPhaser, one of which is listed in the links I provided. We can
>>> implement it with three atomic words and some clever combos of CAS and
>>> GetAndAdd ops, or in other ways. The implementation is not what makes the
>>> primitive what it is to it's users. It's the API and the behavior
>>> guarantees that do that. And I'm pretty sure these behavior guarantees are
>>> not spelled out or provided (even backwards) in Left-Right and variants.
>>> Some of them (like the data stability guarantee for readers even in the
>>> presence of wait-free write activity) would be un-natural to provide in
>>> reverse for writers (since readers are generally not expected to change
>>> stuff).
>>>
>>> Left-Right is cool (really cool), but it focuses purely on wait-free
>>> readers and blocking writers. While that use case may appear to be "the
>>> opposite" of wait-free writers with blocking readers, there are specific
>>> non-mirroring qualities that make that duality invalid. Here are specific
>>> differences between the two mechanisms that make "backwards" use
>>> inapplicable::
>>>
>>> - WriterReaderPhaser provides specific data stability guarantees to
>>> readers (after a flip while under a readLock), even in the presence of
>>> concurrent writer activity. Left-Right does not provide such a guarantee to
>>> writers "backwards". E.g. if Left-Right readers happened to write into the
>>> Left-Right protected data structure (as they would need to in a "backwards
>>> use" attempt like this), Left-Right says nothing about what writers can
>>> expect from that data structure in terms of data consistency or stability.
>>> Note that I'm not saying that no Left-Right implementation could
>>> accidentally provide this behavior without stating it. I'm saying that the
>>> Left-Right mechanism, as described and documented in Left-Right paper and
>>> the various APIs for it's existing variants makes no such guarantee to the
>>> caller, and that a valid Left-Right implementation may or may not provide
>>> this behavior. As such, the user cannot rely on it. And this is the main
>>> guarantee a typical WriterReaderPhaser user will be looking for.
>>>
>>> - Left-Right specifically prohibits readers from writing. E.g. "...To
>>> access in read-only mode do something like this:..." is stated in the
>>> documentation for a LeftRightGuard variants.  In contrast,
>>> WriterReaderPhaser allows it's writers (which would be the readers in a
>>> backwards mapping attempt) to, um, write...
>>>
>>> - Writers that use Left-Right are required to to write their updates
>>> twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g.
>>> "...The exact same operations must be done on the instance before and after
>>> guard.writeToggle()." is stated in the documentation for a LeftRightGuard
>>> variants. In contrast, WriterReaderPhaser does not require reading twice or
>>> writing twice. The two APIs do not mirror each other in this critical
>>> aspect.
>>>
>>> - Left-Right (even when used to replace a Reader-Writer lock) manages
>>> the active and inactive data structures internally (leftInstance and
>>> rightInstance, or firstInstance and secondInstance), and users of
>>> Left-right [must] operate on data structures returned from Left-Right
>>> operations. In contrast, WriterReaderPhaser does not manage the active and
>>> inactive data structures in any way, leaving that job to readers and
>>> writers that operate directly on the shared data structures.
>>>
>>> To be specific, let me detail what a WriterReaderPhaser is (taken from
>>> an updated blog entry that now includes a definition):
>>>
>>> -----------------------------------------------------------------
>>> Definition of WriterReaderPhaser:
>>>
>>> A WriterReaderPhaser provides a means for wait free writers to
>>> coordinate with blocking (but guaranteed forward progress) readers sharing
>>> a set of data structures.
>>>
>>> A WriterReaderPhaser instance provides the following 5 operations:
>>>
>>>          ? writerCriticalSectionEnter
>>>          ? writerCriticalSectionExit
>>>          ? readerLock
>>>          ? readerUnlock
>>>          ? flipPhase
>>>
>>> When a WriterReaderPhaser  instance is used to protect an active [set of
>>> or single] data structure involving [potentially multiple] writers and
>>> [potentially multiple] readers , the assumptions on how readers and writers
>>> act are:
>>>
>>>          ? There are two sets of data structures (an "active" set and an
>>> "inactive" set)
>>>          ? Writing is done to the perceived active version (as perceived
>>> by the writer), and only within critical sections delineated by
>>> writerCriticalSectionEnter and writerCriticalSectionExit operations.
>>>          ? Only readers switch the perceived roles of the active and
>>> inactive data structures. They do so only while holding the readerLock, and
>>> only before execution a flipPhase.
>>>
>>>          ? Readers do not hold onto readerLock indefinitely. Only
>>> readers perform readerLock and readerUnlock.
>>>          ? Writers do not remain in their critical sections
>>> indefinitely. Only writers perform writerCriticalSectionEnter and
>>> writerCriticalSectionExit.
>>>          ? Only readers perform flipPhase operations, and only while
>>> holding the readerLock.
>>>
>>> When the above assumptions are met, WriterReaderPhaser guarantees that
>>> the inactive data structures are not being modified by any writers while
>>> being read while under readerLock protection after a flipPhase operation.
>>>
>>> The following progress guarantees are provided to writers and readers
>>> that adhere to the above stated assumptions:
>>>          ? Writers operations (writerCriticalSectionEnter and
>>> writerCriticalSectionExit) are wait free (on architectures that support
>>> wait-free atomic increment operations).
>>>          ? flipPhase operations are guaranteed to make forward progress,
>>> and will only be blocked by writers whose critical sections were entered
>>> prior to the start of the reader's flipPhase operation, and have not yet
>>> exited their critical sections.
>>>          ? readerLock only block for other readers that are holding the
>>> readerLock.
>>>
>>> -----------------------------------------------------------------
>>> Example use:
>>>
>>> Imagine a simple use case where a large set of rapidly updated counter
>>> is being modified by writers, and a reader needs to gain access to stable
>>> interval samples of those counters for reporting and other analysis
>>> purposes.
>>>
>>> The counters are represented in a volatile array of values (it is the
>>> array reference that is volatile, not the value cells within it):
>>>
>>> volatile long counts[];
>>> ...
>>>
>>> A writer updates a specific count (n) in the set of counters:
>>>
>>> writerCriticalSectionEnter
>>>      counts[n]++;
>>> writerCriticalSectionExit
>>>
>>> A reader gain access to a stable set of counts collected during an
>>> interval, reports on it, and accumulates it:
>>>
>>> long intervalCounts[];
>>> long accumulated_counts[];
>>>
>>> ...
>>> readerLock
>>>      reset(interval_counts);
>>>      long tmp[] = counts;
>>>      counts = interval_counts;
>>>      interval_counts = tmp;
>>> flipPhase
>>>      report_interval_counts(interval_counts);
>>>      accumulated_counts.add(interval_counts);
>>> readerUnlock
>>> -----------------------------------------------------------------
>>>
>>>
>>> Bottom line: the above is defines what a WriterReaderPhaser primitive
>>> is, and shows a simple example of using it. While I provide an example
>>> implementation, many are possible, and I'm sure another 17 will pop up. To
>>> avoid wrongly taking credit for this as a new primitive, I'm looking to see
>>> if there have been previously described primitives that explicitly provide
>>> these (or equivalent) qualities to their users. "Explicitly" being a key
>>> word (since no sane user would rely on an accidental implicit behavior of a
>>> specific implementation of a primitive that does not actually guarantee the
>>> given behavior).
>>>
>>> -- Gil.
>>>
>>> On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
>>> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain
>>> to Gil, the WriterReaderPhaser uses the same concurrency control algorithm
>>> as the Left-Right, and as such it is a variant of the Left-Right (used
>>> "backwards") that uses a (new) ReadIndicator with a single ingress combined
>>> with versionIndex. This variant is not as good for scalability under high
>>> contention as the one you yourself have implemented some time ago, with the
>>> ReadIndicator of ingress/egress with LongAdders. You're better off using
>>> your own implementation, and just do the mutations in lrSet.read() and the
>>> read-only operation in lrSet.modify(), but of course, feel free to try both
>>> and let us your results ;)
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
>>> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers,
>>> Pedro
>>>
>>> On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
>>>
>>> Hi Gil,
>>>
>>> What a coincidence. I was thinking of writing something like that myself
>>> in past couple of days for a similar purpose. It comes as a gift that you
>>> posted this here, thanks!
>>>
>>> My application is an asynchronous cache store implementation. A
>>> distributed cache (Coherence in my case) emits synchronous events when
>>> cache is updated from multiple threads. I want to batch updates and do
>>> asynchronous persistence in a background thread. Coherence already supports
>>> this by itself, but is rather limited in features, so I have to re-create
>>> this functionality and add missing features.
>>>
>>> Regards, Peter
>>>
>>> On 11/24/2014 06:54 AM, Gil Tene wrote:
>>> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think
>>> this may actually be a new, generically useful primitive. Basically, if you
>>> ever needed to analyze or log rapidly mutating data without blocking or
>>> locking out writers, this thing is for you. It supports wait-free writers,
>>> and stable readable data sets for guaranteed-forward-progress readers. And
>>> it makes double buffered data management semi-trivial. See blog entry
>>> explaining stuff : "WriterReaderPhaser: A story about a new (?)
>>> synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/
>>> writerreaderphaser-story-about-new.html>. (with some interesting
>>> discussion comparing it to Left-Right, which does the opposite thing: wait
>>> free readers with blocking writers). See a simple (and very practical)
>>> example of using the primitive at: https://github.com/
>>> HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/
>>> IntervalHistogramRecorder.java And see the primitive qualities and use
>>> rules documented (in the JavaDoc) along with a working implementation at:
>>> https://github.com/HdrHistogram/HdrHistogram/
>>> blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So
>>> please rip this thing apart? Or consider if it may be a useful addition to
>>> j.u.c. It needs a home. And if you've seen it before (i.e. it's not really
>>> new like I seem to think it is), I'd really like to know. ? Gil.
>>> _______________________________________________ Concurrency-interest
>>> mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/b4b845f3/attachment-0001.html>

From aph at redhat.com  Wed Jan 14 07:36:48 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 14 Jan 2015 12:36:48 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B65DD2.5040404@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>	<54B642AC.3050602@redhat.com>
	<54B65DD2.5040404@cs.oswego.edu>
Message-ID: <54B662E0.2060108@redhat.com>

On 01/14/2015 12:15 PM, Doug Lea wrote:
> On 01/14/2015 05:19 AM, Andrew Haley wrote:
>> But we have known from the start that an unpark() may be reordered
>> because
>>
>> 1.  It may return "spuriously".
> 
> That's unrelated to the added spec of:
> 
>   * The
>   * memory ordering semantics of {@code park} are the same as those of
>   * a volatile read and write to a variable also written by any caller
>   * of {@code unpark} for that thread.
> 
>> 2.  park() and unpark() may be no-ops.
> 
> Yes, in which case they are non-reorderable no-ops!

Hmm.  Does that mean that the following implementation is incorrect?

    public static void unpark(Thread thread) {
    }

    public static void park(Object blocker) {
    }

Andrew.

From dl at cs.oswego.edu  Wed Jan 14 07:45:28 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 07:45:28 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B662E0.2060108@redhat.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>	<54B642AC.3050602@redhat.com>
	<54B65DD2.5040404@cs.oswego.edu> <54B662E0.2060108@redhat.com>
Message-ID: <54B664E8.3080803@cs.oswego.edu>

On 01/14/2015 07:36 AM, Andrew Haley wrote:
>>> 2.  park() and unpark() may be no-ops.
>>
>> Yes, in which case they are non-reorderable no-ops!
>
> Hmm.  Does that mean that the following implementation is incorrect?
>
>      public static void unpark(Thread thread) {
>      }
>
>      public static void park(Object blocker) {
>      }
>

It depends on how many angels fit on a pinhead :-)

(If the implementation does nothing, then no one
can detect whether that "nothing" has been reordered.)

-Doug



From vitalyd at gmail.com  Wed Jan 14 08:52:19 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 08:52:19 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B664E8.3080803@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
	<54B642AC.3050602@redhat.com> <54B65DD2.5040404@cs.oswego.edu>
	<54B662E0.2060108@redhat.com> <54B664E8.3080803@cs.oswego.edu>
Message-ID: <CAHjP37Fk7OcJerAejGRNuP4BQBXy19+HrmxuLsDO6mWKLeUesg@mail.gmail.com>

It seems that would be broken then unless compiler knows about these
methods and treats them as barriers?
On Jan 14, 2015 8:06 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/14/2015 07:36 AM, Andrew Haley wrote:
>
>> 2.  park() and unpark() may be no-ops.
>>>>
>>>
>>> Yes, in which case they are non-reorderable no-ops!
>>>
>>
>> Hmm.  Does that mean that the following implementation is incorrect?
>>
>>      public static void unpark(Thread thread) {
>>      }
>>
>>      public static void park(Object blocker) {
>>      }
>>
>>
> It depends on how many angels fit on a pinhead :-)
>
> (If the implementation does nothing, then no one
> can detect whether that "nothing" has been reordered.)
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/0805deb6/attachment.html>

From aph at redhat.com  Wed Jan 14 08:56:27 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 14 Jan 2015 13:56:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B664E8.3080803@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>	<54B642AC.3050602@redhat.com>
	<54B65DD2.5040404@cs.oswego.edu> <54B662E0.2060108@redhat.com>
	<54B664E8.3080803@cs.oswego.edu>
Message-ID: <54B6758B.60203@redhat.com>

On 01/14/2015 12:45 PM, Doug Lea wrote:
> On 01/14/2015 07:36 AM, Andrew Haley wrote:
>>>> 2.  park() and unpark() may be no-ops.
>>>
>>> Yes, in which case they are non-reorderable no-ops!
>>
>> Hmm.  Does that mean that the following implementation is incorrect?
>>
>>      public static void unpark(Thread thread) {
>>      }
>>
>>      public static void park(Object blocker) {
>>      }
> 
> It depends on how many angels fit on a pinhead :-)
> 
> (If the implementation does nothing, then no one
> can detect whether that "nothing" has been reordered.)

Well, yeah.  That's just what I was thinking: if the above is legal,
then I suggest there is nothing useful you can do with the following
statement:

  * The memory ordering semantics of {@code park} are the same as
  * those of a volatile read and write to a variable also written by
  * any caller of {@code unpark} for that thread.

Andrew.

From oleksandr.otenko at oracle.com  Wed Jan 14 08:59:06 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 14 Jan 2015 13:59:06 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B65D87.9010400@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>
	<54B65D87.9010400@redhat.com>
Message-ID: <54B6762A.3030000@oracle.com>

Since Doug already provided the correction, I'll only continue for the 
sake of explaining the need for that.


On 14/01/2015 12:13, Andrew Haley wrote:
> On 01/14/2015 10:48 AM, David Holmes wrote:
>> Andrew Haley writes:
>>> On 14/01/15 02:47, Justin Sampson wrote:
>>>> In the case of park() and unpark(), _you_ know they're native and
>>>> the _compiler_ knows they're native, but the docs don't say that, so
>>>> _I_ don't know they're native, and I especially don't know that
>>>> they'll _always_ be native, and the JMM doesn't have any special
>>>> cases for native code anyway. Which all means that _for all I know_,
>>>> the compiler _might_ reorder parks and unparks in some way that I
>>>> don't expect. All I need is the simple statement that "an unpark()
>>>> happens-before all subsequent returns from park() by the target
>>>> thread" to know that they can't _possibly_ be reordered in a way
>>>> that violates that relationship.
>>> But we have known from the start that an unpark() may be reordered
>>> because
>>>
>>> 1.  It may return "spuriously".
>>> 2.  park() and unpark() may be no-ops.

These two are insufficient for correct reasoning. You also need:

3. park() may not return until unpark().

All three are covered by: the first park() that did not return before 
unpark() started, is guaranteed to eventually return. Yet, this only 
talks about the relationship between these two operations.

If the synchronization edge between park() and unpark() is not specified 
as at least a possibility, then for all you can tell a unpark() may be a 
normal write, and park() may be a normal read. What this means is that 
both threads will observe their own operations executed in program 
order, but not necessarily the operations of each other in that order, 
leading to a race, whether you have volatiles around it or not:

boolean unpark=false; // deliberately non-volatile
volatile int x=0;
...

Thread 1:
x=1;
unpark=true;

Thread 2:
while (x==0){
   while (!unpark);
   unpark=false;
}

Here unpark=true would be allowed to be seen by Thread 2 ahead of seeing 
x==1, Thread 2 observes x==0, and parks again indefinitely.

Because of (3), you need to be sure that if park() is "after" unpark(), 
the volatile reads that appear after park() in program order, observe 
the volatile writes that appear before unpark(). But what is the 
definition of "after"? In the thread further above David mentioned a 
reference to time, but it isn't time, really - it is the total order of 
all synchronization operations. This requires inter-thread ordering of 
unpark and park - there simply is no other way to specify the condition 
when the volatile writes are guaranteed to be visible and consequently 
when /the park() is *not guaranteed* to return/.


Alex


>>>
>>> So I claim you can't say anything about an ordering relationship.
>> You'd need to express it a bit more precisely:
>>
>> An unpark() happens-before a return from park that was trigerred by the
>> unpark.
> But, given that items 1 and 2 above are true, what can you do with
> this information?  You have to communicate using volatiles anyway, and
> this gives you the happens-before guarantees you need.  Allowing
> people to use park() and unpark() without such volatiles is surely a
> non-goal of this API.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/4d595c46/attachment.html>

From dl at cs.oswego.edu  Wed Jan 14 09:03:19 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 09:03:19 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6758B.60203@redhat.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>	<54B642AC.3050602@redhat.com>
	<54B65DD2.5040404@cs.oswego.edu> <54B662E0.2060108@redhat.com>
	<54B664E8.3080803@cs.oswego.edu> <54B6758B.60203@redhat.com>
Message-ID: <54B67727.2030807@cs.oswego.edu>

On 01/14/2015 08:56 AM, Andrew Haley wrote:
> On 01/14/2015 12:45 PM, Doug Lea wrote:
>> (If the implementation does nothing, then no one
>> can detect whether that "nothing" has been reordered.)
>
> Well, yeah.  That's just what I was thinking: if the above is legal,
> then I suggest there is nothing useful you can do with the following
> statement:
>
>    * The memory ordering semantics of {@code park} are the same as
>    * those of a volatile read and write to a variable also written by
>    * any caller of {@code unpark} for that thread.
>

That's why I prefaced my post about this with:
"Not that it seems to enhance anyone's understanding..."

I'm still thinking it does marginally more good than harm.

-Doug




From thurston at nomagicsoftware.com  Wed Jan 14 10:28:03 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 08:28:03 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B65D87.9010400@redhat.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D81EEE@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEGLKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D81F6A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHBKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
	<54B642AC.3050602@redhat.com>
	<NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>
	<54B65D87.9010400@redhat.com>
Message-ID: <1421249283972-11921.post@n7.nabble.com>

Andrew Haley wrote
> But, given that items 1 and 2 above are true, what can you do with
> this information?  You have to communicate using volatiles anyway, and
> this gives you the happens-before guarantees you need.  Allowing
> people to use park() and unpark() without such volatiles is surely a
> non-goal of this API.
> 
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


Well, technically the following program is guaranteed to terminate:
boolean signal = false; //POSV


Thread 1:


while (! signal)
    park()

Thread 2:
    signal = true;
    unpark(Thread1)


It isn't advisable, but according to the JMM, this program must terminate
(given the now explicit hb)





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11921.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Wed Jan 14 10:32:22 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 14 Jan 2015 15:32:22 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6758B.60203@redhat.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>	<54B642AC.3050602@redhat.com>	<54B65DD2.5040404@cs.oswego.edu>
	<54B662E0.2060108@redhat.com>	<54B664E8.3080803@cs.oswego.edu>
	<54B6758B.60203@redhat.com>
Message-ID: <54B68C06.9020503@oracle.com>

But that is not the problematic case.

The implementations will not always return from park. So it is really a 
"if */condition/* then the first future park will return in the future". 
Also, it means that "if not */condition/* in the future, then subsequent 
park may not return". It means the caller must be able to detect the 
/*condition*/ holds (from the point where the condition is checked) 
before it enters park - and both threads must agree on that 
/*condition*/ and what the future is.

Informally, we understand what park/unpark are for, but formally you 
need that common vision of the condition and the future:

volatile park=true;

Thread 1:
park=false;
unpark();

Thread 2:
if (park) park();
park(); // may hang
if (park) park(); // shall never hang.

Alex

On 14/01/2015 13:56, Andrew Haley wrote:
> On 01/14/2015 12:45 PM, Doug Lea wrote:
>> On 01/14/2015 07:36 AM, Andrew Haley wrote:
>>>>> 2.  park() and unpark() may be no-ops.
>>>> Yes, in which case they are non-reorderable no-ops!
>>> Hmm.  Does that mean that the following implementation is incorrect?
>>>
>>>       public static void unpark(Thread thread) {
>>>       }
>>>
>>>       public static void park(Object blocker) {
>>>       }
>> It depends on how many angels fit on a pinhead :-)
>>
>> (If the implementation does nothing, then no one
>> can detect whether that "nothing" has been reordered.)
> Well, yeah.  That's just what I was thinking: if the above is legal,
> then I suggest there is nothing useful you can do with the following
> statement:
>
>    * The memory ordering semantics of {@code park} are the same as
>    * those of a volatile read and write to a variable also written by
>    * any caller of {@code unpark} for that thread.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/7a82f955/attachment-0001.html>

From TEREKHOV at de.ibm.com  Wed Jan 14 11:19:58 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 14 Jan 2015 17:19:58 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37Fk7OcJerAejGRNuP4BQBXy19+HrmxuLsDO6mWKLeUesg@mail.gmail.com>
Message-ID: <OFA268134D.DA1CDDF2-ONC1257DCD.005962E0-C1257DCD.0059BE41@de.ibm.com>

with a thread-specific 'park mutex' (initially locked):

park() {
  Thread.currentThread().parkMutex().unlock();
  Thread.currentThread().parkMutex().lock();
}

unpark(Thread thread) {
  thread.parkMutex().lock();
  thread.parkMutex().unlock();
}

regards,
alexander.

Vitaly Davidovich <vitalyd at gmail.com>@cs.oswego.edu on 14.01.2015 14:52:19

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Doug Lea <dl at cs.oswego.edu>
cc:	concurrency-interest at cs.oswego.edu
Subject:	Re: [concurrency-interest] unpark/park memory visibility


It seems that would be broken then unless compiler knows about these
methods and treats them as barriers?


On Jan 14, 2015 8:06 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
      On 01/14/2015 07:36 AM, Andrew Haley wrote:
                        2.? park() and unpark() may be no-ops.

                  Yes, in which case they are non-reorderable no-ops!

            Hmm.? Does that mean that the following implementation is
            incorrect?

            ? ? ?public static void unpark(Thread thread) {
            ? ? ?}

            ? ? ?public static void park(Object blocker) {
            ? ? ?}


      It depends on how many angels fit on a pinhead :-)

      (If the implementation does nothing, then no one
      can detect whether that "nothing" has been reordered.)

      -Doug


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From aph at redhat.com  Wed Jan 14 11:41:28 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 14 Jan 2015 16:41:28 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6762A.3030000@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCIEHMKMAA.davidcholmes@aapt.net.au>
	<54B65D87.9010400@redhat.com> <54B6762A.3030000@oracle.com>
Message-ID: <54B69C38.9050903@redhat.com>

On 01/14/2015 01:59 PM, Oleksandr Otenko wrote:

> Because of (3), you need to be sure that if park() is "after"
> unpark(), the volatile reads that appear after park() in program
> order, observe the volatile writes that appear before unpark(). But
> what is the definition of "after"? In the thread further above David
> mentioned a reference to time, but it isn't time, really - it is the
> total order of all synchronization operations. This requires
> inter-thread ordering of unpark and park - there simply is no other
> way to specify the condition when the volatile writes are guaranteed
> to be visible and consequently when /the park() is *not guaranteed*
> to return/.

Okay, got that, thanks.

Andrew.


From TEREKHOV at de.ibm.com  Wed Jan 14 11:48:21 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 14 Jan 2015 17:48:21 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <OFA268134D.DA1CDDF2-ONC1257DCD.005962E0-C1257DCD.0059BE41@de.ibm.com>
Message-ID: <OF9B762765.5AE1BE90-ONC1257DCD.005BE386-C1257DCD.005C57B0@de.ibm.com>

and paired with a thread-specific condvar (pthreads-like):

park() {
  Thread.currentThread().parkCondVar.wait(Thread.currentThread().parkMutex
());
}

unpark(Thread thread) {
  thread.parkMutex().lock();
  thread.parkMutex().unlock();
  thread.parkCondVar().signal();
}

regards,
alexander.

Alexander Terekhov/Germany/IBM at IBMDE@cs.oswego.edu on 14.01.2015 17:19:58

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Vitaly Davidovich <vitalyd at gmail.com>
cc:	Doug Lea <dl at cs.oswego.edu>, concurrency-interest at cs.oswego.edu
Subject:	Re: [concurrency-interest] unpark/park memory visibility


with a thread-specific 'park mutex' (initially locked):

park() {
  Thread.currentThread().parkMutex().unlock();
  Thread.currentThread().parkMutex().lock();
}

unpark(Thread thread) {
  thread.parkMutex().lock();
  thread.parkMutex().unlock();
}

regards,
alexander.

Vitaly Davidovich <vitalyd at gmail.com>@cs.oswego.edu on 14.01.2015 14:52:19

Sent by:		 concurrency-interest-bounces at cs.oswego.edu


To:		 Doug Lea <dl at cs.oswego.edu>
cc:		 concurrency-interest at cs.oswego.edu
Subject:		 Re: [concurrency-interest] unpark/park memory visibility


It seems that would be broken then unless compiler knows about these
methods and treats them as barriers?


On Jan 14, 2015 8:06 AM, "Doug Lea" <dl at cs.oswego.edu> wrote:
      On 01/14/2015 07:36 AM, Andrew Haley wrote:
                        2.? park() and unpark() may be no-ops.

                  Yes, in which case they are non-reorderable no-ops!

            Hmm.? Does that mean that the following implementation is
            incorrect?

            ? ? ?public static void unpark(Thread thread) {
            ? ? ?}

            ? ? ?public static void park(Object blocker) {
            ? ? ?}


      It depends on how many angels fit on a pinhead :-)

      (If the implementation does nothing, then no one
      can detect whether that "nothing" has been reordered.)

      -Doug


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





From dl at cs.oswego.edu  Wed Jan 14 12:09:34 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 12:09:34 -0500
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture additions
Message-ID: <54B6A2CE.6040709@cs.oswego.edu>


First among planned jdk9 changes are tentative additions to
CompletableFuture aimed to address suggestions and complaints upon
discovering that it is being used a lot more than we anticipated.
Comments on these would be welcome. Three categories:

1. Delays and timeouts. People need these all the time, but they are a
nuisance to do yourself. Method orTimeout(time, unit) covers the most
common timeout idiom. Method delayedExecutor(delay, unit, executor)
provides a one-shot delegated executor that waits out delay before
submitting.

2. During (and after) jdk8 discussions, there was a lot of
disagreement about what methods to expose in class CompletableFuture
and interface CompletionStage. The result was to make
CompletableFuture "maximal" and CompletionStage "minimal".  But we
didn't provide any reasonable way to provide any in-between forms via
subclassing. Added methods (mainly, a "virtual constructor") make this
easy.

3. A few common utilities were missing and either painful or
non-obvious to implement. Including "defensive copy" methods
and a static method to return a failed (exceptionally completed)
future.

See javadoc at 
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
Including some new guidance in class-level docs. Here's a paste of
added method javadocs:



     public <U> CompletableFuture<U> newIncompleteFuture()
     Creates a new incomplete CompletableFuture of the type to be returned by a 
CompletionStage method. Subclasses should normally override this method to 
return an instance of the same class as this CompletableFuture. The default 
implementation returns an instance of class CompletableFuture.

     public Executor defaultExecutor()
     Returns the default Executor used for async methods that do not specify an 
Executor. This class uses the ForkJoinPool.commonPool(), but may be overridden 
in subclasses with an Executor that provides at least one independent thread.

     public CompletableFuture<T> copy()
     Returns a new CompletableFuture that is completed normally with the same 
value as this Completablefuture when it completes normally. If this 
CompletableFuture completes exceptionally, then the returned CompletableFuture 
completes exceptionally with a CompletionException with this exception as cause. 
The behavior equivalent is to thenApply(x -> x). This method may be useful as a 
form of "defensive copying", to prevent clients from completing, while still 
being able to arrange dependent actions.

     public CompletionStage<T> minimalCompletionStage()
     Returns a new CompletionStage that is completed normally with the same 
value as this Completablefuture when it completes normally, and cannot be 
independently completed or otherwise used in ways not defined by the methods of 
interface CompletionStage. If this CompletableFuture completes exceptionally, 
then the returned CompletionStage completes exceptionally with a 
CompletionException with this exception as cause.

     public CompletableFuture<T> completeAsync(Supplier<T> supplier,
                                               Executor executor)
     Completes this CompletableFuture with the result of the given Supplier 
function invoked from an asynchronous task using the given executor.

     public CompletableFuture<T> completeAsync(Supplier<T> supplier)
     Completes this CompletableFuture with the result of the given Supplier 
function invoked from an asynchronous task using the the default executor.

     public CompletableFuture<T> orTimeout(long timeout,
                                           TimeUnit unit)

     Exceptionally completes this CompletableFuture with a TimeoutException if 
not otherwise completed before the given timeout.
     public static Executor delayedExecutor(long delay,
                                            TimeUnit unit,
                                            Executor executor)

     Returns a new Executor that submits a task to the given base executor after 
the given delay.
     public static Executor delayedExecutor(long delay,
                                            TimeUnit unit)

     Returns a new Executor that submits a task to the default executor after 
the given delay.

     public static <U> CompletionStage<U> completedStage(U value)
     Returns a new CompletionStage that is already completed with the given 
value and supports only those methods in interface CompletionStage.

     public static <U> CompletableFuture<U> failedFuture(Throwable ex)
     Returns a new CompletableFuture that is already completed exceptionally 
with the given exception.


     public static <U> CompletableFuture<U> failedStage(Throwable ex)
     Returns a new CompletionStage that is already completed exceptionally with 
the given exception and supports only those methods in interface CompletionStage.


From jsampson at guidewire.com  Wed Jan 14 12:29:10 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 14 Jan 2015 17:29:10 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B67727.2030807@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D82483@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEHJKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D82924@sm-ex-01-vm.guidewire.com>
	<54B642AC.3050602@redhat.com>	<54B65DD2.5040404@cs.oswego.edu>
	<54B662E0.2060108@redhat.com>	<54B664E8.3080803@cs.oswego.edu>
	<54B6758B.60203@redhat.com> <54B67727.2030807@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D82AF8@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> That's why I prefaced my post about this with:
> "Not that it seems to enhance anyone's understanding..."
>
> I'm still thinking it does marginally more good than harm.

I think the essential thing, semantically, is the permit. The
concerns raised most recently on the thread about a no-op
implementation are ignoring the permit. A park() call is simply
forbidden to block (or remain blocked) if a permit is available,
which a no-op implementation trivially satisfies. But _if_ the
implementation is _not_ a no-op, then it has to keep track of
whether a permit is available. In that case, the one history that we
can't allow, for park/unpark to be useful at all, is for a thread to
consume a permit via park() without seeing all of the writes (at
least the volatile ones) preceding the unpark() call that produced
that particular permit.

Doug, how about this wording, instead of the volatile read/write
wording that you added:

"Memory consistency effects: Any unpark call that releases a permit
*or* observes that a permit is already available _happens-before_
the return of the park call that acquires that particular released
or observed permit."

Cheers,
Justin



From Sebastian.Millies at softwareag.com  Wed Jan 14 12:36:30 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Wed, 14 Jan 2015 17:36:30 +0000
Subject: [concurrency-interest] [Spam]: Candidate jdk9 CompletableFuture
 additions
In-Reply-To: <54B6A2CE.6040709@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89785D@HQMBX5.eur.ad.sag>

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Doug Lea
Sent: Wednesday, January 14, 2015 6:10 PM
To: Concurrency-interest at cs.oswego.edu
Subject: [Spam]: [concurrency-interest] Candidate jdk9 CompletableFuture additions

> Method orTimeout(time, unit) covers the most common timeout idiom
>     public CompletableFuture<T> orTimeout(long timeout,
>                                           TimeUnit unit)
>
>     Exceptionally completes this CompletableFuture with a TimeoutException if
>not otherwise completed before the given timeout.

I'd appreciate a method like orTimeout(defaultValue, time, unit), which completes normally with the given default value if not otherwise completed before the given timeout. (I would have expected this use case to be at least as completing exceptionally.)

-- Sebastian

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From jsampson at guidewire.com  Wed Jan 14 12:41:47 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 14 Jan 2015 17:41:47 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <OFA268134D.DA1CDDF2-ONC1257DCD.005962E0-C1257DCD.0059BE41@de.ibm.com>
References: <CAHjP37Fk7OcJerAejGRNuP4BQBXy19+HrmxuLsDO6mWKLeUesg@mail.gmail.com>
	<OFA268134D.DA1CDDF2-ONC1257DCD.005962E0-C1257DCD.0059BE41@de.ibm.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D82B2B@sm-ex-01-vm.guidewire.com>

Alexander Terekhov wrote:

> with a thread-specific 'park mutex' (initially locked):
>
> park() {
>   Thread.currentThread().parkMutex().unlock();
>   Thread.currentThread().parkMutex().lock();
> }
>
> unpark(Thread thread) {
>   thread.parkMutex().lock();
>   thread.parkMutex().unlock();
> }

That would just make unpark() block until the target thread calls
park(), wouldn't it?

The reference to a semaphore in the LockSupport javadoc hints at a
more accurate model:

semaphore = new Semaphore(0);

park() {
  semaphore.acquire();
  semaphore.drainPermits();
}

unpark() {
  semaphore.release();
}

Cheers,
Justin


From oleksandr.otenko at oracle.com  Wed Jan 14 13:12:20 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 14 Jan 2015 18:12:20 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <861813C6-31E6-4228-961C-0216D143B1D3@cox.net>
References: <NFBBKALFDCPFIDBNKAPCCEFJKMAA.davidcholmes@aapt.net.au>	<fWos1p00902hR0p01WotVD>
	<861813C6-31E6-4228-961C-0216D143B1D3@cox.net>
Message-ID: <54B6B184.7010501@oracle.com>

Understanding the meaning of simultaneity in this world is key.

Alex

On 13/01/2015 21:51, Gregg Wonderly wrote:
> What continues to amaze me, is the fact that this conversation is about 100% Java code.  The majority of Java applications utilize I/O of some sort to relate to things happening outside of the JVM.  None of those methods seem to be identified by the JIT as ?writing? to anything ?volatile? in general.  How can the JIT ever, really know how functions interact with data, 100% correctly when there are not controls available to the developer to absolutely manage that?
>
> What matters the most, is that Java and the JVM has always been a program order based language with only multi-threaded code representing the opportunity for ?ordering? to need control between threads.  Yes, there are things like hyper-threading and other multi-ALU processors in general, that can ?benefit? from instruction scheduling to some degree.  But, practically, it?s been difficult to see that happening without the use of threads to fully load that hardware up with stuff to do.
>
> If we are going to continue to break program ordering premises that most developers feel are important to them, (yes, I am still livid about the loop test hoist in Java 5.0), why do we even think about threads any longer?  Why have we not decided to just create a new language specification which is 100% data-flow driven with complete ordering 100% controlled by references and modification of data read from those references; i.e. if we really want a data-flow language, why not just design one?   All of the work on AT&T?s original EMSP (http://www.google.com/search?q=AT%26T+EMSP) was about data-flow design and absolute, highest speed possible with the hardware.  Continuing to cause developers in a ?program order? based language, to worry about what actually happens in a thread, based on their own hap-hazard placement of keywords that are optional, but very important, seems amazingly wrong.
>
> Gregg Wonderly
>
>> On Jan 13, 2015, at 12:37 PM, DT <dt at flyingtroika.com> wrote:
>>
>> I am not sure if I can compare LockSupport.park / unpark and ReentrankLock.lock/ unlock semantics. Though is there any reason why LockSupport does not have tryPark ? I am trying to get some common background rules where LockSupport should be used based on current thread.
>>
>> Sent from my iPhone
>>
>>> On Jan 10, 2015, at 1:43 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>>>
>>> Not sure what you mean by "user-level" - it's all Java code. park/unpark are
>>> low-level API's only normally used via higher-level concurrency abstractions
>>> which also contain the volatile state variable(s) - eg
>>> AbstractQueuedSynchronizer etc.
>>>
>>> David
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
>>>> thurstonn
>>>> Sent: Sunday, 11 January 2015 7:24 AM
>>>> To: concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] unpark/park memory visibility
>>>>
>>>>
>>>> David Holmes-6 wrote
>>>>> Interruption is a top-level Thread API so it was decided not
>>>> unreasonable
>>>>> to
>>>>> have it behave as-if there were a "volatile boolean
>>>> interrupted" field in
>>>>> Thread. Hence the HB relation.
>>>>>
>>>>> Park/unpark is the lowest-level of API and in
>>>> *
>>>>> 99.99% of cases will be used
>>>>> in conjunction with volatile state variables
>>>> *
>>>>> , which provide the HB
>>>>> relations. It would have been an excessive burden to require HB for the
>>>>> unpark/park themselves.
>>>>>
>>>>> David
>>>> Thanks.
>>>> Just so I understand, you mean **user-level code** will pair unparks/parks
>>>> with volatile variables, right?
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-
>>> tp11812p11842.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From dl at cs.oswego.edu  Wed Jan 14 13:57:41 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 13:57:41 -0500
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89785D@HQMBX5.eur.ad.sag>
References: <54B6A2CE.6040709@cs.oswego.edu>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89785D@HQMBX5.eur.ad.sag>
Message-ID: <54B6BC25.4020501@cs.oswego.edu>

On 01/14/2015 12:36 PM, Millies, Sebastian wrote:

> I'd appreciate a method like orTimeout(defaultValue, time, unit),

Yes; thanks. To be a little more consistent with naming...

     /**
      * Completes this CompletableFuture with the given value if not
      * otherwise completed before the given timeout.
      *
      * @param value the value to use upon timeout
      * @param timeout how long to wait before completing.
      * @param unit a {@code TimeUnit} determining how to interpret the
      *        {@code timeout} parameter
      * @return this CompletableFuture
      * @since 1.9
      */
     public CompletableFuture<T> completeOnTimeout(T value, long timeout,
                                                   TimeUnit unit)

-Doug



From thurston at nomagicsoftware.com  Wed Jan 14 14:08:41 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 12:08:41 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1420743543652-11812.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
Message-ID: <1421262521215-11932.post@n7.nabble.com>


Explicit specification of the memory semantics is important for 2 reasons:

1.  It allows developers to definitively reason about the correctness of
their code (doesn't make it easy, but makes it possible)


2.   And just as importantly, it imposes a constraint or invariant on the
implementations of park/unpark (since they are native methods), viz.
**they** must assume the burden of providing the ordering guarantees, and if
that means emitting the necessary memory barriers, then that means emitting
them.

That's at least how I interpret it, and point 2 is because I'm assuming the
JVM has no involvement once the native code is linked, until it returns, and
because the JVM has no "if park/unpark called" branch (nor should it)

So, I'm confused when I see things like park/unpark can be a no-op; if
"no-op" includes doing nothing but emitting barriers, then OK; if it
literally means resulting in no instructions being sent to the CPU - then I
don't get that - that sounds like magic to me



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11932.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From forax at univ-mlv.fr  Wed Jan 14 14:26:48 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 14 Jan 2015 20:26:48 +0100
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B6A2CE.6040709@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu>
Message-ID: <54B6C2F8.7070804@univ-mlv.fr>

The Supplier of completeAsync() should be a Supplier<? extends T>
(and yes, supplyAsync() should also take a Supplier<? extends T> but 
it's too late).

cheers,
R?mi

On 01/14/2015 06:09 PM, Doug Lea wrote:
>
> First among planned jdk9 changes are tentative additions to
> CompletableFuture aimed to address suggestions and complaints upon
> discovering that it is being used a lot more than we anticipated.
> Comments on these would be welcome. Three categories:
>
> 1. Delays and timeouts. People need these all the time, but they are a
> nuisance to do yourself. Method orTimeout(time, unit) covers the most
> common timeout idiom. Method delayedExecutor(delay, unit, executor)
> provides a one-shot delegated executor that waits out delay before
> submitting.
>
> 2. During (and after) jdk8 discussions, there was a lot of
> disagreement about what methods to expose in class CompletableFuture
> and interface CompletionStage. The result was to make
> CompletableFuture "maximal" and CompletionStage "minimal".  But we
> didn't provide any reasonable way to provide any in-between forms via
> subclassing. Added methods (mainly, a "virtual constructor") make this
> easy.
>
> 3. A few common utilities were missing and either painful or
> non-obvious to implement. Including "defensive copy" methods
> and a static method to return a failed (exceptionally completed)
> future.
>
> See javadoc at 
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html
> Including some new guidance in class-level docs. Here's a paste of
> added method javadocs:
>
>
>
>     public <U> CompletableFuture<U> newIncompleteFuture()
>     Creates a new incomplete CompletableFuture of the type to be 
> returned by a CompletionStage method. Subclasses should normally 
> override this method to return an instance of the same class as this 
> CompletableFuture. The default implementation returns an instance of 
> class CompletableFuture.
>
>     public Executor defaultExecutor()
>     Returns the default Executor used for async methods that do not 
> specify an Executor. This class uses the ForkJoinPool.commonPool(), 
> but may be overridden in subclasses with an Executor that provides at 
> least one independent thread.
>
>     public CompletableFuture<T> copy()
>     Returns a new CompletableFuture that is completed normally with 
> the same value as this Completablefuture when it completes normally. 
> If this CompletableFuture completes exceptionally, then the returned 
> CompletableFuture completes exceptionally with a CompletionException 
> with this exception as cause. The behavior equivalent is to 
> thenApply(x -> x). This method may be useful as a form of "defensive 
> copying", to prevent clients from completing, while still being able 
> to arrange dependent actions.
>
>     public CompletionStage<T> minimalCompletionStage()
>     Returns a new CompletionStage that is completed normally with the 
> same value as this Completablefuture when it completes normally, and 
> cannot be independently completed or otherwise used in ways not 
> defined by the methods of interface CompletionStage. If this 
> CompletableFuture completes exceptionally, then the returned 
> CompletionStage completes exceptionally with a CompletionException 
> with this exception as cause.
>
>     public CompletableFuture<T> completeAsync(Supplier<T> supplier,
>                                               Executor executor)
>     Completes this CompletableFuture with the result of the given 
> Supplier function invoked from an asynchronous task using the given 
> executor.
>
>     public CompletableFuture<T> completeAsync(Supplier<T> supplier)
>     Completes this CompletableFuture with the result of the given 
> Supplier function invoked from an asynchronous task using the the 
> default executor.
>
>     public CompletableFuture<T> orTimeout(long timeout,
>                                           TimeUnit unit)
>
>     Exceptionally completes this CompletableFuture with a 
> TimeoutException if not otherwise completed before the given timeout.
>     public static Executor delayedExecutor(long delay,
>                                            TimeUnit unit,
>                                            Executor executor)
>
>     Returns a new Executor that submits a task to the given base 
> executor after the given delay.
>     public static Executor delayedExecutor(long delay,
>                                            TimeUnit unit)
>
>     Returns a new Executor that submits a task to the default executor 
> after the given delay.
>
>     public static <U> CompletionStage<U> completedStage(U value)
>     Returns a new CompletionStage that is already completed with the 
> given value and supports only those methods in interface CompletionStage.
>
>     public static <U> CompletableFuture<U> failedFuture(Throwable ex)
>     Returns a new CompletableFuture that is already completed 
> exceptionally with the given exception.
>
>
>     public static <U> CompletableFuture<U> failedStage(Throwable ex)
>     Returns a new CompletionStage that is already completed 
> exceptionally with the given exception and supports only those methods 
> in interface CompletionStage.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Wed Jan 14 14:39:51 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 14 Jan 2015 19:39:51 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421262521215-11932.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
Message-ID: <54B6C607.9040706@oracle.com>

When park() is a no-op, then the loop with park() becomes a busy wait. 
When park() is not a no-op, the busy wait consumption of the CPU may be 
replaced with a voluntary context switch. In this case unpark() is the 
call that resumes the parked thread. The caller of park() should know 
what condition it is waiting for, and the caller of unpark() should know 
what condition someone might be waiting for.

Alex

On 14/01/2015 19:08, thurstonn wrote:
> Explicit specification of the memory semantics is important for 2 reasons:
>
> 1.  It allows developers to definitively reason about the correctness of
> their code (doesn't make it easy, but makes it possible)
>
>
> 2.   And just as importantly, it imposes a constraint or invariant on the
> implementations of park/unpark (since they are native methods), viz.
> **they** must assume the burden of providing the ordering guarantees, and if
> that means emitting the necessary memory barriers, then that means emitting
> them.
>
> That's at least how I interpret it, and point 2 is because I'm assuming the
> JVM has no involvement once the native code is linked, until it returns, and
> because the JVM has no "if park/unpark called" branch (nor should it)
>
> So, I'm confused when I see things like park/unpark can be a no-op; if
> "no-op" includes doing nothing but emitting barriers, then OK; if it
> literally means resulting in no instructions being sent to the CPU - then I
> don't get that - that sounds like magic to me
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11932.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Wed Jan 14 15:03:37 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 13:03:37 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6C607.9040706@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
Message-ID: <1421265817182-11935.post@n7.nabble.com>

Hello Alex,

This is what I'm saying:

the below code is slightly (but importantly different than our canonical
example).


boolean signal = false;  // POSV ==> Plain Old Shared Variable


Thread 1:
while (! signal)
     park();

Thread 2:
signal = true;
unpark(Thread1)



Must this program terminate?  YES.


Note - I am not advising this code, or trying to be pedantic





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Wed Jan 14 15:56:31 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 14 Jan 2015 20:56:31 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421265817182-11935.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8302C@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> the below code is slightly (but importantly different than our
> canonical example).
>
> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>
> Thread 1:
> while (! signal)
>      park();
>
> Thread 2:
> signal = true;
> unpark(Thread1)
>
> Must this program terminate?  YES.

Ah, true. Now that we know there's a happens-before from unpark() to
park(), T1 is guaranteed to wake up and see signal == true in order
to exit the loop. So there must be at least a memory barrier in both
operations even if they're otherwise no-ops.

As a side note, though, T1 might see signal == true when it first
arrives at the loop and therefore never call park() at all. So even
though it's guaranteed to see the write to signal eventually, T1
isn't guaranteed to see any other writes by T2 unless signal itself
is volatile. That was my earlier point about not _practically_ being
able to piggyback on the happens-before from unpark() to park().

Cheers,
Justin


From martinrb at google.com  Wed Jan 14 16:41:16 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 14 Jan 2015 13:41:16 -0800
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B6A2CE.6040709@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu>
Message-ID: <CA+kOe0_zR6NABhorw8cH_teAsEeSY4+H=LsE9MGa8aL7gZeubQ@mail.gmail.com>

Does delayedExecutor belong in Executors.java?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/1f7e8380/attachment.html>

From thurston at nomagicsoftware.com  Wed Jan 14 16:44:18 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 14:44:18 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8302C@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8302C@sm-ex-01-vm.guidewire.com>
Message-ID: <1421271858051-11937.post@n7.nabble.com>

Yes, you are correct and I concede all those points (hence the disclaimer). 
But I'm trying to get consensus on that deliberately simple 4 line program
(for all those that are following along), because there's a follow-up point
that I'd like to make; and if there is one ineluctable truth when it comes
to discussing concurrent programming, is that it's a lot easier to arrive at
consensus/common understanding on a 4 line program, than an n > 4 , n-line
program.




Justin Sampson wrote
> thurstonn wrote:
> 
>> the below code is slightly (but importantly different than our
>> canonical example).
>>
>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>
>> Thread 1:
>> while (! signal)
>>      park();
>>
>> Thread 2:
>> signal = true;
>> unpark(Thread1)
>>
>> Must this program terminate?  YES.
> 
> Ah, true. Now that we know there's a happens-before from unpark() to
> park(), T1 is guaranteed to wake up and see signal == true in order
> to exit the loop. So there must be at least a memory barrier in both
> operations even if they're otherwise no-ops.
> 
> As a side note, though, T1 might see signal == true when it first
> arrives at the loop and therefore never call park() at all. So even
> though it's guaranteed to see the write to signal eventually, T1
> isn't guaranteed to see any other writes by T2 unless signal itself
> is volatile. That was my earlier point about not _practically_ being
> able to piggyback on the happens-before from unpark() to park().
> 
> Cheers,
> Justin
> 
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11937.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Wed Jan 14 17:02:14 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 14 Jan 2015 22:02:14 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc fix
	request)
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>

Howdy,

The recent discussion of alternative CAS implementations inspired me
to read through the existing docs & code in j.u.c.atomic, whereupon
I noticed some discrepancies in semantics.

For this discussion I'll focus on AtomicReference and
AtomicStampedReference; the other single-value atomics follow the
semantics of AtomicReference while AtomicMarkableReference follows
the semantics of AtomicStampedReference.

The package documentation indicates that compareAndSet is strong in
two ways: it cannot fail spuriously _and_ it provides the memory
effects of _both_ a volatile read and a volatile write. In contrast,
weakCompareAndSet is documented to be weak in both of those
respects, allowing spurious failure and not providing any memory
consistency effects outside of the atomic variable itself.

AtomicReference et al. implement compareAndSet by calling into the
intrinsic CAS operation on Unsafe, so I'll assume they behave as
documented.

AtomicStampedReference, on the other hand, actually implements
something in-between strong and weak CAS semantics, without that
fact being documented. Due to the indirection through a private
Pair object, this class's compareAndSet implementation has to do
some of the work itself before calling into the intrinsic CAS. As a
result, the actual semantics are:

1. It always provides the memory effects of a volatile read.

2. It is only guaranteed to provide the memory effects of a volatile
   write _if_ the stamp or reference is successfully _altered_ to a
   different value than it had before.

3. It may fail spuriously.

The attemptStamp method has exactly the same semantics. The spurious
failure is at least documented for attemptStamp, but not for
compareAndSet, and neither one documents the lack of volatile write
effects on failure or no-op success.

Now, I think these semantics are entirely fine. They seem like the
most intuitive and implementable semantics for CAS in general. If
the intrinsic CAS weren't already spec'd to be strong for many years
I'd advocate for it to have these semantics as well. :) There's no
practical need for volatile write effects unless you've actually
altered the value, since otherwise no other threads are going to
notice you've done anything anyway. And the usual idiom is to call
CAS in a loop, so spurious failure is perfectly acceptable.

Therefore I certainly don't want to advocate for changing the
implementation in AtomicStampedReference, but some clarification of
the docs seems in order.

What would y'all think of changing the _package_ docs to describe
this looser kind of semantics, as a _minimum_ strength for _all_
compareAndSet implementations in the package? I believe it's strong
enough for any practical usage, so for most purposes a programmer
need look no further. In those cases such as AtomicReference where
compareAndSet is implemented much stronger, it can be documented on
that class.

Cheers,
Justin


From kasperni at gmail.com  Wed Jan 14 17:12:26 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Wed, 14 Jan 2015 23:12:26 +0100
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B6A2CE.6040709@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu>
Message-ID: <CAPs6150JFZwCt-gNz7xOyghBbX07PsgDK7fRQoL=-kQQnO4xiQ@mail.gmail.com>

>
>
>     public static <U> CompletionStage<U> completedStage(U value)
>     Returns a new CompletionStage that is already completed with the given
> value and supports only those methods in interface CompletionStage.
>
>     public static <U> CompletableFuture<U> failedStage(Throwable ex)
>     Returns a new CompletionStage that is already completed exceptionally
> with the given exception and supports only those methods in interface
> CompletionStage.
>
> I think it makes more sense to have these methods on CompletionStage
instead of CompletionFuture?
Also shouldn't failedStage return a CompletionStage?

- Kasper




> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/72742102/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 14 17:59:01 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 14 Jan 2015 22:59:01 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421265817182-11935.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
Message-ID: <54B6F4B5.5000506@oracle.com>

No, this code doesn't have to terminate.

Alex

On 14/01/2015 20:03, thurstonn wrote:
> Hello Alex,
>
> This is what I'm saying:
>
> the below code is slightly (but importantly different than our canonical
> example).
>
>
> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>
>
> Thread 1:
> while (! signal)
>       park();
>
> Thread 2:
> signal = true;
> unpark(Thread1)
>
>
>
> Must this program terminate?  YES.
>
>
> Note - I am not advising this code, or trying to be pedantic
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Wed Jan 14 19:00:21 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 19:00:21 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6F4B5.5000506@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
Message-ID: <CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>

Could you elaborate? Based on the conversation thus far, I don't see how
compiler is allowed to hoist/remove the (re)load of signal on the loop
iteration.

On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> No, this code doesn't have to terminate.
>
> Alex
>
>
> On 14/01/2015 20:03, thurstonn wrote:
>
>> Hello Alex,
>>
>> This is what I'm saying:
>>
>> the below code is slightly (but importantly different than our canonical
>> example).
>>
>>
>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>
>>
>> Thread 1:
>> while (! signal)
>>       park();
>>
>> Thread 2:
>> signal = true;
>> unpark(Thread1)
>>
>>
>>
>> Must this program terminate?  YES.
>>
>>
>> Note - I am not advising this code, or trying to be pedantic
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://jsr166-concurrency.
>> 10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/11e605da/attachment-0001.html>

From TEREKHOV at de.ibm.com  Wed Jan 14 19:02:57 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Thu, 15 Jan 2015 01:02:57 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D82B2B@sm-ex-01-vm.guidewire.com>
References: <CAHjP37Fk7OcJerAejGRNuP4BQBXy19+HrmxuLsDO6mWKLeUesg@mail.gmail.com>	<OFA268134D.DA1CDDF2-ONC1257DCD.005962E0-C1257DCD.0059BE41@de.ibm.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D82B2B@sm-ex-01-vm.guidewire.com>
Message-ID: <OF23D2ACB4.A5800894-ONC1257DCE.00001660-C1257DCE.00004563@de.ibm.com>

lets get rid of semas on fast path... with a simple atomic exchange/swap:

status = UNPARKED, semaphore = UNLOCKED // per thread

park() {
  if (swap(status, PARKED, memory_order::...) != UNPARKED)
    semaphore.acquire():
}

unpark() {
  if (swap(status, UNPARKED, memory_order::...) == PARKED)
    semaphore.release();
}

regards,
alexander.

Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 14.01.2015
18:41:47

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE,
       "concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
cc:
Subject:	Re: [concurrency-interest] unpark/park memory visibility


Alexander Terekhov wrote:

> with a thread-specific 'park mutex' (initially locked):
>
> park() {
>   Thread.currentThread().parkMutex().unlock();
>   Thread.currentThread().parkMutex().lock();
> }
>
> unpark(Thread thread) {
>   thread.parkMutex().lock();
>   thread.parkMutex().unlock();
> }

That would just make unpark() block until the target thread calls
park(), wouldn't it?

The reference to a semaphore in the LockSupport javadoc hints at a
more accurate model:

semaphore = new Semaphore(0);

park() {
  semaphore.acquire();
  semaphore.drainPermits();
}

unpark() {
  semaphore.release();
}

Cheers,
Justin

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From thurston at nomagicsoftware.com  Wed Jan 14 19:17:12 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 17:17:12 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B6F4B5.5000506@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
Message-ID: <1421281032985-11944.post@n7.nabble.com>

Yes it does;

here's a history that causes it never to terminate:


T2: unpark()
T1: depark
T1: r1 = signal
T1: test r1 //false
T1: park()
T2: signal = true.


But that is not a valid history according to the JMM.
Because unpark < depark requires that (hb(unpark,depark):
signal = true < unpark < depark < read signal false is not allowed.

that's what partial ordering means, any subsequent read by T1 of signal must
see signal = true

I challenge anyone to produce a valid history where that program never
terminates.

And I realize this makes things very, very difficult in having the JVM
actually realize the memory semantics for park/unpark.




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11944.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Wed Jan 14 19:20:27 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 00:20:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
Message-ID: <54B707CB.9090503@oracle.com>

It is not the hoist / remove of reload, it is the order of reload and 
what might be inside park().

Normal load of signal of the subsequent iteration is allowed to appear 
ahead of whatever is inside park() of the previous iteration, as it is 
unspecified. The way Doug has clarified (or the way I wanted to clarify 
it), the normal load of signal can still appear ahead of whatever except 
the start of park() to some threads. park() is allowed to claim the 
permit in an unspecified way. So, it is perfectly fine for park() to 
start before signal==true is apparent, the subsequent read of signal is 
allowed to appear to others ahead of anything else in park() - ie it is 
allowed to appear right after the decision that it doesn't need to 
suspend. It observes signal==false. Now it is allowed for Thread 2's 
store to signal to become apparent to Thread 1 (but it is not loading it 
anymore), and unpark() is allowed to release one permit. Now it is 
allowed for Thread 1 to claim that permit (or even clear the permit 
count without observing that it was set). Now Thread 1's while loop 
thinks signal==false, and it goes to call park() again, and hangs, 
because no more unpark() is called.

If park() is deemed as a singular synchronization event (I don't even 
require a happens-before, so no extra edges appear; visibility of 
permits issued by unpark() to park() become available in an unspecified 
manner - only total ordering of events around these calls in Java land 
matters), a volatile load of signal would not be allowed to appear to 
anyone ahead of park() nor any part thereof - including the claim of the 
permit. So if unpark() was already entered, the volatile load would 
observe the volatile store of signal. Or, if unpark was not yet entered, 
then whatever the load of signal observes, it can at least go to park() 
again until it either observes a permit from unpark(), or the volatile 
store, whichever becomes observed first.

Doug's clarification is vague enough to permit similar interpretation of 
how volatile load is different from normal loads w.r.t. apparent 
reordering with parts of park().

Alex

On 15/01/2015 00:00, Vitaly Davidovich wrote:
> Could you elaborate? Based on the conversation thus far, I don't see 
> how compiler is allowed to hoist/remove the (re)load of signal on the 
> loop iteration.
>
> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, this code doesn't have to terminate.
>
>     Alex
>
>
>     On 14/01/2015 20:03, thurstonn wrote:
>
>         Hello Alex,
>
>         This is what I'm saying:
>
>         the below code is slightly (but importantly different than our
>         canonical
>         example).
>
>
>         boolean signal = false;  // POSV ==> Plain Old Shared Variable
>
>
>         Thread 1:
>         while (! signal)
>               park();
>
>         Thread 2:
>         signal = true;
>         unpark(Thread1)
>
>
>
>         Must this program terminate?  YES.
>
>
>         Note - I am not advising this code, or trying to be pedantic
>
>
>
>
>
>         --
>         View this message in context:
>         http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>         Sent from the JSR166 Concurrency mailing list archive at
>         Nabble.com.
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/e37edb3d/attachment.html>

From vitalyd at gmail.com  Wed Jan 14 19:27:20 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 19:27:20 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B707CB.9090503@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
Message-ID: <CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>

I don't think subsequent load of signal is allowed to appear before park ()
because it was said earlier that park () acts like a volatile load; if so,
subsequent loads cannot move before it.  Did I misunderstand?
On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  It is not the hoist / remove of reload, it is the order of reload and
> what might be inside park().
>
> Normal load of signal of the subsequent iteration is allowed to appear
> ahead of whatever is inside park() of the previous iteration, as it is
> unspecified. The way Doug has clarified (or the way I wanted to clarify
> it), the normal load of signal can still appear ahead of whatever except
> the start of park() to some threads. park() is allowed to claim the permit
> in an unspecified way. So, it is perfectly fine for park() to start before
> signal==true is apparent, the subsequent read of signal is allowed to
> appear to others ahead of anything else in park() - ie it is allowed to
> appear right after the decision that it doesn't need to suspend. It
> observes signal==false. Now it is allowed for Thread 2's store to signal to
> become apparent to Thread 1 (but it is not loading it anymore), and
> unpark() is allowed to release one permit. Now it is allowed for Thread 1
> to claim that permit (or even clear the permit count without observing that
> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
> call park() again, and hangs, because no more unpark() is called.
>
> If park() is deemed as a singular synchronization event (I don't even
> require a happens-before, so no extra edges appear; visibility of permits
> issued by unpark() to park() become available in an unspecified manner -
> only total ordering of events around these calls in Java land matters), a
> volatile load of signal would not be allowed to appear to anyone ahead of
> park() nor any part thereof - including the claim of the permit. So if
> unpark() was already entered, the volatile load would observe the volatile
> store of signal. Or, if unpark was not yet entered, then whatever the load
> of signal observes, it can at least go to park() again until it either
> observes a permit from unpark(), or the volatile store, whichever becomes
> observed first.
>
> Doug's clarification is vague enough to permit similar interpretation of
> how volatile load is different from normal loads w.r.t. apparent reordering
> with parts of park().
>
> Alex
>
> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>
> Could you elaborate? Based on the conversation thus far, I don't see how
> compiler is allowed to hoist/remove the (re)load of signal on the loop
> iteration.
>
> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>> No, this code doesn't have to terminate.
>>
>> Alex
>>
>>
>> On 14/01/2015 20:03, thurstonn wrote:
>>
>>> Hello Alex,
>>>
>>> This is what I'm saying:
>>>
>>> the below code is slightly (but importantly different than our canonical
>>> example).
>>>
>>>
>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>
>>>
>>> Thread 1:
>>> while (! signal)
>>>       park();
>>>
>>> Thread 2:
>>> signal = true;
>>> unpark(Thread1)
>>>
>>>
>>>
>>> Must this program terminate?  YES.
>>>
>>>
>>> Note - I am not advising this code, or trying to be pedantic
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/38331d3e/attachment-0001.html>

From dl at cs.oswego.edu  Wed Jan 14 19:52:41 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 14 Jan 2015 19:52:41 -0500
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B6C2F8.7070804@univ-mlv.fr>
References: <54B6A2CE.6040709@cs.oswego.edu> <54B6C2F8.7070804@univ-mlv.fr>
Message-ID: <54B70F59.6050100@cs.oswego.edu>

Note: Anyone feeling brave can run snapshots on jdk8+ by getting
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
and running java -Xbootclasspath/p:jsr166.jar

On 01/14/2015 02:26 PM, Remi Forax wrote:
> The Supplier of completeAsync() should be a Supplier<? extends T>

Thanks! fixed.

On 01/14/2015 05:12 PM, Kasper Nielsen wrote:
> public static <U> CompletionStage<U> completedStage(U value)
> public static <U> CompletableFuture<U> failedStage(Throwable ex)

> I think it makes more sense to have these methods on CompletionStage instead
> of CompletionFuture?

It would, but static methods are not allowed in interfaces.

> Also shouldn't failedStage return a CompletionStage?

Yes; thanks! Fixed.

On 01/14/2015 04:41 PM, Martin Buchholz wrote:
> Does delayedExecutor belong in Executors.java?

I considered doing this more generally in Executors.
But the scheme here relies on using the 1-thread
ScheduledThreadPoolExecutor only for triggering, cancelling,
and completing (but never running) tasks, so even if an
Executors version were available, we'd probably still use
a customized form here.

-Doug

From thurston at nomagicsoftware.com  Wed Jan 14 19:55:16 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 17:55:16 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421281032985-11944.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<1421281032985-11944.post@n7.nabble.com>
Message-ID: <1421283316615-11947.post@n7.nabble.com>

thurstonn wrote
> Yes it does;
> 
> here's a history that causes it never to terminate:
> 
> 
> T2: unpark()
> T1: depark
> T1: r1 = signal
> T1: test r1 //false
> T1: park()
> T2: signal = true.
> 
> 
> But that is not a valid history according to the JMM.
> Because unpark < depark requires that (hb(unpark,depark):
> signal = true < unpark < depark < read signal false is not allowed.
> 
> that's what partial ordering means, any subsequent read by T1 of signal
> must see signal = true
> 
> I challenge anyone to produce a valid history where that program never
> terminates.
> 
> And I realize this makes things very, very difficult in having the JVM
> actually realize the memory semantics for park/unpark.

Here let me take my own challenge:

T1: r1 = signal
T1; test r1 //false
T1: park()
T2: signal = true
T2: unpark()
T1: depark
T1: test r1 !!! //JVM replaces with a register read, since it has no
obligation not to 'cache' it!!!!

T1: park()


might that be a valid history???  





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11947.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Wed Jan 14 20:16:18 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 01:16:18 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
Message-ID: <54B714E2.7020004@oracle.com>

If park() is seen as a synchronization event, it will still require 
volatile accesses to establish a hb (it will only stop reordering of 
volatile accesses).

If park() is seen as a very particular synchronization event that 
creates a hb edge, then obviously normal reads should observe normal 
writes and it should terminate, but this is not what people write, and I 
don't know if that is what is intended (eg see the objections - people 
didn't think mentioning hb edge was all that useful).

Alex

On 15/01/2015 00:27, Vitaly Davidovich wrote:
>
> I don't think subsequent load of signal is allowed to appear before 
> park () because it was said earlier that park () acts like a volatile 
> load; if so, subsequent loads cannot move before it.  Did I misunderstand?
>
> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     It is not the hoist / remove of reload, it is the order of reload
>     and what might be inside park().
>
>     Normal load of signal of the subsequent iteration is allowed to
>     appear ahead of whatever is inside park() of the previous
>     iteration, as it is unspecified. The way Doug has clarified (or
>     the way I wanted to clarify it), the normal load of signal can
>     still appear ahead of whatever except the start of park() to some
>     threads. park() is allowed to claim the permit in an unspecified
>     way. So, it is perfectly fine for park() to start before
>     signal==true is apparent, the subsequent read of signal is allowed
>     to appear to others ahead of anything else in park() - ie it is
>     allowed to appear right after the decision that it doesn't need to
>     suspend. It observes signal==false. Now it is allowed for Thread
>     2's store to signal to become apparent to Thread 1 (but it is not
>     loading it anymore), and unpark() is allowed to release one
>     permit. Now it is allowed for Thread 1 to claim that permit (or
>     even clear the permit count without observing that it was set).
>     Now Thread 1's while loop thinks signal==false, and it goes to
>     call park() again, and hangs, because no more unpark() is called.
>
>     If park() is deemed as a singular synchronization event (I don't
>     even require a happens-before, so no extra edges appear;
>     visibility of permits issued by unpark() to park() become
>     available in an unspecified manner - only total ordering of events
>     around these calls in Java land matters), a volatile load of
>     signal would not be allowed to appear to anyone ahead of park()
>     nor any part thereof - including the claim of the permit. So if
>     unpark() was already entered, the volatile load would observe the
>     volatile store of signal. Or, if unpark was not yet entered, then
>     whatever the load of signal observes, it can at least go to park()
>     again until it either observes a permit from unpark(), or the
>     volatile store, whichever becomes observed first.
>
>     Doug's clarification is vague enough to permit similar
>     interpretation of how volatile load is different from normal loads
>     w.r.t. apparent reordering with parts of park().
>
>     Alex
>
>     On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>     Could you elaborate? Based on the conversation thus far, I don't
>>     see how compiler is allowed to hoist/remove the (re)load of
>>     signal on the loop iteration.
>>
>>     On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, this code doesn't have to terminate.
>>
>>         Alex
>>
>>
>>         On 14/01/2015 20:03, thurstonn wrote:
>>
>>             Hello Alex,
>>
>>             This is what I'm saying:
>>
>>             the below code is slightly (but importantly different
>>             than our canonical
>>             example).
>>
>>
>>             boolean signal = false;  // POSV ==> Plain Old Shared
>>             Variable
>>
>>
>>             Thread 1:
>>             while (! signal)
>>                   park();
>>
>>             Thread 2:
>>             signal = true;
>>             unpark(Thread1)
>>
>>
>>
>>             Must this program terminate?  YES.
>>
>>
>>             Note - I am not advising this code, or trying to be pedantic
>>
>>
>>
>>
>>
>>             --
>>             View this message in context:
>>             http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>             Sent from the JSR166 Concurrency mailing list archive at
>>             Nabble.com.
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/1b4d0aac/attachment.html>

From vitalyd at gmail.com  Wed Jan 14 20:26:09 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 20:26:09 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B714E2.7020004@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
Message-ID: <CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>

My interpretation is pretty much what Jason said earlier; loop definitively
terminates (eventually) but thread 1 may not see other writes if it never
enters the loop.  That seems consistent with Doug's clarification.

sent from my phone
On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  If park() is seen as a synchronization event, it will still require
> volatile accesses to establish a hb (it will only stop reordering of
> volatile accesses).
>
> If park() is seen as a very particular synchronization event that creates
> a hb edge, then obviously normal reads should observe normal writes and it
> should terminate, but this is not what people write, and I don't know if
> that is what is intended (eg see the objections - people didn't think
> mentioning hb edge was all that useful).
>
> Alex
>
> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>
> I don't think subsequent load of signal is allowed to appear before park
> () because it was said earlier that park () acts like a volatile load; if
> so, subsequent loads cannot move before it.  Did I misunderstand?
> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  It is not the hoist / remove of reload, it is the order of reload and
>> what might be inside park().
>>
>> Normal load of signal of the subsequent iteration is allowed to appear
>> ahead of whatever is inside park() of the previous iteration, as it is
>> unspecified. The way Doug has clarified (or the way I wanted to clarify
>> it), the normal load of signal can still appear ahead of whatever except
>> the start of park() to some threads. park() is allowed to claim the permit
>> in an unspecified way. So, it is perfectly fine for park() to start before
>> signal==true is apparent, the subsequent read of signal is allowed to
>> appear to others ahead of anything else in park() - ie it is allowed to
>> appear right after the decision that it doesn't need to suspend. It
>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>> become apparent to Thread 1 (but it is not loading it anymore), and
>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>> to claim that permit (or even clear the permit count without observing that
>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>> call park() again, and hangs, because no more unpark() is called.
>>
>> If park() is deemed as a singular synchronization event (I don't even
>> require a happens-before, so no extra edges appear; visibility of permits
>> issued by unpark() to park() become available in an unspecified manner -
>> only total ordering of events around these calls in Java land matters), a
>> volatile load of signal would not be allowed to appear to anyone ahead of
>> park() nor any part thereof - including the claim of the permit. So if
>> unpark() was already entered, the volatile load would observe the volatile
>> store of signal. Or, if unpark was not yet entered, then whatever the load
>> of signal observes, it can at least go to park() again until it either
>> observes a permit from unpark(), or the volatile store, whichever becomes
>> observed first.
>>
>> Doug's clarification is vague enough to permit similar interpretation of
>> how volatile load is different from normal loads w.r.t. apparent reordering
>> with parts of park().
>>
>> Alex
>>
>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>
>> Could you elaborate? Based on the conversation thus far, I don't see how
>> compiler is allowed to hoist/remove the (re)load of signal on the loop
>> iteration.
>>
>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>> No, this code doesn't have to terminate.
>>>
>>> Alex
>>>
>>>
>>> On 14/01/2015 20:03, thurstonn wrote:
>>>
>>>> Hello Alex,
>>>>
>>>> This is what I'm saying:
>>>>
>>>> the below code is slightly (but importantly different than our canonical
>>>> example).
>>>>
>>>>
>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>
>>>>
>>>> Thread 1:
>>>> while (! signal)
>>>>       park();
>>>>
>>>> Thread 2:
>>>> signal = true;
>>>> unpark(Thread1)
>>>>
>>>>
>>>>
>>>> Must this program terminate?  YES.
>>>>
>>>>
>>>> Note - I am not advising this code, or trying to be pedantic
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/c569a3f8/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 14 20:27:51 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 20:27:51 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421283316615-11947.post@n7.nabble.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<1421281032985-11944.post@n7.nabble.com>
	<1421283316615-11947.post@n7.nabble.com>
Message-ID: <CAHjP37Gp5N0WBYCkLSHA+UujkWkX=hkiT3h4q_cmSeJ0E-L2CQ@mail.gmail.com>

This is what I thought Alex meant when I mentioned hoisting/removal of the
reload, but I don't think that's valid if park () has volatile load
semantics.

sent from my phone
On Jan 14, 2015 8:21 PM, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> thurstonn wrote
> > Yes it does;
> >
> > here's a history that causes it never to terminate:
> >
> >
> > T2: unpark()
> > T1: depark
> > T1: r1 = signal
> > T1: test r1 //false
> > T1: park()
> > T2: signal = true.
> >
> >
> > But that is not a valid history according to the JMM.
> > Because unpark < depark requires that (hb(unpark,depark):
> > signal = true < unpark < depark < read signal false is not allowed.
> >
> > that's what partial ordering means, any subsequent read by T1 of signal
> > must see signal = true
> >
> > I challenge anyone to produce a valid history where that program never
> > terminates.
> >
> > And I realize this makes things very, very difficult in having the JVM
> > actually realize the memory semantics for park/unpark.
>
> Here let me take my own challenge:
>
> T1: r1 = signal
> T1; test r1 //false
> T1: park()
> T2: signal = true
> T2: unpark()
> T1: depark
> T1: test r1 !!! //JVM replaces with a register read, since it has no
> obligation not to 'cache' it!!!!
>
> T1: park()
>
>
> might that be a valid history???
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11947.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/2c3fc5de/attachment.html>

From vitalyd at gmail.com  Wed Jan 14 20:28:54 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 14 Jan 2015 20:28:54 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
Message-ID: <CAHjP37EDvY8KqGgO3C98in0k92jkGPERuwAig11PgPZ+6sOijw@mail.gmail.com>

Err, Justin I mean - sorry!

sent from my phone
On Jan 14, 2015 8:26 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> My interpretation is pretty much what Jason said earlier; loop
> definitively terminates (eventually) but thread 1 may not see other writes
> if it never enters the loop.  That seems consistent with Doug's
> clarification.
>
> sent from my phone
> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  If park() is seen as a synchronization event, it will still require
>> volatile accesses to establish a hb (it will only stop reordering of
>> volatile accesses).
>>
>> If park() is seen as a very particular synchronization event that creates
>> a hb edge, then obviously normal reads should observe normal writes and it
>> should terminate, but this is not what people write, and I don't know if
>> that is what is intended (eg see the objections - people didn't think
>> mentioning hb edge was all that useful).
>>
>> Alex
>>
>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>
>> I don't think subsequent load of signal is allowed to appear before park
>> () because it was said earlier that park () acts like a volatile load; if
>> so, subsequent loads cannot move before it.  Did I misunderstand?
>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  It is not the hoist / remove of reload, it is the order of reload and
>>> what might be inside park().
>>>
>>> Normal load of signal of the subsequent iteration is allowed to appear
>>> ahead of whatever is inside park() of the previous iteration, as it is
>>> unspecified. The way Doug has clarified (or the way I wanted to clarify
>>> it), the normal load of signal can still appear ahead of whatever except
>>> the start of park() to some threads. park() is allowed to claim the permit
>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>> signal==true is apparent, the subsequent read of signal is allowed to
>>> appear to others ahead of anything else in park() - ie it is allowed to
>>> appear right after the decision that it doesn't need to suspend. It
>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>> to claim that permit (or even clear the permit count without observing that
>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>> call park() again, and hangs, because no more unpark() is called.
>>>
>>> If park() is deemed as a singular synchronization event (I don't even
>>> require a happens-before, so no extra edges appear; visibility of permits
>>> issued by unpark() to park() become available in an unspecified manner -
>>> only total ordering of events around these calls in Java land matters), a
>>> volatile load of signal would not be allowed to appear to anyone ahead of
>>> park() nor any part thereof - including the claim of the permit. So if
>>> unpark() was already entered, the volatile load would observe the volatile
>>> store of signal. Or, if unpark was not yet entered, then whatever the load
>>> of signal observes, it can at least go to park() again until it either
>>> observes a permit from unpark(), or the volatile store, whichever becomes
>>> observed first.
>>>
>>> Doug's clarification is vague enough to permit similar interpretation of
>>> how volatile load is different from normal loads w.r.t. apparent reordering
>>> with parts of park().
>>>
>>> Alex
>>>
>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>
>>> Could you elaborate? Based on the conversation thus far, I don't see how
>>> compiler is allowed to hoist/remove the (re)load of signal on the loop
>>> iteration.
>>>
>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>> No, this code doesn't have to terminate.
>>>>
>>>> Alex
>>>>
>>>>
>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>
>>>>> Hello Alex,
>>>>>
>>>>> This is what I'm saying:
>>>>>
>>>>> the below code is slightly (but importantly different than our
>>>>> canonical
>>>>> example).
>>>>>
>>>>>
>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>
>>>>>
>>>>> Thread 1:
>>>>> while (! signal)
>>>>>       park();
>>>>>
>>>>> Thread 2:
>>>>> signal = true;
>>>>> unpark(Thread1)
>>>>>
>>>>>
>>>>>
>>>>> Must this program terminate?  YES.
>>>>>
>>>>>
>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150114/d43ce8a3/attachment.html>

From jsampson at guidewire.com  Wed Jan 14 20:57:27 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 01:57:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B714E2.7020004@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83566@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko:

> If park() is seen as a synchronization event, it will still
> require volatile accesses to establish a hb (it will only stop
> reordering of volatile accesses).
>
> If park() is seen as a very particular synchronization event that
> creates a hb edge, then obviously normal reads should observe
> normal writes and it should terminate, but this is not what people
> write, and I don't know if that is what is intended (eg see the
> objections - people didn't think mentioning hb edge was all that
> useful).

I'm one of those who is of the opinion that a proper happens-before
edge is not necessary or terribly useful, as long as _some_ kind of
ordering rule related to synchronization order is specified. Or
restated in terms of reordering, all that's _necessary_ for park()
and unpark() to be useful is that they aren't reordered with
_volatile_ reads and writes. I embarked on that line of reasoning
partly in response to David's assertion early in the discussion that
"it would have been an excessive burden to require HB for the
unpark/park themselves." I was curious what the minimum required
ordering semantics might be.

But Doug has asserted, and David has agreed, that there is, in fact,
a proper happens-before edge from unpark() to park(). These examples
and your concerns about reordering the inner workings of park() are
making me even more convinced that stating the rule in terms of how
the permit is handled is the way to go, so I'll restate my latest
wording proposal so it doesn't get lost:

"Memory consistency effects: Any unpark call that releases a permit
*or* observes that a permit is already available _happens-before_
the return of the park call that acquires that particular released
or observed permit."

Without _any_ ordering semantics, even the volatile-state version
might not terminate. With _minimal_ ordering semantics, the volatile
version terminates but the non-volatile version might not. With full
happens-before semantics, even the non-volatile-state example is
guaranteed to terminate.

Cheers,
Justin

P.S. I get called Jason all the time, so no worries, Vitaly. :)


From thurston at nomagicsoftware.com  Thu Jan 15 00:59:16 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 14 Jan 2015 22:59:16 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37Gp5N0WBYCkLSHA+UujkWkX=hkiT3h4q_cmSeJ0E-L2CQ@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<1421281032985-11944.post@n7.nabble.com>
	<1421283316615-11947.post@n7.nabble.com>
	<CAHjP37Gp5N0WBYCkLSHA+UujkWkX=hkiT3h4q_cmSeJ0E-L2CQ@mail.gmail.com>
Message-ID: <1421301556154-11954.post@n7.nabble.com>

Got it.

In the end I guess I just come to:
say you're charged with implementing a native park/unpark and part of that
task is guaranteeing that (in conjunction with extant JVMs), that the hb
semantics of unpark/depark are effected.

How could you prevent a JVM from caching #signal?
JVM's don't know park/unpark from foo/goo - the JVM sees a loop based on the
read of a POSV - why wouldn't it cache it/hoist from loop?

But at the very same time - if you were implementing park/unpark in pure
*java*, let's say based on a
unpark: volatile write
park: wake up (somehow) and do a volatile read (establishing the hb at that
point),
of course we would conclude (I think) that the program must terminate.
It logically follows, that the JVM **not cache** the read of #signal in this
case.

But the JMM doesn't have anything to say about native methods vs. pure java
methods, and native methods are free to issue barriers of course (which is
how the JVM ensures release/acquire semantics for volatile write/read
pairs).

And of course I'm conflating two separate issues: what does the JMM say wrt
to the program and how possibly does the JVM effect it

It's a puzzle.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11954.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From kasperni at gmail.com  Thu Jan 15 04:01:46 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Thu, 15 Jan 2015 10:01:46 +0100
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B70F59.6050100@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu> <54B6C2F8.7070804@univ-mlv.fr>
	<54B70F59.6050100@cs.oswego.edu>
Message-ID: <CAPs6151CS8_dopj+8yRrG56+KqEA50eUvTtdHjNfK8hN0h2Dsg@mail.gmail.com>

>
> On 01/14/2015 05:12 PM, Kasper Nielsen wrote:
>
>> public static <U> CompletionStage<U> completedStage(U value)
>> public static <U> CompletableFuture<U> failedStage(Throwable ex)
>>
>
>  I think it makes more sense to have these methods on CompletionStage
>> instead
>> of CompletionFuture?
>>
>
> It would, but static methods are not allowed in interfaces.


Since Java 8 they are.

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/c9416148/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 08:26:45 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 13:26:45 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
Message-ID: <54B7C015.4030804@oracle.com>

No, I object.

If you permit normal writes to be visible in program order from 
unpark(), then no-op cannot be a permitted implementation of park() - 
since that would imply any program order should be visible in that order 
after park/unpark.

Alex

On 15/01/2015 01:26, Vitaly Davidovich wrote:
>
> My interpretation is pretty much what Jason said earlier; loop 
> definitively terminates (eventually) but thread 1 may not see other 
> writes if it never enters the loop.  That seems consistent with Doug's 
> clarification.
>
> sent from my phone
>
> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     If park() is seen as a synchronization event, it will still
>     require volatile accesses to establish a hb (it will only stop
>     reordering of volatile accesses).
>
>     If park() is seen as a very particular synchronization event that
>     creates a hb edge, then obviously normal reads should observe
>     normal writes and it should terminate, but this is not what people
>     write, and I don't know if that is what is intended (eg see the
>     objections - people didn't think mentioning hb edge was all that
>     useful).
>
>     Alex
>
>     On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>
>>     I don't think subsequent load of signal is allowed to appear
>>     before park () because it was said earlier that park () acts like
>>     a volatile load; if so, subsequent loads cannot move before it. 
>>     Did I misunderstand?
>>
>>     On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         It is not the hoist / remove of reload, it is the order of
>>         reload and what might be inside park().
>>
>>         Normal load of signal of the subsequent iteration is allowed
>>         to appear ahead of whatever is inside park() of the previous
>>         iteration, as it is unspecified. The way Doug has clarified
>>         (or the way I wanted to clarify it), the normal load of
>>         signal can still appear ahead of whatever except the start of
>>         park() to some threads. park() is allowed to claim the permit
>>         in an unspecified way. So, it is perfectly fine for park() to
>>         start before signal==true is apparent, the subsequent read of
>>         signal is allowed to appear to others ahead of anything else
>>         in park() - ie it is allowed to appear right after the
>>         decision that it doesn't need to suspend. It observes
>>         signal==false. Now it is allowed for Thread 2's store to
>>         signal to become apparent to Thread 1 (but it is not loading
>>         it anymore), and unpark() is allowed to release one permit.
>>         Now it is allowed for Thread 1 to claim that permit (or even
>>         clear the permit count without observing that it was set).
>>         Now Thread 1's while loop thinks signal==false, and it goes
>>         to call park() again, and hangs, because no more unpark() is
>>         called.
>>
>>         If park() is deemed as a singular synchronization event (I
>>         don't even require a happens-before, so no extra edges
>>         appear; visibility of permits issued by unpark() to park()
>>         become available in an unspecified manner - only total
>>         ordering of events around these calls in Java land matters),
>>         a volatile load of signal would not be allowed to appear to
>>         anyone ahead of park() nor any part thereof - including the
>>         claim of the permit. So if unpark() was already entered, the
>>         volatile load would observe the volatile store of signal. Or,
>>         if unpark was not yet entered, then whatever the load of
>>         signal observes, it can at least go to park() again until it
>>         either observes a permit from unpark(), or the volatile
>>         store, whichever becomes observed first.
>>
>>         Doug's clarification is vague enough to permit similar
>>         interpretation of how volatile load is different from normal
>>         loads w.r.t. apparent reordering with parts of park().
>>
>>         Alex
>>
>>         On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>         Could you elaborate? Based on the conversation thus far, I
>>>         don't see how compiler is allowed to hoist/remove the
>>>         (re)load of signal on the loop iteration.
>>>
>>>         On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             No, this code doesn't have to terminate.
>>>
>>>             Alex
>>>
>>>
>>>             On 14/01/2015 20:03, thurstonn wrote:
>>>
>>>                 Hello Alex,
>>>
>>>                 This is what I'm saying:
>>>
>>>                 the below code is slightly (but importantly
>>>                 different than our canonical
>>>                 example).
>>>
>>>
>>>                 boolean signal = false;  // POSV ==> Plain Old
>>>                 Shared Variable
>>>
>>>
>>>                 Thread 1:
>>>                 while (! signal)
>>>                       park();
>>>
>>>                 Thread 2:
>>>                 signal = true;
>>>                 unpark(Thread1)
>>>
>>>
>>>
>>>                 Must this program terminate?  YES.
>>>
>>>
>>>                 Note - I am not advising this code, or trying to be
>>>                 pedantic
>>>
>>>
>>>
>>>
>>>
>>>                 --
>>>                 View this message in context:
>>>                 http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>                 Sent from the JSR166 Concurrency mailing list
>>>                 archive at Nabble.com.
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/fed7e8d6/attachment.html>

From vitalyd at gmail.com  Thu Jan 15 09:07:59 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 15 Jan 2015 09:07:59 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7C015.4030804@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
Message-ID: <CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>

Right, I agree that it can't be a noop.

sent from my phone
On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  No, I object.
>
> If you permit normal writes to be visible in program order from unpark(),
> then no-op cannot be a permitted implementation of park() - since that
> would imply any program order should be visible in that order after
> park/unpark.
>
> Alex
>
> On 15/01/2015 01:26, Vitaly Davidovich wrote:
>
> My interpretation is pretty much what Jason said earlier; loop
> definitively terminates (eventually) but thread 1 may not see other writes
> if it never enters the loop.  That seems consistent with Doug's
> clarification.
>
> sent from my phone
> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  If park() is seen as a synchronization event, it will still require
>> volatile accesses to establish a hb (it will only stop reordering of
>> volatile accesses).
>>
>> If park() is seen as a very particular synchronization event that creates
>> a hb edge, then obviously normal reads should observe normal writes and it
>> should terminate, but this is not what people write, and I don't know if
>> that is what is intended (eg see the objections - people didn't think
>> mentioning hb edge was all that useful).
>>
>> Alex
>>
>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>
>> I don't think subsequent load of signal is allowed to appear before park
>> () because it was said earlier that park () acts like a volatile load; if
>> so, subsequent loads cannot move before it.  Did I misunderstand?
>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  It is not the hoist / remove of reload, it is the order of reload and
>>> what might be inside park().
>>>
>>> Normal load of signal of the subsequent iteration is allowed to appear
>>> ahead of whatever is inside park() of the previous iteration, as it is
>>> unspecified. The way Doug has clarified (or the way I wanted to clarify
>>> it), the normal load of signal can still appear ahead of whatever except
>>> the start of park() to some threads. park() is allowed to claim the permit
>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>> signal==true is apparent, the subsequent read of signal is allowed to
>>> appear to others ahead of anything else in park() - ie it is allowed to
>>> appear right after the decision that it doesn't need to suspend. It
>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>> to claim that permit (or even clear the permit count without observing that
>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>> call park() again, and hangs, because no more unpark() is called.
>>>
>>> If park() is deemed as a singular synchronization event (I don't even
>>> require a happens-before, so no extra edges appear; visibility of permits
>>> issued by unpark() to park() become available in an unspecified manner -
>>> only total ordering of events around these calls in Java land matters), a
>>> volatile load of signal would not be allowed to appear to anyone ahead of
>>> park() nor any part thereof - including the claim of the permit. So if
>>> unpark() was already entered, the volatile load would observe the volatile
>>> store of signal. Or, if unpark was not yet entered, then whatever the load
>>> of signal observes, it can at least go to park() again until it either
>>> observes a permit from unpark(), or the volatile store, whichever becomes
>>> observed first.
>>>
>>> Doug's clarification is vague enough to permit similar interpretation of
>>> how volatile load is different from normal loads w.r.t. apparent reordering
>>> with parts of park().
>>>
>>> Alex
>>>
>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>
>>> Could you elaborate? Based on the conversation thus far, I don't see how
>>> compiler is allowed to hoist/remove the (re)load of signal on the loop
>>> iteration.
>>>
>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>> No, this code doesn't have to terminate.
>>>>
>>>> Alex
>>>>
>>>>
>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>
>>>>> Hello Alex,
>>>>>
>>>>> This is what I'm saying:
>>>>>
>>>>> the below code is slightly (but importantly different than our
>>>>> canonical
>>>>> example).
>>>>>
>>>>>
>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>
>>>>>
>>>>> Thread 1:
>>>>> while (! signal)
>>>>>       park();
>>>>>
>>>>> Thread 2:
>>>>> signal = true;
>>>>> unpark(Thread1)
>>>>>
>>>>>
>>>>>
>>>>> Must this program terminate?  YES.
>>>>>
>>>>>
>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/5167b5af/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 09:24:27 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 14:24:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
Message-ID: <54B7CD9B.1070908@oracle.com>

I don't. It should be allowed to be almost anything, even a no-op. It is 
only an optimization of the busy wait, and it is ok to not optimize, or 
just have a micro-pause that doesn't communicate with unpark. What it 
cannot be - it cannot be an implementation that may hang indefinitely, 
if appears after unpark (for some definition of "after"), and can't 
permit an implementation that doesn't establish hb between volatiles 
(before and after the unpark and park), if park appears after unpark 
(again, for some definition of "before" and "after"). It is this 
reference to "before" and "after" that require to relate it somehow to 
the synchronization order. Existence of JMM hb edge between unpark and 
park is not necessary.

Also, read again how Doug specified park - it is not just a volatile 
read, but also a write. Let's not pretend the specification like that is 
perfect, or final, or won't be retracted, but that specification still 
permits the behaviour I described - the hang may still occur, because 
the normal read of signal from the next iteration is allowed to "go 
ahead" of the volatile write of the "volatile variable" shared with 
unpark, from the park of the previous iteration. This is not so for 
volatile reads.

Alex

On 15/01/2015 14:07, Vitaly Davidovich wrote:
>
> Right, I agree that it can't be a noop.
>
> sent from my phone
>
> On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, I object.
>
>     If you permit normal writes to be visible in program order from
>     unpark(), then no-op cannot be a permitted implementation of
>     park() - since that would imply any program order should be
>     visible in that order after park/unpark.
>
>     Alex
>
>     On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>
>>     My interpretation is pretty much what Jason said earlier; loop
>>     definitively terminates (eventually) but thread 1 may not see
>>     other writes if it never enters the loop.  That seems consistent
>>     with Doug's clarification.
>>
>>     sent from my phone
>>
>>     On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         If park() is seen as a synchronization event, it will still
>>         require volatile accesses to establish a hb (it will only
>>         stop reordering of volatile accesses).
>>
>>         If park() is seen as a very particular synchronization event
>>         that creates a hb edge, then obviously normal reads should
>>         observe normal writes and it should terminate, but this is
>>         not what people write, and I don't know if that is what is
>>         intended (eg see the objections - people didn't think
>>         mentioning hb edge was all that useful).
>>
>>         Alex
>>
>>         On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>
>>>         I don't think subsequent load of signal is allowed to appear
>>>         before park () because it was said earlier that park () acts
>>>         like a volatile load; if so, subsequent loads cannot move
>>>         before it.  Did I misunderstand?
>>>
>>>         On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             It is not the hoist / remove of reload, it is the order
>>>             of reload and what might be inside park().
>>>
>>>             Normal load of signal of the subsequent iteration is
>>>             allowed to appear ahead of whatever is inside park() of
>>>             the previous iteration, as it is unspecified. The way
>>>             Doug has clarified (or the way I wanted to clarify it),
>>>             the normal load of signal can still appear ahead of
>>>             whatever except the start of park() to some threads.
>>>             park() is allowed to claim the permit in an unspecified
>>>             way. So, it is perfectly fine for park() to start before
>>>             signal==true is apparent, the subsequent read of signal
>>>             is allowed to appear to others ahead of anything else in
>>>             park() - ie it is allowed to appear right after the
>>>             decision that it doesn't need to suspend. It observes
>>>             signal==false. Now it is allowed for Thread 2's store to
>>>             signal to become apparent to Thread 1 (but it is not
>>>             loading it anymore), and unpark() is allowed to release
>>>             one permit. Now it is allowed for Thread 1 to claim that
>>>             permit (or even clear the permit count without observing
>>>             that it was set). Now Thread 1's while loop thinks
>>>             signal==false, and it goes to call park() again, and
>>>             hangs, because no more unpark() is called.
>>>
>>>             If park() is deemed as a singular synchronization event
>>>             (I don't even require a happens-before, so no extra
>>>             edges appear; visibility of permits issued by unpark()
>>>             to park() become available in an unspecified manner -
>>>             only total ordering of events around these calls in Java
>>>             land matters), a volatile load of signal would not be
>>>             allowed to appear to anyone ahead of park() nor any part
>>>             thereof - including the claim of the permit. So if
>>>             unpark() was already entered, the volatile load would
>>>             observe the volatile store of signal. Or, if unpark was
>>>             not yet entered, then whatever the load of signal
>>>             observes, it can at least go to park() again until it
>>>             either observes a permit from unpark(), or the volatile
>>>             store, whichever becomes observed first.
>>>
>>>             Doug's clarification is vague enough to permit similar
>>>             interpretation of how volatile load is different from
>>>             normal loads w.r.t. apparent reordering with parts of
>>>             park().
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>             Could you elaborate? Based on the conversation thus
>>>>             far, I don't see how compiler is allowed to
>>>>             hoist/remove the (re)load of signal on the loop iteration.
>>>>
>>>>             On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 No, this code doesn't have to terminate.
>>>>
>>>>                 Alex
>>>>
>>>>
>>>>                 On 14/01/2015 20:03, thurstonn wrote:
>>>>
>>>>                     Hello Alex,
>>>>
>>>>                     This is what I'm saying:
>>>>
>>>>                     the below code is slightly (but importantly
>>>>                     different than our canonical
>>>>                     example).
>>>>
>>>>
>>>>                     boolean signal = false;  // POSV ==> Plain Old
>>>>                     Shared Variable
>>>>
>>>>
>>>>                     Thread 1:
>>>>                     while (! signal)
>>>>                           park();
>>>>
>>>>                     Thread 2:
>>>>                     signal = true;
>>>>                     unpark(Thread1)
>>>>
>>>>
>>>>
>>>>                     Must this program terminate? YES.
>>>>
>>>>
>>>>                     Note - I am not advising this code, or trying
>>>>                     to be pedantic
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>                     --
>>>>                     View this message in context:
>>>>                     http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>                     Sent from the JSR166 Concurrency mailing list
>>>>                     archive at Nabble.com.
>>>>                     _______________________________________________
>>>>                     Concurrency-interest mailing list
>>>>                     Concurrency-interest at cs.oswego.edu
>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/3bb77da1/attachment.html>

From vitalyd at gmail.com  Thu Jan 15 09:32:14 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 15 Jan 2015 09:32:14 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7CD9B.1070908@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
Message-ID: <CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>

Saying it's both a volatile read and write doesn't imply, to me at least,
that subsequent loads of signal can appear before park () of prior
iteration.  If park () had the semantics of *only* a volatile write, then
yes, but by having volatile load I don't see how that's valid.

sent from my phone
On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  I don't. It should be allowed to be almost anything, even a no-op. It is
> only an optimization of the busy wait, and it is ok to not optimize, or
> just have a micro-pause that doesn't communicate with unpark. What it
> cannot be - it cannot be an implementation that may hang indefinitely, if
> appears after unpark (for some definition of "after"), and can't permit an
> implementation that doesn't establish hb between volatiles (before and
> after the unpark and park), if park appears after unpark (again, for some
> definition of "before" and "after"). It is this reference to "before" and
> "after" that require to relate it somehow to the synchronization order.
> Existence of JMM hb edge between unpark and park is not necessary.
>
> Also, read again how Doug specified park - it is not just a volatile read,
> but also a write. Let's not pretend the specification like that is perfect,
> or final, or won't be retracted, but that specification still permits the
> behaviour I described - the hang may still occur, because the normal read
> of signal from the next iteration is allowed to "go ahead" of the volatile
> write of the "volatile variable" shared with unpark, from the park of the
> previous iteration. This is not so for volatile reads.
>
> Alex
>
> On 15/01/2015 14:07, Vitaly Davidovich wrote:
>
> Right, I agree that it can't be a noop.
>
> sent from my phone
> On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  No, I object.
>>
>> If you permit normal writes to be visible in program order from unpark(),
>> then no-op cannot be a permitted implementation of park() - since that
>> would imply any program order should be visible in that order after
>> park/unpark.
>>
>> Alex
>>
>> On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>
>> My interpretation is pretty much what Jason said earlier; loop
>> definitively terminates (eventually) but thread 1 may not see other writes
>> if it never enters the loop.  That seems consistent with Doug's
>> clarification.
>>
>> sent from my phone
>> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  If park() is seen as a synchronization event, it will still require
>>> volatile accesses to establish a hb (it will only stop reordering of
>>> volatile accesses).
>>>
>>> If park() is seen as a very particular synchronization event that
>>> creates a hb edge, then obviously normal reads should observe normal writes
>>> and it should terminate, but this is not what people write, and I don't
>>> know if that is what is intended (eg see the objections - people didn't
>>> think mentioning hb edge was all that useful).
>>>
>>> Alex
>>>
>>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>
>>> I don't think subsequent load of signal is allowed to appear before park
>>> () because it was said earlier that park () acts like a volatile load; if
>>> so, subsequent loads cannot move before it.  Did I misunderstand?
>>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>>  It is not the hoist / remove of reload, it is the order of reload and
>>>> what might be inside park().
>>>>
>>>> Normal load of signal of the subsequent iteration is allowed to appear
>>>> ahead of whatever is inside park() of the previous iteration, as it is
>>>> unspecified. The way Doug has clarified (or the way I wanted to clarify
>>>> it), the normal load of signal can still appear ahead of whatever except
>>>> the start of park() to some threads. park() is allowed to claim the permit
>>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>>> signal==true is apparent, the subsequent read of signal is allowed to
>>>> appear to others ahead of anything else in park() - ie it is allowed to
>>>> appear right after the decision that it doesn't need to suspend. It
>>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>>> to claim that permit (or even clear the permit count without observing that
>>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>>> call park() again, and hangs, because no more unpark() is called.
>>>>
>>>> If park() is deemed as a singular synchronization event (I don't even
>>>> require a happens-before, so no extra edges appear; visibility of permits
>>>> issued by unpark() to park() become available in an unspecified manner -
>>>> only total ordering of events around these calls in Java land matters), a
>>>> volatile load of signal would not be allowed to appear to anyone ahead of
>>>> park() nor any part thereof - including the claim of the permit. So if
>>>> unpark() was already entered, the volatile load would observe the volatile
>>>> store of signal. Or, if unpark was not yet entered, then whatever the load
>>>> of signal observes, it can at least go to park() again until it either
>>>> observes a permit from unpark(), or the volatile store, whichever becomes
>>>> observed first.
>>>>
>>>> Doug's clarification is vague enough to permit similar interpretation
>>>> of how volatile load is different from normal loads w.r.t. apparent
>>>> reordering with parts of park().
>>>>
>>>> Alex
>>>>
>>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>
>>>> Could you elaborate? Based on the conversation thus far, I don't see
>>>> how compiler is allowed to hoist/remove the (re)load of signal on the loop
>>>> iteration.
>>>>
>>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>> No, this code doesn't have to terminate.
>>>>>
>>>>> Alex
>>>>>
>>>>>
>>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>>
>>>>>> Hello Alex,
>>>>>>
>>>>>> This is what I'm saying:
>>>>>>
>>>>>> the below code is slightly (but importantly different than our
>>>>>> canonical
>>>>>> example).
>>>>>>
>>>>>>
>>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>>
>>>>>>
>>>>>> Thread 1:
>>>>>> while (! signal)
>>>>>>       park();
>>>>>>
>>>>>> Thread 2:
>>>>>> signal = true;
>>>>>> unpark(Thread1)
>>>>>>
>>>>>>
>>>>>>
>>>>>> Must this program terminate?  YES.
>>>>>>
>>>>>>
>>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> View this message in context:
>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/24ac0999/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 09:58:02 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 14:58:02 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
Message-ID: <54B7D57A.6000703@oracle.com>

I've explained the steps. You don't need the normal load of signal to 
overtake the whole park() - only the volatile write inside the park(). 
This is enough for signal to read the value before unpark() starts, for 
volatile write in park() to clear the permits issued by unpark(), and 
for all this to work properly, if signal is volatile.


Alex


On 15/01/2015 14:32, Vitaly Davidovich wrote:
>
> Saying it's both a volatile read and write doesn't imply, to me at 
> least, that subsequent loads of signal can appear before park () of 
> prior iteration.  If park () had the semantics of *only* a volatile 
> write, then yes, but by having volatile load I don't see how that's valid.
>
> sent from my phone
>
> On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I don't. It should be allowed to be almost anything, even a no-op.
>     It is only an optimization of the busy wait, and it is ok to not
>     optimize, or just have a micro-pause that doesn't communicate with
>     unpark. What it cannot be - it cannot be an implementation that
>     may hang indefinitely, if appears after unpark (for some
>     definition of "after"), and can't permit an implementation that
>     doesn't establish hb between volatiles (before and after the
>     unpark and park), if park appears after unpark (again, for some
>     definition of "before" and "after"). It is this reference to
>     "before" and "after" that require to relate it somehow to the
>     synchronization order. Existence of JMM hb edge between unpark and
>     park is not necessary.
>
>     Also, read again how Doug specified park - it is not just a
>     volatile read, but also a write. Let's not pretend the
>     specification like that is perfect, or final, or won't be
>     retracted, but that specification still permits the behaviour I
>     described - the hang may still occur, because the normal read of
>     signal from the next iteration is allowed to "go ahead" of the
>     volatile write of the "volatile variable" shared with unpark, from
>     the park of the previous iteration. This is not so for volatile reads.
>
>     Alex
>
>     On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>
>>     Right, I agree that it can't be a noop.
>>
>>     sent from my phone
>>
>>     On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, I object.
>>
>>         If you permit normal writes to be visible in program order
>>         from unpark(), then no-op cannot be a permitted
>>         implementation of park() - since that would imply any program
>>         order should be visible in that order after park/unpark.
>>
>>         Alex
>>
>>         On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>
>>>         My interpretation is pretty much what Jason said earlier;
>>>         loop definitively terminates (eventually) but thread 1 may
>>>         not see other writes if it never enters the loop.  That
>>>         seems consistent with Doug's clarification.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             If park() is seen as a synchronization event, it will
>>>             still require volatile accesses to establish a hb (it
>>>             will only stop reordering of volatile accesses).
>>>
>>>             If park() is seen as a very particular synchronization
>>>             event that creates a hb edge, then obviously normal
>>>             reads should observe normal writes and it should
>>>             terminate, but this is not what people write, and I
>>>             don't know if that is what is intended (eg see the
>>>             objections - people didn't think mentioning hb edge was
>>>             all that useful).
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>
>>>>             I don't think subsequent load of signal is allowed to
>>>>             appear before park () because it was said earlier that
>>>>             park () acts like a volatile load; if so, subsequent
>>>>             loads cannot move before it.  Did I misunderstand?
>>>>
>>>>             On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 It is not the hoist / remove of reload, it is the
>>>>                 order of reload and what might be inside park().
>>>>
>>>>                 Normal load of signal of the subsequent iteration
>>>>                 is allowed to appear ahead of whatever is inside
>>>>                 park() of the previous iteration, as it is
>>>>                 unspecified. The way Doug has clarified (or the way
>>>>                 I wanted to clarify it), the normal load of signal
>>>>                 can still appear ahead of whatever except the start
>>>>                 of park() to some threads. park() is allowed to
>>>>                 claim the permit in an unspecified way. So, it is
>>>>                 perfectly fine for park() to start before
>>>>                 signal==true is apparent, the subsequent read of
>>>>                 signal is allowed to appear to others ahead of
>>>>                 anything else in park() - ie it is allowed to
>>>>                 appear right after the decision that it doesn't
>>>>                 need to suspend. It observes signal==false. Now it
>>>>                 is allowed for Thread 2's store to signal to become
>>>>                 apparent to Thread 1 (but it is not loading it
>>>>                 anymore), and unpark() is allowed to release one
>>>>                 permit. Now it is allowed for Thread 1 to claim
>>>>                 that permit (or even clear the permit count without
>>>>                 observing that it was set). Now Thread 1's while
>>>>                 loop thinks signal==false, and it goes to call
>>>>                 park() again, and hangs, because no more unpark()
>>>>                 is called.
>>>>
>>>>                 If park() is deemed as a singular synchronization
>>>>                 event (I don't even require a happens-before, so no
>>>>                 extra edges appear; visibility of permits issued by
>>>>                 unpark() to park() become available in an
>>>>                 unspecified manner - only total ordering of events
>>>>                 around these calls in Java land matters), a
>>>>                 volatile load of signal would not be allowed to
>>>>                 appear to anyone ahead of park() nor any part
>>>>                 thereof - including the claim of the permit. So if
>>>>                 unpark() was already entered, the volatile load
>>>>                 would observe the volatile store of signal. Or, if
>>>>                 unpark was not yet entered, then whatever the load
>>>>                 of signal observes, it can at least go to park()
>>>>                 again until it either observes a permit from
>>>>                 unpark(), or the volatile store, whichever becomes
>>>>                 observed first.
>>>>
>>>>                 Doug's clarification is vague enough to permit
>>>>                 similar interpretation of how volatile load is
>>>>                 different from normal loads w.r.t. apparent
>>>>                 reordering with parts of park().
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>                 Could you elaborate? Based on the conversation
>>>>>                 thus far, I don't see how compiler is allowed to
>>>>>                 hoist/remove the (re)load of signal on the loop
>>>>>                 iteration.
>>>>>
>>>>>                 On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     No, this code doesn't have to terminate.
>>>>>
>>>>>                     Alex
>>>>>
>>>>>
>>>>>                     On 14/01/2015 20:03, thurstonn wrote:
>>>>>
>>>>>                         Hello Alex,
>>>>>
>>>>>                         This is what I'm saying:
>>>>>
>>>>>                         the below code is slightly (but
>>>>>                         importantly different than our canonical
>>>>>                         example).
>>>>>
>>>>>
>>>>>                         boolean signal = false;  // POSV ==> Plain
>>>>>                         Old Shared Variable
>>>>>
>>>>>
>>>>>                         Thread 1:
>>>>>                         while (! signal)
>>>>>                               park();
>>>>>
>>>>>                         Thread 2:
>>>>>                         signal = true;
>>>>>                         unpark(Thread1)
>>>>>
>>>>>
>>>>>
>>>>>                         Must this program terminate?  YES.
>>>>>
>>>>>
>>>>>                         Note - I am not advising this code, or
>>>>>                         trying to be pedantic
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>                         --
>>>>>                         View this message in context:
>>>>>                         http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>                         Sent from the JSR166 Concurrency mailing
>>>>>                         list archive at Nabble.com.
>>>>>                         _______________________________________________
>>>>>                         Concurrency-interest mailing list
>>>>>                         Concurrency-interest at cs.oswego.edu
>>>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>                     _______________________________________________
>>>>>                     Concurrency-interest mailing list
>>>>>                     Concurrency-interest at cs.oswego.edu
>>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/f66d3d4d/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 10:04:07 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 15:04:07 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7D57A.6000703@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
Message-ID: <54B7D6E7.70405@oracle.com>

Also, I hate the discussion in terms of reorderings, because here we are 
in the mode of discussing what optimizations may lead to that, what is 
possible, etc.

But look at synchronization order. It is possible for volatile read and 
two writes of the variable shared by park/unpark to appear like so:

T2 park loads
T1 stores signal
T1 unpark stores
T2 park stores
T2 loads signal

There is no hb to stores signal, so any value of signal is possible. 
There is no requirement for park to wake up on the next iteration, 
because its store appears after unpark store, so hang is also possible.

Alex


On 15/01/2015 14:58, Oleksandr Otenko wrote:
> I've explained the steps. You don't need the normal load of signal to 
> overtake the whole park() - only the volatile write inside the park(). 
> This is enough for signal to read the value before unpark() starts, 
> for volatile write in park() to clear the permits issued by unpark(), 
> and for all this to work properly, if signal is volatile.
>
>
> Alex
>
>
> On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>
>> Saying it's both a volatile read and write doesn't imply, to me at 
>> least, that subsequent loads of signal can appear before park () of 
>> prior iteration.  If park () had the semantics of *only* a volatile 
>> write, then yes, but by having volatile load I don't see how that's 
>> valid.
>>
>> sent from my phone
>>
>> On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     I don't. It should be allowed to be almost anything, even a
>>     no-op. It is only an optimization of the busy wait, and it is ok
>>     to not optimize, or just have a micro-pause that doesn't
>>     communicate with unpark. What it cannot be - it cannot be an
>>     implementation that may hang indefinitely, if appears after
>>     unpark (for some definition of "after"), and can't permit an
>>     implementation that doesn't establish hb between volatiles
>>     (before and after the unpark and park), if park appears after
>>     unpark (again, for some definition of "before" and "after"). It
>>     is this reference to "before" and "after" that require to relate
>>     it somehow to the synchronization order. Existence of JMM hb edge
>>     between unpark and park is not necessary.
>>
>>     Also, read again how Doug specified park - it is not just a
>>     volatile read, but also a write. Let's not pretend the
>>     specification like that is perfect, or final, or won't be
>>     retracted, but that specification still permits the behaviour I
>>     described - the hang may still occur, because the normal read of
>>     signal from the next iteration is allowed to "go ahead" of the
>>     volatile write of the "volatile variable" shared with unpark,
>>     from the park of the previous iteration. This is not so for
>>     volatile reads.
>>
>>     Alex
>>
>>     On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>
>>>     Right, I agree that it can't be a noop.
>>>
>>>     sent from my phone
>>>
>>>     On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>>     <oleksandr.otenko at oracle.com
>>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>         No, I object.
>>>
>>>         If you permit normal writes to be visible in program order
>>>         from unpark(), then no-op cannot be a permitted
>>>         implementation of park() - since that would imply any
>>>         program order should be visible in that order after park/unpark.
>>>
>>>         Alex
>>>
>>>         On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>
>>>>         My interpretation is pretty much what Jason said earlier;
>>>>         loop definitively terminates (eventually) but thread 1 may
>>>>         not see other writes if it never enters the loop. That
>>>>         seems consistent with Doug's clarification.
>>>>
>>>>         sent from my phone
>>>>
>>>>         On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>>         <oleksandr.otenko at oracle.com
>>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>             If park() is seen as a synchronization event, it will
>>>>             still require volatile accesses to establish a hb (it
>>>>             will only stop reordering of volatile accesses).
>>>>
>>>>             If park() is seen as a very particular synchronization
>>>>             event that creates a hb edge, then obviously normal
>>>>             reads should observe normal writes and it should
>>>>             terminate, but this is not what people write, and I
>>>>             don't know if that is what is intended (eg see the
>>>>             objections - people didn't think mentioning hb edge was
>>>>             all that useful).
>>>>
>>>>             Alex
>>>>
>>>>             On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>
>>>>>             I don't think subsequent load of signal is allowed to
>>>>>             appear before park () because it was said earlier that
>>>>>             park () acts like a volatile load; if so, subsequent
>>>>>             loads cannot move before it.  Did I misunderstand?
>>>>>
>>>>>             On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>>             <oleksandr.otenko at oracle.com
>>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                 It is not the hoist / remove of reload, it is the
>>>>>                 order of reload and what might be inside park().
>>>>>
>>>>>                 Normal load of signal of the subsequent iteration
>>>>>                 is allowed to appear ahead of whatever is inside
>>>>>                 park() of the previous iteration, as it is
>>>>>                 unspecified. The way Doug has clarified (or the
>>>>>                 way I wanted to clarify it), the normal load of
>>>>>                 signal can still appear ahead of whatever except
>>>>>                 the start of park() to some threads. park() is
>>>>>                 allowed to claim the permit in an unspecified way.
>>>>>                 So, it is perfectly fine for park() to start
>>>>>                 before signal==true is apparent, the subsequent
>>>>>                 read of signal is allowed to appear to others
>>>>>                 ahead of anything else in park() - ie it is
>>>>>                 allowed to appear right after the decision that it
>>>>>                 doesn't need to suspend. It observes
>>>>>                 signal==false. Now it is allowed for Thread 2's
>>>>>                 store to signal to become apparent to Thread 1
>>>>>                 (but it is not loading it anymore), and unpark()
>>>>>                 is allowed to release one permit. Now it is
>>>>>                 allowed for Thread 1 to claim that permit (or even
>>>>>                 clear the permit count without observing that it
>>>>>                 was set). Now Thread 1's while loop thinks
>>>>>                 signal==false, and it goes to call park() again,
>>>>>                 and hangs, because no more unpark() is called.
>>>>>
>>>>>                 If park() is deemed as a singular synchronization
>>>>>                 event (I don't even require a happens-before, so
>>>>>                 no extra edges appear; visibility of permits
>>>>>                 issued by unpark() to park() become available in
>>>>>                 an unspecified manner - only total ordering of
>>>>>                 events around these calls in Java land matters), a
>>>>>                 volatile load of signal would not be allowed to
>>>>>                 appear to anyone ahead of park() nor any part
>>>>>                 thereof - including the claim of the permit. So if
>>>>>                 unpark() was already entered, the volatile load
>>>>>                 would observe the volatile store of signal. Or, if
>>>>>                 unpark was not yet entered, then whatever the load
>>>>>                 of signal observes, it can at least go to park()
>>>>>                 again until it either observes a permit from
>>>>>                 unpark(), or the volatile store, whichever becomes
>>>>>                 observed first.
>>>>>
>>>>>                 Doug's clarification is vague enough to permit
>>>>>                 similar interpretation of how volatile load is
>>>>>                 different from normal loads w.r.t. apparent
>>>>>                 reordering with parts of park().
>>>>>
>>>>>                 Alex
>>>>>
>>>>>                 On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>                 Could you elaborate? Based on the conversation
>>>>>>                 thus far, I don't see how compiler is allowed to
>>>>>>                 hoist/remove the (re)load of signal on the loop
>>>>>>                 iteration.
>>>>>>
>>>>>>                 On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>>>>>                 <oleksandr.otenko at oracle.com
>>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>
>>>>>>                     No, this code doesn't have to terminate.
>>>>>>
>>>>>>                     Alex
>>>>>>
>>>>>>
>>>>>>                     On 14/01/2015 20:03, thurstonn wrote:
>>>>>>
>>>>>>                         Hello Alex,
>>>>>>
>>>>>>                         This is what I'm saying:
>>>>>>
>>>>>>                         the below code is slightly (but
>>>>>>                         importantly different than our canonical
>>>>>>                         example).
>>>>>>
>>>>>>
>>>>>>                         boolean signal = false;  // POSV ==>
>>>>>>                         Plain Old Shared Variable
>>>>>>
>>>>>>
>>>>>>                         Thread 1:
>>>>>>                         while (! signal)
>>>>>>                               park();
>>>>>>
>>>>>>                         Thread 2:
>>>>>>                         signal = true;
>>>>>>                         unpark(Thread1)
>>>>>>
>>>>>>
>>>>>>
>>>>>>                         Must this program terminate?  YES.
>>>>>>
>>>>>>
>>>>>>                         Note - I am not advising this code, or
>>>>>>                         trying to be pedantic
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>                         --
>>>>>>                         View this message in context:
>>>>>>                         http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>                         Sent from the JSR166 Concurrency mailing
>>>>>>                         list archive at Nabble.com.
>>>>>>                         _______________________________________________
>>>>>>                         Concurrency-interest mailing list
>>>>>>                         Concurrency-interest at cs.oswego.edu
>>>>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>                     _______________________________________________
>>>>>>                     Concurrency-interest mailing list
>>>>>>                     Concurrency-interest at cs.oswego.edu
>>>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/e2a33167/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 15 10:09:56 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 15 Jan 2015 10:09:56 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7D57A.6000703@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
Message-ID: <CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>

Ok I think I know why we're going in circles.  I'm treating park and unpark
as black boxes and only caring about their memory ordering effects as a
whole - I don't care how it provides that, it can issue additional fences
inside to ensure its internal state isn't moved out of the call or external
operations move into it.  You seem to be "peeking" inside them and
rationalizing based on some implementation details.  So to me, it's sort of
like lock/unlock calls - all I know is how my operations are supposed to
interact across those method calls, I don't care how that's actually
provided.

sent from my phone
On Jan 15, 2015 9:58 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  I've explained the steps. You don't need the normal load of signal to
> overtake the whole park() - only the volatile write inside the park(). This
> is enough for signal to read the value before unpark() starts, for volatile
> write in park() to clear the permits issued by unpark(), and for all this
> to work properly, if signal is volatile.
>
>
> Alex
>
>
> On 15/01/2015 14:32, Vitaly Davidovich wrote:
>
> Saying it's both a volatile read and write doesn't imply, to me at least,
> that subsequent loads of signal can appear before park () of prior
> iteration.  If park () had the semantics of *only* a volatile write, then
> yes, but by having volatile load I don't see how that's valid.
>
> sent from my phone
> On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  I don't. It should be allowed to be almost anything, even a no-op. It is
>> only an optimization of the busy wait, and it is ok to not optimize, or
>> just have a micro-pause that doesn't communicate with unpark. What it
>> cannot be - it cannot be an implementation that may hang indefinitely, if
>> appears after unpark (for some definition of "after"), and can't permit an
>> implementation that doesn't establish hb between volatiles (before and
>> after the unpark and park), if park appears after unpark (again, for some
>> definition of "before" and "after"). It is this reference to "before" and
>> "after" that require to relate it somehow to the synchronization order.
>> Existence of JMM hb edge between unpark and park is not necessary.
>>
>> Also, read again how Doug specified park - it is not just a volatile
>> read, but also a write. Let's not pretend the specification like that is
>> perfect, or final, or won't be retracted, but that specification still
>> permits the behaviour I described - the hang may still occur, because the
>> normal read of signal from the next iteration is allowed to "go ahead" of
>> the volatile write of the "volatile variable" shared with unpark, from the
>> park of the previous iteration. This is not so for volatile reads.
>>
>> Alex
>>
>> On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>
>> Right, I agree that it can't be a noop.
>>
>> sent from my phone
>> On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  No, I object.
>>>
>>> If you permit normal writes to be visible in program order from
>>> unpark(), then no-op cannot be a permitted implementation of park() - since
>>> that would imply any program order should be visible in that order after
>>> park/unpark.
>>>
>>> Alex
>>>
>>> On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>
>>> My interpretation is pretty much what Jason said earlier; loop
>>> definitively terminates (eventually) but thread 1 may not see other writes
>>> if it never enters the loop.  That seems consistent with Doug's
>>> clarification.
>>>
>>> sent from my phone
>>> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>>  If park() is seen as a synchronization event, it will still require
>>>> volatile accesses to establish a hb (it will only stop reordering of
>>>> volatile accesses).
>>>>
>>>> If park() is seen as a very particular synchronization event that
>>>> creates a hb edge, then obviously normal reads should observe normal writes
>>>> and it should terminate, but this is not what people write, and I don't
>>>> know if that is what is intended (eg see the objections - people didn't
>>>> think mentioning hb edge was all that useful).
>>>>
>>>> Alex
>>>>
>>>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>
>>>> I don't think subsequent load of signal is allowed to appear before
>>>> park () because it was said earlier that park () acts like a volatile load;
>>>> if so, subsequent loads cannot move before it.  Did I misunderstand?
>>>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>>  It is not the hoist / remove of reload, it is the order of reload and
>>>>> what might be inside park().
>>>>>
>>>>> Normal load of signal of the subsequent iteration is allowed to appear
>>>>> ahead of whatever is inside park() of the previous iteration, as it is
>>>>> unspecified. The way Doug has clarified (or the way I wanted to clarify
>>>>> it), the normal load of signal can still appear ahead of whatever except
>>>>> the start of park() to some threads. park() is allowed to claim the permit
>>>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>>>> signal==true is apparent, the subsequent read of signal is allowed to
>>>>> appear to others ahead of anything else in park() - ie it is allowed to
>>>>> appear right after the decision that it doesn't need to suspend. It
>>>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>>>> to claim that permit (or even clear the permit count without observing that
>>>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>>>> call park() again, and hangs, because no more unpark() is called.
>>>>>
>>>>> If park() is deemed as a singular synchronization event (I don't even
>>>>> require a happens-before, so no extra edges appear; visibility of permits
>>>>> issued by unpark() to park() become available in an unspecified manner -
>>>>> only total ordering of events around these calls in Java land matters), a
>>>>> volatile load of signal would not be allowed to appear to anyone ahead of
>>>>> park() nor any part thereof - including the claim of the permit. So if
>>>>> unpark() was already entered, the volatile load would observe the volatile
>>>>> store of signal. Or, if unpark was not yet entered, then whatever the load
>>>>> of signal observes, it can at least go to park() again until it either
>>>>> observes a permit from unpark(), or the volatile store, whichever becomes
>>>>> observed first.
>>>>>
>>>>> Doug's clarification is vague enough to permit similar interpretation
>>>>> of how volatile load is different from normal loads w.r.t. apparent
>>>>> reordering with parts of park().
>>>>>
>>>>> Alex
>>>>>
>>>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>
>>>>> Could you elaborate? Based on the conversation thus far, I don't see
>>>>> how compiler is allowed to hoist/remove the (re)load of signal on the loop
>>>>> iteration.
>>>>>
>>>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>>> No, this code doesn't have to terminate.
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>>
>>>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>>>
>>>>>>> Hello Alex,
>>>>>>>
>>>>>>> This is what I'm saying:
>>>>>>>
>>>>>>> the below code is slightly (but importantly different than our
>>>>>>> canonical
>>>>>>> example).
>>>>>>>
>>>>>>>
>>>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>>>
>>>>>>>
>>>>>>> Thread 1:
>>>>>>> while (! signal)
>>>>>>>       park();
>>>>>>>
>>>>>>> Thread 2:
>>>>>>> signal = true;
>>>>>>> unpark(Thread1)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Must this program terminate?  YES.
>>>>>>>
>>>>>>>
>>>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> View this message in context:
>>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/0360b997/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 10:45:12 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 15:45:12 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
Message-ID: <54B7E088.40409@oracle.com>

Yes, that's a possible interpretation of Doug's wording, but since it 
doesn't mention atomicity, I am not treating them as atomic, and I don't 
see why I should - given the tradition to assume they are no-ops.

Alex

On 15/01/2015 15:09, Vitaly Davidovich wrote:
>
> Ok I think I know why we're going in circles.  I'm treating park and 
> unpark as black boxes and only caring about their memory ordering 
> effects as a whole - I don't care how it provides that, it can issue 
> additional fences inside to ensure its internal state isn't moved out 
> of the call or external operations move into it.  You seem to be 
> "peeking" inside them and rationalizing based on some implementation 
> details.  So to me, it's sort of like lock/unlock calls - all I know 
> is how my operations are supposed to interact across those method 
> calls, I don't care how that's actually provided.
>
> sent from my phone
>
> On Jan 15, 2015 9:58 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I've explained the steps. You don't need the normal load of signal
>     to overtake the whole park() - only the volatile write inside the
>     park(). This is enough for signal to read the value before
>     unpark() starts, for volatile write in park() to clear the permits
>     issued by unpark(), and for all this to work properly, if signal
>     is volatile.
>
>
>     Alex
>
>
>     On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>
>>     Saying it's both a volatile read and write doesn't imply, to me
>>     at least, that subsequent loads of signal can appear before park
>>     () of prior iteration.  If park () had the semantics of *only* a
>>     volatile write, then yes, but by having volatile load I don't see
>>     how that's valid.
>>
>>     sent from my phone
>>
>>     On Jan 15, 2015 9:24 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         I don't. It should be allowed to be almost anything, even a
>>         no-op. It is only an optimization of the busy wait, and it is
>>         ok to not optimize, or just have a micro-pause that doesn't
>>         communicate with unpark. What it cannot be - it cannot be an
>>         implementation that may hang indefinitely, if appears after
>>         unpark (for some definition of "after"), and can't permit an
>>         implementation that doesn't establish hb between volatiles
>>         (before and after the unpark and park), if park appears after
>>         unpark (again, for some definition of "before" and "after").
>>         It is this reference to "before" and "after" that require to
>>         relate it somehow to the synchronization order. Existence of
>>         JMM hb edge between unpark and park is not necessary.
>>
>>         Also, read again how Doug specified park - it is not just a
>>         volatile read, but also a write. Let's not pretend the
>>         specification like that is perfect, or final, or won't be
>>         retracted, but that specification still permits the behaviour
>>         I described - the hang may still occur, because the normal
>>         read of signal from the next iteration is allowed to "go
>>         ahead" of the volatile write of the "volatile variable"
>>         shared with unpark, from the park of the previous iteration.
>>         This is not so for volatile reads.
>>
>>         Alex
>>
>>         On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>
>>>         Right, I agree that it can't be a noop.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             No, I object.
>>>
>>>             If you permit normal writes to be visible in program
>>>             order from unpark(), then no-op cannot be a permitted
>>>             implementation of park() - since that would imply any
>>>             program order should be visible in that order after
>>>             park/unpark.
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>
>>>>             My interpretation is pretty much what Jason said
>>>>             earlier; loop definitively terminates (eventually) but
>>>>             thread 1 may not see other writes if it never enters
>>>>             the loop.  That seems consistent with Doug's clarification.
>>>>
>>>>             sent from my phone
>>>>
>>>>             On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 If park() is seen as a synchronization event, it
>>>>                 will still require volatile accesses to establish a
>>>>                 hb (it will only stop reordering of volatile accesses).
>>>>
>>>>                 If park() is seen as a very particular
>>>>                 synchronization event that creates a hb edge, then
>>>>                 obviously normal reads should observe normal writes
>>>>                 and it should terminate, but this is not what
>>>>                 people write, and I don't know if that is what is
>>>>                 intended (eg see the objections - people didn't
>>>>                 think mentioning hb edge was all that useful).
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>
>>>>>                 I don't think subsequent load of signal is allowed
>>>>>                 to appear before park () because it was said
>>>>>                 earlier that park () acts like a volatile load; if
>>>>>                 so, subsequent loads cannot move before it.  Did I
>>>>>                 misunderstand?
>>>>>
>>>>>                 On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     It is not the hoist / remove of reload, it is
>>>>>                     the order of reload and what might be inside
>>>>>                     park().
>>>>>
>>>>>                     Normal load of signal of the subsequent
>>>>>                     iteration is allowed to appear ahead of
>>>>>                     whatever is inside park() of the previous
>>>>>                     iteration, as it is unspecified. The way Doug
>>>>>                     has clarified (or the way I wanted to clarify
>>>>>                     it), the normal load of signal can still
>>>>>                     appear ahead of whatever except the start of
>>>>>                     park() to some threads. park() is allowed to
>>>>>                     claim the permit in an unspecified way. So, it
>>>>>                     is perfectly fine for park() to start before
>>>>>                     signal==true is apparent, the subsequent read
>>>>>                     of signal is allowed to appear to others ahead
>>>>>                     of anything else in park() - ie it is allowed
>>>>>                     to appear right after the decision that it
>>>>>                     doesn't need to suspend. It observes
>>>>>                     signal==false. Now it is allowed for Thread
>>>>>                     2's store to signal to become apparent to
>>>>>                     Thread 1 (but it is not loading it anymore),
>>>>>                     and unpark() is allowed to release one permit.
>>>>>                     Now it is allowed for Thread 1 to claim that
>>>>>                     permit (or even clear the permit count without
>>>>>                     observing that it was set). Now Thread 1's
>>>>>                     while loop thinks signal==false, and it goes
>>>>>                     to call park() again, and hangs, because no
>>>>>                     more unpark() is called.
>>>>>
>>>>>                     If park() is deemed as a singular
>>>>>                     synchronization event (I don't even require a
>>>>>                     happens-before, so no extra edges appear;
>>>>>                     visibility of permits issued by unpark() to
>>>>>                     park() become available in an unspecified
>>>>>                     manner - only total ordering of events around
>>>>>                     these calls in Java land matters), a volatile
>>>>>                     load of signal would not be allowed to appear
>>>>>                     to anyone ahead of park() nor any part thereof
>>>>>                     - including the claim of the permit. So if
>>>>>                     unpark() was already entered, the volatile
>>>>>                     load would observe the volatile store of
>>>>>                     signal. Or, if unpark was not yet entered,
>>>>>                     then whatever the load of signal observes, it
>>>>>                     can at least go to park() again until it
>>>>>                     either observes a permit from unpark(), or the
>>>>>                     volatile store, whichever becomes observed first.
>>>>>
>>>>>                     Doug's clarification is vague enough to permit
>>>>>                     similar interpretation of how volatile load is
>>>>>                     different from normal loads w.r.t. apparent
>>>>>                     reordering with parts of park().
>>>>>
>>>>>                     Alex
>>>>>
>>>>>                     On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>                     Could you elaborate? Based on the
>>>>>>                     conversation thus far, I don't see how
>>>>>>                     compiler is allowed to hoist/remove the
>>>>>>                     (re)load of signal on the loop iteration.
>>>>>>
>>>>>>                     On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr
>>>>>>                     Otenko <oleksandr.otenko at oracle.com
>>>>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>
>>>>>>                         No, this code doesn't have to terminate.
>>>>>>
>>>>>>                         Alex
>>>>>>
>>>>>>
>>>>>>                         On 14/01/2015 20:03, thurstonn wrote:
>>>>>>
>>>>>>                             Hello Alex,
>>>>>>
>>>>>>                             This is what I'm saying:
>>>>>>
>>>>>>                             the below code is slightly (but
>>>>>>                             importantly different than our canonical
>>>>>>                             example).
>>>>>>
>>>>>>
>>>>>>                             boolean signal = false;  // POSV ==>
>>>>>>                             Plain Old Shared Variable
>>>>>>
>>>>>>
>>>>>>                             Thread 1:
>>>>>>                             while (! signal)
>>>>>>                                   park();
>>>>>>
>>>>>>                             Thread 2:
>>>>>>                             signal = true;
>>>>>>                             unpark(Thread1)
>>>>>>
>>>>>>
>>>>>>
>>>>>>                             Must this program terminate? YES.
>>>>>>
>>>>>>
>>>>>>                             Note - I am not advising this code,
>>>>>>                             or trying to be pedantic
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>                             --
>>>>>>                             View this message in context:
>>>>>>                             http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>                             Sent from the JSR166 Concurrency
>>>>>>                             mailing list archive at Nabble.com.
>>>>>>                             _______________________________________________
>>>>>>                             Concurrency-interest mailing list
>>>>>>                             Concurrency-interest at cs.oswego.edu
>>>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>                         _______________________________________________
>>>>>>                         Concurrency-interest mailing list
>>>>>>                         Concurrency-interest at cs.oswego.edu
>>>>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/f8656808/attachment-0001.html>

From thurston at nomagicsoftware.com  Thu Jan 15 11:21:29 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 15 Jan 2015 09:21:29 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7D6E7.70405@oracle.com>
References: <54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com> <54B7D6E7.70405@oracle.com>
Message-ID: <1421338889094-11964.post@n7.nabble.com>

oleksandr otenko wrote
> 
> T2 park loads 
> T1 stores signal
> T1 unpark stores  
> T2 park stores   
> T2 loads signal
> 
> There is no hb to stores signal, so any value of signal is possible. 
> There is no requirement for park to wake up on the next iteration, 
> because its store appears after unpark store, so hang is also possible.
> 
> Alex

I'm not exactly sure of your history here; so, let's say the shared
"volatile" between park and unpark is:
volatile int permits  = 0 // int to make it easy to distinguish between
#signal

1. T2 park loads permits (0)
2. T1 stores signal
3. T1 unpark stores  (permits = 1)
4. T2 park stores   ????? why would it store anything (it read 0)
5. T2 loads signal

sure, if 4 becomes:
T2 park stores (permits = 0)
then yes it could loop forever,but that's just a broken implementation of
park (non-atomic RMW).

Maybe I'm missing something




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11964.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Thu Jan 15 11:34:50 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 16:34:50 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
Message-ID: <54B7EC2A.1040409@oracle.com>

If you treat park and unpark as black boxes, it is like saying "thou 
shalt not implement them in Java".

Once you permit Java, you can no longer require that it is not inlined, 
optimized, reordered; its contents are subject to JMM.

Once you permit spurious wake-ups, you can no longer forbid no-op as the 
implementation.

Once you permit no-op as the implementation, you can no longer rely on 
normal reads for observing the writes.

Once you started using volatile accesses to observe the writes, you no 
longer need to require park() to be atomic.


Alex


On 15/01/2015 15:09, Vitaly Davidovich wrote:
>
> Ok I think I know why we're going in circles.  I'm treating park and 
> unpark as black boxes and only caring about their memory ordering 
> effects as a whole - I don't care how it provides that, it can issue 
> additional fences inside to ensure its internal state isn't moved out 
> of the call or external operations move into it.  You seem to be 
> "peeking" inside them and rationalizing based on some implementation 
> details.  So to me, it's sort of like lock/unlock calls - all I know 
> is how my operations are supposed to interact across those method 
> calls, I don't care how that's actually provided.
>
> sent from my phone
>
> On Jan 15, 2015 9:58 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I've explained the steps. You don't need the normal load of signal
>     to overtake the whole park() - only the volatile write inside the
>     park(). This is enough for signal to read the value before
>     unpark() starts, for volatile write in park() to clear the permits
>     issued by unpark(), and for all this to work properly, if signal
>     is volatile.
>
>
>     Alex
>
>
>     On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>
>>     Saying it's both a volatile read and write doesn't imply, to me
>>     at least, that subsequent loads of signal can appear before park
>>     () of prior iteration.  If park () had the semantics of *only* a
>>     volatile write, then yes, but by having volatile load I don't see
>>     how that's valid.
>>
>>     sent from my phone
>>
>>     On Jan 15, 2015 9:24 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         I don't. It should be allowed to be almost anything, even a
>>         no-op. It is only an optimization of the busy wait, and it is
>>         ok to not optimize, or just have a micro-pause that doesn't
>>         communicate with unpark. What it cannot be - it cannot be an
>>         implementation that may hang indefinitely, if appears after
>>         unpark (for some definition of "after"), and can't permit an
>>         implementation that doesn't establish hb between volatiles
>>         (before and after the unpark and park), if park appears after
>>         unpark (again, for some definition of "before" and "after").
>>         It is this reference to "before" and "after" that require to
>>         relate it somehow to the synchronization order. Existence of
>>         JMM hb edge between unpark and park is not necessary.
>>
>>         Also, read again how Doug specified park - it is not just a
>>         volatile read, but also a write. Let's not pretend the
>>         specification like that is perfect, or final, or won't be
>>         retracted, but that specification still permits the behaviour
>>         I described - the hang may still occur, because the normal
>>         read of signal from the next iteration is allowed to "go
>>         ahead" of the volatile write of the "volatile variable"
>>         shared with unpark, from the park of the previous iteration.
>>         This is not so for volatile reads.
>>
>>         Alex
>>
>>         On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>
>>>         Right, I agree that it can't be a noop.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             No, I object.
>>>
>>>             If you permit normal writes to be visible in program
>>>             order from unpark(), then no-op cannot be a permitted
>>>             implementation of park() - since that would imply any
>>>             program order should be visible in that order after
>>>             park/unpark.
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>
>>>>             My interpretation is pretty much what Jason said
>>>>             earlier; loop definitively terminates (eventually) but
>>>>             thread 1 may not see other writes if it never enters
>>>>             the loop.  That seems consistent with Doug's clarification.
>>>>
>>>>             sent from my phone
>>>>
>>>>             On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 If park() is seen as a synchronization event, it
>>>>                 will still require volatile accesses to establish a
>>>>                 hb (it will only stop reordering of volatile accesses).
>>>>
>>>>                 If park() is seen as a very particular
>>>>                 synchronization event that creates a hb edge, then
>>>>                 obviously normal reads should observe normal writes
>>>>                 and it should terminate, but this is not what
>>>>                 people write, and I don't know if that is what is
>>>>                 intended (eg see the objections - people didn't
>>>>                 think mentioning hb edge was all that useful).
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>
>>>>>                 I don't think subsequent load of signal is allowed
>>>>>                 to appear before park () because it was said
>>>>>                 earlier that park () acts like a volatile load; if
>>>>>                 so, subsequent loads cannot move before it.  Did I
>>>>>                 misunderstand?
>>>>>
>>>>>                 On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     It is not the hoist / remove of reload, it is
>>>>>                     the order of reload and what might be inside
>>>>>                     park().
>>>>>
>>>>>                     Normal load of signal of the subsequent
>>>>>                     iteration is allowed to appear ahead of
>>>>>                     whatever is inside park() of the previous
>>>>>                     iteration, as it is unspecified. The way Doug
>>>>>                     has clarified (or the way I wanted to clarify
>>>>>                     it), the normal load of signal can still
>>>>>                     appear ahead of whatever except the start of
>>>>>                     park() to some threads. park() is allowed to
>>>>>                     claim the permit in an unspecified way. So, it
>>>>>                     is perfectly fine for park() to start before
>>>>>                     signal==true is apparent, the subsequent read
>>>>>                     of signal is allowed to appear to others ahead
>>>>>                     of anything else in park() - ie it is allowed
>>>>>                     to appear right after the decision that it
>>>>>                     doesn't need to suspend. It observes
>>>>>                     signal==false. Now it is allowed for Thread
>>>>>                     2's store to signal to become apparent to
>>>>>                     Thread 1 (but it is not loading it anymore),
>>>>>                     and unpark() is allowed to release one permit.
>>>>>                     Now it is allowed for Thread 1 to claim that
>>>>>                     permit (or even clear the permit count without
>>>>>                     observing that it was set). Now Thread 1's
>>>>>                     while loop thinks signal==false, and it goes
>>>>>                     to call park() again, and hangs, because no
>>>>>                     more unpark() is called.
>>>>>
>>>>>                     If park() is deemed as a singular
>>>>>                     synchronization event (I don't even require a
>>>>>                     happens-before, so no extra edges appear;
>>>>>                     visibility of permits issued by unpark() to
>>>>>                     park() become available in an unspecified
>>>>>                     manner - only total ordering of events around
>>>>>                     these calls in Java land matters), a volatile
>>>>>                     load of signal would not be allowed to appear
>>>>>                     to anyone ahead of park() nor any part thereof
>>>>>                     - including the claim of the permit. So if
>>>>>                     unpark() was already entered, the volatile
>>>>>                     load would observe the volatile store of
>>>>>                     signal. Or, if unpark was not yet entered,
>>>>>                     then whatever the load of signal observes, it
>>>>>                     can at least go to park() again until it
>>>>>                     either observes a permit from unpark(), or the
>>>>>                     volatile store, whichever becomes observed first.
>>>>>
>>>>>                     Doug's clarification is vague enough to permit
>>>>>                     similar interpretation of how volatile load is
>>>>>                     different from normal loads w.r.t. apparent
>>>>>                     reordering with parts of park().
>>>>>
>>>>>                     Alex
>>>>>
>>>>>                     On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>                     Could you elaborate? Based on the
>>>>>>                     conversation thus far, I don't see how
>>>>>>                     compiler is allowed to hoist/remove the
>>>>>>                     (re)load of signal on the loop iteration.
>>>>>>
>>>>>>                     On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr
>>>>>>                     Otenko <oleksandr.otenko at oracle.com
>>>>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>
>>>>>>                         No, this code doesn't have to terminate.
>>>>>>
>>>>>>                         Alex
>>>>>>
>>>>>>
>>>>>>                         On 14/01/2015 20:03, thurstonn wrote:
>>>>>>
>>>>>>                             Hello Alex,
>>>>>>
>>>>>>                             This is what I'm saying:
>>>>>>
>>>>>>                             the below code is slightly (but
>>>>>>                             importantly different than our canonical
>>>>>>                             example).
>>>>>>
>>>>>>
>>>>>>                             boolean signal = false;  // POSV ==>
>>>>>>                             Plain Old Shared Variable
>>>>>>
>>>>>>
>>>>>>                             Thread 1:
>>>>>>                             while (! signal)
>>>>>>                                   park();
>>>>>>
>>>>>>                             Thread 2:
>>>>>>                             signal = true;
>>>>>>                             unpark(Thread1)
>>>>>>
>>>>>>
>>>>>>
>>>>>>                             Must this program terminate? YES.
>>>>>>
>>>>>>
>>>>>>                             Note - I am not advising this code,
>>>>>>                             or trying to be pedantic
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>                             --
>>>>>>                             View this message in context:
>>>>>>                             http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>                             Sent from the JSR166 Concurrency
>>>>>>                             mailing list archive at Nabble.com.
>>>>>>                             _______________________________________________
>>>>>>                             Concurrency-interest mailing list
>>>>>>                             Concurrency-interest at cs.oswego.edu
>>>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>>                         _______________________________________________
>>>>>>                         Concurrency-interest mailing list
>>>>>>                         Concurrency-interest at cs.oswego.edu
>>>>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/61100f27/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 15 11:45:00 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 15 Jan 2015 11:45:00 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7EC2A.1040409@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
Message-ID: <CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>

>
> If you treat park and unpark as black boxes, it is like saying "thou shalt
> not implement them in Java".
> Once you permit Java, you can no longer require that it is not inlined,
> optimized, reordered; its contents are subject to JMM.


Why's that? Can't a java implementation use fences (e.g.
Unsafe.load/store/fullFence) to ensure the body of the methods does not
reorder with outside reads/writes, as appropriate? I don't see how this is
any different from other methods in java that stipulate what the memory
ordering semantics are with respect to calling those methods.

On Thu, Jan 15, 2015 at 11:34 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  If you treat park and unpark as black boxes, it is like saying "thou
> shalt not implement them in Java".
>
> Once you permit Java, you can no longer require that it is not inlined,
> optimized, reordered; its contents are subject to JMM.
>
> Once you permit spurious wake-ups, you can no longer forbid no-op as the
> implementation.
>
> Once you permit no-op as the implementation, you can no longer rely on
> normal reads for observing the writes.
>
> Once you started using volatile accesses to observe the writes, you no
> longer need to require park() to be atomic.
>
>
> Alex
>
>
> On 15/01/2015 15:09, Vitaly Davidovich wrote:
>
> Ok I think I know why we're going in circles.  I'm treating park and
> unpark as black boxes and only caring about their memory ordering effects
> as a whole - I don't care how it provides that, it can issue additional
> fences inside to ensure its internal state isn't moved out of the call or
> external operations move into it.  You seem to be "peeking" inside them and
> rationalizing based on some implementation details.  So to me, it's sort of
> like lock/unlock calls - all I know is how my operations are supposed to
> interact across those method calls, I don't care how that's actually
> provided.
>
> sent from my phone
> On Jan 15, 2015 9:58 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  I've explained the steps. You don't need the normal load of signal to
>> overtake the whole park() - only the volatile write inside the park(). This
>> is enough for signal to read the value before unpark() starts, for volatile
>> write in park() to clear the permits issued by unpark(), and for all this
>> to work properly, if signal is volatile.
>>
>>
>> Alex
>>
>>
>> On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>
>> Saying it's both a volatile read and write doesn't imply, to me at least,
>> that subsequent loads of signal can appear before park () of prior
>> iteration.  If park () had the semantics of *only* a volatile write, then
>> yes, but by having volatile load I don't see how that's valid.
>>
>> sent from my phone
>> On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  I don't. It should be allowed to be almost anything, even a no-op. It
>>> is only an optimization of the busy wait, and it is ok to not optimize, or
>>> just have a micro-pause that doesn't communicate with unpark. What it
>>> cannot be - it cannot be an implementation that may hang indefinitely, if
>>> appears after unpark (for some definition of "after"), and can't permit an
>>> implementation that doesn't establish hb between volatiles (before and
>>> after the unpark and park), if park appears after unpark (again, for some
>>> definition of "before" and "after"). It is this reference to "before" and
>>> "after" that require to relate it somehow to the synchronization order.
>>> Existence of JMM hb edge between unpark and park is not necessary.
>>>
>>> Also, read again how Doug specified park - it is not just a volatile
>>> read, but also a write. Let's not pretend the specification like that is
>>> perfect, or final, or won't be retracted, but that specification still
>>> permits the behaviour I described - the hang may still occur, because the
>>> normal read of signal from the next iteration is allowed to "go ahead" of
>>> the volatile write of the "volatile variable" shared with unpark, from the
>>> park of the previous iteration. This is not so for volatile reads.
>>>
>>> Alex
>>>
>>> On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>
>>> Right, I agree that it can't be a noop.
>>>
>>> sent from my phone
>>> On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>>  No, I object.
>>>>
>>>> If you permit normal writes to be visible in program order from
>>>> unpark(), then no-op cannot be a permitted implementation of park() - since
>>>> that would imply any program order should be visible in that order after
>>>> park/unpark.
>>>>
>>>> Alex
>>>>
>>>> On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>
>>>> My interpretation is pretty much what Jason said earlier; loop
>>>> definitively terminates (eventually) but thread 1 may not see other writes
>>>> if it never enters the loop.  That seems consistent with Doug's
>>>> clarification.
>>>>
>>>> sent from my phone
>>>> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>>  If park() is seen as a synchronization event, it will still require
>>>>> volatile accesses to establish a hb (it will only stop reordering of
>>>>> volatile accesses).
>>>>>
>>>>> If park() is seen as a very particular synchronization event that
>>>>> creates a hb edge, then obviously normal reads should observe normal writes
>>>>> and it should terminate, but this is not what people write, and I don't
>>>>> know if that is what is intended (eg see the objections - people didn't
>>>>> think mentioning hb edge was all that useful).
>>>>>
>>>>> Alex
>>>>>
>>>>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>
>>>>> I don't think subsequent load of signal is allowed to appear before
>>>>> park () because it was said earlier that park () acts like a volatile load;
>>>>> if so, subsequent loads cannot move before it.  Did I misunderstand?
>>>>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <
>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>>>  It is not the hoist / remove of reload, it is the order of reload
>>>>>> and what might be inside park().
>>>>>>
>>>>>> Normal load of signal of the subsequent iteration is allowed to
>>>>>> appear ahead of whatever is inside park() of the previous iteration, as it
>>>>>> is unspecified. The way Doug has clarified (or the way I wanted to clarify
>>>>>> it), the normal load of signal can still appear ahead of whatever except
>>>>>> the start of park() to some threads. park() is allowed to claim the permit
>>>>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>>>>> signal==true is apparent, the subsequent read of signal is allowed to
>>>>>> appear to others ahead of anything else in park() - ie it is allowed to
>>>>>> appear right after the decision that it doesn't need to suspend. It
>>>>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>>>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>>>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>>>>> to claim that permit (or even clear the permit count without observing that
>>>>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>>>>> call park() again, and hangs, because no more unpark() is called.
>>>>>>
>>>>>> If park() is deemed as a singular synchronization event (I don't even
>>>>>> require a happens-before, so no extra edges appear; visibility of permits
>>>>>> issued by unpark() to park() become available in an unspecified manner -
>>>>>> only total ordering of events around these calls in Java land matters), a
>>>>>> volatile load of signal would not be allowed to appear to anyone ahead of
>>>>>> park() nor any part thereof - including the claim of the permit. So if
>>>>>> unpark() was already entered, the volatile load would observe the volatile
>>>>>> store of signal. Or, if unpark was not yet entered, then whatever the load
>>>>>> of signal observes, it can at least go to park() again until it either
>>>>>> observes a permit from unpark(), or the volatile store, whichever becomes
>>>>>> observed first.
>>>>>>
>>>>>> Doug's clarification is vague enough to permit similar interpretation
>>>>>> of how volatile load is different from normal loads w.r.t. apparent
>>>>>> reordering with parts of park().
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>
>>>>>> Could you elaborate? Based on the conversation thus far, I don't see
>>>>>> how compiler is allowed to hoist/remove the (re)load of signal on the loop
>>>>>> iteration.
>>>>>>
>>>>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>
>>>>>>> No, this code doesn't have to terminate.
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>>
>>>>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>>>>
>>>>>>>> Hello Alex,
>>>>>>>>
>>>>>>>> This is what I'm saying:
>>>>>>>>
>>>>>>>> the below code is slightly (but importantly different than our
>>>>>>>> canonical
>>>>>>>> example).
>>>>>>>>
>>>>>>>>
>>>>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>>>>
>>>>>>>>
>>>>>>>> Thread 1:
>>>>>>>> while (! signal)
>>>>>>>>       park();
>>>>>>>>
>>>>>>>> Thread 2:
>>>>>>>> signal = true;
>>>>>>>> unpark(Thread1)
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Must this program terminate?  YES.
>>>>>>>>
>>>>>>>>
>>>>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> View this message in context:
>>>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/64f91b97/attachment-0001.html>

From dl at cs.oswego.edu  Thu Jan 15 12:25:17 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Jan 2015 12:25:17 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
Message-ID: <54B7F7FD.6000507@cs.oswego.edu>


Here's the only set of candidates for new jdk9 j.u.c classes:

As discussed a few months ago, there is no single best fluent
async/parallel API. CompletableFuture/CompletionStage best supports
continuation-style programming on futures, and java.util.stream best
supports (multi-stage, possibly-parallel) "pull" style operations on
the elements of collections. Until now, one missing category was
"push" style operations on items as they become available from an
active source. We are not alone in wanting a standard way to support
this. Over the past year, the "reactive-streams"
(http://www.reactive-streams.org/) effort has been defining a minimal
set of interfaces expressing commonalities and allowing
interoperablility across frameworks (including Rx and Akka Play), that
is nearing release. These interfaces include provisions for a simple
form of async flow control allowing developers to address resource
control issues that can otherwise cause problems in push-based
systems. Supporting this mini-framework helps avoid unpleasant
surprises possible when trying to use pull-style APIs for "hot"
reactive sources (but conversely is not as good a choice as
java.util.Stream for "cold" sources like collections).

The four intertwined interfaces (Publisher, Subscriber, Subscription,
Processor) are defined within the same class "Flow", that also
includes the first of some planned support methods to establish and
use Flow components, including tie-ins to java.util.streams and
CompletableFutures. See
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html

(Logistically, the only alternative to class Flow would have been
to introduce a subpackage, which unnecessarily complicates usage.  And
"import static java.util.concurrent.Flow;" is about as convenient as
"import java.util.concurrent.flow.*;" would be.)

Also, the new stand-alone class SubmissionPublisher can serve as a
bridge from various kinds of item producers to Flow components, and is
useful in its own right. It is a form of ordered multicaster that
people have complained that we don't support in j.u.c, but now do.
See
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html

Disclaimers: These are only candidates for inclusion.  The are in
preliminary form and will change. But comments and suggestions would
be welcome. As with the other candidate additions, if you are brave,
you can try out snapshots on jdk8+ by getting
   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
and running java -Xbootclasspath/p:jsr166.jar

-Doug


From oleksandr.otenko at oracle.com  Thu Jan 15 12:43:44 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 17:43:44 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
Message-ID: <54B7FC50.7020108@oracle.com>

Is there a use-case for atomic treatment?

I think someone first needs to explain the need for normal accesses to 
become visible. Some old-school even don't think the hb between 
unpark-park is needed. To my mind, any clarification is only needed to 
explain global ordering of volatile accesses and park/unpark - and that 
only being picky. If such clarification only creates false hopes or 
flawed reasoning, then forget it, let's keep the documentation as it was 
before.


Alex


On 15/01/2015 16:45, Vitaly Davidovich wrote:
>
>     If you treat park and unpark as black boxes, it is like saying
>     "thou shalt not implement them in Java".
>     Once you permit Java, you can no longer require that it is not
>     inlined, optimized, reordered; its contents are subject to JMM.
>
>
> Why's that? Can't a java implementation use fences (e.g. 
> Unsafe.load/store/fullFence) to ensure the body of the methods does 
> not reorder with outside reads/writes, as appropriate? I don't see how 
> this is any different from other methods in java that stipulate what 
> the memory ordering semantics are with respect to calling those methods.
>
> On Thu, Jan 15, 2015 at 11:34 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     If you treat park and unpark as black boxes, it is like saying
>     "thou shalt not implement them in Java".
>
>     Once you permit Java, you can no longer require that it is not
>     inlined, optimized, reordered; its contents are subject to JMM.
>
>     Once you permit spurious wake-ups, you can no longer forbid no-op
>     as the implementation.
>
>     Once you permit no-op as the implementation, you can no longer
>     rely on normal reads for observing the writes.
>
>     Once you started using volatile accesses to observe the writes,
>     you no longer need to require park() to be atomic.
>
>
>     Alex
>
>
>     On 15/01/2015 15:09, Vitaly Davidovich wrote:
>>
>>     Ok I think I know why we're going in circles.  I'm treating park
>>     and unpark as black boxes and only caring about their memory
>>     ordering effects as a whole - I don't care how it provides that,
>>     it can issue additional fences inside to ensure its internal
>>     state isn't moved out of the call or external operations move
>>     into it.  You seem to be "peeking" inside them and rationalizing
>>     based on some implementation details.  So to me, it's sort of
>>     like lock/unlock calls - all I know is how my operations are
>>     supposed to interact across those method calls, I don't care how
>>     that's actually provided.
>>
>>     sent from my phone
>>
>>     On Jan 15, 2015 9:58 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         I've explained the steps. You don't need the normal load of
>>         signal to overtake the whole park() - only the volatile write
>>         inside the park(). This is enough for signal to read the
>>         value before unpark() starts, for volatile write in park() to
>>         clear the permits issued by unpark(), and for all this to
>>         work properly, if signal is volatile.
>>
>>
>>         Alex
>>
>>
>>         On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>>
>>>         Saying it's both a volatile read and write doesn't imply, to
>>>         me at least, that subsequent loads of signal can appear
>>>         before park () of prior iteration. If park () had the
>>>         semantics of *only* a volatile write, then yes, but by
>>>         having volatile load I don't see how that's valid.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 15, 2015 9:24 AM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             I don't. It should be allowed to be almost anything,
>>>             even a no-op. It is only an optimization of the busy
>>>             wait, and it is ok to not optimize, or just have a
>>>             micro-pause that doesn't communicate with unpark. What
>>>             it cannot be - it cannot be an implementation that may
>>>             hang indefinitely, if appears after unpark (for some
>>>             definition of "after"), and can't permit an
>>>             implementation that doesn't establish hb between
>>>             volatiles (before and after the unpark and park), if
>>>             park appears after unpark (again, for some definition of
>>>             "before" and "after"). It is this reference to "before"
>>>             and "after" that require to relate it somehow to the
>>>             synchronization order. Existence of JMM hb edge between
>>>             unpark and park is not necessary.
>>>
>>>             Also, read again how Doug specified park - it is not
>>>             just a volatile read, but also a write. Let's not
>>>             pretend the specification like that is perfect, or
>>>             final, or won't be retracted, but that specification
>>>             still permits the behaviour I described - the hang may
>>>             still occur, because the normal read of signal from the
>>>             next iteration is allowed to "go ahead" of the volatile
>>>             write of the "volatile variable" shared with unpark,
>>>             from the park of the previous iteration. This is not so
>>>             for volatile reads.
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>>
>>>>             Right, I agree that it can't be a noop.
>>>>
>>>>             sent from my phone
>>>>
>>>>             On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 No, I object.
>>>>
>>>>                 If you permit normal writes to be visible in
>>>>                 program order from unpark(), then no-op cannot be a
>>>>                 permitted implementation of park() - since that
>>>>                 would imply any program order should be visible in
>>>>                 that order after park/unpark.
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>>
>>>>>                 My interpretation is pretty much what Jason said
>>>>>                 earlier; loop definitively terminates (eventually)
>>>>>                 but thread 1 may not see other writes if it never
>>>>>                 enters the loop.  That seems consistent with
>>>>>                 Doug's clarification.
>>>>>
>>>>>                 sent from my phone
>>>>>
>>>>>                 On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     If park() is seen as a synchronization event,
>>>>>                     it will still require volatile accesses to
>>>>>                     establish a hb (it will only stop reordering
>>>>>                     of volatile accesses).
>>>>>
>>>>>                     If park() is seen as a very particular
>>>>>                     synchronization event that creates a hb edge,
>>>>>                     then obviously normal reads should observe
>>>>>                     normal writes and it should terminate, but
>>>>>                     this is not what people write, and I don't
>>>>>                     know if that is what is intended (eg see the
>>>>>                     objections - people didn't think mentioning hb
>>>>>                     edge was all that useful).
>>>>>
>>>>>                     Alex
>>>>>
>>>>>                     On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>>
>>>>>>                     I don't think subsequent load of signal is
>>>>>>                     allowed to appear before park () because it
>>>>>>                     was said earlier that park () acts like a
>>>>>>                     volatile load; if so, subsequent loads cannot
>>>>>>                     move before it.  Did I misunderstand?
>>>>>>
>>>>>>                     On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>>>                     <oleksandr.otenko at oracle.com
>>>>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>
>>>>>>                         It is not the hoist / remove of reload,
>>>>>>                         it is the order of reload and what might
>>>>>>                         be inside park().
>>>>>>
>>>>>>                         Normal load of signal of the subsequent
>>>>>>                         iteration is allowed to appear ahead of
>>>>>>                         whatever is inside park() of the previous
>>>>>>                         iteration, as it is unspecified. The way
>>>>>>                         Doug has clarified (or the way I wanted
>>>>>>                         to clarify it), the normal load of signal
>>>>>>                         can still appear ahead of whatever except
>>>>>>                         the start of park() to some threads.
>>>>>>                         park() is allowed to claim the permit in
>>>>>>                         an unspecified way. So, it is perfectly
>>>>>>                         fine for park() to start before
>>>>>>                         signal==true is apparent, the subsequent
>>>>>>                         read of signal is allowed to appear to
>>>>>>                         others ahead of anything else in park() -
>>>>>>                         ie it is allowed to appear right after
>>>>>>                         the decision that it doesn't need to
>>>>>>                         suspend. It observes signal==false. Now
>>>>>>                         it is allowed for Thread 2's store to
>>>>>>                         signal to become apparent to Thread 1
>>>>>>                         (but it is not loading it anymore), and
>>>>>>                         unpark() is allowed to release one
>>>>>>                         permit. Now it is allowed for Thread 1 to
>>>>>>                         claim that permit (or even clear the
>>>>>>                         permit count without observing that it
>>>>>>                         was set). Now Thread 1's while loop
>>>>>>                         thinks signal==false, and it goes to call
>>>>>>                         park() again, and hangs, because no more
>>>>>>                         unpark() is called.
>>>>>>
>>>>>>                         If park() is deemed as a singular
>>>>>>                         synchronization event (I don't even
>>>>>>                         require a happens-before, so no extra
>>>>>>                         edges appear; visibility of permits
>>>>>>                         issued by unpark() to park() become
>>>>>>                         available in an unspecified manner - only
>>>>>>                         total ordering of events around these
>>>>>>                         calls in Java land matters), a volatile
>>>>>>                         load of signal would not be allowed to
>>>>>>                         appear to anyone ahead of park() nor any
>>>>>>                         part thereof - including the claim of the
>>>>>>                         permit. So if unpark() was already
>>>>>>                         entered, the volatile load would observe
>>>>>>                         the volatile store of signal. Or, if
>>>>>>                         unpark was not yet entered, then whatever
>>>>>>                         the load of signal observes, it can at
>>>>>>                         least go to park() again until it either
>>>>>>                         observes a permit from unpark(), or the
>>>>>>                         volatile store, whichever becomes
>>>>>>                         observed first.
>>>>>>
>>>>>>                         Doug's clarification is vague enough to
>>>>>>                         permit similar interpretation of how
>>>>>>                         volatile load is different from normal
>>>>>>                         loads w.r.t. apparent reordering with
>>>>>>                         parts of park().
>>>>>>
>>>>>>                         Alex
>>>>>>
>>>>>>                         On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>>                         Could you elaborate? Based on the
>>>>>>>                         conversation thus far, I don't see how
>>>>>>>                         compiler is allowed to hoist/remove the
>>>>>>>                         (re)load of signal on the loop iteration.
>>>>>>>
>>>>>>>                         On Wed, Jan 14, 2015 at 5:59 PM,
>>>>>>>                         Oleksandr Otenko
>>>>>>>                         <oleksandr.otenko at oracle.com
>>>>>>>                         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>>
>>>>>>>                             No, this code doesn't have to terminate.
>>>>>>>
>>>>>>>                             Alex
>>>>>>>
>>>>>>>
>>>>>>>                             On 14/01/2015 20:03, thurstonn wrote:
>>>>>>>
>>>>>>>                                 Hello Alex,
>>>>>>>
>>>>>>>                                 This is what I'm saying:
>>>>>>>
>>>>>>>                                 the below code is slightly (but
>>>>>>>                                 importantly different than our
>>>>>>>                                 canonical
>>>>>>>                                 example).
>>>>>>>
>>>>>>>
>>>>>>>                                 boolean signal = false;  // POSV
>>>>>>>                                 ==> Plain Old Shared Variable
>>>>>>>
>>>>>>>
>>>>>>>                                 Thread 1:
>>>>>>>                                 while (! signal)
>>>>>>>                                       park();
>>>>>>>
>>>>>>>                                 Thread 2:
>>>>>>>                                 signal = true;
>>>>>>>                                 unpark(Thread1)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>                                 Must this program terminate? YES.
>>>>>>>
>>>>>>>
>>>>>>>                                 Note - I am not advising this
>>>>>>>                                 code, or trying to be pedantic
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>                                 --
>>>>>>>                                 View this message in context:
>>>>>>>                                 http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>>                                 Sent from the JSR166 Concurrency
>>>>>>>                                 mailing list archive at Nabble.com.
>>>>>>>                                 _______________________________________________
>>>>>>>                                 Concurrency-interest mailing list
>>>>>>>                                 Concurrency-interest at cs.oswego.edu
>>>>>>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>>                             _______________________________________________
>>>>>>>                             Concurrency-interest mailing list
>>>>>>>                             Concurrency-interest at cs.oswego.edu
>>>>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/71cc2ad2/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 15 12:51:47 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 15 Jan 2015 12:51:47 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7FC50.7020108@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>
	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com>
Message-ID: <CAHjP37HTk0UzYQKAzOHQ0zn2nYua5i_zzbU5uHy33PL3z_4AHA@mail.gmail.com>

The use case would be uses of these methods to communicate between threads
with the ability to suspend/resume the participant threads.  That is, some
may want to piggy back on these constructs as-if they were volatile fields.

If that's not "allowed", then yes, nothing needs to change.  But based on
the length of this thread, I'm not sure that's agreed upon.

sent from my phone
On Jan 15, 2015 12:43 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Is there a use-case for atomic treatment?
>
> I think someone first needs to explain the need for normal accesses to
> become visible. Some old-school even don't think the hb between unpark-park
> is needed. To my mind, any clarification is only needed to explain global
> ordering of volatile accesses and park/unpark - and that only being picky.
> If such clarification only creates false hopes or flawed reasoning, then
> forget it, let's keep the documentation as it was before.
>
>
> Alex
>
>
> On 15/01/2015 16:45, Vitaly Davidovich wrote:
>
>  If you treat park and unpark as black boxes, it is like saying "thou
>> shalt not implement them in Java".
>> Once you permit Java, you can no longer require that it is not inlined,
>> optimized, reordered; its contents are subject to JMM.
>
>
>  Why's that? Can't a java implementation use fences (e.g.
> Unsafe.load/store/fullFence) to ensure the body of the methods does not
> reorder with outside reads/writes, as appropriate? I don't see how this is
> any different from other methods in java that stipulate what the memory
> ordering semantics are with respect to calling those methods.
>
> On Thu, Jan 15, 2015 at 11:34 AM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  If you treat park and unpark as black boxes, it is like saying "thou
>> shalt not implement them in Java".
>>
>> Once you permit Java, you can no longer require that it is not inlined,
>> optimized, reordered; its contents are subject to JMM.
>>
>> Once you permit spurious wake-ups, you can no longer forbid no-op as the
>> implementation.
>>
>> Once you permit no-op as the implementation, you can no longer rely on
>> normal reads for observing the writes.
>>
>> Once you started using volatile accesses to observe the writes, you no
>> longer need to require park() to be atomic.
>>
>>
>> Alex
>>
>>
>> On 15/01/2015 15:09, Vitaly Davidovich wrote:
>>
>> Ok I think I know why we're going in circles.  I'm treating park and
>> unpark as black boxes and only caring about their memory ordering effects
>> as a whole - I don't care how it provides that, it can issue additional
>> fences inside to ensure its internal state isn't moved out of the call or
>> external operations move into it.  You seem to be "peeking" inside them and
>> rationalizing based on some implementation details.  So to me, it's sort of
>> like lock/unlock calls - all I know is how my operations are supposed to
>> interact across those method calls, I don't care how that's actually
>> provided.
>>
>> sent from my phone
>> On Jan 15, 2015 9:58 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>>  I've explained the steps. You don't need the normal load of signal to
>>> overtake the whole park() - only the volatile write inside the park(). This
>>> is enough for signal to read the value before unpark() starts, for volatile
>>> write in park() to clear the permits issued by unpark(), and for all this
>>> to work properly, if signal is volatile.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>>
>>> Saying it's both a volatile read and write doesn't imply, to me at
>>> least, that subsequent loads of signal can appear before park () of prior
>>> iteration.  If park () had the semantics of *only* a volatile write, then
>>> yes, but by having volatile load I don't see how that's valid.
>>>
>>> sent from my phone
>>> On Jan 15, 2015 9:24 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>>  I don't. It should be allowed to be almost anything, even a no-op. It
>>>> is only an optimization of the busy wait, and it is ok to not optimize, or
>>>> just have a micro-pause that doesn't communicate with unpark. What it
>>>> cannot be - it cannot be an implementation that may hang indefinitely, if
>>>> appears after unpark (for some definition of "after"), and can't permit an
>>>> implementation that doesn't establish hb between volatiles (before and
>>>> after the unpark and park), if park appears after unpark (again, for some
>>>> definition of "before" and "after"). It is this reference to "before" and
>>>> "after" that require to relate it somehow to the synchronization order.
>>>> Existence of JMM hb edge between unpark and park is not necessary.
>>>>
>>>> Also, read again how Doug specified park - it is not just a volatile
>>>> read, but also a write. Let's not pretend the specification like that is
>>>> perfect, or final, or won't be retracted, but that specification still
>>>> permits the behaviour I described - the hang may still occur, because the
>>>> normal read of signal from the next iteration is allowed to "go ahead" of
>>>> the volatile write of the "volatile variable" shared with unpark, from the
>>>> park of the previous iteration. This is not so for volatile reads.
>>>>
>>>> Alex
>>>>
>>>> On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>>
>>>> Right, I agree that it can't be a noop.
>>>>
>>>> sent from my phone
>>>> On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>>  No, I object.
>>>>>
>>>>> If you permit normal writes to be visible in program order from
>>>>> unpark(), then no-op cannot be a permitted implementation of park() - since
>>>>> that would imply any program order should be visible in that order after
>>>>> park/unpark.
>>>>>
>>>>> Alex
>>>>>
>>>>> On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>>
>>>>> My interpretation is pretty much what Jason said earlier; loop
>>>>> definitively terminates (eventually) but thread 1 may not see other writes
>>>>> if it never enters the loop.  That seems consistent with Doug's
>>>>> clarification.
>>>>>
>>>>> sent from my phone
>>>>> On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <
>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>>>  If park() is seen as a synchronization event, it will still require
>>>>>> volatile accesses to establish a hb (it will only stop reordering of
>>>>>> volatile accesses).
>>>>>>
>>>>>> If park() is seen as a very particular synchronization event that
>>>>>> creates a hb edge, then obviously normal reads should observe normal writes
>>>>>> and it should terminate, but this is not what people write, and I don't
>>>>>> know if that is what is intended (eg see the objections - people didn't
>>>>>> think mentioning hb edge was all that useful).
>>>>>>
>>>>>> Alex
>>>>>>
>>>>>> On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>>
>>>>>> I don't think subsequent load of signal is allowed to appear before
>>>>>> park () because it was said earlier that park () acts like a volatile load;
>>>>>> if so, subsequent loads cannot move before it.  Did I misunderstand?
>>>>>> On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <
>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>
>>>>>>>  It is not the hoist / remove of reload, it is the order of reload
>>>>>>> and what might be inside park().
>>>>>>>
>>>>>>> Normal load of signal of the subsequent iteration is allowed to
>>>>>>> appear ahead of whatever is inside park() of the previous iteration, as it
>>>>>>> is unspecified. The way Doug has clarified (or the way I wanted to clarify
>>>>>>> it), the normal load of signal can still appear ahead of whatever except
>>>>>>> the start of park() to some threads. park() is allowed to claim the permit
>>>>>>> in an unspecified way. So, it is perfectly fine for park() to start before
>>>>>>> signal==true is apparent, the subsequent read of signal is allowed to
>>>>>>> appear to others ahead of anything else in park() - ie it is allowed to
>>>>>>> appear right after the decision that it doesn't need to suspend. It
>>>>>>> observes signal==false. Now it is allowed for Thread 2's store to signal to
>>>>>>> become apparent to Thread 1 (but it is not loading it anymore), and
>>>>>>> unpark() is allowed to release one permit. Now it is allowed for Thread 1
>>>>>>> to claim that permit (or even clear the permit count without observing that
>>>>>>> it was set). Now Thread 1's while loop thinks signal==false, and it goes to
>>>>>>> call park() again, and hangs, because no more unpark() is called.
>>>>>>>
>>>>>>> If park() is deemed as a singular synchronization event (I don't
>>>>>>> even require a happens-before, so no extra edges appear; visibility of
>>>>>>> permits issued by unpark() to park() become available in an unspecified
>>>>>>> manner - only total ordering of events around these calls in Java land
>>>>>>> matters), a volatile load of signal would not be allowed to appear to
>>>>>>> anyone ahead of park() nor any part thereof - including the claim of the
>>>>>>> permit. So if unpark() was already entered, the volatile load would observe
>>>>>>> the volatile store of signal. Or, if unpark was not yet entered, then
>>>>>>> whatever the load of signal observes, it can at least go to park() again
>>>>>>> until it either observes a permit from unpark(), or the volatile store,
>>>>>>> whichever becomes observed first.
>>>>>>>
>>>>>>> Doug's clarification is vague enough to permit similar
>>>>>>> interpretation of how volatile load is different from normal loads w.r.t.
>>>>>>> apparent reordering with parts of park().
>>>>>>>
>>>>>>> Alex
>>>>>>>
>>>>>>> On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>>>
>>>>>>> Could you elaborate? Based on the conversation thus far, I don't see
>>>>>>> how compiler is allowed to hoist/remove the (re)load of signal on the loop
>>>>>>> iteration.
>>>>>>>
>>>>>>> On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <
>>>>>>> oleksandr.otenko at oracle.com> wrote:
>>>>>>>
>>>>>>>> No, this code doesn't have to terminate.
>>>>>>>>
>>>>>>>> Alex
>>>>>>>>
>>>>>>>>
>>>>>>>> On 14/01/2015 20:03, thurstonn wrote:
>>>>>>>>
>>>>>>>>> Hello Alex,
>>>>>>>>>
>>>>>>>>> This is what I'm saying:
>>>>>>>>>
>>>>>>>>> the below code is slightly (but importantly different than our
>>>>>>>>> canonical
>>>>>>>>> example).
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> boolean signal = false;  // POSV ==> Plain Old Shared Variable
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Thread 1:
>>>>>>>>> while (! signal)
>>>>>>>>>       park();
>>>>>>>>>
>>>>>>>>> Thread 2:
>>>>>>>>> signal = true;
>>>>>>>>> unpark(Thread1)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Must this program terminate?  YES.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Note - I am not advising this code, or trying to be pedantic
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> View this message in context:
>>>>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>>>> Sent from the JSR166 Concurrency mailing list archive at
>>>>>>>>> Nabble.com.
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/01224360/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 13:02:21 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 18:02:21 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <CAHjP37HTk0UzYQKAzOHQ0zn2nYua5i_zzbU5uHy33PL3z_4AHA@mail.gmail.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<CAHjP37HTk0UzYQKAzOHQ0zn2nYua5i_zzbU5uHy33PL3z_4AHA@mail.gmail.com>
Message-ID: <54B800AD.3090808@oracle.com>

Non-atomic treatment results in just three synchronization orders:

a.
unpark stores
park loads
park stores

b.
park loads
unpark stores
park stores

c.
park loads
park stores
unpark stores

Atomicity of park excludes only one case: b. What is the use-case among 
those permissible before the clarification would expose that b is not 
allowed? (this is the case that hangs thurstonn's example)

Alex

On 15/01/2015 17:51, Vitaly Davidovich wrote:
>
> The use case would be uses of these methods to communicate between 
> threads with the ability to suspend/resume the participant threads.  
> That is, some may want to piggy back on these constructs as-if they 
> were volatile fields.
>
> If that's not "allowed", then yes, nothing needs to change.  But based 
> on the length of this thread, I'm not sure that's agreed upon.
>
> sent from my phone
>
> On Jan 15, 2015 12:43 PM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Is there a use-case for atomic treatment?
>
>     I think someone first needs to explain the need for normal
>     accesses to become visible. Some old-school even don't think the
>     hb between unpark-park is needed. To my mind, any clarification is
>     only needed to explain global ordering of volatile accesses and
>     park/unpark - and that only being picky. If such clarification
>     only creates false hopes or flawed reasoning, then forget it,
>     let's keep the documentation as it was before.
>
>
>     Alex
>
>
>     On 15/01/2015 16:45, Vitaly Davidovich wrote:
>>
>>         If you treat park and unpark as black boxes, it is like
>>         saying "thou shalt not implement them in Java".
>>         Once you permit Java, you can no longer require that it is
>>         not inlined, optimized, reordered; its contents are subject
>>         to JMM.
>>
>>
>>     Why's that? Can't a java implementation use fences (e.g.
>>     Unsafe.load/store/fullFence) to ensure the body of the methods
>>     does not reorder with outside reads/writes, as appropriate? I
>>     don't see how this is any different from other methods in java
>>     that stipulate what the memory ordering semantics are with
>>     respect to calling those methods.
>>
>>     On Thu, Jan 15, 2015 at 11:34 AM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         If you treat park and unpark as black boxes, it is like
>>         saying "thou shalt not implement them in Java".
>>
>>         Once you permit Java, you can no longer require that it is
>>         not inlined, optimized, reordered; its contents are subject
>>         to JMM.
>>
>>         Once you permit spurious wake-ups, you can no longer forbid
>>         no-op as the implementation.
>>
>>         Once you permit no-op as the implementation, you can no
>>         longer rely on normal reads for observing the writes.
>>
>>         Once you started using volatile accesses to observe the
>>         writes, you no longer need to require park() to be atomic.
>>
>>
>>         Alex
>>
>>
>>         On 15/01/2015 15:09, Vitaly Davidovich wrote:
>>>
>>>         Ok I think I know why we're going in circles.  I'm treating
>>>         park and unpark as black boxes and only caring about their
>>>         memory ordering effects as a whole - I don't care how it
>>>         provides that, it can issue additional fences inside to
>>>         ensure its internal state isn't moved out of the call or
>>>         external operations move into it.  You seem to be "peeking"
>>>         inside them and rationalizing based on some implementation
>>>         details.  So to me, it's sort of like lock/unlock calls -
>>>         all I know is how my operations are supposed to interact
>>>         across those method calls, I don't care how that's actually
>>>         provided.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 15, 2015 9:58 AM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             I've explained the steps. You don't need the normal load
>>>             of signal to overtake the whole park() - only the
>>>             volatile write inside the park(). This is enough for
>>>             signal to read the value before unpark() starts, for
>>>             volatile write in park() to clear the permits issued by
>>>             unpark(), and for all this to work properly, if signal
>>>             is volatile.
>>>
>>>
>>>             Alex
>>>
>>>
>>>             On 15/01/2015 14:32, Vitaly Davidovich wrote:
>>>>
>>>>             Saying it's both a volatile read and write doesn't
>>>>             imply, to me at least, that subsequent loads of signal
>>>>             can appear before park () of prior iteration.  If park
>>>>             () had the semantics of *only* a volatile write, then
>>>>             yes, but by having volatile load I don't see how that's
>>>>             valid.
>>>>
>>>>             sent from my phone
>>>>
>>>>             On Jan 15, 2015 9:24 AM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 I don't. It should be allowed to be almost
>>>>                 anything, even a no-op. It is only an optimization
>>>>                 of the busy wait, and it is ok to not optimize, or
>>>>                 just have a micro-pause that doesn't communicate
>>>>                 with unpark. What it cannot be - it cannot be an
>>>>                 implementation that may hang indefinitely, if
>>>>                 appears after unpark (for some definition of
>>>>                 "after"), and can't permit an implementation that
>>>>                 doesn't establish hb between volatiles (before and
>>>>                 after the unpark and park), if park appears after
>>>>                 unpark (again, for some definition of "before" and
>>>>                 "after"). It is this reference to "before" and
>>>>                 "after" that require to relate it somehow to the
>>>>                 synchronization order. Existence of JMM hb edge
>>>>                 between unpark and park is not necessary.
>>>>
>>>>                 Also, read again how Doug specified park - it is
>>>>                 not just a volatile read, but also a write. Let's
>>>>                 not pretend the specification like that is perfect,
>>>>                 or final, or won't be retracted, but that
>>>>                 specification still permits the behaviour I
>>>>                 described - the hang may still occur, because the
>>>>                 normal read of signal from the next iteration is
>>>>                 allowed to "go ahead" of the volatile write of the
>>>>                 "volatile variable" shared with unpark, from the
>>>>                 park of the previous iteration. This is not so for
>>>>                 volatile reads.
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>>>>
>>>>>                 Right, I agree that it can't be a noop.
>>>>>
>>>>>                 sent from my phone
>>>>>
>>>>>                 On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     No, I object.
>>>>>
>>>>>                     If you permit normal writes to be visible in
>>>>>                     program order from unpark(), then no-op cannot
>>>>>                     be a permitted implementation of park() -
>>>>>                     since that would imply any program order
>>>>>                     should be visible in that order after park/unpark.
>>>>>
>>>>>                     Alex
>>>>>
>>>>>                     On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>>>>
>>>>>>                     My interpretation is pretty much what Jason
>>>>>>                     said earlier; loop definitively terminates
>>>>>>                     (eventually) but thread 1 may not see other
>>>>>>                     writes if it never enters the loop.  That
>>>>>>                     seems consistent with Doug's clarification.
>>>>>>
>>>>>>                     sent from my phone
>>>>>>
>>>>>>                     On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>>>>                     <oleksandr.otenko at oracle.com
>>>>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>
>>>>>>                         If park() is seen as a synchronization
>>>>>>                         event, it will still require volatile
>>>>>>                         accesses to establish a hb (it will only
>>>>>>                         stop reordering of volatile accesses).
>>>>>>
>>>>>>                         If park() is seen as a very particular
>>>>>>                         synchronization event that creates a hb
>>>>>>                         edge, then obviously normal reads should
>>>>>>                         observe normal writes and it should
>>>>>>                         terminate, but this is not what people
>>>>>>                         write, and I don't know if that is what
>>>>>>                         is intended (eg see the objections -
>>>>>>                         people didn't think mentioning hb edge
>>>>>>                         was all that useful).
>>>>>>
>>>>>>                         Alex
>>>>>>
>>>>>>                         On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>>>>
>>>>>>>                         I don't think subsequent load of signal
>>>>>>>                         is allowed to appear before park ()
>>>>>>>                         because it was said earlier that park ()
>>>>>>>                         acts like a volatile load; if so,
>>>>>>>                         subsequent loads cannot move before it. 
>>>>>>>                         Did I misunderstand?
>>>>>>>
>>>>>>>                         On Jan 14, 2015 7:20 PM, "Oleksandr
>>>>>>>                         Otenko" <oleksandr.otenko at oracle.com
>>>>>>>                         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>>>
>>>>>>>                             It is not the hoist / remove of
>>>>>>>                             reload, it is the order of reload
>>>>>>>                             and what might be inside park().
>>>>>>>
>>>>>>>                             Normal load of signal of the
>>>>>>>                             subsequent iteration is allowed to
>>>>>>>                             appear ahead of whatever is inside
>>>>>>>                             park() of the previous iteration, as
>>>>>>>                             it is unspecified. The way Doug has
>>>>>>>                             clarified (or the way I wanted to
>>>>>>>                             clarify it), the normal load of
>>>>>>>                             signal can still appear ahead of
>>>>>>>                             whatever except the start of park()
>>>>>>>                             to some threads. park() is allowed
>>>>>>>                             to claim the permit in an
>>>>>>>                             unspecified way. So, it is perfectly
>>>>>>>                             fine for park() to start before
>>>>>>>                             signal==true is apparent, the
>>>>>>>                             subsequent read of signal is allowed
>>>>>>>                             to appear to others ahead of
>>>>>>>                             anything else in park() - ie it is
>>>>>>>                             allowed to appear right after the
>>>>>>>                             decision that it doesn't need to
>>>>>>>                             suspend. It observes signal==false.
>>>>>>>                             Now it is allowed for Thread 2's
>>>>>>>                             store to signal to become apparent
>>>>>>>                             to Thread 1 (but it is not loading
>>>>>>>                             it anymore), and unpark() is allowed
>>>>>>>                             to release one permit. Now it is
>>>>>>>                             allowed for Thread 1 to claim that
>>>>>>>                             permit (or even clear the permit
>>>>>>>                             count without observing that it was
>>>>>>>                             set). Now Thread 1's while loop
>>>>>>>                             thinks signal==false, and it goes to
>>>>>>>                             call park() again, and hangs,
>>>>>>>                             because no more unpark() is called.
>>>>>>>
>>>>>>>                             If park() is deemed as a singular
>>>>>>>                             synchronization event (I don't even
>>>>>>>                             require a happens-before, so no
>>>>>>>                             extra edges appear; visibility of
>>>>>>>                             permits issued by unpark() to park()
>>>>>>>                             become available in an unspecified
>>>>>>>                             manner - only total ordering of
>>>>>>>                             events around these calls in Java
>>>>>>>                             land matters), a volatile load of
>>>>>>>                             signal would not be allowed to
>>>>>>>                             appear to anyone ahead of park() nor
>>>>>>>                             any part thereof - including the
>>>>>>>                             claim of the permit. So if unpark()
>>>>>>>                             was already entered, the volatile
>>>>>>>                             load would observe the volatile
>>>>>>>                             store of signal. Or, if unpark was
>>>>>>>                             not yet entered, then whatever the
>>>>>>>                             load of signal observes, it can at
>>>>>>>                             least go to park() again until it
>>>>>>>                             either observes a permit from
>>>>>>>                             unpark(), or the volatile store,
>>>>>>>                             whichever becomes observed first.
>>>>>>>
>>>>>>>                             Doug's clarification is vague enough
>>>>>>>                             to permit similar interpretation of
>>>>>>>                             how volatile load is different from
>>>>>>>                             normal loads w.r.t. apparent
>>>>>>>                             reordering with parts of park().
>>>>>>>
>>>>>>>                             Alex
>>>>>>>
>>>>>>>                             On 15/01/2015 00:00, Vitaly
>>>>>>>                             Davidovich wrote:
>>>>>>>>                             Could you elaborate? Based on the
>>>>>>>>                             conversation thus far, I don't see
>>>>>>>>                             how compiler is allowed to
>>>>>>>>                             hoist/remove the (re)load of signal
>>>>>>>>                             on the loop iteration.
>>>>>>>>
>>>>>>>>                             On Wed, Jan 14, 2015 at 5:59 PM,
>>>>>>>>                             Oleksandr Otenko
>>>>>>>>                             <oleksandr.otenko at oracle.com
>>>>>>>>                             <mailto:oleksandr.otenko at oracle.com>>
>>>>>>>>                             wrote:
>>>>>>>>
>>>>>>>>                                 No, this code doesn't have to
>>>>>>>>                                 terminate.
>>>>>>>>
>>>>>>>>                                 Alex
>>>>>>>>
>>>>>>>>
>>>>>>>>                                 On 14/01/2015 20:03, thurstonn
>>>>>>>>                                 wrote:
>>>>>>>>
>>>>>>>>                                     Hello Alex,
>>>>>>>>
>>>>>>>>                                     This is what I'm saying:
>>>>>>>>
>>>>>>>>                                     the below code is slightly
>>>>>>>>                                     (but importantly different
>>>>>>>>                                     than our canonical
>>>>>>>>                                     example).
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     boolean signal = false;  //
>>>>>>>>                                     POSV ==> Plain Old Shared
>>>>>>>>                                     Variable
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     Thread 1:
>>>>>>>>                                     while (! signal)
>>>>>>>>                                           park();
>>>>>>>>
>>>>>>>>                                     Thread 2:
>>>>>>>>                                     signal = true;
>>>>>>>>                                     unpark(Thread1)
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     Must this program
>>>>>>>>                                     terminate? YES.
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     Note - I am not advising
>>>>>>>>                                     this code, or trying to be
>>>>>>>>                                     pedantic
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>                                     --
>>>>>>>>                                     View this message in
>>>>>>>>                                     context:
>>>>>>>>                                     http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>>>>                                     Sent from the JSR166
>>>>>>>>                                     Concurrency mailing list
>>>>>>>>                                     archive at Nabble.com.
>>>>>>>>                                     _______________________________________________
>>>>>>>>                                     Concurrency-interest
>>>>>>>>                                     mailing list
>>>>>>>>                                     Concurrency-interest at cs.oswego.edu
>>>>>>>>                                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>                                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>>                                 _______________________________________________
>>>>>>>>                                 Concurrency-interest mailing list
>>>>>>>>                                 Concurrency-interest at cs.oswego.edu
>>>>>>>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/c5160f71/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 13:17:05 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 18:17:05 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421338889094-11964.post@n7.nabble.com>
References: <54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>
	<54B7D6E7.70405@oracle.com>
	<1421338889094-11964.post@n7.nabble.com>
Message-ID: <54B80421.4090400@oracle.com>

That wasn't a history. That was a sloppy mixture of several orders and a 
time line. I thought it might become an issue.

Also, you approached park() from the angle of what implementation makes 
sense, but instead you should approach it from the angle of what's 
allowed according to spec. We can't specify what makes sense.

Proper treatment should have been:

synchronization order:
park loads
unpark stores
park stores
park loads (next iteration)

program order:
park loads
park stores
loads signal (next iteration)
park loads

and

stores signal
unpark stores

As can be seen, synchronization order does not create new hb edges, 
because there are no volatile stores from T1 preceding volatile loads 
from T2. There is no hb edge from stores signal in T1 to loads signal in 
T2, so it may not see the store indefinitely, and the subsequent park 
will be allowed to hang.

If signal is volatile, it will be a completely different story - because 
store and load signal will appear in synchronization order, and will 
always have a hb - either unpark stores before park stores ensures store 
signal is before load signal (hence synchronizes-with), or park stores 
before unpark stores, in which case at least the next park will not 
block indefinitely, which proves there is a signal load after signal 
store in synchronization order, so synchronizes-with still exists. Since 
in this proof the presence of hb between park and unpark was not used, 
that hb doesn't even need to exist.


Alex

On 15/01/2015 16:21, thurstonn wrote:
> oleksandr otenko wrote
>> T2 park loads
>> T1 stores signal
>> T1 unpark stores
>> T2 park stores
>> T2 loads signal
>>
>> There is no hb to stores signal, so any value of signal is possible.
>> There is no requirement for park to wake up on the next iteration,
>> because its store appears after unpark store, so hang is also possible.
>>
>> Alex
> I'm not exactly sure of your history here; so, let's say the shared
> "volatile" between park and unpark is:
> volatile int permits  = 0 // int to make it easy to distinguish between
> #signal
>
> 1. T2 park loads permits (0)
> 2. T1 stores signal
> 3. T1 unpark stores  (permits = 1)
> 4. T2 park stores   ????? why would it store anything (it read 0)
> 5. T2 loads signal
>
> sure, if 4 becomes:
> T2 park stores (permits = 0)
> then yes it could loop forever,but that's just a broken implementation of
> park (non-atomic RMW).
>
> Maybe I'm missing something
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11964.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Thu Jan 15 13:30:01 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Jan 2015 13:30:01 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7FC50.7020108@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com>
Message-ID: <54B80729.7030609@cs.oswego.edu>

On 01/15/2015 12:43 PM, Oleksandr Otenko wrote:

> I think someone first needs to explain the need for normal accesses to become
> visible. Some old-school even don't think the hb between unpark-park is needed.
> To my mind, any clarification is only needed to explain global ordering of
> volatile accesses and park/unpark - and that only being picky. If such
> clarification only creates false hopes or flawed reasoning, then forget it,
> let's keep the documentation as it was before.
>

That's my more-good or more-harm question. The added sentence
doesn't specify anything that any broken usage might be trying to
rely on. (Thanks for laboriously demonstrating this!) So in that
sense is useless. On the other hand, it seems marginally better
to provide more vs less information in these cases.

-Doug


From kasperni at gmail.com  Thu Jan 15 14:14:59 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Thu, 15 Jan 2015 20:14:59 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B7F7FD.6000507@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>
Message-ID: <CAPs6153oD6ZyaETZaOKJP6Pf7VaeTCbQRJsnPHbfdHh8wtjoNg@mail.gmail.com>

On Thu, Jan 15, 2015 at 6:25 PM, Doug Lea <dl at cs.oswego.edu> wrote:

>
> Also, the new stand-alone class SubmissionPublisher can serve as a
> bridge from various kinds of item producers to Flow components, and is
> useful in its own right. It is a form of ordered multicaster that
> people have complained that we don't support in j.u.c, but now do.
> See
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
> SubmissionPublisher.html
>
>
I don't if it is an oversight but shouldn't SubmissionPublished allow for
the cancellation of subscriptions.
For example, by letting SubmissionPublished.subscribe() return a
subscription object you can use?

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/541f16f8/attachment.html>

From dl at cs.oswego.edu  Thu Jan 15 14:28:36 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Jan 2015 14:28:36 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAPs6153oD6ZyaETZaOKJP6Pf7VaeTCbQRJsnPHbfdHh8wtjoNg@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<CAPs6153oD6ZyaETZaOKJP6Pf7VaeTCbQRJsnPHbfdHh8wtjoNg@mail.gmail.com>
Message-ID: <54B814E4.9080701@cs.oswego.edu>

On 01/15/2015 02:14 PM, Kasper Nielsen wrote:
>     <http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html>
>
>
> I don't if it is an oversight but shouldn't SubmissionPublished allow for the
> cancellation of subscriptions.
> For example, by letting SubmissionPublished.subscribe() return a subscription
> object you can use?

Publisher.subscribe doesn't return a subscription, it instead invokes
onSubscribe with one. See the usage example (pasted from javadoc) below
for Subscriber side usage.

The APIs have a few tradeoffs like this that make them usable
across both concurrent and distributed contexts. It would be
possible to add a second form of subscribe to the implementation class
SubmissionPublisher, that would sometimes be more convenient, but also
more confusing, so probably not worthwhile.


      class SampleSubscriber<T> implements Subscriber<T> {
        final Consumer<? super T> consumer;
        Subscription subscription;
        final long requestSize;
        long count;
        SampleSubscriber(long requestSize, Consumer<? super T> consumer) {
          this.requestSize = requestSize;
          this.consumer = consumer;
        }
        public void onSubscribe(Subscription subscription) {
          count = requestSize / 2; // re-request when half consumed
          (this.subscription = subscription).request(requestSize);
        }
        public void onNext(T item) {
          if (--count <= 0)
            subscription.request(count = requestSize);
          consumer.accept(item);
        }
        public void onError(Throwable ex) { ex.printStackTrace(); }
        public void onComplete() {}
      }



From dl at cs.oswego.edu  Thu Jan 15 14:50:25 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Jan 2015 14:50:25 -0500
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <CAPs6151CS8_dopj+8yRrG56+KqEA50eUvTtdHjNfK8hN0h2Dsg@mail.gmail.com>
References: <54B6A2CE.6040709@cs.oswego.edu>
	<54B6C2F8.7070804@univ-mlv.fr>	<54B70F59.6050100@cs.oswego.edu>
	<CAPs6151CS8_dopj+8yRrG56+KqEA50eUvTtdHjNfK8hN0h2Dsg@mail.gmail.com>
Message-ID: <54B81A01.3010800@cs.oswego.edu>

On 01/15/2015 04:01 AM, Kasper Nielsen wrote:
>     On 01/14/2015 05:12 PM, Kasper Nielsen wrote:
>
>         public static <U> CompletionStage<U> completedStage(U value)
>         public static <U> CompletionStage<U> failedStage(Throwable ex)>
>
>         I think it makes more sense to have these methods on CompletionStage instead
>         of CompletionFuture?
>
>
>     It would, but static methods are not allowed in interfaces.
>
>
> Since Java 8 they are.
>

Right; thanks. I'm wavering about whether we want to do this here though.
Keeping CompletionStage all-abstract and CompletableFuture all-concrete
seems mildly preferable. But other opinions would be welcome.

-Doug



From jsampson at guidewire.com  Thu Jan 15 15:47:24 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 20:47:24 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B80729.7030609@cs.oswego.edu>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>

Wow! So many interesting twists and turns in this discussion. I
won't try to respond to individual messages since it's moving so
fast.

Doug, the existing docs suggest that the permit is treated like a
semaphore. If so, then releasing the permit in unpark() and
acquiring it in park() _are_ atomic operations, and there _is_ a
proper happens-before edge from the release to the acquire.

Alex, I think you're essentially disagreeing on the atomicity. But
how can the semantics of the permit possibly be implemented without
the whole operation being atomic?

If we imagine pseudo-code like this:

Thread { volatile boolean permit = false; }

static void park() {
  if (!currentThread().permit) {
    Scheduler.stopRunning(currentThread());
  }
  permit = false;
}

static void unpark(thread) {
  thread.permit = true;
  Scheduler.startRunningIfStopped(thread);
}

Then without atomicity some very bad executions are possible:

T1: if (!permit) // sees permit == false
T2: permit = true;
T2: Scheduler.startRunningIfStopped(thread); // does nothing
T1: Scheduler.stopRunning(currentThread());

That is, with the permit simply being a volatile field, without
atomicity, and without even any reordering, park() might block
indefinitely even through unpark() has been called.

So this _can't_ be the way that park() and unpark() are implemented.
The handling of the permit _must_ be integrated atomically into the
thread scheduling mechanism itself, mustn't it?

Cheers,
Justin


From oleksandr.otenko at oracle.com  Thu Jan 15 17:05:19 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 22:05:19 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
Message-ID: <54B8399F.6000106@oracle.com>

Justin,

No, with two non-atomic writes you can make sure you didn't miss the 
permit being issued, if you have a idempotent operation (reschedule() 
below is a idempotent operation that may result in what we perceive as 
spurious wake-ups). The volatile variable then guards expensive trips 
into kernel. There may be atomic operations somewhere deep, but on the 
surface non-atomic ops will suffice - we don't expect high contention on 
wake-up calls, do we.

pseudo code:
volatile boolean suspended=false;
volatile boolean park=true;

park(){
   suspended=true;
   if (park) yield(); // if parking, pick a new task from the list of 
tasks for this CPU; there may be some atomic op inside to pick the head 
of the list, but then, there may not - could be just HLT; it doesn't 
even matter
   // return point whose address OS scheduler knows, is inside yield()
   park=true;
   suspended=false;
}

unpark(Thread t){
   t.park=false;
   if (t.suspended) reschedule(t); // tell OS scheduler to run the 
thread again - it knows the return point; if more than one return points 
possible, depending on context, then mark used return points as stale
   // rescheduling may involve some queue manipulation
}


Alex


On 15/01/2015 20:47, Justin Sampson wrote:
> Wow! So many interesting twists and turns in this discussion. I
> won't try to respond to individual messages since it's moving so
> fast.
>
> Doug, the existing docs suggest that the permit is treated like a
> semaphore. If so, then releasing the permit in unpark() and
> acquiring it in park() _are_ atomic operations, and there _is_ a
> proper happens-before edge from the release to the acquire.
>
> Alex, I think you're essentially disagreeing on the atomicity. But
> how can the semantics of the permit possibly be implemented without
> the whole operation being atomic?
>
> If we imagine pseudo-code like this:
>
> Thread { volatile boolean permit = false; }
>
> static void park() {
>    if (!currentThread().permit) {
>      Scheduler.stopRunning(currentThread());
>    }
>    permit = false;
> }
>
> static void unpark(thread) {
>    thread.permit = true;
>    Scheduler.startRunningIfStopped(thread);
> }
>
> Then without atomicity some very bad executions are possible:
>
> T1: if (!permit) // sees permit == false
> T2: permit = true;
> T2: Scheduler.startRunningIfStopped(thread); // does nothing
> T1: Scheduler.stopRunning(currentThread());
>
> That is, with the permit simply being a volatile field, without
> atomicity, and without even any reordering, park() might block
> indefinitely even through unpark() has been called.
>
> So this _can't_ be the way that park() and unpark() are implemented.
> The handling of the permit _must_ be integrated atomically into the
> thread scheduling mechanism itself, mustn't it?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Thu Jan 15 17:06:20 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 15 Jan 2015 23:06:20 +0100
Subject: [concurrency-interest] Candidate jdk9 CompletableFuture
	additions
In-Reply-To: <54B81A01.3010800@cs.oswego.edu>
References: <54B6A2CE.6040709@cs.oswego.edu> <54B6C2F8.7070804@univ-mlv.fr>
	<54B70F59.6050100@cs.oswego.edu>
	<CAPs6151CS8_dopj+8yRrG56+KqEA50eUvTtdHjNfK8hN0h2Dsg@mail.gmail.com>
	<54B81A01.3010800@cs.oswego.edu>
Message-ID: <CANPzfU-xZNmDE0oefLi+B-ok8ZFTx4vp1DTLOX54QMfNbj33-Q@mail.gmail.com>

Keeping the interfaces as untangled as possible would be nice IMO

-- 
Cheers,
?
On 15 Jan 2015 21:12, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/15/2015 04:01 AM, Kasper Nielsen wrote:
>
>>     On 01/14/2015 05:12 PM, Kasper Nielsen wrote:
>>
>>         public static <U> CompletionStage<U> completedStage(U value)
>>         public static <U> CompletionStage<U> failedStage(Throwable ex)>
>>
>>         I think it makes more sense to have these methods on
>> CompletionStage instead
>>         of CompletionFuture?
>>
>>
>>     It would, but static methods are not allowed in interfaces.
>>
>>
>> Since Java 8 they are.
>>
>>
> Right; thanks. I'm wavering about whether we want to do this here though.
> Keeping CompletionStage all-abstract and CompletableFuture all-concrete
> seems mildly preferable. But other opinions would be welcome.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/6b80fe08/attachment-0001.html>

From viktor.klang at gmail.com  Thu Jan 15 17:12:55 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 15 Jan 2015 23:12:55 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B814E4.9080701@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<CAPs6153oD6ZyaETZaOKJP6Pf7VaeTCbQRJsnPHbfdHh8wtjoNg@mail.gmail.com>
	<54B814E4.9080701@cs.oswego.edu>
Message-ID: <CANPzfU8FO7OvAt_3JYQfbFvOwZ=vXStRbvY0rGUN-r--meAObQ@mail.gmail.com>

What Doug said :-)

-- 
Cheers,
?
On 15 Jan 2015 20:50, "Doug Lea" <dl at cs.oswego.edu> wrote:

> On 01/15/2015 02:14 PM, Kasper Nielsen wrote:
>
>>     <http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>> SubmissionPublisher.html>
>>
>>
>> I don't if it is an oversight but shouldn't SubmissionPublished allow for
>> the
>> cancellation of subscriptions.
>> For example, by letting SubmissionPublished.subscribe() return a
>> subscription
>> object you can use?
>>
>
> Publisher.subscribe doesn't return a subscription, it instead invokes
> onSubscribe with one. See the usage example (pasted from javadoc) below
> for Subscriber side usage.
>
> The APIs have a few tradeoffs like this that make them usable
> across both concurrent and distributed contexts. It would be
> possible to add a second form of subscribe to the implementation class
> SubmissionPublisher, that would sometimes be more convenient, but also
> more confusing, so probably not worthwhile.
>
>
>      class SampleSubscriber<T> implements Subscriber<T> {
>        final Consumer<? super T> consumer;
>        Subscription subscription;
>        final long requestSize;
>        long count;
>        SampleSubscriber(long requestSize, Consumer<? super T> consumer) {
>          this.requestSize = requestSize;
>          this.consumer = consumer;
>        }
>        public void onSubscribe(Subscription subscription) {
>          count = requestSize / 2; // re-request when half consumed
>          (this.subscription = subscription).request(requestSize);
>        }
>        public void onNext(T item) {
>          if (--count <= 0)
>            subscription.request(count = requestSize);
>          consumer.accept(item);
>        }
>        public void onError(Throwable ex) { ex.printStackTrace(); }
>        public void onComplete() {}
>      }
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/cfca2048/attachment.html>

From thurston at nomagicsoftware.com  Thu Jan 15 17:15:05 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 15 Jan 2015 15:15:05 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
References: <CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
Message-ID: <1421360105102-11978.post@n7.nabble.com>

So I think Alex is saying is the following,slightly modified version, is
perfectly acceptable:


Thread { volatile boolean permit = false; }

static void park() 
{
  Thread self = currentThread();
  if (! self.permit) {
    Scheduler.pause(self, max:2 seconds);
  }
  self.permit = false;
}

static void unpark(thread) {
  thread.permit = true;
  Scheduler.startRunningIfStopped(thread);
} 

This code ensures 2 things:
the volatile boolean signal:

while ! signal
   park()
- - - - - - -
signal = true;
unpark(thread1)

is provably correct (will always terminate)


and importantly, that the same code, but with signal as a POSV,
is provably *incorrect* (i.e. there is at least one valid history where the
program never terminates) - specifically in the case where there is no
hb(unpark, depark), (and that could happen), the program may never
terminate.

and at first glance, it looks right to me.
It means that the implementation of unpark() needs to "provide" a
synchronization action (thread.permit = true in the above code), but any
other guarantee is gratuitous; it logically follows that park() could be a
no op and no explicit hb guarantee is necessary

I don't see how unpark can be a no-op - the synchronization action is what
guarantees that all valid histories must have:
signal = true < unpark(thread), which we all agreed is necessary for the
volatile idiom to work and even for unpark/park to be useful






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11978.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Thu Jan 15 17:20:07 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 22:20:07 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B8399F.6000106@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> pseudo code:
> volatile boolean suspended=false;
> volatile boolean park=true;
>
> park(){
>   suspended=true;
>   if (park) yield();
>   park=true;
>   suspended=false;
> }
>
> unpark(Thread t){
>   t.park=false;
>   if (t.suspended) reschedule(t);
> }

I'm pretty sure that suffers from the same race condition I just
described...

T1: suspended=true;
T1: if (park) // sees true
T2: t.park = false;
T2: if (t.suspended) // sees true
T2: reschedule(t); // does nothing
T1: yield(); // hangs!

That is, unless you really meant yield() as essentially a no-op
anyway...

I'm writing up another reply with another strawman implementation to
discuss and another doc wording proposal. The discussion just
continues to move so fast!

Cheers,
Justin


From boehm at acm.org  Thu Jan 15 17:21:05 2015
From: boehm at acm.org (Hans Boehm)
Date: Thu, 15 Jan 2015 14:21:05 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
Message-ID: <CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>

I don't think it makes sense in the memory model to say something has
volatile write semantics if nothing is actually written.  The spec could
perhaps be clearer that it's not saying that.  But if it does, that's
arguably a vacuous statement anyway.

It does seem to be (barely) observable whether a write of the value that
was already there has volatile semantics or not:

(v volatile, everything initially zero)
Thread 1:
x = 1;
v = 0;

Thread 2:
v = 1;

Thread 3:
r1 = v;
r2 = x;

After all threads are joined:
r3 = v;

If r3 = 1, we know that v = 0 preceded v = 1 in the synchronization order.
Hence r1 = r3 = 1 must imply that v = 0 and v = 1 both synchronize with r1
= v and hence r2 = 1.  I think that would not be true if the v = 0
assignment were not volatile.

Thus it does seem to matter whether we atomically replace a value by itself
or don't write at all.

Hans

On Wed, Jan 14, 2015 at 2:02 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> Howdy,
>
> The recent discussion of alternative CAS implementations inspired me
> to read through the existing docs & code in j.u.c.atomic, whereupon
> I noticed some discrepancies in semantics.
>
> For this discussion I'll focus on AtomicReference and
> AtomicStampedReference; the other single-value atomics follow the
> semantics of AtomicReference while AtomicMarkableReference follows
> the semantics of AtomicStampedReference.
>
> The package documentation indicates that compareAndSet is strong in
> two ways: it cannot fail spuriously _and_ it provides the memory
> effects of _both_ a volatile read and a volatile write. In contrast,
> weakCompareAndSet is documented to be weak in both of those
> respects, allowing spurious failure and not providing any memory
> consistency effects outside of the atomic variable itself.
>
> AtomicReference et al. implement compareAndSet by calling into the
> intrinsic CAS operation on Unsafe, so I'll assume they behave as
> documented.
>
> AtomicStampedReference, on the other hand, actually implements
> something in-between strong and weak CAS semantics, without that
> fact being documented. Due to the indirection through a private
> Pair object, this class's compareAndSet implementation has to do
> some of the work itself before calling into the intrinsic CAS. As a
> result, the actual semantics are:
>
> 1. It always provides the memory effects of a volatile read.
>
> 2. It is only guaranteed to provide the memory effects of a volatile
>    write _if_ the stamp or reference is successfully _altered_ to a
>    different value than it had before.
>
> 3. It may fail spuriously.
>
> The attemptStamp method has exactly the same semantics. The spurious
> failure is at least documented for attemptStamp, but not for
> compareAndSet, and neither one documents the lack of volatile write
> effects on failure or no-op success.
>
> Now, I think these semantics are entirely fine. They seem like the
> most intuitive and implementable semantics for CAS in general. If
> the intrinsic CAS weren't already spec'd to be strong for many years
> I'd advocate for it to have these semantics as well. :) There's no
> practical need for volatile write effects unless you've actually
> altered the value, since otherwise no other threads are going to
> notice you've done anything anyway. And the usual idiom is to call
> CAS in a loop, so spurious failure is perfectly acceptable.
>
> Therefore I certainly don't want to advocate for changing the
> implementation in AtomicStampedReference, but some clarification of
> the docs seems in order.
>
> What would y'all think of changing the _package_ docs to describe
> this looser kind of semantics, as a _minimum_ strength for _all_
> compareAndSet implementations in the package? I believe it's strong
> enough for any practical usage, so for most purposes a programmer
> need look no further. In those cases such as AtomicReference where
> compareAndSet is implemented much stronger, it can be documented on
> that class.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150115/96f105ed/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 17:25:14 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 22:25:14 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B8399F.6000106@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
Message-ID: <54B83E4A.4040209@oracle.com>

well, this would work even with one volatile - perhaps, marginally less 
efficiently, but closer to the point that it's feasible.

park(){
   if (park) yield(); // pick a runnable thread from the head of the 
queue; contend with other CPUs who gets to run it - perhaps, an atomic 
operation
   park=true;
}

unpark(Thread t){
   t.park=false;
   reschedule(t); // just always add to the head of queue of runnable 
threads
}

The point here is that even if it looks less efficient, and does contain 
some atomic, the execution of that operation is not guaranteed, so the 
method as a whole doesn't guarantee atomicity or barriers of that kind.

It may race to reschedule the same thread at the time it was leaving 
park() for whatever reason (previous unpark, or perhaps an 
interruption), and unparks may be called more times than park - but all 
that will do, is add the thread to the queue many times, and CPUs will 
pick it up eventually. They will either contend to execute it, and those 
failing to grab the task just silently discard it, or may run it many 
times without obvious reason - without unpark that may seem to wake it 
up - that's the nature of spurious wake-ups for you.

Alex

On 15/01/2015 22:05, Oleksandr Otenko wrote:
> Justin,
>
> No, with two non-atomic writes you can make sure you didn't miss the 
> permit being issued, if you have a idempotent operation (reschedule() 
> below is a idempotent operation that may result in what we perceive as 
> spurious wake-ups). The volatile variable then guards expensive trips 
> into kernel. There may be atomic operations somewhere deep, but on the 
> surface non-atomic ops will suffice - we don't expect high contention 
> on wake-up calls, do we.
>
> pseudo code:
> volatile boolean suspended=false;
> volatile boolean park=true;
>
> park(){
>   suspended=true;
>   if (park) yield(); // if parking, pick a new task from the list of 
> tasks for this CPU; there may be some atomic op inside to pick the 
> head of the list, but then, there may not - could be just HLT; it 
> doesn't even matter
>   // return point whose address OS scheduler knows, is inside yield()
>   park=true;
>   suspended=false;
> }
>
> unpark(Thread t){
>   t.park=false;
>   if (t.suspended) reschedule(t); // tell OS scheduler to run the 
> thread again - it knows the return point; if more than one return 
> points possible, depending on context, then mark used return points as 
> stale
>   // rescheduling may involve some queue manipulation
> }
>
>
> Alex
>
>
> On 15/01/2015 20:47, Justin Sampson wrote:
>> Wow! So many interesting twists and turns in this discussion. I
>> won't try to respond to individual messages since it's moving so
>> fast.
>>
>> Doug, the existing docs suggest that the permit is treated like a
>> semaphore. If so, then releasing the permit in unpark() and
>> acquiring it in park() _are_ atomic operations, and there _is_ a
>> proper happens-before edge from the release to the acquire.
>>
>> Alex, I think you're essentially disagreeing on the atomicity. But
>> how can the semantics of the permit possibly be implemented without
>> the whole operation being atomic?
>>
>> If we imagine pseudo-code like this:
>>
>> Thread { volatile boolean permit = false; }
>>
>> static void park() {
>>    if (!currentThread().permit) {
>>      Scheduler.stopRunning(currentThread());
>>    }
>>    permit = false;
>> }
>>
>> static void unpark(thread) {
>>    thread.permit = true;
>>    Scheduler.startRunningIfStopped(thread);
>> }
>>
>> Then without atomicity some very bad executions are possible:
>>
>> T1: if (!permit) // sees permit == false
>> T2: permit = true;
>> T2: Scheduler.startRunningIfStopped(thread); // does nothing
>> T1: Scheduler.stopRunning(currentThread());
>>
>> That is, with the permit simply being a volatile field, without
>> atomicity, and without even any reordering, park() might block
>> indefinitely even through unpark() has been called.
>>
>> So this _can't_ be the way that park() and unpark() are implemented.
>> The handling of the permit _must_ be integrated atomically into the
>> thread scheduling mechanism itself, mustn't it?
>>
>> Cheers,
>> Justin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From davidcholmes at aapt.net.au  Thu Jan 15 17:30:52 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Jan 2015 08:30:52 +1000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B7CD9B.1070908@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>

At the risk of never escaping from this quagmire ... :)

With the additional spec the implementation can't be a no-op, it must contain the stated volatile accesses.

Th atomicity argument has me lost. Like Justin later says if this is a real blocking based implementation then there has to be some atomicity for it work correctly. But what that has to do with the HB discussion I have no idea.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr Otenko
  Sent: Friday, 16 January 2015 12:24 AM
  To: Vitaly Davidovich
  Cc: thurston; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] unpark/park memory visibility


  I don't. It should be allowed to be almost anything, even a no-op. It is only an optimization of the busy wait, and it is ok to not optimize, or just have a micro-pause that doesn't communicate with unpark. What it cannot be - it cannot be an implementation that may hang indefinitely, if appears after unpark (for some definition of "after"), and can't permit an implementation that doesn't establish hb between volatiles (before and after the unpark and park), if park appears after unpark (again, for some definition of "before" and "after"). It is this reference to "before" and "after" that require to relate it somehow to the synchronization order. Existence of JMM hb edge between unpark and park is not necessary.

  Also, read again how Doug specified park - it is not just a volatile read, but also a write. Let's not pretend the specification like that is perfect, or final, or won't be retracted, but that specification still permits the behaviour I described - the hang may still occur, because the normal read of signal from the next iteration is allowed to "go ahead" of the volatile write of the "volatile variable" shared with unpark, from the park of the previous iteration. This is not so for volatile reads.

  Alex


  On 15/01/2015 14:07, Vitaly Davidovich wrote:

    Right, I agree that it can't be a noop.

    sent from my phone

    On Jan 15, 2015 8:26 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:

      No, I object.

      If you permit normal writes to be visible in program order from unpark(), then no-op cannot be a permitted implementation of park() - since that would imply any program order should be visible in that order after park/unpark.

      Alex


      On 15/01/2015 01:26, Vitaly Davidovich wrote:

        My interpretation is pretty much what Jason said earlier; loop definitively terminates (eventually) but thread 1 may not see other writes if it never enters the loop.  That seems consistent with Doug's clarification.

        sent from my phone

        On Jan 14, 2015 8:16 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:

          If park() is seen as a synchronization event, it will still require volatile accesses to establish a hb (it will only stop reordering of volatile accesses).

          If park() is seen as a very particular synchronization event that creates a hb edge, then obviously normal reads should observe normal writes and it should terminate, but this is not what people write, and I don't know if that is what is intended (eg see the objections - people didn't think mentioning hb edge was all that useful).

          Alex


          On 15/01/2015 00:27, Vitaly Davidovich wrote:

            I don't think subsequent load of signal is allowed to appear before park () because it was said earlier that park () acts like a volatile load; if so, subsequent loads cannot move before it.  Did I misunderstand?

            On Jan 14, 2015 7:20 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:

              It is not the hoist / remove of reload, it is the order of reload and what might be inside park().

              Normal load of signal of the subsequent iteration is allowed to appear ahead of whatever is inside park() of the previous iteration, as it is unspecified. The way Doug has clarified (or the way I wanted to clarify it), the normal load of signal can still appear ahead of whatever except the start of park() to some threads. park() is allowed to claim the permit in an unspecified way. So, it is perfectly fine for park() to start before signal==true is apparent, the subsequent read of signal is allowed to appear to others ahead of anything else in park() - ie it is allowed to appear right after the decision that it doesn't need to suspend. It observes signal==false. Now it is allowed for Thread 2's store to signal to become apparent to Thread 1 (but it is not loading it anymore), and unpark() is allowed to release one permit. Now it is allowed for Thread 1 to claim that permit (or even clear the permit count without observing that it was set). Now Thread 1's while loop thinks signal==false, and it goes to call park() again, and hangs, because no more unpark() is called.

              If park() is deemed as a singular synchronization event (I don't even require a happens-before, so no extra edges appear; visibility of permits issued by unpark() to park() become available in an unspecified manner - only total ordering of events around these calls in Java land matters), a volatile load of signal would not be allowed to appear to anyone ahead of park() nor any part thereof - including the claim of the permit. So if unpark() was already entered, the volatile load would observe the volatile store of signal. Or, if unpark was not yet entered, then whatever the load of signal observes, it can at least go to park() again until it either observes a permit from unpark(), or the volatile store, whichever becomes observed first.

              Doug's clarification is vague enough to permit similar interpretation of how volatile load is different from normal loads w.r.t. apparent reordering with parts of park().

              Alex


              On 15/01/2015 00:00, Vitaly Davidovich wrote:

                Could you elaborate? Based on the conversation thus far, I don't see how compiler is allowed to hoist/remove the (re)load of signal on the loop iteration.


                On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:

                  No, this code doesn't have to terminate.

                  Alex 


                  On 14/01/2015 20:03, thurstonn wrote:

                    Hello Alex,

                    This is what I'm saying:

                    the below code is slightly (but importantly different than our canonical
                    example).


                    boolean signal = false;  // POSV ==> Plain Old Shared Variable


                    Thread 1:
                    while (! signal)
                          park();

                    Thread 2:
                    signal = true;
                    unpark(Thread1)



                    Must this program terminate?  YES.


                    Note - I am not advising this code, or trying to be pedantic





                    --
                    View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
                    Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
                    _______________________________________________
                    Concurrency-interest mailing list
                    Concurrency-interest at cs.oswego.edu
                    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


                  _______________________________________________
                  Concurrency-interest mailing list
                  Concurrency-interest at cs.oswego.edu
                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/d6082e6f/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 15 17:33:57 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 22:33:57 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
Message-ID: <54B84055.2000701@oracle.com>

No, making reschedule and yield work in a mutually exclusive manner is 
out of scope. The point is you may have a "fast path" in park guarded by 
a non-atomic, but it won't always go down that path. Yet you get the 
weakest guarantees of all the paths.

Alex

On 15/01/2015 22:20, Justin Sampson wrote:
> Oleksandr Otenko wrote:
>
>> pseudo code:
>> volatile boolean suspended=false;
>> volatile boolean park=true;
>>
>> park(){
>>    suspended=true;
>>    if (park) yield();
>>    park=true;
>>    suspended=false;
>> }
>>
>> unpark(Thread t){
>>    t.park=false;
>>    if (t.suspended) reschedule(t);
>> }
> I'm pretty sure that suffers from the same race condition I just
> described...
>
> T1: suspended=true;
> T1: if (park) // sees true
> T2: t.park = false;
> T2: if (t.suspended) // sees true
> T2: reschedule(t); // does nothing
> T1: yield(); // hangs!
>
> That is, unless you really meant yield() as essentially a no-op
> anyway...
>
> I'm writing up another reply with another strawman implementation to
> discuss and another doc wording proposal. The discussion just
> continues to move so fast!
>
> Cheers,
> Justin


From jsampson at guidewire.com  Thu Jan 15 17:48:10 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 22:48:10 +0000
Subject: [concurrency-interest] unpark/park memory visibility
References: <1420743543652-11812.post@n7.nabble.com>
	<1421262521215-11932.post@n7.nabble.com>	<54B6C607.9040706@oracle.com>
	<1421265817182-11935.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com> 
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83AB4@sm-ex-01-vm.guidewire.com>

Okay, here's a strawman spinning implementation that isn't atomic
but does the right thing with the permit:

volatile boolean permit = false;

park() {
  while (!permit) {}
  permit = false;
}

unpark() {
  permit = true;
}

As soon as park() sees permit == true, there is indeed a proper
happens-before from the write in unpark(). Normal reads after the
park() call can't be reordered before the read of permit that saw
the true value set by the unpark() call.

However, if _another_ thread called unpark() concurrently, with
its write ordered between the park() call's last read but before its
write of false, there's no happens-before from it to any part of the
park(). It has a definite place in the synchronization order, so
preceding and following _volatile_ accesses can't be reordered, but
normal accesses can.

So, is this a legal implementation?

If so, here's yet another wording proposal, combining the elements I
like best from my earlier ones:

"Memory consistency effects: Any unpark call that releases a permit
*or* observes that a permit is already available necessarily
precedes, in the overall synchronization order, the return of the
park call that acquires that particular released or observed permit.
No additional _happens-before_ guarantees are provided."

I believe that with this wording, a no-op implementation is valid,
and yet the ordering is just strong enough for all practical
purposes.

Cheers,
Justin :)


From forax at univ-mlv.fr  Thu Jan 15 17:59:39 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Thu, 15 Jan 2015 23:59:39 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B7F7FD.6000507@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>
Message-ID: <54B8465B.4020001@univ-mlv.fr>

I think it's too soon to try to include an API like this in JDK9.
Currently, everybody seems to think it can come with a better API for a 
reactive framework.
I think it's better to wait and see.

R?mi

On 01/15/2015 06:25 PM, Doug Lea wrote:
>
> Here's the only set of candidates for new jdk9 j.u.c classes:
>
> As discussed a few months ago, there is no single best fluent
> async/parallel API. CompletableFuture/CompletionStage best supports
> continuation-style programming on futures, and java.util.stream best
> supports (multi-stage, possibly-parallel) "pull" style operations on
> the elements of collections. Until now, one missing category was
> "push" style operations on items as they become available from an
> active source. We are not alone in wanting a standard way to support
> this. Over the past year, the "reactive-streams"
> (http://www.reactive-streams.org/) effort has been defining a minimal
> set of interfaces expressing commonalities and allowing
> interoperablility across frameworks (including Rx and Akka Play), that
> is nearing release. These interfaces include provisions for a simple
> form of async flow control allowing developers to address resource
> control issues that can otherwise cause problems in push-based
> systems. Supporting this mini-framework helps avoid unpleasant
> surprises possible when trying to use pull-style APIs for "hot"
> reactive sources (but conversely is not as good a choice as
> java.util.Stream for "cold" sources like collections).
>
> The four intertwined interfaces (Publisher, Subscriber, Subscription,
> Processor) are defined within the same class "Flow", that also
> includes the first of some planned support methods to establish and
> use Flow components, including tie-ins to java.util.streams and
> CompletableFutures. See
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html 
>
>
> (Logistically, the only alternative to class Flow would have been
> to introduce a subpackage, which unnecessarily complicates usage. And
> "import static java.util.concurrent.Flow;" is about as convenient as
> "import java.util.concurrent.flow.*;" would be.)
>
> Also, the new stand-alone class SubmissionPublisher can serve as a
> bridge from various kinds of item producers to Flow components, and is
> useful in its own right. It is a form of ordered multicaster that
> people have complained that we don't support in j.u.c, but now do.
> See
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html 
>
>
> Disclaimers: These are only candidates for inclusion.  The are in
> preliminary form and will change. But comments and suggestions would
> be welcome. As with the other candidate additions, if you are brave,
> you can try out snapshots on jdk8+ by getting
>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
> and running java -Xbootclasspath/p:jsr166.jar
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Thu Jan 15 18:03:33 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 23:03:33 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421360105102-11978.post@n7.nabble.com>
References: <CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<1421360105102-11978.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83AD2@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> I don't see how unpark can be a no-op - the synchronization action
> is what guarantees that all valid histories must have:
> signal = true < unpark(thread), which we all agreed is necessary
> for the volatile idiom to work and even for unpark/park to be
> useful

Given a no-op implementation of park() and unpark(), simply pick any
point in the synchronization order between the closest preceding and
following synchronization actions in each thread. It doesn't matter
where. Now look at each park() call: If there aren't any unpark()
calls in the synchronization order since the last park() call, say
that this park() call spuriously returns. If there is any unpark()
call since the last park() call, say that this park() call acquires
the permit that was released by that unpark() call. Either way,
unpark() and park() both always return immediately. Their placement
in the synchronization order is arbitrary. All that matters is the
synchronization order of the actual synchronization actions on each
thread, which isn't actually affected by the park() and the unpark()
calls. It only matters if park() is actually going to suspend the
thread, because then there's actually some observable semantics to
the whole permit business -- _unless_ park() and unpark() are spec'd
to always have a happens-before edge, which is what we're debating!

Your example will always terminate with no-op park() and unpark():

> while !signal
>    park()
> - - - - - - -
> signal = true;
> unpark(thread1)

Becomes:

while !signal {}
- - - - - - -
signal = true;

Cheers,
Justin


From oleksandr.otenko at oracle.com  Thu Jan 15 18:05:48 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 23:05:48 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83AB4@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B6F4B5.5000506@oracle.com>	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83AB4@sm-ex-01-vm.guidewire.com>
Message-ID: <54B847CC.7080308@oracle.com>

Yes, that's legal.

Alex

On 15/01/2015 22:48, Justin Sampson wrote:
> Okay, here's a strawman spinning implementation that isn't atomic
> but does the right thing with the permit:
>
> volatile boolean permit = false;
>
> park() {
>    while (!permit) {}
>    permit = false;
> }
>
> unpark() {
>    permit = true;
> }
>
> As soon as park() sees permit == true, there is indeed a proper
> happens-before from the write in unpark(). Normal reads after the
> park() call can't be reordered before the read of permit that saw
> the true value set by the unpark() call.
>
> However, if _another_ thread called unpark() concurrently, with
> its write ordered between the park() call's last read but before its
> write of false, there's no happens-before from it to any part of the
> park(). It has a definite place in the synchronization order, so
> preceding and following _volatile_ accesses can't be reordered, but
> normal accesses can.
>
> So, is this a legal implementation?
>
> If so, here's yet another wording proposal, combining the elements I
> like best from my earlier ones:
>
> "Memory consistency effects: Any unpark call that releases a permit
> *or* observes that a permit is already available necessarily
> precedes, in the overall synchronization order, the return of the
> park call that acquires that particular released or observed permit.
> No additional _happens-before_ guarantees are provided."
>
> I believe that with this wording, a no-op implementation is valid,
> and yet the ordering is just strong enough for all practical
> purposes.
>
> Cheers,
> Justin :)


From oleksandr.otenko at oracle.com  Thu Jan 15 18:08:13 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 15 Jan 2015 23:08:13 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421360105102-11978.post@n7.nabble.com>
References: <CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<1421360105102-11978.post@n7.nabble.com>
Message-ID: <54B8485D.1070604@oracle.com>

Seems correct.

unpark as a no-op makes sense when park is a no-op, too.

Alex

On 15/01/2015 22:15, thurstonn wrote:
> So I think Alex is saying is the following,slightly modified version, is
> perfectly acceptable:
>
>
> Thread { volatile boolean permit = false; }
>
> static void park()
> {
>    Thread self = currentThread();
>    if (! self.permit) {
>      Scheduler.pause(self, max:2 seconds);
>    }
>    self.permit = false;
> }
>
> static void unpark(thread) {
>    thread.permit = true;
>    Scheduler.startRunningIfStopped(thread);
> }
>
> This code ensures 2 things:
> the volatile boolean signal:
>
> while ! signal
>     park()
> - - - - - - -
> signal = true;
> unpark(thread1)
>
> is provably correct (will always terminate)
>
>
> and importantly, that the same code, but with signal as a POSV,
> is provably *incorrect* (i.e. there is at least one valid history where the
> program never terminates) - specifically in the case where there is no
> hb(unpark, depark), (and that could happen), the program may never
> terminate.
>
> and at first glance, it looks right to me.
> It means that the implementation of unpark() needs to "provide" a
> synchronization action (thread.permit = true in the above code), but any
> other guarantee is gratuitous; it logically follows that park() could be a
> no op and no explicit hb guarantee is necessary
>
> I don't see how unpark can be a no-op - the synchronization action is what
> guarantees that all valid histories must have:
> signal = true < unpark(thread), which we all agreed is necessary for the
> volatile idiom to work and even for unpark/park to be useful
>
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11978.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Thu Jan 15 18:14:27 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 15 Jan 2015 23:14:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B84055.2000701@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B6F4B5.5000506@oracle.com>
	<CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> No, making reschedule and yield work in a mutually exclusive
> manner is out of scope. The point is you may have a "fast path" in
> park guarded by a non-atomic, but it won't always go down that
> path. Yet you get the weakest guarantees of all the paths.

If the yield() in your example might possibly leave the thread
suspended indefinitely, then it's identical to the example of mine
that you were replying to, and is not a valid implementation due to
the race I described between reading and writing the permit. The
decision to call yield() is based on having read park == true in the
branch condition, which may already be stale at the point of calling
yield().

Quizzically,
Justin


From dl at cs.oswego.edu  Thu Jan 15 18:50:27 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 15 Jan 2015 18:50:27 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B8465B.4020001@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
Message-ID: <54B85243.90403@cs.oswego.edu>

On 01/15/2015 05:59 PM, Remi Forax wrote:
> I think it's too soon to try to include an API like this in JDK9.
> Currently, everybody seems to think it can come with a better API for a reactive
> framework.

Where a lot of those everybodies were involved in formulating
these APIs. See the list at http://www.reactive-streams.org/

> I think it's better to wait and see.

We waited. We saw :-)

-Doug


>
> R?mi
>
> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>
>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>
>> As discussed a few months ago, there is no single best fluent
>> async/parallel API. CompletableFuture/CompletionStage best supports
>> continuation-style programming on futures, and java.util.stream best
>> supports (multi-stage, possibly-parallel) "pull" style operations on
>> the elements of collections. Until now, one missing category was
>> "push" style operations on items as they become available from an
>> active source. We are not alone in wanting a standard way to support
>> this. Over the past year, the "reactive-streams"
>> (http://www.reactive-streams.org/) effort has been defining a minimal
>> set of interfaces expressing commonalities and allowing
>> interoperablility across frameworks (including Rx and Akka Play), that
>> is nearing release. These interfaces include provisions for a simple
>> form of async flow control allowing developers to address resource
>> control issues that can otherwise cause problems in push-based
>> systems. Supporting this mini-framework helps avoid unpleasant
>> surprises possible when trying to use pull-style APIs for "hot"
>> reactive sources (but conversely is not as good a choice as
>> java.util.Stream for "cold" sources like collections).
>>
>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>> Processor) are defined within the same class "Flow", that also
>> includes the first of some planned support methods to establish and
>> use Flow components, including tie-ins to java.util.streams and
>> CompletableFutures. See
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>
>> (Logistically, the only alternative to class Flow would have been
>> to introduce a subpackage, which unnecessarily complicates usage. And
>> "import static java.util.concurrent.Flow;" is about as convenient as
>> "import java.util.concurrent.flow.*;" would be.)
>>
>> Also, the new stand-alone class SubmissionPublisher can serve as a
>> bridge from various kinds of item producers to Flow components, and is
>> useful in its own right. It is a form of ordered multicaster that
>> people have complained that we don't support in j.u.c, but now do.
>> See
>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>
>>
>> Disclaimers: These are only candidates for inclusion.  The are in
>> preliminary form and will change. But comments and suggestions would
>> be welcome. As with the other candidate additions, if you are brave,
>> you can try out snapshots on jdk8+ by getting
>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>> and running java -Xbootclasspath/p:jsr166.jar
>>
>> -Doug
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>




From kasperni at gmail.com  Thu Jan 15 19:23:58 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Fri, 16 Jan 2015 01:23:58 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B814E4.9080701@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<CAPs6153oD6ZyaETZaOKJP6Pf7VaeTCbQRJsnPHbfdHh8wtjoNg@mail.gmail.com>
	<54B814E4.9080701@cs.oswego.edu>
Message-ID: <CAPs6150m6H14_mjTsp4YpPWeLewqqa821C4HBszpNh_xCfjvxQ@mail.gmail.com>

On Thu, Jan 15, 2015 at 8:28 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/15/2015 02:14 PM, Kasper Nielsen wrote:
>
>>     <http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>> SubmissionPublisher.html>
>>
>>
>> I don't if it is an oversight but shouldn't SubmissionPublished allow for
>> the
>> cancellation of subscriptions.
>> For example, by letting SubmissionPublished.subscribe() return a
>> subscription
>> object you can use?
>>
>
> Publisher.subscribe doesn't return a subscription, it instead invokes
> onSubscribe with one. See the usage example (pasted from javadoc) below
> for Subscriber side usage.
>
> The APIs have a few tradeoffs like this that make them usable
> across both concurrent and distributed contexts. It would be
> possible to add a second form of subscribe to the implementation class
> SubmissionPublisher, that would sometimes be more convenient, but also
> more confusing, so probably not worthwhile.
>
>
Right didn't get the API at first.

A couple of suggestions:
1)
Why not take the Subscription object for every parameter.
This avoid having to store it in a field.

2)
make onComplete a default method.

3)
rename onSubscribe/onComplete to something like onStart/onStop..
onComplete is okay but onSubscribe does not immediately convey that it is
the first mehod being invoked

Ending up with something like

interface Subscriber<T> {
  void onStart(Subscription subscription);
  void onNext(Subscription subscription, T item);
  void onError(Subscription subscription, Throwable cause);
  default void onStop(Subscription subscription) {}
}

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/8188cf10/attachment-0001.html>

From forax at univ-mlv.fr  Thu Jan 15 19:48:10 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 16 Jan 2015 01:48:10 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B85243.90403@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu>
Message-ID: <54B85FCA.4010005@univ-mlv.fr>



On 01/16/2015 12:50 AM, Doug Lea wrote:
> On 01/15/2015 05:59 PM, Remi Forax wrote:
>> I think it's too soon to try to include an API like this in JDK9.
>> Currently, everybody seems to think it can come with a better API for 
>> a reactive
>> framework.
>
> Where a lot of those everybodies were involved in formulating
> these APIs. See the list at http://www.reactive-streams.org/

yes, I know, having a lot of people hammering the API is great thing,
but why do you want to include that in the JDK,
it's better for the API to stay out of the JDK because the release cycle 
is faster,
it's better for the JDK because the API is still moving.

>
>> I think it's better to wait and see.
>
> We waited. We saw :-)

I heard about Rx in 2009, and since that I have observed two things,
the use cases have changed, the APIs have changed, even the manifesto 
has changed recently.
This is not a bad thing, it means the community around the reactive 
stuff is vibrant so
why do you want to stop that rush of cool ideas.

And, if you think you want to freeze the API, I think a JSR is a better 
vehicle.

>
> -Doug

R?mi

>
>
>>
>> R?mi
>>
>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>
>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>
>>> As discussed a few months ago, there is no single best fluent
>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>> continuation-style programming on futures, and java.util.stream best
>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>> the elements of collections. Until now, one missing category was
>>> "push" style operations on items as they become available from an
>>> active source. We are not alone in wanting a standard way to support
>>> this. Over the past year, the "reactive-streams"
>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>> set of interfaces expressing commonalities and allowing
>>> interoperablility across frameworks (including Rx and Akka Play), that
>>> is nearing release. These interfaces include provisions for a simple
>>> form of async flow control allowing developers to address resource
>>> control issues that can otherwise cause problems in push-based
>>> systems. Supporting this mini-framework helps avoid unpleasant
>>> surprises possible when trying to use pull-style APIs for "hot"
>>> reactive sources (but conversely is not as good a choice as
>>> java.util.Stream for "cold" sources like collections).
>>>
>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>> Processor) are defined within the same class "Flow", that also
>>> includes the first of some planned support methods to establish and
>>> use Flow components, including tie-ins to java.util.streams and
>>> CompletableFutures. See
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html 
>>>
>>>
>>> (Logistically, the only alternative to class Flow would have been
>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>> "import java.util.concurrent.flow.*;" would be.)
>>>
>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>> bridge from various kinds of item producers to Flow components, and is
>>> useful in its own right. It is a form of ordered multicaster that
>>> people have complained that we don't support in j.u.c, but now do.
>>> See
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html 
>>>
>>>
>>>
>>> Disclaimers: These are only candidates for inclusion.  The are in
>>> preliminary form and will change. But comments and suggestions would
>>> be welcome. As with the other candidate additions, if you are brave,
>>> you can try out snapshots on jdk8+ by getting
>>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>> and running java -Xbootclasspath/p:jsr166.jar
>>>
>>> -Doug
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jsampson at guidewire.com  Thu Jan 15 20:04:00 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 16 Jan 2015 01:04:00 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>
References: <54B7CD9B.1070908@oracle.com>
	<NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83C3F@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> At the risk of never escaping from this quagmire ... :)
>
> With the additional spec the implementation can't be a no-op, it
> must contain the stated volatile accesses.

Yeah, we're kind of coming back around to questioning that
additional spec again. The existing parenthetical references to
Semaphore aren't enough, I think, but the wording about volatile
reads and writes isn't terribly helpful either.

> Th atomicity argument has me lost. Like Justin later says if this
> is a real blocking based implementation then there has to be some
> atomicity for it work correctly. But what that has to do with the
> HB discussion I have no idea.

Well, the happens-before semantics of an atomic operation are
different from that of two separate operations. Alex's point, I
think, was that even if the spec says something about volatile reads
and writes, it's still hard to reason about what reorderings are
possible, so the user still has to make sure their state variables
are volatile as well. I agree with that conclusion for other
reasons, but not so much for the atomicity argument. :)

I think the upshot of the atomicity part of the discussion is that
saying "park() has the memory effects of a volatile read and write"
really isn't clear _because_ it matters whether we're talking about
a _single_ synchronization action, which has the effects of both a
volatile read and a volatile write, or _multiple_ synchronization
actions, some of which behave as volatile reads and others of which
behave as volatile writes. (This problem is also relevant to the
other thread I started yesterday about CAS semantics.)

My personal preference, gradually refined over the course of this
discussion, is to be more explicit about the semantics using the
actual terminology of the JMM but making as few promises as possible
to enable correct usage. Referring to some unspecified sequence of
volatile reads and writes of some unspecified memory locations
doesn't do the trick.

Even if the current implementation does induce some actual
happens-before edges from some part of unpark() to some part of
park(), it's too hard to reason about the resulting reordering
rules, and due to various reasons discussed on this thread any
associated state variables have to be volatile anyway for correct
usage.

The essential thing we need to establish is that park() and unpark()
behave as if they were synchronization actions, such that the permit
semantics are governed by the same synchronization order that
governs volatile reads and writes. That's enough to prevent
reordering park() and unpark() calls with nearby volatile reads and
writes, while still allowing the implementation some leeway to
reorder nearby non-volatile reads and writes.

Hence my most recent wording proposal, repeated here verbatim:

"Memory consistency effects: Any unpark call that releases a permit
*or* observes that a permit is already available necessarily
precedes, in the overall synchronization order, the return of the
park call that acquires that particular released or observed permit.
No additional _happens-before_ guarantees are provided."

Cheers,
Justin


From jsampson at guidewire.com  Thu Jan 15 20:38:55 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 16 Jan 2015 01:38:55 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> I don't think it makes sense in the memory model to say something
> has volatile write semantics if nothing is actually written. The
> spec could perhaps be clearer that it's not saying that. But if it
> does, that's arguably a vacuous statement anyway.

>From java/util/concurrent/atomic/package-summary.html:

"compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing
volatile variables."

I don't know if this is actually true for the intrinsic CAS -- if
you think it's not, or shouldn't be, then it definitely needs to be
rewritten. :) But it's _definitely_ not true for the compareAndSet
on AtomicStampedReference, which only has the memory effects of
writing a volatile variable if it actually writes a different value
than was there before (and it can fail spuriously).

> It does seem to be (barely) observable whether a write of the
> value that was already there has volatile semantics or not:
>
> [...]
>
> Thus it does seem to matter whether we atomically replace a value
> by itself or don't write at all.

Right, the synchronized-with edges come out differently. I don't
think anyone should be relying on the synchronized-with edges coming
from a failed or no-op CAS for any practical purpose, but that's
currently how it's spec'd.

Out of curiosity, is a weakCompareAndSet at least considered a
synchronization action, part of the overall synchronization order,
such that it can't be reordered with any nearby volatile reads or
writes? Or is it truly free-for-all?

Thanks!
Justin


From thurston at nomagicsoftware.com  Fri Jan 16 01:54:13 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 15 Jan 2015 23:54:13 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83C3F@sm-ex-01-vm.guidewire.com>
References: <CAHjP37F7MN=+STG_Ev1xJG2gLq3tzVWCpswxa3JUQOibMNTiVA@mail.gmail.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>
	<0FD072A166C6DC4C851F6115F37DDD2783D83C3F@sm-ex-01-vm.guidewire.com>
Message-ID: <1421391253556-11997.post@n7.nabble.com>

Or we could just state it directly in code:

Any implementation of park && unpark guarantee that the following program
terminates:

boolean volatile signal = false;

Thread 1:
signal = true
unpark(thread2)

Thread 2:
while (! signal)
    park()


No happens-before semantics may be assumed.


That's what we're saying, we should say it directly; it communicates both to
users and implementers what is required, tersely and clearly.
If it's important to leave the flexibility for implementers to not guarantee
happens-before, then it's very difficult to describe this new memory
ordering semantics.
I also wouldn't push the Semaphore analogy too far, because release/acquire
have explicit hb semantics.
This is different than the original say-nothing about memory semantics


This means that if there are any hypothetical impls like this:

unpark(thread)
     NativeThread.unpark(thread)


park()
{
     if (NativeThread.unparked())
         return;
     NativeThread.awaitUnpark();
     NativeThread.unparked()
}

with NativeThread.***park methods mirroring the behavior of their
Thread.interrupt*** counterparts *except that none of them entail any
synchronization actions*, not to mention happens-before relations.

This would be a broken implementation and allow the aforementioned program
to never terminate, as the write to signal may execute after unpark() (the
original motivation) - it would be the implementers' responsibility to fix
it.
  





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11997.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From akarnokd at gmail.com  Fri Jan 16 04:38:10 2015
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 16 Jan 2015 10:38:10 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B85FCA.4010005@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
Message-ID: <CAAWwtm9rviDmFJZLdP1H+-H_=MrH3brcnwXWDkp+2bJv2mDTQA@mail.gmail.com>

I've been working in this Java reactive field for 4 years now. Having some
base interfaces in JDK itself is nice, but libraries have lived quite well
with their JDK-external interfaces/classes. Unless JDK itself capitalizes
on this better observer-pattern (i.e., retrofitting Swing's event
notification mechanism or NIO's async) there is not much gain; programmers
can continue using the external libraries and interops can be built as
needed. (I'd like to have yield return and extension methods instead but
that's not going to happen anytime soon).

Few questions/observations about these new classes:

- Will the executors be strict part of the base classes? I.e., will I be
able to observe a publisher synchronously? (Rx has observeOn and
subscribeOn to do async on demand.)
- subscribe() has to do some enforcing which can be quite heavy, but adding
unsafeSubscribe (or safeSubscribe) makes Publisher no longer pure
functional interface.
- In Rx, we have a rule that onXXX methods can't be called while holding a
lock to avoid deadlocks. And in general, quite a lot of work went into
doing atomic "magic" to be lock-free as much as possible.
- What default operators will be available on a producer? (map, merge,
concat, zip, window, lift, etc).



2015-01-16 1:48 GMT+01:00 Remi Forax <forax at univ-mlv.fr>:

>
>
> On 01/16/2015 12:50 AM, Doug Lea wrote:
>
>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>
>>> I think it's too soon to try to include an API like this in JDK9.
>>> Currently, everybody seems to think it can come with a better API for a
>>> reactive
>>> framework.
>>>
>>
>> Where a lot of those everybodies were involved in formulating
>> these APIs. See the list at http://www.reactive-streams.org/
>>
>
> yes, I know, having a lot of people hammering the API is great thing,
> but why do you want to include that in the JDK,
> it's better for the API to stay out of the JDK because the release cycle
> is faster,
> it's better for the JDK because the API is still moving.
>
>
>>  I think it's better to wait and see.
>>>
>>
>> We waited. We saw :-)
>>
>
> I heard about Rx in 2009, and since that I have observed two things,
> the use cases have changed, the APIs have changed, even the manifesto has
> changed recently.
> This is not a bad thing, it means the community around the reactive stuff
> is vibrant so
> why do you want to stop that rush of cool ideas.
>
> And, if you think you want to freeze the API, I think a JSR is a better
> vehicle.
>
>
>> -Doug
>>
>
> R?mi
>
>
>
>>
>>
>>> R?mi
>>>
>>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>
>>>>
>>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>>
>>>> As discussed a few months ago, there is no single best fluent
>>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>>> continuation-style programming on futures, and java.util.stream best
>>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>>> the elements of collections. Until now, one missing category was
>>>> "push" style operations on items as they become available from an
>>>> active source. We are not alone in wanting a standard way to support
>>>> this. Over the past year, the "reactive-streams"
>>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>>> set of interfaces expressing commonalities and allowing
>>>> interoperablility across frameworks (including Rx and Akka Play), that
>>>> is nearing release. These interfaces include provisions for a simple
>>>> form of async flow control allowing developers to address resource
>>>> control issues that can otherwise cause problems in push-based
>>>> systems. Supporting this mini-framework helps avoid unpleasant
>>>> surprises possible when trying to use pull-style APIs for "hot"
>>>> reactive sources (but conversely is not as good a choice as
>>>> java.util.Stream for "cold" sources like collections).
>>>>
>>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>>> Processor) are defined within the same class "Flow", that also
>>>> includes the first of some planned support methods to establish and
>>>> use Flow components, including tie-ins to java.util.streams and
>>>> CompletableFutures. See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/
>>>> concurrent/Flow.html
>>>>
>>>> (Logistically, the only alternative to class Flow would have been
>>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>>> "import java.util.concurrent.flow.*;" would be.)
>>>>
>>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>>> bridge from various kinds of item producers to Flow components, and is
>>>> useful in its own right. It is a form of ordered multicaster that
>>>> people have complained that we don't support in j.u.c, but now do.
>>>> See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>>>> SubmissionPublisher.html
>>>>
>>>>
>>>> Disclaimers: These are only candidates for inclusion.  The are in
>>>> preliminary form and will change. But comments and suggestions would
>>>> be welcome. As with the other candidate additions, if you are brave,
>>>> you can try out snapshots on jdk8+ by getting
>>>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>>> and running java -Xbootclasspath/p:jsr166.jar
>>>>
>>>> -Doug
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/0b49c753/attachment-0001.html>

From dl at cs.oswego.edu  Fri Jan 16 07:25:55 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 16 Jan 2015 07:25:55 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B85FCA.4010005@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>
Message-ID: <54B90353.8040601@cs.oswego.edu>

On 01/15/2015 07:48 PM, Remi Forax wrote:
> On 01/16/2015 12:50 AM, Doug Lea wrote:
>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>> I think it's too soon to try to include an API like this in JDK9.
>>> Currently, everybody seems to think it can come with a better API for a reactive
>>> framework.
>>
>> Where a lot of those everybodies were involved in formulating
>> these APIs. See the list at http://www.reactive-streams.org/
>
> yes, I know, having a lot of people hammering the API is great thing,
> but why do you want to include that in the JDK,

Flow provides the smallest set of small interfaces (only 7 methods
total) that enables creation of j.u.c implementation components like
SubmissionPublisher to serve this audience. Choosing these
APIs in particular (even if some people don't like the names
of some of the methods) seems to maximize impact.

There are no current plans to recreate full frameworks.
And for now, these are just candidates. If they don't
turn out to be helpful, they won't make it into jdk9.

-Doug


From viktor.klang at gmail.com  Fri Jan 16 07:59:31 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 16 Jan 2015 13:59:31 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B85FCA.4010005@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
Message-ID: <CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>

Hi R?mi,

I'll try to address your concerns below:

On Fri, Jan 16, 2015 at 1:48 AM, Remi Forax <forax at univ-mlv.fr> wrote:

>
>
> On 01/16/2015 12:50 AM, Doug Lea wrote:
>
>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>
>>> I think it's too soon to try to include an API like this in JDK9.
>>> Currently, everybody seems to think it can come with a better API for a
>>> reactive
>>> framework.
>>>
>>
>> Where a lot of those everybodies were involved in formulating
>> these APIs. See the list at http://www.reactive-streams.org/
>>
>
> yes, I know, having a lot of people hammering the API is great thing,
> but why do you want to include that in the JDK,

it's better for the API to stay out of the JDK because the release cycle is
> faster,

it's better for the JDK because the API is still moving.


The RS interfaces and method signatures has not seen an update in a long
time and is considered done (RS is in 1.0.0.RC1 with an RC2 shipped soon
with spec clarifications and improvements to the TCK and 1.0.0.final is
expected shortly thereafter).


>
>
>
>>  I think it's better to wait and see.
>>>
>>
>> We waited. We saw :-)
>>
>
> I heard about Rx in 2009, and since that I have observed two things,
> the use cases have changed, the APIs have changed, even the manifesto has
> changed recently.

This is not a bad thing, it means the community around the reactive stuff
> is vibrant so
> why do you want to stop that rush of cool ideas.
>

RS is specifically for interop?a common vocabulary and semantics to build
asynchronous, non-blocking streaming transformation with non-blocking back
pressure. So from an API perspective it is targeted towards library
developers, just as most of the things in java.util.concurrent. End user
APIs are provided on top, through implementations like SubmissionPublisher
or RxJava, Project Reactor, Akka Streams, Ratpack etc.


>
> And, if you think you want to freeze the API, I think a JSR is a better
> vehicle.
>

Given the the target and scope of RS, it seems to me that it fits nicely
within the scope of JSR166?
There is a spec, 4 pure interfaces with a sum total of 7 methods and a TCK
to validate implementations:
https://github.com/reactive-streams/reactive-streams


>
>> -Doug
>>
>
> R?mi
>
>
>
>>
>>
>>> R?mi
>>>
>>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>
>>>>
>>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>>
>>>> As discussed a few months ago, there is no single best fluent
>>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>>> continuation-style programming on futures, and java.util.stream best
>>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>>> the elements of collections. Until now, one missing category was
>>>> "push" style operations on items as they become available from an
>>>> active source. We are not alone in wanting a standard way to support
>>>> this. Over the past year, the "reactive-streams"
>>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>>> set of interfaces expressing commonalities and allowing
>>>> interoperablility across frameworks (including Rx and Akka Play), that
>>>> is nearing release. These interfaces include provisions for a simple
>>>> form of async flow control allowing developers to address resource
>>>> control issues that can otherwise cause problems in push-based
>>>> systems. Supporting this mini-framework helps avoid unpleasant
>>>> surprises possible when trying to use pull-style APIs for "hot"
>>>> reactive sources (but conversely is not as good a choice as
>>>> java.util.Stream for "cold" sources like collections).
>>>>
>>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>>> Processor) are defined within the same class "Flow", that also
>>>> includes the first of some planned support methods to establish and
>>>> use Flow components, including tie-ins to java.util.streams and
>>>> CompletableFutures. See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/
>>>> concurrent/Flow.html
>>>>
>>>> (Logistically, the only alternative to class Flow would have been
>>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>>> "import java.util.concurrent.flow.*;" would be.)
>>>>
>>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>>> bridge from various kinds of item producers to Flow components, and is
>>>> useful in its own right. It is a form of ordered multicaster that
>>>> people have complained that we don't support in j.u.c, but now do.
>>>> See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>>>> SubmissionPublisher.html
>>>>
>>>>
>>>> Disclaimers: These are only candidates for inclusion.  The are in
>>>> preliminary form and will change. But comments and suggestions would
>>>> be welcome. As with the other candidate additions, if you are brave,
>>>> you can try out snapshots on jdk8+ by getting
>>>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>>> and running java -Xbootclasspath/p:jsr166.jar
>>>>
>>>> -Doug
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/a4d10900/attachment.html>

From dl at cs.oswego.edu  Fri Jan 16 08:20:02 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 16 Jan 2015 08:20:02 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAAWwtm9rviDmFJZLdP1H+-H_=MrH3brcnwXWDkp+2bJv2mDTQA@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>
	<CAAWwtm9rviDmFJZLdP1H+-H_=MrH3brcnwXWDkp+2bJv2mDTQA@mail.gmail.com>
Message-ID: <54B91002.7020900@cs.oswego.edu>

I'll try to minimize involvement in the meta-issues, but here are
some answers to...

On 01/16/2015 04:38 AM, D?vid Karnok wrote:
> Few questions/observations about these new classes:
>
> - Will the executors be strict part of the base classes? I.e., will I be able to
> observe a publisher synchronously? (Rx has observeOn and subscribeOn to do async
> on demand.)

The only j.u.c implementations are async-only.

> - subscribe() has to do some enforcing which can be quite heavy, but adding
> unsafeSubscribe (or safeSubscribe) makes Publisher no longer pure functional
> interface.

It's does require a presence check, but this is exploited in
SubmissionPublisher to also process lazy removals, so the
aggregate cost per subscription is fairly low.

> - In Rx, we have a rule that onXXX methods can't be called while holding a lock
> to avoid deadlocks. And in general, quite a lot of work went into doing atomic
> "magic" to be lock-free as much as possible.

SubmissionPublisher is lock-free except for an outer lock to
ensure only one submitter at a time. Because most usages will
only have one submitter anyway, we use a builtin lock that
can be biased by JVM in the common case and so practically free.

> - What default operators will be available on a producer? (map, merge, concat,
> zip, window, lift, etc).

None are planned, but we do have a java.util.Stream tie-in:
Flow.stream(pub, streamExpr), that will require a few jdk9
internal Stream extensions to work well. In the mean time it
it collects all items before executing.

-Doug





From oleksandr.otenko at oracle.com  Fri Jan 16 09:47:38 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 16 Jan 2015 14:47:38 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
Message-ID: <54B9248A.2070508@oracle.com>

re: synchronizes-with in failing CAS.

The edge is there, if CAS doesn't fail before reading the value. I don't 
know if any hardware may at times fail CAS without reading the value. 
Note also that if CAS fails, then you won't have anything that depends 
on CAS result to synchronize-with.

Alex

On 16/01/2015 01:38, Justin Sampson wrote:
> Hans Boehm wrote:
>
>> I don't think it makes sense in the memory model to say something
>> has volatile write semantics if nothing is actually written. The
>> spec could perhaps be clearer that it's not saying that. But if it
>> does, that's arguably a vacuous statement anyway.
>  From java/util/concurrent/atomic/package-summary.html:
>
> "compareAndSet and all other read-and-update operations such as
> getAndIncrement have the memory effects of both reading and writing
> volatile variables."
>
> I don't know if this is actually true for the intrinsic CAS -- if
> you think it's not, or shouldn't be, then it definitely needs to be
> rewritten. :) But it's _definitely_ not true for the compareAndSet
> on AtomicStampedReference, which only has the memory effects of
> writing a volatile variable if it actually writes a different value
> than was there before (and it can fail spuriously).
>
>> It does seem to be (barely) observable whether a write of the
>> value that was already there has volatile semantics or not:
>>
>> [...]
>>
>> Thus it does seem to matter whether we atomically replace a value
>> by itself or don't write at all.
> Right, the synchronized-with edges come out differently. I don't
> think anyone should be relying on the synchronized-with edges coming
> from a failed or no-op CAS for any practical purpose, but that's
> currently how it's spec'd.
>
> Out of curiosity, is a weakCompareAndSet at least considered a
> synchronization action, part of the overall synchronization order,
> such that it can't be reordered with any nearby volatile reads or
> writes? Or is it truly free-for-all?
>
> Thanks!
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From forax at univ-mlv.fr  Fri Jan 16 10:03:31 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 16 Jan 2015 16:03:31 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
Message-ID: <54B92843.8080100@univ-mlv.fr>

Viktor, Doug,

I see at least 3 changes that in my opinion should be done on the 
current API.
- Subscriber.onSubscribe and onNext/onError/onComplete should not be 
part of the same interface.
   This will make the workflow more explicit and avoid to have the 
subscription object mutable

- onComplete and onError should be one method and not two,
   let's call it onComplete(Throwable throwableOrNull).
   I think it's better two says that you have to way to complete, either 
normally or with an exception.
   And from my experience, the code of onError and onComplete tend to be 
fairly similar.

- onComplete(Throwable) should be a default method (the implementation 
do nothing),
   so Observer and Consumer will be equivalent in term of function type.

   Re-using the example of Doug,

   long requestSize = ...
   Observer<T> observer = consumer::accept;   // auto-conversion from a 
Consumer
   publisher.subscribe(subscription -> {
       long half  = requestSize / 2; // re-request when half consumed
       subscription.request(half);
       return new Observer<T>() {
           private long count = half;
           public void onNext(T item) {
               if (--count <= 0) {
                   subscription.request(count = half);
               }
               observer.onNext(item);
           }
           public void onComplete(Throwable ex) {
             observer.onComplete(ex);
           }
       };
   });

   Note that in term of implementation you can still have an 
implementation that implements both
   Subscriber and Observer with onSubscribe() returning this.

and I'm still not convince that this should be integrated in JDK.

R?mi

On 01/16/2015 01:59 PM, Viktor Klang wrote:
> Hi R?mi,
>
> I'll try to address your concerns below:
>
> On Fri, Jan 16, 2015 at 1:48 AM, Remi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>
>
>     On 01/16/2015 12:50 AM, Doug Lea wrote:
>
>         On 01/15/2015 05:59 PM, Remi Forax wrote:
>
>             I think it's too soon to try to include an API like this
>             in JDK9.
>             Currently, everybody seems to think it can come with a
>             better API for a reactive
>             framework.
>
>
>         Where a lot of those everybodies were involved in formulating
>         these APIs. See the list at http://www.reactive-streams.org/
>
>
>     yes, I know, having a lot of people hammering the API is great thing,
>     but why do you want to include that in the JDK, 
>
>     it's better for the API to stay out of the JDK because the release
>     cycle is faster, 
>
>     it's better for the JDK because the API is still moving.
>
>
> The RS interfaces and method signatures has not seen an update in a 
> long time and is considered done (RS is in 1.0.0.RC1 with an RC2 
> shipped soon with spec clarifications and improvements to the TCK 
> and 1.0.0.final is expected shortly thereafter).
>
>
>
>
>             I think it's better to wait and see.
>
>
>         We waited. We saw :-)
>
>
>     I heard about Rx in 2009, and since that I have observed two things,
>     the use cases have changed, the APIs have changed, even the
>     manifesto has changed recently.
>
>     This is not a bad thing, it means the community around the
>     reactive stuff is vibrant so
>     why do you want to stop that rush of cool ideas.
>
>
> RS is specifically for interop?a common vocabulary and semantics to 
> build asynchronous, non-blocking streaming transformation with 
> non-blocking back pressure. So from an API perspective it is targeted 
> towards library developers, just as most of the things in 
> java.util.concurrent. End user APIs are provided on top, through 
> implementations like SubmissionPublisher or RxJava, Project Reactor, 
> Akka Streams, Ratpack etc.
>
>
>     And, if you think you want to freeze the API, I think a JSR is a
>     better vehicle.
>
>
> Given the the target and scope of RS, it seems to me that it fits 
> nicely within the scope of JSR166?
> There is a spec, 4 pure interfaces with a sum total of 7 methods and a 
> TCK to validate implementations: 
> https://github.com/reactive-streams/reactive-streams
>
>
>
>         -Doug
>
>
>     R?mi
>
>
>
>
>
>             R?mi
>
>             On 01/15/2015 06:25 PM, Doug Lea wrote:
>
>
>                 Here's the only set of candidates for new jdk9 j.u.c
>                 classes:
>
>                 As discussed a few months ago, there is no single best
>                 fluent
>                 async/parallel API. CompletableFuture/CompletionStage
>                 best supports
>                 continuation-style programming on futures, and
>                 java.util.stream best
>                 supports (multi-stage, possibly-parallel) "pull" style
>                 operations on
>                 the elements of collections. Until now, one missing
>                 category was
>                 "push" style operations on items as they become
>                 available from an
>                 active source. We are not alone in wanting a standard
>                 way to support
>                 this. Over the past year, the "reactive-streams"
>                 (http://www.reactive-streams.org/) effort has been
>                 defining a minimal
>                 set of interfaces expressing commonalities and allowing
>                 interoperablility across frameworks (including Rx and
>                 Akka Play), that
>                 is nearing release. These interfaces include
>                 provisions for a simple
>                 form of async flow control allowing developers to
>                 address resource
>                 control issues that can otherwise cause problems in
>                 push-based
>                 systems. Supporting this mini-framework helps avoid
>                 unpleasant
>                 surprises possible when trying to use pull-style APIs
>                 for "hot"
>                 reactive sources (but conversely is not as good a
>                 choice as
>                 java.util.Stream for "cold" sources like collections).
>
>                 The four intertwined interfaces (Publisher,
>                 Subscriber, Subscription,
>                 Processor) are defined within the same class "Flow",
>                 that also
>                 includes the first of some planned support methods to
>                 establish and
>                 use Flow components, including tie-ins to
>                 java.util.streams and
>                 CompletableFutures. See
>                 http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>
>
>                 (Logistically, the only alternative to class Flow
>                 would have been
>                 to introduce a subpackage, which unnecessarily
>                 complicates usage. And
>                 "import static java.util.concurrent.Flow;" is about as
>                 convenient as
>                 "import java.util.concurrent.flow.*;" would be.)
>
>                 Also, the new stand-alone class SubmissionPublisher
>                 can serve as a
>                 bridge from various kinds of item producers to Flow
>                 components, and is
>                 useful in its own right. It is a form of ordered
>                 multicaster that
>                 people have complained that we don't support in j.u.c,
>                 but now do.
>                 See
>                 http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>
>
>
>                 Disclaimers: These are only candidates for inclusion. 
>                 The are in
>                 preliminary form and will change. But comments and
>                 suggestions would
>                 be welcome. As with the other candidate additions, if
>                 you are brave,
>                 you can try out snapshots on jdk8+ by getting
>                 http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>                 and running java -Xbootclasspath/p:jsr166.jar
>
>                 -Doug
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> Cheers,
> ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/c71986cd/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Jan 16 10:06:07 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 16 Jan 2015 15:06:07 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B707CB.9090503@oracle.com>	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
Message-ID: <54B928DF.80106@oracle.com>

You can't claim that it's not possible to implement reschedule in such a 
way that yield cannot tell whether reschedule was called since yield was 
called last time.

reschedule(t): q.put(t);

yield(){
   suspended=true;
   if (!park) reschedule(t);
   Thread t;
   do{
     while((t=q.poll()) == null); // idle loop
   }while(!t.compareAndSet(t.suspended,true,false)); // claim thread for 
further execution - atomically modify suspended flag
   go to t.resumePoint;
}

Alex

On 15/01/2015 23:14, Justin Sampson wrote:
> Oleksandr Otenko wrote:
>
>> No, making reschedule and yield work in a mutually exclusive
>> manner is out of scope. The point is you may have a "fast path" in
>> park guarded by a non-atomic, but it won't always go down that
>> path. Yet you get the weakest guarantees of all the paths.
> If the yield() in your example might possibly leave the thread
> suspended indefinitely, then it's identical to the example of mine
> that you were replying to, and is not a valid implementation due to
> the race I described between reading and writing the permit. The
> decision to call yield() is based on having read park == true in the
> branch condition, which may already be stale at the point of calling
> yield().
>
> Quizzically,
> Justin


From viktor.klang at gmail.com  Fri Jan 16 10:53:05 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 16 Jan 2015 16:53:05 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B92843.8080100@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
Message-ID: <CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>

Hi R?mi,

Thanks for the great questions
(answering inline)

On Fri, Jan 16, 2015 at 4:03 PM, Remi Forax <forax at univ-mlv.fr> wrote:

>  Viktor, Doug,
>
> I see at least 3 changes that in my opinion should be done on the current
> API.
> - Subscriber.onSubscribe and onNext/onError/onComplete should not be part
> of the same interface.
>

Unfortunately that did not pan out (we've did a lot of research/PoCs, I'd
be surprised if we have missed any permutation) for the reason that failure
to create a Subscription due to a failure in the Publisher requires the
possibility of issuing onError without a preceding onSubscribe. We are also
considering allowing fast completion (avoiding having to call onSubscribe
if the stream is already completed).


>   This will make the workflow more explicit and avoid to have the
> subscription object mutable
>

The Subscription need not be mutable right now, did you mean Subscriber?


>
> - onComplete and onError should be one method and not two,
>   let's call it onComplete(Throwable throwableOrNull).
>   I think it's better two says that you have to way to complete, either
> normally or with an exception.
>   And from my experience, the code of onError and onComplete tend to be
> fairly similar.
>

That was an interesting suggestion & discussion we had quite early (1+ year
ago), but we decided both that `null`-checking as being an anti-pattern, as
well as mixing failure management code with success-management code is
brittle and mixes concerns.
Also, a valid follow-on question was: where do you stop?
onNext(Either<Throwable,T> elemOrCompleteOrError)?
Function<Either<Throwable, T>, Void>? Function<Either<Subscription,
Either<Throwable, T>>, Void>?

So it was deemed that having a distinct method not only made it easier to
avoid NPEs, made it easy to separate the concerns in the implementations,
easier to find what you were looking for in the javadoc, the spec etc.


>
> - onComplete(Throwable) should be a default method (the implementation do
> nothing),
>   so Observer and Consumer will be equivalent in term of function type.
>

Since the target is JDK9 onNext, onComplete and onError can definitely be
default methods, the reason why they aren't currently in the RS spec is
because we started before Java 8 shipped and keeping Java6-7 compatibility
made sense.


>
>   Re-using the example of Doug,
>
>   long requestSize = ...
>   Observer<T> observer = consumer::accept;   // auto-conversion from a
> Consumer
>   publisher.subscribe(subscription -> {
>       long half  = requestSize / 2; // re-request when half consumed
>       subscription.request(half);
>       return new Observer<T>() {
>           private long count = half;
>           public void onNext(T item) {
>               if (--count <= 0) {
>                   subscription.request(count = half);
>               }
>               observer.onNext(item);
>           }
>           public void onComplete(Throwable ex) {
>             observer.onComplete(ex);
>           }
>       };
>   });
>
>   Note that in term of implementation you can still have an implementation
> that implements both
>   Subscriber and Observer with onSubscribe() returning this.
>
> and I'm still not convince that this should be integrated in JDK.
>

I hope to be able to address those concerns :)
Anything that I missed to reply to?


>
> R?mi
>
>
> On 01/16/2015 01:59 PM, Viktor Klang wrote:
>
> Hi R?mi,
>
>  I'll try to address your concerns below:
>
> On Fri, Jan 16, 2015 at 1:48 AM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>>
>>
>> On 01/16/2015 12:50 AM, Doug Lea wrote:
>>
>>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>>
>>>> I think it's too soon to try to include an API like this in JDK9.
>>>> Currently, everybody seems to think it can come with a better API for a
>>>> reactive
>>>> framework.
>>>>
>>>
>>> Where a lot of those everybodies were involved in formulating
>>> these APIs. See the list at http://www.reactive-streams.org/
>>>
>>
>>  yes, I know, having a lot of people hammering the API is great thing,
>> but why do you want to include that in the JDK,
>
>  it's better for the API to stay out of the JDK because the release cycle
>> is faster,
>
>  it's better for the JDK because the API is still moving.
>
>
>  The RS interfaces and method signatures has not seen an update in a long
> time and is considered done (RS is in 1.0.0.RC1 with an RC2 shipped soon
> with spec clarifications and improvements to the TCK and 1.0.0.final is
> expected shortly thereafter).
>
>
>>
>>
>>
>>> I think it's better to wait and see.
>>>>
>>>
>>> We waited. We saw :-)
>>>
>>
>>  I heard about Rx in 2009, and since that I have observed two things,
>> the use cases have changed, the APIs have changed, even the manifesto has
>> changed recently.
>
>  This is not a bad thing, it means the community around the reactive stuff
>> is vibrant so
>> why do you want to stop that rush of cool ideas.
>>
>
>  RS is specifically for interop?a common vocabulary and semantics to
> build asynchronous, non-blocking streaming transformation with non-blocking
> back pressure. So from an API perspective it is targeted towards library
> developers, just as most of the things in java.util.concurrent. End user
> APIs are provided on top, through implementations like SubmissionPublisher
> or RxJava, Project Reactor, Akka Streams, Ratpack etc.
>
>
>>
>> And, if you think you want to freeze the API, I think a JSR is a better
>> vehicle.
>>
>
>  Given the the target and scope of RS, it seems to me that it fits nicely
> within the scope of JSR166?
>  There is a spec, 4 pure interfaces with a sum total of 7 methods and a
> TCK to validate implementations:
> https://github.com/reactive-streams/reactive-streams
>
>
>>
>>> -Doug
>>>
>>
>> R?mi
>>
>>
>>
>>>
>>>
>>>> R?mi
>>>>
>>>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>>
>>>>>
>>>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>>>
>>>>> As discussed a few months ago, there is no single best fluent
>>>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>>>> continuation-style programming on futures, and java.util.stream best
>>>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>>>> the elements of collections. Until now, one missing category was
>>>>> "push" style operations on items as they become available from an
>>>>> active source. We are not alone in wanting a standard way to support
>>>>> this. Over the past year, the "reactive-streams"
>>>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>>>> set of interfaces expressing commonalities and allowing
>>>>> interoperablility across frameworks (including Rx and Akka Play), that
>>>>> is nearing release. These interfaces include provisions for a simple
>>>>> form of async flow control allowing developers to address resource
>>>>> control issues that can otherwise cause problems in push-based
>>>>> systems. Supporting this mini-framework helps avoid unpleasant
>>>>> surprises possible when trying to use pull-style APIs for "hot"
>>>>> reactive sources (but conversely is not as good a choice as
>>>>> java.util.Stream for "cold" sources like collections).
>>>>>
>>>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>>>> Processor) are defined within the same class "Flow", that also
>>>>> includes the first of some planned support methods to establish and
>>>>> use Flow components, including tie-ins to java.util.streams and
>>>>> CompletableFutures. See
>>>>>
>>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>>>>
>>>>> (Logistically, the only alternative to class Flow would have been
>>>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>>>> "import java.util.concurrent.flow.*;" would be.)
>>>>>
>>>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>>>> bridge from various kinds of item producers to Flow components, and is
>>>>> useful in its own right. It is a form of ordered multicaster that
>>>>> people have complained that we don't support in j.u.c, but now do.
>>>>> See
>>>>>
>>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>>>>
>>>>>
>>>>> Disclaimers: These are only candidates for inclusion.  The are in
>>>>> preliminary form and will change. But comments and suggestions would
>>>>> be welcome. As with the other candidate additions, if you are brave,
>>>>> you can try out snapshots on jdk8+ by getting
>>>>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>>>> and running java -Xbootclasspath/p:jsr166.jar
>>>>>
>>>>> -Doug
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
>  --
>   Cheers,
> ?
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/5488609d/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Jan 16 11:34:30 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 16 Jan 2015 16:34:30 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEJIKMAA.davidcholmes@aapt.net.au>
Message-ID: <54B93D96.1060801@oracle.com>

In the absence of other synchronization, park/unpark can still be 
implemented as no-op, because in the absence of other synchronization 
the order called (b) in another thread is allowed. The no-op 
implementation is such a implementation that enforces the 
synchronization order always chooses (b) or (c).

synchronization order a:
unpark stores
park loads
park stores

b:
park loads
unpark stores
park stores

c:
park loads
park stores
unpark stores

No other synchronization events - you can't say choosing (b) or (c) is 
wrong.

If you have other synchronization events, you can only prove that the 
unparks before one event hb parks after the other event - but such a 
pair of synchronization events already gives you a stronger hb.

In other words, you have evidence that park could not occur before 
unpark, if you already have a proof hb(X,Y); but then hb(unpark 1,park 
2) doesn't add anything new:

T1:
unpark 1
X
unpark 2

T2:
park 1
Y
park 2

Here park 1 vs unpark 1 may still be ordered like (c), and same for park 
2 vs unpark 2.

If hb(unpark,park) is not needed, what's the fuss about? Well, the fuss 
is about having them appear in synchronization order, so that ordering 
(a) or (b) for unpark 2 vs park 1 guarantee that hb(X,Y) - otherwise, 
the loop may re-enter park and hang.


Atomicity question is about whether (b) is allowed or not. If (b) is not 
allowed, then no-op implementation is not allowed, and no need to use 
volatiles to monitor state in the park loop. Granted, atomic operations 
may occur somewhere inside park(), but it doesn't mean they have to 
occur always, or that you have control over that.

Alex


On 15/01/2015 22:30, David Holmes wrote:
> At the risk of never escaping from this quagmire ... :)
> With the additional spec the implementation can't be a no-op, it must 
> contain the stated volatile accesses.
> Th atomicity argument has me lost. Like Justin later says if this is a 
> real blocking based implementation then there has to be some atomicity 
> for it work correctly. But what that has to do with the HB discussion 
> I have no idea.
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Friday, 16 January 2015 12:24 AM
>     *To:* Vitaly Davidovich
>     *Cc:* thurston; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] unpark/park memory visibility
>
>     I don't. It should be allowed to be almost anything, even a no-op.
>     It is only an optimization of the busy wait, and it is ok to not
>     optimize, or just have a micro-pause that doesn't communicate with
>     unpark. What it cannot be - it cannot be an implementation that
>     may hang indefinitely, if appears after unpark (for some
>     definition of "after"), and can't permit an implementation that
>     doesn't establish hb between volatiles (before and after the
>     unpark and park), if park appears after unpark (again, for some
>     definition of "before" and "after"). It is this reference to
>     "before" and "after" that require to relate it somehow to the
>     synchronization order. Existence of JMM hb edge between unpark and
>     park is not necessary.
>
>     Also, read again how Doug specified park - it is not just a
>     volatile read, but also a write. Let's not pretend the
>     specification like that is perfect, or final, or won't be
>     retracted, but that specification still permits the behaviour I
>     described - the hang may still occur, because the normal read of
>     signal from the next iteration is allowed to "go ahead" of the
>     volatile write of the "volatile variable" shared with unpark, from
>     the park of the previous iteration. This is not so for volatile reads.
>
>     Alex
>
>     On 15/01/2015 14:07, Vitaly Davidovich wrote:
>>
>>     Right, I agree that it can't be a noop.
>>
>>     sent from my phone
>>
>>     On Jan 15, 2015 8:26 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, I object.
>>
>>         If you permit normal writes to be visible in program order
>>         from unpark(), then no-op cannot be a permitted
>>         implementation of park() - since that would imply any program
>>         order should be visible in that order after park/unpark.
>>
>>         Alex
>>
>>         On 15/01/2015 01:26, Vitaly Davidovich wrote:
>>>
>>>         My interpretation is pretty much what Jason said earlier;
>>>         loop definitively terminates (eventually) but thread 1 may
>>>         not see other writes if it never enters the loop.  That
>>>         seems consistent with Doug's clarification.
>>>
>>>         sent from my phone
>>>
>>>         On Jan 14, 2015 8:16 PM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             If park() is seen as a synchronization event, it will
>>>             still require volatile accesses to establish a hb (it
>>>             will only stop reordering of volatile accesses).
>>>
>>>             If park() is seen as a very particular synchronization
>>>             event that creates a hb edge, then obviously normal
>>>             reads should observe normal writes and it should
>>>             terminate, but this is not what people write, and I
>>>             don't know if that is what is intended (eg see the
>>>             objections - people didn't think mentioning hb edge was
>>>             all that useful).
>>>
>>>             Alex
>>>
>>>             On 15/01/2015 00:27, Vitaly Davidovich wrote:
>>>>
>>>>             I don't think subsequent load of signal is allowed to
>>>>             appear before park () because it was said earlier that
>>>>             park () acts like a volatile load; if so, subsequent
>>>>             loads cannot move before it.  Did I misunderstand?
>>>>
>>>>             On Jan 14, 2015 7:20 PM, "Oleksandr Otenko"
>>>>             <oleksandr.otenko at oracle.com
>>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>
>>>>                 It is not the hoist / remove of reload, it is the
>>>>                 order of reload and what might be inside park().
>>>>
>>>>                 Normal load of signal of the subsequent iteration
>>>>                 is allowed to appear ahead of whatever is inside
>>>>                 park() of the previous iteration, as it is
>>>>                 unspecified. The way Doug has clarified (or the way
>>>>                 I wanted to clarify it), the normal load of signal
>>>>                 can still appear ahead of whatever except the start
>>>>                 of park() to some threads. park() is allowed to
>>>>                 claim the permit in an unspecified way. So, it is
>>>>                 perfectly fine for park() to start before
>>>>                 signal==true is apparent, the subsequent read of
>>>>                 signal is allowed to appear to others ahead of
>>>>                 anything else in park() - ie it is allowed to
>>>>                 appear right after the decision that it doesn't
>>>>                 need to suspend. It observes signal==false. Now it
>>>>                 is allowed for Thread 2's store to signal to become
>>>>                 apparent to Thread 1 (but it is not loading it
>>>>                 anymore), and unpark() is allowed to release one
>>>>                 permit. Now it is allowed for Thread 1 to claim
>>>>                 that permit (or even clear the permit count without
>>>>                 observing that it was set). Now Thread 1's while
>>>>                 loop thinks signal==false, and it goes to call
>>>>                 park() again, and hangs, because no more unpark()
>>>>                 is called.
>>>>
>>>>                 If park() is deemed as a singular synchronization
>>>>                 event (I don't even require a happens-before, so no
>>>>                 extra edges appear; visibility of permits issued by
>>>>                 unpark() to park() become available in an
>>>>                 unspecified manner - only total ordering of events
>>>>                 around these calls in Java land matters), a
>>>>                 volatile load of signal would not be allowed to
>>>>                 appear to anyone ahead of park() nor any part
>>>>                 thereof - including the claim of the permit. So if
>>>>                 unpark() was already entered, the volatile load
>>>>                 would observe the volatile store of signal. Or, if
>>>>                 unpark was not yet entered, then whatever the load
>>>>                 of signal observes, it can at least go to park()
>>>>                 again until it either observes a permit from
>>>>                 unpark(), or the volatile store, whichever becomes
>>>>                 observed first.
>>>>
>>>>                 Doug's clarification is vague enough to permit
>>>>                 similar interpretation of how volatile load is
>>>>                 different from normal loads w.r.t. apparent
>>>>                 reordering with parts of park().
>>>>
>>>>                 Alex
>>>>
>>>>                 On 15/01/2015 00:00, Vitaly Davidovich wrote:
>>>>>                 Could you elaborate? Based on the conversation
>>>>>                 thus far, I don't see how compiler is allowed to
>>>>>                 hoist/remove the (re)load of signal on the loop
>>>>>                 iteration.
>>>>>
>>>>>                 On Wed, Jan 14, 2015 at 5:59 PM, Oleksandr Otenko
>>>>>                 <oleksandr.otenko at oracle.com
>>>>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>>>
>>>>>                     No, this code doesn't have to terminate.
>>>>>
>>>>>                     Alex
>>>>>
>>>>>
>>>>>                     On 14/01/2015 20:03, thurstonn wrote:
>>>>>
>>>>>                         Hello Alex,
>>>>>
>>>>>                         This is what I'm saying:
>>>>>
>>>>>                         the below code is slightly (but
>>>>>                         importantly different than our canonical
>>>>>                         example).
>>>>>
>>>>>
>>>>>                         boolean signal = false; // POSV ==> Plain
>>>>>                         Old Shared Variable
>>>>>
>>>>>
>>>>>                         Thread 1:
>>>>>                         while (! signal)
>>>>>                               park();
>>>>>
>>>>>                         Thread 2:
>>>>>                         signal = true;
>>>>>                         unpark(Thread1)
>>>>>
>>>>>
>>>>>
>>>>>                         Must this program terminate?  YES.
>>>>>
>>>>>
>>>>>                         Note - I am not advising this code, or
>>>>>                         trying to be pedantic
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>                         --
>>>>>                         View this message in context:
>>>>>                         http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p11935.html
>>>>>                         Sent from the JSR166 Concurrency mailing
>>>>>                         list archive at Nabble.com.
>>>>>                         _______________________________________________
>>>>>                         Concurrency-interest mailing list
>>>>>                         Concurrency-interest at cs.oswego.edu
>>>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>                     _______________________________________________
>>>>>                     Concurrency-interest mailing list
>>>>>                     Concurrency-interest at cs.oswego.edu
>>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/a01a8552/attachment-0001.html>

From jsampson at guidewire.com  Fri Jan 16 15:58:16 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 16 Jan 2015 20:58:16 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B928DF.80106@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B707CB.9090503@oracle.com>
	<CAHjP37EFEKDJgRgm09gbwkH3X+xeY7qOYVnNsZPT+F7neT968w@mail.gmail.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> You can't claim that it's not possible to implement reschedule in
> such a way that yield cannot tell whether reschedule was called
> since yield was called last time.

Ah, I see what you're getting at now, and I understand what you
meant by talking about a "fast path" earlier.

In your example, the yield/reschedule operations are _themselves_ a
valid implementation of park/unpark, complete with atomic handling
of internal state (the queue) that ensures a reschedule isn't lost.
That wasn't what I was imagining in the example of mine that you
were replying to.

So I think we're actually agreeing that the "permit" semantics do
have to be integrated into atomic operations at some level of
implementation. And I also agree that it's possible to define a
fast-path implementation on top of such semantics that still remains
valid but with less atomicity.

In fact, if I may generalize into a theorem, I believe we can say
that given any valid implementation of park/unpark, we can construct
another valid implementation, call it park'/unpark', like so:

Thread {
  volatile boolean permit';
}

park'() {
  Thread t = Thread.currentThread();
  if (!t.permit') {
    park();
  }
  t.permit' = false;
}

unpark'(Thread t) {
  if (!t.permit') {
    t.permit' = true;
    unpark(t);
  }
}

I'm writing up another attempt at formal semantics that allows us to
prove this theorem, which I'll send out later today. :)

Cheers,
Justin


From jsampson at guidewire.com  Fri Jan 16 17:07:51 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 16 Jan 2015 22:07:51 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54B9248A.2070508@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
	<54B9248A.2070508@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83EF9@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> re: synchronizes-with in failing CAS.
>
> The edge is there, if CAS doesn't fail before reading the value. I
> don't know if any hardware may at times fail CAS without reading
> the value.

That would be a spurious failure, wouldn't it? Doesn't the intrinsic
CAS implementation promise that it never fails spuriously?

The edge is definitely _not_ there in the case of failure or no-op
success in AtomicStampedReference's compareAndSet method, which is
the doc discrepency I'm proposing to fix. (The edge actually _is_
there in the case of spurious failure, though, as long as it's there
for a non-spurious intrinsic CAS failure, due to the way the stamped
pair object is handled. But I wouldn't want to spec that.)

> Note also that if CAS fails, then you won't have anything that
> depends on CAS result to synchronize-with.

Thanks, I think that confirms my intuition. It doesn't seem like any
well-behaved code can actually benefit from such a synchronized-with
edge.

Cheers,
Justin


From oleksandr.otenko at oracle.com  Fri Jan 16 17:10:23 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 16 Jan 2015 22:10:23 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B714E2.7020004@oracle.com>	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
Message-ID: <54B98C4F.1090000@oracle.com>

I would not bother specifying exactly what makes sense; only what's allowed.

For example, what would you say if some hardware offered atomicity of 
update of just the permit, and offer no barrier semantics for the rest 
of the accesses around it?

Alex

On 16/01/2015 20:58, Justin Sampson wrote:
> Oleksandr Otenko wrote:
>
>> You can't claim that it's not possible to implement reschedule in
>> such a way that yield cannot tell whether reschedule was called
>> since yield was called last time.
> Ah, I see what you're getting at now, and I understand what you
> meant by talking about a "fast path" earlier.
>
> In your example, the yield/reschedule operations are _themselves_ a
> valid implementation of park/unpark, complete with atomic handling
> of internal state (the queue) that ensures a reschedule isn't lost.
> That wasn't what I was imagining in the example of mine that you
> were replying to.
>
> So I think we're actually agreeing that the "permit" semantics do
> have to be integrated into atomic operations at some level of
> implementation. And I also agree that it's possible to define a
> fast-path implementation on top of such semantics that still remains
> valid but with less atomicity.
>
> In fact, if I may generalize into a theorem, I believe we can say
> that given any valid implementation of park/unpark, we can construct
> another valid implementation, call it park'/unpark', like so:
>
> Thread {
>    volatile boolean permit';
> }
>
> park'() {
>    Thread t = Thread.currentThread();
>    if (!t.permit') {
>      park();
>    }
>    t.permit' = false;
> }
>
> unpark'(Thread t) {
>    if (!t.permit') {
>      t.permit' = true;
>      unpark(t);
>    }
> }
>
> I'm writing up another attempt at formal semantics that allows us to
> prove this theorem, which I'll send out later today. :)
>
> Cheers,
> Justin


From gergg at cox.net  Fri Jan 16 19:18:53 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 16 Jan 2015 18:18:53 -0600
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <gQ6X1p02f02hR0p01Q6aE1>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<gQ6X1p02f02hR0p01Q6aE1>
Message-ID: <2085A844-341F-493E-B708-C5D172D68D71@cox.net>

It?s always convenient to ?wait and see?.  It?s always better to ?do and adapt? when you are trying to actually accomplish something ?better?.  While programming APIs can always be a problem once they are visible, the larger issue is the limitations that ?nothing yet? imposes compared to ?not complete?.  Of course a minimalist approach to API spec is always good early on, so that refinement is backward compatible for the steps that you can imagine.  Common APIs like this, still missing some 20 years after Java first appeared is pretty depressing.  I?ve rolled so many versions of this stuff into various applications in various places that it seems like old hat.  Would be great to have a name for something that is entirely about different pieces of software interacting with a common interface.

Gregg Wonderly

> On Jan 15, 2015, at 5:50 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> 
> On 01/15/2015 05:59 PM, Remi Forax wrote:
>> I think it's too soon to try to include an API like this in JDK9.
>> Currently, everybody seems to think it can come with a better API for a reactive
>> framework.
> 
> Where a lot of those everybodies were involved in formulating
> these APIs. See the list at http://www.reactive-streams.org/
> 
>> I think it's better to wait and see.
> 
> We waited. We saw :-)
> 
> -Doug
> 
> 
>> 
>> R?mi
>> 
>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>> 
>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>> 
>>> As discussed a few months ago, there is no single best fluent
>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>> continuation-style programming on futures, and java.util.stream best
>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>> the elements of collections. Until now, one missing category was
>>> "push" style operations on items as they become available from an
>>> active source. We are not alone in wanting a standard way to support
>>> this. Over the past year, the "reactive-streams"
>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>> set of interfaces expressing commonalities and allowing
>>> interoperablility across frameworks (including Rx and Akka Play), that
>>> is nearing release. These interfaces include provisions for a simple
>>> form of async flow control allowing developers to address resource
>>> control issues that can otherwise cause problems in push-based
>>> systems. Supporting this mini-framework helps avoid unpleasant
>>> surprises possible when trying to use pull-style APIs for "hot"
>>> reactive sources (but conversely is not as good a choice as
>>> java.util.Stream for "cold" sources like collections).
>>> 
>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>> Processor) are defined within the same class "Flow", that also
>>> includes the first of some planned support methods to establish and
>>> use Flow components, including tie-ins to java.util.streams and
>>> CompletableFutures. See
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>> 
>>> (Logistically, the only alternative to class Flow would have been
>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>> "import java.util.concurrent.flow.*;" would be.)
>>> 
>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>> bridge from various kinds of item producers to Flow components, and is
>>> useful in its own right. It is a form of ordered multicaster that
>>> people have complained that we don't support in j.u.c, but now do.
>>> See
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>> 
>>> 
>>> Disclaimers: These are only candidates for inclusion.  The are in
>>> preliminary form and will change. But comments and suggestions would
>>> be welcome. As with the other candidate additions, if you are brave,
>>> you can try out snapshots on jdk8+ by getting
>>> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>> and running java -Xbootclasspath/p:jsr166.jar
>>> 
>>> -Doug
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From boehm at acm.org  Fri Jan 16 19:59:56 2015
From: boehm at acm.org (Hans Boehm)
Date: Fri, 16 Jan 2015 16:59:56 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
Message-ID: <CAPUmR1Yt_-NY7N9ZT7GoeHjbyf_=nEn+V1=VxiQAxJJR8C2r+g@mail.gmail.com>

On Thu, Jan 15, 2015 at 5:38 PM, Justin Sampson <jsampson at guidewire.com>
wrote:
>
> Hans Boehm wrote:
>
> > I don't think it makes sense in the memory model to say something
> > has volatile write semantics if nothing is actually written. The
> > spec could perhaps be clearer that it's not saying that. But if it
> > does, that's arguably a vacuous statement anyway.
>
> From java/util/concurrent/atomic/package-summary.html:
>
> "compareAndSet and all other read-and-update operations such as
> getAndIncrement have the memory effects of both reading and writing
> volatile variables."
>
> I don't know if this is actually true for the intrinsic CAS -- if
> you think it's not, or shouldn't be, then it definitely needs to be
> rewritten. :) But it's _definitely_ not true for the compareAndSet
> on AtomicStampedReference, which only has the memory effects of
> writing a volatile variable if it actually writes a different value
> than was there before (and it can fail spuriously).

It's not well-written.  But I'm not that concerned if it says it has "the
memory effects of both reading and writing volatile variables", and the
description then states that in some cases it doesn't write the underlying
object at all, as I think the definition of CAS does.  Clearly the
nonexistent write can't be volatile.  If it were to make a volatile write
to some other unspecified location, that wouldn't be observable.  This
seems like a minor editorial issue to me.

The fact that StampedReference seems to elide a "redundant" volatile write
seems much more dubious, and I don't see why that's likely to be a
successful optimization anyway.  It only seems to help if a CAS is used to
conditionally replace a value by itself.  Does anyone understand why it's a
good idea to "optimize" that case?

...

>
> Out of curiosity, is a weakCompareAndSet at least considered a
> synchronization action, part of the overall synchronization order,
> such that it can't be reordered with any nearby volatile reads or
> writes? Or is it truly free-for-all?

The current Java memory model doesn't cover either weakCompareAndSet() or
lazySet().  They weren't around when the memory model work was done.  My
assumption is that weakCompareAndSet is similar to
compare_exchange_weak(..., memory_order_relaxed) in C++, but that leaves
open some questions about cache coherence, I think.  My assumption is that
they do not participate in a single total synchronization order.  (lazySet
operations already can't.)

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150116/f20fbadd/attachment.html>

From jsampson at guidewire.com  Fri Jan 16 20:33:08 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 17 Jan 2015 01:33:08 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B98C4F.1090000@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>

Okay, here's my attempt at formal semantics. Apologies if this
doesn't help anyone else, but it definitely helps me to think
through the semantics this way.

Given the following definitions:

* A call to unpark(t) by any thread is modeled as an unpark[t]
  action, which is a synchronization action without any necessary
  synchronized-with edges leading to it or from it.

* A call to park() by thread t that eventually returns is modeled as
  a returning-park[t] action, which is a synchronization action
  without any necessary synchronized-with edges leading to it or
  from it.

* A call to park() by thread t that never returns is modeled as a
  diverging-park[t] action, which is a thread divergence action.

* A call to parkNanos(t, n) or parkUntil(t, d) by thread t must
  eventually return and is therefore always modeled as a
  returning-park[t] action.

An implementation must satisfy this rule:

* If an execution contains a diverging-park[t] action for any thread
  t, then for every unpark[t] action u in the execution there also
  exists a returning-park[t] action p in the execution such that u
  precedes p in the execution's synchronization order -- so(u, p).

That's it. Intuitively, we only care about preventing divergence
when an unpark is available. So as long as the execution doesn't
contain any diverging parks, we don't care. And if there aren't any
unparks, we also don't care. We only have to disallow executions
where a park diverges when it shouldn't.

Notice that these formal semantics don't mention the permit or
volatile accesses or atomicity. The only ordering guarantee is right
there at the end: Every unpark must be followed by a park that
_doesn't_ diverge, if it's followed by a park at all.

TL;DR? YOU CAN STOP HERE. THE REST IS PROOFS BASED ON THIS RULE.

Looking at thurstonn's sample code:

[volatile] boolean signal;
T1: while (!signal) park();
T2: signal = true; unpark(T1);

Is this guaranteed to terminate given the rule above?

If signal is _not_ volatile, then there are no synchronization
actions other than the park and unpark themselves, which don't
provide any synchronized-with edges. Therefore there aren't any
happens-before edges between T1 and T2, and the read and write of
signal form a data race. T1 might never see signal become true, so
it is not guaranteed to terminate.

But if signal _is_ volatile, then the read and write of signal are
additional synchronization actions. To prove termination we have to
prove that no execution of this program may contain a
diverging-park[T1] action _and_ that T1 eventually sees T2's write.

The second part is easy: As long as park never diverges, T1
comprises an alternating sequence of volatile-read[signal] actions
and returning-park[T1] actions, which are all synchronization
actions and therefore included in the synchronization order
consistently with their program order. T2's volatile-write[signal]
action must appear somewhere in the synchronization order as well,
and it will therefore happen-before any subsequent
volatile-read[signal] actions by T1.

The first part can be proved by contradiction. If an execution of
this program contains a diverging-park[T1] action, then it must be
true that for all unpark[T1] actions in the execution, there exists
a returning-park[T1] action following it in the synchronization
order. The only unpark[T1] action is the one at the end of T2, which
means that we have a synchronization order like:

T2: volatile-write[signal]
...
T2: unpark[T1]
...
T1: returning-park[T1]
...
T1: volatile-read[signal]
...
T1: diverging-park[T1]

Immediately after any returning-park[T1] action, T1 always has a
volatile-read[signal] action, which at that point must see the write
from T2 and exit the loop, never calling park() again and therefore
never allowing the diverging-park[T1] action to occur. Therefore
this program is guaranteed to terminate given the rule above.

Now, let's look at some of the strawman implementations we've been
discussing. A simple semaphore (park = acquire, unpark = release)
obviously satisfies the rule; I won't bother proving it. No-ops for
both park and unpark trivially satisfy the rule because park always
returns! In fact, _any_ implementation in which park is always
guaranteed to eventually return satisfies the rule.

What about the definition of park and unpark as synchronization
actions? How does that square with the no-op implementation? Well,
remember that the implementation is only required to behave _as if_
the memory model works the way it's specified. If park cannot ever
diverge, then the ordering clause at the end of the rule is never
applicable. The fact that they're synchronization actions doesn't
actually impose any limits on reordering because they don't actually
have any synchronized-with edges. Remember that even a volatile
variable can be optimized into a non-volatile variable if the
compiler can prove that it's always accessed by a single thread! The
same reasoning is applicable here.

That brings us to the "theorem" from my previous email. I claimed,
generalizing Alex's argument, that given any valid implementation of
park and unpark, we can construct another valid implementation:

Thread {
  volatile boolean permit';
}

park'() {
  Thread t = Thread.currentThread();
  if (!t.permit') {
    park();
  }
  t.permit' = false;
}

unpark'(Thread t) {
  if (!t.permit') {
    t.permit' = true;
    unpark(t);
  }
}

Let me attempt to prove this theorem. The synchronization actions
are easy:

* If park' returns at all, its write of false to t.permit' serves as
  the required returning-park'[t] synchronization action.

* If the read of t.permit' in unpark' sees true, that read itself
  acts as the required unpark'[t] synchronization action and the
  method returns immediately.

* If the read of t.permit' in unpark' sees false, ignore its
  synchronization effects and move on to the body. The write of true
  to t.permit' serves as the required unpark'[t] synchronization
  action.

In all of these cases, notice that a returning-park'[t] action
always results in t.permit' being false and an unpark'[t] action
always results in t.permit' being true. In fact, the value of
t.permit' never changes _except_ at these specific synchronization
actions. So even without the "permit" being part of the formal
semantics, it's a useful part of the proof.

Now I have to prove that the divergence rule is satisfied, assuming
that it is satisfied for the underlying park and unpark operations.
Clearly, the only way that park' diverges is if it reads false for
t.permit' and then the underlying park diverges.

If park' reads false for t.permit', then we know that up to that
point in the synchronization order (since the read is itself a
synchronization action), either there haven't been any unpark'[t]
actions or there was a returning-park'[t] action following the most
recent unpark'[t] action. Therefore the ordering rule is already
satisfied for all unpark'[t] actions up to this point.

The only question is whether diverging-park[t] can occur in the
execution if an unpark'[t] action occurs after this point without
any additional returning-park'[t] actions. We know that since the
read of t.permit' in park' saw false, a subsequent unpark'[t] action
must follow the code path that sets t.permit' to true, which is
immediately followed by an underlying unpark[t] action. Given the
assumption that the underlying park and unpark implementation
satisfies the ordering rule, there must be another returning-park[t]
action later in the synchronization order. And since the only place
in this program that a returning-park[t] action can take place is
immediately before a returning-park'[t] action (the write to
t.permit' at the end of park'), then we necessarily have such a
returning-park'[t] action later in the synchronization order as
well.

Therefore the existence of a diverging-park'[t] action in the
execution implies that every unpark'[t] action is followed by a
returning-park'[t] action, satisfying the rule.

Q.E.D.,
Justin


From gergg at cox.net  Sat Jan 17 00:27:16 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 16 Jan 2015 23:27:16 -0600
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <gmPL1p01B02hR0p01mPNPG>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR0p01mPNPG>
Message-ID: <F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>


> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
> 
> I would not bother specifying exactly what makes sense; only what's allowed.
> 
> For example, what would you say if some hardware offered atomicity of update of just the permit, and offer no barrier semantics for the rest of the accesses around it?
> 

Why would you want to specify in a language, variations of behavior that complicate software design, and cause software systems developers to fail because they can?t understand what actually will happen (as opposed to what can happen) or correctly implement software because they are having to interact, literally with random hardware behavior due to the language not adequately depicting tangible software behaviors.

I know it?s really fun to play with this stuff and try and wrestle the most out of the hardware with the least possible constraints.  But really, how in the world can we have 100% reliable software systems when we provide 50 ways to be unsuccessful for every 1 way that is the correct way to structure the code?  Why is all of that variation event attractive to you Alex?  Do you really want developer to have to wrestle with software design?  Do you find it fun to manipulate people with such complexity in the system and some how prove that your understanding is more than theirs?  Does this give you some kind of satisfaction or what?

In the end, this is a huge barrier to being successful with Java.  It provides not obvious benefit to the developers from a long term goal of building and maintaining viable software systems.  Instead it creates problem after problem.  Software system become unpredictable with seemingly random behaviors. I just fail to see how this can feel like good software language design.  Maybe you can send me a private response if you don?t care to discuss this stuff publicly.  Practically I find this to be just insane.  All of this discussion in this thread just points out how 100s of hours of peoples time can be wasted on this issue all for nothing but surprise.  Developers with great knowledge of the JMM still have a hard time proving to themselves what the outcome of their code will be. 

Gregg Wonderly

> Alex
> 
> On 16/01/2015 20:58, Justin Sampson wrote:
>> Oleksandr Otenko wrote:
>> 
>>> You can't claim that it's not possible to implement reschedule in
>>> such a way that yield cannot tell whether reschedule was called
>>> since yield was called last time.
>> Ah, I see what you're getting at now, and I understand what you
>> meant by talking about a "fast path" earlier.
>> 
>> In your example, the yield/reschedule operations are _themselves_ a
>> valid implementation of park/unpark, complete with atomic handling
>> of internal state (the queue) that ensures a reschedule isn't lost.
>> That wasn't what I was imagining in the example of mine that you
>> were replying to.
>> 
>> So I think we're actually agreeing that the "permit" semantics do
>> have to be integrated into atomic operations at some level of
>> implementation. And I also agree that it's possible to define a
>> fast-path implementation on top of such semantics that still remains
>> valid but with less atomicity.
>> 
>> In fact, if I may generalize into a theorem, I believe we can say
>> that given any valid implementation of park/unpark, we can construct
>> another valid implementation, call it park'/unpark', like so:
>> 
>> Thread {
>>   volatile boolean permit';
>> }
>> 
>> park'() {
>>   Thread t = Thread.currentThread();
>>   if (!t.permit') {
>>     park();
>>   }
>>   t.permit' = false;
>> }
>> 
>> unpark'(Thread t) {
>>   if (!t.permit') {
>>     t.permit' = true;
>>     unpark(t);
>>   }
>> }
>> 
>> I'm writing up another attempt at formal semantics that allows us to
>> prove this theorem, which I'll send out later today. :)
>> 
>> Cheers,
>> Justin
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From forax at univ-mlv.fr  Sat Jan 17 06:15:40 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Sat, 17 Jan 2015 12:15:40 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <2085A844-341F-493E-B708-C5D172D68D71@cox.net>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<gQ6X1p02f02hR0p01Q6aE1>
	<2085A844-341F-493E-B708-C5D172D68D71@cox.net>
Message-ID: <54BA445C.7050709@univ-mlv.fr>


On 01/17/2015 01:18 AM, Gregg Wonderly wrote:
> It?s always convenient to ?wait and see?.  It?s always better to ?do and adapt? when you are trying to actually accomplish something ?better?.  While programming APIs can always be a problem once they are visible, the larger issue is the limitations that ?nothing yet? imposes compared to ?not complete?.  Of course a minimalist approach to API spec is always good early on, so that refinement is backward compatible for the steps that you can imagine.  Common APIs like this, still missing some 20 years after Java first appeared is pretty depressing.  I?ve rolled so many versions of this stuff into various applications in various places that it seems like old hat.  Would be great to have a name for something that is entirely about different pieces of software interacting with a common interface.
>
> Gregg Wonderly

I don't think the question is whether or not the reactive-streams API is 
useful as a common interface
but more why this API has to be integrated in the JDK.

cheers,
R?mi

>
>> On Jan 15, 2015, at 5:50 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>> I think it's too soon to try to include an API like this in JDK9.
>>> Currently, everybody seems to think it can come with a better API for a reactive
>>> framework.
>> Where a lot of those everybodies were involved in formulating
>> these APIs. See the list at http://www.reactive-streams.org/
>>
>>> I think it's better to wait and see.
>> We waited. We saw :-)
>>
>> -Doug
>>
>>
>>> R?mi
>>>
>>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>>
>>>> As discussed a few months ago, there is no single best fluent
>>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>>> continuation-style programming on futures, and java.util.stream best
>>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>>> the elements of collections. Until now, one missing category was
>>>> "push" style operations on items as they become available from an
>>>> active source. We are not alone in wanting a standard way to support
>>>> this. Over the past year, the "reactive-streams"
>>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>>> set of interfaces expressing commonalities and allowing
>>>> interoperablility across frameworks (including Rx and Akka Play), that
>>>> is nearing release. These interfaces include provisions for a simple
>>>> form of async flow control allowing developers to address resource
>>>> control issues that can otherwise cause problems in push-based
>>>> systems. Supporting this mini-framework helps avoid unpleasant
>>>> surprises possible when trying to use pull-style APIs for "hot"
>>>> reactive sources (but conversely is not as good a choice as
>>>> java.util.Stream for "cold" sources like collections).
>>>>
>>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>>> Processor) are defined within the same class "Flow", that also
>>>> includes the first of some planned support methods to establish and
>>>> use Flow components, including tie-ins to java.util.streams and
>>>> CompletableFutures. See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>>>
>>>> (Logistically, the only alternative to class Flow would have been
>>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>>> "import java.util.concurrent.flow.*;" would be.)
>>>>
>>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>>> bridge from various kinds of item producers to Flow components, and is
>>>> useful in its own right. It is a form of ordered multicaster that
>>>> people have complained that we don't support in j.u.c, but now do.
>>>> See
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>>>
>>>>
>>>> Disclaimers: These are only candidates for inclusion.  The are in
>>>> preliminary form and will change. But comments and suggestions would
>>>> be welcome. As with the other candidate additions, if you are brave,
>>>> you can try out snapshots on jdk8+ by getting
>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>>> and running java -Xbootclasspath/p:jsr166.jar
>>>>
>>>> -Doug
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From forax at univ-mlv.fr  Sat Jan 17 06:44:58 2015
From: forax at univ-mlv.fr (Remi Forax)
Date: Sat, 17 Jan 2015 12:44:58 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
Message-ID: <54BA4B3A.7020809@univ-mlv.fr>


On 01/16/2015 04:53 PM, Viktor Klang wrote:
> Hi R?mi,
>
> Thanks for the great questions
> (answering inline)
>
> On Fri, Jan 16, 2015 at 4:03 PM, Remi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     Viktor, Doug,
>
>     I see at least 3 changes that in my opinion should be done on the
>     current API.
>     - Subscriber.onSubscribe and onNext/onError/onComplete should not
>     be part of the same interface.
>
>
> Unfortunately that did not pan out (we've did a lot of research/PoCs, 
> I'd be surprised if we have missed any permutation) for the reason 
> that failure to create a Subscription due to a failure in the 
> Publisher requires the possibility of issuing onError without a 
> preceding onSubscribe.

This seems plainly wrong to me, it means that you have no way to 
distinguish between a rejection of the subscription and an error in the 
process. So how can you write something useful in onError given that at 
that point you have no way to distinguish those two kinds of error anymore.




> We are also considering allowing fast completion (avoiding having to 
> call onSubscribe if the stream is already completed).
>
>       This will make the workflow more explicit and avoid to have the
>     subscription object mutable
>
>
> The Subscription need not be mutable right now, did you mean Subscriber?

yes, the field that store the subscription is mutable which is a receipt 
for disaster because a Publisher like the SubmissionPublisher will 
expose the Subscriber but accessing the field that store the 
Subscription will lead to a race condition and an unsafe publication.

In my opinion, the Subscription object play the same role as a Future, 
you can control the observer from the inside or from the outside. So 
this object should be created before try to do any async work otherwise 
you mose the burden to access the Subscription object to the user 
instead of to the framework.

>
>     - onComplete and onError should be one method and not two,
>       let's call it onComplete(Throwable throwableOrNull).
>       I think it's better two says that you have to way to complete,
>     either normally or with an exception.
>       And from my experience, the code of onError and onComplete tend
>     to be fairly similar.
>
>
> That was an interesting suggestion & discussion we had quite early (1+ 
> year ago), but we decided both that `null`-checking as being an 
> anti-pattern, as well as mixing failure management code with 
> success-management code is brittle and mixes concerns.
> Also, a valid follow-on question was: where do you stop? 
> onNext(Either<Throwable,T> elemOrCompleteOrError)? 
> Function<Either<Throwable, T>, Void>? Function<Either<Subscription, 
> Either<Throwable, T>>, Void>?
>
> So it was deemed that having a distinct method not only made it easier 
> to avoid NPEs, made it easy to separate the concerns in the 
> implementations, easier to find what you were looking for in the 
> javadoc, the spec etc.

I don't ask to mix onNext and onError (or onNext, onError, onComplete) 
but only onError and onComplete because from my own experience the code 
you write here is fairly similar.
Agree that null-checking is not great, but we have Optional now so 
onComplete(Optional<Throwable>) is IMO fine.

This remember me a point I've forgotten to mention, onError should not 
take a Throwable as parameter,
it means that if a java.lang.Error is raised like OutOfMemoryError 
instead of bailing out, the API suggest to try to recover on this kind 
of error, worst most of the time, the Error will be propagated to all 
the Subscriber along the chain, each of them trying to do something that 
may require to allocate more memory.

>
>     - onComplete(Throwable) should be a default method (the
>     implementation do nothing),
>       so Observer and Consumer will be equivalent in term of function
>     type.
>
>
> Since the target is JDK9 onNext, onComplete and onError can definitely 
> be default methods, the reason why they aren't currently in the RS 
> spec is because we started before Java 8 shipped and keeping Java6-7 
> compatibility made sense.

Java6 compatibility make sense for the reactive-streams API not for the 
j.u.c.Flow API.

>
>       Re-using the example of Doug,
>
>       long requestSize = ...
>       Observer<T> observer = consumer::accept;   // auto-conversion
>     from a Consumer
>       publisher.subscribe(subscription -> {
>           long half  = requestSize / 2; // re-request when half consumed
>           subscription.request(half);
>           return new Observer<T>() {
>               private long count = half;
>               public void onNext(T item) {
>                   if (--count <= 0) {
>                       subscription.request(count = half);
>                   }
>                   observer.onNext(item);
>               }
>               public void onComplete(Throwable ex) {
>                 observer.onComplete(ex);
>               }
>           };
>       });
>
>       Note that in term of implementation you can still have an
>     implementation that implements both
>       Subscriber and Observer with onSubscribe() returning this.
>
>     and I'm still not convince that this should be integrated in JDK.
>
>
> I hope to be able to address those concerns :)
> Anything that I missed to reply to?

Why the reactive-streams API has to be integrated in JDK9,
the only compelling reason I see is the one given by Doug, people will 
try to use Stream for that otherwise.

In my opinion, integrated the reactive-streams API is a kind of 
loose-loose deal,
from the reactive-streams API, it will be set in stone so no v2 that 
overcome the shortcoming of the v1,
from the JDK perspective, having to integrated a sub-optimal API because 
the API has to be Java 6 compatible,
knowing that we will have to maintain forever an API that has changed 
several times in the past few years.

R?mi

>
>     R?mi
>
>
>     On 01/16/2015 01:59 PM, Viktor Klang wrote:
>>     Hi R?mi,
>>
>>     I'll try to address your concerns below:
>>
>>     On Fri, Jan 16, 2015 at 1:48 AM, Remi Forax <forax at univ-mlv.fr
>>     <mailto:forax at univ-mlv.fr>> wrote:
>>
>>
>>
>>         On 01/16/2015 12:50 AM, Doug Lea wrote:
>>
>>             On 01/15/2015 05:59 PM, Remi Forax wrote:
>>
>>                 I think it's too soon to try to include an API like
>>                 this in JDK9.
>>                 Currently, everybody seems to think it can come with
>>                 a better API for a reactive
>>                 framework.
>>
>>
>>             Where a lot of those everybodies were involved in formulating
>>             these APIs. See the list at http://www.reactive-streams.org/
>>
>>
>>         yes, I know, having a lot of people hammering the API is
>>         great thing,
>>         but why do you want to include that in the JDK, 
>>
>>         it's better for the API to stay out of the JDK because the
>>         release cycle is faster, 
>>
>>         it's better for the JDK because the API is still moving.
>>
>>
>>     The RS interfaces and method signatures has not seen an update in
>>     a long time and is considered done (RS is in 1.0.0.RC1 with an
>>     RC2 shipped soon with spec clarifications and improvements to the
>>     TCK and 1.0.0.final is expected shortly thereafter).
>>
>>
>>
>>
>>                 I think it's better to wait and see.
>>
>>
>>             We waited. We saw :-)
>>
>>
>>         I heard about Rx in 2009, and since that I have observed two
>>         things,
>>         the use cases have changed, the APIs have changed, even the
>>         manifesto has changed recently.
>>
>>         This is not a bad thing, it means the community around the
>>         reactive stuff is vibrant so
>>         why do you want to stop that rush of cool ideas.
>>
>>
>>     RS is specifically for interop?a common vocabulary and semantics
>>     to build asynchronous, non-blocking streaming transformation with
>>     non-blocking back pressure. So from an API perspective it is
>>     targeted towards library developers, just as most of the things
>>     in java.util.concurrent. End user APIs are provided on top,
>>     through implementations like SubmissionPublisher or RxJava,
>>     Project Reactor, Akka Streams, Ratpack etc.
>>
>>
>>         And, if you think you want to freeze the API, I think a JSR
>>         is a better vehicle.
>>
>>
>>     Given the the target and scope of RS, it seems to me that it fits
>>     nicely within the scope of JSR166?
>>     There is a spec, 4 pure interfaces with a sum total of 7
>>     methods and a TCK to validate implementations:
>>     https://github.com/reactive-streams/reactive-streams
>>
>>
>>
>>             -Doug
>>
>>
>>         R?mi
>>
>>
>>
>>
>>
>>                 R?mi
>>
>>                 On 01/15/2015 06:25 PM, Doug Lea wrote:
>>
>>
>>                     Here's the only set of candidates for new jdk9
>>                     j.u.c classes:
>>
>>                     As discussed a few months ago, there is no single
>>                     best fluent
>>                     async/parallel API.
>>                     CompletableFuture/CompletionStage best supports
>>                     continuation-style programming on futures, and
>>                     java.util.stream best
>>                     supports (multi-stage, possibly-parallel) "pull"
>>                     style operations on
>>                     the elements of collections. Until now, one
>>                     missing category was
>>                     "push" style operations on items as they become
>>                     available from an
>>                     active source. We are not alone in wanting a
>>                     standard way to support
>>                     this. Over the past year, the "reactive-streams"
>>                     (http://www.reactive-streams.org/) effort has
>>                     been defining a minimal
>>                     set of interfaces expressing commonalities and
>>                     allowing
>>                     interoperablility across frameworks (including Rx
>>                     and Akka Play), that
>>                     is nearing release. These interfaces include
>>                     provisions for a simple
>>                     form of async flow control allowing developers to
>>                     address resource
>>                     control issues that can otherwise cause problems
>>                     in push-based
>>                     systems. Supporting this mini-framework helps
>>                     avoid unpleasant
>>                     surprises possible when trying to use pull-style
>>                     APIs for "hot"
>>                     reactive sources (but conversely is not as good a
>>                     choice as
>>                     java.util.Stream for "cold" sources like
>>                     collections).
>>
>>                     The four intertwined interfaces (Publisher,
>>                     Subscriber, Subscription,
>>                     Processor) are defined within the same class
>>                     "Flow", that also
>>                     includes the first of some planned support
>>                     methods to establish and
>>                     use Flow components, including tie-ins to
>>                     java.util.streams and
>>                     CompletableFutures. See
>>                     http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>
>>
>>                     (Logistically, the only alternative to class Flow
>>                     would have been
>>                     to introduce a subpackage, which unnecessarily
>>                     complicates usage. And
>>                     "import static java.util.concurrent.Flow;" is
>>                     about as convenient as
>>                     "import java.util.concurrent.flow.*;" would be.)
>>
>>                     Also, the new stand-alone class
>>                     SubmissionPublisher can serve as a
>>                     bridge from various kinds of item producers to
>>                     Flow components, and is
>>                     useful in its own right. It is a form of ordered
>>                     multicaster that
>>                     people have complained that we don't support in
>>                     j.u.c, but now do.
>>                     See
>>                     http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>
>>
>>
>>                     Disclaimers: These are only candidates for
>>                     inclusion.  The are in
>>                     preliminary form and will change. But comments
>>                     and suggestions would
>>                     be welcome. As with the other candidate
>>                     additions, if you are brave,
>>                     you can try out snapshots on jdk8+ by getting
>>                     http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>                     and running java -Xbootclasspath/p:jsr166.jar
>>
>>                     -Doug
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>                 Concurrency-interest at cs.oswego.edu
>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     -- 
>>     Cheers,
>>     ?
>
>
>
>
> -- 
> Cheers,
> ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150117/ce6cc1cf/attachment-0001.html>

From dl at cs.oswego.edu  Sat Jan 17 10:48:57 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 17 Jan 2015 10:48:57 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BA4B3A.7020809@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
Message-ID: <54BA8469.1040003@cs.oswego.edu>

On 01/17/2015 06:44 AM, Remi Forax wrote:

>> Anything that I missed to reply to?
>
> Why the reactive-streams API has to be integrated in JDK9,
> the only compelling reason I see is the one given by Doug, people will try to
> use Stream for that otherwise.

This is one of several reasons, all along the lines of:
The primary mission of j.u,c is to provide efficient, well-specified,
standardized well-tested  basic concurrency support components.
If we don't introduce some pubsub-style interfaces, then we cannot
provide components that meet requirements of a growing segment
of our primary audience of infrastructure and middleware developers
(and occasionally wider audiences). For example SubmissionPublisher
is the kind of component that several people building async systems
have complained over the past few years that we do not provide.
(The situation is somewhat analogous to the introduction of
CompletionStage/CompletableFuture.)

The particular choice of Flow/reactive-stream interfaces is the
best way I know to carry this out. The interoperable APIs are bland
and awkward enough that many people will build on top of them for
application-programmer-visible usages. We can/should include a
few of these, like the Flow.consume and Flow.stream methods,
but don't have any kind of world-domination plan for reactive
middleware systems (or any other middleware systems. We just
provide support to them, and love them all equally.)

While I'm at it: One set of uncertainties is whether/how to
provide Flow components based on async IO. We mostly stay
out of the IO business in j.u.c, in part because concurrent
IO usages may use any of the many java.io/nio APIs and
policies surrounding them just to configure usage.  For example,
it would be possible (and not very hard) to add a utility to
use a Socket (or AsynchronousSocketChannel) as a Publisher.
But doing so would require mechanisms/policies for pooling
ByteBuffers, dealing with socket options, and so on. One possibility
(in the spirit of methods like Flow.consume) is to offer a
few simple ones, that can also serve as a model for others.

-Doug


From dl at cs.oswego.edu  Sat Jan 17 16:05:50 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 17 Jan 2015 16:05:50 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>	<54B8399F.6000106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>	<54B84055.2000701@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>	<54B928DF.80106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
Message-ID: <54BACEAE.2080908@cs.oswego.edu>

On 01/16/2015 08:33 PM, Justin Sampson wrote:
> Okay, here's my attempt at formal semantics. Apologies if this
> doesn't help anyone else, but it definitely helps me to think
> through the semantics this way.

Somewhat relatedly, take a look at
   http://www.cl.cam.ac.uk/~pes20/weakmemory/ecoop10.pdf
"Reasoning about the Implementation of Concurrency Abstractions on x86-TSO."
Scott Owens. In ECOOP 2010.

It includes a discussion of a bug that we once introduced
and fixed in the hotspot implementation of park.

I'm not sure your semantics does help anyone enough to
try to adapt in specs. It serves as an analysis of the minimal
amount of user-visible ordering/synchronization possible in a
valid implementation. Which might be correct, but even if so,
doesn't seem to convey usable information.

-Doug



From thurston at nomagicsoftware.com  Sat Jan 17 18:38:42 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 17 Jan 2015 16:38:42 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
References: <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
Message-ID: <1421537922357-12018.post@n7.nabble.com>

Just a few, slight modifications inline.


Justin Sampson wrote
> Okay, here's my attempt at formal semantics. Apologies if this
> doesn't help anyone else, but it definitely helps me to think
> through the semantics this way.
> 
> Given the following definitions:
> 
> * A call to unpark(t) by any thread is modeled as an unpark[t]
>   action, which is a synchronization action without any necessary
>   synchronized-with edges leading to it or from it.
> 
> * A call to park() by thread t that eventually returns is modeled as
>   a returning-park[t] action, which is a synchronization action
>   without any necessary synchronized-with edges leading to it or
>   from it.
> 
> * A call to park() by thread t that never returns is modeled as a
>   diverging-park[t] action, which is a thread divergence action.

A diverging-park action, should also be specified as a synchronization
action (obviously it will never have a synchronized-with order); I don't
know if that was a deliberate omission or not, but it shouldn't be.
In fact, one could alternatively specify the implementation rule as:
only histories with a diverging-park action as the last (in synchronization
order), with no immediately preceding unpark action (again,in the
synchronization order) are valid histories.


> * A call to parkNanos(t, n) or parkUntil(t, d) by thread t must
>   eventually return and is therefore always modeled as a
>   returning-park[t] action.
> 
> An implementation must satisfy this rule:
> 
> * If an execution contains a diverging-park[t] action for any thread
>   t, then for every unpark[t] action u in the execution there also
>   exists a returning-park[t] action p in the execution such that u
>   precedes p in the execution's synchronization order -- so(u, p).
> 
> That's it. Intuitively, we only care about preventing divergence
> when an unpark is available. So as long as the execution doesn't
> contain any diverging parks, we don't care. And if there aren't any
> unparks, we also don't care. We only have to disallow executions
> where a park diverges when it shouldn't.
> 
> Notice that these formal semantics don't mention the permit or
> volatile accesses or atomicity. The only ordering guarantee is right
> there at the end: Every unpark must be followed by a park that
> _doesn't_ diverge, if it's followed by a park at all.

And finally,it's such a quibble that I hesitate to mention it, but there's
Alex's damn prescription that no-ops for both unpark and park are
acceptable.

It's hard to get my head around the idea that in that case, the JVM could
provide a total order over all unpark and park actions (which it
*guarantees* for all synchronization actions); note, no-ops wouldn't violate
the implementation rule (since obviously no divergent-park actions are
possible), and so are "acceptable".
And all synchronization actions are enumerated in the JMM (i.e. chapter 17
of the JLS), i.e. I can't arbitrarily define some method foo() that
constitutes a "new" synchronization action, *even if the implementation only
consists of synchronization actions themselves*.

Anyway, minor quibbles aside, it looks right






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p12018.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Sat Jan 17 20:27:41 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 18 Jan 2015 01:27:41 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421537922357-12018.post@n7.nabble.com>
References: <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<1421537922357-12018.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D88EBE@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> Just a few, slight modifications inline.

Perfect, thanks for reading carefully!

> > * A call to park() by thread t that never returns is modeled as
> >   a diverging-park[t] action, which is a thread divergence
> >   action.
>
> A diverging-park action, should also be specified as a
> synchronization action (obviously it will never have a
> synchronized-with order); I don't know if that was a deliberate
> omission or not, but it shouldn't be. In fact, one could
> alternatively specify the implementation rule as: only histories
> with a diverging-park action as the last (in synchronization
> order), with no immediately preceding unpark action (again, in the
> synchronization order) are valid histories.

The JLS doesn't include thread divergence actions in the category of
synchronization actions... I think the idea is just that having a
thread divergence action in a thread's program order is a handy
formality for saying "this thread isn't going to do anything useful
anymore." I don't think you can really think of divergence as taking
a specific position in the synchronization order, because once a
thread diverges it keeps on diverging forever. To quote JLS section
17.4.2, "If a thread performs a thread divergence action, it will be
followed by an infinite number of thread divergence actions."

> And finally, it's such a quibble that I hesitate to mention it,
> but there's Alex's damn prescription that no-ops for both unpark
> and park are acceptable.
>
> It's hard to get my head around the idea that in that case, the
> JVM could provide a total order over all unpark and park actions
> (which it *guarantees* for all synchronization actions); note,
> no-ops wouldn't violate the implementation rule (since obviously
> no divergent-park actions are possible), and so are "acceptable".

Ah, I addressed that question a few paragraphs further down below
the "TL;DR" line. I can't expect anyone to read that insanely long
email in full, so I'll just quote that one paragraph here:

"What about the definition of park and unpark as synchronization
actions? How does that square with the no-op implementation? Well,
remember that the implementation is only required to behave _as if_
the memory model works the way it's specified. If park cannot ever
diverge, then the ordering clause at the end of the rule is never
applicable. The fact that they're synchronization actions doesn't
actually impose any limits on reordering because they don't actually
have any synchronized-with edges. Remember that even a volatile
variable can be optimized into a non-volatile variable if the
compiler can prove that it's always accessed by a single thread! The
same reasoning is applicable here."

> And all synchronization actions are enumerated in the JMM (i.e.
> chapter 17 of the JLS), i.e. I can't arbitrarily define some
> method foo() that constitutes a "new" synchronization action,
> *even if the implementation only consists of synchronization
> actions themselves*.

Ah, that's an interesting meta-question. A lot of the questions I've
seen arise on this list (and a lot of the questions that I have
myself) about subtle points of java.util.concurrent semantics come
down to the fact that the JMM, as elegant as it is, doesn't
explicitly allow for new synchronization primitives to be defined in
APIs, and therefore there isn't much formality in the j.u.c API
documentation. But I think it's fair to make the attempt. It seems
pretty reasonable to make statements like "this method behaves _as
if_ it were a single synchronization action according to the JMM."

> Anyway, minor quibbles aside, it looks right

Thanks again!
Justin


From jsampson at guidewire.com  Sat Jan 17 20:42:36 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 18 Jan 2015 01:42:36 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54B98C4F.1090000@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D89ED7@sm-ex-01-vm.guidewire.com>

Oleksandr Otenko wrote:

> For example, what would you say if some hardware offered atomicity
> of update of just the permit, and offer no barrier semantics for
> the rest of the accesses around it?

I think we've already agreed that park() and unpark() must at least
prevent reordering nearby _volatile_ accesses with the handling of
the permit, haven't we?

Cheers,
Justin


From jsampson at guidewire.com  Sat Jan 17 21:55:24 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 18 Jan 2015 02:55:24 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR0p01mPNPG> <F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8AF00@sm-ex-01-vm.guidewire.com>

Gregg Wonderly wrote:

> Why would you want to specify in a language, variations of
> behavior that complicate software design, and cause software
> systems developers to fail because they can't understand what
> actually will happen (as opposed to what can happen) or correctly
> implement software because they are having to interact, literally
> with random hardware behavior due to the language not adequately
> depicting tangible software behaviors.

Regarding this _particular_ discussion, the typical developer should
never be touching park/unpark at all, so I think it's not so much a
general language design issue. Pretty early in the thread, everyone
agreed that park/unpark have to be used together with volatile state
variables regardless of any happens-before edges that may or may not
be there between unpark and park. The funny thing is that no one
could quite agree on how best to document their shared intuition
about how things work.

> I know it's really fun to play with this stuff and try and wrestle
> the most out of the hardware with the least possible constraints.
> [...] Developers with great knowledge of the JMM still have a hard
> time proving to themselves what the outcome of their code will be. 

Fun, yes, but I think it's also a pretty important part of Java's
surprisingly-good performance. The amazing thing about the JMM is
how elegant it is while still being compatible with high-performance
modern processors. The challenge in the case of park/unpark (along
with some of the atomic stuff like lazySet & weakCompareAndSet) is
that they haven't gotten the same elegant formal treatment as the
language itself.

Cheers,
Justin


From jsampson at guidewire.com  Sat Jan 17 22:31:05 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 18 Jan 2015 03:31:05 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Yt_-NY7N9ZT7GoeHjbyf_=nEn+V1=VxiQAxJJR8C2r+g@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Yt_-NY7N9ZT7GoeHjbyf_=nEn+V1=VxiQAxJJR8C2r+g@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8AF1D@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> It's not well-written. But I'm not that concerned if it says it
> has "the memory effects of both reading and writing volatile
> variables", and the description then states that in some cases it
> doesn't write the underlying object at all, as I think the
> definition of CAS does. Clearly the nonexistent write can't be
> volatile. If it were to make a volatile write to some other
> unspecified location, that wouldn't be observable. This seems like
> a minor editorial issue to me.

Doesn't a hardware CAS typically involve pretty strong fences on
both sides? Even if the value isn't updated, it's still possible to
ensure that all writes prior to the CAS in program order will be
visible following all subsequent reads of the same atomic variable
by other threads, which is what I figured "the memory effects of
writing volatile variables" was referring to. I don't think it's
important that CAS behave that way, but at least it's a consistent
reading of the spec.

> The fact that StampedReference seems to elide a "redundant"
> volatile write seems much more dubious, and I don't see why that's
> likely to be a successful optimization anyway. It only seems to
> help if a CAS is used to conditionally replace a value by itself.
> Does anyone understand why it's a good idea to "optimize" that
> case?

For what it's worth, it does avoid an object allocation...

I may try my hand at a patch for the docs at some point. You and
Alex have at least confirmed my intuition about how things are
expected to work.

Thanks!
Justin


From gergg at cox.net  Sun Jan 18 16:11:32 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Sun, 18 Jan 2015 15:11:32 -0600
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <hF2w1p00K02hR0p01F2zX1>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<hF2w1p00K02hR0p01F2zX1>
Message-ID: <03EC586E-2917-44FD-A2A4-08884A90122A@cox.net>


> On Jan 17, 2015, at 8:55 PM, Justin Sampson <jsampson at guidewire.com> wrote:
> 
> Gregg Wonderly wrote:
> 
>> Why would you want to specify in a language, variations of
>> behavior that complicate software design, and cause software
>> systems developers to fail because they can't understand what
>> actually will happen (as opposed to what can happen) or correctly
>> implement software because they are having to interact, literally
>> with random hardware behavior due to the language not adequately
>> depicting tangible software behaviors.
> 
> Regarding this _particular_ discussion, the typical developer should
> never be touching park/unpark at all, so I think it's not so much a
> general language design issue. Pretty early in the thread, everyone
> agreed that park/unpark have to be used together with volatile state
> variables regardless of any happens-before edges that may or may not
> be there between unpark and park. The funny thing is that no one
> could quite agree on how best to document their shared intuition
> about how things work.

Well that is the small part of the problem.  The larger issue, for me, is all about how frustratingly complex the exposure of the hardware behavior, is such literal sense with per processor variance issues as well, can be for developers.  Because of nuances such as the loop hoisting on non-volatile tests, and other volatile vs non-volatile declaration, developers who type in programs without the use of volatile or other multi-threaded control constructs, are not creating correct programs.  Instead, the nuances of visibility are ever present and can plague developers for some time.

Very old software using Vector and Hashtable were subjected to courtesy happens before behaviors complete with ?cache syncs?, because of those implementations.  Basic multi-threaded java software just works in that environment (except for the loop hoist case).  So, now, with the advent of Doug?s marvelous concurrency libraries, we can still have great success with highly performant Java concurrency as you state below.  But, there are many things, such as the disagreement about park/unpark specification which illustrate that the target for some is ?hardware behavior? and for others is ?clarity of specification? with some hints of ?we can?t over specify or we?ll never be able to take advantage of new opportunities for ??.

That?s my issue.  At what level of ?comfort? do we have to be to let progress happen.  No we don?t need to ?break? the language by disallowing optimization.  But, we do need to guarantee that the developer can be productive and the software they create can be dependable and stable, without having to have a doctorate in computer science at every place that Java software is written.  It?s seems apparent to me, that this is one of the reasons why smart people pick Java and many others pick C# or Perl or Ruby or PHP or so many other scripting languages to developer software, instead of Java.  Those other languages don?t have a glaring requirement for being incredibly smart to keep from writing broken software that requires careful understanding of concurrency issues to get correct (even if under performing) software systems.  If you look around on the web for discussions about volatile in particular, you find things like the following examples where developers are trying desperately to help other developers not fall into the traps and pitfalls they have:

http://www.javamex.com/tutorials/synchronization_volatile.shtml  for java

http://www.albahari.com/threading/part4.aspx#_The_volatile_keyword for C#

These illustrate how much energy is wasted by developers having to worry about nuances of things that they don?t have full control over, because they don?t actually have a complete understanding of what the hardware actually does nor what the JVM does to try and meet the JMM requirements with java.util.concurrent on top of that trying to control things for itself.

Back to Vector and Hashtable providing courtesy visibility events.  Before volatile was actually implemented in Java 5, there wasn?t a reason to worry about volatile.  Everyone used one of those two collection types and because of the synchronization implementation, there was all kinds conveniences that created lots of lazy software constructions that were horribly prevalent.  Look back at all of the conversation at the release of Java 5 regarding whether many open source systems involving dependency injection and other ?another thread sets that value for this thread? systems.  There was just a ton of peoples time spent dealing with the single issue of making volatile mean ?success?, instead of something else being done to mean ?be more performant?.

There?s lots more to go on about from my perspective, but I?ve probably said enough.

Gregg Wonderly

> 
>> I know it's really fun to play with this stuff and try and wrestle
>> the most out of the hardware with the least possible constraints.
>> [...] Developers with great knowledge of the JMM still have a hard
>> time proving to themselves what the outcome of their code will be. 
> 
> Fun, yes, but I think it's also a pretty important part of Java's
> surprisingly-good performance. The amazing thing about the JMM is
> how elegant it is while still being compatible with high-performance
> modern processors. The challenge in the case of park/unpark (along
> with some of the atomic stuff like lazySet & weakCompareAndSet) is
> that they haven't gotten the same elegant formal treatment as the
> language itself.
> 
> Cheers,
> Justin
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From viktor.klang at gmail.com  Mon Jan 19 05:00:54 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 19 Jan 2015 11:00:54 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BA4B3A.7020809@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
Message-ID: <CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>

Hi R?mi,

Thanks for your reply, I'm sorry my response got delayed. (answers inline)

On Sat, Jan 17, 2015 at 12:44 PM, Remi Forax <forax at univ-mlv.fr> wrote:

>
> On 01/16/2015 04:53 PM, Viktor Klang wrote:
>
> Hi R?mi,
>
>  Thanks for the great questions
> (answering inline)
>
> On Fri, Jan 16, 2015 at 4:03 PM, Remi Forax <forax at univ-mlv.fr> wrote:
>
>>  Viktor, Doug,
>>
>> I see at least 3 changes that in my opinion should be done on the current
>> API.
>> - Subscriber.onSubscribe and onNext/onError/onComplete should not be part
>> of the same interface.
>>
>
>  Unfortunately that did not pan out (we've did a lot of research/PoCs,
> I'd be surprised if we have missed any permutation) for the reason that
> failure to create a Subscription due to a failure in the Publisher requires
> the possibility of issuing onError without a preceding onSubscribe.
>
>
> This seems plainly wrong to me, it means that you have no way to
> distinguish between a rejection of the subscription and an error in the
> process. So how can you write something useful in onError given that at
> that point you have no way to distinguish those two kinds of error anymore.
>

First of all, let me assure you that the RS spec has been thoughtfully and
thoroughly put together?there's been a lot of time and hard work going into
the semantics of all of the interfaces and methods.

RS Subscriber.onError is primarily for fail-fast cleanup, as such there is
not a need for being able to distinguish the difference between
subscription rejection (which can occur at any time) and "error in the
process". I'd consider not leaking resources as a very useful thing,
especially for building robust applications.

The ability to programmatically distinguish between subscription rejection
and "error in the process" has "lo'-to-no" utility as they both have the
exact same consequences for the Subscriber instance. Capabilities are
slightly different for Processor (see spec).


>
>
>
>
>    We are also considering allowing fast completion (avoiding having to
> call onSubscribe if the stream is already completed).
>
>
>>    This will make the workflow more explicit and avoid to have the
>> subscription object mutable
>>
>
>  The Subscription need not be mutable right now, did you mean Subscriber?
>
>
> yes, the field that store the subscription is mutable which is a receipt
> for disaster because a Publisher like the SubmissionPublisher will expose
> the Subscriber but accessing the field that store the Subscription will
> lead to a race condition and an unsafe publication.
>

A Publisher does not expose a Subscriber (`subscribe` returns void). A
Subscription can have a final-field to the Subscriber and then subsequently
publish itself to the Subscriber safely:

val s = new WhateverSubscription(someSubscriber)
someSubscriber.onSubscribe(s)


>
> In my opinion, the Subscription object play the same role as a Future, you
> can control the observer from the inside or from the outside.
>

As per RS spec, it is to be used by the Subscriber.


> So this object should be created before try to do any async work otherwise
> you mose the burden to access the Subscription object to the user instead
> of to the framework.
>

Referring to my previous snippet/example?the Subscription is created
before. In RS the Subscription is the association between a Publisher and a
Subscriber, it is not to be exposed to the outside.


>
>
>
>
>>
>> - onComplete and onError should be one method and not two,
>>   let's call it onComplete(Throwable throwableOrNull).
>>   I think it's better two says that you have to way to complete, either
>> normally or with an exception.
>>   And from my experience, the code of onError and onComplete tend to be
>> fairly similar.
>>
>
> That was an interesting suggestion & discussion we had quite early (1+
> year ago), but we decided both that `null`-checking as being an
> anti-pattern, as well as mixing failure management code with
> success-management code is brittle and mixes concerns.
> Also, a valid follow-on question was: where do you stop?
> onNext(Either<Throwable,T> elemOrCompleteOrError)?
> Function<Either<Throwable, T>, Void>? Function<Either<Subscription,
> Either<Throwable, T>>, Void>?
>
>  So it was deemed that having a distinct method not only made it easier
> to avoid NPEs, made it easy to separate the concerns in the
> implementations, easier to find what you were looking for in the javadoc,
> the spec etc.
>
>
> I don't ask to mix onNext and onError (or onNext, onError, onComplete) but
> only onError and onComplete because from my own experience the code you
> write here is fairly similar.
>

In my experience it is quite different (just as the code I write in a catch
block typically looks different from the code I write in a try block).

A very important fact is that the semantics for onError and onComplete are
quite dissimilar: onError is an ASAP signal, and onComplete is an ALAP
signal.


> Agree that null-checking is not great, but we have Optional now so
> onComplete(Optional<Throwable>) is IMO fine.
>
>
This suggestion is definitely better, but does not address the core problem
of mixing concerns and dissimilar semantics, do you agree?


> This remember me a point I've forgotten to mention, onError should not
> take a Throwable as parameter,
> it means that if a java.lang.Error is raised like OutOfMemoryError instead
> of bailing out, the API suggest to try to recover on this kind of error,
> worst most of the time, the Error will be propagated to all the Subscriber
> along the chain, each of them trying to do something that may require to
> allocate more memory.
>

(onError is not about error recovery, it is about fail-fast cleanup of
resources.)

Whether or not a Publisher implementation catches OOME (which it typically
shouldn't (IMO) as per: "An Error is a subclass of Throwable that indicates
serious problems that a reasonable application should not try to catch." -
https://docs.oracle.com/javase/6/docs/api/java/lang/Error.html)
is not really about RS at all?the same rules apply as whenever a try-catch
is written in any piece of logic.

The reason for onError to take a Throwable rather than an Exception is:
there exist Throwables that aren't Errors and aren't Exceptions that
Publishers would want to be able to signal downstream, and enforcing
onError to take an Exception rather than Throwable wouldn't solve anything,
as a Publisher who would like to pass down an OOME would just pass it down
as the cause of a wrapper Exception.

Besides that, there is quite a bit of precedence for having Throwables in
parameters:
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#completeExceptionally-java.lang.Throwable-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#obtrudeException-java.lang.Throwable-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#exceptionally-java.util.function.Function-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#handle-java.util.function.BiFunction-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#handleAsync-java.util.function.BiFunction-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#handleAsync-java.util.function.BiFunction-java.util.concurrent.Executor-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#whenComplete-java.util.function.BiConsumer-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#whenCompleteAsync-java.util.function.BiConsumer-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#whenCompleteAsync-java.util.function.BiConsumer-
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#whenCompleteAsync-java.util.function.BiConsumer-java.util.concurrent.Executor-

(For even more prior art, search for "with parameters of type Throwable" in
http://docs.oracle.com/javase/8/docs/api/java/lang/class-use/Throwable.html)


>
>
>>
>> - onComplete(Throwable) should be a default method (the implementation do
>> nothing),
>>   so Observer and Consumer will be equivalent in term of function type.
>>
>
>  Since the target is JDK9 onNext, onComplete and onError can definitely
> be default methods, the reason why they aren't currently in the RS spec is
> because we started before Java 8 shipped and keeping Java6-7 compatibility
> made sense.
>
>
> Java6 compatibility make sense for the reactive-streams API not for the
> j.u.c.Flow API.
>

Default methods would not really add value to the RS interfaces (there
isn't AFAICT anything that they could do by default) and since jdk9 would
be compiled with at least Java 8 classfile format version, JDK 6-7 would
still need to use a backport jar, just as jsr166 uses if you want to use
FJP on pre JDK7 meaning that this [the compatibility argument] is not a
problem for RS.


>
>
>
>
>>
>>   Re-using the example of Doug,
>>
>>   long requestSize = ...
>>   Observer<T> observer = consumer::accept;   // auto-conversion from a
>> Consumer
>>   publisher.subscribe(subscription -> {
>>       long half  = requestSize / 2; // re-request when half consumed
>>       subscription.request(half);
>>       return new Observer<T>() {
>>           private long count = half;
>>           public void onNext(T item) {
>>               if (--count <= 0) {
>>                    subscription.request(count = half);
>>               }
>>               observer.onNext(item);
>>           }
>>           public void onComplete(Throwable ex) {
>>             observer.onComplete(ex);
>>           }
>>       };
>>   });
>>
>>   Note that in term of implementation you can still have an
>> implementation that implements both
>>   Subscriber and Observer with onSubscribe() returning this.
>>
>> and I'm still not convince that this should be integrated in JDK.
>>
>
>  I hope to be able to address those concerns :)
> Anything that I missed to reply to?
>
>
> Why the reactive-streams API has to be integrated in JDK9,
> the only compelling reason I see is the one given by Doug, people will try
> to use Stream for that otherwise.
>

For Doug's compelling reasons, but also as a way for library writers and
advanced users to integrate with (the most common users of j.u.c, to have a
common platform for interop in the JDK).
(see Greggs comment earlier)


>
> In my opinion, integrated the reactive-streams API is a kind of
> loose-loose deal,
> from the reactive-streams API, it will be set in stone so no v2 that
> overcome the shortcoming of the v1,
>

This is not an issue?RS has been developed with this in mind.


> from the JDK perspective, having to integrated a sub-optimal API because
> the API has to be Java 6 compatible,
>

This is not an issue either?the RS as in j.u.c.Flow would not need to be
Java 6 compatible since it's easy to keep a compat jar.


> knowing that we will have to maintain forever an API that has changed
> several times in the past few years.
>

I'm not sure we're talking about the same thing here, are you talking about
Rx and not RS?

Since the Reactive Streams effort started, the -only- API change to
Publisher/Subscriber/Subscription/Processor has been switching from `int`
to `long` for `Subscription.request`. (see:
https://github.com/reactive-streams/reactive-streams/blob/e58fed62249ad6fbd36467d1bbe5c486f31a8c0e/spi/src/main/scala/asyncrx/spi/Publisher.scala
)

Fortunately (well, to be honest, by design) the RS API is interfaces
only?with a spec, a TCK and a set of example implementations that are not
only documented but also of course pass the TCK.

With that in mind, I cannot imagine anything more easy to maintain besides
not maintaining it at all, can you?


>
> R?mi
>
>
>
>
>>
>> R?mi
>>
>>
>> On 01/16/2015 01:59 PM, Viktor Klang wrote:
>>
>> Hi R?mi,
>>
>>  I'll try to address your concerns below:
>>
>> On Fri, Jan 16, 2015 at 1:48 AM, Remi Forax <forax at univ-mlv.fr> wrote:
>>
>>>
>>>
>>> On 01/16/2015 12:50 AM, Doug Lea wrote:
>>>
>>>> On 01/15/2015 05:59 PM, Remi Forax wrote:
>>>>
>>>>> I think it's too soon to try to include an API like this in JDK9.
>>>>> Currently, everybody seems to think it can come with a better API for
>>>>> a reactive
>>>>> framework.
>>>>>
>>>>
>>>> Where a lot of those everybodies were involved in formulating
>>>> these APIs. See the list at http://www.reactive-streams.org/
>>>>
>>>
>>>  yes, I know, having a lot of people hammering the API is great thing,
>>> but why do you want to include that in the JDK,
>>
>>  it's better for the API to stay out of the JDK because the release cycle
>>> is faster,
>>
>>  it's better for the JDK because the API is still moving.
>>
>>
>>  The RS interfaces and method signatures has not seen an update in a
>> long time and is considered done (RS is in 1.0.0.RC1 with an RC2 shipped
>> soon with spec clarifications and improvements to the TCK and 1.0.0.final
>> is expected shortly thereafter).
>>
>>
>>>
>>>
>>>
>>>> I think it's better to wait and see.
>>>>>
>>>>
>>>> We waited. We saw :-)
>>>>
>>>
>>>  I heard about Rx in 2009, and since that I have observed two things,
>>> the use cases have changed, the APIs have changed, even the manifesto
>>> has changed recently.
>>
>>  This is not a bad thing, it means the community around the reactive
>>> stuff is vibrant so
>>> why do you want to stop that rush of cool ideas.
>>>
>>
>>  RS is specifically for interop?a common vocabulary and semantics to
>> build asynchronous, non-blocking streaming transformation with non-blocking
>> back pressure. So from an API perspective it is targeted towards library
>> developers, just as most of the things in java.util.concurrent. End user
>> APIs are provided on top, through implementations like SubmissionPublisher
>> or RxJava, Project Reactor, Akka Streams, Ratpack etc.
>>
>>
>>>
>>> And, if you think you want to freeze the API, I think a JSR is a better
>>> vehicle.
>>>
>>
>>  Given the the target and scope of RS, it seems to me that it fits
>> nicely within the scope of JSR166?
>>  There is a spec, 4 pure interfaces with a sum total of 7 methods and a
>> TCK to validate implementations:
>> https://github.com/reactive-streams/reactive-streams
>>
>>
>>>
>>>> -Doug
>>>>
>>>
>>> R?mi
>>>
>>>
>>>
>>>>
>>>>
>>>>> R?mi
>>>>>
>>>>> On 01/15/2015 06:25 PM, Doug Lea wrote:
>>>>>
>>>>>>
>>>>>> Here's the only set of candidates for new jdk9 j.u.c classes:
>>>>>>
>>>>>> As discussed a few months ago, there is no single best fluent
>>>>>> async/parallel API. CompletableFuture/CompletionStage best supports
>>>>>> continuation-style programming on futures, and java.util.stream best
>>>>>> supports (multi-stage, possibly-parallel) "pull" style operations on
>>>>>> the elements of collections. Until now, one missing category was
>>>>>> "push" style operations on items as they become available from an
>>>>>> active source. We are not alone in wanting a standard way to support
>>>>>> this. Over the past year, the "reactive-streams"
>>>>>> (http://www.reactive-streams.org/) effort has been defining a minimal
>>>>>> set of interfaces expressing commonalities and allowing
>>>>>> interoperablility across frameworks (including Rx and Akka Play), that
>>>>>> is nearing release. These interfaces include provisions for a simple
>>>>>> form of async flow control allowing developers to address resource
>>>>>> control issues that can otherwise cause problems in push-based
>>>>>> systems. Supporting this mini-framework helps avoid unpleasant
>>>>>> surprises possible when trying to use pull-style APIs for "hot"
>>>>>> reactive sources (but conversely is not as good a choice as
>>>>>> java.util.Stream for "cold" sources like collections).
>>>>>>
>>>>>> The four intertwined interfaces (Publisher, Subscriber, Subscription,
>>>>>> Processor) are defined within the same class "Flow", that also
>>>>>> includes the first of some planned support methods to establish and
>>>>>> use Flow components, including tie-ins to java.util.streams and
>>>>>> CompletableFutures. See
>>>>>>
>>>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
>>>>>>
>>>>>> (Logistically, the only alternative to class Flow would have been
>>>>>> to introduce a subpackage, which unnecessarily complicates usage. And
>>>>>> "import static java.util.concurrent.Flow;" is about as convenient as
>>>>>> "import java.util.concurrent.flow.*;" would be.)
>>>>>>
>>>>>> Also, the new stand-alone class SubmissionPublisher can serve as a
>>>>>> bridge from various kinds of item producers to Flow components, and is
>>>>>> useful in its own right. It is a form of ordered multicaster that
>>>>>> people have complained that we don't support in j.u.c, but now do.
>>>>>> See
>>>>>>
>>>>>> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
>>>>>>
>>>>>>
>>>>>> Disclaimers: These are only candidates for inclusion.  The are in
>>>>>> preliminary form and will change. But comments and suggestions would
>>>>>> be welcome. As with the other candidate additions, if you are brave,
>>>>>> you can try out snapshots on jdk8+ by getting
>>>>>>   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
>>>>>> and running java -Xbootclasspath/p:jsr166.jar
>>>>>>
>>>>>> -Doug
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>>  --
>>   Cheers,
>> ?
>>
>>
>>
>
>
>  --
>   Cheers,
> ?
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/b4dbc360/attachment-0001.html>

From dl at cs.oswego.edu  Mon Jan 19 09:04:07 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Jan 2015 09:04:07 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
Message-ID: <54BD0ED7.50907@cs.oswego.edu>


I updated the documentation to address some of the questions
and concerns raised so far (please keep them coming) and
added some examples. See (also pasted below):
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html

...

public final class Flow
extends Object

Interrelated interfaces and static methods for establishing flow-controlled 
components in which Publishers produce items consumed by one or more 
Subscribers, each managed by a Subscription.

These interfaces correspond to the reactive-streams specification. They apply in 
both concurrent and distributed asynchronous settings: All (seven) methods are 
defined in void "oneway" message style. Communication entails a simple form of 
flow control (method Flow.Subscription.request(long)) that can be used to avoid 
resource management problems that may otherwise occur in "push" based systems.

Examples. A Flow.Publisher usually defines its own Flow.Subscription 
implementation; constructing one in method subscribe and issuing it to the 
calling Flow.Subscriber. It publishes items to the subscriber asynchronously, 
normally using an Executor. For example, here is a very simple publisher that 
only issues (when requested) a single TRUE item to any subscriber. Because each 
subscriber receives only the same single item, this class does not need the 
buffering and ordering control required in most implementations (for example 
SubmissionPublisher).


  class OneShotPublisher implements Publisher<Boolean> {
    final Executor executor = Executors.newSingleThreadExecutor();
    public void subscribe(Subscriber<? super Boolean> subscriber) {
        subscriber.onSubscribe(new OneShotSubscription(subscriber, executor));
    }
    static class OneShotSubscription implements Subscription {
      final Subscriber<? super Boolean> subscriber;
      final Executor executor;
      boolean completed;
      OneShotSubscription(Subscriber<? super Boolean> subscriber,
                          Executor executor) {
        this.subscriber = subscriber;
        this.executor = executor;
      }
      public synchronized void request(long n) {
        if (n > 0 && !completed) {
          completed = true;
          executor.execute(() -> {
                    subscriber.onNext(Boolean.TRUE);
                    subscriber.onComplete();
                });
        }
        else if (n < 0) {
          completed = true;
          subscriber.onError(new IllegalArgumentException());
          }
      }
      public synchronized void cancel() { completed = true; }
    }
  }

A Flow.Subscriber arranges that items be requested and processed. Items 
(invocations of Flow.Subscriber.onNext(T)) are not issued unless requested, but 
multiple items may be requested. Many Subscriber implementations can arrange 
this in the style of the following example, where a buffer size of 1 
single-steps, and larger sizes usually allow for more efficient overlapped 
processing with less communication; for example with a value of 64, this keeps 
total outstanding requests between 32 and 64. (See also consume(long, Publisher, 
Consumer) that automates a common case.) Notice that because Subscriber method 
invocations for a given Flow.Subscription are strictly ordered, there is no need 
for these methods to use locks or volatiles unless a Subscriber maintains 
multiple Subscriptions (in which case is it preferable to instead define 
multiple Subscribers, each with its own Subscription).


  class SampleSubscriber<T> implements Subscriber<T> {
    final Consumer<? super T> consumer;
    Subscription subscription;
    final long bufferSize;
    long count;
    SampleSubscriber(long bufferSize, Consumer<? super T> consumer) {
      this.bufferSize = bufferSize;
      this.consumer = consumer;
    }
    public void onSubscribe(Subscription subscription) {
      (this.subscription = subscription).request(bufferSize);
      count = bufferSize - bufferSize / 2; // re-request when half consumed
    }
    public void onNext(T item) {
      if (--count <= 0)
        subscription.request(count = bufferSize - bufferSize / 2);
      consumer.accept(item);
    }
    public void onError(Throwable ex) { ex.printStackTrace(); }
    public void onComplete() {}
  }

When flow control is not needed, a subscriber may initially request an 
effectively unbounded number of items, as in:


  class UnboundedSubscriber<T> implements Subscriber<T> {
    public void onSubscribe(Subscription subscription) {
      subscription.request(Long.MAX_VALUE); // effectively unbounded
    }
    public void onNext(T item) { use(item); }
    public void onError(Throwable ex) { ex.printStackTrace(); }
    public void onComplete() {}
    void use(T item) { ... }
  }


===================================

public class SubmissionPublisher<T>
extends Object
implements Flow.Publisher<T>, AutoCloseable

A Flow.Publisher that asynchronously issues submitted items to current 
subscribers until it is closed. Each current subscriber receives newly submitted 
items in the same order unless drops or exceptions are encountered. Using a 
SubmissionPublisher allows item generators to act as Publishers, although 
without integrated flow control. Instead they rely on drop handling and/or blocking.

A SubmissionPublisher uses the Executor supplied in its constructor for delivery 
to subscribers. The best choice of Executor depends on expected usage. If the 
generator(s) of submitted items run in separate threads, and the number of 
subscribers can be estimated, consider using a 
Executors.newFixedThreadPool(int). Otherwise consider using a work-stealing pool 
(including ForkJoinPool.commonPool()).

Buffering allows producers and consumers to transiently operate at different 
rates. Each subscriber uses an independent buffer. Buffers are created upon 
first use with a given initial capacity, and are resized as needed up to the 
maximum. (Capacity arguments may be rounded up to powers of two.) Invocations of 
Flow.Subscription.request(long) do not directly result in buffer expansion, but 
risk saturation if unfulfilled requests exceed the maximum capacity. Choices of 
buffer parameters rely on expected rates, resources, and usages, that usually 
benefit from empirical testing. As first guesses, consider initial 8 and maximum 
1024.

Publication methods support different policies about what to do when buffers are 
saturated. Method submit(T) blocks until resources are available. This is 
simplest, but least responsive. The offer methods may either immediately, or 
with bounded timeout, drop items, but provide an opportunity to interpose a 
handler and then retry.

If any Subscriber method throws an exception, its subscription is cancelled. If 
the supplied Executor throws RejectedExecutionException (or any other 
RuntimeException or Error) when attempting to execute a task, or a drop handler 
throws an exception when processing a dropped item, then the exception is 
rethrown. In these cases, some but not all subscribers may have received items. 
It is usually good practice to closeExceptionally in these cases.

This class may also serve as a convenient base for subclasses that generate 
items, and use the methods in this class to publish them. For example here is a 
class that periodically publishes the items generated from a supplier. (In 
practice you might add methods to independently start and stop generation, to 
share schedulers among publishers, and so on, or instead use a 
SubmissionPublisher as a component rather than a superclass.)


  class PeriodicPublisher<T> extends SubmissionPublisher<T> {
    final ScheduledFuture<?> periodicTask;
    final ScheduledExecutorService scheduler;
    PeriodicPublisher(Executor executor, int initialBufferCapacity,
                      int maxBufferCapacity, Supplier<? extends T> supplier,
                      long period, TimeUnit unit) {
      super(executor, initialBufferCapacity, maxBufferCapacity);
      scheduler = new ScheduledThreadPoolExecutor(1);
      periodicTask = scheduler.scheduleAtFixedRate(
        () -> submit(supplier.get()), 0, period, unit);
    }
    public void close() {
      periodicTask.cancel(false);
      scheduler.shutdown();
      super.close();
    }
  }

Here is an example of Flow.Processor subclass (using single-step requests to its 
publisher, for simplicity of illustration):


  class TransformProcessor<S,T> extends SubmissionPublisher<T>
    implements Flow.Processor<S,T> {
    final Function<? super S, ? extends T> function;
    Flow.Subscription subscription;
    TransformProcessor(Executor executor, int initialBufferCapacity,
                       int maxBufferCapacity,
                       Function<? super S, ? extends T> function) {
      super(executor, initialBufferCapacity, maxBufferCapacity);
      this.function = function;
    }
    public void onSubscribe(Flow.Subscription subscription) {
      (this.subscription = subscription).request(1);
    }
    public void onNext(S item) {
      subscription.request(1);
      submit(function.apply(item));
    }
    public void onError(Throwable ex) { closeExceptionally(ex); }
    public void onComplete() { close(); }
  }


From oleksandr.otenko at oracle.com  Mon Jan 19 09:17:49 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 19 Jan 2015 14:17:49 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
Message-ID: <54BD120D.7080700@oracle.com>

I don't think there can be an agreement on what "simple" means, it may 
be easier to consider the prerequisite knowledge of the target audience.

I don't know how many people are really interested in this discussion - 
but the only way to share understanding with the few that are, is 
broadcast on the relevant topic.

If I recall right what your pain point is, declaring all instructions in 
all threads to appear in program order to all threads, does not make an 
awful lot of difference in the reasoning methods, and in many cases even 
no difference in the outcomes. Roughly speaking, given a proof of 
correctness of "totally ordered" program, it may be possible to turn the 
key variables used in decision-making points into volatiles, and turn 
everything else into normal variables. The trouble is, most people don't 
even consider what the proof of correctness of a "totally ordered" 
program would look like. I think very quickly you will want to use 
induction in your proofs, which will touch only a few edges in your 
program - the rest being irrelevant, or can be proven from those few. I 
think it is the use of induction that determines what JMM should be; 
that's the only thing that really reduces complexity of proofs.

For example, your beloved loop example.

while(!signal)....

Why does the loop even exist? To me, it is evidence that somewhere in 
your proof you will have a statement "if signal==true, then ....<some 
consistency assertion here>" - as the program cannot deal with the 
inconsistent state; it does not progress and doesn't read nor write 
anything that appears in program order after the loop, until 
"signal==true". If you always rely on "signal == true", then you don't 
need total ordering - only partial ordering joined through signal read 
and write is sufficient; that means signal should be volatile, and the 
rest doesn't have to be (well, subject to restrictions in the rest of 
the code). If you sometimes do not rely on "signal==true", try to 
convince yourself that <some consistency assertion> holds, can you? If 
you can't convince that the consistency assertion holds, how can you 
show the program is correct? Nowhere here I mentioned total or partial 
ordering of instructions.

Alex

On 17/01/2015 05:27, Gregg Wonderly wrote:
>> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>
>> I would not bother specifying exactly what makes sense; only what's allowed.
>>
>> For example, what would you say if some hardware offered atomicity of update of just the permit, and offer no barrier semantics for the rest of the accesses around it?
>>
> Why would you want to specify in a language, variations of behavior that complicate software design, and cause software systems developers to fail because they can?t understand what actually will happen (as opposed to what can happen) or correctly implement software because they are having to interact, literally with random hardware behavior due to the language not adequately depicting tangible software behaviors.
>
> I know it?s really fun to play with this stuff and try and wrestle the most out of the hardware with the least possible constraints.  But really, how in the world can we have 100% reliable software systems when we provide 50 ways to be unsuccessful for every 1 way that is the correct way to structure the code?  Why is all of that variation event attractive to you Alex?  Do you really want developer to have to wrestle with software design?  Do you find it fun to manipulate people with such complexity in the system and some how prove that your understanding is more than theirs?  Does this give you some kind of satisfaction or what?
>
> In the end, this is a huge barrier to being successful with Java.  It provides not obvious benefit to the developers from a long term goal of building and maintaining viable software systems.  Instead it creates problem after problem.  Software system become unpredictable with seemingly random behaviors. I just fail to see how this can feel like good software language design.  Maybe you can send me a private response if you don?t care to discuss this stuff publicly.  Practically I find this to be just insane.  All of this discussion in this thread just points out how 100s of hours of peoples time can be wasted on this issue all for nothing but surprise.  Developers with great knowledge of the JMM still have a hard time proving to themselves what the outcome of their code will be.
>
> Gregg Wonderly
>
>> Alex
>>
>> On 16/01/2015 20:58, Justin Sampson wrote:
>>> Oleksandr Otenko wrote:
>>>
>>>> You can't claim that it's not possible to implement reschedule in
>>>> such a way that yield cannot tell whether reschedule was called
>>>> since yield was called last time.
>>> Ah, I see what you're getting at now, and I understand what you
>>> meant by talking about a "fast path" earlier.
>>>
>>> In your example, the yield/reschedule operations are _themselves_ a
>>> valid implementation of park/unpark, complete with atomic handling
>>> of internal state (the queue) that ensures a reschedule isn't lost.
>>> That wasn't what I was imagining in the example of mine that you
>>> were replying to.
>>>
>>> So I think we're actually agreeing that the "permit" semantics do
>>> have to be integrated into atomic operations at some level of
>>> implementation. And I also agree that it's possible to define a
>>> fast-path implementation on top of such semantics that still remains
>>> valid but with less atomicity.
>>>
>>> In fact, if I may generalize into a theorem, I believe we can say
>>> that given any valid implementation of park/unpark, we can construct
>>> another valid implementation, call it park'/unpark', like so:
>>>
>>> Thread {
>>>    volatile boolean permit';
>>> }
>>>
>>> park'() {
>>>    Thread t = Thread.currentThread();
>>>    if (!t.permit') {
>>>      park();
>>>    }
>>>    t.permit' = false;
>>> }
>>>
>>> unpark'(Thread t) {
>>>    if (!t.permit') {
>>>      t.permit' = true;
>>>      unpark(t);
>>>    }
>>> }
>>>
>>> I'm writing up another attempt at formal semantics that allows us to
>>> prove this theorem, which I'll send out later today. :)
>>>
>>> Cheers,
>>> Justin
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Mon Jan 19 09:45:37 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 19 Jan 2015 14:45:37 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54BD120D.7080700@oracle.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<54BD120D.7080700@oracle.com>
Message-ID: <54BD1891.5040100@oracle.com>

On 19/01/2015 14:17, Oleksandr Otenko wrote:
> I don't think there can be an agreement on what "simple" means, it may 
> be easier to consider the prerequisite knowledge of the target audience.
>
> I don't know how many people are really interested in this discussion 
> - but the only way to share understanding with the few that are, is 
> broadcast on the relevant topic.
>
> If I recall right what your pain point is, declaring all instructions 
> in all threads to appear in program order to all threads, does not 
> make an awful lot of difference in the reasoning methods, and in many 
> cases even no difference in the outcomes. Roughly speaking, given a 
> proof of correctness of "totally ordered" program, it may be possible 
> to turn the key variables used in decision-making points into 
> volatiles, and turn everything else into normal variables. The trouble 
> is, most people don't even consider what the proof of correctness of a 
> "totally ordered" program would look like. I think very quickly you 
> will want to use induction in your proofs, which will touch only a few 
> edges in your program - the rest being irrelevant, or can be proven 
> from those few. I think it is the use of induction that determines 
> what JMM should be; that's the only thing that really reduces 
> complexity of proofs.
>
> For example, your beloved loop example.
>
> while(!signal)....
>
> Why does the loop even exist? To me, it is evidence that somewhere in 
> your proof you will have a statement "if signal==true, then ....<some 
> consistency assertion here>" - as the program cannot deal with the 
> inconsistent state; it does not progress and doesn't read nor write 
> anything that appears in program order after the loop, until 
> "signal==true". If you always rely on "signal == true", then you don't 
> need total ordering - only partial ordering joined through signal read 
> and write is sufficient; that means signal should be volatile, and the 
> rest doesn't have to be (well, subject to restrictions in the rest of 
> the code). If you sometimes do not rely on "signal==true", try to 
> convince yourself that <some consistency assertion> holds, can you? If 
> you can't convince that the consistency assertion holds, how can you 
> show the program is correct? Nowhere here I mentioned total or partial 
> ordering of instructions.

This of course means that nowhere here I mentioned ordering of 
instructions as the prerequisite of the memory model - only as a 
consequence of how the proof is constructed.

And here's where induction is used here:

Now consider what the <consistency assertion> is, and how are we sure it 
holds - after all, it isn't an axiom; not an arbitrary statement.

In fact, it is the following piece of code, dual of the loop:

<some code leaving the program in a consistent state, whatever that may be>
signal=true;

This code specifies what consistency you may assert. It is the proof of 
the assertion you may make after observing signal==true. It is up to you 
to write the code before "signal=true". It is up to you what you may 
assert as a consequence.

And the induction here is that "for all writes that appear in program 
order before signal=true" they will be seen to "all the reads that 
appear in program order after observing signal==true". The induction 
here is "if you present *evidence* of signal==true, */everything/* that 
was before it is also true / has also occurred / can also be observed"


Alex


>
> Alex
>
> On 17/01/2015 05:27, Gregg Wonderly wrote:
>>> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko 
>>> <oleksandr.otenko at oracle.com> wrote:
>>>
>>> I would not bother specifying exactly what makes sense; only what's 
>>> allowed.
>>>
>>> For example, what would you say if some hardware offered atomicity 
>>> of update of just the permit, and offer no barrier semantics for the 
>>> rest of the accesses around it?
>>>
>> Why would you want to specify in a language, variations of behavior 
>> that complicate software design, and cause software systems 
>> developers to fail because they can?t understand what actually will 
>> happen (as opposed to what can happen) or correctly implement 
>> software because they are having to interact, literally with random 
>> hardware behavior due to the language not adequately depicting 
>> tangible software behaviors.
>>
>> I know it?s really fun to play with this stuff and try and wrestle 
>> the most out of the hardware with the least possible constraints.  
>> But really, how in the world can we have 100% reliable software 
>> systems when we provide 50 ways to be unsuccessful for every 1 way 
>> that is the correct way to structure the code?  Why is all of that 
>> variation event attractive to you Alex?  Do you really want developer 
>> to have to wrestle with software design?  Do you find it fun to 
>> manipulate people with such complexity in the system and some how 
>> prove that your understanding is more than theirs?  Does this give 
>> you some kind of satisfaction or what?
>>
>> In the end, this is a huge barrier to being successful with Java.  It 
>> provides not obvious benefit to the developers from a long term goal 
>> of building and maintaining viable software systems.  Instead it 
>> creates problem after problem.  Software system become unpredictable 
>> with seemingly random behaviors. I just fail to see how this can feel 
>> like good software language design.  Maybe you can send me a private 
>> response if you don?t care to discuss this stuff publicly.  
>> Practically I find this to be just insane.  All of this discussion in 
>> this thread just points out how 100s of hours of peoples time can be 
>> wasted on this issue all for nothing but surprise.  Developers with 
>> great knowledge of the JMM still have a hard time proving to 
>> themselves what the outcome of their code will be.
>>
>> Gregg Wonderly
>>
>>> Alex
>>>
>>> On 16/01/2015 20:58, Justin Sampson wrote:
>>>> Oleksandr Otenko wrote:
>>>>
>>>>> You can't claim that it's not possible to implement reschedule in
>>>>> such a way that yield cannot tell whether reschedule was called
>>>>> since yield was called last time.
>>>> Ah, I see what you're getting at now, and I understand what you
>>>> meant by talking about a "fast path" earlier.
>>>>
>>>> In your example, the yield/reschedule operations are _themselves_ a
>>>> valid implementation of park/unpark, complete with atomic handling
>>>> of internal state (the queue) that ensures a reschedule isn't lost.
>>>> That wasn't what I was imagining in the example of mine that you
>>>> were replying to.
>>>>
>>>> So I think we're actually agreeing that the "permit" semantics do
>>>> have to be integrated into atomic operations at some level of
>>>> implementation. And I also agree that it's possible to define a
>>>> fast-path implementation on top of such semantics that still remains
>>>> valid but with less atomicity.
>>>>
>>>> In fact, if I may generalize into a theorem, I believe we can say
>>>> that given any valid implementation of park/unpark, we can construct
>>>> another valid implementation, call it park'/unpark', like so:
>>>>
>>>> Thread {
>>>>    volatile boolean permit';
>>>> }
>>>>
>>>> park'() {
>>>>    Thread t = Thread.currentThread();
>>>>    if (!t.permit') {
>>>>      park();
>>>>    }
>>>>    t.permit' = false;
>>>> }
>>>>
>>>> unpark'(Thread t) {
>>>>    if (!t.permit') {
>>>>      t.permit' = true;
>>>>      unpark(t);
>>>>    }
>>>> }
>>>>
>>>> I'm writing up another attempt at formal semantics that allows us to
>>>> prove this theorem, which I'll send out later today. :)
>>>>
>>>> Cheers,
>>>> Justin
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/5e314e02/attachment-0001.html>

From akarnokd at gmail.com  Mon Jan 19 09:54:01 2015
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Mon, 19 Jan 2015 15:54:01 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BD0ED7.50907@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
	<54BD0ED7.50907@cs.oswego.edu>
Message-ID: <CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>

2015-01-19 15:04 GMT+01:00 Doug Lea <dl at cs.oswego.edu>:

>
>
>  class OneShotPublisher implements Publisher<Boolean> {
>    final Executor executor = Executors.newSingleThreadExecutor();
>    public void subscribe(Subscriber<? super Boolean> subscriber) {
>        subscriber.onSubscribe(new OneShotSubscription(subscriber,
> executor));
>    }
>    static class OneShotSubscription implements Subscription {
>      final Subscriber<? super Boolean> subscriber;
>      final Executor executor;
>      boolean completed;
>      OneShotSubscription(Subscriber<? super Boolean> subscriber,
>                          Executor executor) {
>        this.subscriber = subscriber;
>        this.executor = executor;
>      }
>      public synchronized void request(long n) {
>        if (n > 0 && !completed) {
>          completed = true;
>          executor.execute(() -> {
>                    subscriber.onNext(Boolean.TRUE);
>                    subscriber.onComplete();
>                });
>        }
>        else if (n < 0) {
>          completed = true;
>          subscriber.onError(new IllegalArgumentException());
>          }
>      }
>      public synchronized void cancel() { completed = true; }
>    }
>  }
>
>
I know this is supposed to be a simple example, but it has a few problems
and may convey a false sense of easiness:

- the executor created is never shut down so it will keep the JVM from
quitting,
- there might be only a small window to cancel the subscription before the
request is issued, and
- there is no way to cancel the scheduled emission, so for example, on the
shared executor, if the processing of the previous true emission takes
longer and the next subscriber times out, it can't say to the executor to
not even bother with its value. (Okay, the spec says cancellation is best
effort, but I'd hope it does more effort especially if
subscriber-uncontrollable resources are present.)

Writing correct backpressure-aware and cancellation-supporting publishers
is challenging, we still couldn't get everything right in RxJava despite
the time invested.
-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/023f8d29/attachment.html>

From dl at cs.oswego.edu  Mon Jan 19 10:17:27 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Jan 2015 10:17:27 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BA4B3A.7020809@univ-mlv.fr>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
Message-ID: <54BD2007.3090907@cs.oswego.edu>

On 01/17/2015 06:44 AM, Remi Forax wrote:

> yes, the field that store the subscription is mutable which is a receipt for
> disaster because a Publisher like the SubmissionPublisher will expose the
> Subscriber but accessing the field that store the Subscription will lead to a
> race condition and an unsafe publication.

The void-ness of Publisher.subscribe seems to be a common concern.
It is NOT racy: publishers/subscriptions are required to invoke
Subscriber methods in strict happens-before order.

However, the use of a "write once" variable that cannot be declared
as "final" looks uncomfortable to concurrent programmers. As
Viktor notes, some usages avoid this. But it might be nice to finally
introduce @WriteOnce so that tools could help make sure you don't
re-assign in usages like:


     class MySubscriber<T> implements Flow.Subscriber<T> {
         @WriteOnce Flow.Subscription subscription;
         MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
         public void onSubscribe(Flow.Subscription subscription) {
             (this.subscription = subscription).request(1);
         }
         public void onNext(S item) {
             subscription.request(1);
             // ... process item ...
         }
         public void onError(Throwable ex) { ex.printStackTrace(); }
         public void onComplete() { ... }
     }

-Doug


From dl at cs.oswego.edu  Mon Jan 19 10:40:53 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Jan 2015 10:40:53 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>	<54BD0ED7.50907@cs.oswego.edu>
	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
Message-ID: <54BD2585.5040903@cs.oswego.edu>

On 01/19/2015 09:54 AM, D?vid Karnok wrote:
>

> I know this is supposed to be a simple example, but it has a few problems and
> may convey a false sense of easiness:

Thanks. The intent was to convey a true sense of un-easiness of anything
beyond a one-shot :-) Further suggestions welcome.

>
> - the executor created is never shut down so it will keep the JVM from quitting,

Thanks. Replaced with
  *   final Executor executor = ForkJoinPool.commonPool(); // daemon-based

> - there might be only a small window to cancel the subscription before the
> request is issued, and

Thanks I added note that if cancel() follows request(), it is ineffective.

This is OK according to spec, and must be OK, since cancel is intrinsically
racy.

> (Okay, the spec says cancellation is best effort, but I'd
> hope it does more effort especially if subscriber-uncontrollable resources are
> present.)

Right.

>
> Writing correct backpressure-aware and cancellation-supporting publishers is
> challenging

Yes, that's one of the main reasons for adding j.u.c support.

-Doug




From peter.levart at gmail.com  Mon Jan 19 10:44:30 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 19 Jan 2015 16:44:30 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Yt_-NY7N9ZT7GoeHjbyf_=nEn+V1=VxiQAxJJR8C2r+g@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D83102@sm-ex-01-vm.guidewire.com>	<CAPUmR1Zuu2Hiw0SjY6OQ5+8UBKs2rwDX1kjVm6JOdosh+u=gcQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83CC7@sm-ex-01-vm.guidewire.com>
	<CAPUmR1Yt_-NY7N9ZT7GoeHjbyf_=nEn+V1=VxiQAxJJR8C2r+g@mail.gmail.com>
Message-ID: <54BD265E.8080702@gmail.com>

On 01/17/2015 01:59 AM, Hans Boehm wrote:
> On Thu, Jan 15, 2015 at 5:38 PM, Justin Sampson <jsampson at guidewire.com>
> wrote:
>> Hans Boehm wrote:
>>
>>> I don't think it makes sense in the memory model to say something
>>> has volatile write semantics if nothing is actually written. The
>>> spec could perhaps be clearer that it's not saying that. But if it
>>> does, that's arguably a vacuous statement anyway.
>>  From java/util/concurrent/atomic/package-summary.html:
>>
>> "compareAndSet and all other read-and-update operations such as
>> getAndIncrement have the memory effects of both reading and writing
>> volatile variables."
>>
>> I don't know if this is actually true for the intrinsic CAS -- if
>> you think it's not, or shouldn't be, then it definitely needs to be
>> rewritten. :) But it's _definitely_ not true for the compareAndSet
>> on AtomicStampedReference, which only has the memory effects of
>> writing a volatile variable if it actually writes a different value
>> than was there before (and it can fail spuriously).
> It's not well-written.  But I'm not that concerned if it says it has "the
> memory effects of both reading and writing volatile variables", and the
> description then states that in some cases it doesn't write the underlying
> object at all, as I think the definition of CAS does.  Clearly the
> nonexistent write can't be volatile.  If it were to make a volatile write
> to some other unspecified location, that wouldn't be observable.  This
> seems like a minor editorial issue to me.
>

Perhaps the intention is to specify that compiler must treat CAS as a 
volatile read+write regarding reordering of surrounding reads and writes 
regardless of whether CAS is successful or not (which compiler can't know).

It's interesting that unsuccessful CAS (at least on Java and Intel i7 
Sandy Bridge) takes exactly the same time to execute as successful. For 
example, the following JMH benchmark:

@BenchmarkMode(Mode.AverageTime)
@Fork(1)
@Warmup(iterations = 5)
@Measurement(iterations = 10)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
public class AtomicBench {

     static final AtomicInteger i = new AtomicInteger();

     @Benchmark
     public int get() {
         return i.get();
     }

     @Benchmark
     public void set() {
         i.set(123);
     }

     @Benchmark
     public int setGet() {
         i.set(123);
         return i.get();
     }

     @Benchmark
     public boolean casSuccess() {
         return i.compareAndSet(0, 0);
     }

     @Benchmark
     public boolean casFail() {
         return i.compareAndSet(42, 43);
     }
}


Prints the following:

Benchmark                     Mode  Samples  Score   Error  Units
j.t.AtomicBench.casFail       avgt       10  7.246 ? 0.255  ns/op
j.t.AtomicBench.casSuccess    avgt       10  7.168 ? 0.283  ns/op
j.t.AtomicBench.get           avgt       10  2.128 ? 0.055  ns/op
j.t.AtomicBench.set           avgt       10  5.506 ? 0.203  ns/op
j.t.AtomicBench.setGet        avgt       10  9.370 ? 0.072  ns/op



So if unsuccessful CAS is not a volatile write, why does it seem to take 
the same time as successful one?

If the benchmark tells the truth, would pre-screening a CAS with a 
volatile read from the same location help if we anticipate failure 
frequently?


Regards, Peter

>
> ...
>
>> Out of curiosity, is a weakCompareAndSet at least considered a
>> synchronization action, part of the overall synchronization order,
>> such that it can't be reordered with any nearby volatile reads or
>> writes? Or is it truly free-for-all?
> The current Java memory model doesn't cover either weakCompareAndSet() or
> lazySet().  They weren't around when the memory model work was done.  My
> assumption is that weakCompareAndSet is similar to
> compare_exchange_weak(..., memory_order_relaxed) in C++, but that leaves
> open some questions about cache coherence, I think.  My assumption is that
> they do not participate in a single total synchronization order.  (lazySet
> operations already can't.)
>
> Hans
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/399c9529/attachment.html>

From oleksandr.otenko at oracle.com  Mon Jan 19 11:02:27 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 19 Jan 2015 16:02:27 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D89ED7@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B7C015.4030804@oracle.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D89ED7@sm-ex-01-vm.guidewire.com>
Message-ID: <54BD2A93.3020808@oracle.com>

What I meant, you still have volatile load first, volatile store last, 
and an atomic with no fences between them. This atomic won't add 
stronger ordering.

Alex

On 18/01/2015 01:42, Justin Sampson wrote:
> Oleksandr Otenko wrote:
>
>> For example, what would you say if some hardware offered atomicity
>> of update of just the permit, and offer no barrier semantics for
>> the rest of the accesses around it?
> I think we've already agreed that park() and unpark() must at least
> prevent reordering nearby _volatile_ accesses with the handling of
> the permit, haven't we?
>
> Cheers,
> Justin


From akarnokd at gmail.com  Mon Jan 19 11:52:45 2015
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Mon, 19 Jan 2015 17:52:45 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BD2585.5040903@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
	<54BD0ED7.50907@cs.oswego.edu>
	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
	<54BD2585.5040903@cs.oswego.edu>
Message-ID: <CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>

I was hoping for something like this:

class OneShotPublisher implements Publisher<Boolean> {
   final ExecutorService executor = ForkJoinPool.commonPool();
   public void subscribe(Subscriber<? super Boolean> subscriber) {
       subscriber.onSubscribe(new OneShotSubscription(subscriber,
executor));
   }
   static class OneShotSubscription implements Subscription {
     final Subscriber<? super Boolean> subscriber;
     final ExecutorService executor;
     boolean completed;
     Future<?> future;
     OneShotSubscription(Subscriber<? super Boolean> subscriber,
                         ExecutorService executor) {
       this.subscriber = subscriber;
       this.executor = executor;
     }
     public synchronized void request(long n) {
       if (n > 0 && !completed) {
         completed = true;
         future = executor.submit(() -> {
                   subscriber.onNext(Boolean.TRUE);
                   subscriber.onComplete();
               });
       }
       else if (n < 0) {
         completed = true;
         subscriber.onError(new IllegalArgumentException());
         }
     }
     public synchronized void cancel() { completed = true;
        if (future != null) future.cancel(true);
     }
   }
 }

I have a further question: Why is subscriber.onError synchronous in the
example? I'd think if subscriber expects its value and termination on an
Executor, it might want to get any exception also there.

> Yes, that's one of the main reasons for adding j.u.c support.

So perhaps there is room for some abstract publisher support? In that case,
I'd like to pledge two helper classes: AbstractOnSubscribe and
BackpressureDrainManager.

The former could be an adapted version of the RxJava's AbstractOnSubscribe (
https://github.com/ReactiveX/RxJava/blob/1.x/src/main/java/rx/observables/AbstractOnSubscribe.java):
it let's the developer implement a Publisher one onNext at a time and with
full backpressure & cancellation awareness.

The second abstract class itself is still awaiting review, but it is aimed
at coordinating when buffering/queueing is involved between upstream and
downstream (
https://github.com/akarnokd/RxJava/blob/OnBackpressureBlockFix/src/main/java/rx/internal/util/BackpressureDrainManager.java
)

2015-01-19 16:40 GMT+01:00 Doug Lea <dl at cs.oswego.edu>:

> On 01/19/2015 09:54 AM, D?vid Karnok wrote:
>
>>
>>
>  I know this is supposed to be a simple example, but it has a few problems
>> and
>> may convey a false sense of easiness:
>>
>
> Thanks. The intent was to convey a true sense of un-easiness of anything
> beyond a one-shot :-) Further suggestions welcome.
>
>
>> - the executor created is never shut down so it will keep the JVM from
>> quitting,
>>
>
> Thanks. Replaced with
>  *   final Executor executor = ForkJoinPool.commonPool(); // daemon-based
>
>  - there might be only a small window to cancel the subscription before the
>> request is issued, and
>>
>
> Thanks I added note that if cancel() follows request(), it is ineffective.
>
> This is OK according to spec, and must be OK, since cancel is intrinsically
> racy.
>
>  (Okay, the spec says cancellation is best effort, but I'd
>> hope it does more effort especially if subscriber-uncontrollable
>> resources are
>> present.)
>>
>
> Right.
>
>
>> Writing correct backpressure-aware and cancellation-supporting publishers
>> is
>> challenging
>>
>
> Yes, that's one of the main reasons for adding j.u.c support.
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/3334e91e/attachment.html>

From oleksandr.otenko at oracle.com  Mon Jan 19 11:55:22 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 19 Jan 2015 16:55:22 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421537922357-12018.post@n7.nabble.com>
References: <54B80729.7030609@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>	<54B8399F.6000106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>	<54B84055.2000701@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>	<54B928DF.80106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>	<54B98C4F.1090000@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<1421537922357-12018.post@n7.nabble.com>
Message-ID: <54BD36FA.1040202@oracle.com>

On 17/01/2015 23:38, thurstonn wrote:
> Alex's damn prescription that no-ops for both unpark and park are
> acceptable.

It is not a prescription; not an unfounded statement; not an axiom. It 
is a /consequence/.

unpark1    park1
X          Y
unpark2    park2

a:             b:             c:
unpark store   park load      park load
park load      unpark store   park store
park store     park store     unpark store


When is the implementation obliged to implement synchronization order 
(a)? Only so(unpark1,park2) when hb(X,Y), but this so does not bring 
stronger hb.

Aren't you free to always implement order (c) in all other cases?
Who can observe violation of ordering of other loads or stores, 
volatiles and not, with park load/store? Only those that 
synchronize-with park store, and those that park load synchronizes-with. 
But no one does; the thread loads only its own park stores, and those 
are in program order anyway. So if park() can work correctly without 
observing unpark store, it doesn't need to load or store. So those parks 
that never block indefinitely, don't need to have the park load. Then if 
no one loads, no one needs to store. /Hence/ no-op park is valid, and 
when park is no-op, no-op unpark is also valid.

Alex
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/0bc01ee8/attachment.html>

From kasperni at gmail.com  Mon Jan 19 13:31:07 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Mon, 19 Jan 2015 19:31:07 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BD2007.3090907@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr> <54BD2007.3090907@cs.oswego.edu>
Message-ID: <CAPs6150wqr7Xjfriv5mvRTio1_fYbzjPkq9kYrULS=mKQwsvPg@mail.gmail.com>

>
> However, the use of a "write once" variable that cannot be declared
> as "final" looks uncomfortable to concurrent programmers. As
> Viktor notes, some usages avoid this. But it might be nice to finally
> introduce @WriteOnce so that tools could help make sure you don't
> re-assign in usages like:
>
>
again, why not let Flow.Subscription subscription be a parameter on each
method.

    class MySubscriber<T> implements Flow.Subscriber<T> {
        MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
        public void onSubscribe(Flow.Subscription subscription) {
           subscription.request(1);
        }
        public void onNext(Flow.Subscription subscription, S item) {
            subscription.request(1);
            // ... process item ...
        }
        public void onError(Flow.Subscription subscription, Throwable ex) {
ex.printStackTrace(); }
        public void onComplete(Flow.Subscription subscription) { ... }

instead of



>
>     class MySubscriber<T> implements Flow.Subscriber<T> {
>         @WriteOnce Flow.Subscription subscription;
>         MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
>         public void onSubscribe(Flow.Subscription subscription) {
>             (this.subscription = subscription).request(1);
>         }
>         public void onNext(S item) {
>             subscription.request(1);
>             // ... process item ...
>         }
>         public void onError(Throwable ex) { ex.printStackTrace(); }
>         public void onComplete() { ... }
>
>     }
>

it might not be pretty, but its prettier than the alternative.

- Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/6935f28b/attachment.html>

From dl at cs.oswego.edu  Mon Jan 19 15:38:17 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Jan 2015 15:38:17 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAPs6150wqr7Xjfriv5mvRTio1_fYbzjPkq9kYrULS=mKQwsvPg@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>	<54BD2007.3090907@cs.oswego.edu>
	<CAPs6150wqr7Xjfriv5mvRTio1_fYbzjPkq9kYrULS=mKQwsvPg@mail.gmail.com>
Message-ID: <54BD6B39.9060901@cs.oswego.edu>

On 01/19/2015 01:31 PM, Kasper Nielsen wrote:
>     However, the use of a "write once" variable that cannot be declared
>     as "final" looks uncomfortable to concurrent programmers. As
>     Viktor notes, some usages avoid this. But it might be nice to finally
>     introduce @WriteOnce so that tools could help make sure you don't
>     re-assign in usages like:
>
> again, why not let Flow.Subscription subscription be a parameter on each method.
>

It's not a bad idea, but saving some people a bit of discomfort
doesn't seem like a winning argument to the broader community :-)

-Doug


>      class MySubscriber<T> implements Flow.Subscriber<T> {
>          MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
>          public void onSubscribe(Flow.Subscription subscription) {
>             subscription.request(1);
>          }
>          public void onNext(Flow.Subscription subscription, S item) {
>              subscription.request(1);
>              // ... process item ...
>          }
>          public void onError(Flow.Subscription subscription, Throwable ex) {
> ex.printStackTrace(); }
>          public void onComplete(Flow.Subscription subscription) { ... }
>
> instead of
>
>
>          class MySubscriber<T> implements Flow.Subscriber<T> {
>              @WriteOnce Flow.Subscription subscription;
>              MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
>              public void onSubscribe(Flow.Subscription subscription) {
>                  (this.subscription = subscription).request(1);
>              }
>              public void onNext(S item) {
>                  subscription.request(1);
>                  // ... process item ...
>              }
>              public void onError(Throwable ex) { ex.printStackTrace(); }
>              public void onComplete() { ... }
>
>          }
>
>
> it might not be pretty, but its prettier than the alternative.
>
> - Kasper
>
>


From dl at cs.oswego.edu  Mon Jan 19 16:19:03 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 19 Jan 2015 16:19:03 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>	<54BD0ED7.50907@cs.oswego.edu>	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>	<54BD2585.5040903@cs.oswego.edu>
	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
Message-ID: <54BD74C7.3020500@cs.oswego.edu>

On 01/19/2015 11:52 AM, D?vid Karnok wrote:
> I was hoping for something like this:
>          if (future != null) future.cancel(true);

OK, added in current updates
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/Flow.html
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html

>
> I have a further question: Why is subscriber.onError synchronous in the example?

Because if it is the only message you get, it might as well be
synchronous. But is an unusual circumstance, so I changed example to

       if (n != 0 && !completed) {
          completed = true;
          future = executor.submit(() -> {
            if (n < 0)
              subscriber.onError(new IllegalArgumentException());
            else {
              subscriber.onNext(Boolean.TRUE);
              subscriber.onComplete();
            }});
        }

>
>>Yes, that's one of the main reasons for adding j.u.c support.
>
> So perhaps there is room for some abstract publisher support?

The intent of SubmissionPublisher is to both be an adaptor
for non-Flow generators, as well as an efficient and convenient base
class or component usable for most  Publishers and Processors
(although Async-IO-based ones might need better alternatives).
This takes a different approach than your ...

> two helper classes: AbstractOnSubscribe and BackpressureDrainManager.

... but it is worth contemplating whether/how to support this
functionality.


-Doug





From davidcholmes at aapt.net.au  Mon Jan 19 17:37:28 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Jan 2015 08:37:28 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <54BD265E.8080702@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMELEKMAA.davidcholmes@aapt.net.au>

In hotspot all the Atomic read-modify-write operations (which includes the
cmpxchg underpinning these CAS operations) are required to have the
semantics of:

fence(); <op>; membar storeload|storestore

so that the overall operation acts as a full bi-directional barrier
regardless of the success or failure of the CAS. This comes from the fact
that on x86 and SPARC the atomic primitives already imply full
bi-directional barriers (so there are no explicit fence/membar issued on
those platforms) but otherwise we want to be able to reason about the code
the same on all platforms, so if the architecture provides weaker atomics
then they must be supplemented to get the required barrier semantics.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter Levart
  Sent: Tuesday, 20 January 2015 1:44 AM
  To: Hans Boehm; Justin Sampson
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another
doc fix request)


  On 01/17/2015 01:59 AM, Hans Boehm wrote:

On Thu, Jan 15, 2015 at 5:38 PM, Justin Sampson <jsampson at guidewire.com>
wrote:
Hans Boehm wrote:

I don't think it makes sense in the memory model to say something
has volatile write semantics if nothing is actually written. The
spec could perhaps be clearer that it's not saying that. But if it
does, that's arguably a vacuous statement anyway.
>From java/util/concurrent/atomic/package-summary.html:

"compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing
volatile variables."

I don't know if this is actually true for the intrinsic CAS -- if
you think it's not, or shouldn't be, then it definitely needs to be
rewritten. :) But it's _definitely_ not true for the compareAndSet
on AtomicStampedReference, which only has the memory effects of
writing a volatile variable if it actually writes a different value
than was there before (and it can fail spuriously).
It's not well-written.  But I'm not that concerned if it says it has "the
memory effects of both reading and writing volatile variables", and the
description then states that in some cases it doesn't write the underlying
object at all, as I think the definition of CAS does.  Clearly the
nonexistent write can't be volatile.  If it were to make a volatile write
to some other unspecified location, that wouldn't be observable.  This
seems like a minor editorial issue to me.


  Perhaps the intention is to specify that compiler must treat CAS as a
volatile read+write regarding reordering of surrounding reads and writes
regardless of whether CAS is successful or not (which compiler can't know).

  It's interesting that unsuccessful CAS (at least on Java and Intel i7
Sandy Bridge) takes exactly the same time to execute as successful. For
example, the following JMH benchmark:

  @BenchmarkMode(Mode.AverageTime)
  @Fork(1)
  @Warmup(iterations = 5)
  @Measurement(iterations = 10)
  @OutputTimeUnit(TimeUnit.NANOSECONDS)
  public class AtomicBench {

      static final AtomicInteger i = new AtomicInteger();

      @Benchmark
      public int get() {
          return i.get();
      }

      @Benchmark
      public void set() {
          i.set(123);
      }

      @Benchmark
      public int setGet() {
          i.set(123);
          return i.get();
      }

      @Benchmark
      public boolean casSuccess() {
          return i.compareAndSet(0, 0);
      }

      @Benchmark
      public boolean casFail() {
          return i.compareAndSet(42, 43);
      }
  }


  Prints the following:

  Benchmark                     Mode  Samples  Score   Error  Units
  j.t.AtomicBench.casFail       avgt       10  7.246 ? 0.255  ns/op
  j.t.AtomicBench.casSuccess    avgt       10  7.168 ? 0.283  ns/op
  j.t.AtomicBench.get           avgt       10  2.128 ? 0.055  ns/op
  j.t.AtomicBench.set           avgt       10  5.506 ? 0.203  ns/op
  j.t.AtomicBench.setGet        avgt       10  9.370 ? 0.072  ns/op



  So if unsuccessful CAS is not a volatile write, why does it seem to take
the same time as successful one?

  If the benchmark tells the truth, would pre-screening a CAS with a
volatile read from the same location help if we anticipate failure
frequently?


  Regards, Peter



...

Out of curiosity, is a weakCompareAndSet at least considered a
synchronization action, part of the overall synchronization order,
such that it can't be reordered with any nearby volatile reads or
writes? Or is it truly free-for-all?
The current Java memory model doesn't cover either weakCompareAndSet() or
lazySet().  They weren't around when the memory model work was done.  My
assumption is that weakCompareAndSet is similar to
compare_exchange_weak(..., memory_order_relaxed) in C++, but that leaves
open some questions about cache coherence, I think.  My assumption is that
they do not participate in a single total synchronization order.  (lazySet
operations already can't.)

Hans




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/9950444b/attachment-0001.html>

From jsampson at guidewire.com  Mon Jan 19 20:24:47 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 20 Jan 2015 01:24:47 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B714E2.7020004@oracle.com>
	<CAHjP37E9Z+1RYJvrH1HRJwEt3sSy4is2ZfnZ=Fu2=5BFBNjgQA@mail.gmail.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>

Okay, Doug -- here's one last proposal. I think the wording that
you've added about reading/writing of a volatile by park/unpark is
both (a) too strong and (b) misleading. :)

The practical thing that needs to be reaffirmed is that the caller
of park/unpark is responsible for making the associated state
variable volatile themselves. That's true regardless of all the
rest. If someone reads that wording and thinks it means they don't
have to make their state variable volatile, then it has indeed done
more harm than good.

So here's my final(?) proposed wording, to replace the sentence
you've added:

"The caller of park and unpark can rely on those calls not being
reordered with any volatile reads or writes in the calling thread.
However, the same promise cannot be made regarding non-volatile
reads or writes. Therefore, the caller should ensure that any state
variables being used in conjunction with park and unpark are
themselves volatile."

Cheers,
Justin

P.S. That paper was a great read, thanks for the link!

P.P.S. I'm enjoying quite a bit of reordering of the messages on
this thread, and haven't even received Doug's message that I'm
replying to. Email does not exhibit sequential consistency. :)


From boehm at acm.org  Mon Jan 19 23:52:48 2015
From: boehm at acm.org (Hans Boehm)
Date: Mon, 19 Jan 2015 20:52:48 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMELEKMAA.davidcholmes@aapt.net.au>
References: <54BD265E.8080702@gmail.com>
	<NFBBKALFDCPFIDBNKAPCMELEKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1Z_R2YGJGT9fw50TmGZyVBW8NFA9qv4zM7toErfa018Rg@mail.gmail.com>

On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:
>
> In hotspot all the Atomic read-modify-write operations (which includes
the cmpxchg underpinning these CAS operations) are required to have the
semantics of:
>
> fence(); <op>; membar storeload|storestore
>
> so that the overall operation acts as a full bi-directional barrier
regardless of the success or failure of the CAS. This comes from the fact
that on x86 and SPARC the atomic primitives already imply full
bi-directional barriers (so there are no explicit fence/membar issued on
those platforms) but otherwise we want to be able to reason about the code
the same on all platforms, so if the architecture provides weaker atomics
then they must be supplemented to get the required barrier semantics.
>
> David

That seems to me to be at odds with some of the original Java memory model
goals, notably the idea that you should be able to remove any fences or
atomicity overhead from synchronization objects accessed by only a local
thread.  It also seems to significantly disadvantage ARMv8 for very minimal
benefit by insisting that CAS operations can be abused as a full fence
purely to order racing operations before and after the fence.  It adds
overhead purely to support incorrectly synchronized/racy programs.  This is
also a property that's currently quite difficult to define in the memory
model.  In short, I think this is a mistake.

On ARMv8, the natural implementation should be a load-exclusive-acquire,
followed by a store-exclusive-release, which does not order prior and
subsequent non-volatile operations.  It should however suffice for a C++
sequentially consistent compare_exchange, since only racy programs or
programs with relaxed atomics can tell the difference.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/837ada26/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 20 00:21:57 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Jan 2015 15:21:57 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1Z_R2YGJGT9fw50TmGZyVBW8NFA9qv4zM7toErfa018Rg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>

Hi Hans,

I don't see how this is at odds with any JMM goals as if you remove the atomic op you would also remove any additional barriers.

These implementations are for internal hotspot use as well as being the back end of the j.u.c.atomic operations. If this is a serious impediment on some platforms then a separate j.u.c.atomic back end could be defined - but then it would have to add back in the barriers related to the volatile read/write accesses implied - which I think would get you very close to where we are now. Your description for ARMv8 below is not providing the semantics of the Java-level volatile read+write AFAICS.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
  Sent: Tuesday, 20 January 2015 2:53 PM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)



  On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
  >
  > In hotspot all the Atomic read-modify-write operations (which includes the cmpxchg underpinning these CAS operations) are required to have the semantics of:
  >  
  > fence(); <op>; membar storeload|storestore
  >  
  > so that the overall operation acts as a full bi-directional barrier regardless of the success or failure of the CAS. This comes from the fact that on x86 and SPARC the atomic primitives already imply full bi-directional barriers (so there are no explicit fence/membar issued on those platforms) but otherwise we want to be able to reason about the code the same on all platforms, so if the architecture provides weaker atomics then they must be supplemented to get the required barrier semantics.
  >  
  > David


  That seems to me to be at odds with some of the original Java memory model goals, notably the idea that you should be able to remove any fences or atomicity overhead from synchronization objects accessed by only a local thread.  It also seems to significantly disadvantage ARMv8 for very minimal benefit by insisting that CAS operations can be abused as a full fence purely to order racing operations before and after the fence.  It adds overhead purely to support incorrectly synchronized/racy programs.  This is also a property that's currently quite difficult to define in the memory model.  In short, I think this is a mistake.


  On ARMv8, the natural implementation should be a load-exclusive-acquire, followed by a store-exclusive-release, which does not order prior and subsequent non-volatile operations.  It should however suffice for a C++ sequentially consistent compare_exchange, since only racy programs or programs with relaxed atomics can tell the difference.


  Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/aaf7e09e/attachment.html>

From boehm at acm.org  Tue Jan 20 00:41:12 2015
From: boehm at acm.org (Hans Boehm)
Date: Mon, 19 Jan 2015 21:41:12 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1Z_R2YGJGT9fw50TmGZyVBW8NFA9qv4zM7toErfa018Rg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1bb_c4VDPdEw8MsqRAz21+usfqo+=VyO4nK03bO=Y3Rzg@mail.gmail.com>

I think the core question here is if I can implement the Dekker's example
with something like the following in each thread, where x and y are not
volatile:

Thread 1:
x = 1;
local1.CAS(...)
r1 = y;

Thread 2:
y = 1;
local2.CAS(....)
r1 = x;

i.e. if I can use CAS on a local as a fence replacement.

If this is intended to work, as implied by the posted description, then the
implementation is not allowed to remove the fence(s) associated with e.g.
local1.CAS, even if local1 is only used by thread1.  That would be quite
unfortunate.

The canonical ARMv8 CAS implementations should guarantee the volatile load
semantics for the CAS, and volatile store semantics for a successful CAS
(which I think is the only reasonable way to read the current j.u.c.
spec).  But I believe it is not usable as a fence, as in this example.

Hans

On Mon, Jan 19, 2015 at 9:21 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Hi Hans,
>
> I don't see how this is at odds with any JMM goals as if you remove the
> atomic op you would also remove any additional barriers.
>
> These implementations are for internal hotspot use as well as being the
> back end of the j.u.c.atomic operations. If this is a serious impediment on
> some platforms then a separate j.u.c.atomic back end could be defined - but
> then it would have to add back in the barriers related to the volatile
> read/write accesses implied - which I think would get you very close to
> where we are now. Your description for ARMv8 below is not providing the
> semantics of the Java-level volatile read+write AFAICS.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
> *Sent:* Tuesday, 20 January 2015 2:53 PM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
>
> On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
> >
> > In hotspot all the Atomic read-modify-write operations (which includes
> the cmpxchg underpinning these CAS operations) are required to have the
> semantics of:
> >
> > fence(); <op>; membar storeload|storestore
> >
> > so that the overall operation acts as a full bi-directional barrier
> regardless of the success or failure of the CAS. This comes from the fact
> that on x86 and SPARC the atomic primitives already imply full
> bi-directional barriers (so there are no explicit fence/membar issued on
> those platforms) but otherwise we want to be able to reason about the code
> the same on all platforms, so if the architecture provides weaker atomics
> then they must be supplemented to get the required barrier semantics.
> >
> > David
>
> That seems to me to be at odds with some of the original Java memory model
> goals, notably the idea that you should be able to remove any fences or
> atomicity overhead from synchronization objects accessed by only a local
> thread.  It also seems to significantly disadvantage ARMv8 for very minimal
> benefit by insisting that CAS operations can be abused as a full fence
> purely to order racing operations before and after the fence.  It adds
> overhead purely to support incorrectly synchronized/racy programs.  This is
> also a property that's currently quite difficult to define in the memory
> model.  In short, I think this is a mistake.
>
> On ARMv8, the natural implementation should be a load-exclusive-acquire,
> followed by a store-exclusive-release, which does not order prior and
> subsequent non-volatile operations.  It should however suffice for a C++
> sequentially consistent compare_exchange, since only racy programs or
> programs with relaxed atomics can tell the difference.
>
> Hans
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150119/2ba2f3dd/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Jan 20 01:10:27 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Jan 2015 16:10:27 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1bb_c4VDPdEw8MsqRAz21+usfqo+=VyO4nK03bO=Y3Rzg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIELHKMAA.davidcholmes@aapt.net.au>

Sorry Hans I've lost the context here - what "posted description" are you referring to? What I was describing were internal hotspot details not anything that can be relied upon at the Java level.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
  Sent: Tuesday, 20 January 2015 3:41 PM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  I think the core question here is if I can implement the Dekker's example with something like the following in each thread, where x and y are not volatile:


  Thread 1:
  x = 1;
  local1.CAS(...)
  r1 = y;


  Thread 2:
  y = 1;
  local2.CAS(....)
  r1 = x;


  i.e. if I can use CAS on a local as a fence replacement.


  If this is intended to work, as implied by the posted description, then the implementation is not allowed to remove the fence(s) associated with e.g. local1.CAS, even if local1 is only used by thread1.  That would be quite unfortunate.


  The canonical ARMv8 CAS implementations should guarantee the volatile load semantics for the CAS, and volatile store semantics for a successful CAS (which I think is the only reasonable way to read the current j.u.c. spec).  But I believe it is not usable as a fence, as in this example. 


  Hans


  On Mon, Jan 19, 2015 at 9:21 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Hi Hans,

    I don't see how this is at odds with any JMM goals as if you remove the atomic op you would also remove any additional barriers.

    These implementations are for internal hotspot use as well as being the back end of the j.u.c.atomic operations. If this is a serious impediment on some platforms then a separate j.u.c.atomic back end could be defined - but then it would have to add back in the barriers related to the volatile read/write accesses implied - which I think would get you very close to where we are now. Your description for ARMv8 below is not providing the semantics of the Java-level volatile read+write AFAICS.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
      Sent: Tuesday, 20 January 2015 2:53 PM
      To: David Holmes
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)




      On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
      >
      > In hotspot all the Atomic read-modify-write operations (which includes the cmpxchg underpinning these CAS operations) are required to have the semantics of:
      >  
      > fence(); <op>; membar storeload|storestore
      >  
      > so that the overall operation acts as a full bi-directional barrier regardless of the success or failure of the CAS. This comes from the fact that on x86 and SPARC the atomic primitives already imply full bi-directional barriers (so there are no explicit fence/membar issued on those platforms) but otherwise we want to be able to reason about the code the same on all platforms, so if the architecture provides weaker atomics then they must be supplemented to get the required barrier semantics.
      >  
      > David


      That seems to me to be at odds with some of the original Java memory model goals, notably the idea that you should be able to remove any fences or atomicity overhead from synchronization objects accessed by only a local thread.  It also seems to significantly disadvantage ARMv8 for very minimal benefit by insisting that CAS operations can be abused as a full fence purely to order racing operations before and after the fence.  It adds overhead purely to support incorrectly synchronized/racy programs.  This is also a property that's currently quite difficult to define in the memory model.  In short, I think this is a mistake.


      On ARMv8, the natural implementation should be a load-exclusive-acquire, followed by a store-exclusive-release, which does not order prior and subsequent non-volatile operations.  It should however suffice for a C++ sequentially consistent compare_exchange, since only racy programs or programs with relaxed atomics can tell the difference.


      Hans

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/a29d6165/attachment.html>

From peter.levart at gmail.com  Tue Jan 20 04:38:11 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 20 Jan 2015 10:38:11 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1bb_c4VDPdEw8MsqRAz21+usfqo+=VyO4nK03bO=Y3Rzg@mail.gmail.com>
References: <CAPUmR1Z_R2YGJGT9fw50TmGZyVBW8NFA9qv4zM7toErfa018Rg@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1bb_c4VDPdEw8MsqRAz21+usfqo+=VyO4nK03bO=Y3Rzg@mail.gmail.com>
Message-ID: <54BE2203.4020007@gmail.com>

On 01/20/2015 06:41 AM, Hans Boehm wrote:
> I think the core question here is if I can implement the Dekker's example
> with something like the following in each thread, where x and y are not
> volatile:
>
> Thread 1:
> x = 1;
> local1.CAS(...)
> r1 = y;
>
> Thread 2:
> y = 1;
> local2.CAS(....)
> r1 = x;
>
> i.e. if I can use CAS on a local as a fence replacement.
>
> If this is intended to work, as implied by the posted description, then the
> implementation is not allowed to remove the fence(s) associated with e.g.
> local1.CAS, even if local1 is only used by thread1.  That would be quite
> unfortunate.

Right. So we can view Java CAS as equivalent to:

synchronized(lock) {
     if (value == expected) {
         value = newValue;
         return true;
     } else {
         return false;
     }
}

...where 'lock' is an Object uniquely associated with the memory 
location being CASed, meaning that synchronized(lock) {} can be elided 
where appropriate (for example, if compiler can prove that memory 
location can only be accessed by single thread).

Isn't this actually the way how 64bit CAS is (or was?) implemented on 
some architectures?

Peter

>
> The canonical ARMv8 CAS implementations should guarantee the volatile load
> semantics for the CAS, and volatile store semantics for a successful CAS
> (which I think is the only reasonable way to read the current j.u.c.
> spec).  But I believe it is not usable as a fence, as in this example.
>
> Hans
>
> On Mon, Jan 19, 2015 at 9:21 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>   Hi Hans,
>>
>> I don't see how this is at odds with any JMM goals as if you remove the
>> atomic op you would also remove any additional barriers.
>>
>> These implementations are for internal hotspot use as well as being the
>> back end of the j.u.c.atomic operations. If this is a serious impediment on
>> some platforms then a separate j.u.c.atomic back end could be defined - but
>> then it would have to add back in the barriers related to the volatile
>> read/write accesses implied - which I think would get you very close to
>> where we are now. Your description for ARMv8 below is not providing the
>> semantics of the Java-level volatile read+write AFAICS.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>> *Sent:* Tuesday, 20 January 2015 2:53 PM
>> *To:* David Holmes
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
>> doc fix request)
>>
>>
>> On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>> In hotspot all the Atomic read-modify-write operations (which includes
>> the cmpxchg underpinning these CAS operations) are required to have the
>> semantics of:
>>> fence(); <op>; membar storeload|storestore
>>>
>>> so that the overall operation acts as a full bi-directional barrier
>> regardless of the success or failure of the CAS. This comes from the fact
>> that on x86 and SPARC the atomic primitives already imply full
>> bi-directional barriers (so there are no explicit fence/membar issued on
>> those platforms) but otherwise we want to be able to reason about the code
>> the same on all platforms, so if the architecture provides weaker atomics
>> then they must be supplemented to get the required barrier semantics.
>>> David
>> That seems to me to be at odds with some of the original Java memory model
>> goals, notably the idea that you should be able to remove any fences or
>> atomicity overhead from synchronization objects accessed by only a local
>> thread.  It also seems to significantly disadvantage ARMv8 for very minimal
>> benefit by insisting that CAS operations can be abused as a full fence
>> purely to order racing operations before and after the fence.  It adds
>> overhead purely to support incorrectly synchronized/racy programs.  This is
>> also a property that's currently quite difficult to define in the memory
>> model.  In short, I think this is a mistake.
>>
>> On ARMv8, the natural implementation should be a load-exclusive-acquire,
>> followed by a store-exclusive-release, which does not order prior and
>> subsequent non-volatile operations.  It should however suffice for a C++
>> sequentially consistent compare_exchange, since only racy programs or
>> programs with relaxed atomics can tell the difference.
>>
>> Hans
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/eed0606b/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 20 04:42:27 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 20 Jan 2015 19:42:27 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <54BE2203.4020007@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKELIKMAA.davidcholmes@aapt.net.au>

There is provision for 64-bit CAS to be implemented using locks on 32-bit
platforms that don't support a 64-bit CAS eg PPC32.

David
  -----Original Message-----
  From: Peter Levart [mailto:peter.levart at gmail.com]
  Sent: Tuesday, 20 January 2015 7:38 PM
  To: Hans Boehm; David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another
doc fix request)


  On 01/20/2015 06:41 AM, Hans Boehm wrote:

I think the core question here is if I can implement the Dekker's example
with something like the following in each thread, where x and y are not
volatile:

Thread 1:
x = 1;
local1.CAS(...)
r1 = y;

Thread 2:
y = 1;
local2.CAS(....)
r1 = x;

i.e. if I can use CAS on a local as a fence replacement.

If this is intended to work, as implied by the posted description, then the
implementation is not allowed to remove the fence(s) associated with e.g.
local1.CAS, even if local1 is only used by thread1.  That would be quite
unfortunate.
  Right. So we can view Java CAS as equivalent to:

  synchronized(lock) {
      if (value == expected) {
          value = newValue;
          return true;
      } else {
          return false;
      }
  }

  ...where 'lock' is an Object uniquely associated with the memory location
being CASed, meaning that synchronized(lock) {} can be elided where
appropriate (for example, if compiler can prove that memory location can
only be accessed by single thread).

  Isn't this actually the way how 64bit CAS is (or was?) implemented on some
architectures?

  Peter



The canonical ARMv8 CAS implementations should guarantee the volatile load
semantics for the CAS, and volatile store semantics for a successful CAS
(which I think is the only reasonable way to read the current j.u.c.
spec).  But I believe it is not usable as a fence, as in this example.

Hans

On Mon, Jan 19, 2015 at 9:21 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

 Hi Hans,

I don't see how this is at odds with any JMM goals as if you remove the
atomic op you would also remove any additional barriers.

These implementations are for internal hotspot use as well as being the
back end of the j.u.c.atomic operations. If this is a serious impediment on
some platforms then a separate j.u.c.atomic back end could be defined - but
then it would have to add back in the barriers related to the volatile
read/write accesses implied - which I think would get you very close to
where we are now. Your description for ARMv8 below is not providing the
semantics of the Java-level volatile read+write AFAICS.

David

-----Original Message-----
*From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
*Sent:* Tuesday, 20 January 2015 2:53 PM
*To:* David Holmes
*Cc:* concurrency-interest at cs.oswego.edu
*Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
doc fix request)


On Mon, Jan 19, 2015 at 2:37 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:
In hotspot all the Atomic read-modify-write operations (which includes
the cmpxchg underpinning these CAS operations) are required to have the
semantics of:
fence(); <op>; membar storeload|storestore

so that the overall operation acts as a full bi-directional barrier
regardless of the success or failure of the CAS. This comes from the fact
that on x86 and SPARC the atomic primitives already imply full
bi-directional barriers (so there are no explicit fence/membar issued on
those platforms) but otherwise we want to be able to reason about the code
the same on all platforms, so if the architecture provides weaker atomics
then they must be supplemented to get the required barrier semantics.
David
That seems to me to be at odds with some of the original Java memory model
goals, notably the idea that you should be able to remove any fences or
atomicity overhead from synchronization objects accessed by only a local
thread.  It also seems to significantly disadvantage ARMv8 for very minimal
benefit by insisting that CAS operations can be abused as a full fence
purely to order racing operations before and after the fence.  It adds
overhead purely to support incorrectly synchronized/racy programs.  This is
also a property that's currently quite difficult to define in the memory
model.  In short, I think this is a mistake.

On ARMv8, the natural implementation should be a load-exclusive-acquire,
followed by a store-exclusive-release, which does not order prior and
subsequent non-volatile operations.  It should however suffice for a C++
sequentially consistent compare_exchange, since only racy programs or
programs with relaxed atomics can tell the difference.

Hans





_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/f805d373/attachment-0001.html>

From aph at redhat.com  Tue Jan 20 04:44:14 2015
From: aph at redhat.com (Andrew Haley)
Date: Tue, 20 Jan 2015 09:44:14 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
Message-ID: <54BE236E.2070208@redhat.com>

On 20/01/15 05:21, David Holmes wrote:

> Your description for ARMv8 below is not providing the semantics of
> the Java-level volatile read+write AFAICS.

Yes it does: ARMv8 load-exclusive-acquire and store-exclusive-release
are designed to provide the semantics of the Java-level volatile
read+write.  At the present time we have to use barriers that are
stronger than necessary because of the way that HotSpot works
internally, but I do intend to try to fix that.

Andrew.

From viktor.klang at gmail.com  Tue Jan 20 05:02:26 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 20 Jan 2015 11:02:26 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAPs6150wqr7Xjfriv5mvRTio1_fYbzjPkq9kYrULS=mKQwsvPg@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr> <54BD2007.3090907@cs.oswego.edu>
	<CAPs6150wqr7Xjfriv5mvRTio1_fYbzjPkq9kYrULS=mKQwsvPg@mail.gmail.com>
Message-ID: <CANPzfU8oHu8k=gfZ5o85ozYjo6o9sJ7QBmfDQTjLmB=SH7OjAw@mail.gmail.com>

Hi Kasper!

On Mon, Jan 19, 2015 at 7:31 PM, Kasper Nielsen <kasperni at gmail.com> wrote:

> However, the use of a "write once" variable that cannot be declared
>> as "final" looks uncomfortable to concurrent programmers. As
>> Viktor notes, some usages avoid this. But it might be nice to finally
>> introduce @WriteOnce so that tools could help make sure you don't
>> re-assign in usages like:
>>
>>
> again, why not let Flow.Subscription subscription be a parameter on each
> method.
>

The reason for not doing this (we discussed this exact proposal in the RS
group about a year ago, IIRC it was me who raised it, so you are in good
company) is that if you think about it from the distributed perspective,
you'd have to send the identifier of the Subscription for every single
element that gets passed downstream, this is just waste, and does not solve
anything since Subscribers are both inherently stateful and virtually
always effectful.


>
>     class MySubscriber<T> implements Flow.Subscriber<T> {
>         MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
>         public void onSubscribe(Flow.Subscription subscription) {
>            subscription.request(1);
>         }
>         public void onNext(Flow.Subscription subscription, S item) {
>             subscription.request(1);
>             // ... process item ...
>         }
>         public void onError(Flow.Subscription subscription, Throwable ex)
> { ex.printStackTrace(); }
>         public void onComplete(Flow.Subscription subscription) { ... }
>
> instead of
>
>
>
>>
>>     class MySubscriber<T> implements Flow.Subscriber<T> {
>>         @WriteOnce Flow.Subscription subscription;
>>         MySubscriber(Flow.Publisher<T> pub) { pub.subscribe(this); }
>>         public void onSubscribe(Flow.Subscription subscription) {
>>             (this.subscription = subscription).request(1);
>>         }
>>         public void onNext(S item) {
>>             subscription.request(1);
>>             // ... process item ...
>>         }
>>         public void onError(Throwable ex) { ex.printStackTrace(); }
>>         public void onComplete() { ... }
>>
>>     }
>>
>
> it might not be pretty, but its prettier than the alternative.
>

Fortunately you can do this yourself if you want to:

public interface MySubscriber<T> {
  void onSubscribe(Subscription s);
  void onNext(Subscription s, T elem);
  void onError(Throwable error);  // No point in passing the Subscription
here since it is not valid anymore at this point
  void onComplete(); // No point in passing the Subscription here since it
is not valid anymore at this point
}

public final class MySubscriberAdapter<T> implements Subscriber<T> {
  private Subscription s;
  private final MySubscriber ms;
  public AStatelessSubscriber<T>(final MySubscriber ms) {
    if (ms == null) throw null;
    this.ms = ms;
  }

  @Override def onSubscribe(final Subscription s) {
    this.s = s;
    ms.onSubscribe(s);
  }

  @Override def onNext(final T elem) {
    ms.onNext(s, elem);
  }

  @Override def onError(final Throwable error) {
    s = null;
    ms.onError(error);
  }

  @Override def onComplete() {
    s = null;
    ms.onComplete();
  }
}


>
> - Kasper
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/cdc1a7b2/attachment.html>

From vitalyd at gmail.com  Tue Jan 20 09:24:36 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 09:24:36 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54BE236E.2070208@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
Message-ID: <CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>

My suspicion is that most people using CAS expect that other non-volatile
read/writes aren't reordered with the CAS in the same way that a volatile
read+write wouldn't permit that.  Hans was saying that on ARMv8 you'd want
to use instructions that don't preclude other non-volatile accesses from
being reordered, if I'm not mistaken.  I'm not seeing how that's equivalent?

sent from my phone
On Jan 20, 2015 5:16 AM, "Andrew Haley" <aph at redhat.com> wrote:

> On 20/01/15 05:21, David Holmes wrote:
>
> > Your description for ARMv8 below is not providing the semantics of
> > the Java-level volatile read+write AFAICS.
>
> Yes it does: ARMv8 load-exclusive-acquire and store-exclusive-release
> are designed to provide the semantics of the Java-level volatile
> read+write.  At the present time we have to use barriers that are
> stronger than necessary because of the way that HotSpot works
> internally, but I do intend to try to fix that.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/56c2ead1/attachment.html>

From aph at redhat.com  Tue Jan 20 10:06:29 2015
From: aph at redhat.com (Andrew Haley)
Date: Tue, 20 Jan 2015 15:06:29 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
Message-ID: <54BE6EF5.8080600@redhat.com>

On 01/20/2015 02:24 PM, Vitaly Davidovich wrote:
> My suspicion is that most people using CAS expect that other non-volatile
> read/writes aren't reordered with the CAS in the same way that a volatile
> read+write wouldn't permit that. 

I agree.

> Hans was saying that on ARMv8 you'd want to use instructions that
> don't preclude other non-volatile accesses from being reordered, if
> I'm not mistaken.  I'm not seeing how that's equivalent?

David said:

> [Hans's] description for ARMv8 below is not providing the semantics
> of the Java-level volatile read+write AFAICS.

I think that's wrong.  the ARMv8 code would provide the semantics of
the Java-level volatile read+write.

There is no doubt in my mind that ARMv8 load-exclusive-acquire and
store-exclusive-release implement semantics which are compatible with
Java's volatile.  However, the CAS is not atomic because neither
load-exclusive-acquire and store-exclusive-release prevent

  store r2 -> x
  load-exclusive-acquire(r2) -> r3
  r4 -> store-exclusive-release(r2)
  load y -> r5

from being turned into

  load-exclusive-acquire(r2) -> r3
  load y -> r5
  store r2 -> x
  r4 -> store-exclusive-release(r2)

Andrew.

From vitalyd at gmail.com  Tue Jan 20 10:26:17 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 10:26:17 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54BE6EF5.8080600@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
Message-ID: <CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>

>
> I think that's wrong.  the ARMv8 code would provide the semantics of
> the Java-level volatile read+write.


> There is no doubt in my mind that ARMv8 load-exclusive-acquire and
> store-exclusive-release implement semantics which are compatible with
> Java's volatile.  However, the CAS is not atomic because neither
> load-exclusive-acquire and store-exclusive-release prevent


>   store r2 -> x
>   load-exclusive-acquire(r2) -> r3
>   r4 -> store-exclusive-release(r2)
>   load y -> r5


> from being turned into


>   load-exclusive-acquire(r2) -> r3
>   load y -> r5
>   store r2 -> x
>   r4 -> store-exclusive-release(r2)


Ok, thanks for clarifying -- yeah, agree on that.  I guess my question is:
when the docs talk about CAS having the effects of a volatile read+write,
does that also imply atomicity or no? i.e. is it like a volatile read +
write as two back to back instructions (as if written in source like that),
or is it 1 instruction that has those reordering effects with surrounding
code.

On Tue, Jan 20, 2015 at 10:06 AM, Andrew Haley <aph at redhat.com> wrote:

> On 01/20/2015 02:24 PM, Vitaly Davidovich wrote:
> > My suspicion is that most people using CAS expect that other non-volatile
> > read/writes aren't reordered with the CAS in the same way that a volatile
> > read+write wouldn't permit that.
>
> I agree.
>
> > Hans was saying that on ARMv8 you'd want to use instructions that
> > don't preclude other non-volatile accesses from being reordered, if
> > I'm not mistaken.  I'm not seeing how that's equivalent?
>
> David said:
>
> > [Hans's] description for ARMv8 below is not providing the semantics
> > of the Java-level volatile read+write AFAICS.
>
> I think that's wrong.  the ARMv8 code would provide the semantics of
> the Java-level volatile read+write.
>
> There is no doubt in my mind that ARMv8 load-exclusive-acquire and
> store-exclusive-release implement semantics which are compatible with
> Java's volatile.  However, the CAS is not atomic because neither
> load-exclusive-acquire and store-exclusive-release prevent
>
>   store r2 -> x
>   load-exclusive-acquire(r2) -> r3
>   r4 -> store-exclusive-release(r2)
>   load y -> r5
>
> from being turned into
>
>   load-exclusive-acquire(r2) -> r3
>   load y -> r5
>   store r2 -> x
>   r4 -> store-exclusive-release(r2)
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/c4f8004f/attachment-0001.html>

From aph at redhat.com  Tue Jan 20 10:41:58 2015
From: aph at redhat.com (Andrew Haley)
Date: Tue, 20 Jan 2015 15:41:58 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>	<54BE236E.2070208@redhat.com>	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
Message-ID: <54BE7746.3000201@redhat.com>

On 01/20/2015 03:26 PM, Vitaly Davidovich wrote:
> I guess my question is:
> when the docs talk about CAS having the effects of a volatile
> read+write, does that also imply atomicity or no? i.e. is it like a
> volatile read + write as two back to back instructions (as if
> written in source like that), or is it 1 instruction that has those
> reordering effects with surrounding code.

I would have thought that when the docs talk about CAS having the
effects of a volatile read+write, that's what the docs mean: if they'd
wanted to imply the effect of an atomic instruction with a full fence
it would have been easy enough to say so.

Andrew.

From vitalyd at gmail.com  Tue Jan 20 10:45:46 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 10:45:46 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54BE7746.3000201@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
Message-ID: <CAHjP37EzoQhpef3T1_5uB+sRFkDVO+ofEDDuJGAW3qbMb6zd4Q@mail.gmail.com>

I agree that's likely the intended meaning, but for something like this, it
may be worthwhile to explicitly state that so that there's less room for
interpretation (and folks more familiar with Intel/AMD CAS semantics don't
assume that it would work the same on weaker models).

On Tue, Jan 20, 2015 at 10:41 AM, Andrew Haley <aph at redhat.com> wrote:

> On 01/20/2015 03:26 PM, Vitaly Davidovich wrote:
> > I guess my question is:
> > when the docs talk about CAS having the effects of a volatile
> > read+write, does that also imply atomicity or no? i.e. is it like a
> > volatile read + write as two back to back instructions (as if
> > written in source like that), or is it 1 instruction that has those
> > reordering effects with surrounding code.
>
> I would have thought that when the docs talk about CAS having the
> effects of a volatile read+write, that's what the docs mean: if they'd
> wanted to imply the effect of an atomic instruction with a full fence
> it would have been easy enough to say so.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/51b4a9b9/attachment.html>

From jsampson at guidewire.com  Tue Jan 20 12:36:51 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 20 Jan 2015 17:36:51 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54BE7746.3000201@redhat.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>

Andrew Haley wrote:

> I would have thought that when the docs talk about CAS having the
> effects of a volatile read+write, that's what the docs mean: if
> they'd wanted to imply the effect of an atomic instruction with a
> full fence it would have been easy enough to say so.

Huh? Of _course_ the docs say that CAS is atomic. The package is
called "atomic"; every class in it is called AtomicSomething; and
the docs for every CAS method say, "Atomically sets the value to the
given updated value if the current value == the expected value."
They shouldn't need to mention specific kinds of fences because the
JMM isn't defined in terms of fences.

Here's a practical question from the API user's perspective: Does
the reordering you described a few messages ago have any observable
effects in the behavior of a program relative to the CAS being truly
atomic?

Confused,
Justin


From boehm at acm.org  Tue Jan 20 13:14:22 2015
From: boehm at acm.org (Hans Boehm)
Date: Tue, 20 Jan 2015 10:14:22 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37EzoQhpef3T1_5uB+sRFkDVO+ofEDDuJGAW3qbMb6zd4Q@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<CAHjP37EzoQhpef3T1_5uB+sRFkDVO+ofEDDuJGAW3qbMb6zd4Q@mail.gmail.com>
Message-ID: <CAPUmR1bvL7XgeqSAhXZtxYO0HFfyc9S6Mk4cOemK4L+Sny73hQ@mail.gmail.com>

As Peter points out, probably the simplest way to look at this is to
consider the case of CAS implemented with a lock.  I believe that has
essentially the same semantics as the ARMv8 implementation:  Prior and
subsequent operations can both move into the critical section, and then be
reordered.  It doesn't act as a fence.  But it does have the semantics the
vast majority of users expect.

We can interpret CAS as

old = volatile_load();
if (old == expected) volatile_store(new);

where in addition volatile_load is constrained to see the previous value
just before the one assigned by volatile_store(new).  That's easy to
formalize, and it's essentially the C++ definition.  It allows both the
ARMv8 and lock-based implementation.

"Atomic" means that the operation doesn't store or load incompletely
modified values, and the loaded value is the one from just before the
store.

Ordering is a separable.  And the defaults, in both C++ and Java, have in
the past generally been chosen so that properly synchronized code, that
doesn't explicitly request weak ordering, looks sequentially consistent.
We have avoided specifications that are weaker than that because they're
hard to use, and avoided anything stronger than that since it constrains
the implementation in ways that don't benefit most users.  The above
definition is consistent with that guideline; requiring a full fence is not
- it's too strong in ways that everybody pays for, but that only benefit
racy programs and experts who should know better.

Hans





On Tue, Jan 20, 2015 at 7:45 AM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> I agree that's likely the intended meaning, but for something like this,
> it may be worthwhile to explicitly state that so that there's less room for
> interpretation (and folks more familiar with Intel/AMD CAS semantics don't
> assume that it would work the same on weaker models).
>
> On Tue, Jan 20, 2015 at 10:41 AM, Andrew Haley <aph at redhat.com> wrote:
>
>> On 01/20/2015 03:26 PM, Vitaly Davidovich wrote:
>> > I guess my question is:
>> > when the docs talk about CAS having the effects of a volatile
>> > read+write, does that also imply atomicity or no? i.e. is it like a
>> > volatile read + write as two back to back instructions (as if
>> > written in source like that), or is it 1 instruction that has those
>> > reordering effects with surrounding code.
>>
>> I would have thought that when the docs talk about CAS having the
>> effects of a volatile read+write, that's what the docs mean: if they'd
>> wanted to imply the effect of an atomic instruction with a full fence
>> it would have been easy enough to say so.
>>
>> Andrew.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/3adfea14/attachment.html>

From vitalyd at gmail.com  Tue Jan 20 13:32:54 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 13:32:54 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1bvL7XgeqSAhXZtxYO0HFfyc9S6Mk4cOemK4L+Sny73hQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<CAHjP37EzoQhpef3T1_5uB+sRFkDVO+ofEDDuJGAW3qbMb6zd4Q@mail.gmail.com>
	<CAPUmR1bvL7XgeqSAhXZtxYO0HFfyc9S6Mk4cOemK4L+Sny73hQ@mail.gmail.com>
Message-ID: <CAHjP37FK7JjN192MqOnn_OoBKzF=a_mUmOtBd6N9q3ZWcRtaNA@mail.gmail.com>

Sounds reasonable to me.  I do still think that it may be worthwhile to
call this out explicitly in the docs (unless it's there and I missed it) to
reduce likelihood of misinterpretation.

sent from my phone
On Jan 20, 2015 1:14 PM, "Hans Boehm" <boehm at acm.org> wrote:

> As Peter points out, probably the simplest way to look at this is to
> consider the case of CAS implemented with a lock.  I believe that has
> essentially the same semantics as the ARMv8 implementation:  Prior and
> subsequent operations can both move into the critical section, and then be
> reordered.  It doesn't act as a fence.  But it does have the semantics the
> vast majority of users expect.
>
> We can interpret CAS as
>
> old = volatile_load();
> if (old == expected) volatile_store(new);
>
> where in addition volatile_load is constrained to see the previous value
> just before the one assigned by volatile_store(new).  That's easy to
> formalize, and it's essentially the C++ definition.  It allows both the
> ARMv8 and lock-based implementation.
>
> "Atomic" means that the operation doesn't store or load incompletely
> modified values, and the loaded value is the one from just before the
> store.
>
> Ordering is a separable.  And the defaults, in both C++ and Java, have in
> the past generally been chosen so that properly synchronized code, that
> doesn't explicitly request weak ordering, looks sequentially consistent.
> We have avoided specifications that are weaker than that because they're
> hard to use, and avoided anything stronger than that since it constrains
> the implementation in ways that don't benefit most users.  The above
> definition is consistent with that guideline; requiring a full fence is not
> - it's too strong in ways that everybody pays for, but that only benefit
> racy programs and experts who should know better.
>
> Hans
>
>
>
>
>
> On Tue, Jan 20, 2015 at 7:45 AM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> I agree that's likely the intended meaning, but for something like this,
>> it may be worthwhile to explicitly state that so that there's less room for
>> interpretation (and folks more familiar with Intel/AMD CAS semantics don't
>> assume that it would work the same on weaker models).
>>
>> On Tue, Jan 20, 2015 at 10:41 AM, Andrew Haley <aph at redhat.com> wrote:
>>
>>> On 01/20/2015 03:26 PM, Vitaly Davidovich wrote:
>>> > I guess my question is:
>>> > when the docs talk about CAS having the effects of a volatile
>>> > read+write, does that also imply atomicity or no? i.e. is it like a
>>> > volatile read + write as two back to back instructions (as if
>>> > written in source like that), or is it 1 instruction that has those
>>> > reordering effects with surrounding code.
>>>
>>> I would have thought that when the docs talk about CAS having the
>>> effects of a volatile read+write, that's what the docs mean: if they'd
>>> wanted to imply the effect of an atomic instruction with a full fence
>>> it would have been easy enough to say so.
>>>
>>> Andrew.
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/5eac96e2/attachment.html>

From jsampson at guidewire.com  Tue Jan 20 17:12:27 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 20 Jan 2015 22:12:27 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>

I wrote:

> Here's a practical question from the API user's perspective: Does
> the reordering you described a few messages ago have any
> observable effects in the behavior of a program relative to the
> CAS being truly atomic?

Okay, here's an example where it makes a difference. The atomicity
issue is kind of secondary to the original question I raised, which
was whether a failed CAS must have the memory effects of a volatile
write. In Peter's example of a synchronized block implementation,
roach motel reordering is allowed but there's still a memory release
even in the case of failure. Indeed, a synchronized block is a
perfect example of something that provides a very strong appearance
of atomicity to the programmer even though it allows roach motel
reordering. So by itself that's kind of a red herring. Instead,
let's look at an example that gets at the combined effects of both
issues:

int a, b; // NOT volatile
final AtomicInteger x = new AtomicInteger(0);

T1:
a = 1;
r1 = x.compareAndSet(0, 1);

T2:
b = 1;
r2 = x.compareAndSet(0, 1);

T3:
r3 = x.compareAndSet(1, 0);
r4 = a;
r5 = b;

After joining T1, T2, & T3:
r6 = x.get();

Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
possible values observed for r4 and r5?

Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
was the final update applied to x; and r2 = false means that T2's
CAS saw the result of T1's CAS. (I've actually over-specified the
givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)

Answer 1: If every CAS is perfectly atomic _and_ has both memory
acquire and memory release effects regardless of success or
failure, then T1's and T2's CAS's both happen-before T3's CAS,
such that r4 and r5 must both be true.

Answer 2: If a failed CAS does _not_ have memory release effects,
then T1's CAS happens-before T3's CAS but T2's CAS does _not_
happen-before T3's CAS, so r4 must be true but r5 could be true or
false (it's racy).

Answer 3: If a failed CAS _does_ have memory release effects but
each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
not actually using a mutex), then things get interesting. T2's write
to b might be reordered to right before the release barrier of its
CAS and T3's read of b might be reordered to right after the acquire
barrier of its CAS. Since the CAS's aren't perfectly atomic or
mutually exclusive, T3's CAS could be executed between the load and
the store of T2's CAS. T2's CAS would not be affected, because it's
going to report failure either way. T3's CAS could then succeed, but
with T3's read of b failing to see T2's write of b due to their
reordering. Therefore in this case as well, r5 could be true or
false.

As I read the current docs, compareAndSet is most definitely
described as an _atomic_ operation with _both_ volatile read and
volatile write memory effects, supporting Answer 1, such that the
example code above is _not_ racy.

If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
volatile write effects on failure, the example code above _is_ racy.

Since there is a definite difference in the correctness implications
of these two interpretations, and since _at least two_ compareAndSet
implementations in j.u.c.atomic have the latter semantics anyway
(AtomicStampedReference and AtomicMarkableReference), it seems worth
clarifying in the docs.

Cheers,
Justin


From gergg at cox.net  Tue Jan 20 17:44:28 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Tue, 20 Jan 2015 16:44:28 -0600
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <hqKe1p00m02hR0p01qKjmt>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<hqKe1p00m02hR0p01qKjmt>
Message-ID: <129D2A38-A415-4A10-AD3D-6D77FF055651@cox.net>


> On Jan 19, 2015, at 8:17 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
> 
> I don't think there can be an agreement on what "simple" means, it may be easier to consider the prerequisite knowledge of the target audience.
> 
> I don't know how many people are really interested in this discussion - but the only way to share understanding with the few that are, is broadcast on the relevant topic.
> 
> If I recall right what your pain point is, declaring all instructions in all threads to appear in program order to all threads, does not make an awful lot of difference in the reasoning methods, and in many cases even no difference in the outcomes. Roughly speaking, given a proof of correctness of "totally ordered" program, it may be possible to turn the key variables used in decision-making points into volatiles, and turn everything else into normal variables. The trouble is, most people don't even consider what the proof of correctness of a "totally ordered" program would look like. I think very quickly you will want to use induction in your proofs, which will touch only a few edges in your program - the rest being irrelevant, or can be proven from those few. I think it is the use of induction that determines what JMM should be; that's the only thing that really reduces complexity of proofs.

My issue is primarily about the fact that this reordering changes the observable behavior of an application when casually observing it.  Things like logging statements that show the current value of a variable used in another thread for example.  If I have a remote ?observation? feature, and I want to ?see? what is happening in another thread, I have to burden the entire system with shenanigans to try and ?see? what is happening.  I have to burden the processing in that thread with fences/synchronization galore, just so that occasional observation can occur.  Yes, I can also create a synchronized observation point where data is passed between threads, but in doing that, I have to use ?synchronized? or volatile elsewhere in order for there to be a happens before relationship between writes in the observed thread and reads in the observer thread.   This is a very common thing for my software systems.  Optimizations, such as loop lifting are thus a big deal for me, because we have let the hardware issues of ?visibility? exist as they do now, and further anchored that into the expected behavior(s) of the Java platform.

> For example, your beloved loop example.
> 
> while(!signal)....
> 
> Why does the loop even exist? To me, it is evidence that somewhere in your proof you will have a statement "if signal==true, then ....<some consistency assertion here>" - as the program cannot deal with the inconsistent state; it does not progress and doesn't read nor write anything that appears in program order after the loop, until "signal==true?.

First, let me explain that I have used Java since the beginning of time.  I have written huge amounts of Java software in the form of libraries and desktop applications and applets.  I have also written java servlets.  I created my own messaging/brokering engine in 1997/1998 before JMS existed.   There are just countless places where I have had to deal with all aspects of the different Java versions. I never deployed anything on 1.2 due to the huge number of problems in that release (that release pretty much killed the momentum of Java in the market place as a desktop environment, and that was a disastrous outcome for Java overall).  I?ve dealt with very careful tuning of multi-threaded applications, such as my broker, which had hundreds of interacting threads around a handful of data structures.  There are places where I was a bit reckless with synchronization, because the early JVMs didn?t show signs of caring about that.  This whole loop hoisting bit is one of the places where I knew that the values were not strictly synchronized.  But, I did not care, because eventually the change in the loop control variable was always visible.  I never noted any measurable delay in the visibility on the x86 servers.  Hardware specific misbehaviors are a problem for me, because as a Java developer, I can?t always know every version of ever JVM that might exist.

Since volatile did not work in JDK 1.4 and earlier, loops with shared state termination control, had to be written as

	while(true) {
		synchronized(this) {
			if( done )
				break;
		}
		?.
	}

to be completely synchronized with

	public void stop() {
		synchronized(this) { done = true; }
	}

I didn?t like this verbosity all over the place, and so I instead used unsynchronized state as termination control because I didn?t need atomic changes of more than a single value across threads in these cases.

I use threads which process external events and another thread will ask them to stop that processing.  This typically looks like:

public void run() {
	while( !done ) {
		try {
			if( sock == null ) {
				sock = new Socket(?);
			}
			data = readSomething();
			process(data);
		} catch( IOException ex ) {
			try {
				sock.close();
			} catch( Exception exx ) {
				log.fine(exx);
			} finally {
				sock = null;
			}
		} catch( Exception ex ) {
			log.severe(ex);
		}
	}
}

I might then have a stop() method that looks like:

public void stop() {
	done = true;
	sock.close();
}

This method will trivially stop the thread by closing the socket.  This worked fine when the loop hoist was not happening. But suddenly it failed with the Java update.  The whole application completely stopped working because these worker threads would not stop when needed.  They would reopen the socket and keep going.  I Java 5 and later, I do now use volatile everywhere to keep loop hoisting from occurring.

> If you always rely on "signal == true", then you don't need total ordering - only partial ordering joined through signal read and write is sufficient; that means signal should be volatile, and the rest doesn't have to be (well, subject to restrictions in the rest of the code). If you sometimes do not rely on "signal==true", try to convince yourself that <some consistency assertion> holds, can you? If you can't convince that the consistency assertion holds, how can you show the program is correct? Nowhere here I mentioned total or partial ordering of instructions.

The problem is that software which used to work 100% of the time when volatile was not available (and it?s lack of use did not keep the software from working), suddenly stopped working on a Java update.  To me, that is a poor choice for an automatic optimization.  Java never said that the change in ?signal? could not be observed, ever.  This specific change, as I?ve said before, caused many people that I have interacted with and discussed this issue with, to tell me, that they had already decided that program correctness was more important than performance to them.  The said that they had told their developers that all variables would either be declared final or volatile. 

We don?t know what other flow control optimizations might just ?happen? in the JVM implementation to non-volatile values in the future and make software stop working again.  Even now, we still have people carefully crafting racy applications to avoid synchronized() blocks!   Making everything volatile, keeps that from causing ?alteration? to execution paths so that we can guarantee the application stays in working order, no matter what version of the JVM it is run on.  People deploying software systems don?t always have control over what JVM they are deployed on.  I know that Sun and now Oracle seem to have/had no idea how to deploy desktop applications in Java, let alone applets where the whole world of versions exist.  But I have many such applications that I?d really prefer people to have pleasant experiences with, and not have to spend hours on forums or mailing lists or telephone calls to support trying to figure out exactly which version of something that they have no idea that they need, has a problem and ?that thing? needs to be replaced/downgraded/upgraded to make the ?application? work.

I truly believe that volatile should be the default declaration in the JMM.  Performance tuning is what we do after we prove a program correct.  No matter how fast it goes, if it?s not computing correct results, then what good has been accomplished?  We should have to add an annotation or keyword to specify that we are willing to take the visibility risk and management into our own hands.  

I do appreciate your attention to optimization details and trying to understand how developers might be better served by the Java platform.   It is really bothersome and quite disturbing to users of the platform when any optimization suddenly breaks working software systems.  Java desktop applications or applets, delivered to the users as just JAR files that stop working like this are very frustrating for developers to debug.  Especially when the developers have no way of knowing who?s having what problems.  That?s just not good system design, at all.  Yes, developers can test, test and retest and evaluate all kinds of details.  But in the end, why should we decide that more time should be spent to get software to users when there is no direct advantage to users in doing that?   In this particular case, I don?t know how there is any appreciable gain in performance from this optimization for x86 in particular, so why is it so dear and important to have such things going on?

I have a few applications which I wrote on JDK 1.1.1, and they worked just fine on all versions of Java up to Java 5.  It?s that kind of breakage that just doesn?t make sense overall.

How many conversations have been had with developers to ask them why they don?t create Java desktop applications?  What makes that such an unpleasant experience for them or their users?  Really, why isn?t Java a predominant part of the desktop landscape?  Is it because of performance, or something different?  I?m going to guess that performance isn?t the big issue keeping developers from using Java for their applications.

Gregg Wonderly

> Alex
> 
> On 17/01/2015 05:27, Gregg Wonderly wrote:
>>> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>> 
>>> I would not bother specifying exactly what makes sense; only what's allowed.
>>> 
>>> For example, what would you say if some hardware offered atomicity of update of just the permit, and offer no barrier semantics for the rest of the accesses around it?
>>> 
>> Why would you want to specify in a language, variations of behavior that complicate software design, and cause software systems developers to fail because they can?t understand what actually will happen (as opposed to what can happen) or correctly implement software because they are having to interact, literally with random hardware behavior due to the language not adequately depicting tangible software behaviors.
>> 
>> I know it?s really fun to play with this stuff and try and wrestle the most out of the hardware with the least possible constraints.  But really, how in the world can we have 100% reliable software systems when we provide 50 ways to be unsuccessful for every 1 way that is the correct way to structure the code?  Why is all of that variation event attractive to you Alex?  Do you really want developer to have to wrestle with software design?  Do you find it fun to manipulate people with such complexity in the system and some how prove that your understanding is more than theirs?  Does this give you some kind of satisfaction or what?
>> 
>> In the end, this is a huge barrier to being successful with Java.  It provides not obvious benefit to the developers from a long term goal of building and maintaining viable software systems.  Instead it creates problem after problem.  Software system become unpredictable with seemingly random behaviors. I just fail to see how this can feel like good software language design.  Maybe you can send me a private response if you don?t care to discuss this stuff publicly.  Practically I find this to be just insane.  All of this discussion in this thread just points out how 100s of hours of peoples time can be wasted on this issue all for nothing but surprise.  Developers with great knowledge of the JMM still have a hard time proving to themselves what the outcome of their code will be.
>> 
>> Gregg Wonderly
>> 
>>> Alex
>>> 
>>> On 16/01/2015 20:58, Justin Sampson wrote:
>>>> Oleksandr Otenko wrote:
>>>> 
>>>>> You can't claim that it's not possible to implement reschedule in
>>>>> such a way that yield cannot tell whether reschedule was called
>>>>> since yield was called last time.
>>>> Ah, I see what you're getting at now, and I understand what you
>>>> meant by talking about a "fast path" earlier.
>>>> 
>>>> In your example, the yield/reschedule operations are _themselves_ a
>>>> valid implementation of park/unpark, complete with atomic handling
>>>> of internal state (the queue) that ensures a reschedule isn't lost.
>>>> That wasn't what I was imagining in the example of mine that you
>>>> were replying to.
>>>> 
>>>> So I think we're actually agreeing that the "permit" semantics do
>>>> have to be integrated into atomic operations at some level of
>>>> implementation. And I also agree that it's possible to define a
>>>> fast-path implementation on top of such semantics that still remains
>>>> valid but with less atomicity.
>>>> 
>>>> In fact, if I may generalize into a theorem, I believe we can say
>>>> that given any valid implementation of park/unpark, we can construct
>>>> another valid implementation, call it park'/unpark', like so:
>>>> 
>>>> Thread {
>>>>   volatile boolean permit';
>>>> }
>>>> 
>>>> park'() {
>>>>   Thread t = Thread.currentThread();
>>>>   if (!t.permit') {
>>>>     park();
>>>>   }
>>>>   t.permit' = false;
>>>> }
>>>> 
>>>> unpark'(Thread t) {
>>>>   if (!t.permit') {
>>>>     t.permit' = true;
>>>>     unpark(t);
>>>>   }
>>>> }
>>>> 
>>>> I'm writing up another attempt at formal semantics that allows us to
>>>> prove this theorem, which I'll send out later today. :)
>>>> 
>>>> Cheers,
>>>> Justin
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From boehm at acm.org  Tue Jan 20 18:15:00 2015
From: boehm at acm.org (Hans Boehm)
Date: Tue, 20 Jan 2015 15:15:00 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
Message-ID: <CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>

No disagreement about this benefitting from clarification.  But I still
don't think a failed CAS can possibly have release/volatile write
semantics, given that the compareAndSet definition states:

"Atomically sets the value to the given updated value if the current value
== the expected value."

Release/volatile write semantics only make sense if there was a write.   In
the failure case there isn't.  (See 17.4.4: "A write to a volatile variable
v (?8.3.1.4) synchronizes-with all subsequent reads of v by any thread
(where "subsequent" is defined according to the synchronization order)."
 There is no synchronizes with relationship involving volatiles unless
there is a "write to a volatile variable".  A failed CAS fairly
unambiguously doesn't write anything.)

T2s CAS is not guaranteed to happen before T3s CAS because there is no
volatile write in T2 which could possibly synchronize with anything in T3.

Answer 2 is the only one that currently makes sense in the Java memory
model.  To change that we would have to change the CAS definition so that
it unconditionally writes something, possibly the original value.

Hans

On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> I wrote:
>
> > Here's a practical question from the API user's perspective: Does
> > the reordering you described a few messages ago have any
> > observable effects in the behavior of a program relative to the
> > CAS being truly atomic?
>
> Okay, here's an example where it makes a difference. The atomicity
> issue is kind of secondary to the original question I raised, which
> was whether a failed CAS must have the memory effects of a volatile
> write. In Peter's example of a synchronized block implementation,
> roach motel reordering is allowed but there's still a memory release
> even in the case of failure. Indeed, a synchronized block is a
> perfect example of something that provides a very strong appearance
> of atomicity to the programmer even though it allows roach motel
> reordering. So by itself that's kind of a red herring. Instead,
> let's look at an example that gets at the combined effects of both
> issues:
>
> int a, b; // NOT volatile
> final AtomicInteger x = new AtomicInteger(0);
>
> T1:
> a = 1;
> r1 = x.compareAndSet(0, 1);
>
> T2:
> b = 1;
> r2 = x.compareAndSet(0, 1);
>
> T3:
> r3 = x.compareAndSet(1, 0);
> r4 = a;
> r5 = b;
>
> After joining T1, T2, & T3:
> r6 = x.get();
>
> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
> possible values observed for r4 and r5?
>
> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
> was the final update applied to x; and r2 = false means that T2's
> CAS saw the result of T1's CAS. (I've actually over-specified the
> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>
> Answer 1: If every CAS is perfectly atomic _and_ has both memory
> acquire and memory release effects regardless of success or
> failure, then T1's and T2's CAS's both happen-before T3's CAS,
> such that r4 and r5 must both be true.
>
> Answer 2: If a failed CAS does _not_ have memory release effects,
> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
> happen-before T3's CAS, so r4 must be true but r5 could be true or
> false (it's racy).
>
> Answer 3: If a failed CAS _does_ have memory release effects but
> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
> not actually using a mutex), then things get interesting. T2's write
> to b might be reordered to right before the release barrier of its
> CAS and T3's read of b might be reordered to right after the acquire
> barrier of its CAS. Since the CAS's aren't perfectly atomic or
> mutually exclusive, T3's CAS could be executed between the load and
> the store of T2's CAS. T2's CAS would not be affected, because it's
> going to report failure either way. T3's CAS could then succeed, but
> with T3's read of b failing to see T2's write of b due to their
> reordering. Therefore in this case as well, r5 could be true or
> false.
>
> As I read the current docs, compareAndSet is most definitely
> described as an _atomic_ operation with _both_ volatile read and
> volatile write memory effects, supporting Answer 1, such that the
> example code above is _not_ racy.
>
> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
> volatile write effects on failure, the example code above _is_ racy.
>
> Since there is a definite difference in the correctness implications
> of these two interpretations, and since _at least two_ compareAndSet
> implementations in j.u.c.atomic have the latter semantics anyway
> (AtomicStampedReference and AtomicMarkableReference), it seems worth
> clarifying in the docs.
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/e6f4b08e/attachment.html>

From vitalyd at gmail.com  Tue Jan 20 18:36:45 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 18:36:45 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
Message-ID: <CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>

I'm having a hard time imagining where it would be useful to have a failing
CAS have different memory ordering effects from a successful one.  If one
wanted to leverage the volatile write-like semantics of CAS yet a CAS
failure is an ok condition correctness-wise, how would they go about
ensuring that? Check if CAS failed, and then do what? Issue a store fence
manually? Furthermore, is there even an existing implementation of CAS,
either in software or hardware, where this happens (i.e. failed CAS does
not have volatile write semantics)?

In my opinion, I don't think the ordering effects of CAS should be
conflated with the actual purpose of the function (i.e. atomic exchange).

On Tue, Jan 20, 2015 at 6:15 PM, Hans Boehm <boehm at acm.org> wrote:

> No disagreement about this benefitting from clarification.  But I still
> don't think a failed CAS can possibly have release/volatile write
> semantics, given that the compareAndSet definition states:
>
> "Atomically sets the value to the given updated value if the current value
> == the expected value."
>
> Release/volatile write semantics only make sense if there was a write.
> In the failure case there isn't.  (See 17.4.4: "A write to a volatile
> variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any
> thread (where "subsequent" is defined according to the synchronization
> order)."  There is no synchronizes with relationship involving volatiles
> unless there is a "write to a volatile variable".  A failed CAS fairly
> unambiguously doesn't write anything.)
>
> T2s CAS is not guaranteed to happen before T3s CAS because there is no
> volatile write in T2 which could possibly synchronize with anything in T3.
>
> Answer 2 is the only one that currently makes sense in the Java memory
> model.  To change that we would have to change the CAS definition so that
> it unconditionally writes something, possibly the original value.
>
> Hans
>
> On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
> wrote:
>
>> I wrote:
>>
>> > Here's a practical question from the API user's perspective: Does
>> > the reordering you described a few messages ago have any
>> > observable effects in the behavior of a program relative to the
>> > CAS being truly atomic?
>>
>> Okay, here's an example where it makes a difference. The atomicity
>> issue is kind of secondary to the original question I raised, which
>> was whether a failed CAS must have the memory effects of a volatile
>> write. In Peter's example of a synchronized block implementation,
>> roach motel reordering is allowed but there's still a memory release
>> even in the case of failure. Indeed, a synchronized block is a
>> perfect example of something that provides a very strong appearance
>> of atomicity to the programmer even though it allows roach motel
>> reordering. So by itself that's kind of a red herring. Instead,
>> let's look at an example that gets at the combined effects of both
>> issues:
>>
>> int a, b; // NOT volatile
>> final AtomicInteger x = new AtomicInteger(0);
>>
>> T1:
>> a = 1;
>> r1 = x.compareAndSet(0, 1);
>>
>> T2:
>> b = 1;
>> r2 = x.compareAndSet(0, 1);
>>
>> T3:
>> r3 = x.compareAndSet(1, 0);
>> r4 = a;
>> r5 = b;
>>
>> After joining T1, T2, & T3:
>> r6 = x.get();
>>
>> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
>> possible values observed for r4 and r5?
>>
>> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
>> was the final update applied to x; and r2 = false means that T2's
>> CAS saw the result of T1's CAS. (I've actually over-specified the
>> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>>
>> Answer 1: If every CAS is perfectly atomic _and_ has both memory
>> acquire and memory release effects regardless of success or
>> failure, then T1's and T2's CAS's both happen-before T3's CAS,
>> such that r4 and r5 must both be true.
>>
>> Answer 2: If a failed CAS does _not_ have memory release effects,
>> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
>> happen-before T3's CAS, so r4 must be true but r5 could be true or
>> false (it's racy).
>>
>> Answer 3: If a failed CAS _does_ have memory release effects but
>> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
>> not actually using a mutex), then things get interesting. T2's write
>> to b might be reordered to right before the release barrier of its
>> CAS and T3's read of b might be reordered to right after the acquire
>> barrier of its CAS. Since the CAS's aren't perfectly atomic or
>> mutually exclusive, T3's CAS could be executed between the load and
>> the store of T2's CAS. T2's CAS would not be affected, because it's
>> going to report failure either way. T3's CAS could then succeed, but
>> with T3's read of b failing to see T2's write of b due to their
>> reordering. Therefore in this case as well, r5 could be true or
>> false.
>>
>> As I read the current docs, compareAndSet is most definitely
>> described as an _atomic_ operation with _both_ volatile read and
>> volatile write memory effects, supporting Answer 1, such that the
>> example code above is _not_ racy.
>>
>> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
>> volatile write effects on failure, the example code above _is_ racy.
>>
>> Since there is a definite difference in the correctness implications
>> of these two interpretations, and since _at least two_ compareAndSet
>> implementations in j.u.c.atomic have the latter semantics anyway
>> (AtomicStampedReference and AtomicMarkableReference), it seems worth
>> clarifying in the docs.
>>
>> Cheers,
>> Justin
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/9181929e/attachment.html>

From davidcholmes at aapt.net.au  Tue Jan 20 18:36:20 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 21 Jan 2015 09:36:20 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCELOKMAA.davidcholmes@aapt.net.au>

Hans,

Given the CAS is native we can insert anything we want to make this look like a volatile write happened, even if there is no actual write. Given a volatile write in hotspot has the effect of:

release; store; fence

then we can insert a fence after the CAS if needed. (This is why I'm still unsure the ARMv8 st.rel is sufficient for a volatile write in the general case.)

I've gone back to check my archives on this and AFAICS the intent to indicate the effect of volatile write even on a failed (strong) CAS was deliberate.

David
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Wednesday, 21 January 2015 9:15 AM
  To: Justin Sampson
  Cc: Andrew Haley; Vitaly Davidovich; concurrency-interest at cs.oswego.edu; dholmes at ieee.org
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  No disagreement about this benefitting from clarification.  But I still don't think a failed CAS can possibly have release/volatile write semantics, given that the compareAndSet definition states:

  "Atomically sets the value to the given updated value if the current value == the expected value."


  Release/volatile write semantics only make sense if there was a write.   In the failure case there isn't.  (See 17.4.4: "A write to a volatile variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any thread (where "subsequent" is defined according to the synchronization order)."  There is no synchronizes with relationship involving volatiles unless there is a "write to a volatile variable".  A failed CAS fairly unambiguously doesn't write anything.)


  T2s CAS is not guaranteed to happen before T3s CAS because there is no volatile write in T2 which could possibly synchronize with anything in T3.


  Answer 2 is the only one that currently makes sense in the Java memory model.  To change that we would have to change the CAS definition so that it unconditionally writes something, possibly the original value.


  Hans


  On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com> wrote:

    I wrote:

    > Here's a practical question from the API user's perspective: Does
    > the reordering you described a few messages ago have any
    > observable effects in the behavior of a program relative to the
    > CAS being truly atomic?

    Okay, here's an example where it makes a difference. The atomicity
    issue is kind of secondary to the original question I raised, which
    was whether a failed CAS must have the memory effects of a volatile
    write. In Peter's example of a synchronized block implementation,
    roach motel reordering is allowed but there's still a memory release
    even in the case of failure. Indeed, a synchronized block is a
    perfect example of something that provides a very strong appearance
    of atomicity to the programmer even though it allows roach motel
    reordering. So by itself that's kind of a red herring. Instead,
    let's look at an example that gets at the combined effects of both
    issues:

    int a, b; // NOT volatile
    final AtomicInteger x = new AtomicInteger(0);

    T1:
    a = 1;
    r1 = x.compareAndSet(0, 1);

    T2:
    b = 1;
    r2 = x.compareAndSet(0, 1);

    T3:
    r3 = x.compareAndSet(1, 0);
    r4 = a;
    r5 = b;

    After joining T1, T2, & T3:
    r6 = x.get();

    Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
    possible values observed for r4 and r5?

    Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
    was the final update applied to x; and r2 = false means that T2's
    CAS saw the result of T1's CAS. (I've actually over-specified the
    givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)

    Answer 1: If every CAS is perfectly atomic _and_ has both memory
    acquire and memory release effects regardless of success or
    failure, then T1's and T2's CAS's both happen-before T3's CAS,
    such that r4 and r5 must both be true.

    Answer 2: If a failed CAS does _not_ have memory release effects,
    then T1's CAS happens-before T3's CAS but T2's CAS does _not_
    happen-before T3's CAS, so r4 must be true but r5 could be true or
    false (it's racy).

    Answer 3: If a failed CAS _does_ have memory release effects but
    each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
    not actually using a mutex), then things get interesting. T2's write
    to b might be reordered to right before the release barrier of its
    CAS and T3's read of b might be reordered to right after the acquire
    barrier of its CAS. Since the CAS's aren't perfectly atomic or
    mutually exclusive, T3's CAS could be executed between the load and
    the store of T2's CAS. T2's CAS would not be affected, because it's
    going to report failure either way. T3's CAS could then succeed, but
    with T3's read of b failing to see T2's write of b due to their
    reordering. Therefore in this case as well, r5 could be true or
    false.

    As I read the current docs, compareAndSet is most definitely
    described as an _atomic_ operation with _both_ volatile read and
    volatile write memory effects, supporting Answer 1, such that the
    example code above is _not_ racy.

    If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
    volatile write effects on failure, the example code above _is_ racy.

    Since there is a definite difference in the correctness implications
    of these two interpretations, and since _at least two_ compareAndSet
    implementations in j.u.c.atomic have the latter semantics anyway
    (AtomicStampedReference and AtomicMarkableReference), it seems worth
    clarifying in the docs.

    Cheers,
    Justin


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/8527e56b/attachment-0001.html>

From stephan.diestelhorst at gmail.com  Tue Jan 20 19:08:52 2015
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Wed, 21 Jan 2015 00:08:52 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
	<CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>
Message-ID: <CAJR39EyRNHCm26GSFEcM6+W0kTbyFWcGdbSWk-u+2P_jnskX_g@mail.gmail.com>

In theory, this could give you the benefits of test-test-and-set: no /less
costly fencing, possibly only sending read snoops.

In practice, this might differ, as HW impls of CAS may or may not have
stronger built-in semantics (compare ARM ldx / stx and similar sequences vs
LOCK CMPXCHG on x86).

Stephan
On 20 Jan 2015 23:58, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> I'm having a hard time imagining where it would be useful to have a
> failing CAS have different memory ordering effects from a successful one.
> If one wanted to leverage the volatile write-like semantics of CAS yet a
> CAS failure is an ok condition correctness-wise, how would they go about
> ensuring that? Check if CAS failed, and then do what? Issue a store fence
> manually? Furthermore, is there even an existing implementation of CAS,
> either in software or hardware, where this happens (i.e. failed CAS does
> not have volatile write semantics)?
>
> In my opinion, I don't think the ordering effects of CAS should be
> conflated with the actual purpose of the function (i.e. atomic exchange).
>
> On Tue, Jan 20, 2015 at 6:15 PM, Hans Boehm <boehm at acm.org> wrote:
>
>> No disagreement about this benefitting from clarification.  But I still
>> don't think a failed CAS can possibly have release/volatile write
>> semantics, given that the compareAndSet definition states:
>>
>> "Atomically sets the value to the given updated value if the current
>> value == the expected value."
>>
>> Release/volatile write semantics only make sense if there was a write.
>> In the failure case there isn't.  (See 17.4.4: "A write to a volatile
>> variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any
>> thread (where "subsequent" is defined according to the synchronization
>> order)."  There is no synchronizes with relationship involving volatiles
>> unless there is a "write to a volatile variable".  A failed CAS fairly
>> unambiguously doesn't write anything.)
>>
>> T2s CAS is not guaranteed to happen before T3s CAS because there is no
>> volatile write in T2 which could possibly synchronize with anything in T3.
>>
>> Answer 2 is the only one that currently makes sense in the Java memory
>> model.  To change that we would have to change the CAS definition so that
>> it unconditionally writes something, possibly the original value.
>>
>> Hans
>>
>> On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>>
>>> I wrote:
>>>
>>> > Here's a practical question from the API user's perspective: Does
>>> > the reordering you described a few messages ago have any
>>> > observable effects in the behavior of a program relative to the
>>> > CAS being truly atomic?
>>>
>>> Okay, here's an example where it makes a difference. The atomicity
>>> issue is kind of secondary to the original question I raised, which
>>> was whether a failed CAS must have the memory effects of a volatile
>>> write. In Peter's example of a synchronized block implementation,
>>> roach motel reordering is allowed but there's still a memory release
>>> even in the case of failure. Indeed, a synchronized block is a
>>> perfect example of something that provides a very strong appearance
>>> of atomicity to the programmer even though it allows roach motel
>>> reordering. So by itself that's kind of a red herring. Instead,
>>> let's look at an example that gets at the combined effects of both
>>> issues:
>>>
>>> int a, b; // NOT volatile
>>> final AtomicInteger x = new AtomicInteger(0);
>>>
>>> T1:
>>> a = 1;
>>> r1 = x.compareAndSet(0, 1);
>>>
>>> T2:
>>> b = 1;
>>> r2 = x.compareAndSet(0, 1);
>>>
>>> T3:
>>> r3 = x.compareAndSet(1, 0);
>>> r4 = a;
>>> r5 = b;
>>>
>>> After joining T1, T2, & T3:
>>> r6 = x.get();
>>>
>>> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
>>> possible values observed for r4 and r5?
>>>
>>> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
>>> was the final update applied to x; and r2 = false means that T2's
>>> CAS saw the result of T1's CAS. (I've actually over-specified the
>>> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>>>
>>> Answer 1: If every CAS is perfectly atomic _and_ has both memory
>>> acquire and memory release effects regardless of success or
>>> failure, then T1's and T2's CAS's both happen-before T3's CAS,
>>> such that r4 and r5 must both be true.
>>>
>>> Answer 2: If a failed CAS does _not_ have memory release effects,
>>> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
>>> happen-before T3's CAS, so r4 must be true but r5 could be true or
>>> false (it's racy).
>>>
>>> Answer 3: If a failed CAS _does_ have memory release effects but
>>> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
>>> not actually using a mutex), then things get interesting. T2's write
>>> to b might be reordered to right before the release barrier of its
>>> CAS and T3's read of b might be reordered to right after the acquire
>>> barrier of its CAS. Since the CAS's aren't perfectly atomic or
>>> mutually exclusive, T3's CAS could be executed between the load and
>>> the store of T2's CAS. T2's CAS would not be affected, because it's
>>> going to report failure either way. T3's CAS could then succeed, but
>>> with T3's read of b failing to see T2's write of b due to their
>>> reordering. Therefore in this case as well, r5 could be true or
>>> false.
>>>
>>> As I read the current docs, compareAndSet is most definitely
>>> described as an _atomic_ operation with _both_ volatile read and
>>> volatile write memory effects, supporting Answer 1, such that the
>>> example code above is _not_ racy.
>>>
>>> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
>>> volatile write effects on failure, the example code above _is_ racy.
>>>
>>> Since there is a definite difference in the correctness implications
>>> of these two interpretations, and since _at least two_ compareAndSet
>>> implementations in j.u.c.atomic have the latter semantics anyway
>>> (AtomicStampedReference and AtomicMarkableReference), it seems worth
>>> clarifying in the docs.
>>>
>>> Cheers,
>>> Justin
>>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/ccc21d8a/attachment.html>

From vitalyd at gmail.com  Tue Jan 20 19:20:34 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 20 Jan 2015 19:20:34 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAJR39EyRNHCm26GSFEcM6+W0kTbyFWcGdbSWk-u+2P_jnskX_g@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
	<CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>
	<CAJR39EyRNHCm26GSFEcM6+W0kTbyFWcGdbSWk-u+2P_jnskX_g@mail.gmail.com>
Message-ID: <CAHjP37F6ryq-hmr5emMbMMYGZSfobh8OJ2iLTZ10uxg2odG01w@mail.gmail.com>

>
> In theory, this could give you the benefits of test-test-and-set: no /less
> costly fencing, possibly only sending read snoops.


I thought of something like that, but how would read snoops ensure
atomicity? That is, if the initial test indicates the value is the same,
there's no guarantee is stays the same immediately after -- not clear how
that would allow implementing a strong CAS.

On Tue, Jan 20, 2015 at 7:08 PM, Stephan Diestelhorst <
stephan.diestelhorst at gmail.com> wrote:

> In theory, this could give you the benefits of test-test-and-set: no /less
> costly fencing, possibly only sending read snoops.
>
> In practice, this might differ, as HW impls of CAS may or may not have
> stronger built-in semantics (compare ARM ldx / stx and similar sequences vs
> LOCK CMPXCHG on x86).
>
> Stephan
> On 20 Jan 2015 23:58, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>
>> I'm having a hard time imagining where it would be useful to have a
>> failing CAS have different memory ordering effects from a successful one.
>> If one wanted to leverage the volatile write-like semantics of CAS yet a
>> CAS failure is an ok condition correctness-wise, how would they go about
>> ensuring that? Check if CAS failed, and then do what? Issue a store fence
>> manually? Furthermore, is there even an existing implementation of CAS,
>> either in software or hardware, where this happens (i.e. failed CAS does
>> not have volatile write semantics)?
>>
>> In my opinion, I don't think the ordering effects of CAS should be
>> conflated with the actual purpose of the function (i.e. atomic exchange).
>>
>> On Tue, Jan 20, 2015 at 6:15 PM, Hans Boehm <boehm at acm.org> wrote:
>>
>>> No disagreement about this benefitting from clarification.  But I still
>>> don't think a failed CAS can possibly have release/volatile write
>>> semantics, given that the compareAndSet definition states:
>>>
>>> "Atomically sets the value to the given updated value if the current
>>> value == the expected value."
>>>
>>> Release/volatile write semantics only make sense if there was a write.
>>> In the failure case there isn't.  (See 17.4.4: "A write to a volatile
>>> variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any
>>> thread (where "subsequent" is defined according to the synchronization
>>> order)."  There is no synchronizes with relationship involving volatiles
>>> unless there is a "write to a volatile variable".  A failed CAS fairly
>>> unambiguously doesn't write anything.)
>>>
>>> T2s CAS is not guaranteed to happen before T3s CAS because there is no
>>> volatile write in T2 which could possibly synchronize with anything in T3.
>>>
>>> Answer 2 is the only one that currently makes sense in the Java memory
>>> model.  To change that we would have to change the CAS definition so that
>>> it unconditionally writes something, possibly the original value.
>>>
>>> Hans
>>>
>>> On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
>>> wrote:
>>>
>>>> I wrote:
>>>>
>>>> > Here's a practical question from the API user's perspective: Does
>>>> > the reordering you described a few messages ago have any
>>>> > observable effects in the behavior of a program relative to the
>>>> > CAS being truly atomic?
>>>>
>>>> Okay, here's an example where it makes a difference. The atomicity
>>>> issue is kind of secondary to the original question I raised, which
>>>> was whether a failed CAS must have the memory effects of a volatile
>>>> write. In Peter's example of a synchronized block implementation,
>>>> roach motel reordering is allowed but there's still a memory release
>>>> even in the case of failure. Indeed, a synchronized block is a
>>>> perfect example of something that provides a very strong appearance
>>>> of atomicity to the programmer even though it allows roach motel
>>>> reordering. So by itself that's kind of a red herring. Instead,
>>>> let's look at an example that gets at the combined effects of both
>>>> issues:
>>>>
>>>> int a, b; // NOT volatile
>>>> final AtomicInteger x = new AtomicInteger(0);
>>>>
>>>> T1:
>>>> a = 1;
>>>> r1 = x.compareAndSet(0, 1);
>>>>
>>>> T2:
>>>> b = 1;
>>>> r2 = x.compareAndSet(0, 1);
>>>>
>>>> T3:
>>>> r3 = x.compareAndSet(1, 0);
>>>> r4 = a;
>>>> r5 = b;
>>>>
>>>> After joining T1, T2, & T3:
>>>> r6 = x.get();
>>>>
>>>> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
>>>> possible values observed for r4 and r5?
>>>>
>>>> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
>>>> was the final update applied to x; and r2 = false means that T2's
>>>> CAS saw the result of T1's CAS. (I've actually over-specified the
>>>> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>>>>
>>>> Answer 1: If every CAS is perfectly atomic _and_ has both memory
>>>> acquire and memory release effects regardless of success or
>>>> failure, then T1's and T2's CAS's both happen-before T3's CAS,
>>>> such that r4 and r5 must both be true.
>>>>
>>>> Answer 2: If a failed CAS does _not_ have memory release effects,
>>>> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
>>>> happen-before T3's CAS, so r4 must be true but r5 could be true or
>>>> false (it's racy).
>>>>
>>>> Answer 3: If a failed CAS _does_ have memory release effects but
>>>> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
>>>> not actually using a mutex), then things get interesting. T2's write
>>>> to b might be reordered to right before the release barrier of its
>>>> CAS and T3's read of b might be reordered to right after the acquire
>>>> barrier of its CAS. Since the CAS's aren't perfectly atomic or
>>>> mutually exclusive, T3's CAS could be executed between the load and
>>>> the store of T2's CAS. T2's CAS would not be affected, because it's
>>>> going to report failure either way. T3's CAS could then succeed, but
>>>> with T3's read of b failing to see T2's write of b due to their
>>>> reordering. Therefore in this case as well, r5 could be true or
>>>> false.
>>>>
>>>> As I read the current docs, compareAndSet is most definitely
>>>> described as an _atomic_ operation with _both_ volatile read and
>>>> volatile write memory effects, supporting Answer 1, such that the
>>>> example code above is _not_ racy.
>>>>
>>>> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
>>>> volatile write effects on failure, the example code above _is_ racy.
>>>>
>>>> Since there is a definite difference in the correctness implications
>>>> of these two interpretations, and since _at least two_ compareAndSet
>>>> implementations in j.u.c.atomic have the latter semantics anyway
>>>> (AtomicStampedReference and AtomicMarkableReference), it seems worth
>>>> clarifying in the docs.
>>>>
>>>> Cheers,
>>>> Justin
>>>>
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/fd57cd47/attachment-0001.html>

From boehm at acm.org  Tue Jan 20 20:06:49 2015
From: boehm at acm.org (Hans Boehm)
Date: Tue, 20 Jan 2015 17:06:49 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCELOKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCELOKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>

My primary concern with insisting on "release" semantics for a failed CAS
is that it really doesn't make sense in the memory model.  A "release"
operation is defined by a "synchronizes with" edge from the write.  A
failed CAS has no associated write.  What does such a "release" operation
mean?  Can someone propose a definition consistent with the current
language spec?  I think we would have to change the spec to include an
unconditional write, which seems quite contrived and misleading.  It also
adds bogus data races to some code (though I don't know how common such
code is).

Remember that the current memory model says nothing about fences, which are
in fact quite hard to define in a portable way.

I don't know of any natural examples where such "release" semantics for a
failed CAS might matter, though clearly Justin successfully contrived one.
Since such failed operations have no effect on the shared state (again no
write), it's really difficult for another thread to tell they completed and
somehow try to rely on that fact.  Without that the release semantics are
invisible.

I agree that most conventional implementations provide some sort of
fence-like semantics even in the failure case.  But I think the natural
implementation on ARMv8 does not.  The code would do a
load-exclusive-acquire, compare to the expected value, and just skip the
release store if the comparison failed.  (I think the verdict is still out
on whether this will eventually translate into real improved performance
over fence-based models.  But I believe it reflects current programming
advice for ARMv8.)

The release; store; fence implementation of a volatile store in hotspot is
actually overkill in two ways, neither of which are required by the Java
memory model:

1) Separating out the initial release as a fence orders prior operations
with respect to ALL subsequent stores, not just the volatile store.  That's
unnecessary.  Later non-volatile stores can advance past the volatile one
without violating the memory model.  This affects ARMv8, Itanium, and
compiler transformations.

2) The fence orders all prior and later memory operations.  The only
required ordering is between this particular volatile store and subsequent
VOLATILE loads.  Those volatile loads are often a long ways off, and there
is no reason to stall anything until you encounter one.  This affects ARMv8
and compiler transformations.  ARMv8 guarantees the correct ordering
constraint with respect to only later acquire loads.  (Itanium's st.rel
needs a trailing fence.)

This implementation model also doesn't seem to correctly reflect store
atomicity considerations.  You have to model a volatile load as load;
acquire.  That works on Power if a "release" is an LWSYNC, fence is SYNC,
but  "acquire" is also SYNC, which seems very weird.

I think all major VMs currently do things along these lines.  But I don't
think this is the right implementation model to strive for; it was a great
initial approach that we should be trying to move away from.

Hans

On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Hans,
>
> Given the CAS is native we can insert anything we want to make this look
> like a volatile write happened, even if there is no actual write. Given a
> volatile write in hotspot has the effect of:
>
> release; store; fence
>
> then we can insert a fence after the CAS if needed. (This is why I'm still
> unsure the ARMv8 st.rel is sufficient for a volatile write in the general
> case.)
>
> I've gone back to check my archives on this and AFAICS the intent to
> indicate the effect of volatile write even on a failed (strong) CAS was
> deliberate.
>
> David
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Wednesday, 21 January 2015 9:15 AM
> *To:* Justin Sampson
> *Cc:* Andrew Haley; Vitaly Davidovich; concurrency-interest at cs.oswego.edu;
> dholmes at ieee.org
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
> No disagreement about this benefitting from clarification.  But I still
> don't think a failed CAS can possibly have release/volatile write
> semantics, given that the compareAndSet definition states:
>
> "Atomically sets the value to the given updated value if the current value
> == the expected value."
>
> Release/volatile write semantics only make sense if there was a write.
> In the failure case there isn't.  (See 17.4.4: "A write to a volatile
> variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any
> thread (where "subsequent" is defined according to the synchronization
> order)."  There is no synchronizes with relationship involving volatiles
> unless there is a "write to a volatile variable".  A failed CAS fairly
> unambiguously doesn't write anything.)
>
> T2s CAS is not guaranteed to happen before T3s CAS because there is no
> volatile write in T2 which could possibly synchronize with anything in T3.
>
> Answer 2 is the only one that currently makes sense in the Java memory
> model.  To change that we would have to change the CAS definition so that
> it unconditionally writes something, possibly the original value.
>
> Hans
>
> On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
> wrote:
>
>> I wrote:
>>
>> > Here's a practical question from the API user's perspective: Does
>> > the reordering you described a few messages ago have any
>> > observable effects in the behavior of a program relative to the
>> > CAS being truly atomic?
>>
>> Okay, here's an example where it makes a difference. The atomicity
>> issue is kind of secondary to the original question I raised, which
>> was whether a failed CAS must have the memory effects of a volatile
>> write. In Peter's example of a synchronized block implementation,
>> roach motel reordering is allowed but there's still a memory release
>> even in the case of failure. Indeed, a synchronized block is a
>> perfect example of something that provides a very strong appearance
>> of atomicity to the programmer even though it allows roach motel
>> reordering. So by itself that's kind of a red herring. Instead,
>> let's look at an example that gets at the combined effects of both
>> issues:
>>
>> int a, b; // NOT volatile
>> final AtomicInteger x = new AtomicInteger(0);
>>
>> T1:
>> a = 1;
>> r1 = x.compareAndSet(0, 1);
>>
>> T2:
>> b = 1;
>> r2 = x.compareAndSet(0, 1);
>>
>> T3:
>> r3 = x.compareAndSet(1, 0);
>> r4 = a;
>> r5 = b;
>>
>> After joining T1, T2, & T3:
>> r6 = x.get();
>>
>> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
>> possible values observed for r4 and r5?
>>
>> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
>> was the final update applied to x; and r2 = false means that T2's
>> CAS saw the result of T1's CAS. (I've actually over-specified the
>> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>>
>> Answer 1: If every CAS is perfectly atomic _and_ has both memory
>> acquire and memory release effects regardless of success or
>> failure, then T1's and T2's CAS's both happen-before T3's CAS,
>> such that r4 and r5 must both be true.
>>
>> Answer 2: If a failed CAS does _not_ have memory release effects,
>> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
>> happen-before T3's CAS, so r4 must be true but r5 could be true or
>> false (it's racy).
>>
>> Answer 3: If a failed CAS _does_ have memory release effects but
>> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
>> not actually using a mutex), then things get interesting. T2's write
>> to b might be reordered to right before the release barrier of its
>> CAS and T3's read of b might be reordered to right after the acquire
>> barrier of its CAS. Since the CAS's aren't perfectly atomic or
>> mutually exclusive, T3's CAS could be executed between the load and
>> the store of T2's CAS. T2's CAS would not be affected, because it's
>> going to report failure either way. T3's CAS could then succeed, but
>> with T3's read of b failing to see T2's write of b due to their
>> reordering. Therefore in this case as well, r5 could be true or
>> false.
>>
>> As I read the current docs, compareAndSet is most definitely
>> described as an _atomic_ operation with _both_ volatile read and
>> volatile write memory effects, supporting Answer 1, such that the
>> example code above is _not_ racy.
>>
>> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
>> volatile write effects on failure, the example code above _is_ racy.
>>
>> Since there is a definite difference in the correctness implications
>> of these two interpretations, and since _at least two_ compareAndSet
>> implementations in j.u.c.atomic have the latter semantics anyway
>> (AtomicStampedReference and AtomicMarkableReference), it seems worth
>> clarifying in the docs.
>>
>> Cheers,
>> Justin
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/13380139/attachment.html>

From jsampson at guidewire.com  Tue Jan 20 20:18:26 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 01:18:26 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOELGKMAA.davidcholmes@aapt.net.au>
	<54BE236E.2070208@redhat.com>
	<CAHjP37HO0+Ch5+8r6-DZ48+BWAiVSO2GKJ4MZEPkHoffAwKh5Q@mail.gmail.com>
	<54BE6EF5.8080600@redhat.com>
	<CAHjP37FXoHSB3=TS9Bwm5jRH36gOqUoN+7t7yq4=mLog8M7scw@mail.gmail.com>
	<54BE7746.3000201@redhat.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C4B1@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C584@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZA4wN=WdpFCrRCRoZxGky+L6OR3Rof2pfwWDwDxWZeUQ@mail.gmail.com>
	<CAHjP37EXfZCf7=0WucAbo6=6nTtB56bu0Pi7eGky4E4Gmww9HQ@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C70E@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Furthermore, is there even an existing implementation of CAS,
> either in software or hardware, where this happens (i.e. failed
> CAS does not have volatile write semantics)?

I keep mentioning AtomicStampedReference.

Actually, taking a fresh look, I see that even its set() method
doesn't have volatile write memory effects unless the value is
actually changed, even though the javadoc says "Unconditionally sets
the value..."

Again, I totally agree with Hans's position that there's not any
practical utility to providing volatile write memory effects when
the value hasn't actually changed. I don't think spurious failures
are generally a problem either. And apparently whoever wrote
AtomicStampedReference (Doug?) had a similar intuition when writing
it a dozen years ago. :)

Cheers,
Justin


From davidcholmes at aapt.net.au  Tue Jan 20 20:29:27 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 21 Jan 2015 11:29:27 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>

I'm struggling to follow this as we seem to be continually switching between different layers of abstraction. The spec says CAS acts as a volatile read plus volatile write. So your mental model for this can simply be that the final action of the CAS is a write: either the new value if the CAS was successful, or the existing value if not. That's a conceptual model - no implementation need actually do an explicit write.

Each implementation then has to provide the necessary memory barriers to effect the conceptual model.

With regard to volatile read/write implementation in hotspot, yes it is stronger than needed. As has been discussed numerous times, at least in C1/interpreter/Unsafe hotspot doesn't consider the pairing of the loads and stores but only looks at the current load/store. So it has to emit the worst-case barriers needed. C2 may be more clever about this but I don't know - and wouldn't be surprised if it isn't.

David

  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
  Sent: Wednesday, 21 January 2015 11:07 AM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  My primary concern with insisting on "release" semantics for a failed CAS is that it really doesn't make sense in the memory model.  A "release" operation is defined by a "synchronizes with" edge from the write.  A failed CAS has no associated write.  What does such a "release" operation mean?  Can someone propose a definition consistent with the current language spec?  I think we would have to change the spec to include an unconditional write, which seems quite contrived and misleading.  It also adds bogus data races to some code (though I don't know how common such code is).


  Remember that the current memory model says nothing about fences, which are in fact quite hard to define in a portable way. 


  I don't know of any natural examples where such "release" semantics for a failed CAS might matter, though clearly Justin successfully contrived one.  Since such failed operations have no effect on the shared state (again no write), it's really difficult for another thread to tell they completed and somehow try to rely on that fact.  Without that the release semantics are invisible.


  I agree that most conventional implementations provide some sort of fence-like semantics even in the failure case.  But I think the natural implementation on ARMv8 does not.  The code would do a load-exclusive-acquire, compare to the expected value, and just skip the release store if the comparison failed.  (I think the verdict is still out on whether this will eventually translate into real improved performance over fence-based models.  But I believe it reflects current programming advice for ARMv8.)


  The release; store; fence implementation of a volatile store in hotspot is actually overkill in two ways, neither of which are required by the Java memory model:


  1) Separating out the initial release as a fence orders prior operations with respect to ALL subsequent stores, not just the volatile store.  That's unnecessary.  Later non-volatile stores can advance past the volatile one without violating the memory model.  This affects ARMv8, Itanium, and compiler transformations.


  2) The fence orders all prior and later memory operations.  The only required ordering is between this particular volatile store and subsequent VOLATILE loads.  Those volatile loads are often a long ways off, and there is no reason to stall anything until you encounter one.  This affects ARMv8 and compiler transformations.  ARMv8 guarantees the correct ordering constraint with respect to only later acquire loads.  (Itanium's st.rel needs a trailing fence.)


  This implementation model also doesn't seem to correctly reflect store atomicity considerations.  You have to model a volatile load as load; acquire.  That works on Power if a "release" is an LWSYNC, fence is SYNC, but  "acquire" is also SYNC, which seems very weird.


  I think all major VMs currently do things along these lines.  But I don't think this is the right implementation model to strive for; it was a great initial approach that we should be trying to move away from.


  Hans


  On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Hans,

    Given the CAS is native we can insert anything we want to make this look like a volatile write happened, even if there is no actual write. Given a volatile write in hotspot has the effect of:

    release; store; fence

    then we can insert a fence after the CAS if needed. (This is why I'm still unsure the ARMv8 st.rel is sufficient for a volatile write in the general case.)

    I've gone back to check my archives on this and AFAICS the intent to indicate the effect of volatile write even on a failed (strong) CAS was deliberate.

    David
      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
      Sent: Wednesday, 21 January 2015 9:15 AM
      To: Justin Sampson
      Cc: Andrew Haley; Vitaly Davidovich; concurrency-interest at cs.oswego.edu; dholmes at ieee.org
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


      No disagreement about this benefitting from clarification.  But I still don't think a failed CAS can possibly have release/volatile write semantics, given that the compareAndSet definition states:

      "Atomically sets the value to the given updated value if the current value == the expected value." 


      Release/volatile write semantics only make sense if there was a write.   In the failure case there isn't.  (See 17.4.4: "A write to a volatile variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any thread (where "subsequent" is defined according to the synchronization order)."  There is no synchronizes with relationship involving volatiles unless there is a "write to a volatile variable".  A failed CAS fairly unambiguously doesn't write anything.)


      T2s CAS is not guaranteed to happen before T3s CAS because there is no volatile write in T2 which could possibly synchronize with anything in T3.


      Answer 2 is the only one that currently makes sense in the Java memory model.  To change that we would have to change the CAS definition so that it unconditionally writes something, possibly the original value.


      Hans


      On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com> wrote:

        I wrote:

        > Here's a practical question from the API user's perspective: Does
        > the reordering you described a few messages ago have any
        > observable effects in the behavior of a program relative to the
        > CAS being truly atomic?

        Okay, here's an example where it makes a difference. The atomicity
        issue is kind of secondary to the original question I raised, which
        was whether a failed CAS must have the memory effects of a volatile
        write. In Peter's example of a synchronized block implementation,
        roach motel reordering is allowed but there's still a memory release
        even in the case of failure. Indeed, a synchronized block is a
        perfect example of something that provides a very strong appearance
        of atomicity to the programmer even though it allows roach motel
        reordering. So by itself that's kind of a red herring. Instead,
        let's look at an example that gets at the combined effects of both
        issues:

        int a, b; // NOT volatile
        final AtomicInteger x = new AtomicInteger(0);

        T1:
        a = 1;
        r1 = x.compareAndSet(0, 1);

        T2:
        b = 1;
        r2 = x.compareAndSet(0, 1);

        T3:
        r3 = x.compareAndSet(1, 0);
        r4 = a;
        r5 = b;

        After joining T1, T2, & T3:
        r6 = x.get();

        Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
        possible values observed for r4 and r5?

        Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
        was the final update applied to x; and r2 = false means that T2's
        CAS saw the result of T1's CAS. (I've actually over-specified the
        givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)

        Answer 1: If every CAS is perfectly atomic _and_ has both memory
        acquire and memory release effects regardless of success or
        failure, then T1's and T2's CAS's both happen-before T3's CAS,
        such that r4 and r5 must both be true.

        Answer 2: If a failed CAS does _not_ have memory release effects,
        then T1's CAS happens-before T3's CAS but T2's CAS does _not_
        happen-before T3's CAS, so r4 must be true but r5 could be true or
        false (it's racy).

        Answer 3: If a failed CAS _does_ have memory release effects but
        each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
        not actually using a mutex), then things get interesting. T2's write
        to b might be reordered to right before the release barrier of its
        CAS and T3's read of b might be reordered to right after the acquire
        barrier of its CAS. Since the CAS's aren't perfectly atomic or
        mutually exclusive, T3's CAS could be executed between the load and
        the store of T2's CAS. T2's CAS would not be affected, because it's
        going to report failure either way. T3's CAS could then succeed, but
        with T3's read of b failing to see T2's write of b due to their
        reordering. Therefore in this case as well, r5 could be true or
        false.

        As I read the current docs, compareAndSet is most definitely
        described as an _atomic_ operation with _both_ volatile read and
        volatile write memory effects, supporting Answer 1, such that the
        example code above is _not_ racy.

        If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
        volatile write effects on failure, the example code above _is_ racy.

        Since there is a definite difference in the correctness implications
        of these two interpretations, and since _at least two_ compareAndSet
        implementations in j.u.c.atomic have the latter semantics anyway
        (AtomicStampedReference and AtomicMarkableReference), it seems worth
        clarifying in the docs.

        Cheers,
        Justin




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/3e68a4d1/attachment-0001.html>

From boehm at acm.org  Tue Jan 20 21:21:04 2015
From: boehm at acm.org (Hans Boehm)
Date: Tue, 20 Jan 2015 18:21:04 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>

On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  I'm struggling to follow this as we seem to be continually switching
> between different layers of abstraction. The spec says CAS acts as a
> volatile read plus volatile write.
>

That's not my reading, though I will admit it could be misinterpreted that
way. :-)


> So your mental model for this can simply be that the final action of the
> CAS is a write: either the new value if the CAS was successful, or the
> existing value if not. That's a conceptual model - no implementation need
> actually do an explicit write.
>

The CAS spec in the individual j.u.c.a classes says (admittedly not quite
as clearly as it could) that only a successful CAS writes anything:

"Atomically sets the value to the given updated value if the current value
== the expected value."

The top level j.u.c.a spec says:

"compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing
volatile variables."

which I read as "all read and write operations performed are volatile", not
as overriding the individual CAS specs to say that the variable is now
unconditionally written.

Officially requiring unconditional writes would have the unfortunate side
effect that e.g. code that "locks" an integer field by using a CAS to set a
bit, and then accesses the other bits with non-volatile operations, is now
officially no longer "correctly synchronized", since it "races" with CASes
in other threads that unsuccessfully try to set the lock bit. I would
really rather not say "CAS always writes the value, but implementors
shouldn't really implement it that way, and programmers and race detectors
should ignore the bogus data races introduced by our saying that".  We'd be
complicating the model, misleading both implementors and users, and
probably eventually slowing down implementations on a very important
machine architecture, for no real benefit.


>
> Each implementation then has to provide the necessary memory barriers to
> effect the conceptual model.
>

Except that not all architectures enforce ordering with fences/barriers.
Itanium and ARMv8 often use constraints on load and store instructions
instead.  Which are semantically different from fences/barriers.


>
> With regard to volatile read/write implementation in hotspot, yes it is
> stronger than needed. As has been discussed numerous times, at least in
> C1/interpreter/Unsafe hotspot doesn't consider the pairing of the loads and
> stores but only looks at the current load/store. So it has to emit the
> worst-case barriers needed.
>

We're not talking about any sort of more global analysis here.  The problem
is that even an entirely dumb compiler on ARMv8 or Itanium should translate
a volatile load to an acquire-load instruction, not a load plus some sort
of fence.

Hans

C2 may be more clever about this but I don't know - and wouldn't be
> surprised if it isn't.
>
> David
>
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
> *Sent:* Wednesday, 21 January 2015 11:07 AM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
> My primary concern with insisting on "release" semantics for a failed CAS
> is that it really doesn't make sense in the memory model.  A "release"
> operation is defined by a "synchronizes with" edge from the write.  A
> failed CAS has no associated write.  What does such a "release" operation
> mean?  Can someone propose a definition consistent with the current
> language spec?  I think we would have to change the spec to include an
> unconditional write, which seems quite contrived and misleading.  It also
> adds bogus data races to some code (though I don't know how common such
> code is).
>
> Remember that the current memory model says nothing about fences, which
> are in fact quite hard to define in a portable way.
>
> I don't know of any natural examples where such "release" semantics for a
> failed CAS might matter, though clearly Justin successfully contrived one.
> Since such failed operations have no effect on the shared state (again no
> write), it's really difficult for another thread to tell they completed and
> somehow try to rely on that fact.  Without that the release semantics are
> invisible.
>
> I agree that most conventional implementations provide some sort of
> fence-like semantics even in the failure case.  But I think the natural
> implementation on ARMv8 does not.  The code would do a
> load-exclusive-acquire, compare to the expected value, and just skip the
> release store if the comparison failed.  (I think the verdict is still out
> on whether this will eventually translate into real improved performance
> over fence-based models.  But I believe it reflects current programming
> advice for ARMv8.)
>
> The release; store; fence implementation of a volatile store in hotspot is
> actually overkill in two ways, neither of which are required by the Java
> memory model:
>
> 1) Separating out the initial release as a fence orders prior operations
> with respect to ALL subsequent stores, not just the volatile store.  That's
> unnecessary.  Later non-volatile stores can advance past the volatile one
> without violating the memory model.  This affects ARMv8, Itanium, and
> compiler transformations.
>
> 2) The fence orders all prior and later memory operations.  The only
> required ordering is between this particular volatile store and subsequent
> VOLATILE loads.  Those volatile loads are often a long ways off, and there
> is no reason to stall anything until you encounter one.  This affects ARMv8
> and compiler transformations.  ARMv8 guarantees the correct ordering
> constraint with respect to only later acquire loads.  (Itanium's st.rel
> needs a trailing fence.)
>
> This implementation model also doesn't seem to correctly reflect store
> atomicity considerations.  You have to model a volatile load as load;
> acquire.  That works on Power if a "release" is an LWSYNC, fence is SYNC,
> but  "acquire" is also SYNC, which seems very weird.
>
> I think all major VMs currently do things along these lines.  But I don't
> think this is the right implementation model to strive for; it was a great
> initial approach that we should be trying to move away from.
>
> Hans
>
> On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  Hans,
>>
>> Given the CAS is native we can insert anything we want to make this look
>> like a volatile write happened, even if there is no actual write. Given a
>> volatile write in hotspot has the effect of:
>>
>> release; store; fence
>>
>> then we can insert a fence after the CAS if needed. (This is why I'm
>> still unsure the ARMv8 st.rel is sufficient for a volatile write in the
>> general case.)
>>
>> I've gone back to check my archives on this and AFAICS the intent to
>> indicate the effect of volatile write even on a failed (strong) CAS was
>> deliberate.
>>
>> David
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>> *Sent:* Wednesday, 21 January 2015 9:15 AM
>> *To:* Justin Sampson
>> *Cc:* Andrew Haley; Vitaly Davidovich; concurrency-interest at cs.oswego.edu;
>> dholmes at ieee.org
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>  No disagreement about this benefitting from clarification.  But I still
>> don't think a failed CAS can possibly have release/volatile write
>> semantics, given that the compareAndSet definition states:
>>
>> "Atomically sets the value to the given updated value if the current
>> value == the expected value."
>>
>> Release/volatile write semantics only make sense if there was a write.
>> In the failure case there isn't.  (See 17.4.4: "A write to a volatile
>> variable v (?8.3.1.4) synchronizes-with all subsequent reads of v by any
>> thread (where "subsequent" is defined according to the synchronization
>> order)."  There is no synchronizes with relationship involving volatiles
>> unless there is a "write to a volatile variable".  A failed CAS fairly
>> unambiguously doesn't write anything.)
>>
>> T2s CAS is not guaranteed to happen before T3s CAS because there is no
>> volatile write in T2 which could possibly synchronize with anything in T3.
>>
>> Answer 2 is the only one that currently makes sense in the Java memory
>> model.  To change that we would have to change the CAS definition so that
>> it unconditionally writes something, possibly the original value.
>>
>> Hans
>>
>> On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>>
>>> I wrote:
>>>
>>> > Here's a practical question from the API user's perspective: Does
>>> > the reordering you described a few messages ago have any
>>> > observable effects in the behavior of a program relative to the
>>> > CAS being truly atomic?
>>>
>>> Okay, here's an example where it makes a difference. The atomicity
>>> issue is kind of secondary to the original question I raised, which
>>> was whether a failed CAS must have the memory effects of a volatile
>>> write. In Peter's example of a synchronized block implementation,
>>> roach motel reordering is allowed but there's still a memory release
>>> even in the case of failure. Indeed, a synchronized block is a
>>> perfect example of something that provides a very strong appearance
>>> of atomicity to the programmer even though it allows roach motel
>>> reordering. So by itself that's kind of a red herring. Instead,
>>> let's look at an example that gets at the combined effects of both
>>> issues:
>>>
>>> int a, b; // NOT volatile
>>> final AtomicInteger x = new AtomicInteger(0);
>>>
>>> T1:
>>> a = 1;
>>> r1 = x.compareAndSet(0, 1);
>>>
>>> T2:
>>> b = 1;
>>> r2 = x.compareAndSet(0, 1);
>>>
>>> T3:
>>> r3 = x.compareAndSet(1, 0);
>>> r4 = a;
>>> r5 = b;
>>>
>>> After joining T1, T2, & T3:
>>> r6 = x.get();
>>>
>>> Question: Given (r1,r2,r3,r6) = (true,false,true,0), what are the
>>> possible values observed for r4 and r5?
>>>
>>> Premise: Assuming no spurious failures, r6 = 0 means that T3's CAS
>>> was the final update applied to x; and r2 = false means that T2's
>>> CAS saw the result of T1's CAS. (I've actually over-specified the
>>> givens, because (r2,r6) = (false,0) implies (r1,r3) = (true,true).)
>>>
>>> Answer 1: If every CAS is perfectly atomic _and_ has both memory
>>> acquire and memory release effects regardless of success or
>>> failure, then T1's and T2's CAS's both happen-before T3's CAS,
>>> such that r4 and r5 must both be true.
>>>
>>> Answer 2: If a failed CAS does _not_ have memory release effects,
>>> then T1's CAS happens-before T3's CAS but T2's CAS does _not_
>>> happen-before T3's CAS, so r4 must be true but r5 could be true or
>>> false (it's racy).
>>>
>>> Answer 3: If a failed CAS _does_ have memory release effects but
>>> each CAS is _not_ perfectly atomic (i.e., roach motel reordering but
>>> not actually using a mutex), then things get interesting. T2's write
>>> to b might be reordered to right before the release barrier of its
>>> CAS and T3's read of b might be reordered to right after the acquire
>>> barrier of its CAS. Since the CAS's aren't perfectly atomic or
>>> mutually exclusive, T3's CAS could be executed between the load and
>>> the store of T2's CAS. T2's CAS would not be affected, because it's
>>> going to report failure either way. T3's CAS could then succeed, but
>>> with T3's read of b failing to see T2's write of b due to their
>>> reordering. Therefore in this case as well, r5 could be true or
>>> false.
>>>
>>> As I read the current docs, compareAndSet is most definitely
>>> described as an _atomic_ operation with _both_ volatile read and
>>> volatile write memory effects, supporting Answer 1, such that the
>>> example code above is _not_ racy.
>>>
>>> If CAS is allowed _either_ to be not-quite-atomic _or_ to not have
>>> volatile write effects on failure, the example code above _is_ racy.
>>>
>>> Since there is a definite difference in the correctness implications
>>> of these two interpretations, and since _at least two_ compareAndSet
>>> implementations in j.u.c.atomic have the latter semantics anyway
>>> (AtomicStampedReference and AtomicMarkableReference), it seems worth
>>> clarifying in the docs.
>>>
>>> Cheers,
>>> Justin
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/2ba208b1/attachment-0001.html>

From jsampson at guidewire.com  Tue Jan 20 21:33:14 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 02:33:14 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> Officially requiring unconditional writes would have the
> unfortunate side effect that e.g. code that "locks" an integer
> field by using a CAS to set a bit, and then accesses the other
> bits with non-volatile operations, is now officially no longer
> "correctly synchronized", since it "races" with CASes in other
> threads that unsuccessfully try to set the lock bit.

Wait, how can _more_ happens-before edges possibly lead to _more_
data races? Isn't the definition of a data race simply two
conflicting accesses that _aren't_ ordered by happens-before edges?

Cheers,
Justin



From jsampson at guidewire.com  Tue Jan 20 23:01:20 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 04:01:20 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>

I wrote:

> Hans Boehm wrote:
>
> > Officially requiring unconditional writes would have the
> > unfortunate side effect that e.g. code that "locks" an integer
> > field by using a CAS to set a bit, and then accesses the other
> > bits with non-volatile operations, is now officially no longer
> > "correctly synchronized", since it "races" with CASes in other
> > threads that unsuccessfully try to set the lock bit.
>
> Wait, how can _more_ happens-before edges possibly lead to _more_
> data races? Isn't the definition of a data race simply two
> conflicting accesses that _aren't_ ordered by happens-before
> edges?

Oh, I think I get what you're saying. You're talking about
non-volatile accesses to the same volatile variable used for the
CAS. Well, there's no "official" way to perform non-volatile
accesses on volatile variables anyway, so what do you care what's
"officially" required for the CAS? :) The JMM says, "A write to a
volatile variable v synchronizes-with all subsequent reads of v by
any thread" -- not just a _volatile_ write to a volatile variable
and a _volatile_ read of that variable, but _any_ write to a
volatile variable and _any_ read of that variable. The fact that you
can access a volatile variable with non-volatile semantics via
Unsafe is already a circumvention of the spec.

Besides, why the heck would you do something like that? Presuming
the "other bits" could be in any state when you try to set the lock
bit, you always have to do a volatile read before the CAS anyway. If
the lock bit is set, you don't even do the CAS, so all you've done
is a volatile read. And then if you do succeed in setting the lock
bit via CAS, why not just do all of the "non-volatile" operations in
local variables, saving the final bit values at the end in the same
volatile write that clears the lock bit? I'm truly curious. Is this
some low-level memory-mapped I/O register kind of situation?

Cheers,
Justin


From martinrb at google.com  Wed Jan 21 00:28:52 2015
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 20 Jan 2015 21:28:52 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
Message-ID: <CA+kOe09ik7nfEC8wT3v_1e5kwYhytrzH+Thhp6ARz_xxmEYxOA@mail.gmail.com>

The classic case for when you want to do a non-volatile access to a CAS'ed
variable is when piggy-backing on some other CAS, notably before enqueuing
a new Node via CAS, all its fields are in private memory, so do a relaxed
write.  after enqueuing, access the field via CAS.

On Tue, Jan 20, 2015 at 8:01 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> I wrote:
>
> > Hans Boehm wrote:
> >
> > > Officially requiring unconditional writes would have the
> > > unfortunate side effect that e.g. code that "locks" an integer
> > > field by using a CAS to set a bit, and then accesses the other
> > > bits with non-volatile operations, is now officially no longer
> > > "correctly synchronized", since it "races" with CASes in other
> > > threads that unsuccessfully try to set the lock bit.
> >
> > Wait, how can _more_ happens-before edges possibly lead to _more_
> > data races? Isn't the definition of a data race simply two
> > conflicting accesses that _aren't_ ordered by happens-before
> > edges?
>
> Oh, I think I get what you're saying. You're talking about
> non-volatile accesses to the same volatile variable used for the
> CAS. Well, there's no "official" way to perform non-volatile
> accesses on volatile variables anyway, so what do you care what's
> "officially" required for the CAS? :) The JMM says, "A write to a
> volatile variable v synchronizes-with all subsequent reads of v by
> any thread" -- not just a _volatile_ write to a volatile variable
> and a _volatile_ read of that variable, but _any_ write to a
> volatile variable and _any_ read of that variable. The fact that you
> can access a volatile variable with non-volatile semantics via
> Unsafe is already a circumvention of the spec.
>
> Besides, why the heck would you do something like that? Presuming
> the "other bits" could be in any state when you try to set the lock
> bit, you always have to do a volatile read before the CAS anyway. If
> the lock bit is set, you don't even do the CAS, so all you've done
> is a volatile read. And then if you do succeed in setting the lock
> bit via CAS, why not just do all of the "non-volatile" operations in
> local variables, saving the final bit values at the end in the same
> volatile write that clears the lock bit? I'm truly curious. Is this
> some low-level memory-mapped I/O register kind of situation?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150120/a2982986/attachment.html>

From jsampson at guidewire.com  Wed Jan 21 02:09:17 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 07:09:17 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CA+kOe09ik7nfEC8wT3v_1e5kwYhytrzH+Thhp6ARz_xxmEYxOA@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
	<CA+kOe09ik7nfEC8wT3v_1e5kwYhytrzH+Thhp6ARz_xxmEYxOA@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8C821@sm-ex-01-vm.guidewire.com>

Martin Buchholz wrote:

> The classic case for when you want to do a non-volatile access to
> a CAS'ed variable is when piggy-backing on some other CAS, notably
> before enqueuing a new Node via CAS, all its fields are in private
> memory, so do a relaxed write. after enqueuing, access the field
> via CAS.

Hi Martin! Yeah, that usage makes total sense to me. Even _that_ is
technically outside the formality of the JMM, but as long as the
object in question hasn't been published yet, non-volatile set-up
seems pretty reasonable. Hans was describing non-volatile
access to a volatile field that is already shared between threads,
which seems more dubious, but I'm curious about the details (unless
it's going to take us on too much of a tangent).

Cheers,
Justin


From TEREKHOV at de.ibm.com  Wed Jan 21 04:30:38 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 21 Jan 2015 10:30:38 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
Message-ID: <OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>

Hans Boehm <boehm at acm.org> wrote:
[...]
> We'd be complicating the model, misleading both implementors and users,
> and probably eventually slowing down implementations on a very important
> machine architecture, for no real benefit.

Well, with

"compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing
volatile variables."

IRIW would work without most heavy fences by using non-modifying RMWs
for loads 'as if stores are made atomically with respect to all threads
performing an RMW'.

That's kind of a benefit, oder?

regards,
alexander.

Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 03:21:04

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	David Holmes <dholmes at ieee.org>
cc:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Varieties of CAS semantics (another
       doc fix request)



On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:
      I'm struggling to follow this as we seem to be continually switching
      between different layers of abstraction. The spec says?CAS acts as a
      volatile read plus volatile write.

That's not my reading, though I will admit it could be misinterpreted that
way. :-)

      So your mental model for this can simply be that the final action of
      the CAS is a write: either the new value if the CAS was successful,
      or the existing value if not. That's a conceptual model -?no
      implementation?need actually do an explicit write.

The CAS spec in the individual j.u.c.a classes says (admittedly not quite
as clearly as it could) that only a successful CAS writes anything:

"Atomically sets the value to the given updated value if the current value
== the expected value."

The top level j.u.c.a spec says:

"compareAndSet and all other read-and-update operations such as
getAndIncrement have the memory effects of both reading and writing
volatile variables."

which I read as "all read and write operations performed are volatile", not
as overriding the individual CAS specs to say that the variable is now
unconditionally written.

Officially requiring unconditional writes would have the unfortunate side
effect that e.g. code that "locks" an integer field by using a CAS to set a
bit, and then accesses the other bits with non-volatile operations, is now
officially no longer "correctly synchronized", since it "races" with CASes
in other threads that unsuccessfully try to set the lock bit. I would
really rather not say "CAS always writes the value, but implementors
shouldn't really implement it that way, and programmers and race detectors
should ignore the bogus data races introduced by our saying that".? We'd be
complicating the model, misleading both implementors and users, and
probably eventually slowing down implementations on a very important
machine architecture, for no real benefit.


      Each implementation then has to provide the necessary memory barriers
      to effect the?conceptual model.

Except that not all architectures enforce ordering with fences/barriers.
Itanium and ARMv8 often use constraints on load and store instructions
instead.? Which are semantically different from fences/barriers.


      With regard to volatile read/write implementation in hotspot, yes it
      is stronger than needed. As has been discussed numerous times, at
      least in C1/interpreter/Unsafe hotspot doesn't consider the pairing
      of the loads and stores but only looks at the current load/store. So
      it has to emit the worst-case barriers needed.

We're not talking about any sort of more global analysis here.? The problem
is that even an entirely dumb compiler on ARMv8 or Itanium should translate
a volatile load to an acquire-load instruction, not a load plus some sort
of fence.

Hans

      C2 may be more clever about this but I don't know - and wouldn't be
      surprised if it isn't.

      David

      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:
      concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
      Sent: Wednesday, 21 January 2015 11:07 AM
      To: David Holmes
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics
      (another doc fix request)

      My primary concern with insisting on "release" semantics for a failed
      CAS is that it really doesn't make sense in the memory model.? A
      "release" operation is defined by a "synchronizes with" edge from the
      write.? A failed CAS has no associated write.? What does such a
      "release" operation mean?? Can someone propose a definition
      consistent with the current language spec?? I think we would have to
      change the spec to include an unconditional write, which seems quite
      contrived and misleading.? It also adds bogus data races to some code
      (though I don't know how common such code is).

      Remember that the current memory model says nothing about fences,
      which are in fact quite hard to define in a portable way.

      I don't know of any natural examples where such "release" semantics
      for a failed CAS might matter, though clearly Justin successfully
      contrived one.? Since such failed operations have no effect on the
      shared state (again no write), it's really difficult for another
      thread to tell they completed and somehow try to rely on that fact.
      Without that the release semantics are invisible.

      I agree that most conventional implementations provide some sort of
      fence-like semantics even in the failure case.? But I think the
      natural implementation on ARMv8 does not.? The code would do a
      load-exclusive-acquire, compare to the expected value, and just skip
      the release store if the comparison failed. ?(I think the verdict is
      still out on whether this will eventually translate into real
      improved performance over fence-based models.? But I believe it
      reflects current programming advice for ARMv8.)

      The release; store; fence implementation of a volatile store in
      hotspot is actually overkill in two ways, neither of which are
      required by the Java memory model:

      1) Separating out the initial release as a fence orders prior
      operations with respect to ALL subsequent stores, not just the
      volatile store.? That's unnecessary.? Later non-volatile stores can
      advance past the volatile one without violating the memory model.
      This affects ARMv8, Itanium, and compiler transformations.

      2) The fence orders all prior and later memory operations.? The only
      required ordering is between this particular volatile store and
      subsequent VOLATILE loads.? Those volatile loads are often a long
      ways off, and there is no reason to stall anything until you
      encounter one.? This affects ARMv8 and compiler transformations.
      ARMv8 guarantees the correct ordering constraint with respect to only
      later acquire loads. ?(Itanium's st.rel needs a trailing fence.)

      This implementation model also doesn't seem to correctly reflect
      store atomicity considerations.? You have to model a volatile load as
      load; acquire.? That works on Power if a "release" is an LWSYNC,
      fence is SYNC, but ?"acquire" is also SYNC, which seems very weird.

      I think all major VMs currently do things along these lines.? But I
      don't think this is the right implementation model to strive for; it
      was a great initial approach that we should be trying to move away
      from.

      Hans

      On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <
      davidcholmes at aapt.net.au> wrote:
            Hans,

            Given the CAS is native we can insert anything we want to make
            this look like a volatile write happened, even if there is no
            actual write. Given a volatile write in hotspot has the effect
            of:

            release; store; fence

            then we can insert a fence after the CAS if needed. (This is
            why I'm still unsure the ARMv8 st.rel is sufficient for a
            volatile write in the general case.)

            I've gone back to check my archives on this and AFAICS the
            intent to indicate the effect of volatile write even on a
            failed (strong) CAS was deliberate.

            David
             -----Original Message-----
             From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On
             Behalf Of Hans Boehm
             Sent: Wednesday, 21 January 2015 9:15 AM
             To: Justin Sampson
             Cc: Andrew Haley; Vitaly Davidovich;
             concurrency-interest at cs.oswego.edu; dholmes at ieee.org
             Subject: Re: [concurrency-interest] Varieties of CAS semantics
             (another doc fix request)

             No disagreement about this benefitting from clarification.
             But I still don't think a failed CAS can possibly have
             release/volatile write semantics, given that the compareAndSet
             definition states:

             "Atomically sets the value to the given updated value if the
             current value == the expected value."

             Release/volatile write semantics only make sense if there was
             a write. ? In the failure case there isn't. ?(See 17.4.4: "A
             write to a volatile variable v (?8.3.1.4) synchronizes-with
             all subsequent reads of v by any thread (where "subsequent" is
             defined according to the synchronization order)." ?There is no
             synchronizes with relationship involving volatiles unless
             there is a "write to a volatile variable".? A failed CAS
             fairly unambiguously doesn't write anything.)

             T2s CAS is not guaranteed to happen before T3s CAS because
             there is no volatile write in T2 which could possibly
             synchronize with anything in T3.

             Answer 2 is the only one that currently makes sense in the
             Java memory model.? To change that we would have to change the
             CAS definition so that it unconditionally writes something,
             possibly the original value.

             Hans

             On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <
             jsampson at guidewire.com> wrote:
                   I wrote:

                   > Here's a practical question from the API user's
                   perspective: Does
                   > the reordering you described a few messages ago have
                   any
                   > observable effects in the behavior of a program
                   relative to the
                   > CAS being truly atomic?

                   Okay, here's an example where it makes a difference. The
                   atomicity
                   issue is kind of secondary to the original question I
                   raised, which
                   was whether a failed CAS must have the memory effects of
                   a volatile
                   write. In Peter's example of a synchronized block
                   implementation,
                   roach motel reordering is allowed but there's still a
                   memory release
                   even in the case of failure. Indeed, a synchronized
                   block is a
                   perfect example of something that provides a very strong
                   appearance
                   of atomicity to the programmer even though it allows
                   roach motel
                   reordering. So by itself that's kind of a red herring.
                   Instead,
                   let's look at an example that gets at the combined
                   effects of both
                   issues:

                   int a, b; // NOT volatile
                   final AtomicInteger x = new AtomicInteger(0);

                   T1:
                   a = 1;
                   r1 = x.compareAndSet(0, 1);

                   T2:
                   b = 1;
                   r2 = x.compareAndSet(0, 1);

                   T3:
                   r3 = x.compareAndSet(1, 0);
                   r4 = a;
                   r5 = b;

                   After joining T1, T2, & T3:
                   r6 = x.get();

                   Question: Given (r1,r2,r3,r6) = (true,false,true,0),
                   what are the
                   possible values observed for r4 and r5?

                   Premise: Assuming no spurious failures, r6 = 0 means
                   that T3's CAS
                   was the final update applied to x; and r2 = false means
                   that T2's
                   CAS saw the result of T1's CAS. (I've actually
                   over-specified the
                   givens, because (r2,r6) = (false,0) implies (r1,r3) =
                   (true,true).)

                   Answer 1: If every CAS is perfectly atomic _and_ has
                   both memory
                   acquire and memory release effects regardless of success
                   or
                   failure, then T1's and T2's CAS's both happen-before
                   T3's CAS,
                   such that r4 and r5 must both be true.

                   Answer 2: If a failed CAS does _not_ have memory release
                   effects,
                   then T1's CAS happens-before T3's CAS but T2's CAS does
                   _not_
                   happen-before T3's CAS, so r4 must be true but r5 could
                   be true or
                   false (it's racy).

                   Answer 3: If a failed CAS _does_ have memory release
                   effects but
                   each CAS is _not_ perfectly atomic (i.e., roach motel
                   reordering but
                   not actually using a mutex), then things get
                   interesting. T2's write
                   to b might be reordered to right before the release
                   barrier of its
                   CAS and T3's read of b might be reordered to right after
                   the acquire
                   barrier of its CAS. Since the CAS's aren't perfectly
                   atomic or
                   mutually exclusive, T3's CAS could be executed between
                   the load and
                   the store of T2's CAS. T2's CAS would not be affected,
                   because it's
                   going to report failure either way. T3's CAS could then
                   succeed, but
                   with T3's read of b failing to see T2's write of b due
                   to their
                   reordering. Therefore in this case as well, r5 could be
                   true or
                   false.

                   As I read the current docs, compareAndSet is most
                   definitely
                   described as an _atomic_ operation with _both_ volatile
                   read and
                   volatile write memory effects, supporting Answer 1, such
                   that the
                   example code above is _not_ racy.

                   If CAS is allowed _either_ to be not-quite-atomic _or_
                   to not have
                   volatile write effects on failure, the example code
                   above _is_ racy.

                   Since there is a definite difference in the correctness
                   implications
                   of these two interpretations, and since _at least two_
                   compareAndSet
                   implementations in j.u.c.atomic have the latter
                   semantics anyway
                   (AtomicStampedReference and AtomicMarkableReference), it
                   seems worth
                   clarifying in the docs.

                   Cheers,
                   Justin


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From aph at redhat.com  Wed Jan 21 05:19:11 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 21 Jan 2015 10:19:11 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
Message-ID: <54BF7D1F.6010806@redhat.com>

On 21/01/15 01:29, David Holmes wrote:

> C2 may be more clever about this but I don't know - and wouldn't be
> surprised if it isn't.

This is handled in the back end, which has a choice about which
barriers to emit and instructions to use.  C2 emits a ton of barriers
and store release instructions in a belt-and-suspenders way but no
real implementation generates code for all of them AFAIK.

Andrew.


From aph at redhat.com  Wed Jan 21 05:23:58 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 21 Jan 2015 10:23:58 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <129D2A38-A415-4A10-AD3D-6D77FF055651@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>	<54B8399F.6000106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>	<54B84055.2000701@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>	<54B928DF.80106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>	<gmPL1p01B02hR!
	0p01mPNPG>	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>	<hqKe1p00m02hR0p01qKjmt>
	<129D2A38-A415-4A10-AD3D-6D77FF055651@cox.ne! t>
Message-ID: <54BF7E3E.7040303@redhat.com>

On 20/01/15 22:44, Gregg Wonderly wrote:

> There are just countless places where I have had to deal with all
> aspects of the different Java versions. I never deployed anything on
> 1.2 due to the huge number of problems in that release (that release
> pretty much killed the momentum of Java in the market place as a
> desktop environment, and that was a disastrous outcome for Java
> overall).  I?ve dealt with very careful tuning of multi-threaded
> applications, such as my broker, which had hundreds of interacting
> threads around a handful of data structures.  There are places where
> I was a bit reckless with synchronization, because the early JVMs
> didn?t show signs of caring about that.  This whole loop hoisting
> bit is one of the places where I knew that the values were not
> strictly synchronized.  But, I did not care, because eventually the
> change in the loop control variable was always visible.

Loop hoisting in the compiler is the least of your problems: the
hardware also does it for you.  You might have been able to get away
with it on x86, which goes to great lengths to automagically handle
cache coherency for you.

We're trying to write the next generation of high-performance software
for the next generation of high-performance hardware.  Newer
processors buffer writes to memory (sometimes for a very long time)
and need explicit synchronization for correctness.  They do this for
very good reasons to do with power consumption and performance: if you
have a many-core processor you need to minimize traffic associated
with managing cache coherency.  Ergo, volatile.  If we were to make
all field accesses sequentially consistent a many-core machine might
have so much traffic that it'd slow down to the speed of a few cores.
This would make Java unsuitable for programming those machines.

Sure, those of us who have been around the block a few times sometimes
wish that the world were as simple as we think it used to be.  But
people who do not know where to make variables volatile are unlikely
to write correct multi-threaded code which accesses shared state in
memory, even without needing volatile annotations.  Sure, they might
test it on x86 and declare it to be correct, but that's not the same
thing at all.

Andrew.

From boehm at acm.org  Wed Jan 21 11:40:47 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 08:40:47 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
Message-ID: <CAPUmR1ZDVSNYeH+dNuciVH9os76H=KUJM5Te95fTR3X5pe+yog@mail.gmail.com>

You're right that there is currently no sanctioned way to access the same
object both as a volatile and non-volatile.  But there seem to be good
reasons for supporting that.  E.g. the pointer/flag assignment in
double-checked locking doesn't race and hence could be non-volatile.  There
is ongoing discussion about better supporting this in C++, which already
has memory_order_relaxed.

(Does the constructor for e.g. AtomicInteger perform a volatile assignment?
The documentation appears unclear.  I would expect they do not, in which
case there is already a way to get simultaneous volatile and nonvolatile
access - communicate an AtomicInteger reference to another thread via a
non-volatile reference and access it another thread before construction is
visibly complete.  But that's not very useful.)

If you had a way to do that, it would be possible to update e.g. an
AtomicLong x along the following lines:

1. Atomically set (using CAS) a designated lock bit in x.  If it was
already set wait or give up.

2. If I succeed, I now "own" x, since nobody else can concurrently modify
it.

3. Repeatedly read (other parts of) x using non-volatile semantics.  Since
I own the lock bit, nobody else can concurrently modify it.  Thus these
accesses don't race.

4. Use a volatile store to clear the lock bit.

This develops a race if a failed CAS is viewed as writing the object.  I
don't know how useful or common that is.  (It may be if the lock bit
actually protects a larger object including x; the alternative would be to
explicitly pass the original value of x to everyone who needs to look at
the object.)  But the notion of intentionally mis-specifying an operation
to write when it really doesn't just seems wrong to me.

Hans

On Tue, Jan 20, 2015 at 8:01 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> I wrote:
>
> > Hans Boehm wrote:
> >
> > > Officially requiring unconditional writes would have the
> > > unfortunate side effect that e.g. code that "locks" an integer
> > > field by using a CAS to set a bit, and then accesses the other
> > > bits with non-volatile operations, is now officially no longer
> > > "correctly synchronized", since it "races" with CASes in other
> > > threads that unsuccessfully try to set the lock bit.
> >
> > Wait, how can _more_ happens-before edges possibly lead to _more_
> > data races? Isn't the definition of a data race simply two
> > conflicting accesses that _aren't_ ordered by happens-before
> > edges?
>
> Oh, I think I get what you're saying. You're talking about
> non-volatile accesses to the same volatile variable used for the
> CAS. Well, there's no "official" way to perform non-volatile
> accesses on volatile variables anyway, so what do you care what's
> "officially" required for the CAS? :) The JMM says, "A write to a
> volatile variable v synchronizes-with all subsequent reads of v by
> any thread" -- not just a _volatile_ write to a volatile variable
> and a _volatile_ read of that variable, but _any_ write to a
> volatile variable and _any_ read of that variable. The fact that you
> can access a volatile variable with non-volatile semantics via
> Unsafe is already a circumvention of the spec.
>
> Besides, why the heck would you do something like that? Presuming
> the "other bits" could be in any state when you try to set the lock
> bit, you always have to do a volatile read before the CAS anyway. If
> the lock bit is set, you don't even do the CAS, so all you've done
> is a volatile read. And then if you do succeed in setting the lock
> bit via CAS, why not just do all of the "non-volatile" operations in
> local variables, saving the final bit values at the end in the same
> volatile write that clears the lock bit? I'm truly curious. Is this
> some low-level memory-mapped I/O register kind of situation?
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/b1b8773f/attachment.html>

From boehm at acm.org  Wed Jan 21 11:53:02 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 08:53:02 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
Message-ID: <CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>

Alexander -

I don't understand the IRIW argument.  We currently promise to prevent the
IRIW for all volatiles, whether updated with CAS or not.  I think that
remains true in any of the formulations and with any of the proposed
implementations  (though it's less clear in the fence-based definitions).
ARMv8s release-store becomes visible at the same time to all other cores.
I don't think my formulation adds overhead anywhere.

Hans

On Wed, Jan 21, 2015 at 1:30 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> Hans Boehm <boehm at acm.org> wrote:
> [...]
> > We'd be complicating the model, misleading both implementors and users,
> > and probably eventually slowing down implementations on a very important
> > machine architecture, for no real benefit.
>
> Well, with
>
> "compareAndSet and all other read-and-update operations such as
> getAndIncrement have the memory effects of both reading and writing
> volatile variables."
>
> IRIW would work without most heavy fences by using non-modifying RMWs
> for loads 'as if stores are made atomically with respect to all threads
> performing an RMW'.
>
> That's kind of a benefit, oder?
>
> regards,
> alexander.
>
> Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 03:21:04
>
> Sent by:        concurrency-interest-bounces at cs.oswego.edu
>
>
> To:     David Holmes <dholmes at ieee.org>
> cc:     "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>
> Subject:        Re: [concurrency-interest] Varieties of CAS semantics
> (another
>        doc fix request)
>
>
>
> On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>       I'm struggling to follow this as we seem to be continually switching
>       between different layers of abstraction. The spec says CAS acts as a
>       volatile read plus volatile write.
>
> That's not my reading, though I will admit it could be misinterpreted that
> way. :-)
>
>       So your mental model for this can simply be that the final action of
>       the CAS is a write: either the new value if the CAS was successful,
>       or the existing value if not. That's a conceptual model - no
>       implementation need actually do an explicit write.
>
> The CAS spec in the individual j.u.c.a classes says (admittedly not quite
> as clearly as it could) that only a successful CAS writes anything:
>
> "Atomically sets the value to the given updated value if the current value
> == the expected value."
>
> The top level j.u.c.a spec says:
>
> "compareAndSet and all other read-and-update operations such as
> getAndIncrement have the memory effects of both reading and writing
> volatile variables."
>
> which I read as "all read and write operations performed are volatile", not
> as overriding the individual CAS specs to say that the variable is now
> unconditionally written.
>
> Officially requiring unconditional writes would have the unfortunate side
> effect that e.g. code that "locks" an integer field by using a CAS to set a
> bit, and then accesses the other bits with non-volatile operations, is now
> officially no longer "correctly synchronized", since it "races" with CASes
> in other threads that unsuccessfully try to set the lock bit. I would
> really rather not say "CAS always writes the value, but implementors
> shouldn't really implement it that way, and programmers and race detectors
> should ignore the bogus data races introduced by our saying that".  We'd be
> complicating the model, misleading both implementors and users, and
> probably eventually slowing down implementations on a very important
> machine architecture, for no real benefit.
>
>
>       Each implementation then has to provide the necessary memory barriers
>       to effect the conceptual model.
>
> Except that not all architectures enforce ordering with fences/barriers.
> Itanium and ARMv8 often use constraints on load and store instructions
> instead.  Which are semantically different from fences/barriers.
>
>
>       With regard to volatile read/write implementation in hotspot, yes it
>       is stronger than needed. As has been discussed numerous times, at
>       least in C1/interpreter/Unsafe hotspot doesn't consider the pairing
>       of the loads and stores but only looks at the current load/store. So
>       it has to emit the worst-case barriers needed.
>
> We're not talking about any sort of more global analysis here.  The problem
> is that even an entirely dumb compiler on ARMv8 or Itanium should translate
> a volatile load to an acquire-load instruction, not a load plus some sort
> of fence.
>
> Hans
>
>       C2 may be more clever about this but I don't know - and wouldn't be
>       surprised if it isn't.
>
>       David
>
>       -----Original Message-----
>       From: concurrency-interest-bounces at cs.oswego.edu [mailto:
>       concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
>       Sent: Wednesday, 21 January 2015 11:07 AM
>       To: David Holmes
>       Cc: concurrency-interest at cs.oswego.edu
>       Subject: Re: [concurrency-interest] Varieties of CAS semantics
>       (another doc fix request)
>
>       My primary concern with insisting on "release" semantics for a failed
>       CAS is that it really doesn't make sense in the memory model.  A
>       "release" operation is defined by a "synchronizes with" edge from the
>       write.  A failed CAS has no associated write.  What does such a
>       "release" operation mean?  Can someone propose a definition
>       consistent with the current language spec?  I think we would have to
>       change the spec to include an unconditional write, which seems quite
>       contrived and misleading.  It also adds bogus data races to some code
>       (though I don't know how common such code is).
>
>       Remember that the current memory model says nothing about fences,
>       which are in fact quite hard to define in a portable way.
>
>       I don't know of any natural examples where such "release" semantics
>       for a failed CAS might matter, though clearly Justin successfully
>       contrived one.  Since such failed operations have no effect on the
>       shared state (again no write), it's really difficult for another
>       thread to tell they completed and somehow try to rely on that fact.
>       Without that the release semantics are invisible.
>
>       I agree that most conventional implementations provide some sort of
>       fence-like semantics even in the failure case.  But I think the
>       natural implementation on ARMv8 does not.  The code would do a
>       load-exclusive-acquire, compare to the expected value, and just skip
>       the release store if the comparison failed.  (I think the verdict is
>       still out on whether this will eventually translate into real
>       improved performance over fence-based models.  But I believe it
>       reflects current programming advice for ARMv8.)
>
>       The release; store; fence implementation of a volatile store in
>       hotspot is actually overkill in two ways, neither of which are
>       required by the Java memory model:
>
>       1) Separating out the initial release as a fence orders prior
>       operations with respect to ALL subsequent stores, not just the
>       volatile store.  That's unnecessary.  Later non-volatile stores can
>       advance past the volatile one without violating the memory model.
>       This affects ARMv8, Itanium, and compiler transformations.
>
>       2) The fence orders all prior and later memory operations.  The only
>       required ordering is between this particular volatile store and
>       subsequent VOLATILE loads.  Those volatile loads are often a long
>       ways off, and there is no reason to stall anything until you
>       encounter one.  This affects ARMv8 and compiler transformations.
>       ARMv8 guarantees the correct ordering constraint with respect to only
>       later acquire loads.  (Itanium's st.rel needs a trailing fence.)
>
>       This implementation model also doesn't seem to correctly reflect
>       store atomicity considerations.  You have to model a volatile load as
>       load; acquire.  That works on Power if a "release" is an LWSYNC,
>       fence is SYNC, but  "acquire" is also SYNC, which seems very weird.
>
>       I think all major VMs currently do things along these lines.  But I
>       don't think this is the right implementation model to strive for; it
>       was a great initial approach that we should be trying to move away
>       from.
>
>       Hans
>
>       On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <
>       davidcholmes at aapt.net.au> wrote:
>             Hans,
>
>             Given the CAS is native we can insert anything we want to make
>             this look like a volatile write happened, even if there is no
>             actual write. Given a volatile write in hotspot has the effect
>             of:
>
>             release; store; fence
>
>             then we can insert a fence after the CAS if needed. (This is
>             why I'm still unsure the ARMv8 st.rel is sufficient for a
>             volatile write in the general case.)
>
>             I've gone back to check my archives on this and AFAICS the
>             intent to indicate the effect of volatile write even on a
>             failed (strong) CAS was deliberate.
>
>             David
>              -----Original Message-----
>              From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On
>              Behalf Of Hans Boehm
>              Sent: Wednesday, 21 January 2015 9:15 AM
>              To: Justin Sampson
>              Cc: Andrew Haley; Vitaly Davidovich;
>              concurrency-interest at cs.oswego.edu; dholmes at ieee.org
>              Subject: Re: [concurrency-interest] Varieties of CAS semantics
>              (another doc fix request)
>
>              No disagreement about this benefitting from clarification.
>              But I still don't think a failed CAS can possibly have
>              release/volatile write semantics, given that the compareAndSet
>              definition states:
>
>              "Atomically sets the value to the given updated value if the
>              current value == the expected value."
>
>              Release/volatile write semantics only make sense if there was
>              a write.   In the failure case there isn't.  (See 17.4.4: "A
>              write to a volatile variable v (?8.3.1.4) synchronizes-with
>              all subsequent reads of v by any thread (where "subsequent" is
>              defined according to the synchronization order)."  There is no
>              synchronizes with relationship involving volatiles unless
>              there is a "write to a volatile variable".  A failed CAS
>              fairly unambiguously doesn't write anything.)
>
>              T2s CAS is not guaranteed to happen before T3s CAS because
>              there is no volatile write in T2 which could possibly
>              synchronize with anything in T3.
>
>              Answer 2 is the only one that currently makes sense in the
>              Java memory model.  To change that we would have to change the
>              CAS definition so that it unconditionally writes something,
>              possibly the original value.
>
>              Hans
>
>              On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <
>              jsampson at guidewire.com> wrote:
>                    I wrote:
>
>                    > Here's a practical question from the API user's
>                    perspective: Does
>                    > the reordering you described a few messages ago have
>                    any
>                    > observable effects in the behavior of a program
>                    relative to the
>                    > CAS being truly atomic?
>
>                    Okay, here's an example where it makes a difference. The
>                    atomicity
>                    issue is kind of secondary to the original question I
>                    raised, which
>                    was whether a failed CAS must have the memory effects of
>                    a volatile
>                    write. In Peter's example of a synchronized block
>                    implementation,
>                    roach motel reordering is allowed but there's still a
>                    memory release
>                    even in the case of failure. Indeed, a synchronized
>                    block is a
>                    perfect example of something that provides a very strong
>                    appearance
>                    of atomicity to the programmer even though it allows
>                    roach motel
>                    reordering. So by itself that's kind of a red herring.
>                    Instead,
>                    let's look at an example that gets at the combined
>                    effects of both
>                    issues:
>
>                    int a, b; // NOT volatile
>                    final AtomicInteger x = new AtomicInteger(0);
>
>                    T1:
>                    a = 1;
>                    r1 = x.compareAndSet(0, 1);
>
>                    T2:
>                    b = 1;
>                    r2 = x.compareAndSet(0, 1);
>
>                    T3:
>                    r3 = x.compareAndSet(1, 0);
>                    r4 = a;
>                    r5 = b;
>
>                    After joining T1, T2, & T3:
>                    r6 = x.get();
>
>                    Question: Given (r1,r2,r3,r6) = (true,false,true,0),
>                    what are the
>                    possible values observed for r4 and r5?
>
>                    Premise: Assuming no spurious failures, r6 = 0 means
>                    that T3's CAS
>                    was the final update applied to x; and r2 = false means
>                    that T2's
>                    CAS saw the result of T1's CAS. (I've actually
>                    over-specified the
>                    givens, because (r2,r6) = (false,0) implies (r1,r3) =
>                    (true,true).)
>
>                    Answer 1: If every CAS is perfectly atomic _and_ has
>                    both memory
>                    acquire and memory release effects regardless of success
>                    or
>                    failure, then T1's and T2's CAS's both happen-before
>                    T3's CAS,
>                    such that r4 and r5 must both be true.
>
>                    Answer 2: If a failed CAS does _not_ have memory release
>                    effects,
>                    then T1's CAS happens-before T3's CAS but T2's CAS does
>                    _not_
>                    happen-before T3's CAS, so r4 must be true but r5 could
>                    be true or
>                    false (it's racy).
>
>                    Answer 3: If a failed CAS _does_ have memory release
>                    effects but
>                    each CAS is _not_ perfectly atomic (i.e., roach motel
>                    reordering but
>                    not actually using a mutex), then things get
>                    interesting. T2's write
>                    to b might be reordered to right before the release
>                    barrier of its
>                    CAS and T3's read of b might be reordered to right after
>                    the acquire
>                    barrier of its CAS. Since the CAS's aren't perfectly
>                    atomic or
>                    mutually exclusive, T3's CAS could be executed between
>                    the load and
>                    the store of T2's CAS. T2's CAS would not be affected,
>                    because it's
>                    going to report failure either way. T3's CAS could then
>                    succeed, but
>                    with T3's read of b failing to see T2's write of b due
>                    to their
>                    reordering. Therefore in this case as well, r5 could be
>                    true or
>                    false.
>
>                    As I read the current docs, compareAndSet is most
>                    definitely
>                    described as an _atomic_ operation with _both_ volatile
>                    read and
>                    volatile write memory effects, supporting Answer 1, such
>                    that the
>                    example code above is _not_ racy.
>
>                    If CAS is allowed _either_ to be not-quite-atomic _or_
>                    to not have
>                    volatile write effects on failure, the example code
>                    above _is_ racy.
>
>                    Since there is a definite difference in the correctness
>                    implications
>                    of these two interpretations, and since _at least two_
>                    compareAndSet
>                    implementations in j.u.c.atomic have the latter
>                    semantics anyway
>                    (AtomicStampedReference and AtomicMarkableReference), it
>                    seems worth
>                    clarifying in the docs.
>
>                    Cheers,
>                    Justin
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/4bf2b5c1/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Wed Jan 21 14:25:32 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Wed, 21 Jan 2015 19:25:32 +0000
Subject: [concurrency-interest] Spurious LockSupport.park() return?
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>

Hello there,

in the recent discussion about  unpark/park memory visibility, it was explained why LockSupport.park() is allowed to return spuriously.
It hasn?t become clear to me if that?s just semantics (we can?t forbid it), or whether it?s a real problem.

I am just now dealing with someone else?s code that behaves strangely, and the strangeness may be explained by park() returning
spuriously. My question is: Is that a hypothesis to be taken seriously, or is spurious unparking not a practical possibility?

FWIW: the structure of the code is like this:

Threads A and B:

for (;;) {
  LockSupport.park();
  synchronized (monitor) {
    if (mayRun) {
      queuedThreads.remove(); // remove this from the queue (**)
      mayRun = reconsider();
      break;
    }
    // otherwise park again and stay at the head of the queue
  }
}

Thread C:
LockSupport.unpark(queuedThreads.peek())

if A, B are in the queue, C wakes up A which is should remove itself from the queue (and later unpark B, etc.) However, if B is also unparked (spontaneously and at ca. the same time) and races ahead of A to remove A from the head of the queue and set mayRun to false, then A may never be unparked again, because it?s no longer in the queue.

Clearly, the line (**) is buggy because it does not check the assumptions that are made in the comments. But I find this scenario far-fetched. Could it really happen?



Sebastian Millies
PPM, Saarbr?cken C1.67, +49 681 210 3221


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/1b6f09c0/attachment.html>

From jsampson at guidewire.com  Wed Jan 21 14:48:22 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 19:48:22 +0000
Subject: [concurrency-interest] Spurious LockSupport.park() return?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CA78@sm-ex-01-vm.guidewire.com>

Sebastian Millies wrote:

> in the recent discussion about  unpark/park memory visibility, it
> was explained why LockSupport.park() is allowed to return
> spuriously. It hasn't become clear to me if that's just semantics
> (we can't forbid it), or whether it's a real problem.

There are lots of reasons that park() can appear to return
"spuriously," having to do with the timing of park and unpark. There
can be left-over unparks from earlier synchronization operations,
for example, which has to be allowed in order to implement locking
correctly (it's better to have too many unparks than too few!).
There can also be races between park() and unpark() themselves, in
which case park() errs on the side of returning spuriously rather
than risk missing an unpark().

> Clearly, the line (**) is buggy because it does not check the
> assumptions that are made in the comments. But I find this
> scenario far-fetched. Could it really happen?

If the code is buggy, it has to be fixed. :) It seems weird to be
using park/unpark in conjunction with a synchronized block. Why not
use higher-level concurrency utilities?

Cheers,
Justin


From dl at cs.oswego.edu  Wed Jan 21 14:48:54 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 21 Jan 2015 14:48:54 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54BD74C7.3020500@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>	<54BD0ED7.50907@cs.oswego.edu>	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>	<54BD2585.5040903@cs.oswego.edu>	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
	<54BD74C7.3020500@cs.oswego.edu>
Message-ID: <54C002A6.9060204@cs.oswego.edu>


Thanks to a few people putting up with daily API and functionality
changes to the in-progress SubmissionPublisher class, which is
slowly stabilizing enough that others might want to try it out
as well. Current javadoc at
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
And you can run with jdk8+ with -Xbootclasspath/p:jsr166.jar from
http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
For fun, I also placed a snapshot of one of my little internal
development perf tests at http://gee.cs.oswego.edu/dl/wwwtmp/SPL4.java

It shows throughput of a bunch of 3-stage flows. (The test doesn't
use JMH, to simplify transiently adding and removing internal
instrumentation while developing.)

-Doug



From david.lloyd at redhat.com  Wed Jan 21 14:58:18 2015
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 21 Jan 2015 13:58:18 -0600
Subject: [concurrency-interest] Spurious LockSupport.park() return?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>
Message-ID: <54C004DA.308@redhat.com>

Yes, you should always treat spurious return as a real possibility. 
It's not the implementation of park necessarily (though it may be that 
as well), so much as the fact that anyone can call unpark() on a thread 
for a wide variety of reasons, including there being a "leftover" 
unpark() "permit" from some previous user.

On 01/21/2015 01:25 PM, Millies, Sebastian wrote:
> Hello there,
>
> in the recent discussion about  unpark/park memory visibility, it was
> explained why LockSupport.park() is allowed to return spuriously.
>
> It hasn?t become clear to me if that?s just semantics (we can?t forbid
> it), or whether it?s a real problem.
>
> I am just now dealing with someone else?s code that behaves strangely,
> and the strangeness may be explained by park() returning
>
> spuriously. My question is: Is that a hypothesis to be taken seriously,
> or is spurious unparking not a practical possibility?
>
> FWIW: the structure of the code is like this:
>
> Threads A and B:
>
> for (;;) {
>
>    LockSupport.park();
>
>    synchronized (monitor) {
>
>      if (mayRun) {
>
>        queuedThreads.remove(); // remove this from the queue (**)
>
>        mayRun = reconsider();
>
>        break;
>
>      }
>
>      // otherwise park again and stay at the head of the queue
>
>    }
>
> }
>
> Thread C:
>
> LockSupport.unpark(queuedThreads.peek())
>
> if A, B are in the queue, C wakes up A which is should remove itself
> from the queue (and later unpark B, etc.) However, if B is also unparked
> (spontaneously and at ca. the same time) and races ahead of A to remove
> A from the head of the queue and set mayRun to false, then A may never
> be unparked again, because it?s no longer in the queue.
>
> Clearly, the line (**) is buggy because it does not check the
> assumptions that are made in the comments. But I find this scenario
> far-fetched. Could it really happen?
>
> *Sebastian Milli**es*
>
> PPM, Saarbr?cken C1.67, +49 681 210 3221
>
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com*
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-- 
- DML

From oleksandr.otenko at oracle.com  Wed Jan 21 15:35:50 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 21 Jan 2015 20:35:50 +0000
Subject: [concurrency-interest] Spurious LockSupport.park() return?
In-Reply-To: <54C004DA.308@redhat.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BA6E@HQMBX5.eur.ad.sag>
	<54C004DA.308@redhat.com>
Message-ID: <54C00DA6.9030305@oracle.com>

Not necessarily even a leftover unpark from previous user. It can be 
your own.

Here's why.

You can't do all of these atomically:

tell who to wake up (ie add yourself to wait list)
check condition does not hold yet
park()

I mean, of course, you could wrap the first two steps in synchronized, 
but then you don't get anything new on top of wait(), so we suppose we 
aren't using synchronized for some reason.

If you check condition, then tell who to wake up, then you can miss an 
unpark. If you tell who to wake up, then check the condition, then you 
can get a spare unpark.

Alex

On 21/01/2015 19:58, David M. Lloyd wrote:
> Yes, you should always treat spurious return as a real possibility. 
> It's not the implementation of park necessarily (though it may be that 
> as well), so much as the fact that anyone can call unpark() on a 
> thread for a wide variety of reasons, including there being a 
> "leftover" unpark() "permit" from some previous user.
>
> On 01/21/2015 01:25 PM, Millies, Sebastian wrote:
>> Hello there,
>>
>> in the recent discussion about  unpark/park memory visibility, it was
>> explained why LockSupport.park() is allowed to return spuriously.
>>
>> It hasn?t become clear to me if that?s just semantics (we can?t forbid
>> it), or whether it?s a real problem.
>>
>> I am just now dealing with someone else?s code that behaves strangely,
>> and the strangeness may be explained by park() returning
>>
>> spuriously. My question is: Is that a hypothesis to be taken seriously,
>> or is spurious unparking not a practical possibility?
>>
>> FWIW: the structure of the code is like this:
>>
>> Threads A and B:
>>
>> for (;;) {
>>
>>    LockSupport.park();
>>
>>    synchronized (monitor) {
>>
>>      if (mayRun) {
>>
>>        queuedThreads.remove(); // remove this from the queue (**)
>>
>>        mayRun = reconsider();
>>
>>        break;
>>
>>      }
>>
>>      // otherwise park again and stay at the head of the queue
>>
>>    }
>>
>> }
>>
>> Thread C:
>>
>> LockSupport.unpark(queuedThreads.peek())
>>
>> if A, B are in the queue, C wakes up A which is should remove itself
>> from the queue (and later unpark B, etc.) However, if B is also unparked
>> (spontaneously and at ca. the same time) and races ahead of A to remove
>> A from the head of the queue and set mayRun to false, then A may never
>> be unparked again, because it?s no longer in the queue.
>>
>> Clearly, the line (**) is buggy because it does not check the
>> assumptions that are made in the comments. But I find this scenario
>> far-fetched. Could it really happen?
>>
>> *Sebastian Milli**es*
>>
>> PPM, Saarbr?cken C1.67, +49 681 210 3221
>>
>>
>> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
>> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
>> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
>> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
>> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
>> Bereczky - *http://www.softwareag.com*
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>


From jsampson at guidewire.com  Wed Jan 21 15:41:28 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 20:41:28 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1ZDVSNYeH+dNuciVH9os76H=KUJM5Te95fTR3X5pe+yog@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZDVSNYeH+dNuciVH9os76H=KUJM5Te95fTR3X5pe+yog@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CAC0@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> You're right that there is currently no sanctioned way to access
> the same object both as a volatile and non-volatile. But there
> seem to be good reasons for supporting that. E.g. the pointer/flag
> assignment in double-checked locking doesn't race and hence could
> be non-volatile.

Do you mean that the write marked #2 below could be non-volatile,
while the read marked #1 is still volatile?

  private volatile Something instance;

  public Something getInstance() {
    if (instance == null) { // #1
      synchronized (this) {
        if (instance == null)
          instance = new Something(); // #2
      } // #3
    }
    return instance;
  }

If so, I don't think that works. Another thread calling getInstance
could still see the Something not fully initialized, if its line #1
executes between the writing thread's lines #2 and #3 (exiting the
monitor).

> (Does the constructor for e.g. AtomicInteger perform a volatile
> assignment? The documentation appears unclear. [...])

This the latest code in CVS:

  private volatile int value;

  /**
   * Creates a new AtomicInteger with the given initial value.
   *
   * @param initialValue the initial value
   */
  public AtomicInteger(int initialValue) {
    value = initialValue;
  }

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup

> But the notion of intentionally mis-specifying an operation to
> write when it really doesn't just seems wrong to me.

You and I are agreeing on this point. :)

But you also have to be prepared that the implementation _might_
actually do a write. Don't some CPU architectures implement CAS with
a store cycle to main memory regardless of success or failure, just
inserting the current value in the store buffer late in the cycle if
it didn't match the expected value? Therefore if your code is
actually sensitive to whether other threads are indeed writing that
memory location, it's going to be buggy on those architectures.

I'm having trouble imagining what could actually go wrong, though.
Is it possible that a no-op CAS would appear to undo non-volatile
changes in another thread? E.g.:

T1: r1 = x; // volatile read, sees 0
T1: x = 3; // non-volatile write, not flushed yet
T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
T1: r2 = x; // non-volatile read, could see either 3 or 0?

Cheers,
Justin


From TEREKHOV at de.ibm.com  Wed Jan 21 16:02:45 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 21 Jan 2015 22:02:45 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>	<OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
	<CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>
Message-ID: <OFE847C215.FFE41248-ONC1257DD4.00729225-C1257DD4.00739BFD@de.ibm.com>

err... I forgot that under Java it is not possible to do anything less
heavier than SC...

so what you want on powerpc is

volatile load: hwsync; ld; cmp; bd; isync
volatile store: hwsync; st
CAS: hwsync;_loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit: isync

right?

regards,
alexander.


Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 17:53:02

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>, David Holmes
       <dholmes at ieee.org>
Subject:	Re: [concurrency-interest] Varieties of CAS semantics (another
       doc fix request)


Alexander -

I don't understand the IRIW argument.? We currently promise to prevent the
IRIW for all volatiles, whether updated with CAS or not.? I think that
remains true in any of the formulations and with any of the proposed
implementations ?(though it's less clear in the fence-based definitions).
ARMv8s release-store becomes visible at the same time to all other cores.
I don't think my formulation adds overhead anywhere.

Hans

On Wed, Jan 21, 2015 at 1:30 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      Hans Boehm <boehm at acm.org> wrote:
      [...]
      > We'd be complicating the model, misleading both implementors and
      users,
      > and probably eventually slowing down implementations on a very
      important
      > machine architecture, for no real benefit.

      Well, with

      "compareAndSet and all other read-and-update operations such as
      getAndIncrement have the memory effects of both reading and writing
      volatile variables."

      IRIW would work without most heavy fences by using non-modifying RMWs
      for loads 'as if stores are made atomically with respect to all
      threads
      performing an RMW'.

      That's kind of a benefit, oder?

      regards,
      alexander.

      Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 03:21:04

      Sent by:? ? ? ? concurrency-interest-bounces at cs.oswego.edu


      To:? ? ?David Holmes <dholmes at ieee.org>
      cc:? ? ?"concurrency-interest at cs.oswego.edu"
      ? ? ? ?<concurrency-interest at cs.oswego.edu>
      Subject:? ? ? ? Re: [concurrency-interest] Varieties of CAS semantics
      (another
      ? ? ? ?doc fix request)



      On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <
      davidcholmes at aapt.net.au>
      wrote:
      ? ? ? I'm struggling to follow this as we seem to be continually
      switching
      ? ? ? between different layers of abstraction. The spec says?CAS acts
      as a
      ? ? ? volatile read plus volatile write.

      That's not my reading, though I will admit it could be misinterpreted
      that
      way. :-)

      ? ? ? So your mental model for this can simply be that the final
      action of
      ? ? ? the CAS is a write: either the new value if the CAS was
      successful,
      ? ? ? or the existing value if not. That's a conceptual model -?no
      ? ? ? implementation?need actually do an explicit write.

      The CAS spec in the individual j.u.c.a classes says (admittedly not
      quite
      as clearly as it could) that only a successful CAS writes anything:

      "Atomically sets the value to the given updated value if the current
      value
      == the expected value."

      The top level j.u.c.a spec says:

      "compareAndSet and all other read-and-update operations such as
      getAndIncrement have the memory effects of both reading and writing
      volatile variables."

      which I read as "all read and write operations performed are
      volatile", not
      as overriding the individual CAS specs to say that the variable is
      now
      unconditionally written.

      Officially requiring unconditional writes would have the unfortunate
      side
      effect that e.g. code that "locks" an integer field by using a CAS to
      set a
      bit, and then accesses the other bits with non-volatile operations,
      is now
      officially no longer "correctly synchronized", since it "races" with
      CASes
      in other threads that unsuccessfully try to set the lock bit. I would
      really rather not say "CAS always writes the value, but implementors
      shouldn't really implement it that way, and programmers and race
      detectors
      should ignore the bogus data races introduced by our saying that".
      We'd be
      complicating the model, misleading both implementors and users, and
      probably eventually slowing down implementations on a very important
      machine architecture, for no real benefit.


      ? ? ? Each implementation then has to provide the necessary memory
      barriers
      ? ? ? to effect the?conceptual model.

      Except that not all architectures enforce ordering with
      fences/barriers.
      Itanium and ARMv8 often use constraints on load and store
      instructions
      instead.? Which are semantically different from fences/barriers.


      ? ? ? With regard to volatile read/write implementation in hotspot,
      yes it
      ? ? ? is stronger than needed. As has been discussed numerous times,
      at
      ? ? ? least in C1/interpreter/Unsafe hotspot doesn't consider the
      pairing
      ? ? ? of the loads and stores but only looks at the current
      load/store. So
      ? ? ? it has to emit the worst-case barriers needed.

      We're not talking about any sort of more global analysis here.? The
      problem
      is that even an entirely dumb compiler on ARMv8 or Itanium should
      translate
      a volatile load to an acquire-load instruction, not a load plus some
      sort
      of fence.

      Hans

      ? ? ? C2 may be more clever about this but I don't know - and
      wouldn't be
      ? ? ? surprised if it isn't.

      ? ? ? David

      ? ? ? -----Original Message-----
      ? ? ? From: concurrency-interest-bounces at cs.oswego.edu [mailto:
      ? ? ? concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans
      Boehm
      ? ? ? Sent: Wednesday, 21 January 2015 11:07 AM
      ? ? ? To: David Holmes
      ? ? ? Cc: concurrency-interest at cs.oswego.edu
      ? ? ? Subject: Re: [concurrency-interest] Varieties of CAS semantics
      ? ? ? (another doc fix request)

      ? ? ? My primary concern with insisting on "release" semantics for a
      failed
      ? ? ? CAS is that it really doesn't make sense in the memory model.
      A
      ? ? ? "release" operation is defined by a "synchronizes with" edge
      from the
      ? ? ? write.? A failed CAS has no associated write.? What does such a
      ? ? ? "release" operation mean?? Can someone propose a definition
      ? ? ? consistent with the current language spec?? I think we would
      have to
      ? ? ? change the spec to include an unconditional write, which seems
      quite
      ? ? ? contrived and misleading.? It also adds bogus data races to
      some code
      ? ? ? (though I don't know how common such code is).

      ? ? ? Remember that the current memory model says nothing about
      fences,
      ? ? ? which are in fact quite hard to define in a portable way.

      ? ? ? I don't know of any natural examples where such "release"
      semantics
      ? ? ? for a failed CAS might matter, though clearly Justin
      successfully
      ? ? ? contrived one.? Since such failed operations have no effect on
      the
      ? ? ? shared state (again no write), it's really difficult for
      another
      ? ? ? thread to tell they completed and somehow try to rely on that
      fact.
      ? ? ? Without that the release semantics are invisible.

      ? ? ? I agree that most conventional implementations provide some
      sort of
      ? ? ? fence-like semantics even in the failure case.? But I think the
      ? ? ? natural implementation on ARMv8 does not.? The code would do a
      ? ? ? load-exclusive-acquire, compare to the expected value, and just
      skip
      ? ? ? the release store if the comparison failed. ?(I think the
      verdict is
      ? ? ? still out on whether this will eventually translate into real
      ? ? ? improved performance over fence-based models.? But I believe it
      ? ? ? reflects current programming advice for ARMv8.)

      ? ? ? The release; store; fence implementation of a volatile store in
      ? ? ? hotspot is actually overkill in two ways, neither of which are
      ? ? ? required by the Java memory model:

      ? ? ? 1) Separating out the initial release as a fence orders prior
      ? ? ? operations with respect to ALL subsequent stores, not just the
      ? ? ? volatile store.? That's unnecessary.? Later non-volatile stores
      can
      ? ? ? advance past the volatile one without violating the memory
      model.
      ? ? ? This affects ARMv8, Itanium, and compiler transformations.

      ? ? ? 2) The fence orders all prior and later memory operations.? The
      only
      ? ? ? required ordering is between this particular volatile store and
      ? ? ? subsequent VOLATILE loads.? Those volatile loads are often a
      long
      ? ? ? ways off, and there is no reason to stall anything until you
      ? ? ? encounter one.? This affects ARMv8 and compiler
      transformations.
      ? ? ? ARMv8 guarantees the correct ordering constraint with respect
      to only
      ? ? ? later acquire loads. ?(Itanium's st.rel needs a trailing
      fence.)

      ? ? ? This implementation model also doesn't seem to correctly
      reflect
      ? ? ? store atomicity considerations.? You have to model a volatile
      load as
      ? ? ? load; acquire.? That works on Power if a "release" is an
      LWSYNC,
      ? ? ? fence is SYNC, but ?"acquire" is also SYNC, which seems very
      weird.

      ? ? ? I think all major VMs currently do things along these lines.
      But I
      ? ? ? don't think this is the right implementation model to strive
      for; it
      ? ? ? was a great initial approach that we should be trying to move
      away
      ? ? ? from.

      ? ? ? Hans

      ? ? ? On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <
      ? ? ? davidcholmes at aapt.net.au> wrote:
      ? ? ? ? ? ? Hans,

      ? ? ? ? ? ? Given the CAS is native we can insert anything we want to
      make
      ? ? ? ? ? ? this look like a volatile write happened, even if there
      is no
      ? ? ? ? ? ? actual write. Given a volatile write in hotspot has the
      effect
      ? ? ? ? ? ? of:

      ? ? ? ? ? ? release; store; fence

      ? ? ? ? ? ? then we can insert a fence after the CAS if needed. (This
      is
      ? ? ? ? ? ? why I'm still unsure the ARMv8 st.rel is sufficient for a
      ? ? ? ? ? ? volatile write in the general case.)

      ? ? ? ? ? ? I've gone back to check my archives on this and AFAICS
      the
      ? ? ? ? ? ? intent to indicate the effect of volatile write even on a
      ? ? ? ? ? ? failed (strong) CAS was deliberate.

      ? ? ? ? ? ? David
      ? ? ? ? ? ? ?-----Original Message-----
      ? ? ? ? ? ? ?From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On
      ? ? ? ? ? ? ?Behalf Of Hans Boehm
      ? ? ? ? ? ? ?Sent: Wednesday, 21 January 2015 9:15 AM
      ? ? ? ? ? ? ?To: Justin Sampson
      ? ? ? ? ? ? ?Cc: Andrew Haley; Vitaly Davidovich;
      ? ? ? ? ? ? ?concurrency-interest at cs.oswego.edu; dholmes at ieee.org
      ? ? ? ? ? ? ?Subject: Re: [concurrency-interest] Varieties of CAS
      semantics
      ? ? ? ? ? ? ?(another doc fix request)

      ? ? ? ? ? ? ?No disagreement about this benefitting from
      clarification.
      ? ? ? ? ? ? ?But I still don't think a failed CAS can possibly have
      ? ? ? ? ? ? ?release/volatile write semantics, given that the
      compareAndSet
      ? ? ? ? ? ? ?definition states:

      ? ? ? ? ? ? ?"Atomically sets the value to the given updated value if
      the
      ? ? ? ? ? ? ?current value == the expected value."

      ? ? ? ? ? ? ?Release/volatile write semantics only make sense if
      there was
      ? ? ? ? ? ? ?a write. ? In the failure case there isn't. ?(See
      17.4.4: "A
      ? ? ? ? ? ? ?write to a volatile variable v (?8.3.1.4)
      synchronizes-with
      ? ? ? ? ? ? ?all subsequent reads of v by any thread (where
      "subsequent" is
      ? ? ? ? ? ? ?defined according to the synchronization order)." ?There
      is no
      ? ? ? ? ? ? ?synchronizes with relationship involving volatiles
      unless
      ? ? ? ? ? ? ?there is a "write to a volatile variable".? A failed CAS
      ? ? ? ? ? ? ?fairly unambiguously doesn't write anything.)

      ? ? ? ? ? ? ?T2s CAS is not guaranteed to happen before T3s CAS
      because
      ? ? ? ? ? ? ?there is no volatile write in T2 which could possibly
      ? ? ? ? ? ? ?synchronize with anything in T3.

      ? ? ? ? ? ? ?Answer 2 is the only one that currently makes sense in
      the
      ? ? ? ? ? ? ?Java memory model.? To change that we would have to
      change the
      ? ? ? ? ? ? ?CAS definition so that it unconditionally writes
      something,
      ? ? ? ? ? ? ?possibly the original value.

      ? ? ? ? ? ? ?Hans

      ? ? ? ? ? ? ?On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <
      ? ? ? ? ? ? ?jsampson at guidewire.com> wrote:
      ? ? ? ? ? ? ? ? ? ?I wrote:

      ? ? ? ? ? ? ? ? ? ?> Here's a practical question from the API user's
      ? ? ? ? ? ? ? ? ? ?perspective: Does
      ? ? ? ? ? ? ? ? ? ?> the reordering you described a few messages ago
      have
      ? ? ? ? ? ? ? ? ? ?any
      ? ? ? ? ? ? ? ? ? ?> observable effects in the behavior of a program
      ? ? ? ? ? ? ? ? ? ?relative to the
      ? ? ? ? ? ? ? ? ? ?> CAS being truly atomic?

      ? ? ? ? ? ? ? ? ? ?Okay, here's an example where it makes a
      difference. The
      ? ? ? ? ? ? ? ? ? ?atomicity
      ? ? ? ? ? ? ? ? ? ?issue is kind of secondary to the original
      question I
      ? ? ? ? ? ? ? ? ? ?raised, which
      ? ? ? ? ? ? ? ? ? ?was whether a failed CAS must have the memory
      effects of
      ? ? ? ? ? ? ? ? ? ?a volatile
      ? ? ? ? ? ? ? ? ? ?write. In Peter's example of a synchronized block
      ? ? ? ? ? ? ? ? ? ?implementation,
      ? ? ? ? ? ? ? ? ? ?roach motel reordering is allowed but there's
      still a
      ? ? ? ? ? ? ? ? ? ?memory release
      ? ? ? ? ? ? ? ? ? ?even in the case of failure. Indeed, a
      synchronized
      ? ? ? ? ? ? ? ? ? ?block is a
      ? ? ? ? ? ? ? ? ? ?perfect example of something that provides a very
      strong
      ? ? ? ? ? ? ? ? ? ?appearance
      ? ? ? ? ? ? ? ? ? ?of atomicity to the programmer even though it
      allows
      ? ? ? ? ? ? ? ? ? ?roach motel
      ? ? ? ? ? ? ? ? ? ?reordering. So by itself that's kind of a red
      herring.
      ? ? ? ? ? ? ? ? ? ?Instead,
      ? ? ? ? ? ? ? ? ? ?let's look at an example that gets at the combined
      ? ? ? ? ? ? ? ? ? ?effects of both
      ? ? ? ? ? ? ? ? ? ?issues:

      ? ? ? ? ? ? ? ? ? ?int a, b; // NOT volatile
      ? ? ? ? ? ? ? ? ? ?final AtomicInteger x = new AtomicInteger(0);

      ? ? ? ? ? ? ? ? ? ?T1:
      ? ? ? ? ? ? ? ? ? ?a = 1;
      ? ? ? ? ? ? ? ? ? ?r1 = x.compareAndSet(0, 1);

      ? ? ? ? ? ? ? ? ? ?T2:
      ? ? ? ? ? ? ? ? ? ?b = 1;
      ? ? ? ? ? ? ? ? ? ?r2 = x.compareAndSet(0, 1);

      ? ? ? ? ? ? ? ? ? ?T3:
      ? ? ? ? ? ? ? ? ? ?r3 = x.compareAndSet(1, 0);
      ? ? ? ? ? ? ? ? ? ?r4 = a;
      ? ? ? ? ? ? ? ? ? ?r5 = b;

      ? ? ? ? ? ? ? ? ? ?After joining T1, T2, & T3:
      ? ? ? ? ? ? ? ? ? ?r6 = x.get();

      ? ? ? ? ? ? ? ? ? ?Question: Given (r1,r2,r3,r6) =
      (true,false,true,0),
      ? ? ? ? ? ? ? ? ? ?what are the
      ? ? ? ? ? ? ? ? ? ?possible values observed for r4 and r5?

      ? ? ? ? ? ? ? ? ? ?Premise: Assuming no spurious failures, r6 = 0
      means
      ? ? ? ? ? ? ? ? ? ?that T3's CAS
      ? ? ? ? ? ? ? ? ? ?was the final update applied to x; and r2 = false
      means
      ? ? ? ? ? ? ? ? ? ?that T2's
      ? ? ? ? ? ? ? ? ? ?CAS saw the result of T1's CAS. (I've actually
      ? ? ? ? ? ? ? ? ? ?over-specified the
      ? ? ? ? ? ? ? ? ? ?givens, because (r2,r6) = (false,0) implies
      (r1,r3) =
      ? ? ? ? ? ? ? ? ? ?(true,true).)

      ? ? ? ? ? ? ? ? ? ?Answer 1: If every CAS is perfectly atomic _and_
      has
      ? ? ? ? ? ? ? ? ? ?both memory
      ? ? ? ? ? ? ? ? ? ?acquire and memory release effects regardless of
      success
      ? ? ? ? ? ? ? ? ? ?or
      ? ? ? ? ? ? ? ? ? ?failure, then T1's and T2's CAS's both
      happen-before
      ? ? ? ? ? ? ? ? ? ?T3's CAS,
      ? ? ? ? ? ? ? ? ? ?such that r4 and r5 must both be true.

      ? ? ? ? ? ? ? ? ? ?Answer 2: If a failed CAS does _not_ have memory
      release
      ? ? ? ? ? ? ? ? ? ?effects,
      ? ? ? ? ? ? ? ? ? ?then T1's CAS happens-before T3's CAS but T2's CAS
      does
      ? ? ? ? ? ? ? ? ? ?_not_
      ? ? ? ? ? ? ? ? ? ?happen-before T3's CAS, so r4 must be true but r5
      could
      ? ? ? ? ? ? ? ? ? ?be true or
      ? ? ? ? ? ? ? ? ? ?false (it's racy).

      ? ? ? ? ? ? ? ? ? ?Answer 3: If a failed CAS _does_ have memory
      release
      ? ? ? ? ? ? ? ? ? ?effects but
      ? ? ? ? ? ? ? ? ? ?each CAS is _not_ perfectly atomic (i.e., roach
      motel
      ? ? ? ? ? ? ? ? ? ?reordering but
      ? ? ? ? ? ? ? ? ? ?not actually using a mutex), then things get
      ? ? ? ? ? ? ? ? ? ?interesting. T2's write
      ? ? ? ? ? ? ? ? ? ?to b might be reordered to right before the
      release
      ? ? ? ? ? ? ? ? ? ?barrier of its
      ? ? ? ? ? ? ? ? ? ?CAS and T3's read of b might be reordered to right
      after
      ? ? ? ? ? ? ? ? ? ?the acquire
      ? ? ? ? ? ? ? ? ? ?barrier of its CAS. Since the CAS's aren't
      perfectly
      ? ? ? ? ? ? ? ? ? ?atomic or
      ? ? ? ? ? ? ? ? ? ?mutually exclusive, T3's CAS could be executed
      between
      ? ? ? ? ? ? ? ? ? ?the load and
      ? ? ? ? ? ? ? ? ? ?the store of T2's CAS. T2's CAS would not be
      affected,
      ? ? ? ? ? ? ? ? ? ?because it's
      ? ? ? ? ? ? ? ? ? ?going to report failure either way. T3's CAS could
      then
      ? ? ? ? ? ? ? ? ? ?succeed, but
      ? ? ? ? ? ? ? ? ? ?with T3's read of b failing to see T2's write of b
      due
      ? ? ? ? ? ? ? ? ? ?to their
      ? ? ? ? ? ? ? ? ? ?reordering. Therefore in this case as well, r5
      could be
      ? ? ? ? ? ? ? ? ? ?true or
      ? ? ? ? ? ? ? ? ? ?false.

      ? ? ? ? ? ? ? ? ? ?As I read the current docs, compareAndSet is most
      ? ? ? ? ? ? ? ? ? ?definitely
      ? ? ? ? ? ? ? ? ? ?described as an _atomic_ operation with _both_
      volatile
      ? ? ? ? ? ? ? ? ? ?read and
      ? ? ? ? ? ? ? ? ? ?volatile write memory effects, supporting Answer
      1, such
      ? ? ? ? ? ? ? ? ? ?that the
      ? ? ? ? ? ? ? ? ? ?example code above is _not_ racy.

      ? ? ? ? ? ? ? ? ? ?If CAS is allowed _either_ to be not-quite-atomic
      _or_
      ? ? ? ? ? ? ? ? ? ?to not have
      ? ? ? ? ? ? ? ? ? ?volatile write effects on failure, the example
      code
      ? ? ? ? ? ? ? ? ? ?above _is_ racy.

      ? ? ? ? ? ? ? ? ? ?Since there is a definite difference in the
      correctness
      ? ? ? ? ? ? ? ? ? ?implications
      ? ? ? ? ? ? ? ? ? ?of these two interpretations, and since _at least
      two_
      ? ? ? ? ? ? ? ? ? ?compareAndSet
      ? ? ? ? ? ? ? ? ? ?implementations in j.u.c.atomic have the latter
      ? ? ? ? ? ? ? ? ? ?semantics anyway
      ? ? ? ? ? ? ? ? ? ?(AtomicStampedReference and
      AtomicMarkableReference), it
      ? ? ? ? ? ? ? ? ? ?seems worth
      ? ? ? ? ? ? ? ? ? ?clarifying in the docs.

      ? ? ? ? ? ? ? ? ? ?Cheers,
      ? ? ? ? ? ? ? ? ? ?Justin


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From boehm at acm.org  Wed Jan 21 16:48:48 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 13:48:48 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <OFE847C215.FFE41248-ONC1257DD4.00729225-C1257DD4.00739BFD@de.ibm.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
	<CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>
	<OFE847C215.FFE41248-ONC1257DD4.00729225-C1257DD4.00739BFD@de.ibm.com>
Message-ID: <CAPUmR1Yt-T=w0iGh9GGFdGgAdVhxZnHX5QVuASupi_dgMYPmUQ@mail.gmail.com>

Yes, your recommendation mirrors that for C++ seq_cst operations in
http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html, and I think those
are the semi-official IBM recommendations.  The Java and C++ mappings
should be the same to ensure correct interoperation.

(I continue to be surprised that the choice was made to use two fences on a
load instead of a store.  I think either choice is correct, so long as it's
used consistently.)

On Wed, Jan 21, 2015 at 1:02 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> err... I forgot that under Java it is not possible to do anything less
> heavier than SC...
>
> so what you want on powerpc is
>
> volatile load: hwsync; ld; cmp; bd; isync
> volatile store: hwsync; st
> CAS: hwsync;_loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit: isync
>
> right?
>
> regards,
> alexander.
>
>
> Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 17:53:02
>
> Sent by:        concurrency-interest-bounces at cs.oswego.edu
>
>
> To:     Alexander Terekhov/Germany/IBM at IBMDE
> cc:     "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>, David Holmes
>        <dholmes at ieee.org>
> Subject:        Re: [concurrency-interest] Varieties of CAS semantics
> (another
>        doc fix request)
>
>
> Alexander -
>
> I don't understand the IRIW argument.  We currently promise to prevent the
> IRIW for all volatiles, whether updated with CAS or not.  I think that
> remains true in any of the formulations and with any of the proposed
> implementations  (though it's less clear in the fence-based definitions).
> ARMv8s release-store becomes visible at the same time to all other cores.
> I don't think my formulation adds overhead anywhere.
>
> Hans
>
> On Wed, Jan 21, 2015 at 1:30 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
> wrote:
>       Hans Boehm <boehm at acm.org> wrote:
>       [...]
>       > We'd be complicating the model, misleading both implementors and
>       users,
>       > and probably eventually slowing down implementations on a very
>       important
>       > machine architecture, for no real benefit.
>
>       Well, with
>
>       "compareAndSet and all other read-and-update operations such as
>       getAndIncrement have the memory effects of both reading and writing
>       volatile variables."
>
>       IRIW would work without most heavy fences by using non-modifying RMWs
>       for loads 'as if stores are made atomically with respect to all
>       threads
>       performing an RMW'.
>
>       That's kind of a benefit, oder?
>
>       regards,
>       alexander.
>
>       Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 03:21:04
>
>       Sent by:        concurrency-interest-bounces at cs.oswego.edu
>
>
>       To:     David Holmes <dholmes at ieee.org>
>       cc:     "concurrency-interest at cs.oswego.edu"
>              <concurrency-interest at cs.oswego.edu>
>       Subject:        Re: [concurrency-interest] Varieties of CAS semantics
>       (another
>              doc fix request)
>
>
>
>       On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <
>       davidcholmes at aapt.net.au>
>       wrote:
>             I'm struggling to follow this as we seem to be continually
>       switching
>             between different layers of abstraction. The spec says CAS acts
>       as a
>             volatile read plus volatile write.
>
>       That's not my reading, though I will admit it could be misinterpreted
>       that
>       way. :-)
>
>             So your mental model for this can simply be that the final
>       action of
>             the CAS is a write: either the new value if the CAS was
>       successful,
>             or the existing value if not. That's a conceptual model - no
>             implementation need actually do an explicit write.
>
>       The CAS spec in the individual j.u.c.a classes says (admittedly not
>       quite
>       as clearly as it could) that only a successful CAS writes anything:
>
>       "Atomically sets the value to the given updated value if the current
>       value
>       == the expected value."
>
>       The top level j.u.c.a spec says:
>
>       "compareAndSet and all other read-and-update operations such as
>       getAndIncrement have the memory effects of both reading and writing
>       volatile variables."
>
>       which I read as "all read and write operations performed are
>       volatile", not
>       as overriding the individual CAS specs to say that the variable is
>       now
>       unconditionally written.
>
>       Officially requiring unconditional writes would have the unfortunate
>       side
>       effect that e.g. code that "locks" an integer field by using a CAS to
>       set a
>       bit, and then accesses the other bits with non-volatile operations,
>       is now
>       officially no longer "correctly synchronized", since it "races" with
>       CASes
>       in other threads that unsuccessfully try to set the lock bit. I would
>       really rather not say "CAS always writes the value, but implementors
>       shouldn't really implement it that way, and programmers and race
>       detectors
>       should ignore the bogus data races introduced by our saying that".
>       We'd be
>       complicating the model, misleading both implementors and users, and
>       probably eventually slowing down implementations on a very important
>       machine architecture, for no real benefit.
>
>
>             Each implementation then has to provide the necessary memory
>       barriers
>             to effect the conceptual model.
>
>       Except that not all architectures enforce ordering with
>       fences/barriers.
>       Itanium and ARMv8 often use constraints on load and store
>       instructions
>       instead.  Which are semantically different from fences/barriers.
>
>
>             With regard to volatile read/write implementation in hotspot,
>       yes it
>             is stronger than needed. As has been discussed numerous times,
>       at
>             least in C1/interpreter/Unsafe hotspot doesn't consider the
>       pairing
>             of the loads and stores but only looks at the current
>       load/store. So
>             it has to emit the worst-case barriers needed.
>
>       We're not talking about any sort of more global analysis here.  The
>       problem
>       is that even an entirely dumb compiler on ARMv8 or Itanium should
>       translate
>       a volatile load to an acquire-load instruction, not a load plus some
>       sort
>       of fence.
>
>       Hans
>
>             C2 may be more clever about this but I don't know - and
>       wouldn't be
>             surprised if it isn't.
>
>             David
>
>             -----Original Message-----
>             From: concurrency-interest-bounces at cs.oswego.edu [mailto:
>             concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans
>       Boehm
>             Sent: Wednesday, 21 January 2015 11:07 AM
>             To: David Holmes
>             Cc: concurrency-interest at cs.oswego.edu
>             Subject: Re: [concurrency-interest] Varieties of CAS semantics
>             (another doc fix request)
>
>             My primary concern with insisting on "release" semantics for a
>       failed
>             CAS is that it really doesn't make sense in the memory model.
>       A
>             "release" operation is defined by a "synchronizes with" edge
>       from the
>             write.  A failed CAS has no associated write.  What does such a
>             "release" operation mean?  Can someone propose a definition
>             consistent with the current language spec?  I think we would
>       have to
>             change the spec to include an unconditional write, which seems
>       quite
>             contrived and misleading.  It also adds bogus data races to
>       some code
>             (though I don't know how common such code is).
>
>             Remember that the current memory model says nothing about
>       fences,
>             which are in fact quite hard to define in a portable way.
>
>             I don't know of any natural examples where such "release"
>       semantics
>             for a failed CAS might matter, though clearly Justin
>       successfully
>             contrived one.  Since such failed operations have no effect on
>       the
>             shared state (again no write), it's really difficult for
>       another
>             thread to tell they completed and somehow try to rely on that
>       fact.
>             Without that the release semantics are invisible.
>
>             I agree that most conventional implementations provide some
>       sort of
>             fence-like semantics even in the failure case.  But I think the
>             natural implementation on ARMv8 does not.  The code would do a
>             load-exclusive-acquire, compare to the expected value, and just
>       skip
>             the release store if the comparison failed.  (I think the
>       verdict is
>             still out on whether this will eventually translate into real
>             improved performance over fence-based models.  But I believe it
>             reflects current programming advice for ARMv8.)
>
>             The release; store; fence implementation of a volatile store in
>             hotspot is actually overkill in two ways, neither of which are
>             required by the Java memory model:
>
>             1) Separating out the initial release as a fence orders prior
>             operations with respect to ALL subsequent stores, not just the
>             volatile store.  That's unnecessary.  Later non-volatile stores
>       can
>             advance past the volatile one without violating the memory
>       model.
>             This affects ARMv8, Itanium, and compiler transformations.
>
>             2) The fence orders all prior and later memory operations.  The
>       only
>             required ordering is between this particular volatile store and
>             subsequent VOLATILE loads.  Those volatile loads are often a
>       long
>             ways off, and there is no reason to stall anything until you
>             encounter one.  This affects ARMv8 and compiler
>       transformations.
>             ARMv8 guarantees the correct ordering constraint with respect
>       to only
>             later acquire loads.  (Itanium's st.rel needs a trailing
>       fence.)
>
>             This implementation model also doesn't seem to correctly
>       reflect
>             store atomicity considerations.  You have to model a volatile
>       load as
>             load; acquire.  That works on Power if a "release" is an
>       LWSYNC,
>             fence is SYNC, but  "acquire" is also SYNC, which seems very
>       weird.
>
>             I think all major VMs currently do things along these lines.
>       But I
>             don't think this is the right implementation model to strive
>       for; it
>             was a great initial approach that we should be trying to move
>       away
>             from.
>
>             Hans
>
>             On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <
>             davidcholmes at aapt.net.au> wrote:
>                   Hans,
>
>                   Given the CAS is native we can insert anything we want to
>       make
>                   this look like a volatile write happened, even if there
>       is no
>                   actual write. Given a volatile write in hotspot has the
>       effect
>                   of:
>
>                   release; store; fence
>
>                   then we can insert a fence after the CAS if needed. (This
>       is
>                   why I'm still unsure the ARMv8 st.rel is sufficient for a
>                   volatile write in the general case.)
>
>                   I've gone back to check my archives on this and AFAICS
>       the
>                   intent to indicate the effect of volatile write even on a
>                   failed (strong) CAS was deliberate.
>
>                   David
>                    -----Original Message-----
>                    From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com
> ]On
>                    Behalf Of Hans Boehm
>                    Sent: Wednesday, 21 January 2015 9:15 AM
>                    To: Justin Sampson
>                    Cc: Andrew Haley; Vitaly Davidovich;
>                    concurrency-interest at cs.oswego.edu; dholmes at ieee.org
>                    Subject: Re: [concurrency-interest] Varieties of CAS
>       semantics
>                    (another doc fix request)
>
>                    No disagreement about this benefitting from
>       clarification.
>                    But I still don't think a failed CAS can possibly have
>                    release/volatile write semantics, given that the
>       compareAndSet
>                    definition states:
>
>                    "Atomically sets the value to the given updated value if
>       the
>                    current value == the expected value."
>
>                    Release/volatile write semantics only make sense if
>       there was
>                    a write.   In the failure case there isn't.  (See
>       17.4.4: "A
>                    write to a volatile variable v (?8.3.1.4)
>       synchronizes-with
>                    all subsequent reads of v by any thread (where
>       "subsequent" is
>                    defined according to the synchronization order)."  There
>       is no
>                    synchronizes with relationship involving volatiles
>       unless
>                    there is a "write to a volatile variable".  A failed CAS
>                    fairly unambiguously doesn't write anything.)
>
>                    T2s CAS is not guaranteed to happen before T3s CAS
>       because
>                    there is no volatile write in T2 which could possibly
>                    synchronize with anything in T3.
>
>                    Answer 2 is the only one that currently makes sense in
>       the
>                    Java memory model.  To change that we would have to
>       change the
>                    CAS definition so that it unconditionally writes
>       something,
>                    possibly the original value.
>
>                    Hans
>
>                    On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <
>                    jsampson at guidewire.com> wrote:
>                          I wrote:
>
>                          > Here's a practical question from the API user's
>                          perspective: Does
>                          > the reordering you described a few messages ago
>       have
>                          any
>                          > observable effects in the behavior of a program
>                          relative to the
>                          > CAS being truly atomic?
>
>                          Okay, here's an example where it makes a
>       difference. The
>                          atomicity
>                          issue is kind of secondary to the original
>       question I
>                          raised, which
>                          was whether a failed CAS must have the memory
>       effects of
>                          a volatile
>                          write. In Peter's example of a synchronized block
>                          implementation,
>                          roach motel reordering is allowed but there's
>       still a
>                          memory release
>                          even in the case of failure. Indeed, a
>       synchronized
>                          block is a
>                          perfect example of something that provides a very
>       strong
>                          appearance
>                          of atomicity to the programmer even though it
>       allows
>                          roach motel
>                          reordering. So by itself that's kind of a red
>       herring.
>                          Instead,
>                          let's look at an example that gets at the combined
>                          effects of both
>                          issues:
>
>                          int a, b; // NOT volatile
>                          final AtomicInteger x = new AtomicInteger(0);
>
>                          T1:
>                          a = 1;
>                          r1 = x.compareAndSet(0, 1);
>
>                          T2:
>                          b = 1;
>                          r2 = x.compareAndSet(0, 1);
>
>                          T3:
>                          r3 = x.compareAndSet(1, 0);
>                          r4 = a;
>                          r5 = b;
>
>                          After joining T1, T2, & T3:
>                          r6 = x.get();
>
>                          Question: Given (r1,r2,r3,r6) =
>       (true,false,true,0),
>                          what are the
>                          possible values observed for r4 and r5?
>
>                          Premise: Assuming no spurious failures, r6 = 0
>       means
>                          that T3's CAS
>                          was the final update applied to x; and r2 = false
>       means
>                          that T2's
>                          CAS saw the result of T1's CAS. (I've actually
>                          over-specified the
>                          givens, because (r2,r6) = (false,0) implies
>       (r1,r3) =
>                          (true,true).)
>
>                          Answer 1: If every CAS is perfectly atomic _and_
>       has
>                          both memory
>                          acquire and memory release effects regardless of
>       success
>                          or
>                          failure, then T1's and T2's CAS's both
>       happen-before
>                          T3's CAS,
>                          such that r4 and r5 must both be true.
>
>                          Answer 2: If a failed CAS does _not_ have memory
>       release
>                          effects,
>                          then T1's CAS happens-before T3's CAS but T2's CAS
>       does
>                          _not_
>                          happen-before T3's CAS, so r4 must be true but r5
>       could
>                          be true or
>                          false (it's racy).
>
>                          Answer 3: If a failed CAS _does_ have memory
>       release
>                          effects but
>                          each CAS is _not_ perfectly atomic (i.e., roach
>       motel
>                          reordering but
>                          not actually using a mutex), then things get
>                          interesting. T2's write
>                          to b might be reordered to right before the
>       release
>                          barrier of its
>                          CAS and T3's read of b might be reordered to right
>       after
>                          the acquire
>                          barrier of its CAS. Since the CAS's aren't
>       perfectly
>                          atomic or
>                          mutually exclusive, T3's CAS could be executed
>       between
>                          the load and
>                          the store of T2's CAS. T2's CAS would not be
>       affected,
>                          because it's
>                          going to report failure either way. T3's CAS could
>       then
>                          succeed, but
>                          with T3's read of b failing to see T2's write of b
>       due
>                          to their
>                          reordering. Therefore in this case as well, r5
>       could be
>                          true or
>                          false.
>
>                          As I read the current docs, compareAndSet is most
>                          definitely
>                          described as an _atomic_ operation with _both_
>       volatile
>                          read and
>                          volatile write memory effects, supporting Answer
>       1, such
>                          that the
>                          example code above is _not_ racy.
>
>                          If CAS is allowed _either_ to be not-quite-atomic
>       _or_
>                          to not have
>                          volatile write effects on failure, the example
>       code
>                          above _is_ racy.
>
>                          Since there is a definite difference in the
>       correctness
>                          implications
>                          of these two interpretations, and since _at least
>       two_
>                          compareAndSet
>                          implementations in j.u.c.atomic have the latter
>                          semantics anyway
>                          (AtomicStampedReference and
>       AtomicMarkableReference), it
>                          seems worth
>                          clarifying in the docs.
>
>                          Cheers,
>                          Justin
>
>
>       _______________________________________________
>       Concurrency-interest mailing list
>       Concurrency-interest at cs.oswego.edu
>       http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/2297fda2/attachment-0001.html>

From boehm at acm.org  Wed Jan 21 18:04:32 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 15:04:32 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8CAC0@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C78C@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C7E1@sm-ex-01-vm.guidewire.com>
	<CAPUmR1ZDVSNYeH+dNuciVH9os76H=KUJM5Te95fTR3X5pe+yog@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CAC0@sm-ex-01-vm.guidewire.com>
Message-ID: <CAPUmR1aOws3vgOL7Yhdrfy-9x0RW01rqer8UQDQDHMJtbxgMfg@mail.gmail.com>

On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com>
wrote:
>
> Hans Boehm wrote:
>
> > You're right that there is currently no sanctioned way to access
> > the same object both as a volatile and non-volatile. But there
> > seem to be good reasons for supporting that. E.g. the pointer/flag
> > assignment in double-checked locking doesn't race and hence could
> > be non-volatile.
>
> Do you mean that the write marked #2 below could be non-volatile,
> while the read marked #1 is still volatile?
>
>   private volatile Something instance;
>
>   public Something getInstance() {
>     if (instance == null) { // #1
>       synchronized (this) {
>         if (instance == null) // #4: added by HB
>           instance = new Something(); // #2
>       } // #3
>     }
>     return instance;
>   }
>
> If so, I don't think that works. Another thread calling getInstance
> could still see the Something not fully initialized, if its line #1
> executes between the writing thread's lines #2 and #3 (exiting the
> monitor).

Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't
need to be volatile since racing writes are locked out anyway.

>
> > (Does the constructor for e.g. AtomicInteger perform a volatile
> > assignment? The documentation appears unclear. [...])
>
> This the latest code in CVS:
>
>   private volatile int value;
>
>   /**
>    * Creates a new AtomicInteger with the given initial value.
>    *
>    * @param initialValue the initial value
>    */
>   public AtomicInteger(int initialValue) {
>     value = initialValue;
>   }
>
>
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup

Interesting.  There's clearly not much of a reason for this to be
volatile.  Any code that cared about ordering would also have to be
prepared to see a pre-initialization zero value, which seems extremely
unlikely.  C++11 carefully defines the initialization not to be atomic.

>
> > But the notion of intentionally mis-specifying an operation to
> > write when it really doesn't just seems wrong to me.
>
> You and I are agreeing on this point. :)
>
> But you also have to be prepared that the implementation _might_
> actually do a write. Don't some CPU architectures implement CAS with
> a store cycle to main memory regardless of success or failure, just
> inserting the current value in the store buffer late in the cycle if
> it didn't match the expected value? Therefore if your code is
> actually sensitive to whether other threads are indeed writing that
> memory location, it's going to be buggy on those architectures.

Indeed, now that I checked, the Intel x86 documentation seems to say that
in places, though that part of the description looks quite dated to me.  I
have always been told that modern processors acquire the cache line in
exclusive mode and then perform the operation in cache.  And performance
numbers seem to confirm that.

In either case, this isn't really programmer visible one way or the other.
On an ll/sc architecture like ARM, it would generally be silly to do the
store unconditionally, and we want to discourage that.  If the hardware
does it under the covers (which I doubt for modern hardware; gratuitously
dirtying the cache line doesn't seem free), so be it.

I'm just arguing for a description like the current one in the specific
class descriptions (and the CAS description in Wikipedia and most other
places) that doesn't gratuitously look like it imposes additional
requirements, and actually does impose expensive and useless ordering
requirements on some important architectures.

>
> I'm having trouble imagining what could actually go wrong, though.
> Is it possible that a no-op CAS would appear to undo non-volatile
> changes in another thread? E.g.:
>
> T1: r1 = x; // volatile read, sees 0
> T1: x = 3; // non-volatile write, not flushed yet
> T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
> T1: r2 = x; // non-volatile read, could see either 3 or 0?

I don't think anything would actually go wrong, though we would have to be
careful to specify the write to disallow the 0 write in your example.  I
just dislike specifying CAS in a new and nonstandard way for no positive
benefit.

Hans

>
> Cheers,
> Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/1ca939f1/attachment.html>

From Sebastian.Millies at softwareag.com  Wed Jan 21 18:16:14 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Wed, 21 Jan 2015 23:16:14 +0000
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
	LockSupport.park() return?)
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>

everyone, thanks for the explanations regarding LockSupport.park().

Please let me describe the problem that the coding I am faced with appears to be trying to solve. Perhaps this rings a bell and someone can point me to some higher-level construct from j.u.c which one might use. My own proposal follows below.


?         Concurrent requests are made of a server. There is one thread per request. The server calculates how much heap space will be needed to fulfill each request and keeps track of available memory.

?         Requests that can be served with the available memory may run immediately. Other requests have to wait in a queue.

?         Whenever a request finishes, it wakes up the longest waiting thread to give it a chance to run.

?         Whenever a thread wakes up and finds it can run within the available memory, it removes itself from the queue and also recursively wakes up the next longest waiting thread, to give it a chance to run, too.

Doesn?t sound too unusual.

The current implementation uses a central coordinating singleton, which keeps track of the memory requirements per thread in a ThreadLocal variable, available memory in an instance variable, and has a BlockingQueue for waiting tasks. These resources are protected using synchronized blocks. But this is interspersed (in code stretches not protected with a lock) with calls to park() and unpark(queue.peek()) for suspending and resuming threads. The solution is obviously incorrect, as described in a previous post.

I wonder if a rewrite might not be better than a fix. I was thinking that maybe ReentrantLock (the fair version, because I want the FIFO behavior) would be good to use. I might get a condition from the lock and use Condition.await()/Condition.signal() to suspend/resume threads. I think I might not even need an explicit queue. Is that right? Sort of like this (comments welcome):

public class WaitQueue
{
  private final ReentrantLock lock = new ReentrantLock( true ); // use a fair lock.
  private final Condition mayRun = lock.newCondition();

  private final ThreadLocal<Long> requiredMemory = new ThreadLocal<Long>();
  private long availableMemory;

  public WaitQueue( long availableMemory )
  {
    this.availableMemory = availableMemory;
  }

  public final void await() throws InterruptedException
  {
    lock.lock();
    try {
      while( availableMemory < requiredMemory.get() ) {
        mayRun.await();
      }
      availableMemory -= requiredMemory.get();
      mayRun.signal(); // give next in line a chance, too
    }
    finally {
      lock.unlock();
    }
  }

  public final void release()
  {
    lock.lock();
    try {
      availableMemory += requiredMemory.get();
      mayRun.signal();
    }
    finally {
      lock.unlock();
    }
  }

}

I?d use it like this:

interface Request
{
  long requiredMemory();
  void serve();
}

public void serveRequest( Request req )
{
  requiredMemory.set( req.requiredMemory() );
  try {
    await();
  }
  catch( InterruptedException e ) {
    return; // thread shutdown requested
  }

  try {
    req.serve();
  }
  finally {
    release();
    requiredMemory.remove();
  }
}

Or should I do something completely different? Is there a standard approach to this kind of problem?

Sebastian Millies
PPM, Saarbr?cken C1.67, +49 681 210 3221


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/4627f375/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Jan 21 18:20:35 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 09:20:35 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1aOws3vgOL7Yhdrfy-9x0RW01rqer8UQDQDHMJtbxgMfg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEMHKMAA.davidcholmes@aapt.net.au>

The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 9:05 AM
  To: Justin Sampson
  Cc: concurrency-interest at cs.oswego.edu; David Holmes
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)




  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
  >
  > Hans Boehm wrote:
  >
  > > You're right that there is currently no sanctioned way to access
  > > the same object both as a volatile and non-volatile. But there
  > > seem to be good reasons for supporting that. E.g. the pointer/flag
  > > assignment in double-checked locking doesn't race and hence could
  > > be non-volatile.
  >
  > Do you mean that the write marked #2 below could be non-volatile,
  > while the read marked #1 is still volatile?
  >
  >   private volatile Something instance;
  >
  >   public Something getInstance() {
  >     if (instance == null) { // #1
  >       synchronized (this) {
  >         if (instance == null) // #4: added by HB
  >           instance = new Something(); // #2
  >       } // #3
  >     }
  >     return instance;
  >   }
  >
  > If so, I don't think that works. Another thread calling getInstance
  > could still see the Something not fully initialized, if its line #1
  > executes between the writing thread's lines #2 and #3 (exiting the
  > monitor).


  Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

  >
  > > (Does the constructor for e.g. AtomicInteger perform a volatile
  > > assignment? The documentation appears unclear. [...])
  >
  > This the latest code in CVS:
  >
  >   private volatile int value;
  >
  >   /**
  >    * Creates a new AtomicInteger with the given initial value.
  >    *
  >    * @param initialValue the initial value
  >    */
  >   public AtomicInteger(int initialValue) {
  >     value = initialValue;
  >   }
  >
  > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


  Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

  >
  > > But the notion of intentionally mis-specifying an operation to
  > > write when it really doesn't just seems wrong to me.
  >
  > You and I are agreeing on this point. :)
  >
  > But you also have to be prepared that the implementation _might_
  > actually do a write. Don't some CPU architectures implement CAS with
  > a store cycle to main memory regardless of success or failure, just
  > inserting the current value in the store buffer late in the cycle if
  > it didn't match the expected value? Therefore if your code is
  > actually sensitive to whether other threads are indeed writing that
  > memory location, it's going to be buggy on those architectures.


  Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


  In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


  I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


  >
  > I'm having trouble imagining what could actually go wrong, though.
  > Is it possible that a no-op CAS would appear to undo non-volatile
  > changes in another thread? E.g.:
  >
  > T1: r1 = x; // volatile read, sees 0
  > T1: x = 3; // non-volatile write, not flushed yet
  > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
  > T1: r2 = x; // non-volatile read, could see either 3 or 0?


  I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


  Hans

  >
  > Cheers,
  > Justin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/6b5c6b50/attachment.html>

From boehm at acm.org  Wed Jan 21 18:32:28 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 15:32:28 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEMHKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1aOws3vgOL7Yhdrfy-9x0RW01rqer8UQDQDHMJtbxgMfg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEMHKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>

That does have the advantage that it's a nice simple model.  It has the
disadvantage that you're paying for a property that arguably no reasonable
code cares about.  At least I'm having a hard time constructing a useful
example where it matters.

On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  The AtomicXXX classes are supposed to act like volatile fields with the
> addition of the atomic operations. Hence the actual field inside the
> AtomicXXX class, is and should be, volatile.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
> *Sent:* Thursday, 22 January 2015 9:05 AM
> *To:* Justin Sampson
> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
>
>
> On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com>
> wrote:
> >
> > Hans Boehm wrote:
> >
> > > You're right that there is currently no sanctioned way to access
> > > the same object both as a volatile and non-volatile. But there
> > > seem to be good reasons for supporting that. E.g. the pointer/flag
> > > assignment in double-checked locking doesn't race and hence could
> > > be non-volatile.
> >
> > Do you mean that the write marked #2 below could be non-volatile,
> > while the read marked #1 is still volatile?
> >
> >   private volatile Something instance;
> >
> >   public Something getInstance() {
> >     if (instance == null) { // #1
> >       synchronized (this) {
> >         if (instance == null) // #4: added by HB
> >           instance = new Something(); // #2
> >       } // #3
> >     }
> >     return instance;
> >   }
> >
> > If so, I don't think that works. Another thread calling getInstance
> > could still see the Something not fully initialized, if its line #1
> > executes between the writing thread's lines #2 and #3 (exiting the
> > monitor).
>
> Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't
> need to be volatile since racing writes are locked out anyway.
>
> >
> > > (Does the constructor for e.g. AtomicInteger perform a volatile
> > > assignment? The documentation appears unclear. [...])
> >
> > This the latest code in CVS:
> >
> >   private volatile int value;
> >
> >   /**
> >    * Creates a new AtomicInteger with the given initial value.
> >    *
> >    * @param initialValue the initial value
> >    */
> >   public AtomicInteger(int initialValue) {
> >     value = initialValue;
> >   }
> >
> >
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>
> Interesting.  There's clearly not much of a reason for this to be
> volatile.  Any code that cared about ordering would also have to be
> prepared to see a pre-initialization zero value, which seems extremely
> unlikely.  C++11 carefully defines the initialization not to be atomic.
>
> >
> > > But the notion of intentionally mis-specifying an operation to
> > > write when it really doesn't just seems wrong to me.
> >
> > You and I are agreeing on this point. :)
> >
> > But you also have to be prepared that the implementation _might_
> > actually do a write. Don't some CPU architectures implement CAS with
> > a store cycle to main memory regardless of success or failure, just
> > inserting the current value in the store buffer late in the cycle if
> > it didn't match the expected value? Therefore if your code is
> > actually sensitive to whether other threads are indeed writing that
> > memory location, it's going to be buggy on those architectures.
>
> Indeed, now that I checked, the Intel x86 documentation seems to say that
> in places, though that part of the description looks quite dated to me.  I
> have always been told that modern processors acquire the cache line in
> exclusive mode and then perform the operation in cache.  And performance
> numbers seem to confirm that.
>
> In either case, this isn't really programmer visible one way or the
> other.  On an ll/sc architecture like ARM, it would generally be silly to
> do the store unconditionally, and we want to discourage that.  If the
> hardware does it under the covers (which I doubt for modern hardware;
> gratuitously dirtying the cache line doesn't seem free), so be it.
>
> I'm just arguing for a description like the current one in the specific
> class descriptions (and the CAS description in Wikipedia and most other
> places) that doesn't gratuitously look like it imposes additional
> requirements, and actually does impose expensive and useless ordering
> requirements on some important architectures.
>
> >
> > I'm having trouble imagining what could actually go wrong, though.
> > Is it possible that a no-op CAS would appear to undo non-volatile
> > changes in another thread? E.g.:
> >
> > T1: r1 = x; // volatile read, sees 0
> > T1: x = 3; // non-volatile write, not flushed yet
> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>
> I don't think anything would actually go wrong, though we would have to be
> careful to specify the write to disallow the 0 write in your example.  I
> just dislike specifying CAS in a new and nonstandard way for no positive
> benefit.
>
> Hans
>
> >
> > Cheers,
> > Justin
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/5b860c46/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Jan 21 18:37:49 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 09:37:49 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>

??? set/get are direct loads and stores and required to have volatile semantics. An actual read/wrote of a volatile field seems perfect to me.

David
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 9:32 AM
  To: David Holmes
  Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  That does have the advantage that it's a nice simple model.  It has the disadvantage that you're paying for a property that arguably no reasonable code cares about.  At least I'm having a hard time constructing a useful example where it matters.


  On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
      Sent: Thursday, 22 January 2015 9:05 AM
      To: Justin Sampson
      Cc: concurrency-interest at cs.oswego.edu; David Holmes
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)





      On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
      >
      > Hans Boehm wrote:
      >
      > > You're right that there is currently no sanctioned way to access
      > > the same object both as a volatile and non-volatile. But there
      > > seem to be good reasons for supporting that. E.g. the pointer/flag
      > > assignment in double-checked locking doesn't race and hence could
      > > be non-volatile.
      >
      > Do you mean that the write marked #2 below could be non-volatile,
      > while the read marked #1 is still volatile?
      >
      >   private volatile Something instance;
      >
      >   public Something getInstance() {
      >     if (instance == null) { // #1
      >       synchronized (this) {
      >         if (instance == null) // #4: added by HB
      >           instance = new Something(); // #2
      >       } // #3
      >     }
      >     return instance;
      >   }
      >
      > If so, I don't think that works. Another thread calling getInstance
      > could still see the Something not fully initialized, if its line #1
      > executes between the writing thread's lines #2 and #3 (exiting the
      > monitor). 


      Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

      >
      > > (Does the constructor for e.g. AtomicInteger perform a volatile
      > > assignment? The documentation appears unclear. [...])
      >
      > This the latest code in CVS:
      >
      >   private volatile int value;
      >
      >   /**
      >    * Creates a new AtomicInteger with the given initial value.
      >    *
      >    * @param initialValue the initial value
      >    */
      >   public AtomicInteger(int initialValue) {
      >     value = initialValue;
      >   }
      >
      > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


      Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

      >
      > > But the notion of intentionally mis-specifying an operation to
      > > write when it really doesn't just seems wrong to me.
      >
      > You and I are agreeing on this point. :)
      >
      > But you also have to be prepared that the implementation _might_
      > actually do a write. Don't some CPU architectures implement CAS with
      > a store cycle to main memory regardless of success or failure, just
      > inserting the current value in the store buffer late in the cycle if
      > it didn't match the expected value? Therefore if your code is
      > actually sensitive to whether other threads are indeed writing that
      > memory location, it's going to be buggy on those architectures.


      Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


      In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


      I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


      >
      > I'm having trouble imagining what could actually go wrong, though.
      > Is it possible that a no-op CAS would appear to undo non-volatile
      > changes in another thread? E.g.:
      >
      > T1: r1 = x; // volatile read, sees 0
      > T1: x = 3; // non-volatile write, not flushed yet
      > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
      > T1: r2 = x; // non-volatile read, could see either 3 or 0?


      I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


      Hans

      >
      > Cheers,
      > Justin

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/d06e7b3c/attachment.html>

From jsampson at guidewire.com  Wed Jan 21 18:48:55 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Wed, 21 Jan 2015 23:48:55 +0000
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CBF3@sm-ex-01-vm.guidewire.com>

Sebastian Millies wrote:

> I wonder if a rewrite might not be better than a fix. I was thinking
> that maybe ReentrantLock (the fair version, because I want the FIFO
> behavior) would be good to use. I might get a condition from the lock
> and use Condition.await()/Condition.signal() to suspend/resume
> threads. I think I might not even need an explicit queue. Is that
> right? Sort of like this (comments welcome):

Others more expert than myself may have more ideas for standard
solutions. I personally think you're on the right track with the
rewrite, but I just have a couple of quick comments. 

First, a simple signal() won't work in this case because each of the
waiters is really waiting for a _different_ condition. If the signal()
wakes up a thread that is waiting for more than the available memory, it
will go right back to waiting without waking up another thread. You
could use signalAll() or create a Condition for each possible amount
being waited for.

Second, if FIFO ordering of requests is absolutely necessary, I think
you'll have to arrange for it more explicitly. Making a ReentrantLock
"fair" just means that acquiring the lock itself is FIFO ordered. If
threads are waiting and being signalled, they're going back and forth
between the lock queue and the wait queue, which doesn't preserve any
overall ordering.

Cheers,
Justin


From boehm at acm.org  Wed Jan 21 18:54:32 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 15:54:32 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>

I suspect we're miscommunicating, but I'm not sure.

The typical use case for AtomicInteger presumably looks like:

1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
embedded volatile field.

2) communicate x to other threads by assigning x to a volatile, passing it
to a new thread, using a concurrent data structure, or some synchronized
methods.

3) access x.a in a variety of threads.

Typically (1) happens-before (3) because (2) established the necessary
synchronizes-with relationships.

(a) If (1) happens-before all instances of (3), there is no way to tell
whether the assignment of 17 is treated as volatile or not, and it doesn't
matter.

(b) If (1) does not happen before an instance of (3), then that instance of
(3) can see the uninitialized 0  value of x.a rather than the value 17 or a
later one.  I claim it is very unlikely for such code to be correct whether
or not the assignment of 17 is treated as volatile.

Thus there are somewhere between zero and very few cases in which the
volatile treatment of the assignment of 17 actually matters.  It doesn't
matter for any code I would consider to be properly synchronized, since it
only matters for cases in which construction of AtomicInteger does not
happen-before a call of one of its methods.  I agree that in the Java model
it is cleaner to simply treat the assignment as volatile, as is currently
done.  But it adds overhead that's not well-justified, i.e. one or more
fences for every AtomicInteger construction.

This of course no longer has anything to do with CAS semantics.

Hans

On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  ??? set/get are direct loads and stores and required to have volatile
> semantics. An actual read/wrote of a volatile field seems perfect to me.
>
> David
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Thursday, 22 January 2015 9:32 AM
> *To:* David Holmes
> *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
> That does have the advantage that it's a nice simple model.  It has the
> disadvantage that you're paying for a property that arguably no reasonable
> code cares about.  At least I'm having a hard time constructing a useful
> example where it matters.
>
> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  The AtomicXXX classes are supposed to act like volatile fields with the
>> addition of the atomic operations. Hence the actual field inside the
>> AtomicXXX class, is and should be, volatile.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>> *Sent:* Thursday, 22 January 2015 9:05 AM
>> *To:* Justin Sampson
>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>
>>
>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>> >
>> > Hans Boehm wrote:
>> >
>> > > You're right that there is currently no sanctioned way to access
>> > > the same object both as a volatile and non-volatile. But there
>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>> > > assignment in double-checked locking doesn't race and hence could
>> > > be non-volatile.
>> >
>> > Do you mean that the write marked #2 below could be non-volatile,
>> > while the read marked #1 is still volatile?
>> >
>> >   private volatile Something instance;
>> >
>> >   public Something getInstance() {
>> >     if (instance == null) { // #1
>> >       synchronized (this) {
>> >         if (instance == null) // #4: added by HB
>> >           instance = new Something(); // #2
>> >       } // #3
>> >     }
>> >     return instance;
>> >   }
>> >
>> > If so, I don't think that works. Another thread calling getInstance
>> > could still see the Something not fully initialized, if its line #1
>> > executes between the writing thread's lines #2 and #3 (exiting the
>> > monitor).
>>
>> Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't
>> need to be volatile since racing writes are locked out anyway.
>>
>> >
>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>> > > assignment? The documentation appears unclear. [...])
>> >
>> > This the latest code in CVS:
>> >
>> >   private volatile int value;
>> >
>> >   /**
>> >    * Creates a new AtomicInteger with the given initial value.
>> >    *
>> >    * @param initialValue the initial value
>> >    */
>> >   public AtomicInteger(int initialValue) {
>> >     value = initialValue;
>> >   }
>> >
>> >
>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>
>> Interesting.  There's clearly not much of a reason for this to be
>> volatile.  Any code that cared about ordering would also have to be
>> prepared to see a pre-initialization zero value, which seems extremely
>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>
>> >
>> > > But the notion of intentionally mis-specifying an operation to
>> > > write when it really doesn't just seems wrong to me.
>> >
>> > You and I are agreeing on this point. :)
>> >
>> > But you also have to be prepared that the implementation _might_
>> > actually do a write. Don't some CPU architectures implement CAS with
>> > a store cycle to main memory regardless of success or failure, just
>> > inserting the current value in the store buffer late in the cycle if
>> > it didn't match the expected value? Therefore if your code is
>> > actually sensitive to whether other threads are indeed writing that
>> > memory location, it's going to be buggy on those architectures.
>>
>> Indeed, now that I checked, the Intel x86 documentation seems to say that
>> in places, though that part of the description looks quite dated to me.  I
>> have always been told that modern processors acquire the cache line in
>> exclusive mode and then perform the operation in cache.  And performance
>> numbers seem to confirm that.
>>
>> In either case, this isn't really programmer visible one way or the
>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>> do the store unconditionally, and we want to discourage that.  If the
>> hardware does it under the covers (which I doubt for modern hardware;
>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>
>> I'm just arguing for a description like the current one in the specific
>> class descriptions (and the CAS description in Wikipedia and most other
>> places) that doesn't gratuitously look like it imposes additional
>> requirements, and actually does impose expensive and useless ordering
>> requirements on some important architectures.
>>
>> >
>> > I'm having trouble imagining what could actually go wrong, though.
>> > Is it possible that a no-op CAS would appear to undo non-volatile
>> > changes in another thread? E.g.:
>> >
>> > T1: r1 = x; // volatile read, sees 0
>> > T1: x = 3; // non-volatile write, not flushed yet
>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>
>> I don't think anything would actually go wrong, though we would have to
>> be careful to specify the write to disallow the 0 write in your example.  I
>> just dislike specifying CAS in a new and nonstandard way for no positive
>> benefit.
>>
>> Hans
>>
>> >
>> > Cheers,
>> > Justin
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/02161b1c/attachment-0001.html>

From jsampson at guidewire.com  Wed Jan 21 19:09:37 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 22 Jan 2015 00:09:37 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
References: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CC8A@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> (b) If (1) does not happen before an instance of (3), then that
> instance of (3) can see the uninitialized 0 value of x.a rather
> than the value 17 or a later one. I claim it is very unlikely for
> such code to be correct whether or not the assignment of 17 is
> treated as volatile.

The volatile write in the constructor _itself_ creates the
happens-before you're looking for. Making it volatile ensures that
an AtomicWhatever behaves correctly even if a reference to it is
shared racily. Besides, an AtomicWhatever is accessed many more
times than it is constructed, so taking the volatile write out of
the constructor seems like a dubious optimization.

Cheers,
Justin


From davidcholmes at aapt.net.au  Wed Jan 21 19:14:53 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 10:14:53 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEMJKMAA.davidcholmes@aapt.net.au>

Publication is more likely via a final field:

final static AtomicInteger level = new AtomicInteger(17);

Setting the internal field happens-before setting of level regardless of whether internal field is volatile.

level.increment();  // atomic inc doesn't vare if internal field is volatile

if (level.get() == x) // volatile read of internal field essential

level.set(0); // reset operation - volatile write essential

David
-----
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 9:55 AM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  I suspect we're miscommunicating, but I'm not sure.



  The typical use case for AtomicInteger presumably looks like:


  1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the embedded volatile field.


  2) communicate x to other threads by assigning x to a volatile, passing it to a new thread, using a concurrent data structure, or some synchronized methods.


  3) access x.a in a variety of threads.


  Typically (1) happens-before (3) because (2) established the necessary synchronizes-with relationships.


  (a) If (1) happens-before all instances of (3), there is no way to tell whether the assignment of 17 is treated as volatile or not, and it doesn't matter.


  (b) If (1) does not happen before an instance of (3), then that instance of (3) can see the uninitialized 0  value of x.a rather than the value 17 or a later one.  I claim it is very unlikely for such code to be correct whether or not the assignment of 17 is treated as volatile.


  Thus there are somewhere between zero and very few cases in which the volatile treatment of the assignment of 17 actually matters.  It doesn't matter for any code I would consider to be properly synchronized, since it only matters for cases in which construction of AtomicInteger does not happen-before a call of one of its methods.  I agree that in the Java model it is cleaner to simply treat the assignment as volatile, as is currently done.  But it adds overhead that's not well-justified, i.e. one or more fences for every AtomicInteger construction.


  This of course no longer has anything to do with CAS semantics.


  Hans


  On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    ??? set/get are direct loads and stores and required to have volatile semantics. An actual read/wrote of a volatile field seems perfect to me.

    David
      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
      Sent: Thursday, 22 January 2015 9:32 AM
      To: David Holmes

      Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


      That does have the advantage that it's a nice simple model.  It has the disadvantage that you're paying for a property that arguably no reasonable code cares about.  At least I'm having a hard time constructing a useful example where it matters.


      On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

        The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

        David
          -----Original Message-----
          From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
          Sent: Thursday, 22 January 2015 9:05 AM
          To: Justin Sampson
          Cc: concurrency-interest at cs.oswego.edu; David Holmes
          Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)





          On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
          >
          > Hans Boehm wrote:
          >
          > > You're right that there is currently no sanctioned way to access
          > > the same object both as a volatile and non-volatile. But there
          > > seem to be good reasons for supporting that. E.g. the pointer/flag
          > > assignment in double-checked locking doesn't race and hence could
          > > be non-volatile.
          >
          > Do you mean that the write marked #2 below could be non-volatile,
          > while the read marked #1 is still volatile?
          >
          >   private volatile Something instance;
          >
          >   public Something getInstance() {
          >     if (instance == null) { // #1
          >       synchronized (this) {
          >         if (instance == null) // #4: added by HB
          >           instance = new Something(); // #2
          >       } // #3
          >     }
          >     return instance;
          >   }
          >
          > If so, I don't think that works. Another thread calling getInstance
          > could still see the Something not fully initialized, if its line #1
          > executes between the writing thread's lines #2 and #3 (exiting the
          > monitor). 


          Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

          >
          > > (Does the constructor for e.g. AtomicInteger perform a volatile
          > > assignment? The documentation appears unclear. [...])
          >
          > This the latest code in CVS:
          >
          >   private volatile int value;
          >
          >   /**
          >    * Creates a new AtomicInteger with the given initial value.
          >    *
          >    * @param initialValue the initial value
          >    */
          >   public AtomicInteger(int initialValue) {
          >     value = initialValue;
          >   }
          >
          > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


          Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

          >
          > > But the notion of intentionally mis-specifying an operation to
          > > write when it really doesn't just seems wrong to me.
          >
          > You and I are agreeing on this point. :)
          >
          > But you also have to be prepared that the implementation _might_
          > actually do a write. Don't some CPU architectures implement CAS with
          > a store cycle to main memory regardless of success or failure, just
          > inserting the current value in the store buffer late in the cycle if
          > it didn't match the expected value? Therefore if your code is
          > actually sensitive to whether other threads are indeed writing that
          > memory location, it's going to be buggy on those architectures.


          Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


          In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


          I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


          >
          > I'm having trouble imagining what could actually go wrong, though.
          > Is it possible that a no-op CAS would appear to undo non-volatile
          > changes in another thread? E.g.:
          >
          > T1: r1 = x; // volatile read, sees 0
          > T1: x = 3; // non-volatile write, not flushed yet
          > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
          > T1: r2 = x; // non-volatile read, could see either 3 or 0?


          I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


          Hans

          >
          > Cheers,
          > Justin



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/38baad97/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 21 19:31:41 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 19:31:41 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
References: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
Message-ID: <CAHjP37Fi2qfCr_=j-tN1aakxKM8Z2h2t1q3Or_YSXXMMdMVg8w@mail.gmail.com>

I think David's point is that AtomicInteger get()/set() are supposed to act
just like a volatile write; easiest way to do that is to have those methods
do a read and write of the internal volatile field, just like if there was
no AtomicInteger and one was using the volatile field on its own.  However,
technically speaking, one could use a non-volatile field internally and
implement get()/set() in terms of Unsafe methods that provide volatile
semantics.

On Wed, Jan 21, 2015 at 6:54 PM, Hans Boehm <boehm at acm.org> wrote:

> I suspect we're miscommunicating, but I'm not sure.
>
> The typical use case for AtomicInteger presumably looks like:
>
> 1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
> embedded volatile field.
>
> 2) communicate x to other threads by assigning x to a volatile, passing it
> to a new thread, using a concurrent data structure, or some synchronized
> methods.
>
> 3) access x.a in a variety of threads.
>
> Typically (1) happens-before (3) because (2) established the necessary
> synchronizes-with relationships.
>
> (a) If (1) happens-before all instances of (3), there is no way to tell
> whether the assignment of 17 is treated as volatile or not, and it doesn't
> matter.
>
> (b) If (1) does not happen before an instance of (3), then that instance
> of (3) can see the uninitialized 0  value of x.a rather than the value 17
> or a later one.  I claim it is very unlikely for such code to be correct
> whether or not the assignment of 17 is treated as volatile.
>
> Thus there are somewhere between zero and very few cases in which the
> volatile treatment of the assignment of 17 actually matters.  It doesn't
> matter for any code I would consider to be properly synchronized, since it
> only matters for cases in which construction of AtomicInteger does not
> happen-before a call of one of its methods.  I agree that in the Java model
> it is cleaner to simply treat the assignment as volatile, as is currently
> done.  But it adds overhead that's not well-justified, i.e. one or more
> fences for every AtomicInteger construction.
>
> This of course no longer has anything to do with CAS semantics.
>
> Hans
>
> On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  ??? set/get are direct loads and stores and required to have volatile
>> semantics. An actual read/wrote of a volatile field seems perfect to me.
>>
>> David
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>> *Sent:* Thursday, 22 January 2015 9:32 AM
>> *To:* David Holmes
>> *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>> That does have the advantage that it's a nice simple model.  It has the
>> disadvantage that you're paying for a property that arguably no reasonable
>> code cares about.  At least I'm having a hard time constructing a useful
>> example where it matters.
>>
>> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>  The AtomicXXX classes are supposed to act like volatile fields with
>>> the addition of the atomic operations. Hence the actual field inside the
>>> AtomicXXX class, is and should be, volatile.
>>>
>>> David
>>>
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>>> *Sent:* Thursday, 22 January 2015 9:05 AM
>>> *To:* Justin Sampson
>>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>> (another doc fix request)
>>>
>>>
>>>
>>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
>>> jsampson at guidewire.com> wrote:
>>> >
>>> > Hans Boehm wrote:
>>> >
>>> > > You're right that there is currently no sanctioned way to access
>>> > > the same object both as a volatile and non-volatile. But there
>>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>>> > > assignment in double-checked locking doesn't race and hence could
>>> > > be non-volatile.
>>> >
>>> > Do you mean that the write marked #2 below could be non-volatile,
>>> > while the read marked #1 is still volatile?
>>> >
>>> >   private volatile Something instance;
>>> >
>>> >   public Something getInstance() {
>>> >     if (instance == null) { // #1
>>> >       synchronized (this) {
>>> >         if (instance == null) // #4: added by HB
>>> >           instance = new Something(); // #2
>>> >       } // #3
>>> >     }
>>> >     return instance;
>>> >   }
>>> >
>>> > If so, I don't think that works. Another thread calling getInstance
>>> > could still see the Something not fully initialized, if its line #1
>>> > executes between the writing thread's lines #2 and #3 (exiting the
>>> > monitor).
>>>
>>> Sorry.  You're right; I misstated that.  It's the load at #4 that
>>> doesn't need to be volatile since racing writes are locked out anyway.
>>>
>>> >
>>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>>> > > assignment? The documentation appears unclear. [...])
>>> >
>>> > This the latest code in CVS:
>>> >
>>> >   private volatile int value;
>>> >
>>> >   /**
>>> >    * Creates a new AtomicInteger with the given initial value.
>>> >    *
>>> >    * @param initialValue the initial value
>>> >    */
>>> >   public AtomicInteger(int initialValue) {
>>> >     value = initialValue;
>>> >   }
>>> >
>>> >
>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>>
>>> Interesting.  There's clearly not much of a reason for this to be
>>> volatile.  Any code that cared about ordering would also have to be
>>> prepared to see a pre-initialization zero value, which seems extremely
>>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>>
>>> >
>>> > > But the notion of intentionally mis-specifying an operation to
>>> > > write when it really doesn't just seems wrong to me.
>>> >
>>> > You and I are agreeing on this point. :)
>>> >
>>> > But you also have to be prepared that the implementation _might_
>>> > actually do a write. Don't some CPU architectures implement CAS with
>>> > a store cycle to main memory regardless of success or failure, just
>>> > inserting the current value in the store buffer late in the cycle if
>>> > it didn't match the expected value? Therefore if your code is
>>> > actually sensitive to whether other threads are indeed writing that
>>> > memory location, it's going to be buggy on those architectures.
>>>
>>> Indeed, now that I checked, the Intel x86 documentation seems to say
>>> that in places, though that part of the description looks quite dated to
>>> me.  I have always been told that modern processors acquire the cache line
>>> in exclusive mode and then perform the operation in cache.  And performance
>>> numbers seem to confirm that.
>>>
>>> In either case, this isn't really programmer visible one way or the
>>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>>> do the store unconditionally, and we want to discourage that.  If the
>>> hardware does it under the covers (which I doubt for modern hardware;
>>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>>
>>> I'm just arguing for a description like the current one in the specific
>>> class descriptions (and the CAS description in Wikipedia and most other
>>> places) that doesn't gratuitously look like it imposes additional
>>> requirements, and actually does impose expensive and useless ordering
>>> requirements on some important architectures.
>>>
>>> >
>>> > I'm having trouble imagining what could actually go wrong, though.
>>> > Is it possible that a no-op CAS would appear to undo non-volatile
>>> > changes in another thread? E.g.:
>>> >
>>> > T1: r1 = x; // volatile read, sees 0
>>> > T1: x = 3; // non-volatile write, not flushed yet
>>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>>
>>> I don't think anything would actually go wrong, though we would have to
>>> be careful to specify the write to disallow the 0 write in your example.  I
>>> just dislike specifying CAS in a new and nonstandard way for no positive
>>> benefit.
>>>
>>> Hans
>>>
>>> >
>>> > Cheers,
>>> > Justin
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/50ffd484/attachment.html>

From dl at cs.oswego.edu  Wed Jan 21 19:46:05 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 21 Jan 2015 19:46:05 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>
References: <1420743543652-11812.post@n7.nabble.com>	<54B7CD9B.1070908@oracle.com>	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>	<54B7D57A.6000703@oracle.com>	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>	<54B7EC2A.1040409@oracle.com>	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>	<54B7FC50.7020108@oracle.com>
	<54B80729.7030609@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>	<54B8399F.6000106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>	<54B84055.2000701@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>	<54B928DF.80106@oracle.com>	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>
Message-ID: <54C0484D.4030700@cs.oswego.edu>

Back to...

On 01/19/2015 08:24 PM, Justin Sampson wrote:
> Okay, Doug -- here's one last proposal. I think the wording that
> you've added about reading/writing of a volatile by park/unpark is
> both (a) too strong and (b) misleading. :)

OK. Which brings us back to providing usage guidance rather
than a (semi) formal spec. About which most people probably agree.

>
> So here's my final(?) proposed wording, to replace the sentence
> you've added:

> "The caller of park and unpark can rely on those calls not being
> reordered with any volatile reads or writes in the calling thread.
> However, the same promise cannot be made regarding non-volatile
> reads or writes. Therefore, the caller should ensure that any state
> variables being used in conjunction with park and unpark are
> themselves volatile."

Adapted as:

  * Reliable usage requires the use of volatile (or atomic) variables
  * to control when to park or unpark.  Orderings of calls to these
  * methods are maintained with respect to volatile variable accesses,
  * but not necessarily non-volatile variable accesses.

(Which is intentionally imprecise about what form of "ordering"
we mean here.)

> P.P.S. I'm enjoying quite a bit of reordering of the messages on
> this thread, and haven't even received Doug's message that I'm
> replying to. Email does not exhibit sequential consistency. :)

Yes. In case anyone wonders, the most common reason is because
Mailman batches sends, and if you are unlucky enough to be in
a batch  with a send that gets stuck, it can be hours or even days
out of order. Mailman eventually auto-unsubscribes most problematic
ones though.

-Doug


From boehm at acm.org  Wed Jan 21 20:01:48 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 17:01:48 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEMJKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEMJKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1bD0WKeOyoBCb-U_PRLWaWHBXNrVPz-_TNV-ehMzfNT+w@mail.gmail.com>

If the object containing level is properly published, I think it still
doesn't matter whether the assignment of 17 is treated as volatile or not.
The assignment of 17 doesn't happen-before the later operations on the
level field, but is treated as such by the final field rules, since we get
to it by a dereference chain from a final field.

Aside (from the aside from the aside):  I'm surprised by your "more likely"
statement.  It would be interesting to know how common it is to
intentionally pass objects via final fields without additional
happens-before ordering.  My mental mode is that happens in exactly three
cases:

1) Clever and very tricky performance optimizations that elide "volatile"
for concurrently accessed fields.

2) Bugs.

3) (Rarely) Malware trying to exploit races.

Hans


On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Publication is more likely via a final field:
>
> final static AtomicInteger level = new AtomicInteger(17);
>
> Setting the internal field happens-before setting of level regardless of
> whether internal field is volatile.
>
> level.increment();  // atomic inc doesn't vare if internal field is
> volatile
>
> if (level.get() == x) // volatile read of internal field essential
>
> level.set(0); // reset operation - volatile write essential
>
> David
> -----
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Thursday, 22 January 2015 9:55 AM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
>  I suspect we're miscommunicating, but I'm not sure.
>
> The typical use case for AtomicInteger presumably looks like:
>
> 1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
> embedded volatile field.
>
> 2) communicate x to other threads by assigning x to a volatile, passing it
> to a new thread, using a concurrent data structure, or some synchronized
> methods.
>
> 3) access x.a in a variety of threads.
>
> Typically (1) happens-before (3) because (2) established the necessary
> synchronizes-with relationships.
>
> (a) If (1) happens-before all instances of (3), there is no way to tell
> whether the assignment of 17 is treated as volatile or not, and it doesn't
> matter.
>
> (b) If (1) does not happen before an instance of (3), then that instance
> of (3) can see the uninitialized 0  value of x.a rather than the value 17
> or a later one.  I claim it is very unlikely for such code to be correct
> whether or not the assignment of 17 is treated as volatile.
>
> Thus there are somewhere between zero and very few cases in which the
> volatile treatment of the assignment of 17 actually matters.  It doesn't
> matter for any code I would consider to be properly synchronized, since it
> only matters for cases in which construction of AtomicInteger does not
> happen-before a call of one of its methods.  I agree that in the Java model
> it is cleaner to simply treat the assignment as volatile, as is currently
> done.  But it adds overhead that's not well-justified, i.e. one or more
> fences for every AtomicInteger construction.
>
> This of course no longer has anything to do with CAS semantics.
>
> Hans
>
> On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  ??? set/get are direct loads and stores and required to have volatile
>> semantics. An actual read/wrote of a volatile field seems perfect to me.
>>
>> David
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>> *Sent:* Thursday, 22 January 2015 9:32 AM
>> *To:* David Holmes
>>  *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>  That does have the advantage that it's a nice simple model.  It has the
>> disadvantage that you're paying for a property that arguably no reasonable
>> code cares about.  At least I'm having a hard time constructing a useful
>> example where it matters.
>>
>> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>  The AtomicXXX classes are supposed to act like volatile fields with
>>> the addition of the atomic operations. Hence the actual field inside the
>>> AtomicXXX class, is and should be, volatile.
>>>
>>> David
>>>
>>> -----Original Message-----
>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>>> *Sent:* Thursday, 22 January 2015 9:05 AM
>>> *To:* Justin Sampson
>>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>> (another doc fix request)
>>>
>>>
>>>
>>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
>>> jsampson at guidewire.com> wrote:
>>> >
>>> > Hans Boehm wrote:
>>> >
>>> > > You're right that there is currently no sanctioned way to access
>>> > > the same object both as a volatile and non-volatile. But there
>>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>>> > > assignment in double-checked locking doesn't race and hence could
>>> > > be non-volatile.
>>> >
>>> > Do you mean that the write marked #2 below could be non-volatile,
>>> > while the read marked #1 is still volatile?
>>> >
>>> >   private volatile Something instance;
>>> >
>>> >   public Something getInstance() {
>>> >     if (instance == null) { // #1
>>> >       synchronized (this) {
>>> >         if (instance == null) // #4: added by HB
>>> >           instance = new Something(); // #2
>>> >       } // #3
>>> >     }
>>> >     return instance;
>>> >   }
>>> >
>>> > If so, I don't think that works. Another thread calling getInstance
>>> > could still see the Something not fully initialized, if its line #1
>>> > executes between the writing thread's lines #2 and #3 (exiting the
>>> > monitor).
>>>
>>> Sorry.  You're right; I misstated that.  It's the load at #4 that
>>> doesn't need to be volatile since racing writes are locked out anyway.
>>>
>>> >
>>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>>> > > assignment? The documentation appears unclear. [...])
>>> >
>>> > This the latest code in CVS:
>>> >
>>> >   private volatile int value;
>>> >
>>> >   /**
>>> >    * Creates a new AtomicInteger with the given initial value.
>>> >    *
>>> >    * @param initialValue the initial value
>>> >    */
>>> >   public AtomicInteger(int initialValue) {
>>> >     value = initialValue;
>>> >   }
>>> >
>>> >
>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>>
>>> Interesting.  There's clearly not much of a reason for this to be
>>> volatile.  Any code that cared about ordering would also have to be
>>> prepared to see a pre-initialization zero value, which seems extremely
>>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>>
>>> >
>>> > > But the notion of intentionally mis-specifying an operation to
>>> > > write when it really doesn't just seems wrong to me.
>>> >
>>> > You and I are agreeing on this point. :)
>>> >
>>> > But you also have to be prepared that the implementation _might_
>>> > actually do a write. Don't some CPU architectures implement CAS with
>>> > a store cycle to main memory regardless of success or failure, just
>>> > inserting the current value in the store buffer late in the cycle if
>>> > it didn't match the expected value? Therefore if your code is
>>> > actually sensitive to whether other threads are indeed writing that
>>> > memory location, it's going to be buggy on those architectures.
>>>
>>> Indeed, now that I checked, the Intel x86 documentation seems to say
>>> that in places, though that part of the description looks quite dated to
>>> me.  I have always been told that modern processors acquire the cache line
>>> in exclusive mode and then perform the operation in cache.  And performance
>>> numbers seem to confirm that.
>>>
>>> In either case, this isn't really programmer visible one way or the
>>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>>> do the store unconditionally, and we want to discourage that.  If the
>>> hardware does it under the covers (which I doubt for modern hardware;
>>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>>
>>> I'm just arguing for a description like the current one in the specific
>>> class descriptions (and the CAS description in Wikipedia and most other
>>> places) that doesn't gratuitously look like it imposes additional
>>> requirements, and actually does impose expensive and useless ordering
>>> requirements on some important architectures.
>>>
>>> >
>>> > I'm having trouble imagining what could actually go wrong, though.
>>> > Is it possible that a no-op CAS would appear to undo non-volatile
>>> > changes in another thread? E.g.:
>>> >
>>> > T1: r1 = x; // volatile read, sees 0
>>> > T1: x = 3; // non-volatile write, not flushed yet
>>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>>
>>> I don't think anything would actually go wrong, though we would have to
>>> be careful to specify the write to disallow the 0 write in your example.  I
>>> just dislike specifying CAS in a new and nonstandard way for no positive
>>> benefit.
>>>
>>> Hans
>>>
>>> >
>>> > Cheers,
>>> > Justin
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/aa37a604/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 21 20:12:28 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 20:12:28 -0500
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
Message-ID: <CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>

Have you looked into seeing if Semaphore (with fairness) would help? Your
use case sounds like the classical resource usage throttle, with heap being
the resource.

sent from my phone
On Jan 21, 2015 6:38 PM, "Millies, Sebastian" <
Sebastian.Millies at softwareag.com> wrote:

>  everyone, thanks for the explanations regarding LockSupport.park().
>
>
>
> Please let me describe the problem that the coding I am faced with appears
> to be trying to solve. Perhaps this rings a bell and someone can point me
> to some higher-level construct from j.u.c which one might use. My own
> proposal follows below.
>
>
>
> ?         Concurrent requests are made of a server. There is one thread
> per request. The server calculates how much heap space will be needed to
> fulfill each request and keeps track of available memory.
>
> ?         Requests that can be served with the available memory may run
> immediately. Other requests have to wait in a queue.
>
> ?         Whenever a request finishes, it wakes up the longest waiting
> thread to give it a chance to run.
>
> ?         Whenever a thread wakes up and finds it can run within the
> available memory, it removes itself from the queue and also recursively
> wakes up the next longest waiting thread, to give it a chance to run, too.
>
>
>
> Doesn?t sound too unusual.
>
>
>
> The current implementation uses a central coordinating singleton, which
> keeps track of the memory requirements per thread in a ThreadLocal
> variable, available memory in an instance variable, and has a BlockingQueue
> for waiting tasks. These resources are protected using synchronized blocks.
> But this is interspersed (in code stretches not protected with a lock) with
> calls to park() and unpark(queue.peek()) for suspending and resuming
> threads. The solution is obviously incorrect, as described in a previous
> post.
>
>
>
> I wonder if a rewrite might not be better than a fix. I was thinking that
> maybe ReentrantLock (the fair version, because I want the FIFO behavior)
> would be good to use. I might get a condition from the lock and use
> Condition.await()/Condition.signal() to suspend/resume threads. I think I
> might not even need an explicit queue. Is that right? Sort of like this
> (comments welcome):
>
>
>
> public class WaitQueue
>
> {
>
>   private final ReentrantLock lock = new ReentrantLock( true ); // use a
> fair lock.
>
>   private final Condition mayRun = lock.newCondition();
>
>
>
>   private final ThreadLocal<Long> requiredMemory = new ThreadLocal<Long>();
>
>   private long availableMemory;
>
>
>
>   public WaitQueue( long availableMemory )
>
>   {
>
>     this.availableMemory = availableMemory;
>
>   }
>
>
>
>   public final void await() throws InterruptedException
>
>   {
>
>     lock.lock();
>
>     try {
>
>       while( availableMemory < requiredMemory.get() ) {
>
>         mayRun.await();
>
>       }
>
>       availableMemory -= requiredMemory.get();
>
>       mayRun.signal(); // give next in line a chance, too
>
>     }
>
>     finally {
>
>       lock.unlock();
>
>     }
>
>   }
>
>
>
>   public final void release()
>
>   {
>
>     lock.lock();
>
>     try {
>
>       availableMemory += requiredMemory.get();
>
>       mayRun.signal();
>
>     }
>
>     finally {
>
>       lock.unlock();
>
>     }
>
>   }
>
>
>
> }
>
>
>
> I?d use it like this:
>
>
>
> interface Request
>
> {
>
>   long requiredMemory();
>
>   void serve();
>
> }
>
>
>
> public void serveRequest( Request req )
>
> {
>
>   requiredMemory.set( req.requiredMemory() );
>
>   try {
>
>     await();
>
>   }
>
>   catch( InterruptedException e ) {
>
>     return; // thread shutdown requested
>
>   }
>
>
>
>   try {
>
>     req.serve();
>
>   }
>
>   finally {
>
>     release();
>
>     requiredMemory.remove();
>
>   }
>
> }
>
>
>
> Or should I do something completely different? Is there a standard
> approach to this kind of problem?
>
>
>
> *Sebastian Millies*
>
> PPM, Saarbr?cken C1.67, +49 681 210 3221
>
>
>
>    Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297
> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB
> 1562 - Vorstand/Management Board: Karl-Heinz Streibich
> (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/f49ec398/attachment.html>

From davidcholmes at aapt.net.au  Wed Jan 21 20:13:32 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 11:13:32 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1bD0WKeOyoBCb-U_PRLWaWHBXNrVPz-_TNV-ehMzfNT+w@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEMKKMAA.davidcholmes@aapt.net.au>

Hans, the assignment in the constructor may not depend on volatile - but so what? Implementing as an actual volatile field that you can read and write directly is a lot cleaner and simpler than having to use Unsafe for basic loads and stores. As Justin said mutating is far more common than construction.

As for "more likely" there are two major patterns for sharing objects:
- share a final reference to a "thread-safe" mutable object (like AtomicInteger)
- share a volatile reference to an immutable object

Of course you could also share a violatile reference to a thread-safe mutable object, but that would be the more questionable pattern to me. Certainly needed some times, but not what I would expect to be the prevalent case.

YMMV.

David


  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 11:02 AM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  If the object containing level is properly published, I think it still doesn't matter whether the assignment of 17 is treated as volatile or not.  The assignment of 17 doesn't happen-before the later operations on the level field, but is treated as such by the final field rules, since we get to it by a dereference chain from a final field.


  Aside (from the aside from the aside):  I'm surprised by your "more likely" statement.  It would be interesting to know how common it is to intentionally pass objects via final fields without additional happens-before ordering.  My mental mode is that happens in exactly three cases:


  1) Clever and very tricky performance optimizations that elide "volatile" for concurrently accessed fields.


  2) Bugs.


  3) (Rarely) Malware trying to exploit races.


  Hans




  On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Publication is more likely via a final field:

    final static AtomicInteger level = new AtomicInteger(17);

    Setting the internal field happens-before setting of level regardless of whether internal field is volatile.

    level.increment();  // atomic inc doesn't vare if internal field is volatile

    if (level.get() == x) // volatile read of internal field essential

    level.set(0); // reset operation - volatile write essential

    David
    -----
      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
      Sent: Thursday, 22 January 2015 9:55 AM
      To: David Holmes

      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


      I suspect we're miscommunicating, but I'm not sure.



      The typical use case for AtomicInteger presumably looks like:


      1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the embedded volatile field.


      2) communicate x to other threads by assigning x to a volatile, passing it to a new thread, using a concurrent data structure, or some synchronized methods.


      3) access x.a in a variety of threads.


      Typically (1) happens-before (3) because (2) established the necessary synchronizes-with relationships.


      (a) If (1) happens-before all instances of (3), there is no way to tell whether the assignment of 17 is treated as volatile or not, and it doesn't matter.


      (b) If (1) does not happen before an instance of (3), then that instance of (3) can see the uninitialized 0  value of x.a rather than the value 17 or a later one.  I claim it is very unlikely for such code to be correct whether or not the assignment of 17 is treated as volatile.


      Thus there are somewhere between zero and very few cases in which the volatile treatment of the assignment of 17 actually matters.  It doesn't matter for any code I would consider to be properly synchronized, since it only matters for cases in which construction of AtomicInteger does not happen-before a call of one of its methods.  I agree that in the Java model it is cleaner to simply treat the assignment as volatile, as is currently done.  But it adds overhead that's not well-justified, i.e. one or more fences for every AtomicInteger construction.


      This of course no longer has anything to do with CAS semantics.


      Hans


      On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

        ??? set/get are direct loads and stores and required to have volatile semantics. An actual read/wrote of a volatile field seems perfect to me.

        David
          -----Original Message-----
          From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
          Sent: Thursday, 22 January 2015 9:32 AM
          To: David Holmes

          Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
          Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


          That does have the advantage that it's a nice simple model.  It has the disadvantage that you're paying for a property that arguably no reasonable code cares about.  At least I'm having a hard time constructing a useful example where it matters.


          On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

            The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

            David
              -----Original Message-----
              From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
              Sent: Thursday, 22 January 2015 9:05 AM
              To: Justin Sampson
              Cc: concurrency-interest at cs.oswego.edu; David Holmes
              Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)





              On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
              >
              > Hans Boehm wrote:
              >
              > > You're right that there is currently no sanctioned way to access
              > > the same object both as a volatile and non-volatile. But there
              > > seem to be good reasons for supporting that. E.g. the pointer/flag
              > > assignment in double-checked locking doesn't race and hence could
              > > be non-volatile.
              >
              > Do you mean that the write marked #2 below could be non-volatile,
              > while the read marked #1 is still volatile?
              >
              >   private volatile Something instance;
              >
              >   public Something getInstance() {
              >     if (instance == null) { // #1
              >       synchronized (this) {
              >         if (instance == null) // #4: added by HB
              >           instance = new Something(); // #2
              >       } // #3
              >     }
              >     return instance;
              >   }
              >
              > If so, I don't think that works. Another thread calling getInstance
              > could still see the Something not fully initialized, if its line #1
              > executes between the writing thread's lines #2 and #3 (exiting the
              > monitor). 


              Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

              >
              > > (Does the constructor for e.g. AtomicInteger perform a volatile
              > > assignment? The documentation appears unclear. [...])
              >
              > This the latest code in CVS:
              >
              >   private volatile int value;
              >
              >   /**
              >    * Creates a new AtomicInteger with the given initial value.
              >    *
              >    * @param initialValue the initial value
              >    */
              >   public AtomicInteger(int initialValue) {
              >     value = initialValue;
              >   }
              >
              > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


              Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

              >
              > > But the notion of intentionally mis-specifying an operation to
              > > write when it really doesn't just seems wrong to me.
              >
              > You and I are agreeing on this point. :)
              >
              > But you also have to be prepared that the implementation _might_
              > actually do a write. Don't some CPU architectures implement CAS with
              > a store cycle to main memory regardless of success or failure, just
              > inserting the current value in the store buffer late in the cycle if
              > it didn't match the expected value? Therefore if your code is
              > actually sensitive to whether other threads are indeed writing that
              > memory location, it's going to be buggy on those architectures.


              Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


              In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


              I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


              >
              > I'm having trouble imagining what could actually go wrong, though.
              > Is it possible that a no-op CAS would appear to undo non-volatile
              > changes in another thread? E.g.:
              >
              > T1: r1 = x; // volatile read, sees 0
              > T1: x = 3; // non-volatile write, not flushed yet
              > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
              > T1: r2 = x; // non-volatile read, could see either 3 or 0?


              I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


              Hans

              >
              > Cheers,
              > Justin





-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/af9611c5/attachment-0001.html>

From boehm at acm.org  Wed Jan 21 20:40:18 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 17:40:18 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEMKKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1bD0WKeOyoBCb-U_PRLWaWHBXNrVPz-_TNV-ehMzfNT+w@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEMKKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>

What I was suggesting, I think, was to only use unsafe in the constructor
(in addition to the places it's currently used).  But I agree with you
about cleanliness.

I would still be interested in code that shares a final field without an
additional happens-before relationship (i.e. in a way that would be racy
without the final field rules), and without very clever and subtle
reasoning that would have been unnecessary with the addition of a volatile
declaration.  I really don't think that's a natural idiom, though it is
clearly used for performance on occasion.  And it's sometimes necessary to
deal with that case for malware defense.

Hans

On Wed, Jan 21, 2015 at 5:13 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Hans, the assignment in the constructor may not depend on volatile - but
> so what? Implementing as an actual volatile field that you can read and
> write directly is a lot cleaner and simpler than having to use Unsafe for
> basic loads and stores. As Justin said mutating is far more common than
> construction.
>
> As for "more likely" there are two major patterns for sharing objects:
> - share a final reference to a "thread-safe" mutable object (like
> AtomicInteger)
> - share a volatile reference to an immutable object
>
> Of course you could also share a violatile reference to a thread-safe
> mutable object, but that would be the more questionable pattern to me.
> Certainly needed some times, but not what I would expect to be the
> prevalent case.
>
> YMMV.
>
> David
>
>
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Thursday, 22 January 2015 11:02 AM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
>  If the object containing level is properly published, I think it still
> doesn't matter whether the assignment of 17 is treated as volatile or not.
> The assignment of 17 doesn't happen-before the later operations on the
> level field, but is treated as such by the final field rules, since we get
> to it by a dereference chain from a final field.
>
> Aside (from the aside from the aside):  I'm surprised by your "more
> likely" statement.  It would be interesting to know how common it is to
> intentionally pass objects via final fields without additional
> happens-before ordering.  My mental mode is that happens in exactly three
> cases:
>
> 1) Clever and very tricky performance optimizations that elide "volatile"
> for concurrently accessed fields.
>
> 2) Bugs.
>
> 3) (Rarely) Malware trying to exploit races.
>
> Hans
>
>
> On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  Publication is more likely via a final field:
>>
>> final static AtomicInteger level = new AtomicInteger(17);
>>
>> Setting the internal field happens-before setting of level regardless of
>> whether internal field is volatile.
>>
>> level.increment();  // atomic inc doesn't vare if internal field is
>> volatile
>>
>> if (level.get() == x) // volatile read of internal field essential
>>
>> level.set(0); // reset operation - volatile write essential
>>
>> David
>> -----
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>> *Sent:* Thursday, 22 January 2015 9:55 AM
>> *To:* David Holmes
>>  *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>   I suspect we're miscommunicating, but I'm not sure.
>>
>> The typical use case for AtomicInteger presumably looks like:
>>
>> 1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
>> embedded volatile field.
>>
>> 2) communicate x to other threads by assigning x to a volatile, passing
>> it to a new thread, using a concurrent data structure, or some synchronized
>> methods.
>>
>> 3) access x.a in a variety of threads.
>>
>> Typically (1) happens-before (3) because (2) established the necessary
>> synchronizes-with relationships.
>>
>> (a) If (1) happens-before all instances of (3), there is no way to tell
>> whether the assignment of 17 is treated as volatile or not, and it doesn't
>> matter.
>>
>> (b) If (1) does not happen before an instance of (3), then that instance
>> of (3) can see the uninitialized 0  value of x.a rather than the value 17
>> or a later one.  I claim it is very unlikely for such code to be correct
>> whether or not the assignment of 17 is treated as volatile.
>>
>> Thus there are somewhere between zero and very few cases in which the
>> volatile treatment of the assignment of 17 actually matters.  It doesn't
>> matter for any code I would consider to be properly synchronized, since it
>> only matters for cases in which construction of AtomicInteger does not
>> happen-before a call of one of its methods.  I agree that in the Java model
>> it is cleaner to simply treat the assignment as volatile, as is currently
>> done.  But it adds overhead that's not well-justified, i.e. one or more
>> fences for every AtomicInteger construction.
>>
>> This of course no longer has anything to do with CAS semantics.
>>
>> Hans
>>
>> On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>  ??? set/get are direct loads and stores and required to have volatile
>>> semantics. An actual read/wrote of a volatile field seems perfect to me.
>>>
>>> David
>>>
>>> -----Original Message-----
>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>>> Boehm
>>> *Sent:* Thursday, 22 January 2015 9:32 AM
>>> *To:* David Holmes
>>>  *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>> (another doc fix request)
>>>
>>>  That does have the advantage that it's a nice simple model.  It has
>>> the disadvantage that you're paying for a property that arguably no
>>> reasonable code cares about.  At least I'm having a hard time constructing
>>> a useful example where it matters.
>>>
>>> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
>>> wrote:
>>>
>>>>  The AtomicXXX classes are supposed to act like volatile fields with
>>>> the addition of the atomic operations. Hence the actual field inside the
>>>> AtomicXXX class, is and should be, volatile.
>>>>
>>>> David
>>>>
>>>> -----Original Message-----
>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>>>> *Sent:* Thursday, 22 January 2015 9:05 AM
>>>> *To:* Justin Sampson
>>>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>> (another doc fix request)
>>>>
>>>>
>>>>
>>>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
>>>> jsampson at guidewire.com> wrote:
>>>> >
>>>> > Hans Boehm wrote:
>>>> >
>>>> > > You're right that there is currently no sanctioned way to access
>>>> > > the same object both as a volatile and non-volatile. But there
>>>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>>>> > > assignment in double-checked locking doesn't race and hence could
>>>> > > be non-volatile.
>>>> >
>>>> > Do you mean that the write marked #2 below could be non-volatile,
>>>> > while the read marked #1 is still volatile?
>>>> >
>>>> >   private volatile Something instance;
>>>> >
>>>> >   public Something getInstance() {
>>>> >     if (instance == null) { // #1
>>>> >       synchronized (this) {
>>>> >         if (instance == null) // #4: added by HB
>>>> >           instance = new Something(); // #2
>>>> >       } // #3
>>>> >     }
>>>> >     return instance;
>>>> >   }
>>>> >
>>>> > If so, I don't think that works. Another thread calling getInstance
>>>> > could still see the Something not fully initialized, if its line #1
>>>> > executes between the writing thread's lines #2 and #3 (exiting the
>>>> > monitor).
>>>>
>>>> Sorry.  You're right; I misstated that.  It's the load at #4 that
>>>> doesn't need to be volatile since racing writes are locked out anyway.
>>>>
>>>> >
>>>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>>>> > > assignment? The documentation appears unclear. [...])
>>>> >
>>>> > This the latest code in CVS:
>>>> >
>>>> >   private volatile int value;
>>>> >
>>>> >   /**
>>>> >    * Creates a new AtomicInteger with the given initial value.
>>>> >    *
>>>> >    * @param initialValue the initial value
>>>> >    */
>>>> >   public AtomicInteger(int initialValue) {
>>>> >     value = initialValue;
>>>> >   }
>>>> >
>>>> >
>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>>>
>>>> Interesting.  There's clearly not much of a reason for this to be
>>>> volatile.  Any code that cared about ordering would also have to be
>>>> prepared to see a pre-initialization zero value, which seems extremely
>>>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>>>
>>>> >
>>>> > > But the notion of intentionally mis-specifying an operation to
>>>> > > write when it really doesn't just seems wrong to me.
>>>> >
>>>> > You and I are agreeing on this point. :)
>>>> >
>>>> > But you also have to be prepared that the implementation _might_
>>>> > actually do a write. Don't some CPU architectures implement CAS with
>>>> > a store cycle to main memory regardless of success or failure, just
>>>> > inserting the current value in the store buffer late in the cycle if
>>>> > it didn't match the expected value? Therefore if your code is
>>>> > actually sensitive to whether other threads are indeed writing that
>>>> > memory location, it's going to be buggy on those architectures.
>>>>
>>>> Indeed, now that I checked, the Intel x86 documentation seems to say
>>>> that in places, though that part of the description looks quite dated to
>>>> me.  I have always been told that modern processors acquire the cache line
>>>> in exclusive mode and then perform the operation in cache.  And performance
>>>> numbers seem to confirm that.
>>>>
>>>> In either case, this isn't really programmer visible one way or the
>>>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>>>> do the store unconditionally, and we want to discourage that.  If the
>>>> hardware does it under the covers (which I doubt for modern hardware;
>>>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>>>
>>>> I'm just arguing for a description like the current one in the specific
>>>> class descriptions (and the CAS description in Wikipedia and most other
>>>> places) that doesn't gratuitously look like it imposes additional
>>>> requirements, and actually does impose expensive and useless ordering
>>>> requirements on some important architectures.
>>>>
>>>> >
>>>> > I'm having trouble imagining what could actually go wrong, though.
>>>> > Is it possible that a no-op CAS would appear to undo non-volatile
>>>> > changes in another thread? E.g.:
>>>> >
>>>> > T1: r1 = x; // volatile read, sees 0
>>>> > T1: x = 3; // non-volatile write, not flushed yet
>>>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>>>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>>>
>>>> I don't think anything would actually go wrong, though we would have to
>>>> be careful to specify the write to disallow the 0 write in your example.  I
>>>> just dislike specifying CAS in a new and nonstandard way for no positive
>>>> benefit.
>>>>
>>>> Hans
>>>>
>>>> >
>>>> > Cheers,
>>>> > Justin
>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/08bec3e7/attachment-0001.html>

From jsampson at guidewire.com  Wed Jan 21 20:47:54 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 22 Jan 2015 01:47:54 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54C0484D.4030700@cs.oswego.edu>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>
	<54C0484D.4030700@cs.oswego.edu>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CD90@sm-ex-01-vm.guidewire.com>

Doug Lea wrote:

> OK. Which brings us back to providing usage guidance rather than
> a (semi) formal spec. About which most people probably agree.

Yes. Just out of curiosity, as a newby in these here parts, is there
any intention to come up with formal specs for such things to be put
somewhere official, even if not in the individual class docs? Either
a revision of or an addendum to the JMM, for example? I'm
particularly interested in lazySet & weakCompareAndSet, in addition
to park & unpark, and maybe even some of the other atomics stuff and
_maybe_ even non-volatile accesses of volatile variables. I've been
jotting down notes about what I might call the minimum reliable
semantics of each one throughout these discussions, which I guess I
could just blog about myself, but I'd much rather see an officially
blessed consensus statement somewhere.

> Adapted as:
>
> * Reliable usage requires the use of volatile (or atomic) variables
> * to control when to park or unpark.  Orderings of calls to these
> * methods are maintained with respect to volatile variable accesses,
> * but not necessarily non-volatile variable accesses.
>
> (Which is intentionally imprecise about what form of "ordering" we
> mean here.)

Beautiful. :)

Thanks!
Justin


From davidcholmes at aapt.net.au  Wed Jan 21 20:48:47 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 11:48:47 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>

Sharing via _static_ final fields isn't racy.

David
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 11:40 AM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  What I was suggesting, I think, was to only use unsafe in the constructor (in addition to the places it's currently used).  But I agree with you about cleanliness.


  I would still be interested in code that shares a final field without an additional happens-before relationship (i.e. in a way that would be racy without the final field rules), and without very clever and subtle reasoning that would have been unnecessary with the addition of a volatile declaration.  I really don't think that's a natural idiom, though it is clearly used for performance on occasion.  And it's sometimes necessary to deal with that case for malware defense.


  Hans


  On Wed, Jan 21, 2015 at 5:13 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Hans, the assignment in the constructor may not depend on volatile - but so what? Implementing as an actual volatile field that you can read and write directly is a lot cleaner and simpler than having to use Unsafe for basic loads and stores. As Justin said mutating is far more common than construction.

    As for "more likely" there are two major patterns for sharing objects:
    - share a final reference to a "thread-safe" mutable object (like AtomicInteger)
    - share a volatile reference to an immutable object

    Of course you could also share a violatile reference to a thread-safe mutable object, but that would be the more questionable pattern to me. Certainly needed some times, but not what I would expect to be the prevalent case.

    YMMV.

    David


      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm

      Sent: Thursday, 22 January 2015 11:02 AM
      To: David Holmes
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


      If the object containing level is properly published, I think it still doesn't matter whether the assignment of 17 is treated as volatile or not.  The assignment of 17 doesn't happen-before the later operations on the level field, but is treated as such by the final field rules, since we get to it by a dereference chain from a final field.


      Aside (from the aside from the aside):  I'm surprised by your "more likely" statement.  It would be interesting to know how common it is to intentionally pass objects via final fields without additional happens-before ordering.  My mental mode is that happens in exactly three cases: 


      1) Clever and very tricky performance optimizations that elide "volatile" for concurrently accessed fields.


      2) Bugs.


      3) (Rarely) Malware trying to exploit races.


      Hans




      On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

        Publication is more likely via a final field:

        final static AtomicInteger level = new AtomicInteger(17);

        Setting the internal field happens-before setting of level regardless of whether internal field is volatile.

        level.increment();  // atomic inc doesn't vare if internal field is volatile

        if (level.get() == x) // volatile read of internal field essential

        level.set(0); // reset operation - volatile write essential

        David
        -----
          -----Original Message-----
          From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
          Sent: Thursday, 22 January 2015 9:55 AM
          To: David Holmes

          Cc: concurrency-interest at cs.oswego.edu
          Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


          I suspect we're miscommunicating, but I'm not sure.



          The typical use case for AtomicInteger presumably looks like:


          1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the embedded volatile field.


          2) communicate x to other threads by assigning x to a volatile, passing it to a new thread, using a concurrent data structure, or some synchronized methods.


          3) access x.a in a variety of threads.


          Typically (1) happens-before (3) because (2) established the necessary synchronizes-with relationships.


          (a) If (1) happens-before all instances of (3), there is no way to tell whether the assignment of 17 is treated as volatile or not, and it doesn't matter.


          (b) If (1) does not happen before an instance of (3), then that instance of (3) can see the uninitialized 0  value of x.a rather than the value 17 or a later one.  I claim it is very unlikely for such code to be correct whether or not the assignment of 17 is treated as volatile.


          Thus there are somewhere between zero and very few cases in which the volatile treatment of the assignment of 17 actually matters.  It doesn't matter for any code I would consider to be properly synchronized, since it only matters for cases in which construction of AtomicInteger does not happen-before a call of one of its methods.  I agree that in the Java model it is cleaner to simply treat the assignment as volatile, as is currently done.  But it adds overhead that's not well-justified, i.e. one or more fences for every AtomicInteger construction.


          This of course no longer has anything to do with CAS semantics.


          Hans


          On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

            ??? set/get are direct loads and stores and required to have volatile semantics. An actual read/wrote of a volatile field seems perfect to me.

            David
              -----Original Message-----
              From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
              Sent: Thursday, 22 January 2015 9:32 AM
              To: David Holmes

              Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
              Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


              That does have the advantage that it's a nice simple model.  It has the disadvantage that you're paying for a property that arguably no reasonable code cares about.  At least I'm having a hard time constructing a useful example where it matters.


              On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

                The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

                David
                  -----Original Message-----
                  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
                  Sent: Thursday, 22 January 2015 9:05 AM
                  To: Justin Sampson
                  Cc: concurrency-interest at cs.oswego.edu; David Holmes
                  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)





                  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
                  >
                  > Hans Boehm wrote:
                  >
                  > > You're right that there is currently no sanctioned way to access
                  > > the same object both as a volatile and non-volatile. But there
                  > > seem to be good reasons for supporting that. E.g. the pointer/flag
                  > > assignment in double-checked locking doesn't race and hence could
                  > > be non-volatile.
                  >
                  > Do you mean that the write marked #2 below could be non-volatile,
                  > while the read marked #1 is still volatile?
                  >
                  >   private volatile Something instance;
                  >
                  >   public Something getInstance() {
                  >     if (instance == null) { // #1
                  >       synchronized (this) {
                  >         if (instance == null) // #4: added by HB
                  >           instance = new Something(); // #2
                  >       } // #3
                  >     }
                  >     return instance;
                  >   }
                  >
                  > If so, I don't think that works. Another thread calling getInstance
                  > could still see the Something not fully initialized, if its line #1
                  > executes between the writing thread's lines #2 and #3 (exiting the
                  > monitor). 


                  Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

                  >
                  > > (Does the constructor for e.g. AtomicInteger perform a volatile
                  > > assignment? The documentation appears unclear. [...])
                  >
                  > This the latest code in CVS:
                  >
                  >   private volatile int value;
                  >
                  >   /**
                  >    * Creates a new AtomicInteger with the given initial value.
                  >    *
                  >    * @param initialValue the initial value
                  >    */
                  >   public AtomicInteger(int initialValue) {
                  >     value = initialValue;
                  >   }
                  >
                  > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


                  Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

                  >
                  > > But the notion of intentionally mis-specifying an operation to
                  > > write when it really doesn't just seems wrong to me.
                  >
                  > You and I are agreeing on this point. :)
                  >
                  > But you also have to be prepared that the implementation _might_
                  > actually do a write. Don't some CPU architectures implement CAS with
                  > a store cycle to main memory regardless of success or failure, just
                  > inserting the current value in the store buffer late in the cycle if
                  > it didn't match the expected value? Therefore if your code is
                  > actually sensitive to whether other threads are indeed writing that
                  > memory location, it's going to be buggy on those architectures.


                  Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


                  In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


                  I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


                  >
                  > I'm having trouble imagining what could actually go wrong, though.
                  > Is it possible that a no-op CAS would appear to undo non-volatile
                  > changes in another thread? E.g.:
                  >
                  > T1: r1 = x; // volatile read, sees 0
                  > T1: x = 3; // non-volatile write, not flushed yet
                  > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
                  > T1: r2 = x; // non-volatile read, could see either 3 or 0?


                  I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


                  Hans

                  >
                  > Cheers,
                  > Justin







-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/eaa58519/attachment-0001.html>

From boehm at acm.org  Wed Jan 21 21:00:18 2015
From: boehm at acm.org (Hans Boehm)
Date: Wed, 21 Jan 2015 18:00:18 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>

Sorry, I missed the "static", for which the happens-before relationship
clearly exists.  It sounds like we're in agreement that having the
assignment in the constructor be volatile is not likely to matter in
practice, but the current implementation is cleaner at some performance
cost?

Hans

On Wed, Jan 21, 2015 at 5:48 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Sharing via _static_ final fields isn't racy.
>
> David
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Thursday, 22 January 2015 11:40 AM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
> What I was suggesting, I think, was to only use unsafe in the constructor
> (in addition to the places it's currently used).  But I agree with you
> about cleanliness.
>
> I would still be interested in code that shares a final field without an
> additional happens-before relationship (i.e. in a way that would be racy
> without the final field rules), and without very clever and subtle
> reasoning that would have been unnecessary with the addition of a volatile
> declaration.  I really don't think that's a natural idiom, though it is
> clearly used for performance on occasion.  And it's sometimes necessary to
> deal with that case for malware defense.
>
> Hans
>
> On Wed, Jan 21, 2015 at 5:13 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  Hans, the assignment in the constructor may not depend on volatile -
>> but so what? Implementing as an actual volatile field that you can read and
>> write directly is a lot cleaner and simpler than having to use Unsafe for
>> basic loads and stores. As Justin said mutating is far more common than
>> construction.
>>
>> As for "more likely" there are two major patterns for sharing objects:
>> - share a final reference to a "thread-safe" mutable object (like
>> AtomicInteger)
>> - share a volatile reference to an immutable object
>>
>> Of course you could also share a violatile reference to a thread-safe
>> mutable object, but that would be the more questionable pattern to me.
>> Certainly needed some times, but not what I would expect to be the
>> prevalent case.
>>
>> YMMV.
>>
>> David
>>
>>
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>>  *Sent:* Thursday, 22 January 2015 11:02 AM
>> *To:* David Holmes
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>   If the object containing level is properly published, I think it still
>> doesn't matter whether the assignment of 17 is treated as volatile or not.
>> The assignment of 17 doesn't happen-before the later operations on the
>> level field, but is treated as such by the final field rules, since we get
>> to it by a dereference chain from a final field.
>>
>> Aside (from the aside from the aside):  I'm surprised by your "more
>> likely" statement.  It would be interesting to know how common it is to
>> intentionally pass objects via final fields without additional
>> happens-before ordering.  My mental mode is that happens in exactly three
>> cases:
>>
>> 1) Clever and very tricky performance optimizations that elide "volatile"
>> for concurrently accessed fields.
>>
>> 2) Bugs.
>>
>> 3) (Rarely) Malware trying to exploit races.
>>
>> Hans
>>
>>
>> On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>  Publication is more likely via a final field:
>>>
>>> final static AtomicInteger level = new AtomicInteger(17);
>>>
>>> Setting the internal field happens-before setting of level regardless of
>>> whether internal field is volatile.
>>>
>>> level.increment();  // atomic inc doesn't vare if internal field is
>>> volatile
>>>
>>> if (level.get() == x) // volatile read of internal field essential
>>>
>>> level.set(0); // reset operation - volatile write essential
>>>
>>> David
>>> -----
>>>
>>> -----Original Message-----
>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>>> Boehm
>>> *Sent:* Thursday, 22 January 2015 9:55 AM
>>> *To:* David Holmes
>>>  *Cc:* concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>> (another doc fix request)
>>>
>>>   I suspect we're miscommunicating, but I'm not sure.
>>>
>>> The typical use case for AtomicInteger presumably looks like:
>>>
>>> 1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
>>> embedded volatile field.
>>>
>>> 2) communicate x to other threads by assigning x to a volatile, passing
>>> it to a new thread, using a concurrent data structure, or some synchronized
>>> methods.
>>>
>>> 3) access x.a in a variety of threads.
>>>
>>> Typically (1) happens-before (3) because (2) established the necessary
>>> synchronizes-with relationships.
>>>
>>> (a) If (1) happens-before all instances of (3), there is no way to tell
>>> whether the assignment of 17 is treated as volatile or not, and it doesn't
>>> matter.
>>>
>>> (b) If (1) does not happen before an instance of (3), then that instance
>>> of (3) can see the uninitialized 0  value of x.a rather than the value 17
>>> or a later one.  I claim it is very unlikely for such code to be correct
>>> whether or not the assignment of 17 is treated as volatile.
>>>
>>> Thus there are somewhere between zero and very few cases in which the
>>> volatile treatment of the assignment of 17 actually matters.  It doesn't
>>> matter for any code I would consider to be properly synchronized, since it
>>> only matters for cases in which construction of AtomicInteger does not
>>> happen-before a call of one of its methods.  I agree that in the Java model
>>> it is cleaner to simply treat the assignment as volatile, as is currently
>>> done.  But it adds overhead that's not well-justified, i.e. one or more
>>> fences for every AtomicInteger construction.
>>>
>>> This of course no longer has anything to do with CAS semantics.
>>>
>>> Hans
>>>
>>> On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au>
>>> wrote:
>>>
>>>>  ??? set/get are direct loads and stores and required to have volatile
>>>> semantics. An actual read/wrote of a volatile field seems perfect to me.
>>>>
>>>> David
>>>>
>>>> -----Original Message-----
>>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>>>> Boehm
>>>> *Sent:* Thursday, 22 January 2015 9:32 AM
>>>> *To:* David Holmes
>>>>  *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>> (another doc fix request)
>>>>
>>>>  That does have the advantage that it's a nice simple model.  It has
>>>> the disadvantage that you're paying for a property that arguably no
>>>> reasonable code cares about.  At least I'm having a hard time constructing
>>>> a useful example where it matters.
>>>>
>>>> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au
>>>> > wrote:
>>>>
>>>>>  The AtomicXXX classes are supposed to act like volatile fields with
>>>>> the addition of the atomic operations. Hence the actual field inside the
>>>>> AtomicXXX class, is and should be, volatile.
>>>>>
>>>>> David
>>>>>
>>>>> -----Original Message-----
>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>>>>> *Sent:* Thursday, 22 January 2015 9:05 AM
>>>>> *To:* Justin Sampson
>>>>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>>> (another doc fix request)
>>>>>
>>>>>
>>>>>
>>>>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
>>>>> jsampson at guidewire.com> wrote:
>>>>> >
>>>>> > Hans Boehm wrote:
>>>>> >
>>>>> > > You're right that there is currently no sanctioned way to access
>>>>> > > the same object both as a volatile and non-volatile. But there
>>>>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>>>>> > > assignment in double-checked locking doesn't race and hence could
>>>>> > > be non-volatile.
>>>>> >
>>>>> > Do you mean that the write marked #2 below could be non-volatile,
>>>>> > while the read marked #1 is still volatile?
>>>>> >
>>>>> >   private volatile Something instance;
>>>>> >
>>>>> >   public Something getInstance() {
>>>>> >     if (instance == null) { // #1
>>>>> >       synchronized (this) {
>>>>> >         if (instance == null) // #4: added by HB
>>>>> >           instance = new Something(); // #2
>>>>> >       } // #3
>>>>> >     }
>>>>> >     return instance;
>>>>> >   }
>>>>> >
>>>>> > If so, I don't think that works. Another thread calling getInstance
>>>>> > could still see the Something not fully initialized, if its line #1
>>>>> > executes between the writing thread's lines #2 and #3 (exiting the
>>>>> > monitor).
>>>>>
>>>>> Sorry.  You're right; I misstated that.  It's the load at #4 that
>>>>> doesn't need to be volatile since racing writes are locked out anyway.
>>>>>
>>>>> >
>>>>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>>>>> > > assignment? The documentation appears unclear. [...])
>>>>> >
>>>>> > This the latest code in CVS:
>>>>> >
>>>>> >   private volatile int value;
>>>>> >
>>>>> >   /**
>>>>> >    * Creates a new AtomicInteger with the given initial value.
>>>>> >    *
>>>>> >    * @param initialValue the initial value
>>>>> >    */
>>>>> >   public AtomicInteger(int initialValue) {
>>>>> >     value = initialValue;
>>>>> >   }
>>>>> >
>>>>> >
>>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>>>>
>>>>> Interesting.  There's clearly not much of a reason for this to be
>>>>> volatile.  Any code that cared about ordering would also have to be
>>>>> prepared to see a pre-initialization zero value, which seems extremely
>>>>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>>>>
>>>>> >
>>>>> > > But the notion of intentionally mis-specifying an operation to
>>>>> > > write when it really doesn't just seems wrong to me.
>>>>> >
>>>>> > You and I are agreeing on this point. :)
>>>>> >
>>>>> > But you also have to be prepared that the implementation _might_
>>>>> > actually do a write. Don't some CPU architectures implement CAS with
>>>>> > a store cycle to main memory regardless of success or failure, just
>>>>> > inserting the current value in the store buffer late in the cycle if
>>>>> > it didn't match the expected value? Therefore if your code is
>>>>> > actually sensitive to whether other threads are indeed writing that
>>>>> > memory location, it's going to be buggy on those architectures.
>>>>>
>>>>> Indeed, now that I checked, the Intel x86 documentation seems to say
>>>>> that in places, though that part of the description looks quite dated to
>>>>> me.  I have always been told that modern processors acquire the cache line
>>>>> in exclusive mode and then perform the operation in cache.  And performance
>>>>> numbers seem to confirm that.
>>>>>
>>>>> In either case, this isn't really programmer visible one way or the
>>>>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>>>>> do the store unconditionally, and we want to discourage that.  If the
>>>>> hardware does it under the covers (which I doubt for modern hardware;
>>>>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>>>>
>>>>> I'm just arguing for a description like the current one in the
>>>>> specific class descriptions (and the CAS description in Wikipedia and most
>>>>> other places) that doesn't gratuitously look like it imposes additional
>>>>> requirements, and actually does impose expensive and useless ordering
>>>>> requirements on some important architectures.
>>>>>
>>>>> >
>>>>> > I'm having trouble imagining what could actually go wrong, though.
>>>>> > Is it possible that a no-op CAS would appear to undo non-volatile
>>>>> > changes in another thread? E.g.:
>>>>> >
>>>>> > T1: r1 = x; // volatile read, sees 0
>>>>> > T1: x = 3; // non-volatile write, not flushed yet
>>>>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>>>>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>>>>
>>>>> I don't think anything would actually go wrong, though we would have
>>>>> to be careful to specify the write to disallow the 0 write in your
>>>>> example.  I just dislike specifying CAS in a new and nonstandard way for no
>>>>> positive benefit.
>>>>>
>>>>> Hans
>>>>>
>>>>> >
>>>>> > Cheers,
>>>>> > Justin
>>>>>
>>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/30262494/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Jan 21 21:06:13 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 12:06:13 +1000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
	fix request)
In-Reply-To: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEMMKMAA.davidcholmes@aapt.net.au>

I'm not clear if AtomicXXX is intended for safe use via unsafe publication; or whether the volatile even aids in that. If not then the volatile write in the constructor seems not strictly necessary. Performance will depend on whether the JIT can inline and remove the native Unsafe call.

David
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Thursday, 22 January 2015 12:00 PM
  To: David Holmes
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


  Sorry, I missed the "static", for which the happens-before relationship clearly exists.  It sounds like we're in agreement that having the assignment in the constructor be volatile is not likely to matter in practice, but the current implementation is cleaner at some performance cost?


  Hans


  On Wed, Jan 21, 2015 at 5:48 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Sharing via _static_ final fields isn't racy.

    David
      -----Original Message-----
      From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm

      Sent: Thursday, 22 January 2015 11:40 AM
      To: David Holmes
      Cc: concurrency-interest at cs.oswego.edu
      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


      What I was suggesting, I think, was to only use unsafe in the constructor (in addition to the places it's currently used).  But I agree with you about cleanliness. 


      I would still be interested in code that shares a final field without an additional happens-before relationship (i.e. in a way that would be racy without the final field rules), and without very clever and subtle reasoning that would have been unnecessary with the addition of a volatile declaration.  I really don't think that's a natural idiom, though it is clearly used for performance on occasion.  And it's sometimes necessary to deal with that case for malware defense.


      Hans


      On Wed, Jan 21, 2015 at 5:13 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

        Hans, the assignment in the constructor may not depend on volatile - but so what? Implementing as an actual volatile field that you can read and write directly is a lot cleaner and simpler than having to use Unsafe for basic loads and stores. As Justin said mutating is far more common than construction.

        As for "more likely" there are two major patterns for sharing objects:
        - share a final reference to a "thread-safe" mutable object (like AtomicInteger)
        - share a volatile reference to an immutable object

        Of course you could also share a violatile reference to a thread-safe mutable object, but that would be the more questionable pattern to me. Certainly needed some times, but not what I would expect to be the prevalent case.

        YMMV.

        David


          -----Original Message-----
          From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm

          Sent: Thursday, 22 January 2015 11:02 AM
          To: David Holmes
          Cc: concurrency-interest at cs.oswego.edu
          Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


          If the object containing level is properly published, I think it still doesn't matter whether the assignment of 17 is treated as volatile or not.  The assignment of 17 doesn't happen-before the later operations on the level field, but is treated as such by the final field rules, since we get to it by a dereference chain from a final field.


          Aside (from the aside from the aside):  I'm surprised by your "more likely" statement.  It would be interesting to know how common it is to intentionally pass objects via final fields without additional happens-before ordering.  My mental mode is that happens in exactly three cases: 


          1) Clever and very tricky performance optimizations that elide "volatile" for concurrently accessed fields.


          2) Bugs.


          3) (Rarely) Malware trying to exploit races.


          Hans




          On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

            Publication is more likely via a final field:

            final static AtomicInteger level = new AtomicInteger(17);

            Setting the internal field happens-before setting of level regardless of whether internal field is volatile.

            level.increment();  // atomic inc doesn't vare if internal field is volatile

            if (level.get() == x) // volatile read of internal field essential

            level.set(0); // reset operation - volatile write essential

            David
            -----
              -----Original Message-----
              From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
              Sent: Thursday, 22 January 2015 9:55 AM
              To: David Holmes

              Cc: concurrency-interest at cs.oswego.edu
              Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


              I suspect we're miscommunicating, but I'm not sure.



              The typical use case for AtomicInteger presumably looks like:


              1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the embedded volatile field.


              2) communicate x to other threads by assigning x to a volatile, passing it to a new thread, using a concurrent data structure, or some synchronized methods.


              3) access x.a in a variety of threads.


              Typically (1) happens-before (3) because (2) established the necessary synchronizes-with relationships.


              (a) If (1) happens-before all instances of (3), there is no way to tell whether the assignment of 17 is treated as volatile or not, and it doesn't matter.


              (b) If (1) does not happen before an instance of (3), then that instance of (3) can see the uninitialized 0  value of x.a rather than the value 17 or a later one.  I claim it is very unlikely for such code to be correct whether or not the assignment of 17 is treated as volatile.


              Thus there are somewhere between zero and very few cases in which the volatile treatment of the assignment of 17 actually matters.  It doesn't matter for any code I would consider to be properly synchronized, since it only matters for cases in which construction of AtomicInteger does not happen-before a call of one of its methods.  I agree that in the Java model it is cleaner to simply treat the assignment as volatile, as is currently done.  But it adds overhead that's not well-justified, i.e. one or more fences for every AtomicInteger construction.


              This of course no longer has anything to do with CAS semantics.


              Hans


              On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

                ??? set/get are direct loads and stores and required to have volatile semantics. An actual read/wrote of a volatile field seems perfect to me.

                David
                  -----Original Message-----
                  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
                  Sent: Thursday, 22 January 2015 9:32 AM
                  To: David Holmes

                  Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
                  Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)


                  That does have the advantage that it's a nice simple model.  It has the disadvantage that you're paying for a property that arguably no reasonable code cares about.  At least I'm having a hard time constructing a useful example where it matters.


                  On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

                    The AtomicXXX classes are supposed to act like volatile fields with the addition of the atomic operations. Hence the actual field inside the AtomicXXX class, is and should be, volatile.

                    David
                      -----Original Message-----
                      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
                      Sent: Thursday, 22 January 2015 9:05 AM
                      To: Justin Sampson
                      Cc: concurrency-interest at cs.oswego.edu; David Holmes
                      Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc fix request)





                      On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <jsampson at guidewire.com> wrote:
                      >
                      > Hans Boehm wrote:
                      >
                      > > You're right that there is currently no sanctioned way to access
                      > > the same object both as a volatile and non-volatile. But there
                      > > seem to be good reasons for supporting that. E.g. the pointer/flag
                      > > assignment in double-checked locking doesn't race and hence could
                      > > be non-volatile.
                      >
                      > Do you mean that the write marked #2 below could be non-volatile,
                      > while the read marked #1 is still volatile?
                      >
                      >   private volatile Something instance;
                      >
                      >   public Something getInstance() {
                      >     if (instance == null) { // #1
                      >       synchronized (this) {
                      >         if (instance == null) // #4: added by HB
                      >           instance = new Something(); // #2
                      >       } // #3
                      >     }
                      >     return instance;
                      >   }
                      >
                      > If so, I don't think that works. Another thread calling getInstance
                      > could still see the Something not fully initialized, if its line #1
                      > executes between the writing thread's lines #2 and #3 (exiting the
                      > monitor). 


                      Sorry.  You're right; I misstated that.  It's the load at #4 that doesn't need to be volatile since racing writes are locked out anyway.

                      >
                      > > (Does the constructor for e.g. AtomicInteger perform a volatile
                      > > assignment? The documentation appears unclear. [...])
                      >
                      > This the latest code in CVS:
                      >
                      >   private volatile int value;
                      >
                      >   /**
                      >    * Creates a new AtomicInteger with the given initial value.
                      >    *
                      >    * @param initialValue the initial value
                      >    */
                      >   public AtomicInteger(int initialValue) {
                      >     value = initialValue;
                      >   }
                      >
                      > http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup


                      Interesting.  There's clearly not much of a reason for this to be volatile.  Any code that cared about ordering would also have to be prepared to see a pre-initialization zero value, which seems extremely unlikely.  C++11 carefully defines the initialization not to be atomic.

                      >
                      > > But the notion of intentionally mis-specifying an operation to
                      > > write when it really doesn't just seems wrong to me.
                      >
                      > You and I are agreeing on this point. :)
                      >
                      > But you also have to be prepared that the implementation _might_
                      > actually do a write. Don't some CPU architectures implement CAS with
                      > a store cycle to main memory regardless of success or failure, just
                      > inserting the current value in the store buffer late in the cycle if
                      > it didn't match the expected value? Therefore if your code is
                      > actually sensitive to whether other threads are indeed writing that
                      > memory location, it's going to be buggy on those architectures.


                      Indeed, now that I checked, the Intel x86 documentation seems to say that in places, though that part of the description looks quite dated to me.  I have always been told that modern processors acquire the cache line in exclusive mode and then perform the operation in cache.  And performance numbers seem to confirm that. 


                      In either case, this isn't really programmer visible one way or the other.  On an ll/sc architecture like ARM, it would generally be silly to do the store unconditionally, and we want to discourage that.  If the hardware does it under the covers (which I doubt for modern hardware; gratuitously dirtying the cache line doesn't seem free), so be it.


                      I'm just arguing for a description like the current one in the specific class descriptions (and the CAS description in Wikipedia and most other places) that doesn't gratuitously look like it imposes additional requirements, and actually does impose expensive and useless ordering requirements on some important architectures.


                      >
                      > I'm having trouble imagining what could actually go wrong, though.
                      > Is it possible that a no-op CAS would appear to undo non-volatile
                      > changes in another thread? E.g.:
                      >
                      > T1: r1 = x; // volatile read, sees 0
                      > T1: x = 3; // non-volatile write, not flushed yet
                      > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
                      > T1: r2 = x; // non-volatile read, could see either 3 or 0?


                      I don't think anything would actually go wrong, though we would have to be careful to specify the write to disallow the 0 write in your example.  I just dislike specifying CAS in a new and nonstandard way for no positive benefit.


                      Hans

                      >
                      > Cheers,
                      > Justin









-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/4c20a4dc/attachment-0001.html>

From jsampson at guidewire.com  Wed Jan 21 21:14:11 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 22 Jan 2015 02:14:11 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>

Hans Boehm wrote:

> Sorry, I missed the "static", for which the happens-before
> relationship clearly exists. It sounds like we're in agreement
> that having the assignment in the constructor be volatile is not
> likely to matter in practice, but the current implementation is
> cleaner at some performance cost?

Well, to be quite thorough:

Publishing via static initializer (final or not) is non-racy because
it occurs during initialization of the containing class, which is
synchronized by the JVM.

Publishing via instance final is non-racy because it creates a
pseudo-happens-before edge (JLS section 17.5.1) from the end of the
containing object's constructor to any dereference. I say "pseudo"
because it doesn't transitively close with other happens-before
edges, but it's good enough for anything referenced indirectly from
the final field.

But it really doesn't matter how an AtomicWhatever is published, as
long as the write in the constructor is volatile.

AtomicInteger x = null; // NOT volatile

T1: x = new AtomicInteger(5)  

T2: r1 = x.get() // either NPE or 5, cannot be 0

There's not a happens-before from T1's write of x to T2's read of x,
but there _is_ a happens-before from T1's write of the AtomicInteger
instance's value field to T2's read of that same instance's value
field (unless its read of x sees null, in which case it doesn't read
the instance's value field at all).

Cheers,
Justin


From TEREKHOV at de.ibm.com  Wed Jan 21 21:34:38 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Thu, 22 Jan 2015 03:34:38 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1Yt-T=w0iGh9GGFdGgAdVhxZnHX5QVuASupi_dgMYPmUQ@mail.gmail.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>	<OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
	<CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>	<OFE847C215.FFE41248-ONC1257DD4.00729225-C1257DD4.00739BFD@de.ibm.com>
	<CAPUmR1Yt-T=w0iGh9GGFdGgAdVhxZnHX5QVuASupi_dgMYPmUQ@mail.gmail.com>
Message-ID: <OFB7E11BF2.4B4C0F9A-ONC1257DD5.000DBD14-C1257DD5.000E2863@de.ibm.com>

you mean a mapping with a traling hwsync:

volatile load: ld; hwsync;
volatile store: lwsync; st; hwsync
CAS: lwsync; _loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit: hwsync

and you are surprised because it imposes less penalty on loads (but more on
stores), right?

Well, I suppose that the 'conspiracy' was based on the idea to preserve
isync to make it easier to reason while relaxing unnecessary SC to (at
least) release-acquire:

acquire load: ld; cmp; bd; isync
realease store: lwsync; st;
CAS.rel-acq: lwsync ;_loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit:
isync;

because for the most programs hwsync is not really needed...

regards,
alexander.

Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 22:48:48

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>, David Holmes
       <dholmes at ieee.org>
Subject:	Re: [concurrency-interest] Varieties of CAS semantics (another
       doc fix request)


Yes, your recommendation mirrors that for C++ seq_cst operations in
http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html, and I think those
are the semi-official IBM recommendations.? The Java and C++ mappings
should be the same to ensure correct interoperation.

(I continue to be surprised that the choice was made to use two fences on a
load instead of a store.? I think either choice is correct, so long as it's
used consistently.)

On Wed, Jan 21, 2015 at 1:02 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      err... I forgot that under Java it is not possible to do anything
      less
      heavier than SC...

      so what you want on powerpc is

      volatile load: hwsync; ld; cmp; bd; isync
      volatile store: hwsync; st
      CAS: hwsync;_loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit:
      isync

      right?

      regards,
      alexander.


      Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 17:53:02

      Sent by:? ? ? ? concurrency-interest-bounces at cs.oswego.edu


      To:? ? ?Alexander Terekhov/Germany/IBM at IBMDE
      cc:? ? ?"concurrency-interest at cs.oswego.edu"
      ? ? ? ?<concurrency-interest at cs.oswego.edu>, David Holmes
      ? ? ? ?<dholmes at ieee.org>
      Subject:? ? ? ? Re: [concurrency-interest] Varieties of CAS semantics
      (another
      ? ? ? ?doc fix request)


      Alexander -

      I don't understand the IRIW argument.? We currently promise to
      prevent the
      IRIW for all volatiles, whether updated with CAS or not.? I think
      that
      remains true in any of the formulations and with any of the proposed
      implementations ?(though it's less clear in the fence-based
      definitions).
      ARMv8s release-store becomes visible at the same time to all other
      cores.
      I don't think my formulation adds overhead anywhere.

      Hans

      On Wed, Jan 21, 2015 at 1:30 AM, Alexander Terekhov <
      TEREKHOV at de.ibm.com>
      wrote:
      ? ? ? Hans Boehm <boehm at acm.org> wrote:
      ? ? ? [...]
      ? ? ? > We'd be complicating the model, misleading both implementors
      and
      ? ? ? users,
      ? ? ? > and probably eventually slowing down implementations on a
      very
      ? ? ? important
      ? ? ? > machine architecture, for no real benefit.

      ? ? ? Well, with

      ? ? ? "compareAndSet and all other read-and-update operations such as
      ? ? ? getAndIncrement have the memory effects of both reading and
      writing
      ? ? ? volatile variables."

      ? ? ? IRIW would work without most heavy fences by using
      non-modifying RMWs
      ? ? ? for loads 'as if stores are made atomically with respect to all
      ? ? ? threads
      ? ? ? performing an RMW'.

      ? ? ? That's kind of a benefit, oder?

      ? ? ? regards,
      ? ? ? alexander.

      ? ? ? Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 03:21:04

      ? ? ? Sent by:? ? ? ? concurrency-interest-bounces at cs.oswego.edu


      ? ? ? To:? ? ?David Holmes <dholmes at ieee.org>
      ? ? ? cc:? ? ?"concurrency-interest at cs.oswego.edu"
      ? ? ? ? ? ? ?<concurrency-interest at cs.oswego.edu>
      ? ? ? Subject:? ? ? ? Re: [concurrency-interest] Varieties of CAS
      semantics
      ? ? ? (another
      ? ? ? ? ? ? ?doc fix request)



      ? ? ? On Tue, Jan 20, 2015 at 5:29 PM, David Holmes <
      ? ? ? davidcholmes at aapt.net.au>
      ? ? ? wrote:
      ? ? ? ? ? ? I'm struggling to follow this as we seem to be
      continually
      ? ? ? switching
      ? ? ? ? ? ? between different layers of abstraction. The spec
      says?CAS acts
      ? ? ? as a
      ? ? ? ? ? ? volatile read plus volatile write.

      ? ? ? That's not my reading, though I will admit it could be
      misinterpreted
      ? ? ? that
      ? ? ? way. :-)

      ? ? ? ? ? ? So your mental model for this can simply be that the
      final
      ? ? ? action of
      ? ? ? ? ? ? the CAS is a write: either the new value if the CAS was
      ? ? ? successful,
      ? ? ? ? ? ? or the existing value if not. That's a conceptual model
      -?no
      ? ? ? ? ? ? implementation?need actually do an explicit write.

      ? ? ? The CAS spec in the individual j.u.c.a classes says (admittedly
      not
      ? ? ? quite
      ? ? ? as clearly as it could) that only a successful CAS writes
      anything:

      ? ? ? "Atomically sets the value to the given updated value if the
      current
      ? ? ? value
      ? ? ? == the expected value."

      ? ? ? The top level j.u.c.a spec says:

      ? ? ? "compareAndSet and all other read-and-update operations such as
      ? ? ? getAndIncrement have the memory effects of both reading and
      writing
      ? ? ? volatile variables."

      ? ? ? which I read as "all read and write operations performed are
      ? ? ? volatile", not
      ? ? ? as overriding the individual CAS specs to say that the variable
      is
      ? ? ? now
      ? ? ? unconditionally written.

      ? ? ? Officially requiring unconditional writes would have the
      unfortunate
      ? ? ? side
      ? ? ? effect that e.g. code that "locks" an integer field by using a
      CAS to
      ? ? ? set a
      ? ? ? bit, and then accesses the other bits with non-volatile
      operations,
      ? ? ? is now
      ? ? ? officially no longer "correctly synchronized", since it "races"
      with
      ? ? ? CASes
      ? ? ? in other threads that unsuccessfully try to set the lock bit. I
      would
      ? ? ? really rather not say "CAS always writes the value, but
      implementors
      ? ? ? shouldn't really implement it that way, and programmers and
      race
      ? ? ? detectors
      ? ? ? should ignore the bogus data races introduced by our saying
      that".
      ? ? ? We'd be
      ? ? ? complicating the model, misleading both implementors and users,
      and
      ? ? ? probably eventually slowing down implementations on a very
      important
      ? ? ? machine architecture, for no real benefit.


      ? ? ? ? ? ? Each implementation then has to provide the necessary
      memory
      ? ? ? barriers
      ? ? ? ? ? ? to effect the?conceptual model.

      ? ? ? Except that not all architectures enforce ordering with
      ? ? ? fences/barriers.
      ? ? ? Itanium and ARMv8 often use constraints on load and store
      ? ? ? instructions
      ? ? ? instead.? Which are semantically different from
      fences/barriers.


      ? ? ? ? ? ? With regard to volatile read/write implementation in
      hotspot,
      ? ? ? yes it
      ? ? ? ? ? ? is stronger than needed. As has been discussed numerous
      times,
      ? ? ? at
      ? ? ? ? ? ? least in C1/interpreter/Unsafe hotspot doesn't consider
      the
      ? ? ? pairing
      ? ? ? ? ? ? of the loads and stores but only looks at the current
      ? ? ? load/store. So
      ? ? ? ? ? ? it has to emit the worst-case barriers needed.

      ? ? ? We're not talking about any sort of more global analysis here.
      The
      ? ? ? problem
      ? ? ? is that even an entirely dumb compiler on ARMv8 or Itanium
      should
      ? ? ? translate
      ? ? ? a volatile load to an acquire-load instruction, not a load plus
      some
      ? ? ? sort
      ? ? ? of fence.

      ? ? ? Hans

      ? ? ? ? ? ? C2 may be more clever about this but I don't know - and
      ? ? ? wouldn't be
      ? ? ? ? ? ? surprised if it isn't.

      ? ? ? ? ? ? David

      ? ? ? ? ? ? -----Original Message-----
      ? ? ? ? ? ? From: concurrency-interest-bounces at cs.oswego.edu [mailto:
      ? ? ? ? ? ? concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
      Hans
      ? ? ? Boehm
      ? ? ? ? ? ? Sent: Wednesday, 21 January 2015 11:07 AM
      ? ? ? ? ? ? To: David Holmes
      ? ? ? ? ? ? Cc: concurrency-interest at cs.oswego.edu
      ? ? ? ? ? ? Subject: Re: [concurrency-interest] Varieties of CAS
      semantics
      ? ? ? ? ? ? (another doc fix request)

      ? ? ? ? ? ? My primary concern with insisting on "release" semantics
      for a
      ? ? ? failed
      ? ? ? ? ? ? CAS is that it really doesn't make sense in the memory
      model.
      ? ? ? A
      ? ? ? ? ? ? "release" operation is defined by a "synchronizes with"
      edge
      ? ? ? from the
      ? ? ? ? ? ? write.? A failed CAS has no associated write.? What does
      such a
      ? ? ? ? ? ? "release" operation mean?? Can someone propose a
      definition
      ? ? ? ? ? ? consistent with the current language spec?? I think we
      would
      ? ? ? have to
      ? ? ? ? ? ? change the spec to include an unconditional write, which
      seems
      ? ? ? quite
      ? ? ? ? ? ? contrived and misleading.? It also adds bogus data races
      to
      ? ? ? some code
      ? ? ? ? ? ? (though I don't know how common such code is).

      ? ? ? ? ? ? Remember that the current memory model says nothing about
      ? ? ? fences,
      ? ? ? ? ? ? which are in fact quite hard to define in a portable way.

      ? ? ? ? ? ? I don't know of any natural examples where such "release"
      ? ? ? semantics
      ? ? ? ? ? ? for a failed CAS might matter, though clearly Justin
      ? ? ? successfully
      ? ? ? ? ? ? contrived one.? Since such failed operations have no
      effect on
      ? ? ? the
      ? ? ? ? ? ? shared state (again no write), it's really difficult for
      ? ? ? another
      ? ? ? ? ? ? thread to tell they completed and somehow try to rely on
      that
      ? ? ? fact.
      ? ? ? ? ? ? Without that the release semantics are invisible.

      ? ? ? ? ? ? I agree that most conventional implementations provide
      some
      ? ? ? sort of
      ? ? ? ? ? ? fence-like semantics even in the failure case.? But I
      think the
      ? ? ? ? ? ? natural implementation on ARMv8 does not.? The code would
      do a
      ? ? ? ? ? ? load-exclusive-acquire, compare to the expected value,
      and just
      ? ? ? skip
      ? ? ? ? ? ? the release store if the comparison failed. ?(I think the
      ? ? ? verdict is
      ? ? ? ? ? ? still out on whether this will eventually translate into
      real
      ? ? ? ? ? ? improved performance over fence-based models.? But I
      believe it
      ? ? ? ? ? ? reflects current programming advice for ARMv8.)

      ? ? ? ? ? ? The release; store; fence implementation of a volatile
      store in
      ? ? ? ? ? ? hotspot is actually overkill in two ways, neither of
      which are
      ? ? ? ? ? ? required by the Java memory model:

      ? ? ? ? ? ? 1) Separating out the initial release as a fence orders
      prior
      ? ? ? ? ? ? operations with respect to ALL subsequent stores, not
      just the
      ? ? ? ? ? ? volatile store.? That's unnecessary.? Later non-volatile
      stores
      ? ? ? can
      ? ? ? ? ? ? advance past the volatile one without violating the
      memory
      ? ? ? model.
      ? ? ? ? ? ? This affects ARMv8, Itanium, and compiler
      transformations.

      ? ? ? ? ? ? 2) The fence orders all prior and later memory
      operations.? The
      ? ? ? only
      ? ? ? ? ? ? required ordering is between this particular volatile
      store and
      ? ? ? ? ? ? subsequent VOLATILE loads.? Those volatile loads are
      often a
      ? ? ? long
      ? ? ? ? ? ? ways off, and there is no reason to stall anything until
      you
      ? ? ? ? ? ? encounter one.? This affects ARMv8 and compiler
      ? ? ? transformations.
      ? ? ? ? ? ? ARMv8 guarantees the correct ordering constraint with
      respect
      ? ? ? to only
      ? ? ? ? ? ? later acquire loads. ?(Itanium's st.rel needs a trailing
      ? ? ? fence.)

      ? ? ? ? ? ? This implementation model also doesn't seem to correctly
      ? ? ? reflect
      ? ? ? ? ? ? store atomicity considerations.? You have to model a
      volatile
      ? ? ? load as
      ? ? ? ? ? ? load; acquire.? That works on Power if a "release" is an
      ? ? ? LWSYNC,
      ? ? ? ? ? ? fence is SYNC, but ?"acquire" is also SYNC, which seems
      very
      ? ? ? weird.

      ? ? ? ? ? ? I think all major VMs currently do things along these
      lines.
      ? ? ? But I
      ? ? ? ? ? ? don't think this is the right implementation model to
      strive
      ? ? ? for; it
      ? ? ? ? ? ? was a great initial approach that we should be trying to
      move
      ? ? ? away
      ? ? ? ? ? ? from.

      ? ? ? ? ? ? Hans

      ? ? ? ? ? ? On Tue, Jan 20, 2015 at 3:36 PM, David Holmes <
      ? ? ? ? ? ? davidcholmes at aapt.net.au> wrote:
      ? ? ? ? ? ? ? ? ? Hans,

      ? ? ? ? ? ? ? ? ? Given the CAS is native we can insert anything we
      want to
      ? ? ? make
      ? ? ? ? ? ? ? ? ? this look like a volatile write happened, even if
      there
      ? ? ? is no
      ? ? ? ? ? ? ? ? ? actual write. Given a volatile write in hotspot has
      the
      ? ? ? effect
      ? ? ? ? ? ? ? ? ? of:

      ? ? ? ? ? ? ? ? ? release; store; fence

      ? ? ? ? ? ? ? ? ? then we can insert a fence after the CAS if needed.
      (This
      ? ? ? is
      ? ? ? ? ? ? ? ? ? why I'm still unsure the ARMv8 st.rel is sufficient
      for a
      ? ? ? ? ? ? ? ? ? volatile write in the general case.)

      ? ? ? ? ? ? ? ? ? I've gone back to check my archives on this and
      AFAICS
      ? ? ? the
      ? ? ? ? ? ? ? ? ? intent to indicate the effect of volatile write
      even on a
      ? ? ? ? ? ? ? ? ? failed (strong) CAS was deliberate.

      ? ? ? ? ? ? ? ? ? David
      ? ? ? ? ? ? ? ? ? ?-----Original Message-----
      ? ? ? ? ? ? ? ? ? ?From: hjkhboehm at gmail.com [mailto:
      hjkhboehm at gmail.com]On
      ? ? ? ? ? ? ? ? ? ?Behalf Of Hans Boehm
      ? ? ? ? ? ? ? ? ? ?Sent: Wednesday, 21 January 2015 9:15 AM
      ? ? ? ? ? ? ? ? ? ?To: Justin Sampson
      ? ? ? ? ? ? ? ? ? ?Cc: Andrew Haley; Vitaly Davidovich;
      ? ? ? ? ? ? ? ? ? ?concurrency-interest at cs.oswego.edu;
      dholmes at ieee.org
      ? ? ? ? ? ? ? ? ? ?Subject: Re: [concurrency-interest] Varieties of
      CAS
      ? ? ? semantics
      ? ? ? ? ? ? ? ? ? ?(another doc fix request)

      ? ? ? ? ? ? ? ? ? ?No disagreement about this benefitting from
      ? ? ? clarification.
      ? ? ? ? ? ? ? ? ? ?But I still don't think a failed CAS can possibly
      have
      ? ? ? ? ? ? ? ? ? ?release/volatile write semantics, given that the
      ? ? ? compareAndSet
      ? ? ? ? ? ? ? ? ? ?definition states:

      ? ? ? ? ? ? ? ? ? ?"Atomically sets the value to the given updated
      value if
      ? ? ? the
      ? ? ? ? ? ? ? ? ? ?current value == the expected value."

      ? ? ? ? ? ? ? ? ? ?Release/volatile write semantics only make sense
      if
      ? ? ? there was
      ? ? ? ? ? ? ? ? ? ?a write. ? In the failure case there isn't. ?(See
      ? ? ? 17.4.4: "A
      ? ? ? ? ? ? ? ? ? ?write to a volatile variable v (?8.3.1.4)
      ? ? ? synchronizes-with
      ? ? ? ? ? ? ? ? ? ?all subsequent reads of v by any thread (where
      ? ? ? "subsequent" is
      ? ? ? ? ? ? ? ? ? ?defined according to the synchronization order)."
      There
      ? ? ? is no
      ? ? ? ? ? ? ? ? ? ?synchronizes with relationship involving volatiles
      ? ? ? unless
      ? ? ? ? ? ? ? ? ? ?there is a "write to a volatile variable".? A
      failed CAS
      ? ? ? ? ? ? ? ? ? ?fairly unambiguously doesn't write anything.)

      ? ? ? ? ? ? ? ? ? ?T2s CAS is not guaranteed to happen before T3s CAS
      ? ? ? because
      ? ? ? ? ? ? ? ? ? ?there is no volatile write in T2 which could
      possibly
      ? ? ? ? ? ? ? ? ? ?synchronize with anything in T3.

      ? ? ? ? ? ? ? ? ? ?Answer 2 is the only one that currently makes
      sense in
      ? ? ? the
      ? ? ? ? ? ? ? ? ? ?Java memory model.? To change that we would have
      to
      ? ? ? change the
      ? ? ? ? ? ? ? ? ? ?CAS definition so that it unconditionally writes
      ? ? ? something,
      ? ? ? ? ? ? ? ? ? ?possibly the original value.

      ? ? ? ? ? ? ? ? ? ?Hans

      ? ? ? ? ? ? ? ? ? ?On Tue, Jan 20, 2015 at 2:12 PM, Justin Sampson <
      ? ? ? ? ? ? ? ? ? ?jsampson at guidewire.com
      > wrote:
      ? ? ? ? ? ? ? ? ? ? ? ? ?I wrote:

      ? ? ? ? ? ? ? ? ? ? ? ? ?> Here's a practical question from the API
      user's
      ? ? ? ? ? ? ? ? ? ? ? ? ?perspective: Does
      ? ? ? ? ? ? ? ? ? ? ? ? ?> the reordering you described a few
      messages ago
      ? ? ? have
      ? ? ? ? ? ? ? ? ? ? ? ? ?any
      ? ? ? ? ? ? ? ? ? ? ? ? ?> observable effects in the behavior of a
      program
      ? ? ? ? ? ? ? ? ? ? ? ? ?relative to the
      ? ? ? ? ? ? ? ? ? ? ? ? ?> CAS being truly atomic?

      ? ? ? ? ? ? ? ? ? ? ? ? ?Okay, here's an example where it makes a
      ? ? ? difference. The
      ? ? ? ? ? ? ? ? ? ? ? ? ?atomicity
      ? ? ? ? ? ? ? ? ? ? ? ? ?issue is kind of secondary to the original
      ? ? ? question I
      ? ? ? ? ? ? ? ? ? ? ? ? ?raised, which
      ? ? ? ? ? ? ? ? ? ? ? ? ?was whether a failed CAS must have the
      memory
      ? ? ? effects of
      ? ? ? ? ? ? ? ? ? ? ? ? ?a volatile
      ? ? ? ? ? ? ? ? ? ? ? ? ?write. In Peter's example of a synchronized
      block
      ? ? ? ? ? ? ? ? ? ? ? ? ?implementation,
      ? ? ? ? ? ? ? ? ? ? ? ? ?roach motel reordering is allowed but
      there's
      ? ? ? still a
      ? ? ? ? ? ? ? ? ? ? ? ? ?memory release
      ? ? ? ? ? ? ? ? ? ? ? ? ?even in the case of failure. Indeed, a
      ? ? ? synchronized
      ? ? ? ? ? ? ? ? ? ? ? ? ?block is a
      ? ? ? ? ? ? ? ? ? ? ? ? ?perfect example of something that provides a
      very
      ? ? ? strong
      ? ? ? ? ? ? ? ? ? ? ? ? ?appearance
      ? ? ? ? ? ? ? ? ? ? ? ? ?of atomicity to the programmer even though
      it
      ? ? ? allows
      ? ? ? ? ? ? ? ? ? ? ? ? ?roach motel
      ? ? ? ? ? ? ? ? ? ? ? ? ?reordering. So by itself that's kind of a
      red
      ? ? ? herring.
      ? ? ? ? ? ? ? ? ? ? ? ? ?Instead,
      ? ? ? ? ? ? ? ? ? ? ? ? ?let's look at an example that gets at the
      combined
      ? ? ? ? ? ? ? ? ? ? ? ? ?effects of both
      ? ? ? ? ? ? ? ? ? ? ? ? ?issues:

      ? ? ? ? ? ? ? ? ? ? ? ? ?int a, b; // NOT volatile
      ? ? ? ? ? ? ? ? ? ? ? ? ?final AtomicInteger x = new AtomicInteger
      (0);

      ? ? ? ? ? ? ? ? ? ? ? ? ?T1:
      ? ? ? ? ? ? ? ? ? ? ? ? ?a = 1;
      ? ? ? ? ? ? ? ? ? ? ? ? ?r1 = x.compareAndSet(0, 1);

      ? ? ? ? ? ? ? ? ? ? ? ? ?T2:
      ? ? ? ? ? ? ? ? ? ? ? ? ?b = 1;
      ? ? ? ? ? ? ? ? ? ? ? ? ?r2 = x.compareAndSet(0, 1);

      ? ? ? ? ? ? ? ? ? ? ? ? ?T3:
      ? ? ? ? ? ? ? ? ? ? ? ? ?r3 = x.compareAndSet(1, 0);
      ? ? ? ? ? ? ? ? ? ? ? ? ?r4 = a;
      ? ? ? ? ? ? ? ? ? ? ? ? ?r5 = b;

      ? ? ? ? ? ? ? ? ? ? ? ? ?After joining T1, T2, & T3:
      ? ? ? ? ? ? ? ? ? ? ? ? ?r6 = x.get();

      ? ? ? ? ? ? ? ? ? ? ? ? ?Question: Given (r1,r2,r3,r6) =
      ? ? ? (true,false,true,0),
      ? ? ? ? ? ? ? ? ? ? ? ? ?what are the
      ? ? ? ? ? ? ? ? ? ? ? ? ?possible values observed for r4 and r5?

      ? ? ? ? ? ? ? ? ? ? ? ? ?Premise: Assuming no spurious failures, r6 =
      0
      ? ? ? means
      ? ? ? ? ? ? ? ? ? ? ? ? ?that T3's CAS
      ? ? ? ? ? ? ? ? ? ? ? ? ?was the final update applied to x; and r2 =
      false
      ? ? ? means
      ? ? ? ? ? ? ? ? ? ? ? ? ?that T2's
      ? ? ? ? ? ? ? ? ? ? ? ? ?CAS saw the result of T1's CAS. (I've
      actually
      ? ? ? ? ? ? ? ? ? ? ? ? ?over-specified the
      ? ? ? ? ? ? ? ? ? ? ? ? ?givens, because (r2,r6) = (false,0) implies
      ? ? ? (r1,r3) =
      ? ? ? ? ? ? ? ? ? ? ? ? ?(true,true).)

      ? ? ? ? ? ? ? ? ? ? ? ? ?Answer 1: If every CAS is perfectly atomic
      _and_
      ? ? ? has
      ? ? ? ? ? ? ? ? ? ? ? ? ?both memory
      ? ? ? ? ? ? ? ? ? ? ? ? ?acquire and memory release effects
      regardless of
      ? ? ? success
      ? ? ? ? ? ? ? ? ? ? ? ? ?or
      ? ? ? ? ? ? ? ? ? ? ? ? ?failure, then T1's and T2's CAS's both
      ? ? ? happen-before
      ? ? ? ? ? ? ? ? ? ? ? ? ?T3's CAS,
      ? ? ? ? ? ? ? ? ? ? ? ? ?such that r4 and r5 must both be true.

      ? ? ? ? ? ? ? ? ? ? ? ? ?Answer 2: If a failed CAS does _not_ have
      memory
      ? ? ? release
      ? ? ? ? ? ? ? ? ? ? ? ? ?effects,
      ? ? ? ? ? ? ? ? ? ? ? ? ?then T1's CAS happens-before T3's CAS but
      T2's CAS
      ? ? ? does
      ? ? ? ? ? ? ? ? ? ? ? ? ?_not_
      ? ? ? ? ? ? ? ? ? ? ? ? ?happen-before T3's CAS, so r4 must be true
      but r5
      ? ? ? could
      ? ? ? ? ? ? ? ? ? ? ? ? ?be true or
      ? ? ? ? ? ? ? ? ? ? ? ? ?false (it's racy).

      ? ? ? ? ? ? ? ? ? ? ? ? ?Answer 3: If a failed CAS _does_ have memory
      ? ? ? release
      ? ? ? ? ? ? ? ? ? ? ? ? ?effects but
      ? ? ? ? ? ? ? ? ? ? ? ? ?each CAS is _not_ perfectly atomic (i.e.,
      roach
      ? ? ? motel
      ? ? ? ? ? ? ? ? ? ? ? ? ?reordering but
      ? ? ? ? ? ? ? ? ? ? ? ? ?not actually using a mutex), then things get
      ? ? ? ? ? ? ? ? ? ? ? ? ?interesting. T2's write
      ? ? ? ? ? ? ? ? ? ? ? ? ?to b might be reordered to right before the
      ? ? ? release
      ? ? ? ? ? ? ? ? ? ? ? ? ?barrier of its
      ? ? ? ? ? ? ? ? ? ? ? ? ?CAS and T3's read of b might be reordered to
      right
      ? ? ? after
      ? ? ? ? ? ? ? ? ? ? ? ? ?the acquire
      ? ? ? ? ? ? ? ? ? ? ? ? ?barrier of its CAS. Since the CAS's aren't
      ? ? ? perfectly
      ? ? ? ? ? ? ? ? ? ? ? ? ?atomic or
      ? ? ? ? ? ? ? ? ? ? ? ? ?mutually exclusive, T3's CAS could be
      executed
      ? ? ? between
      ? ? ? ? ? ? ? ? ? ? ? ? ?the load and
      ? ? ? ? ? ? ? ? ? ? ? ? ?the store of T2's CAS. T2's CAS would not be
      ? ? ? affected,
      ? ? ? ? ? ? ? ? ? ? ? ? ?because it's
      ? ? ? ? ? ? ? ? ? ? ? ? ?going to report failure either way. T3's CAS
      could
      ? ? ? then
      ? ? ? ? ? ? ? ? ? ? ? ? ?succeed, but
      ? ? ? ? ? ? ? ? ? ? ? ? ?with T3's read of b failing to see T2's
      write of b
      ? ? ? due
      ? ? ? ? ? ? ? ? ? ? ? ? ?to their
      ? ? ? ? ? ? ? ? ? ? ? ? ?reordering. Therefore in this case as well,
      r5
      ? ? ? could be
      ? ? ? ? ? ? ? ? ? ? ? ? ?true or
      ? ? ? ? ? ? ? ? ? ? ? ? ?false.

      ? ? ? ? ? ? ? ? ? ? ? ? ?As I read the current docs, compareAndSet is
      most
      ? ? ? ? ? ? ? ? ? ? ? ? ?definitely
      ? ? ? ? ? ? ? ? ? ? ? ? ?described as an _atomic_ operation with
      _both_
      ? ? ? volatile
      ? ? ? ? ? ? ? ? ? ? ? ? ?read and
      ? ? ? ? ? ? ? ? ? ? ? ? ?volatile write memory effects, supporting
      Answer
      ? ? ? 1, such
      ? ? ? ? ? ? ? ? ? ? ? ? ?that the
      ? ? ? ? ? ? ? ? ? ? ? ? ?example code above is _not_ racy.

      ? ? ? ? ? ? ? ? ? ? ? ? ?If CAS is allowed _either_ to be
      not-quite-atomic
      ? ? ? _or_
      ? ? ? ? ? ? ? ? ? ? ? ? ?to not have
      ? ? ? ? ? ? ? ? ? ? ? ? ?volatile write effects on failure, the
      example
      ? ? ? code
      ? ? ? ? ? ? ? ? ? ? ? ? ?above _is_ racy.

      ? ? ? ? ? ? ? ? ? ? ? ? ?Since there is a definite difference in the
      ? ? ? correctness
      ? ? ? ? ? ? ? ? ? ? ? ? ?implications
      ? ? ? ? ? ? ? ? ? ? ? ? ?of these two interpretations, and since _at
      least
      ? ? ? two_
      ? ? ? ? ? ? ? ? ? ? ? ? ?compareAndSet
      ? ? ? ? ? ? ? ? ? ? ? ? ?implementations in j.u.c.atomic have the
      latter
      ? ? ? ? ? ? ? ? ? ? ? ? ?semantics anyway
      ? ? ? ? ? ? ? ? ? ? ? ? ?(AtomicStampedReference and
      ? ? ? AtomicMarkableReference), it
      ? ? ? ? ? ? ? ? ? ? ? ? ?seems worth
      ? ? ? ? ? ? ? ? ? ? ? ? ?clarifying in the docs.

      ? ? ? ? ? ? ? ? ? ? ? ? ?Cheers,
      ? ? ? ? ? ? ? ? ? ? ? ? ?Justin


      ? ? ? _______________________________________________
      ? ? ? Concurrency-interest mailing list
      ? ? ? Concurrency-interest at cs.oswego.edu
      ? ? ? http://cs.oswego.edu/mailman/listinfo/concurrency-interest


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From jsampson at guidewire.com  Wed Jan 21 21:35:24 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 22 Jan 2015 02:35:24 +0000
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
In-Reply-To: <CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
	<CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CEC5@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Have you looked into seeing if Semaphore (with fairness) would
> help? Your use case sounds like the classical resource usage
> throttle, with heap being the resource.

Ah, of course! A fair Semaphore would maintain the ordering because
there's no distinction between lock and wait queue.

Indeed, to take a quick flashback, another recent discussion we had
about Semaphore semantics established the advice that if various
threads are passing different values to acquire(N), as they would be
in this case, it ONLY makes sense if the Semaphore is fair. The
behavior is potentially surprising otherwise.

Cheers,
Justin


From vitalyd at gmail.com  Wed Jan 21 21:43:13 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 21:43:13 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>

Don't think that's true; the write of the reference can be reordered with
the volatile write, leading to 0.  Volatile store prevents prior writes
from moving past it, not subsequent ones.

sent from my phone
On Jan 21, 2015 9:40 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> Hans Boehm wrote:
>
> > Sorry, I missed the "static", for which the happens-before
> > relationship clearly exists. It sounds like we're in agreement
> > that having the assignment in the constructor be volatile is not
> > likely to matter in practice, but the current implementation is
> > cleaner at some performance cost?
>
> Well, to be quite thorough:
>
> Publishing via static initializer (final or not) is non-racy because
> it occurs during initialization of the containing class, which is
> synchronized by the JVM.
>
> Publishing via instance final is non-racy because it creates a
> pseudo-happens-before edge (JLS section 17.5.1) from the end of the
> containing object's constructor to any dereference. I say "pseudo"
> because it doesn't transitively close with other happens-before
> edges, but it's good enough for anything referenced indirectly from
> the final field.
>
> But it really doesn't matter how an AtomicWhatever is published, as
> long as the write in the constructor is volatile.
>
> AtomicInteger x = null; // NOT volatile
>
> T1: x = new AtomicInteger(5)
>
> T2: r1 = x.get() // either NPE or 5, cannot be 0
>
> There's not a happens-before from T1's write of x to T2's read of x,
> but there _is_ a happens-before from T1's write of the AtomicInteger
> instance's value field to T2's read of that same instance's value
> field (unless its read of x sees null, in which case it doesn't read
> the instance's value field at all).
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/ea0c0929/attachment.html>

From vitalyd at gmail.com  Wed Jan 21 21:48:51 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 21:48:51 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEMMKMAA.davidcholmes@aapt.net.au>
References: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEMMKMAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37F37NFJ9csjbs4QbCyDwYUFBxFuhOtyUt+1=dwC0Qs+Cw@mail.gmail.com>

FWIW most of the Unsafe class has intrinsics in C2, including the various
put and get calls.

But coming back to AtomicXXX case, I agree with earlier comments that these
classes aren't typically constructed over and over, so any performance
gains here would unlikely to actually be detectable in real usage.

sent from my phone
On Jan 21, 2015 9:32 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

>  I'm not clear if AtomicXXX is intended for safe use via unsafe
> publication; or whether the volatile even aids in that. If not then the
> volatile write in the constructor seems not strictly necessary. Performance
> will depend on whether the JIT can inline and remove the native Unsafe call.
>
> David
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Thursday, 22 January 2015 12:00 PM
> *To:* David Holmes
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics (another
> doc fix request)
>
> Sorry, I missed the "static", for which the happens-before relationship
> clearly exists.  It sounds like we're in agreement that having the
> assignment in the constructor be volatile is not likely to matter in
> practice, but the current implementation is cleaner at some performance
> cost?
>
> Hans
>
> On Wed, Jan 21, 2015 at 5:48 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  Sharing via _static_ final fields isn't racy.
>>
>> David
>>
>> -----Original Message-----
>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>> Boehm
>>  *Sent:* Thursday, 22 January 2015 11:40 AM
>> *To:* David Holmes
>> *Cc:* concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>> (another doc fix request)
>>
>>  What I was suggesting, I think, was to only use unsafe in the
>> constructor (in addition to the places it's currently used).  But I agree
>> with you about cleanliness.
>>
>> I would still be interested in code that shares a final field without an
>> additional happens-before relationship (i.e. in a way that would be racy
>> without the final field rules), and without very clever and subtle
>> reasoning that would have been unnecessary with the addition of a volatile
>> declaration.  I really don't think that's a natural idiom, though it is
>> clearly used for performance on occasion.  And it's sometimes necessary to
>> deal with that case for malware defense.
>>
>> Hans
>>
>> On Wed, Jan 21, 2015 at 5:13 PM, David Holmes <davidcholmes at aapt.net.au>
>> wrote:
>>
>>>  Hans, the assignment in the constructor may not depend on volatile -
>>> but so what? Implementing as an actual volatile field that you can read and
>>> write directly is a lot cleaner and simpler than having to use Unsafe for
>>> basic loads and stores. As Justin said mutating is far more common than
>>> construction.
>>>
>>> As for "more likely" there are two major patterns for sharing objects:
>>> - share a final reference to a "thread-safe" mutable object (like
>>> AtomicInteger)
>>> - share a volatile reference to an immutable object
>>>
>>> Of course you could also share a violatile reference to a thread-safe
>>> mutable object, but that would be the more questionable pattern to me.
>>> Certainly needed some times, but not what I would expect to be the
>>> prevalent case.
>>>
>>> YMMV.
>>>
>>> David
>>>
>>>
>>>
>>> -----Original Message-----
>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>>> Boehm
>>>  *Sent:* Thursday, 22 January 2015 11:02 AM
>>> *To:* David Holmes
>>> *Cc:* concurrency-interest at cs.oswego.edu
>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>> (another doc fix request)
>>>
>>>   If the object containing level is properly published, I think it
>>> still doesn't matter whether the assignment of 17 is treated as volatile or
>>> not.  The assignment of 17 doesn't happen-before the later operations on
>>> the level field, but is treated as such by the final field rules, since we
>>> get to it by a dereference chain from a final field.
>>>
>>> Aside (from the aside from the aside):  I'm surprised by your "more
>>> likely" statement.  It would be interesting to know how common it is to
>>> intentionally pass objects via final fields without additional
>>> happens-before ordering.  My mental mode is that happens in exactly three
>>> cases:
>>>
>>> 1) Clever and very tricky performance optimizations that elide
>>> "volatile" for concurrently accessed fields.
>>>
>>> 2) Bugs.
>>>
>>> 3) (Rarely) Malware trying to exploit races.
>>>
>>> Hans
>>>
>>>
>>> On Wed, Jan 21, 2015 at 4:14 PM, David Holmes <davidcholmes at aapt.net.au>
>>> wrote:
>>>
>>>>  Publication is more likely via a final field:
>>>>
>>>> final static AtomicInteger level = new AtomicInteger(17);
>>>>
>>>> Setting the internal field happens-before setting of level regardless
>>>> of whether internal field is volatile.
>>>>
>>>> level.increment();  // atomic inc doesn't vare if internal field is
>>>> volatile
>>>>
>>>> if (level.get() == x) // volatile read of internal field essential
>>>>
>>>> level.set(0); // reset operation - volatile write essential
>>>>
>>>> David
>>>> -----
>>>>
>>>> -----Original Message-----
>>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
>>>> Boehm
>>>> *Sent:* Thursday, 22 January 2015 9:55 AM
>>>> *To:* David Holmes
>>>>  *Cc:* concurrency-interest at cs.oswego.edu
>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>> (another doc fix request)
>>>>
>>>>   I suspect we're miscommunicating, but I'm not sure.
>>>>
>>>> The typical use case for AtomicInteger presumably looks like:
>>>>
>>>> 1) x.a = new AtomicInteger(17);  // Involves an assignment of 17 to the
>>>> embedded volatile field.
>>>>
>>>> 2) communicate x to other threads by assigning x to a volatile, passing
>>>> it to a new thread, using a concurrent data structure, or some synchronized
>>>> methods.
>>>>
>>>> 3) access x.a in a variety of threads.
>>>>
>>>> Typically (1) happens-before (3) because (2) established the necessary
>>>> synchronizes-with relationships.
>>>>
>>>> (a) If (1) happens-before all instances of (3), there is no way to tell
>>>> whether the assignment of 17 is treated as volatile or not, and it doesn't
>>>> matter.
>>>>
>>>> (b) If (1) does not happen before an instance of (3), then that
>>>> instance of (3) can see the uninitialized 0  value of x.a rather than the
>>>> value 17 or a later one.  I claim it is very unlikely for such code to be
>>>> correct whether or not the assignment of 17 is treated as volatile.
>>>>
>>>> Thus there are somewhere between zero and very few cases in which the
>>>> volatile treatment of the assignment of 17 actually matters.  It doesn't
>>>> matter for any code I would consider to be properly synchronized, since it
>>>> only matters for cases in which construction of AtomicInteger does not
>>>> happen-before a call of one of its methods.  I agree that in the Java model
>>>> it is cleaner to simply treat the assignment as volatile, as is currently
>>>> done.  But it adds overhead that's not well-justified, i.e. one or more
>>>> fences for every AtomicInteger construction.
>>>>
>>>> This of course no longer has anything to do with CAS semantics.
>>>>
>>>> Hans
>>>>
>>>> On Wed, Jan 21, 2015 at 3:37 PM, David Holmes <davidcholmes at aapt.net.au
>>>> > wrote:
>>>>
>>>>>  ??? set/get are direct loads and stores and required to have
>>>>> volatile semantics. An actual read/wrote of a volatile field seems perfect
>>>>> to me.
>>>>>
>>>>> David
>>>>>
>>>>> -----Original Message-----
>>>>> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of
>>>>> *Hans Boehm
>>>>> *Sent:* Thursday, 22 January 2015 9:32 AM
>>>>> *To:* David Holmes
>>>>>  *Cc:* Justin Sampson; concurrency-interest at cs.oswego.edu
>>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>>> (another doc fix request)
>>>>>
>>>>>  That does have the advantage that it's a nice simple model.  It has
>>>>> the disadvantage that you're paying for a property that arguably no
>>>>> reasonable code cares about.  At least I'm having a hard time constructing
>>>>> a useful example where it matters.
>>>>>
>>>>> On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <
>>>>> davidcholmes at aapt.net.au> wrote:
>>>>>
>>>>>>  The AtomicXXX classes are supposed to act like volatile fields with
>>>>>> the addition of the atomic operations. Hence the actual field inside the
>>>>>> AtomicXXX class, is and should be, volatile.
>>>>>>
>>>>>> David
>>>>>>
>>>>>> -----Original Message-----
>>>>>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>>>>>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hans Boehm
>>>>>> *Sent:* Thursday, 22 January 2015 9:05 AM
>>>>>> *To:* Justin Sampson
>>>>>> *Cc:* concurrency-interest at cs.oswego.edu; David Holmes
>>>>>> *Subject:* Re: [concurrency-interest] Varieties of CAS semantics
>>>>>> (another doc fix request)
>>>>>>
>>>>>>
>>>>>>
>>>>>>  On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
>>>>>> jsampson at guidewire.com> wrote:
>>>>>> >
>>>>>> > Hans Boehm wrote:
>>>>>> >
>>>>>> > > You're right that there is currently no sanctioned way to access
>>>>>> > > the same object both as a volatile and non-volatile. But there
>>>>>> > > seem to be good reasons for supporting that. E.g. the pointer/flag
>>>>>> > > assignment in double-checked locking doesn't race and hence could
>>>>>> > > be non-volatile.
>>>>>> >
>>>>>> > Do you mean that the write marked #2 below could be non-volatile,
>>>>>> > while the read marked #1 is still volatile?
>>>>>> >
>>>>>> >   private volatile Something instance;
>>>>>> >
>>>>>> >   public Something getInstance() {
>>>>>> >     if (instance == null) { // #1
>>>>>> >       synchronized (this) {
>>>>>> >         if (instance == null) // #4: added by HB
>>>>>> >           instance = new Something(); // #2
>>>>>> >       } // #3
>>>>>> >     }
>>>>>> >     return instance;
>>>>>> >   }
>>>>>> >
>>>>>> > If so, I don't think that works. Another thread calling getInstance
>>>>>> > could still see the Something not fully initialized, if its line #1
>>>>>> > executes between the writing thread's lines #2 and #3 (exiting the
>>>>>> > monitor).
>>>>>>
>>>>>> Sorry.  You're right; I misstated that.  It's the load at #4 that
>>>>>> doesn't need to be volatile since racing writes are locked out anyway.
>>>>>>
>>>>>> >
>>>>>> > > (Does the constructor for e.g. AtomicInteger perform a volatile
>>>>>> > > assignment? The documentation appears unclear. [...])
>>>>>> >
>>>>>> > This the latest code in CVS:
>>>>>> >
>>>>>> >   private volatile int value;
>>>>>> >
>>>>>> >   /**
>>>>>> >    * Creates a new AtomicInteger with the given initial value.
>>>>>> >    *
>>>>>> >    * @param initialValue the initial value
>>>>>> >    */
>>>>>> >   public AtomicInteger(int initialValue) {
>>>>>> >     value = initialValue;
>>>>>> >   }
>>>>>> >
>>>>>> >
>>>>>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup
>>>>>>
>>>>>> Interesting.  There's clearly not much of a reason for this to be
>>>>>> volatile.  Any code that cared about ordering would also have to be
>>>>>> prepared to see a pre-initialization zero value, which seems extremely
>>>>>> unlikely.  C++11 carefully defines the initialization not to be atomic.
>>>>>>
>>>>>> >
>>>>>> > > But the notion of intentionally mis-specifying an operation to
>>>>>> > > write when it really doesn't just seems wrong to me.
>>>>>> >
>>>>>> > You and I are agreeing on this point. :)
>>>>>> >
>>>>>> > But you also have to be prepared that the implementation _might_
>>>>>> > actually do a write. Don't some CPU architectures implement CAS with
>>>>>> > a store cycle to main memory regardless of success or failure, just
>>>>>> > inserting the current value in the store buffer late in the cycle if
>>>>>> > it didn't match the expected value? Therefore if your code is
>>>>>> > actually sensitive to whether other threads are indeed writing that
>>>>>> > memory location, it's going to be buggy on those architectures.
>>>>>>
>>>>>> Indeed, now that I checked, the Intel x86 documentation seems to say
>>>>>> that in places, though that part of the description looks quite dated to
>>>>>> me.  I have always been told that modern processors acquire the cache line
>>>>>> in exclusive mode and then perform the operation in cache.  And performance
>>>>>> numbers seem to confirm that.
>>>>>>
>>>>>> In either case, this isn't really programmer visible one way or the
>>>>>> other.  On an ll/sc architecture like ARM, it would generally be silly to
>>>>>> do the store unconditionally, and we want to discourage that.  If the
>>>>>> hardware does it under the covers (which I doubt for modern hardware;
>>>>>> gratuitously dirtying the cache line doesn't seem free), so be it.
>>>>>>
>>>>>> I'm just arguing for a description like the current one in the
>>>>>> specific class descriptions (and the CAS description in Wikipedia and most
>>>>>> other places) that doesn't gratuitously look like it imposes additional
>>>>>> requirements, and actually does impose expensive and useless ordering
>>>>>> requirements on some important architectures.
>>>>>>
>>>>>> >
>>>>>> > I'm having trouble imagining what could actually go wrong, though.
>>>>>> > Is it possible that a no-op CAS would appear to undo non-volatile
>>>>>> > changes in another thread? E.g.:
>>>>>> >
>>>>>> > T1: r1 = x; // volatile read, sees 0
>>>>>> > T1: x = 3; // non-volatile write, not flushed yet
>>>>>> > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
>>>>>> > T1: r2 = x; // non-volatile read, could see either 3 or 0?
>>>>>>
>>>>>> I don't think anything would actually go wrong, though we would have
>>>>>> to be careful to specify the write to disallow the 0 write in your
>>>>>> example.  I just dislike specifying CAS in a new and nonstandard way for no
>>>>>> positive benefit.
>>>>>>
>>>>>> Hans
>>>>>>
>>>>>> >
>>>>>> > Cheers,
>>>>>> > Justin
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/d03d578b/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 21 22:06:31 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 22:06:31 -0500
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <129D2A38-A415-4A10-AD3D-6D77FF055651@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7C015.4030804@oracle.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<129D2A38-A415-4A10-AD3D-6D77FF055651@cox.net>
Message-ID: <CAHjP37HkG-gHMtupOjoY+_ZemmBoovm8BgeAzqHRVRzwkP_09Q@mail.gmail.com>

I'm not sure what you're proposing, to be honest.  That the compiler
doesn't do optimizations? It sounds like you had broken code all along with
the loop termination example, and we're simply getting lucky that nothing
broke until the jvm was upgraded.  But how is that java's fault? Calling it
"working code" is a far far stretch.

Also, if you think java does "crazy" optimizations, you must've not looked
at what C and C++ compilers do.

sent from my phone
On Jan 20, 2015 6:07 PM, "Gregg Wonderly" <gergg at cox.net> wrote:

>
> > On Jan 19, 2015, at 8:17 AM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
> >
> > I don't think there can be an agreement on what "simple" means, it may
> be easier to consider the prerequisite knowledge of the target audience.
> >
> > I don't know how many people are really interested in this discussion -
> but the only way to share understanding with the few that are, is broadcast
> on the relevant topic.
> >
> > If I recall right what your pain point is, declaring all instructions in
> all threads to appear in program order to all threads, does not make an
> awful lot of difference in the reasoning methods, and in many cases even no
> difference in the outcomes. Roughly speaking, given a proof of correctness
> of "totally ordered" program, it may be possible to turn the key variables
> used in decision-making points into volatiles, and turn everything else
> into normal variables. The trouble is, most people don't even consider what
> the proof of correctness of a "totally ordered" program would look like. I
> think very quickly you will want to use induction in your proofs, which
> will touch only a few edges in your program - the rest being irrelevant, or
> can be proven from those few. I think it is the use of induction that
> determines what JMM should be; that's the only thing that really reduces
> complexity of proofs.
>
> My issue is primarily about the fact that this reordering changes the
> observable behavior of an application when casually observing it.  Things
> like logging statements that show the current value of a variable used in
> another thread for example.  If I have a remote ?observation? feature, and
> I want to ?see? what is happening in another thread, I have to burden the
> entire system with shenanigans to try and ?see? what is happening.  I have
> to burden the processing in that thread with fences/synchronization galore,
> just so that occasional observation can occur.  Yes, I can also create a
> synchronized observation point where data is passed between threads, but in
> doing that, I have to use ?synchronized? or volatile elsewhere in order for
> there to be a happens before relationship between writes in the observed
> thread and reads in the observer thread.   This is a very common thing for
> my software systems.  Optimizations, such as loop lifting are thus a big
> deal for me, because we have let the hardware issues of ?visibility? exist
> as they do now, and further anchored that into the expected behavior(s) of
> the Java platform.
>
> > For example, your beloved loop example.
> >
> > while(!signal)....
> >
> > Why does the loop even exist? To me, it is evidence that somewhere in
> your proof you will have a statement "if signal==true, then ....<some
> consistency assertion here>" - as the program cannot deal with the
> inconsistent state; it does not progress and doesn't read nor write
> anything that appears in program order after the loop, until "signal==true?.
>
> First, let me explain that I have used Java since the beginning of time.
> I have written huge amounts of Java software in the form of libraries and
> desktop applications and applets.  I have also written java servlets.  I
> created my own messaging/brokering engine in 1997/1998 before JMS existed.
>  There are just countless places where I have had to deal with all aspects
> of the different Java versions. I never deployed anything on 1.2 due to the
> huge number of problems in that release (that release pretty much killed
> the momentum of Java in the market place as a desktop environment, and that
> was a disastrous outcome for Java overall).  I?ve dealt with very careful
> tuning of multi-threaded applications, such as my broker, which had
> hundreds of interacting threads around a handful of data structures.  There
> are places where I was a bit reckless with synchronization, because the
> early JVMs didn?t show signs of caring about that.  This whole loop
> hoisting bit is one of the places where I knew that the values were not
> strictly synchronized.  But, I did not care, because eventually the change
> in the loop control variable was always visible.  I never noted any
> measurable delay in the visibility on the x86 servers.  Hardware specific
> misbehaviors are a problem for me, because as a Java developer, I can?t
> always know every version of ever JVM that might exist.
>
> Since volatile did not work in JDK 1.4 and earlier, loops with shared
> state termination control, had to be written as
>
>         while(true) {
>                 synchronized(this) {
>                         if( done )
>                                 break;
>                 }
>                 ?.
>         }
>
> to be completely synchronized with
>
>         public void stop() {
>                 synchronized(this) { done = true; }
>         }
>
> I didn?t like this verbosity all over the place, and so I instead used
> unsynchronized state as termination control because I didn?t need atomic
> changes of more than a single value across threads in these cases.
>
> I use threads which process external events and another thread will ask
> them to stop that processing.  This typically looks like:
>
> public void run() {
>         while( !done ) {
>                 try {
>                         if( sock == null ) {
>                                 sock = new Socket(?);
>                         }
>                         data = readSomething();
>                         process(data);
>                 } catch( IOException ex ) {
>                         try {
>                                 sock.close();
>                         } catch( Exception exx ) {
>                                 log.fine(exx);
>                         } finally {
>                                 sock = null;
>                         }
>                 } catch( Exception ex ) {
>                         log.severe(ex);
>                 }
>         }
> }
>
> I might then have a stop() method that looks like:
>
> public void stop() {
>         done = true;
>         sock.close();
> }
>
> This method will trivially stop the thread by closing the socket.  This
> worked fine when the loop hoist was not happening. But suddenly it failed
> with the Java update.  The whole application completely stopped working
> because these worker threads would not stop when needed.  They would reopen
> the socket and keep going.  I Java 5 and later, I do now use volatile
> everywhere to keep loop hoisting from occurring.
>
> > If you always rely on "signal == true", then you don't need total
> ordering - only partial ordering joined through signal read and write is
> sufficient; that means signal should be volatile, and the rest doesn't have
> to be (well, subject to restrictions in the rest of the code). If you
> sometimes do not rely on "signal==true", try to convince yourself that
> <some consistency assertion> holds, can you? If you can't convince that the
> consistency assertion holds, how can you show the program is correct?
> Nowhere here I mentioned total or partial ordering of instructions.
>
> The problem is that software which used to work 100% of the time when
> volatile was not available (and it?s lack of use did not keep the software
> from working), suddenly stopped working on a Java update.  To me, that is a
> poor choice for an automatic optimization.  Java never said that the change
> in ?signal? could not be observed, ever.  This specific change, as I?ve
> said before, caused many people that I have interacted with and discussed
> this issue with, to tell me, that they had already decided that program
> correctness was more important than performance to them.  The said that
> they had told their developers that all variables would either be declared
> final or volatile.
>
> We don?t know what other flow control optimizations might just ?happen? in
> the JVM implementation to non-volatile values in the future and make
> software stop working again.  Even now, we still have people carefully
> crafting racy applications to avoid synchronized() blocks!   Making
> everything volatile, keeps that from causing ?alteration? to execution
> paths so that we can guarantee the application stays in working order, no
> matter what version of the JVM it is run on.  People deploying software
> systems don?t always have control over what JVM they are deployed on.  I
> know that Sun and now Oracle seem to have/had no idea how to deploy desktop
> applications in Java, let alone applets where the whole world of versions
> exist.  But I have many such applications that I?d really prefer people to
> have pleasant experiences with, and not have to spend hours on forums or
> mailing lists or telephone calls to support trying to figure out exactly
> which version of something that they have no idea that they need, has a
> problem and ?that thing? needs to be replaced/downgraded/upgraded to make
> the ?application? work.
>
> I truly believe that volatile should be the default declaration in the
> JMM.  Performance tuning is what we do after we prove a program correct.
> No matter how fast it goes, if it?s not computing correct results, then
> what good has been accomplished?  We should have to add an annotation or
> keyword to specify that we are willing to take the visibility risk and
> management into our own hands.
>
> I do appreciate your attention to optimization details and trying to
> understand how developers might be better served by the Java platform.   It
> is really bothersome and quite disturbing to users of the platform when any
> optimization suddenly breaks working software systems.  Java desktop
> applications or applets, delivered to the users as just JAR files that stop
> working like this are very frustrating for developers to debug.  Especially
> when the developers have no way of knowing who?s having what problems.
> That?s just not good system design, at all.  Yes, developers can test, test
> and retest and evaluate all kinds of details.  But in the end, why should
> we decide that more time should be spent to get software to users when
> there is no direct advantage to users in doing that?   In this particular
> case, I don?t know how there is any appreciable gain in performance from
> this optimization for x86 in particular, so why is it so dear and important
> to have such things going on?
>
> I have a few applications which I wrote on JDK 1.1.1, and they worked just
> fine on all versions of Java up to Java 5.  It?s that kind of breakage that
> just doesn?t make sense overall.
>
> How many conversations have been had with developers to ask them why they
> don?t create Java desktop applications?  What makes that such an unpleasant
> experience for them or their users?  Really, why isn?t Java a predominant
> part of the desktop landscape?  Is it because of performance, or something
> different?  I?m going to guess that performance isn?t the big issue keeping
> developers from using Java for their applications.
>
> Gregg Wonderly
>
> > Alex
> >
> > On 17/01/2015 05:27, Gregg Wonderly wrote:
> >>> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
> >>>
> >>> I would not bother specifying exactly what makes sense; only what's
> allowed.
> >>>
> >>> For example, what would you say if some hardware offered atomicity of
> update of just the permit, and offer no barrier semantics for the rest of
> the accesses around it?
> >>>
> >> Why would you want to specify in a language, variations of behavior
> that complicate software design, and cause software systems developers to
> fail because they can?t understand what actually will happen (as opposed to
> what can happen) or correctly implement software because they are having to
> interact, literally with random hardware behavior due to the language not
> adequately depicting tangible software behaviors.
> >>
> >> I know it?s really fun to play with this stuff and try and wrestle the
> most out of the hardware with the least possible constraints.  But really,
> how in the world can we have 100% reliable software systems when we provide
> 50 ways to be unsuccessful for every 1 way that is the correct way to
> structure the code?  Why is all of that variation event attractive to you
> Alex?  Do you really want developer to have to wrestle with software
> design?  Do you find it fun to manipulate people with such complexity in
> the system and some how prove that your understanding is more than theirs?
> Does this give you some kind of satisfaction or what?
> >>
> >> In the end, this is a huge barrier to being successful with Java.  It
> provides not obvious benefit to the developers from a long term goal of
> building and maintaining viable software systems.  Instead it creates
> problem after problem.  Software system become unpredictable with seemingly
> random behaviors. I just fail to see how this can feel like good software
> language design.  Maybe you can send me a private response if you don?t
> care to discuss this stuff publicly.  Practically I find this to be just
> insane.  All of this discussion in this thread just points out how 100s of
> hours of peoples time can be wasted on this issue all for nothing but
> surprise.  Developers with great knowledge of the JMM still have a hard
> time proving to themselves what the outcome of their code will be.
> >>
> >> Gregg Wonderly
> >>
> >>> Alex
> >>>
> >>> On 16/01/2015 20:58, Justin Sampson wrote:
> >>>> Oleksandr Otenko wrote:
> >>>>
> >>>>> You can't claim that it's not possible to implement reschedule in
> >>>>> such a way that yield cannot tell whether reschedule was called
> >>>>> since yield was called last time.
> >>>> Ah, I see what you're getting at now, and I understand what you
> >>>> meant by talking about a "fast path" earlier.
> >>>>
> >>>> In your example, the yield/reschedule operations are _themselves_ a
> >>>> valid implementation of park/unpark, complete with atomic handling
> >>>> of internal state (the queue) that ensures a reschedule isn't lost.
> >>>> That wasn't what I was imagining in the example of mine that you
> >>>> were replying to.
> >>>>
> >>>> So I think we're actually agreeing that the "permit" semantics do
> >>>> have to be integrated into atomic operations at some level of
> >>>> implementation. And I also agree that it's possible to define a
> >>>> fast-path implementation on top of such semantics that still remains
> >>>> valid but with less atomicity.
> >>>>
> >>>> In fact, if I may generalize into a theorem, I believe we can say
> >>>> that given any valid implementation of park/unpark, we can construct
> >>>> another valid implementation, call it park'/unpark', like so:
> >>>>
> >>>> Thread {
> >>>>   volatile boolean permit';
> >>>> }
> >>>>
> >>>> park'() {
> >>>>   Thread t = Thread.currentThread();
> >>>>   if (!t.permit') {
> >>>>     park();
> >>>>   }
> >>>>   t.permit' = false;
> >>>> }
> >>>>
> >>>> unpark'(Thread t) {
> >>>>   if (!t.permit') {
> >>>>     t.permit' = true;
> >>>>     unpark(t);
> >>>>   }
> >>>> }
> >>>>
> >>>> I'm writing up another attempt at formal semantics that allows us to
> >>>> prove this theorem, which I'll send out later today. :)
> >>>>
> >>>> Cheers,
> >>>> Justin
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/321569f2/attachment-0001.html>

From jsampson at guidewire.com  Wed Jan 21 22:13:19 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 22 Jan 2015 03:13:19 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Don't think that's true; the write of the reference can be
> reordered with the volatile write, leading to 0. Volatile store
> prevents prior writes from moving past it, not subsequent ones.

Oops! Okay, that makes sense. I'm glad I wrote it out so that you
could catch my error. So Hans is correct that the uninitialized 0
value can be observed as long as the reference to the AtomicInteger
itself is shared racily.

To make another attempt: I believe it is still true that if T2 does
see T1's write of 5, then there must be a happens-before edge from
that write to that read, such that anything preceding the
constructor in T1 happens-before anything following the get() in T2.

That's especially important for AtomicReference, because it means
that if a reference is passed into the constructor then the object
it refers to is safely published to any thread that sees it returned
from get(). If the write in the constructor were not volatile, then
the referenced object could be seen partially initialized.

Is that correct?

Cheers,
Justin


From vitalyd at gmail.com  Wed Jan 21 22:23:23 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 21 Jan 2015 22:23:23 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>

That sounds right to me (I'm assuming you're particularly concerned about
unsafe publication of AtomicReference since if that reference is published
safely then that publication creates the hb edge and the rest here is
unimportant).

sent from my phone
On Jan 21, 2015 10:13 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> Vitaly Davidovich wrote:
>
> > Don't think that's true; the write of the reference can be
> > reordered with the volatile write, leading to 0. Volatile store
> > prevents prior writes from moving past it, not subsequent ones.
>
> Oops! Okay, that makes sense. I'm glad I wrote it out so that you
> could catch my error. So Hans is correct that the uninitialized 0
> value can be observed as long as the reference to the AtomicInteger
> itself is shared racily.
>
> To make another attempt: I believe it is still true that if T2 does
> see T1's write of 5, then there must be a happens-before edge from
> that write to that read, such that anything preceding the
> constructor in T1 happens-before anything following the get() in T2.
>
> That's especially important for AtomicReference, because it means
> that if a reference is passed into the constructor then the object
> it refers to is safely published to any thread that sees it returned
> from get(). If the write in the constructor were not volatile, then
> the referenced object could be seen partially initialized.
>
> Is that correct?
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150121/0487b746/attachment.html>

From thurston at nomagicsoftware.com  Thu Jan 22 00:14:42 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 21 Jan 2015 22:14:42 -0700 (MST)
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <54C0484D.4030700@cs.oswego.edu>
References: <54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>
	<54C0484D.4030700@cs.oswego.edu>
Message-ID: <1421903682182-12111.post@n7.nabble.com>

Doug Lea wrote
> Back to...
> 
> On 01/19/2015 08:24 PM, Justin Sampson wrote:
>> Okay, Doug -- here's one last proposal. I think the wording that
>> you've added about reading/writing of a volatile by park/unpark is
>> both (a) too strong and (b) misleading. :)
> 
> OK. Which brings us back to providing usage guidance rather
> than a (semi) formal spec. About which most people probably agree.
> 
>>
>> So here's my final(?) proposed wording, to replace the sentence
>> you've added:
> 
>> "The caller of park and unpark can rely on those calls not being
>> reordered with any volatile reads or writes in the calling thread.
>> However, the same promise cannot be made regarding non-volatile
>> reads or writes. Therefore, the caller should ensure that any state
>> variables being used in conjunction with park and unpark are
>> themselves volatile."
> 
> Adapted as:
> 
>   * Reliable usage requires the use of volatile (or atomic) variables
>   * to control when to park or unpark.  Orderings of calls to these
>   * methods are maintained with respect to volatile variable accesses,
>   * but not necessarily non-volatile variable accesses.
> 
> (Which is intentionally imprecise about what form of "ordering"
> we mean here.)

I'm not sure how to interpret "Orderings of calls to these
  * methods are maintained with respect to volatile variable accesses" and
certainly engineers who haven't followed this thread (that would be all of
them), wouldn't know what to make of that.

I think the best documentation is to reference the 4-line code sample that I
proposed earlier and served as the foundation of this discussion, which I
believe there is unanimity that all implementations must provide.

The point is not just about proper usage, but about proper *implementation*
(of park/unpark) as well.  If implementations don't provide the proper
guarantee (and I provided a hypothetical implementation which wouldn't),
then what good is proper usage?




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p12111.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From peter.levart at gmail.com  Thu Jan 22 03:05:04 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 22 Jan 2015 09:05:04 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>
	<CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>
Message-ID: <54C0AF30.1030501@gmail.com>

For AtomicReference it would be expected that the following:

reference = new AtomicReference(referent); // relaxed assignement to 
reference

...is at least as safe as:

reference = new AtomicReference(); // relaxed assignement to reference
reference.set(referent);

So that if 'referent' pointer is observed in another thread as non-null, 
the underlying object is fully initialized.

Therefore for AtomicReference the constructor should use a volatile 
write, but for other AtomicXXX classes it needs not.

Peter

On 01/22/2015 04:23 AM, Vitaly Davidovich wrote:
>
> That sounds right to me (I'm assuming you're particularly concerned 
> about unsafe publication of AtomicReference since if that reference is 
> published safely then that publication creates the hb edge and the 
> rest here is unimportant).
>
> sent from my phone
>
> On Jan 21, 2015 10:13 PM, "Justin Sampson" <jsampson at guidewire.com 
> <mailto:jsampson at guidewire.com>> wrote:
>
>     Vitaly Davidovich wrote:
>
>     > Don't think that's true; the write of the reference can be
>     > reordered with the volatile write, leading to 0. Volatile store
>     > prevents prior writes from moving past it, not subsequent ones.
>
>     Oops! Okay, that makes sense. I'm glad I wrote it out so that you
>     could catch my error. So Hans is correct that the uninitialized 0
>     value can be observed as long as the reference to the AtomicInteger
>     itself is shared racily.
>
>     To make another attempt: I believe it is still true that if T2 does
>     see T1's write of 5, then there must be a happens-before edge from
>     that write to that read, such that anything preceding the
>     constructor in T1 happens-before anything following the get() in T2.
>
>     That's especially important for AtomicReference, because it means
>     that if a reference is passed into the constructor then the object
>     it refers to is safely published to any thread that sees it returned
>     from get(). If the write in the constructor were not volatile, then
>     the referenced object could be seen partially initialized.
>
>     Is that correct?
>
>     Cheers,
>     Justin
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/aace909d/attachment.html>

From Sebastian.Millies at softwareag.com  Thu Jan 22 03:36:02 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Thu, 22 Jan 2015 08:36:02 +0000
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8CEC5@sm-ex-01-vm.guidewire.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
	<CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CEC5@sm-ex-01-vm.guidewire.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89BC87@HQMBX5.eur.ad.sag>

thanks, I have looked at it.

In my case, the number of permits would have to be the number of available memory in bytes. Each thread would acquire multiple permits at once, which should be OK with a fair Semaphore.

However, a Semaphore can only manage an INTEGER number of permits, which would amount to only ca. 2 Gigabytes. Much too low. How could I manage a LONG number of permits?

Sebastian

-----Original Message-----
From: Justin Sampson [mailto:jsampson at guidewire.com]
Sent: Thursday, January 22, 2015 3:35 AM
To: Vitaly Davidovich; Millies, Sebastian
Cc: concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious LockSupport.park() return?)

Vitaly Davidovich wrote:

> Have you looked into seeing if Semaphore (with fairness) would help?
> Your use case sounds like the classical resource usage throttle, with
> heap being the resource.

Ah, of course! A fair Semaphore would maintain the ordering because there's no distinction between lock and wait queue.

Indeed, to take a quick flashback, another recent discussion we had about Semaphore semantics established the advice that if various threads are passing different values to acquire(N), as they would be in this case, it ONLY makes sense if the Semaphore is fair. The behavior is potentially surprising otherwise.

Cheers,
Justin

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From TEREKHOV at de.ibm.com  Thu Jan 22 03:40:02 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Thu, 22 Jan 2015 09:40:02 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <1421903682182-12111.post@n7.nabble.com>
Message-ID: <OFAAAC8B8C.523BF2C0-ONC1257DD5.002EC356-C1257DD5.002FA48F@de.ibm.com>

How about

This class associates, with each thread that uses it, a permit (in
the sense of the {@link java.util.concurrent.atomic.AtomicInteger}
class). A call to {@code park} will return immediately if the permit
is available, consuming it in the process; otherwise it <em>may</em>
block.  A call to {@code unpark} makes the permit available, if it
was not already available. The memory ordering semantics of {@code
park} are the same as those of a getAndSet of a permit variable,
except that it's only acquire.  The memory ordering semantics of
{@code upark} are the same as those of a lazySet of a permit variable.

park:
  do {
    permitted = permit.getAndSet(0); // consume permit
  } while (!permitted && !WatsonSaysReturnSpuriously());

unpark:
  permit.lazySet(1);

?

regards,
alexander.

thurstonn <thurston at nomagicsoftware.com>@cs.oswego.edu on 22.01.2015
06:14:42

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	concurrency-interest at cs.oswego.edu
cc:
Subject:	Re: [concurrency-interest] unpark/park memory visibility


Doug Lea wrote
> Back to...
>
> On 01/19/2015 08:24 PM, Justin Sampson wrote:
>> Okay, Doug -- here's one last proposal. I think the wording that
>> you've added about reading/writing of a volatile by park/unpark is
>> both (a) too strong and (b) misleading. :)
>
> OK. Which brings us back to providing usage guidance rather
> than a (semi) formal spec. About which most people probably agree.
>
>>
>> So here's my final(?) proposed wording, to replace the sentence
>> you've added:
>
>> "The caller of park and unpark can rely on those calls not being
>> reordered with any volatile reads or writes in the calling thread.
>> However, the same promise cannot be made regarding non-volatile
>> reads or writes. Therefore, the caller should ensure that any state
>> variables being used in conjunction with park and unpark are
>> themselves volatile."
>
> Adapted as:
>
>   * Reliable usage requires the use of volatile (or atomic) variables
>   * to control when to park or unpark.  Orderings of calls to these
>   * methods are maintained with respect to volatile variable accesses,
>   * but not necessarily non-volatile variable accesses.
>
> (Which is intentionally imprecise about what form of "ordering"
> we mean here.)

I'm not sure how to interpret "Orderings of calls to these
  * methods are maintained with respect to volatile variable accesses" and
certainly engineers who haven't followed this thread (that would be all of
them), wouldn't know what to make of that.

I think the best documentation is to reference the 4-line code sample that
I
proposed earlier and served as the foundation of this discussion, which I
believe there is unanimity that all implementations must provide.

The point is not just about proper usage, but about proper *implementation*
(of park/unpark) as well.  If implementations don't provide the proper
guarantee (and I provided a hypothetical implementation which wouldn't),
then what good is proper usage?




--
View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p12111.html

Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From Sebastian.Millies at softwareag.com  Thu Jan 22 04:11:51 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Thu, 22 Jan 2015 09:11:51 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <OFAAAC8B8C.523BF2C0-ONC1257DD5.002EC356-C1257DD5.002FA48F@de.ibm.com>
References: <1421903682182-12111.post@n7.nabble.com>
	<OFAAAC8B8C.523BF2C0-ONC1257DD5.002EC356-C1257DD5.002FA48F@de.ibm.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89BCDC@HQMBX5.eur.ad.sag>

This would not be helpful to me. What is a "permit variable"? There is no public class Permit in the JDK doc, and the referenced AtomicInteger's getAndSet(int) method returns int, not boolean. This explanation totally confuses me.

-- Sebastian

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Alexander Terekhov
Sent: Thursday, January 22, 2015 9:40 AM
To: thurstonn
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] unpark/park memory visibility

How about

This class associates, with each thread that uses it, a permit (in the sense of the {@link java.util.concurrent.atomic.AtomicInteger}
class). A call to {@code park} will return immediately if the permit is available, consuming it in the process; otherwise it <em>may</em> block.  A call to {@code unpark} makes the permit available, if it was not already available. The memory ordering semantics of {@code park} are the same as those of a getAndSet of a permit variable, except that it's only acquire.  The memory ordering semantics of {@code upark} are the same as those of a lazySet of a permit variable.

park:
  do {
    permitted = permit.getAndSet(0); // consume permit
  } while (!permitted && !WatsonSaysReturnSpuriously());

unpark:
  permit.lazySet(1);

?

regards,
alexander.

thurstonn <thurston at nomagicsoftware.com>@cs.oswego.edu on 22.01.2015
06:14:42

Sent by:        concurrency-interest-bounces at cs.oswego.edu


To:     concurrency-interest at cs.oswego.edu
cc:
Subject:        Re: [concurrency-interest] unpark/park memory visibility


Doug Lea wrote
> Back to...
>
> On 01/19/2015 08:24 PM, Justin Sampson wrote:
>> Okay, Doug -- here's one last proposal. I think the wording that
>> you've added about reading/writing of a volatile by park/unpark is
>> both (a) too strong and (b) misleading. :)
>
> OK. Which brings us back to providing usage guidance rather than a
> (semi) formal spec. About which most people probably agree.
>
>>
>> So here's my final(?) proposed wording, to replace the sentence
>> you've added:
>
>> "The caller of park and unpark can rely on those calls not being
>> reordered with any volatile reads or writes in the calling thread.
>> However, the same promise cannot be made regarding non-volatile reads
>> or writes. Therefore, the caller should ensure that any state
>> variables being used in conjunction with park and unpark are
>> themselves volatile."
>
> Adapted as:
>
>   * Reliable usage requires the use of volatile (or atomic) variables
>   * to control when to park or unpark.  Orderings of calls to these
>   * methods are maintained with respect to volatile variable accesses,
>   * but not necessarily non-volatile variable accesses.
>
> (Which is intentionally imprecise about what form of "ordering"
> we mean here.)

I'm not sure how to interpret "Orderings of calls to these
  * methods are maintained with respect to volatile variable accesses" and certainly engineers who haven't followed this thread (that would be all of them), wouldn't know what to make of that.

I think the best documentation is to reference the 4-line code sample that I proposed earlier and served as the foundation of this discussion, which I believe there is unanimity that all implementations must provide.

The point is not just about proper usage, but about proper *implementation* (of park/unpark) as well.  If implementations don't provide the proper guarantee (and I provided a hypothetical implementation which wouldn't), then what good is proper usage?




--
View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p12111.html

Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From Sebastian.Millies at softwareag.com  Thu Jan 22 05:25:38 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Thu, 22 Jan 2015 10:25:38 +0000
Subject: [concurrency-interest] FW: Resource-based waiting (was: Spurious
 LockSupport.park() return?)
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
	<CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CEC5@sm-ex-01-vm.guidewire.com> 
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89BDFE@HQMBX5.eur.ad.sag>

this 1999 implementation of Semaphore still had a LONG number of permits, which is what I need.
http://pag-www.gtisc.gatech.edu/psa/hedc/EDU/oswego/cs/dl/util/concurrent/Semaphore.java.html

Why did that not make it into the JDK?

-- Sebastian

> -----Original Message-----
> From: Sebastian.Millies at softwareag.com
> Sent: Thursday, January 22, 2015 9:36 AM
> To: concurrency-interest at cs.oswego.edu
> Cc: 'Justin Sampson'; Vitaly Davidovich
> Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious
> LockSupport.park() return?)
>
> thanks, I have looked at it.
>
> In my case, the number of permits would have to be the number of available memory
> in bytes. Each thread would acquire multiple permits at once, which should be OK
> with a fair Semaphore.
>
> However, a Semaphore can only manage an INTEGER number of permits, which
> would amount to only ca. 2 Gigabytes. Much too low. How could I manage a LONG
> number of permits?
>
> Sebastian
>
> -----Original Message-----
> From: Justin Sampson [mailto:jsampson at guidewire.com]
> Sent: Thursday, January 22, 2015 3:35 AM
> To: Vitaly Davidovich; Millies, Sebastian
> Cc: concurrency-interest at cs.oswego.edu
> Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious
> LockSupport.park() return?)
>
> Vitaly Davidovich wrote:
>
> > Have you looked into seeing if Semaphore (with fairness) would help?
> > Your use case sounds like the classical resource usage throttle, with
> > heap being the resource.
>
> Ah, of course! A fair Semaphore would maintain the ordering because there's no
> distinction between lock and wait queue.
>
> Indeed, to take a quick flashback, another recent discussion we had about Semaphore
> semantics established the advice that if various threads are passing different values to
> acquire(N), as they would be in this case, it ONLY makes sense if the Semaphore is
> fair. The behavior is potentially surprising otherwise.
>
> Cheers,
> Justin

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From TEREKHOV at de.ibm.com  Thu Jan 22 05:26:20 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Thu, 22 Jan 2015 11:26:20 +0100
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BCDC@HQMBX5.eur.ad.sag>
References: <1421903682182-12111.post@n7.nabble.com>	<OFAAAC8B8C.523BF2C0-ONC1257DD5.002EC356-C1257DD5.002FA48F@de.ibm.com>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89BCDC@HQMBX5.eur.ad.sag>
Message-ID: <OF219900EF.CAFDDD38-ONC1257DD5.0036021E-C1257DD5.003957DA@de.ibm.com>

ok, how about

This class associates, with each thread that uses it, a permit, in the
sense of an instance of {@link java.util.concurrent.atomic.AtomicBoolean}
class. A call to {@code park} will return immediately if the permit is
available and consume it in the process (getAndSet(false) == true);
otherwise it <em>may</em> block.  A call to {@code unpark} makes the permit
available (lazySet(true)). The memory ordering semantics of {@code park}
are the same as those of a getAndSet of a permit variable, except that it's
only acquire.  The memory ordering semantics of {@code upark} are the same
as those of a lazySet of a permit variable.

park:
  do {
    permitted = permit.getAndSet(false); // consume permit
  } while (!permitted && !WatsonSaysReturnSpuriously());

unpark:
  permit.lazySet(true);

regards,
alexander.


"Millies, Sebastian" <Sebastian.Millies at softwareag.com>@cs.oswego.edu on
22.01.2015 10:11:51

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
cc:	thurstonn <thurston at nomagicsoftware.com>, Alexander
       Terekhov/Germany/IBM at IBMDE
Subject:	Re: [concurrency-interest] unpark/park memory visibility


This would not be helpful to me. What is a "permit variable"? There is no
public class Permit in the JDK doc, and the referenced AtomicInteger's
getAndSet(int) method returns int, not boolean. This explanation totally
confuses me.

-- Sebastian

-----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu [
mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Alexander
Terekhov
Sent: Thursday, January 22, 2015 9:40 AM
To: thurstonn
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] unpark/park memory visibility

How about

This class associates, with each thread that uses it, a permit (in the
sense of the {@link java.util.concurrent.atomic.AtomicInteger}
class). A call to {@code park} will return immediately if the permit is
available, consuming it in the process; otherwise it <em>may</em> block.  A
call to {@code unpark} makes the permit available, if it was not already
available. The memory ordering semantics of {@code park} are the same as
those of a getAndSet of a permit variable, except that it's only acquire.
The memory ordering semantics of {@code upark} are the same as those of a
lazySet of a permit variable.

park:
  do {
    permitted = permit.getAndSet(0); // consume permit
  } while (!permitted && !WatsonSaysReturnSpuriously());

unpark:
  permit.lazySet(1);

?

regards,
alexander.

thurstonn <thurston at nomagicsoftware.com>@cs.oswego.edu on 22.01.2015
06:14:42

Sent by:        concurrency-interest-bounces at cs.oswego.edu


To:     concurrency-interest at cs.oswego.edu
cc:
Subject:        Re: [concurrency-interest] unpark/park memory visibility


Doug Lea wrote
> Back to...
>
> On 01/19/2015 08:24 PM, Justin Sampson wrote:
>> Okay, Doug -- here's one last proposal. I think the wording that
>> you've added about reading/writing of a volatile by park/unpark is
>> both (a) too strong and (b) misleading. :)
>
> OK. Which brings us back to providing usage guidance rather than a
> (semi) formal spec. About which most people probably agree.
>
>>
>> So here's my final(?) proposed wording, to replace the sentence
>> you've added:
>
>> "The caller of park and unpark can rely on those calls not being
>> reordered with any volatile reads or writes in the calling thread.
>> However, the same promise cannot be made regarding non-volatile reads
>> or writes. Therefore, the caller should ensure that any state
>> variables being used in conjunction with park and unpark are
>> themselves volatile."
>
> Adapted as:
>
>   * Reliable usage requires the use of volatile (or atomic) variables
>   * to control when to park or unpark.  Orderings of calls to these
>   * methods are maintained with respect to volatile variable accesses,
>   * but not necessarily non-volatile variable accesses.
>
> (Which is intentionally imprecise about what form of "ordering"
> we mean here.)

I'm not sure how to interpret "Orderings of calls to these
  * methods are maintained with respect to volatile variable accesses" and
certainly engineers who haven't followed this thread (that would be all of
them), wouldn't know what to make of that.

I think the best documentation is to reference the 4-line code sample that
I proposed earlier and served as the foundation of this discussion, which I
believe there is unanimity that all implementations must provide.

The point is not just about proper usage, but about proper *implementation*
(of park/unpark) as well.  If implementations don't provide the proper
guarantee (and I provided a hypothetical implementation which wouldn't),
then what good is proper usage?




--
View this message in context:
http://jsr166-concurrency.10961.n7.nabble.com/unpark-park-memory-visibility-tp11812p12111.html


Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
Bereczky - http://www.softwareag.com


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Thu Jan 22 05:50:32 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 22 Jan 2015 20:50:32 +1000
Subject: [concurrency-interest] FW: Resource-based waiting (was:
	Spurious LockSupport.park() return?)
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BDFE@HQMBX5.eur.ad.sag>
Message-ID: <NFBBKALFDCPFIDBNKAPCIENAKMAA.davidcholmes@aapt.net.au>

Millies writes:
> 
> this 1999 implementation of Semaphore still had a LONG number of 
> permits, which is what I need.
> http://pag-www.gtisc.gatech.edu/psa/hedc/EDU/oswego/cs/dl/util/con
> current/Semaphore.java.html
> 
> Why did that not make it into the JDK?

Because there were some platforms (eg PPC) where atomic long updates require locking as there is no native support for 64-bit atomic updates. This would have caused Semaphore to be terribly slow on such platforms. So the API was changed to use int (this was 2003).

David
 
> -- Sebastian
> 
> > -----Original Message-----
> > From: Sebastian.Millies at softwareag.com
> > Sent: Thursday, January 22, 2015 9:36 AM
> > To: concurrency-interest at cs.oswego.edu
> > Cc: 'Justin Sampson'; Vitaly Davidovich
> > Subject: RE: [concurrency-interest] Resource-based waiting 
> (was: Spurious
> > LockSupport.park() return?)
> >
> > thanks, I have looked at it.
> >
> > In my case, the number of permits would have to be the number 
> of available memory
> > in bytes. Each thread would acquire multiple permits at once, 
> which should be OK
> > with a fair Semaphore.
> >
> > However, a Semaphore can only manage an INTEGER number of permits, which
> > would amount to only ca. 2 Gigabytes. Much too low. How could I 
> manage a LONG
> > number of permits?
> >
> > Sebastian
> >
> > -----Original Message-----
> > From: Justin Sampson [mailto:jsampson at guidewire.com]
> > Sent: Thursday, January 22, 2015 3:35 AM
> > To: Vitaly Davidovich; Millies, Sebastian
> > Cc: concurrency-interest at cs.oswego.edu
> > Subject: RE: [concurrency-interest] Resource-based waiting 
> (was: Spurious
> > LockSupport.park() return?)
> >
> > Vitaly Davidovich wrote:
> >
> > > Have you looked into seeing if Semaphore (with fairness) would help?
> > > Your use case sounds like the classical resource usage throttle, with
> > > heap being the resource.
> >
> > Ah, of course! A fair Semaphore would maintain the ordering 
> because there's no
> > distinction between lock and wait queue.
> >
> > Indeed, to take a quick flashback, another recent discussion we 
> had about Semaphore
> > semantics established the advice that if various threads are 
> passing different values to
> > acquire(N), as they would be in this case, it ONLY makes sense 
> if the Semaphore is
> > fair. The behavior is potentially surprising otherwise.
> >
> > Cheers,
> > Justin
> 
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 
> Darmstadt, Germany ? Registergericht/Commercial register: 
> Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz 
> Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram 
> Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the 
> Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From sub at laerad.com  Thu Jan 22 06:05:08 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Thu, 22 Jan 2015 11:05:08 +0000
Subject: [concurrency-interest] FW: Resource-based waiting (was:
 Spurious LockSupport.park() return?)
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BDFE@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
	<CAHjP37Ez=LL-Xb6-zxLg4oC1g_PhYesdMbD64CHBgM9ezCnBuQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CEC5@sm-ex-01-vm.guidewire.com>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89BDFE@HQMBX5.eur.ad.sag>
Message-ID: <CACr06N04Q59bFR_HwNRFh1FigVXqFg7S25qD7AuYN2-YestXAw@mail.gmail.com>

It seems like a simple change would be to allocate to some byte boundary,
so multiples of 32 bytes would permit the Semaphore to manage 64Gb.

If you want a drop-in replacement to your park() and unpark() calls that
work as you intend, though, Cassandra has a simple coordination primitive
called a WaitQueue, which essentially provides single use wrappers around
park/unpark without spurious wakeups. Somewhat like
AbstractQueuedSynchronizer, but without the imposition that you build a
synchronization primitive yourself with it.

You can find a version here
<https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/concurrent/WaitQueue.java>
and a new variant in my private repository here
<https://github.com/belliottsmith/bes-utils/blob/master/src/bes/concurrent/WaitQueue.java>
.

Alternatively you could build your own with AbstractQueuedLongSynchronizer,
by pretty much cutting/pasting from Semaphore, but extending AQLS instead
of AQS.


On 22 January 2015 at 10:25, Millies, Sebastian <
Sebastian.Millies at softwareag.com> wrote:

> this 1999 implementation of Semaphore still had a LONG number of permits,
> which is what I need.
>
> http://pag-www.gtisc.gatech.edu/psa/hedc/EDU/oswego/cs/dl/util/concurrent/Semaphore.java.html
>
> Why did that not make it into the JDK?
>
> -- Sebastian
>
> > -----Original Message-----
> > From: Sebastian.Millies at softwareag.com
> > Sent: Thursday, January 22, 2015 9:36 AM
> > To: concurrency-interest at cs.oswego.edu
> > Cc: 'Justin Sampson'; Vitaly Davidovich
> > Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious
> > LockSupport.park() return?)
> >
> > thanks, I have looked at it.
> >
> > In my case, the number of permits would have to be the number of
> available memory
> > in bytes. Each thread would acquire multiple permits at once, which
> should be OK
> > with a fair Semaphore.
> >
> > However, a Semaphore can only manage an INTEGER number of permits, which
> > would amount to only ca. 2 Gigabytes. Much too low. How could I manage a
> LONG
> > number of permits?
> >
> > Sebastian
> >
> > -----Original Message-----
> > From: Justin Sampson [mailto:jsampson at guidewire.com]
> > Sent: Thursday, January 22, 2015 3:35 AM
> > To: Vitaly Davidovich; Millies, Sebastian
> > Cc: concurrency-interest at cs.oswego.edu
> > Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious
> > LockSupport.park() return?)
> >
> > Vitaly Davidovich wrote:
> >
> > > Have you looked into seeing if Semaphore (with fairness) would help?
> > > Your use case sounds like the classical resource usage throttle, with
> > > heap being the resource.
> >
> > Ah, of course! A fair Semaphore would maintain the ordering because
> there's no
> > distinction between lock and wait queue.
> >
> > Indeed, to take a quick flashback, another recent discussion we had
> about Semaphore
> > semantics established the advice that if various threads are passing
> different values to
> > acquire(N), as they would be in this case, it ONLY makes sense if the
> Semaphore is
> > fair. The behavior is potentially surprising otherwise.
> >
> > Cheers,
> > Justin
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt,
> Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 -
> Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman),
> Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - http://www.softwareag.com
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/6c20e29b/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 22 09:37:23 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Jan 2015 09:37:23 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54C0AF30.1030501@gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>
	<CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>
	<54C0AF30.1030501@gmail.com>
Message-ID: <CAHjP37FF9R2FDjjN0=DHvxRGYb-Utxa_agLFQVB+j_E+dT9JQA@mail.gmail.com>

Just curious - why would this be "expected" given that this only matters if
published unsafely? Is there something about these classes that implies
they're ok to publish unsafely?

sent from my phone
On Jan 22, 2015 3:05 AM, "Peter Levart" <peter.levart at gmail.com> wrote:

>  For AtomicReference it would be expected that the following:
>
> reference = new AtomicReference(referent); // relaxed assignement to
> reference
>
> ...is at least as safe as:
>
> reference = new AtomicReference(); // relaxed assignement to reference
> reference.set(referent);
>
> So that if 'referent' pointer is observed in another thread as non-null,
> the underlying object is fully initialized.
>
> Therefore for AtomicReference the constructor should use a volatile write,
> but for other AtomicXXX classes it needs not.
>
> Peter
>
> On 01/22/2015 04:23 AM, Vitaly Davidovich wrote:
>
> That sounds right to me (I'm assuming you're particularly concerned about
> unsafe publication of AtomicReference since if that reference is published
> safely then that publication creates the hb edge and the rest here is
> unimportant).
>
> sent from my phone
> On Jan 21, 2015 10:13 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:
>
>> Vitaly Davidovich wrote:
>>
>> > Don't think that's true; the write of the reference can be
>> > reordered with the volatile write, leading to 0. Volatile store
>> > prevents prior writes from moving past it, not subsequent ones.
>>
>> Oops! Okay, that makes sense. I'm glad I wrote it out so that you
>> could catch my error. So Hans is correct that the uninitialized 0
>> value can be observed as long as the reference to the AtomicInteger
>> itself is shared racily.
>>
>> To make another attempt: I believe it is still true that if T2 does
>> see T1's write of 5, then there must be a happens-before edge from
>> that write to that read, such that anything preceding the
>> constructor in T1 happens-before anything following the get() in T2.
>>
>> That's especially important for AtomicReference, because it means
>> that if a reference is passed into the constructor then the object
>> it refers to is safely published to any thread that sees it returned
>> from get(). If the write in the constructor were not volatile, then
>> the referenced object could be seen partially initialized.
>>
>> Is that correct?
>>
>> Cheers,
>> Justin
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/674404bd/attachment.html>

From peter.levart at gmail.com  Thu Jan 22 12:10:09 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 22 Jan 2015 18:10:09 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37FF9R2FDjjN0=DHvxRGYb-Utxa_agLFQVB+j_E+dT9JQA@mail.gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>	<CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>	<54C0AF30.1030501@gmail.com>
	<CAHjP37FF9R2FDjjN0=DHvxRGYb-Utxa_agLFQVB+j_E+dT9JQA@mail.gmail.com>
Message-ID: <54C12EF1.8010607@gmail.com>

On 01/22/2015 03:37 PM, Vitaly Davidovich wrote:
> Just curious - why would this be "expected" given that this only matters if
> published unsafely? Is there something about these classes that implies
> they're ok to publish unsafely?

Even if there's nothing that implies they're ok, it would be nice (if 
not "expected") from AtomicReference to behave consistently regardless 
of whether the referent is published via set() or via constructor.

Regards, Peter

>
> sent from my phone
> On Jan 22, 2015 3:05 AM, "Peter Levart" <peter.levart at gmail.com> wrote:
>
>>   For AtomicReference it would be expected that the following:
>>
>> reference = new AtomicReference(referent); // relaxed assignement to
>> reference
>>
>> ...is at least as safe as:
>>
>> reference = new AtomicReference(); // relaxed assignement to reference
>> reference.set(referent);
>>
>> So that if 'referent' pointer is observed in another thread as non-null,
>> the underlying object is fully initialized.
>>
>> Therefore for AtomicReference the constructor should use a volatile write,
>> but for other AtomicXXX classes it needs not.
>>
>> Peter
>>
>> On 01/22/2015 04:23 AM, Vitaly Davidovich wrote:
>>
>> That sounds right to me (I'm assuming you're particularly concerned about
>> unsafe publication of AtomicReference since if that reference is published
>> safely then that publication creates the hb edge and the rest here is
>> unimportant).
>>
>> sent from my phone
>> On Jan 21, 2015 10:13 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:
>>
>>> Vitaly Davidovich wrote:
>>>
>>>> Don't think that's true; the write of the reference can be
>>>> reordered with the volatile write, leading to 0. Volatile store
>>>> prevents prior writes from moving past it, not subsequent ones.
>>> Oops! Okay, that makes sense. I'm glad I wrote it out so that you
>>> could catch my error. So Hans is correct that the uninitialized 0
>>> value can be observed as long as the reference to the AtomicInteger
>>> itself is shared racily.
>>>
>>> To make another attempt: I believe it is still true that if T2 does
>>> see T1's write of 5, then there must be a happens-before edge from
>>> that write to that read, such that anything preceding the
>>> constructor in T1 happens-before anything following the get() in T2.
>>>
>>> That's especially important for AtomicReference, because it means
>>> that if a reference is passed into the constructor then the object
>>> it refers to is safely published to any thread that sees it returned
>>> from get(). If the write in the constructor were not volatile, then
>>> the referenced object could be seen partially initialized.
>>>
>>> Is that correct?
>>>
>>> Cheers,
>>> Justin
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>


From oleksandr.otenko at oracle.com  Thu Jan 22 14:34:03 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 22 Jan 2015 19:34:03 +0000
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
Message-ID: <54C150AB.9080103@oracle.com>

You want to move signal() to finally

Alex

On 21/01/2015 23:16, Millies, Sebastian wrote:
>
> everyone, thanks for the explanations regarding LockSupport.park().
>
> Please let me describe the problem that the coding I am faced with 
> appears to be trying to solve. Perhaps this rings a bell and someone 
> can point me to some higher-level construct from j.u.c which one might 
> use. My own proposal follows below.
>
> ?Concurrent requests are made of a server. There is one thread per 
> request. The server calculates how much heap space will be needed to 
> fulfill each request and keeps track of available memory.
>
> ?Requests that can be served with the available memory may run 
> immediately. Other requests have to wait in a queue.
>
> ?Whenever a request finishes, it wakes up the longest waiting thread 
> to give it a chance to run.
>
> ?Whenever a thread wakes up and finds it can run within the available 
> memory, it removes itself from the queue and also recursively wakes up 
> the next longest waiting thread, to give it a chance to run, too.
>
> Doesn?t sound too unusual.
>
> The current implementation uses a central coordinating singleton, 
> which keeps track of the memory requirements per thread in a 
> ThreadLocal variable, available memory in an instance variable, and 
> has a BlockingQueue for waiting tasks. These resources are protected 
> using synchronized blocks. But this is interspersed (in code stretches 
> not protected with a lock) with calls to park() and 
> unpark(queue.peek()) for suspending and resuming threads. The solution 
> is obviously incorrect, as described in a previous post.
>
> I wonder if a rewrite might not be better than a fix. I was thinking 
> that maybe ReentrantLock (the fair version, because I want the FIFO 
> behavior) would be good to use. I might get a condition from the lock 
> and use Condition.await()/Condition.signal() to suspend/resume 
> threads. I think I might not even need an explicit queue. Is that 
> right? Sort of like this (comments welcome):
>
> public class WaitQueue
>
> {
>
>   private final ReentrantLock lock = new ReentrantLock( true ); // use 
> a fair lock.
>
>   private final Condition mayRun = lock.newCondition();
>
>   private final ThreadLocal<Long> requiredMemory = new 
> ThreadLocal<Long>();
>
>   private long availableMemory;
>
>   public WaitQueue( long availableMemory )
>
>   {
>
>     this.availableMemory = availableMemory;
>
>   }
>
>   public final void await() throws InterruptedException
>
>   {
>
>     lock.lock();
>
>     try {
>
>       while( availableMemory < requiredMemory.get() ) {
>
>         mayRun.await();
>
>       }
>
>       availableMemory -= requiredMemory.get();
>
>       mayRun.signal(); // give next in line a chance, too
>
>     }
>
>     finally {
>
>       lock.unlock();
>
>     }
>
>   }
>
>   public final void release()
>
>   {
>
>     lock.lock();
>
>     try {
>
>       availableMemory += requiredMemory.get();
>
>       mayRun.signal();
>
>     }
>
>     finally {
>
>       lock.unlock();
>
>     }
>
>   }
>
> }
>
> I?d use it like this:
>
> interface Request
>
> {
>
>   long requiredMemory();
>
>   void serve();
>
> }
>
> public void serveRequest( Request req )
>
> {
>
>   requiredMemory.set( req.requiredMemory() );
>
>   try {
>
>     await();
>
>   }
>
>   catch( InterruptedException e ) {
>
>     return; // thread shutdown requested
>
>   }
>
>   try {
>
>     req.serve();
>
>   }
>
>   finally {
>
>     release();
>
> requiredMemory.remove();
>
>   }
>
> }
>
> Or should I do something completely different? Is there a standard 
> approach to this kind of problem?
>
> *Sebastian Millies*
>
> PPM, Saarbr?cken C1.67, +49 681 210 3221
>
>
> Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 
> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt 
> HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich 
> (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd 
> Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory 
> Board: Dr. Andreas Bereczky - *http://www.softwareag.com*
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/abac2b31/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 22 15:03:15 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Jan 2015 15:03:15 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54C12EF1.8010607@gmail.com>
References: <CAPUmR1Yz3cc7OBYhmH_5H1p0D_tT9kFTg-cgB74aMqqCR3DARQ@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCEEMLKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CE5F@sm-ex-01-vm.guidewire.com>
	<CAHjP37EVUURLNaTg9wQUf9KgucBOFCOoV5Oeya_++Bhhh-SKYw@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CF10@sm-ex-01-vm.guidewire.com>
	<CAHjP37GYitUmUmraewgVVdBOWSf+rrdmG821SM8vDjWa_CqT8w@mail.gmail.com>
	<54C0AF30.1030501@gmail.com>
	<CAHjP37FF9R2FDjjN0=DHvxRGYb-Utxa_agLFQVB+j_E+dT9JQA@mail.gmail.com>
	<54C12EF1.8010607@gmail.com>
Message-ID: <CAHjP37FO9w=Gn4C1H_NO6vjn+Ui7UT+ETAuOLj7sOFAbMFJJUQ@mail.gmail.com>

Ok, fair enough.  I'll agree that if it's "cheap" (perf and maintenance
wise) to facilitate that, no harm in doing it.  On the other hand, that
practice (unsafe publication) should generally be discouraged.

On Thu, Jan 22, 2015 at 12:10 PM, Peter Levart <peter.levart at gmail.com>
wrote:

> On 01/22/2015 03:37 PM, Vitaly Davidovich wrote:
>
>> Just curious - why would this be "expected" given that this only matters
>> if
>> published unsafely? Is there something about these classes that implies
>> they're ok to publish unsafely?
>>
>
> Even if there's nothing that implies they're ok, it would be nice (if not
> "expected") from AtomicReference to behave consistently regardless of
> whether the referent is published via set() or via constructor.
>
> Regards, Peter
>
>
>
>> sent from my phone
>> On Jan 22, 2015 3:05 AM, "Peter Levart" <peter.levart at gmail.com> wrote:
>>
>>    For AtomicReference it would be expected that the following:
>>>
>>> reference = new AtomicReference(referent); // relaxed assignement to
>>> reference
>>>
>>> ...is at least as safe as:
>>>
>>> reference = new AtomicReference(); // relaxed assignement to reference
>>> reference.set(referent);
>>>
>>> So that if 'referent' pointer is observed in another thread as non-null,
>>> the underlying object is fully initialized.
>>>
>>> Therefore for AtomicReference the constructor should use a volatile
>>> write,
>>> but for other AtomicXXX classes it needs not.
>>>
>>> Peter
>>>
>>> On 01/22/2015 04:23 AM, Vitaly Davidovich wrote:
>>>
>>> That sounds right to me (I'm assuming you're particularly concerned about
>>> unsafe publication of AtomicReference since if that reference is
>>> published
>>> safely then that publication creates the hb edge and the rest here is
>>> unimportant).
>>>
>>> sent from my phone
>>> On Jan 21, 2015 10:13 PM, "Justin Sampson" <jsampson at guidewire.com>
>>> wrote:
>>>
>>>  Vitaly Davidovich wrote:
>>>>
>>>>  Don't think that's true; the write of the reference can be
>>>>> reordered with the volatile write, leading to 0. Volatile store
>>>>> prevents prior writes from moving past it, not subsequent ones.
>>>>>
>>>> Oops! Okay, that makes sense. I'm glad I wrote it out so that you
>>>> could catch my error. So Hans is correct that the uninitialized 0
>>>> value can be observed as long as the reference to the AtomicInteger
>>>> itself is shared racily.
>>>>
>>>> To make another attempt: I believe it is still true that if T2 does
>>>> see T1's write of 5, then there must be a happens-before edge from
>>>> that write to that read, such that anything preceding the
>>>> constructor in T1 happens-before anything following the get() in T2.
>>>>
>>>> That's especially important for AtomicReference, because it means
>>>> that if a reference is passed into the constructor then the object
>>>> it refers to is safely published to any thread that sees it returned
>>>> from get(). If the write in the constructor were not volatile, then
>>>> the referenced object could be seen partially initialized.
>>>>
>>>> Is that correct?
>>>>
>>>> Cheers,
>>>> Justin
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.
>>> oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/d4126696/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 22 18:25:56 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 22 Jan 2015 23:25:56 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <129D2A38-A415-4A10-AD3D-6D77FF055651@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<hqKe1p00m02hR0p01qKjmt>
	<129D2A38-A415-4A10-AD3D-6D77FF055651@cox.ne! t>
Message-ID: <54C18704.4090205@oracle.com>

It would be great if Java had the current JMM spec from the start. It 
must be really hard to find a modern JVM that can run 1.1 code 
bug-compatibly.

Alex

On 20/01/2015 22:44, Gregg Wonderly wrote:
>> On Jan 19, 2015, at 8:17 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>
>> I don't think there can be an agreement on what "simple" means, it may be easier to consider the prerequisite knowledge of the target audience.
>>
>> I don't know how many people are really interested in this discussion - but the only way to share understanding with the few that are, is broadcast on the relevant topic.
>>
>> If I recall right what your pain point is, declaring all instructions in all threads to appear in program order to all threads, does not make an awful lot of difference in the reasoning methods, and in many cases even no difference in the outcomes. Roughly speaking, given a proof of correctness of "totally ordered" program, it may be possible to turn the key variables used in decision-making points into volatiles, and turn everything else into normal variables. The trouble is, most people don't even consider what the proof of correctness of a "totally ordered" program would look like. I think very quickly you will want to use induction in your proofs, which will touch only a few edges in your program - the rest being irrelevant, or can be proven from those few. I think it is the use of induction that determines what JMM should be; that's the only thing that really reduces complexity of proofs.
> My issue is primarily about the fact that this reordering changes the observable behavior of an application when casually observing it.  Things like logging statements that show the current value of a variable used in another thread for example.  If I have a remote ?observation? feature, and I want to ?see? what is happening in another thread, I have to burden the entire system with shenanigans to try and ?see? what is happening.  I have to burden the processing in that thread with fences/synchronization galore, just so that occasional observation can occur.  Yes, I can also create a synchronized observation point where data is passed between threads, but in doing that, I have to use ?synchronized? or volatile elsewhere in order for there to be a happens before relationship between writes in the observed thread and reads in the observer thread.   This is a very common thing for my software systems.  Optimizations, such as loop lifting are thus a big deal for me, because we have let the hardware issues of ?visibility? exist as they do now, and further anchored that into the expected behavior(s) of the Java platform.
>
>> For example, your beloved loop example.
>>
>> while(!signal)....
>>
>> Why does the loop even exist? To me, it is evidence that somewhere in your proof you will have a statement "if signal==true, then ....<some consistency assertion here>" - as the program cannot deal with the inconsistent state; it does not progress and doesn't read nor write anything that appears in program order after the loop, until "signal==true?.
> First, let me explain that I have used Java since the beginning of time.  I have written huge amounts of Java software in the form of libraries and desktop applications and applets.  I have also written java servlets.  I created my own messaging/brokering engine in 1997/1998 before JMS existed.   There are just countless places where I have had to deal with all aspects of the different Java versions. I never deployed anything on 1.2 due to the huge number of problems in that release (that release pretty much killed the momentum of Java in the market place as a desktop environment, and that was a disastrous outcome for Java overall).  I?ve dealt with very careful tuning of multi-threaded applications, such as my broker, which had hundreds of interacting threads around a handful of data structures.  There are places where I was a bit reckless with synchronization, because the early JVMs didn?t show signs of caring about that.  This whole loop hoisting bit is one of the places where I knew that the values were not strictly synchronized.  But, I did not care, because eventually the change in the loop control variable was always visible.  I never noted any measurable delay in the visibility on the x86 servers.  Hardware specific misbehaviors are a problem for me, because as a Java developer, I can?t always know every version of ever JVM that might exist.
>
> Since volatile did not work in JDK 1.4 and earlier, loops with shared state termination control, had to be written as
>
> 	while(true) {
> 		synchronized(this) {
> 			if( done )
> 				break;
> 		}
> 		?.
> 	}
>
> to be completely synchronized with
>
> 	public void stop() {
> 		synchronized(this) { done = true; }
> 	}
>
> I didn?t like this verbosity all over the place, and so I instead used unsynchronized state as termination control because I didn?t need atomic changes of more than a single value across threads in these cases.
>
> I use threads which process external events and another thread will ask them to stop that processing.  This typically looks like:
>
> public void run() {
> 	while( !done ) {
> 		try {
> 			if( sock == null ) {
> 				sock = new Socket(?);
> 			}
> 			data = readSomething();
> 			process(data);
> 		} catch( IOException ex ) {
> 			try {
> 				sock.close();
> 			} catch( Exception exx ) {
> 				log.fine(exx);
> 			} finally {
> 				sock = null;
> 			}
> 		} catch( Exception ex ) {
> 			log.severe(ex);
> 		}
> 	}
> }
>
> I might then have a stop() method that looks like:
>
> public void stop() {
> 	done = true;
> 	sock.close();
> }
>
> This method will trivially stop the thread by closing the socket.  This worked fine when the loop hoist was not happening. But suddenly it failed with the Java update.  The whole application completely stopped working because these worker threads would not stop when needed.  They would reopen the socket and keep going.  I Java 5 and later, I do now use volatile everywhere to keep loop hoisting from occurring.
>
>> If you always rely on "signal == true", then you don't need total ordering - only partial ordering joined through signal read and write is sufficient; that means signal should be volatile, and the rest doesn't have to be (well, subject to restrictions in the rest of the code). If you sometimes do not rely on "signal==true", try to convince yourself that <some consistency assertion> holds, can you? If you can't convince that the consistency assertion holds, how can you show the program is correct? Nowhere here I mentioned total or partial ordering of instructions.
> The problem is that software which used to work 100% of the time when volatile was not available (and it?s lack of use did not keep the software from working), suddenly stopped working on a Java update.  To me, that is a poor choice for an automatic optimization.  Java never said that the change in ?signal? could not be observed, ever.  This specific change, as I?ve said before, caused many people that I have interacted with and discussed this issue with, to tell me, that they had already decided that program correctness was more important than performance to them.  The said that they had told their developers that all variables would either be declared final or volatile.
>
> We don?t know what other flow control optimizations might just ?happen? in the JVM implementation to non-volatile values in the future and make software stop working again.  Even now, we still have people carefully crafting racy applications to avoid synchronized() blocks!   Making everything volatile, keeps that from causing ?alteration? to execution paths so that we can guarantee the application stays in working order, no matter what version of the JVM it is run on.  People deploying software systems don?t always have control over what JVM they are deployed on.  I know that Sun and now Oracle seem to have/had no idea how to deploy desktop applications in Java, let alone applets where the whole world of versions exist.  But I have many such applications that I?d really prefer people to have pleasant experiences with, and not have to spend hours on forums or mailing lists or telephone calls to support trying to figure out exactly which version of something that they have no idea that they need, has a problem and ?that thing? needs to be replaced/downgraded/upgraded to make the ?application? work.
>
> I truly believe that volatile should be the default declaration in the JMM.  Performance tuning is what we do after we prove a program correct.  No matter how fast it goes, if it?s not computing correct results, then what good has been accomplished?  We should have to add an annotation or keyword to specify that we are willing to take the visibility risk and management into our own hands.
>
> I do appreciate your attention to optimization details and trying to understand how developers might be better served by the Java platform.   It is really bothersome and quite disturbing to users of the platform when any optimization suddenly breaks working software systems.  Java desktop applications or applets, delivered to the users as just JAR files that stop working like this are very frustrating for developers to debug.  Especially when the developers have no way of knowing who?s having what problems.  That?s just not good system design, at all.  Yes, developers can test, test and retest and evaluate all kinds of details.  But in the end, why should we decide that more time should be spent to get software to users when there is no direct advantage to users in doing that?   In this particular case, I don?t know how there is any appreciable gain in performance from this optimization for x86 in particular, so why is it so dear and important to have such things going on?
>
> I have a few applications which I wrote on JDK 1.1.1, and they worked just fine on all versions of Java up to Java 5.  It?s that kind of breakage that just doesn?t make sense overall.
>
> How many conversations have been had with developers to ask them why they don?t create Java desktop applications?  What makes that such an unpleasant experience for them or their users?  Really, why isn?t Java a predominant part of the desktop landscape?  Is it because of performance, or something different?  I?m going to guess that performance isn?t the big issue keeping developers from using Java for their applications.
>
> Gregg Wonderly
>
>> Alex
>>
>> On 17/01/2015 05:27, Gregg Wonderly wrote:
>>>> On Jan 16, 2015, at 4:10 PM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:
>>>>
>>>> I would not bother specifying exactly what makes sense; only what's allowed.
>>>>
>>>> For example, what would you say if some hardware offered atomicity of update of just the permit, and offer no barrier semantics for the rest of the accesses around it?
>>>>
>>> Why would you want to specify in a language, variations of behavior that complicate software design, and cause software systems developers to fail because they can?t understand what actually will happen (as opposed to what can happen) or correctly implement software because they are having to interact, literally with random hardware behavior due to the language not adequately depicting tangible software behaviors.
>>>
>>> I know it?s really fun to play with this stuff and try and wrestle the most out of the hardware with the least possible constraints.  But really, how in the world can we have 100% reliable software systems when we provide 50 ways to be unsuccessful for every 1 way that is the correct way to structure the code?  Why is all of that variation event attractive to you Alex?  Do you really want developer to have to wrestle with software design?  Do you find it fun to manipulate people with such complexity in the system and some how prove that your understanding is more than theirs?  Does this give you some kind of satisfaction or what?
>>>
>>> In the end, this is a huge barrier to being successful with Java.  It provides not obvious benefit to the developers from a long term goal of building and maintaining viable software systems.  Instead it creates problem after problem.  Software system become unpredictable with seemingly random behaviors. I just fail to see how this can feel like good software language design.  Maybe you can send me a private response if you don?t care to discuss this stuff publicly.  Practically I find this to be just insane.  All of this discussion in this thread just points out how 100s of hours of peoples time can be wasted on this issue all for nothing but surprise.  Developers with great knowledge of the JMM still have a hard time proving to themselves what the outcome of their code will be.
>>>
>>> Gregg Wonderly
>>>
>>>> Alex
>>>>
>>>> On 16/01/2015 20:58, Justin Sampson wrote:
>>>>> Oleksandr Otenko wrote:
>>>>>
>>>>>> You can't claim that it's not possible to implement reschedule in
>>>>>> such a way that yield cannot tell whether reschedule was called
>>>>>> since yield was called last time.
>>>>> Ah, I see what you're getting at now, and I understand what you
>>>>> meant by talking about a "fast path" earlier.
>>>>>
>>>>> In your example, the yield/reschedule operations are _themselves_ a
>>>>> valid implementation of park/unpark, complete with atomic handling
>>>>> of internal state (the queue) that ensures a reschedule isn't lost.
>>>>> That wasn't what I was imagining in the example of mine that you
>>>>> were replying to.
>>>>>
>>>>> So I think we're actually agreeing that the "permit" semantics do
>>>>> have to be integrated into atomic operations at some level of
>>>>> implementation. And I also agree that it's possible to define a
>>>>> fast-path implementation on top of such semantics that still remains
>>>>> valid but with less atomicity.
>>>>>
>>>>> In fact, if I may generalize into a theorem, I believe we can say
>>>>> that given any valid implementation of park/unpark, we can construct
>>>>> another valid implementation, call it park'/unpark', like so:
>>>>>
>>>>> Thread {
>>>>>    volatile boolean permit';
>>>>> }
>>>>>
>>>>> park'() {
>>>>>    Thread t = Thread.currentThread();
>>>>>    if (!t.permit') {
>>>>>      park();
>>>>>    }
>>>>>    t.permit' = false;
>>>>> }
>>>>>
>>>>> unpark'(Thread t) {
>>>>>    if (!t.permit') {
>>>>>      t.permit' = true;
>>>>>      unpark(t);
>>>>>    }
>>>>> }
>>>>>
>>>>> I'm writing up another attempt at formal semantics that allows us to
>>>>> prove this theorem, which I'll send out later today. :)
>>>>>
>>>>> Cheers,
>>>>> Justin
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From ben_manes at yahoo.com  Thu Jan 22 19:23:47 2015
From: ben_manes at yahoo.com (Ben Manes)
Date: Fri, 23 Jan 2015 00:23:47 +0000 (UTC)
Subject: [concurrency-interest] Using relaxed reads and writes
Message-ID: <1538772756.3228485.1421972627053.JavaMail.yahoo@jws10035.mail.ne1.yahoo.com>

I think the following is a safe optimization for my use-case, but I'd appreciate any feedback / concerns.
Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is an entry in an LRU cache with miscellaneous metadata fields such as the queue links, the value, expiration timestamps, etc. When updating the value the node must be synchronized on in order to block if created by a concurrent computation. This means the more optimal CAS loop isn't used, which may have affect on simplifying the following discussion. Lets say that the Node's value and accessTime fields are volatile.
When writing the value the node's lock must be held (or implicitly safe due to Node construction). In that case I think that a relaxed write (Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for visibility (or publishing via insert if created). When the entry is retrieved on a read the accessTime needs to be checked for expiration and updated to the current nano time, and the value returned. As these follow a ConcurrentHashMap lookup, I think that the timestamp and value reads can be relaxed (Unsafe#getLong and Unsafe#getObject) by piggybacking on the Map.get()'s memory barrier. It also appears that updating the accessTime can be relaxed (Unsafe#putOrderedObject) as eviction it is a best effort policy.
The race conditions of seeing stale data appears to me to be benign, already inherent in concurrent data structures, and caches allow a slightly more relaxed attitude toward synchronization. Do these assumption hold water?
Thanks,Ben
P.S. Those interested in explicit code can see a prototype at:?https://github.com/ben-manes/caffeine?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/42b19a08/attachment-0001.html>

From lukeisandberg at gmail.com  Thu Jan 22 20:02:18 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Thu, 22 Jan 2015 17:02:18 -0800
Subject: [concurrency-interest] Safe publishing strategy
Message-ID: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>

Guava has this method:
https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475

Futures.withFallback

which is implemented via a delegating future:

private static class FallbackFuture<V> extends
AbstractFuture.TrustedFuture<V> {
    ListenableFuture<? extends V> input;

    FallbackFuture(ListenableFuture<? extends V> input,
        final FutureFallback<? extends V> fallback,
        final Executor executor) {
      input = input;
      /// a bunch of stuff with listeners
    }

  @Override
    public boolean cancel(boolean mayInterruptIfRunning) {
      ListenableFuture<?> local = this.input;
      if (super.cancel(mayInterruptIfRunning)) {
        if (local != null) {
          local.cancel(mayInterruptIfRunning);
        }
        return true;
      }
      return false;
   }
}

This future does a lot of stuff to handle recovering from failure of the
input future.  But as a general rule in guava, all the Futures.java
utilities try to propagate cancellation.  The question is

How do we ensure that the initial write to 'input' is visible to cancel()?

Because input is non-final, there is no guarantee that it will be visible
if someone unsafely tunnels the FutureFallback to another thread and calls
cancel(). Or are we analyzing the situation incorrectly.

This pattern is common throughout our guava ListenableFuture utilities and
as far as we can tell it is a latent bug since we aren't 'safely
publishing' our delegating future wrappers

Thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/bbb99892/attachment.html>

From boehm at acm.org  Thu Jan 22 20:09:23 2015
From: boehm at acm.org (Hans Boehm)
Date: Thu, 22 Jan 2015 17:09:23 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAHjP37F37NFJ9csjbs4QbCyDwYUFBxFuhOtyUt+1=dwC0Qs+Cw@mail.gmail.com>
References: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEMMKMAA.davidcholmes@aapt.net.au>
	<CAHjP37F37NFJ9csjbs4QbCyDwYUFBxFuhOtyUt+1=dwC0Qs+Cw@mail.gmail.com>
Message-ID: <CAPUmR1ZtEzHRKn3ESJohDH7v=VmXBEQXMLwiFm9jwfE_hDgY1Q@mail.gmail.com>

On Wed, Jan 21, 2015 at 6:48 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:
>
> ...
>
> But coming back to AtomicXXX case, I agree with earlier comments that
these classes aren't typically constructed over and over, so any
performance gains here would unlikely to actually be detectable in real
usage.
>
Looking at e.g. ConcurrentSkipListMap, that appears to be true mostly
because code that cares uses volatile fields and unsafe operations instead
of AtomicXXX?

It does still seem to have a lot of non-racing volatile assignments in
constructors that are likely to result in unnecessary fences.  I would be a
bit surprised if that were not measurable.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/793f579e/attachment.html>

From boehm at acm.org  Thu Jan 22 20:17:45 2015
From: boehm at acm.org (Hans Boehm)
Date: Thu, 22 Jan 2015 17:17:45 -0800
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <OFB7E11BF2.4B4C0F9A-ONC1257DD5.000DBD14-C1257DD5.000E2863@de.ibm.com>
References: <CAPUmR1boL-6LVHN0gPRS5VGz3nW0ES5=g5CW4kUde0ddcfQUQw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEMAKMAA.davidcholmes@aapt.net.au>
	<CAPUmR1Y_KxUibZq+YKtwP2b6Pp66B_-_c5VVyPUrcD2iaWArgg@mail.gmail.com>
	<OF7C5D7DFA.3393EBF7-ONC1257DD4.00329162-C1257DD4.00343E72@de.ibm.com>
	<CAPUmR1aHuNtfQnNf3-5mudrDdrs=bzEq59LkP=m_ksOypF4LxQ@mail.gmail.com>
	<OFE847C215.FFE41248-ONC1257DD4.00729225-C1257DD4.00739BFD@de.ibm.com>
	<CAPUmR1Yt-T=w0iGh9GGFdGgAdVhxZnHX5QVuASupi_dgMYPmUQ@mail.gmail.com>
	<OFB7E11BF2.4B4C0F9A-ONC1257DD5.000DBD14-C1257DD5.000E2863@de.ibm.com>
Message-ID: <CAPUmR1aNnPuLeqzNpJsro+CqYJrzZttQp6mbw0B=5aQSW2njmg@mail.gmail.com>

Yes, I would have expected the load cost to be minimized, especially since
that would make the performance profile a little more consistent with x86.

Can you not mix the acquire/release implementations you propose with the
trailing hwsync implementation for SC atomics?  I haven't thought through
the details.

Hans

On Wed, Jan 21, 2015 at 6:34 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> you mean a mapping with a traling hwsync:
>
> volatile load: ld; hwsync;
> volatile store: lwsync; st; hwsync
> CAS: lwsync; _loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit: hwsync
>
> and you are surprised because it imposes less penalty on loads (but more on
> stores), right?
>
> Well, I suppose that the 'conspiracy' was based on the idea to preserve
> isync to make it easier to reason while relaxing unnecessary SC to (at
> least) release-acquire:
>
> acquire load: ld; cmp; bd; isync
> realease store: lwsync; st;
> CAS.rel-acq: lwsync ;_loop: ldarx; cmp; bc _exit; stcwx; bc _loop; _exit:
> isync;
>
> because for the most programs hwsync is not really needed...
>
> regards,
> alexander.
>
> Hans Boehm <boehm at acm.org>@cs.oswego.edu on 21.01.2015 22:48:48
>
> Sent by:        concurrency-interest-bounces at cs.oswego.edu
>
>
> To:     Alexander Terekhov/Germany/IBM at IBMDE
> cc:     "concurrency-interest at cs.oswego.edu"
>        <concurrency-interest at cs.oswego.edu>, David Holmes
>        <dholmes at ieee.org>
> Subject:        Re: [concurrency-interest] Varieties of CAS semantics
> (another
>        doc fix request)
>
>
> Yes, your recommendation mirrors that for C++ seq_cst operations in
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html, and I think those
> are the semi-official IBM recommendations.  The Java and C++ mappings
> should be the same to ensure correct interoperation.
>
> (I continue to be surprised that the choice was made to use two fences on a
> load instead of a store.  I think either choice is correct, so long as it's
> used consistently.)
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/425bcbf5/attachment.html>

From vitalyd at gmail.com  Thu Jan 22 20:21:35 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Jan 2015 20:21:35 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <CAPUmR1ZtEzHRKn3ESJohDH7v=VmXBEQXMLwiFm9jwfE_hDgY1Q@mail.gmail.com>
References: <CAPUmR1YMhuVpDg9S-9bR+oAGe1i1YhXT5qPVzriTbn8q8RQuZw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEMMKMAA.davidcholmes@aapt.net.au>
	<CAHjP37F37NFJ9csjbs4QbCyDwYUFBxFuhOtyUt+1=dwC0Qs+Cw@mail.gmail.com>
	<CAPUmR1ZtEzHRKn3ESJohDH7v=VmXBEQXMLwiFm9jwfE_hDgY1Q@mail.gmail.com>
Message-ID: <CAHjP37GCqJHkkzxij0QiJghAvdD1Nc7WpheWQ=egbb4c+f67Rw@mail.gmail.com>

In my observations, you don't tend to see a lot of AtomicXXX instances
created because the code in question is performance sensitive, and
allocating these things incessantly would bring a world of GC pain.  The
other aspect is that if you want to embed these classes into other classes
that have a high instance count at runtime, then people tend to avoid
AtomicXXX due to memory overhead.  Finally, there's an in direction
involved with these classes to get at their fields.

So the one thing that may crop up is if you have lots of static or
otherwise singleton instances of these types instantiated.  However, that
code is likely to be running in interpreter anyway, and performance is
already absent.

I'd be curious myself to hear of any real world cases where this would
matter as I haven't come across them personally.

sent from my phone
On Jan 22, 2015 8:09 PM, "Hans Boehm" <boehm at acm.org> wrote:

>
> On Wed, Jan 21, 2015 at 6:48 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
> >
> > ...
> >
> > But coming back to AtomicXXX case, I agree with earlier comments that
> these classes aren't typically constructed over and over, so any
> performance gains here would unlikely to actually be detectable in real
> usage.
> >
> Looking at e.g. ConcurrentSkipListMap, that appears to be true mostly
> because code that cares uses volatile fields and unsafe operations instead
> of AtomicXXX?
>
> It does still seem to have a lot of non-racing volatile assignments in
> constructors that are likely to result in unnecessary fences.  I would be a
> bit surprised if that were not measurable.
>
> Hans
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/feb84394/attachment.html>

From martinrb at google.com  Thu Jan 22 23:01:09 2015
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 22 Jan 2015 20:01:09 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
Message-ID: <CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>

It does look to me like FallbackFuture.running can be accessed via a data
race.  The code seems buggy.

Generally, Future implementations have volatile fields and careful state
transitions via CAS, as with FutureTask.

One is tempted to immediately make running volatile.

As for the initial write to running, it will probably (almost?) always be
seen in practice, especially on x86, since the future itself is likely to
be safely published somehow and there will likely be ordering between the
end of the constructor and the write of the reference.  These kinds of
races are very difficult to demonstrate in practice.

On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Guava has this method:
>
> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>
> Futures.withFallback
>
> which is implemented via a delegating future:
>
> private static class FallbackFuture<V> extends
> AbstractFuture.TrustedFuture<V> {
>     ListenableFuture<? extends V> input;
>
>     FallbackFuture(ListenableFuture<? extends V> input,
>         final FutureFallback<? extends V> fallback,
>         final Executor executor) {
>       input = input;
>       /// a bunch of stuff with listeners
>     }
>
>   @Override
>     public boolean cancel(boolean mayInterruptIfRunning) {
>       ListenableFuture<?> local = this.input;
>       if (super.cancel(mayInterruptIfRunning)) {
>         if (local != null) {
>           local.cancel(mayInterruptIfRunning);
>         }
>         return true;
>       }
>       return false;
>    }
> }
>
> This future does a lot of stuff to handle recovering from failure of the
> input future.  But as a general rule in guava, all the Futures.java
> utilities try to propagate cancellation.  The question is
>
> How do we ensure that the initial write to 'input' is visible to cancel()?
>
>
> Because input is non-final, there is no guarantee that it will be visible
> if someone unsafely tunnels the FutureFallback to another thread and calls
> cancel(). Or are we analyzing the situation incorrectly.
>
> This pattern is common throughout our guava ListenableFuture utilities and
> as far as we can tell it is a latent bug since we aren't 'safely
> publishing' our delegating future wrappers
>
> Thanks
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/11c8a30b/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 22 23:32:52 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 22 Jan 2015 23:32:52 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
Message-ID: <CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>

So I think we've covered this before on this list, but until JMM is
revised, a volatile wouldn't technically prevent that type of reordering
here.  However, it appears that most (all?) JVMs treat volatile like final
in constructor.

However, adding volatile in this case could hurt performance as, unlike the
AtomicXXX case discussed on the other thread, I'm assuming these wrapper
futures are constructed a lot.  It's also a waste if nobody actually
publishes unsafely.

sent from my phone
On Jan 22, 2015 11:24 PM, "Martin Buchholz" <martinrb at google.com> wrote:

> It does look to me like FallbackFuture.running can be accessed via a data
> race.  The code seems buggy.
>
> Generally, Future implementations have volatile fields and careful state
> transitions via CAS, as with FutureTask.
>
> One is tempted to immediately make running volatile.
>
> As for the initial write to running, it will probably (almost?) always be
> seen in practice, especially on x86, since the future itself is likely to
> be safely published somehow and there will likely be ordering between the
> end of the constructor and the write of the reference.  These kinds of
> races are very difficult to demonstrate in practice.
>
> On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> Guava has this method:
>>
>> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>>
>> Futures.withFallback
>>
>> which is implemented via a delegating future:
>>
>> private static class FallbackFuture<V> extends
>> AbstractFuture.TrustedFuture<V> {
>>     ListenableFuture<? extends V> input;
>>
>>     FallbackFuture(ListenableFuture<? extends V> input,
>>         final FutureFallback<? extends V> fallback,
>>         final Executor executor) {
>>       input = input;
>>       /// a bunch of stuff with listeners
>>     }
>>
>>   @Override
>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>       ListenableFuture<?> local = this.input;
>>       if (super.cancel(mayInterruptIfRunning)) {
>>         if (local != null) {
>>           local.cancel(mayInterruptIfRunning);
>>         }
>>         return true;
>>       }
>>       return false;
>>    }
>> }
>>
>> This future does a lot of stuff to handle recovering from failure of the
>> input future.  But as a general rule in guava, all the Futures.java
>> utilities try to propagate cancellation.  The question is
>>
>> How do we ensure that the initial write to 'input' is visible to
>> cancel()?
>>
>> Because input is non-final, there is no guarantee that it will be visible
>> if someone unsafely tunnels the FutureFallback to another thread and calls
>> cancel(). Or are we analyzing the situation incorrectly.
>>
>> This pattern is common throughout our guava ListenableFuture utilities
>> and as far as we can tell it is a latent bug since we aren't 'safely
>> publishing' our delegating future wrappers
>>
>> Thanks
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150122/ac585100/attachment.html>

From Sebastian.Millies at softwareag.com  Fri Jan 23 04:48:25 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Fri, 23 Jan 2015 09:48:25 +0000
Subject: [concurrency-interest] Resource-based waiting
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89C17D@HQMBX5.eur.ad.sag>

From: Justin Sampson [mailto:jsampson at guidewire.com]
Sent: Thursday, January 22, 2015 12:49 AM
To: Millies, Sebastian; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious LockSupport.park() return?) [snip]
>if FIFO ordering of requests is absolutely necessary, I think you'll have to arrange for it more explicitly.

From: Vitaly Davidovich [mailto:vitalyd at gmail.com]
Sent: Thursday, January 22, 2015 2:12 AM
>Have you looked into seeing if Semaphore (with fairness) would help? Your use case sounds like the classical
>resource usage throttle, with heap being the resource.

From: b.elliottsmith at gmail.com [mailto:b.elliottsmith at gmail.com] On Behalf Of Benedict Elliott Smith
Sent: Thursday, January 22, 2015 12:05 PM
[snip]
>Alternatively you could build your own with AbstractQueuedLongSynchronizer, by pretty much cutting/pasting
>from Semaphore, but extending AQLS instead of AQS.

thank you all. Having taken all this advice to heart, I have reconsidered the thing.

Yes, FIFO ordering is important. In fact, the desired behavior is actually the following:
Requests are processed in the order in which they arrive.As many requests as will fit into memory may run in
parallel. Requests that do not fit must wait until memory becomes available. Requests that would fit into memory
must nonetheless wait if another request has arrived previously. The scheme is fair, no request will be starved out.

I now use an additional explicit ConcurrentLinkedQueue to buffer all incoming requests before they reach a Semaphore.
Requests must wait until they become head of the queue. Only then can they proceed to a fair semaphore that guards
memory allocation. Only after the request has passed the Semaphore will it remove itself from the queue and notify()
the next one in the queue.

I have decided to follow Benedict's suggestion to copy/paste j.u.c.Semaphore, internally using
AbstractQueuedLongSynchronizer. This seemed simple to do. It also seemed better than introducing yet another
synchronization construct, because everyone already understands a semaphore. I do hope that
AbstractQueuedLongSynchronizer is supported on all platforms?

I wouldn't want to pollute this forum. But if anyone is interested and would be willing to look at my proposed
solution, I'd be very happy to post it here (or send it privately, whatever is preferable).

-- Sebastian


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com



From TEREKHOV at de.ibm.com  Fri Jan 23 06:24:45 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Fri, 23 Jan 2015 12:24:45 +0100
Subject: [concurrency-interest] Varieties of CAS semantics (another
 doc	fix request)
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>
Message-ID: <OFDE82A357.89D83364-ONC1257DD6.003D7439-C1257DD6.003EB93E@de.ibm.com>

> ??? set/get are direct loads and stores and required to have volatile
semantics.

and what prevents addition of unconstrainedSet/unconstrainedGet akin to
lazySet?

that would not only help to spare barriers but also properly annotate
noncompeting accesses...

regards,
alexander.


Please respond to dholmes at ieee.org

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	"Hans Boehm" <boehm at acm.org>
cc:	concurrency-interest at cs.oswego.edu
Subject:	Re: [concurrency-interest] Varieties of CAS semantics (another
       doc	fix request)


??? set/get are direct loads and stores and required to have volatile
semantics. An actual read/wrote of a volatile field seems perfect to me.

David
-----Original Message-----
From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans
Boehm
Sent: Thursday, 22 January 2015 9:32 AM
To: David Holmes
Cc: Justin Sampson; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Varieties of CAS semantics (another doc
fix request)

That does have the advantage that it's a nice simple model.  It has the
disadvantage that you're paying for a property that arguably no reasonable
code cares about.  At least I'm having a hard time constructing a useful
example where it matters.

On Wed, Jan 21, 2015 at 3:20 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:
      The AtomicXXX classes are supposed to act like volatile fields with
      the addition of the atomic operations. Hence the actual field inside
      the AtomicXXX class, is and should be, volatile.

      David
       -----Original Message-----
       From: concurrency-interest-bounces at cs.oswego.edu [mailto:
       concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hans Boehm
       Sent: Thursday, 22 January 2015 9:05 AM
       To: Justin Sampson
       Cc: concurrency-interest at cs.oswego.edu; David Holmes
       Subject: Re: [concurrency-interest] Varieties of CAS semantics
       (another doc fix request)



       On Wed, Jan 21, 2015 at 12:41 PM, Justin Sampson <
       jsampson at guidewire.com> wrote:
       >
       > Hans Boehm wrote:
       >
       > > You're right that there is currently no sanctioned way to access
       > > the same object both as a volatile and non-volatile. But there
       > > seem to be good reasons for supporting that. E.g. the
       pointer/flag
       > > assignment in double-checked locking doesn't race and hence
       could
       > > be non-volatile.
       >
       > Do you mean that the write marked #2 below could be non-volatile,
       > while the read marked #1 is still volatile?
       >
       >   private volatile Something instance;
       >
       >   public Something getInstance() {
       >     if (instance == null) { // #1
       >       synchronized (this) {
       >         if (instance == null) // #4: added by HB
       >           instance = new Something(); // #2
       >       } // #3
       >     }
       >     return instance;
       >   }
       >
       > If so, I don't think that works. Another thread calling
       getInstance
       > could still see the Something not fully initialized, if its line
       #1
       > executes between the writing thread's lines #2 and #3 (exiting the
       > monitor).

       Sorry.  You're right; I misstated that.  It's the load at #4 that
       doesn't need to be volatile since racing writes are locked out
       anyway.

       >
       > > (Does the constructor for e.g. AtomicInteger perform a volatile
       > > assignment? The documentation appears unclear. [...])
       >
       > This the latest code in CVS:
       >
       >   private volatile int value;
       >
       >   /**
       >    * Creates a new AtomicInteger with the given initial value.
       >    *
       >    * @param initialValue the initial value
       >    */
       >   public AtomicInteger(int initialValue) {
       >     value = initialValue;
       >   }
       >
       >
       http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/atomic/AtomicInteger.java?revision=1.41&view=markup

       Interesting.  There's clearly not much of a reason for this to be
       volatile.  Any code that cared about ordering would also have to be
       prepared to see a pre-initialization zero value, which seems
       extremely unlikely.  C++11 carefully defines the initialization not
       to be atomic.

       >
       > > But the notion of intentionally mis-specifying an operation to
       > > write when it really doesn't just seems wrong to me.
       >
       > You and I are agreeing on this point. :)
       >
       > But you also have to be prepared that the implementation _might_
       > actually do a write. Don't some CPU architectures implement CAS
       with
       > a store cycle to main memory regardless of success or failure,
       just
       > inserting the current value in the store buffer late in the cycle
       if
       > it didn't match the expected value? Therefore if your code is
       > actually sensitive to whether other threads are indeed writing
       that
       > memory location, it's going to be buggy on those architectures.

       Indeed, now that I checked, the Intel x86 documentation seems to say
       that in places, though that part of the description looks quite
       dated to me.  I have always been told that modern processors acquire
       the cache line in exclusive mode and then perform the operation in
       cache.  And performance numbers seem to confirm that.

       In either case, this isn't really programmer visible one way or the
       other.  On an ll/sc architecture like ARM, it would generally be
       silly to do the store unconditionally, and we want to discourage
       that.  If the hardware does it under the covers (which I doubt for
       modern hardware; gratuitously dirtying the cache line doesn't seem
       free), so be it.

       I'm just arguing for a description like the current one in the
       specific class descriptions (and the CAS description in Wikipedia
       and most other places) that doesn't gratuitously look like it
       imposes additional requirements, and actually does impose expensive
       and useless ordering requirements on some important architectures.

       >
       > I'm having trouble imagining what could actually go wrong, though.
       > Is it possible that a no-op CAS would appear to undo non-volatile
       > changes in another thread? E.g.:
       >
       > T1: r1 = x; // volatile read, sees 0
       > T1: x = 3; // non-volatile write, not flushed yet
       > T2: cas(x, 1, 2); // fails, but re-writes 0 to main memory
       > T1: r2 = x; // non-volatile read, could see either 3 or 0?

       I don't think anything would actually go wrong, though we would have
       to be careful to specify the write to disallow the 0 write in your
       example.  I just dislike specifying CAS in a new and nonstandard way
       for no positive benefit.

       Hans

       >
       > Cheers,
       > Justin
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From william.louth at jinspired.com  Fri Jan 23 06:28:12 2015
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Fri, 23 Jan 2015 12:28:12 +0100
Subject: [concurrency-interest] Resource-based waiting (was: Spurious
 LockSupport.park() return?)
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89BB58@HQMBX5.eur.ad.sag>
Message-ID: <54C2304C.6080307@jinspired.com>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/41ed5dee/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Fri Jan 23 09:06:12 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Fri, 23 Jan 2015 14:06:12 +0000
Subject: [concurrency-interest] Resource-based waiting
In-Reply-To: <CACr06N2ZnG_GeZfOkKtt5ahmQtBije4ROdpBEVomRs8qaUqsjQ@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89C17D@HQMBX5.eur.ad.sag>
	<CACr06N2ZnG_GeZfOkKtt5ahmQtBije4ROdpBEVomRs8qaUqsjQ@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89C350@HQMBX5.eur.ad.sag>

I believe the semaphore?s behavior is like this:

Semaphore sem = new Semaphore(6, true);

Thread1: sem.acquire(2); // return immediately
Thread2: sem.acquire(5); // wait, only 4 left
Thread3: sem.acquire(2); // return immediately (***)

I do NOT want (***). While thread 2 is waiting for permits, I want all further acquire?s to queue up as well, even if enough permits are available.

Or am I mistaken about Semaphore? The docs say:

acquire<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html#acquire-int->(int permits)Acquires the given number of permits, if they are available, and returns immediately, reducing the number of available permits by the given amount.

When fairness is set true, the semaphore guarantees that threads invoking any of the acquire<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html#acquire--> methods are selected to obtain permits in the order in which their invocation of those methods was processed (first-in-first-out; FIFO)

I understand that to mean that fairness extends only to the order in which acquires are processed, not to the order in which threads actually obtain permits.


n  Sebastian

From: Benedict Elliott Smith [mailto:b.elliottsmith at gmail.com]
Sent: Friday, January 23, 2015 2:53 PM
To: Millies, Sebastian
Cc: concurrency-interest
Subject: Re: [concurrency-interest] Resource-based waiting


A fair semaphore is FIFO. That's the definition of its implementation of fairness. So you can just use it directly and not worry about proving your solution's correctness.
On 23 Jan 2015 09:58, "Millies, Sebastian" <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
From: Justin Sampson [mailto:jsampson at guidewire.com<mailto:jsampson at guidewire.com>]
Sent: Thursday, January 22, 2015 12:49 AM
To: Millies, Sebastian; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: RE: [concurrency-interest] Resource-based waiting (was: Spurious LockSupport.park() return?) [snip]
>if FIFO ordering of requests is absolutely necessary, I think you'll have to arrange for it more explicitly.

From: Vitaly Davidovich [mailto:vitalyd at gmail.com<mailto:vitalyd at gmail.com>]
Sent: Thursday, January 22, 2015 2:12 AM
>Have you looked into seeing if Semaphore (with fairness) would help? Your use case sounds like the classical
>resource usage throttle, with heap being the resource.

From: b.elliottsmith at gmail.com<mailto:b.elliottsmith at gmail.com> [mailto:b.elliottsmith at gmail.com<mailto:b.elliottsmith at gmail.com>] On Behalf Of Benedict Elliott Smith
Sent: Thursday, January 22, 2015 12:05 PM
[snip]
>Alternatively you could build your own with AbstractQueuedLongSynchronizer, by pretty much cutting/pasting
>from Semaphore, but extending AQLS instead of AQS.

thank you all. Having taken all this advice to heart, I have reconsidered the thing.

Yes, FIFO ordering is important. In fact, the desired behavior is actually the following:
Requests are processed in the order in which they arrive.As many requests as will fit into memory may run in
parallel. Requests that do not fit must wait until memory becomes available. Requests that would fit into memory
must nonetheless wait if another request has arrived previously. The scheme is fair, no request will be starved out.

I now use an additional explicit ConcurrentLinkedQueue to buffer all incoming requests before they reach a Semaphore.
Requests must wait until they become head of the queue. Only then can they proceed to a fair semaphore that guards
memory allocation. Only after the request has passed the Semaphore will it remove itself from the queue and notify()
the next one in the queue.

I have decided to follow Benedict's suggestion to copy/paste j.u.c.Semaphore, internally using
AbstractQueuedLongSynchronizer. This seemed simple to do. It also seemed better than introducing yet another
synchronization construct, because everyone already understands a semaphore. I do hope that
AbstractQueuedLongSynchronizer is supported on all platforms?

I wouldn't want to pollute this forum. But if anyone is interested and would be willing to look at my proposed
solution, I'd be very happy to post it here (or send it privately, whatever is preferable).

-- Sebastian


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/a430ed2c/attachment.html>

From Sebastian.Millies at softwareag.com  Fri Jan 23 09:59:52 2015
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Fri, 23 Jan 2015 14:59:52 +0000
Subject: [concurrency-interest] Resource-based waiting
In-Reply-To: <CACr06N0kcEfCXJvEtw+JV51VHs+vm0SMD_ws0nruB+D6b=39Mg@mail.gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89C17D@HQMBX5.eur.ad.sag>
	<CACr06N2ZnG_GeZfOkKtt5ahmQtBije4ROdpBEVomRs8qaUqsjQ@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89C350@HQMBX5.eur.ad.sag>
	<CACr06N0kcEfCXJvEtw+JV51VHs+vm0SMD_ws0nruB+D6b=39Mg@mail.gmail.com>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD89C3BC@HQMBX5.eur.ad.sag>

thank you. So I misunderstood Semaphore after all ?

n  Sebastian

From: Benedict Elliott Smith [mailto:b.elliottsmith at gmail.com]
Sent: Friday, January 23, 2015 3:15 PM
To: Millies, Sebastian
Cc: concurrency-interest
Subject: Re: [concurrency-interest] Resource-based waiting

When fairness is set true, the semaphore guarantees that threads invoking any of the acquire<http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Semaphore.html#acquire--> methods are selected to obtain permits in the order in which their invocation of those methods was processed (first-in-first-out; FIFO)

This describes exactly the behaviour you desire. Thread3 will not be permitted to acquire until after Thread2 has successfully done so. You can also confirm this by analysis of the FairSync, which returns failure if there are any queued threads ahead of any acquire, regardless of the number of permits available.


On 23 January 2015 at 14:06, Millies, Sebastian <Sebastian.Millies at softwareag.com<mailto:Sebastian.Millies at softwareag.com>> wrote:
I believe the semaphore?s behavior is like this:

Semaphore sem = new Semaphore(6, true);

Thread1: sem.acquire(2); // return immediately
Thread2: sem.acquire(5); // wait, only 4 left
Thread3: sem.acquire(2); // return immediately (***)

I do NOT want (***). While thread 2 is waiting for permits, I want all further acquire?s to queue up as well, even if enough permits are available.

Or am I mistaken about Semaphore?

[snip]


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/3be250e7/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Jan 23 10:00:24 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 23 Jan 2015 15:00:24 +0000
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8CC8A@sm-ex-01-vm.guidewire.com>
References: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>	<CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8CC8A@sm-ex-01-vm.guidewire.com>
Message-ID: <54C26208.7090407@oracle.com>

No it doesn't. You need to see step (1) broken down into 1.a and 1.b:

1.a assign 17 to a field in AtomicInteger
1.b assign AtomicInteger reference to x.a

Only if there is hb between 1.b and (2) can you claim 1.a is also hb 
(3). Otherwise, you might observe x.a assigned with no hb between (1.a) 
and (3).

Alex

On 22/01/2015 00:09, Justin Sampson wrote:
> Hans Boehm wrote:
>
>> (b) If (1) does not happen before an instance of (3), then that
>> instance of (3) can see the uninitialized 0 value of x.a rather
>> than the value 17 or a later one. I claim it is very unlikely for
>> such code to be correct whether or not the assignment of 17 is
>> treated as volatile.
> The volatile write in the constructor _itself_ creates the
> happens-before you're looking for. Making it volatile ensures that
> an AtomicWhatever behaves correctly even if a reference to it is
> shared racily. Besides, an AtomicWhatever is accessed many more
> times than it is constructed, so taking the volatile write out of
> the constructor seems like a dubious optimization.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From lukeisandberg at gmail.com  Fri Jan 23 10:10:34 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 07:10:34 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
Message-ID: <CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>

exactly.  We discussed adding volatile, but then realized it wouldn't fix
it.  We are pretty sure that in practice it doesn't matter (since it
essentially requires a 'malicious' caller)

The only 'state transition' we require in this case is 'fully constructed'
which doesn't seem like a complicated thing to achieve but all the ways i
can think of doing it add complexity and/or overhead.  e.g. we could add a
method like this:

static final class Wrapper<T> {
  volatile T t;
}
static <T> T safelyPublish(T t) {
 Wrapper wrapper = new Wrapper<T> ();
 wrapper.t = t;
 return wrapper.t;
}

Now, i think, that passing any 'unsafely constructed' object through this
helper would create sufficient happens-before edges that this code would be
JMM compliant.

Other ideas we discussed:

1. instead of 'nulling out' the running field, we could assign a special
tombstone value and then cancel() could spin until it reads a non-null
value.  This would require running to be volatile
2. we could wrap the read/write of running in a synchronized block
3. we could use some kind of synchronizer at the end of the constructor
(e.g. countdown a countdownlatch at the end of the constructor and then
call await() at the beginning of cancel())

All of these seem pretty complex/high overhead to solve this (afaik only
theoretical) problem.  Does anyone have other ideas?  How bad would it be
to just rely on our callers not to unsafely publish?  Is there any
precedent for documenting something like this?

On Thu, Jan 22, 2015 at 8:32 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> So I think we've covered this before on this list, but until JMM is
> revised, a volatile wouldn't technically prevent that type of reordering
> here.  However, it appears that most (all?) JVMs treat volatile like final
> in constructor.
>
> However, adding volatile in this case could hurt performance as, unlike
> the AtomicXXX case discussed on the other thread, I'm assuming these
> wrapper futures are constructed a lot.  It's also a waste if nobody
> actually publishes unsafely.
>
> sent from my phone
> On Jan 22, 2015 11:24 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>
>> It does look to me like FallbackFuture.running can be accessed via a data
>> race.  The code seems buggy.
>>
>> Generally, Future implementations have volatile fields and careful state
>> transitions via CAS, as with FutureTask.
>>
>> One is tempted to immediately make running volatile.
>>
>> As for the initial write to running, it will probably (almost?) always be
>> seen in practice, especially on x86, since the future itself is likely to
>> be safely published somehow and there will likely be ordering between the
>> end of the constructor and the write of the reference.  These kinds of
>> races are very difficult to demonstrate in practice.
>>
>> On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <lukeisandberg at gmail.com>
>> wrote:
>>
>>> Guava has this method:
>>>
>>> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>>>
>>> Futures.withFallback
>>>
>>> which is implemented via a delegating future:
>>>
>>> private static class FallbackFuture<V> extends
>>> AbstractFuture.TrustedFuture<V> {
>>>     ListenableFuture<? extends V> input;
>>>
>>>     FallbackFuture(ListenableFuture<? extends V> input,
>>>         final FutureFallback<? extends V> fallback,
>>>         final Executor executor) {
>>>       input = input;
>>>       /// a bunch of stuff with listeners
>>>     }
>>>
>>>   @Override
>>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>>       ListenableFuture<?> local = this.input;
>>>       if (super.cancel(mayInterruptIfRunning)) {
>>>         if (local != null) {
>>>           local.cancel(mayInterruptIfRunning);
>>>         }
>>>         return true;
>>>       }
>>>       return false;
>>>    }
>>> }
>>>
>>> This future does a lot of stuff to handle recovering from failure of the
>>> input future.  But as a general rule in guava, all the Futures.java
>>> utilities try to propagate cancellation.  The question is
>>>
>>> How do we ensure that the initial write to 'input' is visible to
>>> cancel()?
>>>
>>> Because input is non-final, there is no guarantee that it will be
>>> visible if someone unsafely tunnels the FutureFallback to another thread
>>> and calls cancel(). Or are we analyzing the situation incorrectly.
>>>
>>> This pattern is common throughout our guava ListenableFuture utilities
>>> and as far as we can tell it is a latent bug since we aren't 'safely
>>> publishing' our delegating future wrappers
>>>
>>> Thanks
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/01cda46a/attachment.html>

From vitalyd at gmail.com  Fri Jan 23 10:21:55 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 10:21:55 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
Message-ID: <CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>

What about making the field final but instead of storing the future
directly, as now, store it in a wrapper class, so e.g.:

static final class Wrapper<T> {
    ListenableFuture<T> input;
    public Wrapper(ListenableFuture<T> input) { this.input = input; }
}

private static class FallbackFuture<V> extends
AbstractFuture.TrustedFuture<V> {
    final Wrapper<? extends V> wrapper;

    FallbackFuture(ListenableFuture<? extends V> input,
        final FutureFallback<? extends V> fallback,
        final Executor executor) {
      wrapper = new Wrapper(input);
      /// a bunch of stuff with listeners
    }

  @Override
    public boolean cancel(boolean mayInterruptIfRunning) {
      ListenableFuture<?> local = this.wrapper.input;
      if (super.cancel(mayInterruptIfRunning)) {
        if (local != null) {
          local.cancel(mayInterruptIfRunning);
        }
        return true;
      }
      return false;
   }
}

This is somewhat similar to your Wrapper except there's no volatile and
safePublish().

As for "policy" on racy publishing, I think common convention is you only
document if you *allow* racy publication -- default assumption by users
should be that it's not safe.

On Fri, Jan 23, 2015 at 10:10 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> exactly.  We discussed adding volatile, but then realized it wouldn't fix
> it.  We are pretty sure that in practice it doesn't matter (since it
> essentially requires a 'malicious' caller)
>
> The only 'state transition' we require in this case is 'fully constructed'
> which doesn't seem like a complicated thing to achieve but all the ways i
> can think of doing it add complexity and/or overhead.  e.g. we could add a
> method like this:
>
> static final class Wrapper<T> {
>   volatile T t;
> }
> static <T> T safelyPublish(T t) {
>  Wrapper wrapper = new Wrapper<T> ();
>  wrapper.t = t;
>  return wrapper.t;
> }
>
> Now, i think, that passing any 'unsafely constructed' object through this
> helper would create sufficient happens-before edges that this code would be
> JMM compliant.
>
> Other ideas we discussed:
>
> 1. instead of 'nulling out' the running field, we could assign a special
> tombstone value and then cancel() could spin until it reads a non-null
> value.  This would require running to be volatile
> 2. we could wrap the read/write of running in a synchronized block
> 3. we could use some kind of synchronizer at the end of the constructor
> (e.g. countdown a countdownlatch at the end of the constructor and then
> call await() at the beginning of cancel())
>
> All of these seem pretty complex/high overhead to solve this (afaik only
> theoretical) problem.  Does anyone have other ideas?  How bad would it be
> to just rely on our callers not to unsafely publish?  Is there any
> precedent for documenting something like this?
>
> On Thu, Jan 22, 2015 at 8:32 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> So I think we've covered this before on this list, but until JMM is
>> revised, a volatile wouldn't technically prevent that type of reordering
>> here.  However, it appears that most (all?) JVMs treat volatile like final
>> in constructor.
>>
>> However, adding volatile in this case could hurt performance as, unlike
>> the AtomicXXX case discussed on the other thread, I'm assuming these
>> wrapper futures are constructed a lot.  It's also a waste if nobody
>> actually publishes unsafely.
>>
>> sent from my phone
>> On Jan 22, 2015 11:24 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>
>>> It does look to me like FallbackFuture.running can be accessed via a
>>> data race.  The code seems buggy.
>>>
>>> Generally, Future implementations have volatile fields and careful state
>>> transitions via CAS, as with FutureTask.
>>>
>>> One is tempted to immediately make running volatile.
>>>
>>> As for the initial write to running, it will probably (almost?) always
>>> be seen in practice, especially on x86, since the future itself is likely
>>> to be safely published somehow and there will likely be ordering between
>>> the end of the constructor and the write of the reference.  These kinds of
>>> races are very difficult to demonstrate in practice.
>>>
>>> On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <lukeisandberg at gmail.com>
>>> wrote:
>>>
>>>> Guava has this method:
>>>>
>>>> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>>>>
>>>> Futures.withFallback
>>>>
>>>> which is implemented via a delegating future:
>>>>
>>>> private static class FallbackFuture<V> extends
>>>> AbstractFuture.TrustedFuture<V> {
>>>>     ListenableFuture<? extends V> input;
>>>>
>>>>     FallbackFuture(ListenableFuture<? extends V> input,
>>>>         final FutureFallback<? extends V> fallback,
>>>>         final Executor executor) {
>>>>       input = input;
>>>>       /// a bunch of stuff with listeners
>>>>     }
>>>>
>>>>   @Override
>>>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>>>       ListenableFuture<?> local = this.input;
>>>>       if (super.cancel(mayInterruptIfRunning)) {
>>>>         if (local != null) {
>>>>           local.cancel(mayInterruptIfRunning);
>>>>         }
>>>>         return true;
>>>>       }
>>>>       return false;
>>>>    }
>>>> }
>>>>
>>>> This future does a lot of stuff to handle recovering from failure of
>>>> the input future.  But as a general rule in guava, all the Futures.java
>>>> utilities try to propagate cancellation.  The question is
>>>>
>>>> How do we ensure that the initial write to 'input' is visible to
>>>> cancel()?
>>>>
>>>> Because input is non-final, there is no guarantee that it will be
>>>> visible if someone unsafely tunnels the FutureFallback to another thread
>>>> and calls cancel(). Or are we analyzing the situation incorrectly.
>>>>
>>>> This pattern is common throughout our guava ListenableFuture utilities
>>>> and as far as we can tell it is a latent bug since we aren't 'safely
>>>> publishing' our delegating future wrappers
>>>>
>>>> Thanks
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/00b05765/attachment-0001.html>

From lukeisandberg at gmail.com  Fri Jan 23 11:17:05 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 08:17:05 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
Message-ID: <CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>

Thanks, the wrapper was another strategy discussed (and is likely the best
due to simplicity).

Still it seem lame to have to _allocate_ to fix a visibility issue.  It
seems like there should be something similar to the 'freeze action' for
non-final fields:
http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1

Also, (just for my own edification) is it actually possible to observe this
bug on x86?

Finally, It looks like CompletableFuture avoids these issues by not
propagating cancellation and not nulling out input futures (it looks like
everything is in a final field).  If we dropped either of those
requirements, this issue would go away, unfortunately those are both
important features for our users.

On Fri, Jan 23, 2015 at 7:21 AM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> What about making the field final but instead of storing the future
> directly, as now, store it in a wrapper class, so e.g.:
>
> static final class Wrapper<T> {
>     ListenableFuture<T> input;
>     public Wrapper(ListenableFuture<T> input) { this.input = input; }
> }
>
> private static class FallbackFuture<V> extends
> AbstractFuture.TrustedFuture<V> {
>     final Wrapper<? extends V> wrapper;
>
>     FallbackFuture(ListenableFuture<? extends V> input,
>         final FutureFallback<? extends V> fallback,
>         final Executor executor) {
>       wrapper = new Wrapper(input);
>       /// a bunch of stuff with listeners
>     }
>
>   @Override
>     public boolean cancel(boolean mayInterruptIfRunning) {
>       ListenableFuture<?> local = this.wrapper.input;
>       if (super.cancel(mayInterruptIfRunning)) {
>         if (local != null) {
>           local.cancel(mayInterruptIfRunning);
>         }
>         return true;
>       }
>       return false;
>    }
> }
>
> This is somewhat similar to your Wrapper except there's no volatile and
> safePublish().
>
> As for "policy" on racy publishing, I think common convention is you only
> document if you *allow* racy publication -- default assumption by users
> should be that it's not safe.
>
> On Fri, Jan 23, 2015 at 10:10 AM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> exactly.  We discussed adding volatile, but then realized it wouldn't fix
>> it.  We are pretty sure that in practice it doesn't matter (since it
>> essentially requires a 'malicious' caller)
>>
>> The only 'state transition' we require in this case is 'fully
>> constructed' which doesn't seem like a complicated thing to achieve but all
>> the ways i can think of doing it add complexity and/or overhead.  e.g. we
>> could add a method like this:
>>
>> static final class Wrapper<T> {
>>   volatile T t;
>> }
>> static <T> T safelyPublish(T t) {
>>  Wrapper wrapper = new Wrapper<T> ();
>>  wrapper.t = t;
>>  return wrapper.t;
>> }
>>
>> Now, i think, that passing any 'unsafely constructed' object through this
>> helper would create sufficient happens-before edges that this code would be
>> JMM compliant.
>>
>> Other ideas we discussed:
>>
>> 1. instead of 'nulling out' the running field, we could assign a special
>> tombstone value and then cancel() could spin until it reads a non-null
>> value.  This would require running to be volatile
>> 2. we could wrap the read/write of running in a synchronized block
>> 3. we could use some kind of synchronizer at the end of the constructor
>> (e.g. countdown a countdownlatch at the end of the constructor and then
>> call await() at the beginning of cancel())
>>
>> All of these seem pretty complex/high overhead to solve this (afaik only
>> theoretical) problem.  Does anyone have other ideas?  How bad would it be
>> to just rely on our callers not to unsafely publish?  Is there any
>> precedent for documenting something like this?
>>
>> On Thu, Jan 22, 2015 at 8:32 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> So I think we've covered this before on this list, but until JMM is
>>> revised, a volatile wouldn't technically prevent that type of reordering
>>> here.  However, it appears that most (all?) JVMs treat volatile like final
>>> in constructor.
>>>
>>> However, adding volatile in this case could hurt performance as, unlike
>>> the AtomicXXX case discussed on the other thread, I'm assuming these
>>> wrapper futures are constructed a lot.  It's also a waste if nobody
>>> actually publishes unsafely.
>>>
>>> sent from my phone
>>> On Jan 22, 2015 11:24 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>>>
>>>> It does look to me like FallbackFuture.running can be accessed via a
>>>> data race.  The code seems buggy.
>>>>
>>>> Generally, Future implementations have volatile fields and careful
>>>> state transitions via CAS, as with FutureTask.
>>>>
>>>> One is tempted to immediately make running volatile.
>>>>
>>>> As for the initial write to running, it will probably (almost?) always
>>>> be seen in practice, especially on x86, since the future itself is likely
>>>> to be safely published somehow and there will likely be ordering between
>>>> the end of the constructor and the write of the reference.  These kinds of
>>>> races are very difficult to demonstrate in practice.
>>>>
>>>> On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <lukeisandberg at gmail.com
>>>> > wrote:
>>>>
>>>>> Guava has this method:
>>>>>
>>>>> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>>>>>
>>>>> Futures.withFallback
>>>>>
>>>>> which is implemented via a delegating future:
>>>>>
>>>>> private static class FallbackFuture<V> extends
>>>>> AbstractFuture.TrustedFuture<V> {
>>>>>     ListenableFuture<? extends V> input;
>>>>>
>>>>>     FallbackFuture(ListenableFuture<? extends V> input,
>>>>>         final FutureFallback<? extends V> fallback,
>>>>>         final Executor executor) {
>>>>>       input = input;
>>>>>       /// a bunch of stuff with listeners
>>>>>     }
>>>>>
>>>>>   @Override
>>>>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>>>>       ListenableFuture<?> local = this.input;
>>>>>       if (super.cancel(mayInterruptIfRunning)) {
>>>>>         if (local != null) {
>>>>>           local.cancel(mayInterruptIfRunning);
>>>>>         }
>>>>>         return true;
>>>>>       }
>>>>>       return false;
>>>>>    }
>>>>> }
>>>>>
>>>>> This future does a lot of stuff to handle recovering from failure of
>>>>> the input future.  But as a general rule in guava, all the Futures.java
>>>>> utilities try to propagate cancellation.  The question is
>>>>>
>>>>> How do we ensure that the initial write to 'input' is visible to
>>>>> cancel()?
>>>>>
>>>>> Because input is non-final, there is no guarantee that it will be
>>>>> visible if someone unsafely tunnels the FutureFallback to another thread
>>>>> and calls cancel(). Or are we analyzing the situation incorrectly.
>>>>>
>>>>> This pattern is common throughout our guava ListenableFuture utilities
>>>>> and as far as we can tell it is a latent bug since we aren't 'safely
>>>>> publishing' our delegating future wrappers
>>>>>
>>>>> Thanks
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/d1cda8ce/attachment.html>

From vitalyd at gmail.com  Fri Jan 23 11:42:57 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 11:42:57 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
Message-ID: <CAHjP37FhxoGLhVgCJMxe8EQUyCaVO1Tq03EhrFrW22kGvijqkw@mail.gmail.com>

Yes, it's definitely lame to allocate here.  Personally, I'd punt on this
and rely on users publishing safely.  After all, if your users are unsafe
publishing any random class, their app won't work anyway.

x86 does not reorder stores (only store-load combos can appear out of order
due to store buffers) in the instruction stream, but compiler could,
theoretically, reorder the code such that it first writes the
freshly-allocated FallbackFuture to memory and then runs its constructor
(basically, what final field semantics are supposed to prevent).



On Fri, Jan 23, 2015 at 11:17 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Thanks, the wrapper was another strategy discussed (and is likely the best
> due to simplicity).
>
> Still it seem lame to have to _allocate_ to fix a visibility issue.  It
> seems like there should be something similar to the 'freeze action' for
> non-final fields:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1
>
> Also, (just for my own edification) is it actually possible to observe
> this bug on x86?
>
> Finally, It looks like CompletableFuture avoids these issues by not
> propagating cancellation and not nulling out input futures (it looks like
> everything is in a final field).  If we dropped either of those
> requirements, this issue would go away, unfortunately those are both
> important features for our users.
>
> On Fri, Jan 23, 2015 at 7:21 AM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> What about making the field final but instead of storing the future
>> directly, as now, store it in a wrapper class, so e.g.:
>>
>> static final class Wrapper<T> {
>>     ListenableFuture<T> input;
>>     public Wrapper(ListenableFuture<T> input) { this.input = input; }
>> }
>>
>> private static class FallbackFuture<V> extends
>> AbstractFuture.TrustedFuture<V> {
>>     final Wrapper<? extends V> wrapper;
>>
>>     FallbackFuture(ListenableFuture<? extends V> input,
>>         final FutureFallback<? extends V> fallback,
>>         final Executor executor) {
>>       wrapper = new Wrapper(input);
>>       /// a bunch of stuff with listeners
>>     }
>>
>>   @Override
>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>       ListenableFuture<?> local = this.wrapper.input;
>>       if (super.cancel(mayInterruptIfRunning)) {
>>         if (local != null) {
>>           local.cancel(mayInterruptIfRunning);
>>         }
>>         return true;
>>       }
>>       return false;
>>    }
>> }
>>
>> This is somewhat similar to your Wrapper except there's no volatile and
>> safePublish().
>>
>> As for "policy" on racy publishing, I think common convention is you only
>> document if you *allow* racy publication -- default assumption by users
>> should be that it's not safe.
>>
>> On Fri, Jan 23, 2015 at 10:10 AM, Luke Sandberg <lukeisandberg at gmail.com>
>> wrote:
>>
>>> exactly.  We discussed adding volatile, but then realized it wouldn't
>>> fix it.  We are pretty sure that in practice it doesn't matter (since it
>>> essentially requires a 'malicious' caller)
>>>
>>> The only 'state transition' we require in this case is 'fully
>>> constructed' which doesn't seem like a complicated thing to achieve but all
>>> the ways i can think of doing it add complexity and/or overhead.  e.g. we
>>> could add a method like this:
>>>
>>> static final class Wrapper<T> {
>>>   volatile T t;
>>> }
>>> static <T> T safelyPublish(T t) {
>>>  Wrapper wrapper = new Wrapper<T> ();
>>>  wrapper.t = t;
>>>  return wrapper.t;
>>> }
>>>
>>> Now, i think, that passing any 'unsafely constructed' object through
>>> this helper would create sufficient happens-before edges that this code
>>> would be JMM compliant.
>>>
>>> Other ideas we discussed:
>>>
>>> 1. instead of 'nulling out' the running field, we could assign a special
>>> tombstone value and then cancel() could spin until it reads a non-null
>>> value.  This would require running to be volatile
>>> 2. we could wrap the read/write of running in a synchronized block
>>> 3. we could use some kind of synchronizer at the end of the constructor
>>> (e.g. countdown a countdownlatch at the end of the constructor and then
>>> call await() at the beginning of cancel())
>>>
>>> All of these seem pretty complex/high overhead to solve this (afaik only
>>> theoretical) problem.  Does anyone have other ideas?  How bad would it be
>>> to just rely on our callers not to unsafely publish?  Is there any
>>> precedent for documenting something like this?
>>>
>>> On Thu, Jan 22, 2015 at 8:32 PM, Vitaly Davidovich <vitalyd at gmail.com>
>>> wrote:
>>>
>>>> So I think we've covered this before on this list, but until JMM is
>>>> revised, a volatile wouldn't technically prevent that type of reordering
>>>> here.  However, it appears that most (all?) JVMs treat volatile like final
>>>> in constructor.
>>>>
>>>> However, adding volatile in this case could hurt performance as, unlike
>>>> the AtomicXXX case discussed on the other thread, I'm assuming these
>>>> wrapper futures are constructed a lot.  It's also a waste if nobody
>>>> actually publishes unsafely.
>>>>
>>>> sent from my phone
>>>> On Jan 22, 2015 11:24 PM, "Martin Buchholz" <martinrb at google.com>
>>>> wrote:
>>>>
>>>>> It does look to me like FallbackFuture.running can be accessed via a
>>>>> data race.  The code seems buggy.
>>>>>
>>>>> Generally, Future implementations have volatile fields and careful
>>>>> state transitions via CAS, as with FutureTask.
>>>>>
>>>>> One is tempted to immediately make running volatile.
>>>>>
>>>>> As for the initial write to running, it will probably (almost?) always
>>>>> be seen in practice, especially on x86, since the future itself is likely
>>>>> to be safely published somehow and there will likely be ordering between
>>>>> the end of the constructor and the write of the reference.  These kinds of
>>>>> races are very difficult to demonstrate in practice.
>>>>>
>>>>> On Thu, Jan 22, 2015 at 5:02 PM, Luke Sandberg <
>>>>> lukeisandberg at gmail.com> wrote:
>>>>>
>>>>>> Guava has this method:
>>>>>>
>>>>>> https://github.com/google/guava/blob/master/guava/src/com/google/common/util/concurrent/Futures.java#L475
>>>>>>
>>>>>> Futures.withFallback
>>>>>>
>>>>>> which is implemented via a delegating future:
>>>>>>
>>>>>> private static class FallbackFuture<V> extends
>>>>>> AbstractFuture.TrustedFuture<V> {
>>>>>>     ListenableFuture<? extends V> input;
>>>>>>
>>>>>>     FallbackFuture(ListenableFuture<? extends V> input,
>>>>>>         final FutureFallback<? extends V> fallback,
>>>>>>         final Executor executor) {
>>>>>>       input = input;
>>>>>>       /// a bunch of stuff with listeners
>>>>>>     }
>>>>>>
>>>>>>   @Override
>>>>>>     public boolean cancel(boolean mayInterruptIfRunning) {
>>>>>>       ListenableFuture<?> local = this.input;
>>>>>>       if (super.cancel(mayInterruptIfRunning)) {
>>>>>>         if (local != null) {
>>>>>>           local.cancel(mayInterruptIfRunning);
>>>>>>         }
>>>>>>         return true;
>>>>>>       }
>>>>>>       return false;
>>>>>>    }
>>>>>> }
>>>>>>
>>>>>> This future does a lot of stuff to handle recovering from failure of
>>>>>> the input future.  But as a general rule in guava, all the Futures.java
>>>>>> utilities try to propagate cancellation.  The question is
>>>>>>
>>>>>> How do we ensure that the initial write to 'input' is visible to
>>>>>> cancel()?
>>>>>>
>>>>>> Because input is non-final, there is no guarantee that it will be
>>>>>> visible if someone unsafely tunnels the FutureFallback to another thread
>>>>>> and calls cancel(). Or are we analyzing the situation incorrectly.
>>>>>>
>>>>>> This pattern is common throughout our guava ListenableFuture
>>>>>> utilities and as far as we can tell it is a latent bug since we aren't
>>>>>> 'safely publishing' our delegating future wrappers
>>>>>>
>>>>>> Thanks
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/8c0614db/attachment-0001.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 12:36:06 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 20:36:06 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
Message-ID: <54C28686.60306@oracle.com>

On 23.01.2015 18:10, Luke Sandberg wrote:
> The only 'state transition' we require in this case is 'fully
> constructed' which doesn't seem like a complicated thing to achieve but
> all the ways i can think of doing it add complexity and/or overhead.
>  e.g. we could add a method like this:
> 
> static final class Wrapper<T> {
>   volatile T t;
> }
> static <T> T safelyPublish(T t) {
>  Wrapper wrapper = new Wrapper<T> ();
>  wrapper.t = t;
>  return wrapper.t;
> }
> 
> Now, i think, that passing any 'unsafely constructed' object through
> this helper would create sufficient happens-before edges that this code
> would be JMM compliant. 

Erm, JMM compliant how? If you want a happens-before edge between the
write in one thread, and the read in another thread, you have to have
inter-thread synchronizes-with edge. That is, in this case, you have to
write to volatile in publisher thread, and read from volatile in
consumer thread.

Swizzling the value through this magic safelyPublish method does not
introduce any inter-thread edges. If you still have to leak either the
argument or returned t to another thread through the data race, all bets
are off. If you *received* the t from another thread via the data race,
it is again too late to "sanitize" it with safelyPublish.

In other words, once a racy write had happened, there is NO WAY TO
RECOVER. That ship had sailed. JMM-wise, once you have an unordered
write, any read can see it in any happens-before consistent execution
(modulo causality requirements). There are also happens-before
consistent executions where you don't see that racy write. It's in
limbo, it may or may not come.

Spec-wise, the only escape-hatch way to be resilient in the face of
unsafe publication is final. No final -- no guarantees.


> All of these seem pretty complex/high overhead to solve this (afaik only
> theoretical) problem.  Does anyone have other ideas?  How bad would it
> be to just rely on our callers not to unsafely publish?  Is there any
> precedent for documenting something like this?

This is not a theoretical problem. Well, I think the consensus is
educating users that publishing via data race is very wrong, and should
be avoided at all costs. Only a carefully constructed class can survive
unsafe publication, but one should not generally rely on this because
classes are constructed by humans, and humans do mistakes all the time.
Defense in depth here: protect your classes with finals to recover from
accidents, but don't suggest users to unsafely publish them because of that.

Thanks,
-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/bbafa4af/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 12:50:51 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 09:50:51 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C28686.60306@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
Message-ID: <CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>

"A write to a volatile field (?8.3.1.4) happens-before every subsequent
read of that field."

so by pulling the value out of the volatile field we get an HB edge and
program order supplies the rest.  The JMM doesn't require the 'subsequent
read' to be on another thread, it just still works if it it.  I think in
practice this means that no writes will get reordered past this volatile
write.

So that write/read should force all subsequent reads to see all the writes
that happened earlier (notably our constructor field assignments).

On Fri, Jan 23, 2015 at 9:36 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 18:10, Luke Sandberg wrote:
> > The only 'state transition' we require in this case is 'fully
> > constructed' which doesn't seem like a complicated thing to achieve but
> > all the ways i can think of doing it add complexity and/or overhead.
> >  e.g. we could add a method like this:
> >
> > static final class Wrapper<T> {
> >   volatile T t;
> > }
> > static <T> T safelyPublish(T t) {
> >  Wrapper wrapper = new Wrapper<T> ();
> >  wrapper.t = t;
> >  return wrapper.t;
> > }
> >
> > Now, i think, that passing any 'unsafely constructed' object through
> > this helper would create sufficient happens-before edges that this code
> > would be JMM compliant.
>
> Erm, JMM compliant how? If you want a happens-before edge between the
> write in one thread, and the read in another thread, you have to have
> inter-thread synchronizes-with edge. That is, in this case, you have to
> write to volatile in publisher thread, and read from volatile in
> consumer thread.
>
> Swizzling the value through this magic safelyPublish method does not
> introduce any inter-thread edges. If you still have to leak either the
> argument or returned t to another thread through the data race, all bets
> are off. If you *received* the t from another thread via the data race,
> it is again too late to "sanitize" it with safelyPublish.
>
> In other words, once a racy write had happened, there is NO WAY TO
> RECOVER. That ship had sailed. JMM-wise, once you have an unordered
> write, any read can see it in any happens-before consistent execution
> (modulo causality requirements). There are also happens-before
> consistent executions where you don't see that racy write. It's in
> limbo, it may or may not come.
>
> Spec-wise, the only escape-hatch way to be resilient in the face of
> unsafe publication is final. No final -- no guarantees.
>
>
> > All of these seem pretty complex/high overhead to solve this (afaik only
> > theoretical) problem.  Does anyone have other ideas?  How bad would it
> > be to just rely on our callers not to unsafely publish?  Is there any
> > precedent for documenting something like this?
>
> This is not a theoretical problem. Well, I think the consensus is
> educating users that publishing via data race is very wrong, and should
> be avoided at all costs. Only a carefully constructed class can survive
> unsafe publication, but one should not generally rely on this because
> classes are constructed by humans, and humans do mistakes all the time.
> Defense in depth here: protect your classes with finals to recover from
> accidents, but don't suggest users to unsafely publish them because of
> that.
>
> Thanks,
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/b810b012/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 12:59:10 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 20:59:10 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
Message-ID: <54C28BEE.8010201@oracle.com>

On 23.01.2015 20:50, Luke Sandberg wrote:
> "A write to a volatile field (?8.3.1.4) happens-before every subsequent
> read of that field."

...

> The JMM doesn't require the
> 'subsequent read' to be on another thread, it just still works if it
> it.  I think in practice this means that no writes will get reordered
> past this volatile write.

But you have to *read* in another thread, if we are trying to reason
about *publication*. There is no sense in discussing publication when
producer and consumer are the same thread.

> So that write/read should force all subsequent reads to see all the
> writes that happened earlier (notably our constructor field assignments). 

I agree with that part. My challenge is to employ this for inter-thread
communication. In other words, can you take your code and construct the
example with 2 threads?

>     > static final class Wrapper<T> {
>     >   volatile T t;
>     > }
>     > static <T> T safelyPublish(T t) {
>     >  Wrapper wrapper = new Wrapper<T> ();
>     >  wrapper.t = t;
>     >  return wrapper.t;
>     > }

Thanks,
-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/31da8d18/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 13:05:14 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:05:14 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C28BEE.8010201@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
Message-ID: <CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>

Where does the JMM say that the read has to happen on another thread to get
the HB edge?

On Fri, Jan 23, 2015 at 9:59 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 20:50, Luke Sandberg wrote:
> > "A write to a volatile field (?8.3.1.4) happens-before every subsequent
> > read of that field."
>
> ...
>
> > The JMM doesn't require the
> > 'subsequent read' to be on another thread, it just still works if it
> > it.  I think in practice this means that no writes will get reordered
> > past this volatile write.
>
> But you have to *read* in another thread, if we are trying to reason
> about *publication*. There is no sense in discussing publication when
> producer and consumer are the same thread.
>
> > So that write/read should force all subsequent reads to see all the
> > writes that happened earlier (notably our constructor field assignments).
>
> I agree with that part. My challenge is to employ this for inter-thread
> communication. In other words, can you take your code and construct the
> example with 2 threads?
>
> >     > static final class Wrapper<T> {
> >     >   volatile T t;
> >     > }
> >     > static <T> T safelyPublish(T t) {
> >     >  Wrapper wrapper = new Wrapper<T> ();
> >     >  wrapper.t = t;
> >     >  return wrapper.t;
> >     > }
>
> Thanks,
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/7ccc023a/attachment.html>

From dl at cs.oswego.edu  Fri Jan 23 13:08:36 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Jan 2015 13:08:36 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
Message-ID: <54C28E24.8040006@cs.oswego.edu>

On 01/23/2015 11:17 AM, Luke Sandberg wrote:

> Finally, It looks like CompletableFuture avoids these issues by not propagating
> cancellation and not nulling out input futures (it looks like everything is in a
> final field).  If we dropped either of those requirements, this issue would go
> away, unfortunately those are both important features for our users.

CompletableFuture automatically propagates cancellation (and other
exceptions) forward (to dependents), not backwards to sources.
But it is possible to do so using constructions along the lines of:

   void propagateCancel(CompletableFuture<?> f, CompletableFuture<?> source) {
     f.whenComplete((Object r, Throwable ex) ->
       if (f.isCancelled()) source.cancel(true));
   }

(CompletableFuture and other composable futures/promises are
internally very racy, which simplifies things for users but
challenging for implementors.)

-Doug



From aleksey.shipilev at oracle.com  Fri Jan 23 13:11:50 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 21:11:50 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
Message-ID: <54C28EE6.4050303@oracle.com>

On 23.01.2015 21:05, Luke Sandberg wrote:
> Where does the JMM say that the read has to happen on another thread to
> get the HB edge?

I'm sorry, but this is wrong question in this context. I agree the
subsequent reads in the same thread happens-before the preceding writes
in the same thread.

But once again, if we are talking about the publication, then we have to
consider the example when one thread writes (publishes) the object, and
another thread reads (consumes) the object. I invite you to show how
this code helps with safe publication:

>     >     > static final class Wrapper<T> {
>     >     >   volatile T t;
>     >     > }
>     >     > static <T> T safelyPublish(T t) {
>     >     >  Wrapper wrapper = new Wrapper<T> ();
>     >     >  wrapper.t = t;
>     >     >  return wrapper.t;
>     >     > }

That is, who calls safelyPublish -- producer or the consumer? How the
reference to t is communicated between threads? If you want to suggest
this method helps, I would like to see how would a happens-before be
constructed between the creation of T in one thread, and its consumption
in another.

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/5ac8a1b1/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 13:14:13 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:14:13 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C28EE6.4050303@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
Message-ID: <CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>

A simplified example would be
class ForwardingFuture implements Future {
  Future delegate;
  ForwardingFuture(Future delegate) {
    this delegate = checkNotNull(delegate);
  }
...// delegate methods
  boolean cancel(boolean interrupt) {
    return delegate.cancel(interrupt);
  }



static Future unsafe;

T1: unsafe = new ForwardingFuture(someFuture);

T2:
Future local;
while((local =unsafe) == null) {}  // spin
local.cancel(true);


So how do we make sure that T2 can never get an NPE.

If we changed T1 to 'unsafe = safelyPublish(new
ForwardingFuture(someFuture));'

then that would fix it because it it would insert an HB edge between all
the writes to ForwardingFuture.  As would similar tricks involving final
fields.  So the producer would be responsible (In this case I am the
producer and i cannot control consumers).


On Fri, Jan 23, 2015 at 10:11 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 21:05, Luke Sandberg wrote:
> > Where does the JMM say that the read has to happen on another thread to
> > get the HB edge?
>
> I'm sorry, but this is wrong question in this context. I agree the
> subsequent reads in the same thread happens-before the preceding writes
> in the same thread.
>
> But once again, if we are talking about the publication, then we have to
> consider the example when one thread writes (publishes) the object, and
> another thread reads (consumes) the object. I invite you to show how
> this code helps with safe publication:
>
> >     >     > static final class Wrapper<T> {
> >     >     >   volatile T t;
> >     >     > }
> >     >     > static <T> T safelyPublish(T t) {
> >     >     >  Wrapper wrapper = new Wrapper<T> ();
> >     >     >  wrapper.t = t;
> >     >     >  return wrapper.t;
> >     >     > }
>
> That is, who calls safelyPublish -- producer or the consumer? How the
> reference to t is communicated between threads? If you want to suggest
> this method helps, I would like to see how would a happens-before be
> constructed between the creation of T in one thread, and its consumption
> in another.
>
> Thanks,
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/cd708bbf/attachment.html>

From jsampson at guidewire.com  Fri Jan 23 13:20:10 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 18:20:10 +0000
Subject: [concurrency-interest] Resource-based waiting
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD89C3BC@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD89C17D@HQMBX5.eur.ad.sag>
	<CACr06N2ZnG_GeZfOkKtt5ahmQtBije4ROdpBEVomRs8qaUqsjQ@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89C350@HQMBX5.eur.ad.sag>
	<CACr06N0kcEfCXJvEtw+JV51VHs+vm0SMD_ws0nruB+D6b=39Mg@mail.gmail.com>
	<32F15738E8E5524DA4F01A0FA4A8E490FD89C3BC@HQMBX5.eur.ad.sag>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E3E7@sm-ex-01-vm.guidewire.com>

Sebastian Millies wrote:

> thank you. So I misunderstood Semaphore after all :(

Many folks here did as well until a lengthy discussion just last
month (myself included). Doug has made some changes to the docs
hoping to make it clearer, which you can see in the latest revision
in the jsr166 cvs repository:

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/Semaphore.java?revision=1.72&view=markup

If you're interested in the discussion itself, look for the subjects
"Enforcing ordered execution of critical sections?" and "Semaphore
doc bug" in the archives for last month:

http://cs.oswego.edu/pipermail/concurrency-interest/2014-December/thread.html

Cheers,
Justin


From vitalyd at gmail.com  Fri Jan 23 13:20:18 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 13:20:18 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C28EE6.4050303@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
Message-ID: <CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>

He's just trying to emulate final field semantics using volatile store +
load in same thread to prevent compiler reordering.  At any rate, as
mentioned before, it looks like JMM will be revised to specify that
volatile writes in a constructor are treated the same way as final fields,
and current JVMs already do that anyway which means you wouldn't need the
"fake" load of wrapper.t.

On Fri, Jan 23, 2015 at 1:11 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 21:05, Luke Sandberg wrote:
> > Where does the JMM say that the read has to happen on another thread to
> > get the HB edge?
>
> I'm sorry, but this is wrong question in this context. I agree the
> subsequent reads in the same thread happens-before the preceding writes
> in the same thread.
>
> But once again, if we are talking about the publication, then we have to
> consider the example when one thread writes (publishes) the object, and
> another thread reads (consumes) the object. I invite you to show how
> this code helps with safe publication:
>
> >     >     > static final class Wrapper<T> {
> >     >     >   volatile T t;
> >     >     > }
> >     >     > static <T> T safelyPublish(T t) {
> >     >     >  Wrapper wrapper = new Wrapper<T> ();
> >     >     >  wrapper.t = t;
> >     >     >  return wrapper.t;
> >     >     > }
>
> That is, who calls safelyPublish -- producer or the consumer? How the
> reference to t is communicated between threads? If you want to suggest
> this method helps, I would like to see how would a happens-before be
> constructed between the creation of T in one thread, and its consumption
> in another.
>
> Thanks,
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/b34a9e4e/attachment.html>

From martinrb at google.com  Fri Jan 23 13:26:45 2015
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 23 Jan 2015 10:26:45 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
Message-ID: <CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>

I think making delegate volatile would fix the problem in practice if not
in theory.
To fix it in theory, do:

class ForwardingFuture implements Future {
  final AtomicReference<Future> delegate;
  ForwardingFuture(Future delegate) {
    this delegate = new AtomicReference<Future>(checkNotNull(delegate));
  }
...// delegate methods
  boolean cancel(boolean interrupt) {
    return delegate.get().cancel(interrupt);
  }

If you look deeper, at the implementation of AtomicReference itself, it
simply has a volatile field set in the constructor, and there are zero
complaints about AtomicReference.get() returning uninitialized null, so in
practice, with all JVMs, simply making delegate a volatile Future is
sufficient.



On Fri, Jan 23, 2015 at 10:14 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> A simplified example would be
> class ForwardingFuture implements Future {
>   Future delegate;
>   ForwardingFuture(Future delegate) {
>     this delegate = checkNotNull(delegate);
>   }
> ...// delegate methods
>   boolean cancel(boolean interrupt) {
>     return delegate.cancel(interrupt);
>   }
>
>
>
> static Future unsafe;
>
> T1: unsafe = new ForwardingFuture(someFuture);
>
> T2:
> Future local;
> while((local =unsafe) == null) {}  // spin
> local.cancel(true);
>
>
> So how do we make sure that T2 can never get an NPE.
>
> If we changed T1 to 'unsafe = safelyPublish(new
> ForwardingFuture(someFuture));'
>
> then that would fix it because it it would insert an HB edge between all
> the writes to ForwardingFuture.  As would similar tricks involving final
> fields.  So the producer would be responsible (In this case I am the
> producer and i cannot control consumers).
>
>
> On Fri, Jan 23, 2015 at 10:11 AM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 23.01.2015 21:05, Luke Sandberg wrote:
>> > Where does the JMM say that the read has to happen on another thread to
>> > get the HB edge?
>>
>> I'm sorry, but this is wrong question in this context. I agree the
>> subsequent reads in the same thread happens-before the preceding writes
>> in the same thread.
>>
>> But once again, if we are talking about the publication, then we have to
>> consider the example when one thread writes (publishes) the object, and
>> another thread reads (consumes) the object. I invite you to show how
>> this code helps with safe publication:
>>
>> >     >     > static final class Wrapper<T> {
>> >     >     >   volatile T t;
>> >     >     > }
>> >     >     > static <T> T safelyPublish(T t) {
>> >     >     >  Wrapper wrapper = new Wrapper<T> ();
>> >     >     >  wrapper.t = t;
>> >     >     >  return wrapper.t;
>> >     >     > }
>>
>> That is, who calls safelyPublish -- producer or the consumer? How the
>> reference to t is communicated between threads? If you want to suggest
>> this method helps, I would like to see how would a happens-before be
>> constructed between the creation of T in one thread, and its consumption
>> in another.
>>
>> Thanks,
>> -Aleksey.
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/e3137056/attachment-0001.html>

From lukeisandberg at gmail.com  Fri Jan 23 13:26:34 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:26:34 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C28E24.8040006@cs.oswego.edu>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C28E24.8040006@cs.oswego.edu>
Message-ID: <CAO9V1M+kaQRXKfsj55CMbtCWf_xmyc_qP5j0cgJ_C43i-8xKcA@mail.gmail.com>

Would you suggest just switching all these fields to 'volatile' now, since
while it isn't correct according to the JMM, it is with the current
implementations and will be in the JMM in the future?

On Fri, Jan 23, 2015 at 10:08 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/23/2015 11:17 AM, Luke Sandberg wrote:
>
>  Finally, It looks like CompletableFuture avoids these issues by not
>> propagating
>> cancellation and not nulling out input futures (it looks like everything
>> is in a
>> final field).  If we dropped either of those requirements, this issue
>> would go
>> away, unfortunately those are both important features for our users.
>>
>
> CompletableFuture automatically propagates cancellation (and other
> exceptions) forward (to dependents), not backwards to sources.
> But it is possible to do so using constructions along the lines of:
>
>   void propagateCancel(CompletableFuture<?> f, CompletableFuture<?>
> source) {
>     f.whenComplete((Object r, Throwable ex) ->
>       if (f.isCancelled()) source.cancel(true));
>   }
>
> (CompletableFuture and other composable futures/promises are
> internally very racy, which simplifies things for users but
> challenging for implementors.)
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/416b36c1/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 13:26:51 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 21:26:51 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
Message-ID: <54C2926B.9030409@oracle.com>

On 23.01.2015 21:14, Luke Sandberg wrote:
> A simplified example would be
> class ForwardingFuture implements Future {
>   Future delegate;
>   ForwardingFuture(Future delegate) {
>     this delegate = checkNotNull(delegate);
>   }
> ...// delegate methods
>   boolean cancel(boolean interrupt) {
>     return delegate.cancel(interrupt);
>   }
> 
> 
> 
> static Future unsafe;
> 
> T1: unsafe = new ForwardingFuture(someFuture);
> 
> T2: 
> Future local;
> while((local =unsafe) == null) {}  // spin
> local.cancel(true);
> 
> 
> So how do we make sure that T2 can never get an NPE.  
> 
> If we changed T1 to 'unsafe = safelyPublish(new
> ForwardingFuture(someFuture));'
> 
> then that would fix it because it it would insert an HB edge between all
> the writes to ForwardingFuture.  

No, that wouldn't fix it. There is no happens-before between the read of
$unsafe in T2, and the write of $unsafe in T1, sorry. It is a race, T2
is not guaranteed to see the $unsafe contents properly, even if it
succeeds in busy-waiting for non-null $unsafe.

To reiterate: you need a HB between *the write* and *the read*.

-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/e54ae4a3/attachment.bin>

From martinrb at google.com  Fri Jan 23 13:28:34 2015
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 23 Jan 2015 10:28:34 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
Message-ID: <CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>

A good summary of "happens-before" is the
Memory Consistency Properties

section of
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html


"""A write to a volatile field *happens-before* every subsequent read of
that same field."""
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/7203c214/attachment.html>

From lukeisandberg at gmail.com  Fri Jan 23 13:34:29 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:34:29 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
Message-ID: <CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>

>
> No, that wouldn't fix it. There is no happens-before between the read of
> $unsafe in T2, and the write of $unsafe in T1, sorry. It is a race, T2
> is not guaranteed to see the $unsafe contents properly, even if it
> succeeds in busy-waiting for non-null $unsafe.
> To reiterate: you need a HB between *the write* and *the read*.


I don't think so, i already have a read from a volatile.  Anything that
anyone does after that (safe or unsafe) with the reference is guaranteed to
see a fully constructed instance.  If they were able to see a partially
constructed instance, then program order would be violated.

On Fri, Jan 23, 2015 at 10:28 AM, Martin Buchholz <martinrb at google.com>
wrote:

> A good summary of "happens-before" is the
> Memory Consistency Properties
>
> section of
> http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html
>
>
> """A write to a volatile field *happens-before* every subsequent read of
> that same field."""
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/290c7aa9/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 13:35:40 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 21:35:40 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<54C28EE6.4050303@oracle.com>
	<CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>
Message-ID: <54C2947C.6090205@oracle.com>

On 23.01.2015 21:20, Vitaly Davidovich wrote:
> He's just trying to emulate final field semantics using volatile store +
> load in same thread to prevent compiler reordering.  At any rate, as
> mentioned before, it looks like JMM will be revised to specify that
> volatile writes in a constructor are treated the same way as final
> fields, and current JVMs already do that anyway which means you wouldn't
> need the "fake" load of wrapper.t.

Well, could you please stop (trying to) cheat with concurrent things
when your libraries are going to be used in my banking and medical
software? I will appreciate that. The language has a set of rules,
follow them, please, use finals and don't let friends piggyback on data
races.

Thank you,
-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/c4005d1c/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 13:43:12 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:43:12 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2947C.6090205@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>
	<54C2947C.6090205@oracle.com>
Message-ID: <CAO9V1MLoy7DnTbGYN=3stKRRiPoZD=hBqoY0FnO06UmfUzJn9g@mail.gmail.com>

I don't think it is 'cheating' per-se since the JMM guarantees its
behavior.  It is just confusing.

Also, as discussed above, I can't make these fields final, so i would have
to do something like this (which has the benefit of creating a very shortly
lived object which can hopefully be eliminated by escape analysis) or
something like what Vitaly said (store wrappers of non final references in
final fields).

On Fri, Jan 23, 2015 at 10:35 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 21:20, Vitaly Davidovich wrote:
> > He's just trying to emulate final field semantics using volatile store +
> > load in same thread to prevent compiler reordering.  At any rate, as
> > mentioned before, it looks like JMM will be revised to specify that
> > volatile writes in a constructor are treated the same way as final
> > fields, and current JVMs already do that anyway which means you wouldn't
> > need the "fake" load of wrapper.t.
>
> Well, could you please stop (trying to) cheat with concurrent things
> when your libraries are going to be used in my banking and medical
> software? I will appreciate that. The language has a set of rules,
> follow them, please, use finals and don't let friends piggyback on data
> races.
>
> Thank you,
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/6e666fe6/attachment-0001.html>

From jsampson at guidewire.com  Fri Jan 23 13:43:39 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 18:43:39 +0000
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E417@sm-ex-01-vm.guidewire.com>

Luke Sandberg wrote:

> Where does the JMM say that the read has to happen on another
> thread to get the HB edge?

Well, here's the thing: The JMM already says that _every_ action in
one thread happens-before _every_ later action in that same thread,
_without_ any volatiles or finals or whatever. Adding more volatile
reads and writes within a single thread doesn't make a difference!
The compiler and JVM are even allowed by the JMM (though perhaps
none do it) to downgrade a volatile field to non-volatile if they
can prove that it's only ever accessed in a single thread.

So what the other folks here are getting at is that you have to
actually have a volatile write in one thread and a volatile read in
a _different_ thread to create a happens-before link _between_ those
threads (since they already have all the happens-before edges you
could want _within_ each thread). And simply initializing a volatile
field in a constructor isn't good enough for that, if the containing
object itself is published by a data race, because then the volatile
read in the other thread could just as well precede the volatile
write in the constructor as follow it.

I find it _very_ unintuitive that volatile writes in a constructor
are _weaker_ than final writes in a constructor, which caught me in
an error in another discussion here this week. I was just about to
ask whether we could get the JMM changed to make volatile writes as
strong as final writes in a constructor, so I'm excited to see in
another message that that's already planned to happen. :)

Cheers,
Justin


From vitalyd at gmail.com  Fri Jan 23 13:44:11 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 13:44:11 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2947C.6090205@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>
	<54C2947C.6090205@oracle.com>
Message-ID: <CAHjP37F8CXwmMRykVwy0gGMzYNFdeRfX3Ttsrg3FBXmhWb3PRw@mail.gmail.com>

This is why, I believe, (revised) JMM is going to give volatile same
treatment as final.

Doug, since you're here, please confirm (or deny!) this.  But this has been
discussed in the context of SAP ppc port, IIRC, and all known JVMs already
do that.

sent from my phone
On Jan 23, 2015 1:35 PM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> On 23.01.2015 21:20, Vitaly Davidovich wrote:
> > He's just trying to emulate final field semantics using volatile store +
> > load in same thread to prevent compiler reordering.  At any rate, as
> > mentioned before, it looks like JMM will be revised to specify that
> > volatile writes in a constructor are treated the same way as final
> > fields, and current JVMs already do that anyway which means you wouldn't
> > need the "fake" load of wrapper.t.
>
> Well, could you please stop (trying to) cheat with concurrent things
> when your libraries are going to be used in my banking and medical
> software? I will appreciate that. The language has a set of rules,
> follow them, please, use finals and don't let friends piggyback on data
> races.
>
> Thank you,
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/3d3eb102/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 13:47:50 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 21:47:50 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
Message-ID: <54C29756.3090207@oracle.com>

On 23.01.2015 21:34, Luke Sandberg wrote:
>     No, that wouldn't fix it. There is no happens-before between the read of
>     $unsafe in T2, and the write of $unsafe in T1, sorry. It is a race, T2
>     is not guaranteed to see the $unsafe contents properly, even if it
>     succeeds in busy-waiting for non-null $unsafe.
>     To reiterate: you need a HB between *the write* and *the read*.
> 
> 
> I don't think so, i already have a read from a volatile.  Anything that
> anyone does after that (safe or unsafe) with the reference is guaranteed
> to see a fully constructed instance.  If they were able to see a
> partially constructed instance, then program order would be violated.

I think this claim is based on misinterpretation of the Java Memory
Model. Here, take this:
  http://shipilev.net/blog/2014/jmm-pragmatics/

The racy read can see whatever write unordered by happens-before
(subject to causality requirements). The volatile store/load on the
publisher side does not imply the ordering that considers the actions in
the consumer thread. There is no happens-before between the write of
$unsafe in T1, and the read of $unsafe in T2. And you *have* to have
*that* happens-before to claim the publication is working properly.

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/ddc31bc3/attachment.bin>

From aleksey.shipilev at oracle.com  Fri Jan 23 13:50:22 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 21:50:22 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
Message-ID: <54C297EE.5050101@oracle.com>

On 23.01.2015 19:17, Luke Sandberg wrote:
> Thanks, the wrapper was another strategy discussed (and is likely the
> best due to simplicity). 

Final wrapper is a cute trick, but I think it only works if you have
constructed the input in this very thread, or get the input from
somewhere else safely.

I.e. this is still broken:

static final class Wrapper<T> {
    ListenableFuture<T> input;
    public Wrapper(ListenableFuture<T> input) { this.input = input; }
}

private static class FallbackFuture<V>
    final Wrapper<? extends V> wrapper;

    FallbackFuture(ListenableFuture<? extends V> input, ...) {
      wrapper = new Wrapper(input);
      /// a bunch of stuff with listeners
    }
    ...
}


Future f;

Thread 1:
  Future lf1 = <get a future from somewhere>
  f = lf1; // racy write

Thread 2:
  Future lf2 = f; // racy read
  Future ff = new FallbackFuture(lf2); // oops, too late!

Even if lf2 is not null, you can't possibly instruct the writer that you
need all the (initializing) stores for lf1 from Thread 1 right now,
here, in Thread 2.

Because of this, the subsequent (unsafe) publishing of ff with final
wrapper on-board would not also be protected, since we are have been
given the rotten $input.


> Still it seem lame to have to _allocate_ to fix a visibility issue.  It
> seems like there should be something similar to the 'freeze action' for
> non-final
> fields: http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1

Will only work for not-yet-published objects. And if you have to only
deal with non-published objects, there are only a few corner cases that
do not fit into object constructors.


> Also, (just for my own edification) is it actually possible to observe
> this bug on x86?

Yes, if you have enough field stores to spill over the publication of
the object. See also:
 http://shipilev.net/blog/2014/safe-public-construction/

-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/9c8eacd6/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 13:51:08 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 10:51:08 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C29756.3090207@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
Message-ID: <CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>

>
> The racy read can see whatever write unordered by happens-before
> (subject to causality requirements).

doesn't the safelyPublish method order all the earlier writes?

On Fri, Jan 23, 2015 at 10:47 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 21:34, Luke Sandberg wrote:
> >     No, that wouldn't fix it. There is no happens-before between the
> read of
> >     $unsafe in T2, and the write of $unsafe in T1, sorry. It is a race,
> T2
> >     is not guaranteed to see the $unsafe contents properly, even if it
> >     succeeds in busy-waiting for non-null $unsafe.
> >     To reiterate: you need a HB between *the write* and *the read*.
> >
> >
> > I don't think so, i already have a read from a volatile.  Anything that
> > anyone does after that (safe or unsafe) with the reference is guaranteed
> > to see a fully constructed instance.  If they were able to see a
> > partially constructed instance, then program order would be violated.
>
> I think this claim is based on misinterpretation of the Java Memory
> Model. Here, take this:
>   http://shipilev.net/blog/2014/jmm-pragmatics/
>
> The racy read can see whatever write unordered by happens-before
> (subject to causality requirements). The volatile store/load on the
> publisher side does not imply the ordering that considers the actions in
> the consumer thread. There is no happens-before between the write of
> $unsafe in T1, and the read of $unsafe in T2. And you *have* to have
> *that* happens-before to claim the publication is working properly.
>
> Thanks,
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/958c22f7/attachment.html>

From vitalyd at gmail.com  Fri Jan 23 14:03:31 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 14:03:31 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
Message-ID: <CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>

Under strict interpretation of current JMM, it does not prevent the
publishing of the reference from moving before wrapper.t = t; because
volatile stores do not order subsequent stores.  The "fake" volatile load
of wrapper.t, which you're trying to use to prevent that reordering,
creates no happens-before edge because it's done intra-thread, and I think
the JMM abstract model is referring to cross thread edges.

Now, that's all abstract model stuff -- in practice, no compiler can
actually see that a volatile write by T1 is then observed by a volatile
read on T2, and thus they play conservative and issue fences (compiler +
required CPU) right after a volatile write, and do not go looking for any
"happens-before" edges.



On Fri, Jan 23, 2015 at 1:51 PM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> The racy read can see whatever write unordered by happens-before
>> (subject to causality requirements).
>
> doesn't the safelyPublish method order all the earlier writes?
>
> On Fri, Jan 23, 2015 at 10:47 AM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 23.01.2015 21:34, Luke Sandberg wrote:
>> >     No, that wouldn't fix it. There is no happens-before between the
>> read of
>> >     $unsafe in T2, and the write of $unsafe in T1, sorry. It is a race,
>> T2
>> >     is not guaranteed to see the $unsafe contents properly, even if it
>> >     succeeds in busy-waiting for non-null $unsafe.
>> >     To reiterate: you need a HB between *the write* and *the read*.
>> >
>> >
>> > I don't think so, i already have a read from a volatile.  Anything that
>> > anyone does after that (safe or unsafe) with the reference is guaranteed
>> > to see a fully constructed instance.  If they were able to see a
>> > partially constructed instance, then program order would be violated.
>>
>> I think this claim is based on misinterpretation of the Java Memory
>> Model. Here, take this:
>>   http://shipilev.net/blog/2014/jmm-pragmatics/
>>
>> The racy read can see whatever write unordered by happens-before
>> (subject to causality requirements). The volatile store/load on the
>> publisher side does not imply the ordering that considers the actions in
>> the consumer thread. There is no happens-before between the write of
>> $unsafe in T1, and the read of $unsafe in T2. And you *have* to have
>> *that* happens-before to claim the publication is working properly.
>>
>> Thanks,
>> -Aleksey.
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/d7a31586/attachment-0001.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 14:04:11 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 22:04:11 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
Message-ID: <54C29B2B.40605@oracle.com>

On 23.01.2015 21:51, Luke Sandberg wrote:
>     The racy read can see whatever write unordered by happens-before
>     (subject to causality requirements).
> 
> doesn't the safelyPublish method order all the earlier writes?  

The problem is with the definition of "earlier". Happens-before order
defines what's "earlier" = defines what we should see as something
already happened.

The read of $unsafe in T2 is not ordered with the write of $unsafe T2,
because there is no happens-before. Therefore, as far as that read is
concerned, it does not have to observe that write. Because, in your
terms, T2 write of $unsafe is not "earlier", as far as T1 read of
$unsafe is concerned.

-Aleksey.

P.S. Seriously, go read JMM Pragmatics:
   http://shipilev.net/blog/2014/jmm-pragmatics/
 You will regret it, but it's going to worth it.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/59209149/attachment.bin>

From lukeisandberg at gmail.com  Fri Jan 23 14:06:28 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 11:06:28 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C29B2B.40605@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<54C29B2B.40605@oracle.com>
Message-ID: <CAO9V1MKNEiXdQe4xJX5Dcm3tKTJfgUtGZ-kFUxOEhLFzFyQ_nA@mail.gmail.com>

I think causality would prevent the write to unsafe from moving earlier.
 since we write wrapper.t to unsafe, not the original reference.

On Fri, Jan 23, 2015 at 11:04 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 21:51, Luke Sandberg wrote:
> >     The racy read can see whatever write unordered by happens-before
> >     (subject to causality requirements).
> >
> > doesn't the safelyPublish method order all the earlier writes?
>
> The problem is with the definition of "earlier". Happens-before order
> defines what's "earlier" = defines what we should see as something
> already happened.
>
> The read of $unsafe in T2 is not ordered with the write of $unsafe T2,
> because there is no happens-before. Therefore, as far as that read is
> concerned, it does not have to observe that write. Because, in your
> terms, T2 write of $unsafe is not "earlier", as far as T1 read of
> $unsafe is concerned.
>
> -Aleksey.
>
> P.S. Seriously, go read JMM Pragmatics:
>    http://shipilev.net/blog/2014/jmm-pragmatics/
>  You will regret it, but it's going to worth it.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/097d4ca6/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 14:13:32 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 22:13:32 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<54C28EE6.4050303@oracle.com>	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>	<54C29756.3090207@oracle.com>	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
Message-ID: <54C29D5C.6050703@oracle.com>

On 23.01.2015 22:03, Vitaly Davidovich wrote:
> Under strict interpretation of current JMM, it does not prevent the
> publishing of the reference from moving before wrapper.t = t; because
> volatile stores do not order subsequent stores.  The "fake" volatile
> load of wrapper.t, which you're trying to use to prevent that
> reordering, creates no happens-before edge because it's done
> intra-thread, and I think the JMM abstract model is referring to cross
> thread edges.
> 
> Now, that's all abstract model stuff -- in practice, no compiler can
> actually see that a volatile write by T1 is then observed by a volatile
> read on T2, and thus they play conservative and issue fences (compiler +
> required CPU) right after a volatile write, and do not go looking for
> any "happens-before" edges.

Well, I don't like to play the Compiler Card, but thread-local objects
are funny in this particular regard: we can make local analysis, since
we know the object is accessed by a single thread only. Coming back to
this code:

 static final class Wrapper<T> {
    volatile T t;
 }

 static <T> T safelyPublish(T t) {
    Wrapper wrapper = new Wrapper<T> ();
    wrapper.t = t;
    return wrapper.t;
 }

...even a moderately smart compiler can figure out the "volatile T t" is
only accessed by a single thread, and therefore the memory effects of
volatile load/store would not be visible to any other thread. Remember,
spec only cares enough about the actions on the same variable from
different threads. That allows the compiler to optimize safelyPublish() to:

 static <T> T safelyPublish(T t) {
    return t;
 }

Ooops. I would guess that happens in HotSpot now...

Thanks,
-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/3b3c296b/attachment.bin>

From vitalyd at gmail.com  Fri Jan 23 14:16:13 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 14:16:13 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C29D5C.6050703@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
	<54C29D5C.6050703@oracle.com>
Message-ID: <CAHjP37GfO4+qKJC4QTdTSNYDQ3J0YCSV2O9mYy5uCryr2SjPmQ@mail.gmail.com>

What compiler figures this out and how? What about reflection? What about
any future-loaded classes that may peek in there (directly or reflection)?
The current compiler doesn't even trust instance final fields! Please show
me an existing compiler that does the transformation you're talking about.

On Fri, Jan 23, 2015 at 2:13 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 22:03, Vitaly Davidovich wrote:
> > Under strict interpretation of current JMM, it does not prevent the
> > publishing of the reference from moving before wrapper.t = t; because
> > volatile stores do not order subsequent stores.  The "fake" volatile
> > load of wrapper.t, which you're trying to use to prevent that
> > reordering, creates no happens-before edge because it's done
> > intra-thread, and I think the JMM abstract model is referring to cross
> > thread edges.
> >
> > Now, that's all abstract model stuff -- in practice, no compiler can
> > actually see that a volatile write by T1 is then observed by a volatile
> > read on T2, and thus they play conservative and issue fences (compiler +
> > required CPU) right after a volatile write, and do not go looking for
> > any "happens-before" edges.
>
> Well, I don't like to play the Compiler Card, but thread-local objects
> are funny in this particular regard: we can make local analysis, since
> we know the object is accessed by a single thread only. Coming back to
> this code:
>
>  static final class Wrapper<T> {
>     volatile T t;
>  }
>
>  static <T> T safelyPublish(T t) {
>     Wrapper wrapper = new Wrapper<T> ();
>     wrapper.t = t;
>     return wrapper.t;
>  }
>
> ...even a moderately smart compiler can figure out the "volatile T t" is
> only accessed by a single thread, and therefore the memory effects of
> volatile load/store would not be visible to any other thread. Remember,
> spec only cares enough about the actions on the same variable from
> different threads. That allows the compiler to optimize safelyPublish() to:
>
>  static <T> T safelyPublish(T t) {
>     return t;
>  }
>
> Ooops. I would guess that happens in HotSpot now...
>
> Thanks,
> -Aleksey.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/37f740e8/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 14:17:27 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 22:17:27 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MKNEiXdQe4xJX5Dcm3tKTJfgUtGZ-kFUxOEhLFzFyQ_nA@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<54C29B2B.40605@oracle.com>
	<CAO9V1MKNEiXdQe4xJX5Dcm3tKTJfgUtGZ-kFUxOEhLFzFyQ_nA@mail.gmail.com>
Message-ID: <54C29E47.3000402@oracle.com>

On 23.01.2015 22:06, Luke Sandberg wrote:
> I think causality would prevent the write to unsafe from moving earlier.
>  since we write wrapper.t to unsafe, not the original reference.

I don't think it would, but you can prove me wrong. If what you are
saying was indeed true, then the unsafe publication would not be a
problem to begin with, since this would also be prohibited:

class C {
  int x;
  C(int x) { this.x = x; }
}

C c;

Thread 1:
   c = new C(42);

Thread 2:
   C lc = c;
   println(lc.x); // prints 0 -- prohibited?

-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/a45635ee/attachment-0001.bin>

From vitalyd at gmail.com  Fri Jan 23 14:19:21 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 14:19:21 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37GfO4+qKJC4QTdTSNYDQ3J0YCSV2O9mYy5uCryr2SjPmQ@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
	<54C29D5C.6050703@oracle.com>
	<CAHjP37GfO4+qKJC4QTdTSNYDQ3J0YCSV2O9mYy5uCryr2SjPmQ@mail.gmail.com>
Message-ID: <CAHjP37EA5i3wTNd6R8zR=v=SOi4FvXJutrp1YAWFEdD1fqfp=A@mail.gmail.com>

Oh sorry, you're talking about *thread-local* objects.

On Fri, Jan 23, 2015 at 2:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> What compiler figures this out and how? What about reflection? What about
> any future-loaded classes that may peek in there (directly or reflection)?
> The current compiler doesn't even trust instance final fields! Please show
> me an existing compiler that does the transformation you're talking about.
>
> On Fri, Jan 23, 2015 at 2:13 PM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 23.01.2015 22:03, Vitaly Davidovich wrote:
>> > Under strict interpretation of current JMM, it does not prevent the
>> > publishing of the reference from moving before wrapper.t = t; because
>> > volatile stores do not order subsequent stores.  The "fake" volatile
>> > load of wrapper.t, which you're trying to use to prevent that
>> > reordering, creates no happens-before edge because it's done
>> > intra-thread, and I think the JMM abstract model is referring to cross
>> > thread edges.
>> >
>> > Now, that's all abstract model stuff -- in practice, no compiler can
>> > actually see that a volatile write by T1 is then observed by a volatile
>> > read on T2, and thus they play conservative and issue fences (compiler +
>> > required CPU) right after a volatile write, and do not go looking for
>> > any "happens-before" edges.
>>
>> Well, I don't like to play the Compiler Card, but thread-local objects
>> are funny in this particular regard: we can make local analysis, since
>> we know the object is accessed by a single thread only. Coming back to
>> this code:
>>
>>  static final class Wrapper<T> {
>>     volatile T t;
>>  }
>>
>>  static <T> T safelyPublish(T t) {
>>     Wrapper wrapper = new Wrapper<T> ();
>>     wrapper.t = t;
>>     return wrapper.t;
>>  }
>>
>> ...even a moderately smart compiler can figure out the "volatile T t" is
>> only accessed by a single thread, and therefore the memory effects of
>> volatile load/store would not be visible to any other thread. Remember,
>> spec only cares enough about the actions on the same variable from
>> different threads. That allows the compiler to optimize safelyPublish()
>> to:
>>
>>  static <T> T safelyPublish(T t) {
>>     return t;
>>  }
>>
>> Ooops. I would guess that happens in HotSpot now...
>>
>> Thanks,
>> -Aleksey.
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/0acc96a4/attachment.html>

From dl at cs.oswego.edu  Fri Jan 23 14:21:46 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Jan 2015 14:21:46 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1M+kaQRXKfsj55CMbtCWf_xmyc_qP5j0cgJ_C43i-8xKcA@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C28E24.8040006@cs.oswego.edu>
	<CAO9V1M+kaQRXKfsj55CMbtCWf_xmyc_qP5j0cgJ_C43i-8xKcA@mail.gmail.com>
Message-ID: <54C29F4A.7020007@cs.oswego.edu>

On 01/23/2015 01:26 PM, Luke Sandberg wrote:
> Would you suggest just switching all these fields to 'volatile' now, since while
> it isn't correct according to the JMM, it is with the current implementations
> and will be in the JMM in the future?

I was trying to evade answering that question by showing an
approach that doesn't require an answer to it :-)

But yes. The lack of current requirements for final-like
semantics for volatile writes in constructors leaves some
people without decent official solutions in situations like
this, so they end up relying on actual JVM behavior.

Where many "situations like this" are library components
that users "safely publish into". As in, safely publishing
by the act of adding to a ConcurrentLinkedQueue or
ConcurrentHashMap, which is a natural thing to want.
So we explicitly promise to do this in the "Memory
consistency effects" clauses of specs.
But making good on them sometimes requires relying on
unspecified JVM properties, that we hope will someday
be fixed in JMM revisions.

-Doug

>
> On Fri, Jan 23, 2015 at 10:08 AM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     On 01/23/2015 11:17 AM, Luke Sandberg wrote:
>
>         Finally, It looks like CompletableFuture avoids these issues by not
>         propagating
>         cancellation and not nulling out input futures (it looks like everything
>         is in a
>         final field).  If we dropped either of those requirements, this issue
>         would go
>         away, unfortunately those are both important features for our users.
>
>
>     CompletableFuture automatically propagates cancellation (and other
>     exceptions) forward (to dependents), not backwards to sources.
>     But it is possible to do so using constructions along the lines of:
>
>        void propagateCancel(__CompletableFuture<?> f, CompletableFuture<?> source) {
>          f.whenComplete((Object r, Throwable ex) ->
>            if (f.isCancelled()) source.cancel(true));
>        }
>
>     (CompletableFuture and other composable futures/promises are
>     internally very racy, which simplifies things for users but
>     challenging for implementors.)
>
>     -Doug
>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From jsampson at guidewire.com  Fri Jan 23 14:22:17 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 19:22:17 +0000
Subject: [concurrency-interest] Safe publishing strategy
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com> 
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>

Luke Sandberg wrote:

> I think causality would prevent the write to unsafe from moving
> earlier. since we write wrapper.t to unsafe, not the original
> reference.

It sounds like you're mixing up causality and happens-before a bit.
Causality doesn't come into play here because there aren't any
conditional branches. The compiler knows that every line is going to
execute, so it's free to execute them out of order.

Cheers,
Justin


From aleksey.shipilev at oracle.com  Fri Jan 23 14:27:01 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 22:27:01 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
Message-ID: <54C2A085.5090808@oracle.com>

On 23.01.2015 22:22, Justin Sampson wrote:
> Luke Sandberg wrote:
>> I think causality would prevent the write to unsafe from moving
>> earlier. since we write wrapper.t to unsafe, not the original
>> reference.
> 
> It sounds like you're mixing up causality and happens-before a bit.
> Causality doesn't come into play here because there aren't any
> conditional branches. The compiler knows that every line is going to
> execute, so it's free to execute them out of order.

I think it's dangerous to mix causality and particular program flow.

I can do better: as far as my puny brain can wrap around, causality
requirements in JMM try to prevent causality loops, *NOT* provide the
global time. Although providing global time would probably solve
causality problems once and for all, but that is contrary to the spirit
of weak memory model that should allow code transformations.

That is, under JMM's causality requirements, two threads can still
observe different "sequence" of actions with regards to a particular
object. T1 may firmly believe it stores the object reference after all
the stores into it are committed. T2 may at the same time believe the
object reference was stored, but none of it's field stored yet.

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/4c831271/attachment.bin>

From vitalyd at gmail.com  Fri Jan 23 14:39:15 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 14:39:15 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37EA5i3wTNd6R8zR=v=SOi4FvXJutrp1YAWFEdD1fqfp=A@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
	<54C29D5C.6050703@oracle.com>
	<CAHjP37GfO4+qKJC4QTdTSNYDQ3J0YCSV2O9mYy5uCryr2SjPmQ@mail.gmail.com>
	<CAHjP37EA5i3wTNd6R8zR=v=SOi4FvXJutrp1YAWFEdD1fqfp=A@mail.gmail.com>
Message-ID: <CAHjP37HFk_v+=vLuTwguPoU2wbtv4awnyeNUx8HAP2b-gitB3Q@mail.gmail.com>

By the way, *does* this actually happen in Hotspot? I just tried this on
7u60, and it does (a) not allocate the wrapper and (b) forwards the
argument as the return value.  However, it leaves a behind a fence
instruction (on x86).  Granted, one shouldn't rely on this, but knowing how
cautious compilers are with memory semantics, I'm not surprised that it
preserves the barrier effect.  Here's the assembly:

 # parm0:    rsi:rsi   = 'java/lang/Object'
  #           [sp+0x20]  (sp of caller)
  0x00007fe148a4b8c0: sub    $0x18,%rsp
  0x00007fe148a4b8c7: mov    %rbp,0x10(%rsp)
  0x00007fe148a4b8cc: lock addl $0x0,(%rsp)     ;*getfield o
                                                ;
  0x00007fe148a4b8d1: mov    %rsi,%rax
  0x00007fe148a4b8d4: add    $0x10,%rsp
  0x00007fe148a4b8d8: pop    %rbp
  0x00007fe148a4b8d9: test   %eax,0x5cc9721(%rip)        #
0x00007fe14e715000
                                                ;   {poll_return}
  0x00007fe148a4b8df: retq

private static final class Foo {
volatile Object o;
    }

private static Object doIt(final Object t) {
final Foo f = new Foo();
f.o = t;
return f.o;
    }

On Fri, Jan 23, 2015 at 2:19 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Oh sorry, you're talking about *thread-local* objects.
>
> On Fri, Jan 23, 2015 at 2:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> What compiler figures this out and how? What about reflection? What about
>> any future-loaded classes that may peek in there (directly or reflection)?
>> The current compiler doesn't even trust instance final fields! Please show
>> me an existing compiler that does the transformation you're talking about.
>>
>> On Fri, Jan 23, 2015 at 2:13 PM, Aleksey Shipilev <
>> aleksey.shipilev at oracle.com> wrote:
>>
>>> On 23.01.2015 22:03, Vitaly Davidovich wrote:
>>> > Under strict interpretation of current JMM, it does not prevent the
>>> > publishing of the reference from moving before wrapper.t = t; because
>>> > volatile stores do not order subsequent stores.  The "fake" volatile
>>> > load of wrapper.t, which you're trying to use to prevent that
>>> > reordering, creates no happens-before edge because it's done
>>> > intra-thread, and I think the JMM abstract model is referring to cross
>>> > thread edges.
>>> >
>>> > Now, that's all abstract model stuff -- in practice, no compiler can
>>> > actually see that a volatile write by T1 is then observed by a volatile
>>> > read on T2, and thus they play conservative and issue fences (compiler
>>> +
>>> > required CPU) right after a volatile write, and do not go looking for
>>> > any "happens-before" edges.
>>>
>>> Well, I don't like to play the Compiler Card, but thread-local objects
>>> are funny in this particular regard: we can make local analysis, since
>>> we know the object is accessed by a single thread only. Coming back to
>>> this code:
>>>
>>>  static final class Wrapper<T> {
>>>     volatile T t;
>>>  }
>>>
>>>  static <T> T safelyPublish(T t) {
>>>     Wrapper wrapper = new Wrapper<T> ();
>>>     wrapper.t = t;
>>>     return wrapper.t;
>>>  }
>>>
>>> ...even a moderately smart compiler can figure out the "volatile T t" is
>>> only accessed by a single thread, and therefore the memory effects of
>>> volatile load/store would not be visible to any other thread. Remember,
>>> spec only cares enough about the actions on the same variable from
>>> different threads. That allows the compiler to optimize safelyPublish()
>>> to:
>>>
>>>  static <T> T safelyPublish(T t) {
>>>     return t;
>>>  }
>>>
>>> Ooops. I would guess that happens in HotSpot now...
>>>
>>> Thanks,
>>> -Aleksey.
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/83822411/attachment-0001.html>

From lukeisandberg at gmail.com  Fri Jan 23 14:52:27 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 11:52:27 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2A085.5090808@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
	<54C2A085.5090808@oracle.com>
Message-ID: <CAO9V1M+HFGti_q-9BL4+VuL=VrctE23Oq-xu_fqCtZWky16+BQ@mail.gmail.com>

what about this other 'classic situation'
class Holder {
  Object field;
}

class Wrapper {
  final Holder holder;
  Wrapper(Holder holder) {
    this.holder = holder;
  }
}

T1:
unsafe = new Wrapper(new Holder(new Object());

T2:
Holder h;
while ((h = unsafe) == null) {} // spin
h.holder.field.toString();  // can there be an NPE?

So this case is fine because Wrapper is 'safely published' according to
final field semantics and 'Holder' gets to piggy back on this.  Right? So
how can i get those 'freeze' semantics without introducing additional boxes
around every field?


On Fri, Jan 23, 2015 at 11:27 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 22:22, Justin Sampson wrote:
> > Luke Sandberg wrote:
> >> I think causality would prevent the write to unsafe from moving
> >> earlier. since we write wrapper.t to unsafe, not the original
> >> reference.
> >
> > It sounds like you're mixing up causality and happens-before a bit.
> > Causality doesn't come into play here because there aren't any
> > conditional branches. The compiler knows that every line is going to
> > execute, so it's free to execute them out of order.
>
> I think it's dangerous to mix causality and particular program flow.
>
> I can do better: as far as my puny brain can wrap around, causality
> requirements in JMM try to prevent causality loops, *NOT* provide the
> global time. Although providing global time would probably solve
> causality problems once and for all, but that is contrary to the spirit
> of weak memory model that should allow code transformations.
>
> That is, under JMM's causality requirements, two threads can still
> observe different "sequence" of actions with regards to a particular
> object. T1 may firmly believe it stores the object reference after all
> the stores into it are committed. T2 may at the same time believe the
> object reference was stored, but none of it's field stored yet.
>
> Thanks,
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/9fb77759/attachment.html>

From vitalyd at gmail.com  Fri Jan 23 14:54:06 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 14:54:06 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37HFk_v+=vLuTwguPoU2wbtv4awnyeNUx8HAP2b-gitB3Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<54C28EE6.4050303@oracle.com>
	<CAO9V1M+vvnriRgp9s9xvuddoJjfOmxfgdwqE+QBDFFGWWGw9=A@mail.gmail.com>
	<CA+kOe08McY6NoPK21HpZvfTx0p++jKcyEcnHf3sUJTyvsKGKXA@mail.gmail.com>
	<CA+kOe0_r597x+QgcN5p89u+PwOop+hottnoy_tmSUHvtcWgG_w@mail.gmail.com>
	<CAO9V1MK+Eio2aN+W2RF5BWVB2kkdkwMojFsReq1JaXxyhG3Xxg@mail.gmail.com>
	<54C29756.3090207@oracle.com>
	<CAO9V1MJB7AfUuUQt0+xCmN6-9jHexwEbvbZvvNbvDGg6Czj4Ng@mail.gmail.com>
	<CAHjP37F103+Bah366cJYG_6pk_zr+kaZHBn75tbiYq=zK2+Aqg@mail.gmail.com>
	<54C29D5C.6050703@oracle.com>
	<CAHjP37GfO4+qKJC4QTdTSNYDQ3J0YCSV2O9mYy5uCryr2SjPmQ@mail.gmail.com>
	<CAHjP37EA5i3wTNd6R8zR=v=SOi4FvXJutrp1YAWFEdD1fqfp=A@mail.gmail.com>
	<CAHjP37HFk_v+=vLuTwguPoU2wbtv4awnyeNUx8HAP2b-gitB3Q@mail.gmail.com>
Message-ID: <CAHjP37ETsd4eiMNKRobLoF9cYg5COHXJU_uKB4hDVN15sFfjPw@mail.gmail.com>

And FYI, (as expected) it does not remove the barrier if you only store
(i.e. void method):

# parm0:    rsi:rsi   = 'java/lang/Object'
  #           [sp+0x20]  (sp of caller)
  0x00007fdf419f9640: sub    $0x18,%rsp
  0x00007fdf419f9647: mov    %rbp,0x10(%rsp)
  0x00007fdf419f964c: lock addl $0x0,(%rsp)     ;*putfield o

  0x00007fdf419f9651: add    $0x10,%rsp
  0x00007fdf419f9655: pop    %rbp
  0x00007fdf419f9656: test   %eax,0x5cca9a4(%rip)        #
0x00007fdf476c4000
                                                ;   {poll_return}
  0x00007fdf419f965c: retq

On Fri, Jan 23, 2015 at 2:39 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> By the way, *does* this actually happen in Hotspot? I just tried this on
> 7u60, and it does (a) not allocate the wrapper and (b) forwards the
> argument as the return value.  However, it leaves a behind a fence
> instruction (on x86).  Granted, one shouldn't rely on this, but knowing how
> cautious compilers are with memory semantics, I'm not surprised that it
> preserves the barrier effect.  Here's the assembly:
>
>  # parm0:    rsi:rsi   = 'java/lang/Object'
>   #           [sp+0x20]  (sp of caller)
>   0x00007fe148a4b8c0: sub    $0x18,%rsp
>   0x00007fe148a4b8c7: mov    %rbp,0x10(%rsp)
>   0x00007fe148a4b8cc: lock addl $0x0,(%rsp)     ;*getfield o
>                                                 ;
>   0x00007fe148a4b8d1: mov    %rsi,%rax
>   0x00007fe148a4b8d4: add    $0x10,%rsp
>   0x00007fe148a4b8d8: pop    %rbp
>   0x00007fe148a4b8d9: test   %eax,0x5cc9721(%rip)        #
> 0x00007fe14e715000
>                                                 ;   {poll_return}
>   0x00007fe148a4b8df: retq
>
> private static final class Foo {
> volatile Object o;
>     }
>
> private static Object doIt(final Object t) {
> final Foo f = new Foo();
> f.o = t;
> return f.o;
>     }
>
> On Fri, Jan 23, 2015 at 2:19 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> Oh sorry, you're talking about *thread-local* objects.
>>
>> On Fri, Jan 23, 2015 at 2:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> What compiler figures this out and how? What about reflection? What
>>> about any future-loaded classes that may peek in there (directly or
>>> reflection)? The current compiler doesn't even trust instance final fields!
>>> Please show me an existing compiler that does the transformation you're
>>> talking about.
>>>
>>> On Fri, Jan 23, 2015 at 2:13 PM, Aleksey Shipilev <
>>> aleksey.shipilev at oracle.com> wrote:
>>>
>>>> On 23.01.2015 22:03, Vitaly Davidovich wrote:
>>>> > Under strict interpretation of current JMM, it does not prevent the
>>>> > publishing of the reference from moving before wrapper.t = t; because
>>>> > volatile stores do not order subsequent stores.  The "fake" volatile
>>>> > load of wrapper.t, which you're trying to use to prevent that
>>>> > reordering, creates no happens-before edge because it's done
>>>> > intra-thread, and I think the JMM abstract model is referring to cross
>>>> > thread edges.
>>>> >
>>>> > Now, that's all abstract model stuff -- in practice, no compiler can
>>>> > actually see that a volatile write by T1 is then observed by a
>>>> volatile
>>>> > read on T2, and thus they play conservative and issue fences
>>>> (compiler +
>>>> > required CPU) right after a volatile write, and do not go looking for
>>>> > any "happens-before" edges.
>>>>
>>>> Well, I don't like to play the Compiler Card, but thread-local objects
>>>> are funny in this particular regard: we can make local analysis, since
>>>> we know the object is accessed by a single thread only. Coming back to
>>>> this code:
>>>>
>>>>  static final class Wrapper<T> {
>>>>     volatile T t;
>>>>  }
>>>>
>>>>  static <T> T safelyPublish(T t) {
>>>>     Wrapper wrapper = new Wrapper<T> ();
>>>>     wrapper.t = t;
>>>>     return wrapper.t;
>>>>  }
>>>>
>>>> ...even a moderately smart compiler can figure out the "volatile T t" is
>>>> only accessed by a single thread, and therefore the memory effects of
>>>> volatile load/store would not be visible to any other thread. Remember,
>>>> spec only cares enough about the actions on the same variable from
>>>> different threads. That allows the compiler to optimize safelyPublish()
>>>> to:
>>>>
>>>>  static <T> T safelyPublish(T t) {
>>>>     return t;
>>>>  }
>>>>
>>>> Ooops. I would guess that happens in HotSpot now...
>>>>
>>>> Thanks,
>>>> -Aleksey.
>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/e2e54b4f/attachment.html>

From lukeisandberg at gmail.com  Fri Jan 23 15:24:33 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 12:24:33 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C297EE.5050101@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
Message-ID: <CAO9V1MKXCJQhiQH3_DF+2FmLVZbBP-HRNxaeG6=AuObk4WuRsg@mail.gmail.com>

So how would you actually fix this issue?  (ignoring the case where
FallbackFuture is constructed with an unsafely published future, which i do
not care about)

On Fri, Jan 23, 2015 at 10:50 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 19:17, Luke Sandberg wrote:
> > Thanks, the wrapper was another strategy discussed (and is likely the
> > best due to simplicity).
>
> Final wrapper is a cute trick, but I think it only works if you have
> constructed the input in this very thread, or get the input from
> somewhere else safely.
>
> I.e. this is still broken:
>
> static final class Wrapper<T> {
>     ListenableFuture<T> input;
>     public Wrapper(ListenableFuture<T> input) { this.input = input; }
> }
>
> private static class FallbackFuture<V>
>     final Wrapper<? extends V> wrapper;
>
>     FallbackFuture(ListenableFuture<? extends V> input, ...) {
>       wrapper = new Wrapper(input);
>       /// a bunch of stuff with listeners
>     }
>     ...
> }
>
>
> Future f;
>
> Thread 1:
>   Future lf1 = <get a future from somewhere>
>   f = lf1; // racy write
>
> Thread 2:
>   Future lf2 = f; // racy read
>   Future ff = new FallbackFuture(lf2); // oops, too late!
>
> Even if lf2 is not null, you can't possibly instruct the writer that you
> need all the (initializing) stores for lf1 from Thread 1 right now,
> here, in Thread 2.
>
> Because of this, the subsequent (unsafe) publishing of ff with final
> wrapper on-board would not also be protected, since we are have been
> given the rotten $input.
>
>
> > Still it seem lame to have to _allocate_ to fix a visibility issue.  It
> > seems like there should be something similar to the 'freeze action' for
> > non-final
> > fields:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1
>
> Will only work for not-yet-published objects. And if you have to only
> deal with non-published objects, there are only a few corner cases that
> do not fit into object constructors.
>
>
> > Also, (just for my own edification) is it actually possible to observe
> > this bug on x86?
>
> Yes, if you have enough field stores to spill over the publication of
> the object. See also:
>  http://shipilev.net/blog/2014/safe-public-construction/
>
> -Aleksey.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/d9e48ba9/attachment-0001.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 15:28:30 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 23:28:30 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1M+HFGti_q-9BL4+VuL=VrctE23Oq-xu_fqCtZWky16+BQ@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
	<54C2A085.5090808@oracle.com>
	<CAO9V1M+HFGti_q-9BL4+VuL=VrctE23Oq-xu_fqCtZWky16+BQ@mail.gmail.com>
Message-ID: <54C2AEEE.3060607@oracle.com>

On 01/23/2015 10:52 PM, Luke Sandberg wrote:
> what about this other 'classic situation'
> class Holder {
>   Object field;
> }
> 
> class Wrapper {
>   final Holder holder;
>   Wrapper(Holder holder) {
>     this.holder = holder;
>   }
> }
> 
> T1:
> unsafe = new Wrapper(new Holder(new Object());
> 
> T2:
> Holder h;
> while ((h = unsafe) == null) {} // spin
> h.holder.field.toString();  // can there be an NPE?
> 
> So this case is fine because Wrapper is 'safely published' according to
> final field semantics and 'Holder' gets to piggy back on this.  Right?

Well, first, neither "safely published" nor "safely constructed" are
defined in final field semantics. Second, Wrapper is not safely
published, it is safely constructed, let's not conflate the two. Third,
in your example, since you are putting the new unpublished Holder, it's
okay, and no NPE is expected.

However, if your wrapper does wrap already exposed object, then:
(I fixed the types in your example, unsafe should be Wrapper)

class Holder {
  Object field;
}

class Wrapper {
  final Holder holder;
  Wrapper(Holder holder) {
    this.holder = holder;
  }
}

T1:
 Holder h1 = UnsafeSingletonFactory.getOneAndOnlyHolder();
 unsafe = new Wrapper(h1);

T2:
 Holder h2 = UnsafeSingletonFactory.getOneAndOnlyHolder();

 Wrapper w;
 while ((w = unsafe) == null) {} // spin
 Holder h0 = w.holder;

 // at this point, runtime realizes (h0 == h2), boom
 h2.field.toString();  // potential NPE

That's why I'm saying while the final wrapper trick is cute, it has some
preconditions to satisfy first. In terms of your ListenableFuture
example, you cannot guarantee the Future-to-be-wrapped is not exposed
yet, right? That means the Future-that-had-been-wrapped fields are not
guaranteed to be seen properly.


> So how can i get those 'freeze' semantics without introducing additional
> boxes around every field?

You can get freeze only with final fields. I don't know what you want me
to say. There is no other way, as I see it: only finals [*] properly
survive the racy publication, as far as the specification is concerned.
So, if you cannot use finals, you have to avoid races. Scratch that,
avoid races, and use finals where possible.

Until spec mandates the final-field-like semantics for all
initializations in constructors, there is no general solution. Once it
does, the issue will dissolve itself.

-Aleksey.

[*] ...and the objects reachable *only* through them -- spec is
complicated enough there, but it seems to mean exactly this for objects
reachable through final fields.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/3d36dfbb/attachment.bin>

From aleksey.shipilev at oracle.com  Fri Jan 23 15:31:53 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 23:31:53 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MKXCJQhiQH3_DF+2FmLVZbBP-HRNxaeG6=AuObk4WuRsg@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
	<CAO9V1MKXCJQhiQH3_DF+2FmLVZbBP-HRNxaeG6=AuObk4WuRsg@mail.gmail.com>
Message-ID: <54C2AFB9.401@oracle.com>

On 01/23/2015 11:24 PM, Luke Sandberg wrote:
> So how would you actually fix this issue?  (ignoring the case where
> FallbackFuture is constructed with an unsafely published future, which i
> do not care about)

I would recognize that trying to survive against the races without
finals is futile in one way or another, and stop trying. Educate users
all the hell breaks loose when they have racy programs.

-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/978d95fd/attachment.bin>

From jsampson at guidewire.com  Fri Jan 23 15:32:39 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 20:32:39 +0000
Subject: [concurrency-interest] Safe publishing strategy
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com> 
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E546@sm-ex-01-vm.guidewire.com>

I think the conversation has moved on, but I just had to work
through the possible executions to remind myself what's happening,
so I'll go ahead and post it in case it helps someone else (and to
make sure I've got it right myself).

Without any finals, there's nothing special about a constructor, and
it can just be inlined. So hopefully this program snippet is an
accurate depiction of what's going on when a volatile field is
assigned in a constructor and then the object itself is shared
through a data race:

Thread 1:
1) obj.x = 5 // volatile write
2) a = obj // non-volatile write

Thread 2:
3) r1 = a // non-volatile read
4) r2 = r1.x // volatile read

What values can r1 and r2 see?

First off, it's easy to accept that r1 can see either obj or null,
because no matter how a is declared, we could have Thread 1 execute
completely before or completely after Thread 2. If r1 sees null,
then r2 isn't even reached, because r1.x throws NPE. So let's just
focus on executions where r1 does see obj.

Right off the bat, we know that hb(1,2) and hb(3,4) due to the
intra-thread happens-before rule.

The only other happens-before edge that _might_ exist is _if_ the
volatile write actually precedes the volatile read in the
synchronization order, that is: so(1,4) implies hb(1,4).

But potentially it could be the other order, so(4,1), which doesn't
create a happens-before edge at all.

So the possible happens-before graphs are:

1. If so(1,4) then we have hb(1,2), hb(3,4), & hb(1,4). Since
so(1,4), we know that r2 sees 5, if it's reached at all. The hb(1,4)
edge does not transitively close with any other happens-before
edges, so there's nothing to prevent r1 from having seen obj.

2. If so(4,1) then we only have hb(1,2) & hb(3,4). Since so(4,1), we
know that r2 sees 0, if it's reached at all. Since there is no
happens-before edge at all between the two threads, there's nothing
to prevent r1 from having seen obj.

Therefore we do indeed have possible executions in which r1 does see
obj but r2 can see either 0 or 5 because in no case is there a
happens-before edge to prevent it. The only place that causality
comes into play is when r1 sees null, in which case the whole
question is moot.

I recently came across a very nice summary of the difference between
causality and happens-before by Jeremy Manson:

http://jeremymanson.blogspot.com/2007/08/causality-and-java-memory-model.html

Cheers,
Justin


From lukeisandberg at gmail.com  Fri Jan 23 15:35:07 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 12:35:07 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2AFB9.401@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
	<CAO9V1MKXCJQhiQH3_DF+2FmLVZbBP-HRNxaeG6=AuObk4WuRsg@mail.gmail.com>
	<54C2AFB9.401@oracle.com>
Message-ID: <CAO9V1M+MADg0eF9+Y8Hh7i7QuiBeJgdedMNEhe3HoRN6ZFbi=Q@mail.gmail.com>

Ok, so the worst thing that happens in these cases is that cancel() does
not propagate properly, which is fine since propagating cancel() is racy by
definition.  So i think i will just make sure that this can't actually
cause an NPE and live with it.

On Fri, Jan 23, 2015 at 12:31 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 01/23/2015 11:24 PM, Luke Sandberg wrote:
> > So how would you actually fix this issue?  (ignoring the case where
> > FallbackFuture is constructed with an unsafely published future, which i
> > do not care about)
>
> I would recognize that trying to survive against the races without
> finals is futile in one way or another, and stop trying. Educate users
> all the hell breaks loose when they have racy programs.
>
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/b4911010/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 15:49:10 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Jan 2015 23:49:10 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1M+MADg0eF9+Y8Hh7i7QuiBeJgdedMNEhe3HoRN6ZFbi=Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
	<CAO9V1MKXCJQhiQH3_DF+2FmLVZbBP-HRNxaeG6=AuObk4WuRsg@mail.gmail.com>
	<54C2AFB9.401@oracle.com>
	<CAO9V1M+MADg0eF9+Y8Hh7i7QuiBeJgdedMNEhe3HoRN6ZFbi=Q@mail.gmail.com>
Message-ID: <54C2B3C6.1060605@oracle.com>

...and protect from the weird failures within the $input.cancel()
itself, in case its internal state is FUBAR-ed due to a race. Although,
I'm not sure what's a good API strategy in this case would be.

-Aleksey.

On 01/23/2015 11:35 PM, Luke Sandberg wrote:
> Ok, so the worst thing that happens in these cases is that cancel() does
> not propagate properly, which is fine since propagating cancel() is racy
> by definition.  So i think i will just make sure that this can't
> actually cause an NPE and live with it.
> 
> On Fri, Jan 23, 2015 at 12:31 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> 
>     On 01/23/2015 11:24 PM, Luke Sandberg wrote:
>     > So how would you actually fix this issue?  (ignoring the case where
>     > FallbackFuture is constructed with an unsafely published future, which i
>     > do not care about)
> 
>     I would recognize that trying to survive against the races without
>     finals is futile in one way or another, and stop trying. Educate users
>     all the hell breaks loose when they have racy programs.
> 
>     -Aleksey.
> 
> 
> 


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/e40a377f/attachment.bin>

From vitalyd at gmail.com  Fri Jan 23 15:55:32 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 15:55:32 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C297EE.5050101@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
Message-ID: <CAHjP37FJ7985H5dHFk5yyab64RB_yLqYB6Aj285bhFYrXSyWCA@mail.gmail.com>

>
> Final wrapper is a cute trick, but I think it only works if you have
> constructed the input in this very thread, or get the input from
> somewhere else safely.


Yes, the "root" of this chain has to be correct, but I don't think there's
anything surprising about that.


On Fri, Jan 23, 2015 at 1:50 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 19:17, Luke Sandberg wrote:
> > Thanks, the wrapper was another strategy discussed (and is likely the
> > best due to simplicity).
>
> Final wrapper is a cute trick, but I think it only works if you have
> constructed the input in this very thread, or get the input from
> somewhere else safely.
>
> I.e. this is still broken:
>
> static final class Wrapper<T> {
>     ListenableFuture<T> input;
>     public Wrapper(ListenableFuture<T> input) { this.input = input; }
> }
>
> private static class FallbackFuture<V>
>     final Wrapper<? extends V> wrapper;
>
>     FallbackFuture(ListenableFuture<? extends V> input, ...) {
>       wrapper = new Wrapper(input);
>       /// a bunch of stuff with listeners
>     }
>     ...
> }
>
>
> Future f;
>
> Thread 1:
>   Future lf1 = <get a future from somewhere>
>   f = lf1; // racy write
>
> Thread 2:
>   Future lf2 = f; // racy read
>   Future ff = new FallbackFuture(lf2); // oops, too late!
>
> Even if lf2 is not null, you can't possibly instruct the writer that you
> need all the (initializing) stores for lf1 from Thread 1 right now,
> here, in Thread 2.
>
> Because of this, the subsequent (unsafe) publishing of ff with final
> wrapper on-board would not also be protected, since we are have been
> given the rotten $input.
>
>
> > Still it seem lame to have to _allocate_ to fix a visibility issue.  It
> > seems like there should be something similar to the 'freeze action' for
> > non-final
> > fields:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1
>
> Will only work for not-yet-published objects. And if you have to only
> deal with non-published objects, there are only a few corner cases that
> do not fit into object constructors.
>
>
> > Also, (just for my own edification) is it actually possible to observe
> > this bug on x86?
>
> Yes, if you have enough field stores to spill over the publication of
> the object. See also:
>  http://shipilev.net/blog/2014/safe-public-construction/
>
> -Aleksey.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/fb0e6f6d/attachment-0001.html>

From vitalyd at gmail.com  Fri Jan 23 16:25:11 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 16:25:11 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C297EE.5050101@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
Message-ID: <CAHjP37GzHRBc4wPPmNz+9GQj5h+qvR2XSNmR4OGxu6Of=vqv6Q@mail.gmail.com>

>
> Yes, if you have enough field stores to spill over the publication of
> the object. See also:
>  http://shipilev.net/blog/2014/safe-public-construction/


Can you elaborate on what you mean by "enough field stores to spill over
the publication"?

On Fri, Jan 23, 2015 at 1:50 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 23.01.2015 19:17, Luke Sandberg wrote:
> > Thanks, the wrapper was another strategy discussed (and is likely the
> > best due to simplicity).
>
> Final wrapper is a cute trick, but I think it only works if you have
> constructed the input in this very thread, or get the input from
> somewhere else safely.
>
> I.e. this is still broken:
>
> static final class Wrapper<T> {
>     ListenableFuture<T> input;
>     public Wrapper(ListenableFuture<T> input) { this.input = input; }
> }
>
> private static class FallbackFuture<V>
>     final Wrapper<? extends V> wrapper;
>
>     FallbackFuture(ListenableFuture<? extends V> input, ...) {
>       wrapper = new Wrapper(input);
>       /// a bunch of stuff with listeners
>     }
>     ...
> }
>
>
> Future f;
>
> Thread 1:
>   Future lf1 = <get a future from somewhere>
>   f = lf1; // racy write
>
> Thread 2:
>   Future lf2 = f; // racy read
>   Future ff = new FallbackFuture(lf2); // oops, too late!
>
> Even if lf2 is not null, you can't possibly instruct the writer that you
> need all the (initializing) stores for lf1 from Thread 1 right now,
> here, in Thread 2.
>
> Because of this, the subsequent (unsafe) publishing of ff with final
> wrapper on-board would not also be protected, since we are have been
> given the rotten $input.
>
>
> > Still it seem lame to have to _allocate_ to fix a visibility issue.  It
> > seems like there should be something similar to the 'freeze action' for
> > non-final
> > fields:
> http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.1
>
> Will only work for not-yet-published objects. And if you have to only
> deal with non-published objects, there are only a few corner cases that
> do not fit into object constructors.
>
>
> > Also, (just for my own edification) is it actually possible to observe
> > this bug on x86?
>
> Yes, if you have enough field stores to spill over the publication of
> the object. See also:
>  http://shipilev.net/blog/2014/safe-public-construction/
>
> -Aleksey.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/19e4b628/attachment.html>

From vitalyd at gmail.com  Fri Jan 23 16:53:53 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 16:53:53 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37H63ydGqX_zteU2TKor+GCVFY2bs=tvowPykqWZhn=xMw@mail.gmail.com>

Right, there's no causality here; Aleksey's post and the assembly dump I
pasted shows that the compiler eliminates the wrapper entirely (but, it
didn't remove the fence).

sent from my phone
On Jan 23, 2015 4:47 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> Luke Sandberg wrote:
>
> > I think causality would prevent the write to unsafe from moving
> > earlier. since we write wrapper.t to unsafe, not the original
> > reference.
>
> It sounds like you're mixing up causality and happens-before a bit.
> Causality doesn't come into play here because there aren't any
> conditional branches. The compiler knows that every line is going to
> execute, so it's free to execute them out of order.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/580a768f/attachment.html>

From jsampson at guidewire.com  Fri Jan 23 16:58:55 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 21:58:55 +0000
Subject: [concurrency-interest] unpark/park memory visibility
References: <1420743543652-11812.post@n7.nabble.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<54B98C4F.1090000@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83F84@sm-ex-01-vm.guidewire.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8C298@sm-ex-01-vm.guidewire.com>
	<54C0484D.4030700@cs.oswego.edu> 
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E5EB@sm-ex-01-vm.guidewire.com>

I wrote:

> Doug Lea wrote:
>
> > OK. Which brings us back to providing usage guidance rather than
> > a (semi) formal spec. About which most people probably agree.
>
> Yes. Just out of curiosity, as a newby in these here parts, is
> there any intention to come up with formal specs for such things
> to be put somewhere official, even if not in the individual class
> docs? Either a revision of or an addendum to the JMM, for example?

Never mind, I think I've found what I was hoping for. :)

http://openjdk.java.net/jeps/188
http://openjdk.java.net/jeps/193

Yay,
Justin


From lukeisandberg at gmail.com  Fri Jan 23 16:59:58 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Fri, 23 Jan 2015 13:59:58 -0800
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37H63ydGqX_zteU2TKor+GCVFY2bs=tvowPykqWZhn=xMw@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
	<54C28BEE.8010201@oracle.com>
	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>
	<CAHjP37H63ydGqX_zteU2TKor+GCVFY2bs=tvowPykqWZhn=xMw@mail.gmail.com>
Message-ID: <CAO9V1MJ6hET80HH_eCKKMZpHj9CvOZHwfOgYK4YUwJty+1C5KA@mail.gmail.com>

Thanks for everyone's help.  I guess it is good to know that silly hacks
like the ones above don't fix the issue in theory (they are kind of gross
anyway).  So we are just going to code our cancel() methods defensively
against uninitialized fields  (unless someone convinces me otherwise).

Most of them are already safe because they have to deal with other races
involved in futures completing.  In fact I'm pretty sure the current
version of Futures.withFallback is already safe, it is just that if this
ever happens cancel() won't be propagated which is a pretty reasonable
failure mode IMHO.

On Fri, Jan 23, 2015 at 1:53 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Right, there's no causality here; Aleksey's post and the assembly dump I
> pasted shows that the compiler eliminates the wrapper entirely (but, it
> didn't remove the fence).
>
> sent from my phone
> On Jan 23, 2015 4:47 PM, "Justin Sampson" <jsampson at guidewire.com> wrote:
>
>> Luke Sandberg wrote:
>>
>> > I think causality would prevent the write to unsafe from moving
>> > earlier. since we write wrapper.t to unsafe, not the original
>> > reference.
>>
>> It sounds like you're mixing up causality and happens-before a bit.
>> Causality doesn't come into play here because there aren't any
>> conditional branches. The compiler knows that every line is going to
>> execute, so it's free to execute them out of order.
>>
>> Cheers,
>> Justin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/b36bc88b/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 17:37:38 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 24 Jan 2015 01:37:38 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37GzHRBc4wPPmNz+9GQj5h+qvR2XSNmR4OGxu6Of=vqv6Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>	<54C297EE.5050101@oracle.com>
	<CAHjP37GzHRBc4wPPmNz+9GQj5h+qvR2XSNmR4OGxu6Of=vqv6Q@mail.gmail.com>
Message-ID: <54C2CD32.5020404@oracle.com>

On 01/24/2015 12:25 AM, Vitaly Davidovich wrote:
>     Yes, if you have enough field stores to spill over the publication of
>     the object. See also:
>      http://shipilev.net/blog/2014/safe-public-construction/
> 
> 
> Can you elaborate on what you mean by "enough field stores to spill over
> the publication"?

If you do quite a few non-final field stores in constructor, the
instruction scheduler may decide to emit a few field stores, then
publish the reference, then emit the rest of the field stores. I managed
to observe this at least once back in JDK 7 and 32-bit VMs days on a
real-life application.

Out of curiosity, I slapped together a reproducer with jcstress:
 http://hg.openjdk.java.net/code-tools/jcstress/file/7cd3dfada49b/tests-custom/src/main/java/org/openjdk/jcstress/tests/unsafe/UnsafePublication.java

On my i7-4790K, JDK 8u40 EA, Linux x86_64, it does:

$ java -XX:-UseCompressedOops -jar tests-custom/target/jcstress.jar -t
.*UnsafePublication.*  -v

[OK] o.o.j.t.unsafe.UnsafePublication
Observed state     Occurrences    Expectation   Interpretation
          [-1] (   97,403,388)     ACCEPTABLE   Not published yet.
           [0] (        2,731)     ACCEPTABLE   0 fields visible.
           [1] (        6,099)     ACCEPTABLE   1 field visible.
           [2] (        6,533)     ACCEPTABLE   2 fields visible.
           [3] (       17,607)     ACCEPTABLE   3 fields visible.
           [4] (   24,692,082)     ACCEPTABLE   All fields visible.

Tell me something about "theoretical" again?

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/dca75147/attachment-0001.bin>

From vitalyd at gmail.com  Fri Jan 23 17:42:36 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 17:42:36 -0500
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2CD32.5020404@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>
	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>
	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>
	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>
	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>
	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>
	<54C297EE.5050101@oracle.com>
	<CAHjP37GzHRBc4wPPmNz+9GQj5h+qvR2XSNmR4OGxu6Of=vqv6Q@mail.gmail.com>
	<54C2CD32.5020404@oracle.com>
Message-ID: <CAHjP37HDAiaSswgx1-YmQL+3Kkm6YX1wRViCcgWbb1c7Kjo=6A@mail.gmail.com>

Yes, I know compiler is allowed to do that (without final and volatile*),
but I was having a hard time imagining where it would actually make that
type of transformation because it doesn't seem profitable at first thought.

In the case you observed, did it actually make sense for it to have done
that? I.e. was there some tangible optimization or was it just some
artifact of implementation?

* - assuming we're all in agreement now that volatile has the same effect,
although not officially in current JMM

On Fri, Jan 23, 2015 at 5:37 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 01/24/2015 12:25 AM, Vitaly Davidovich wrote:
> >     Yes, if you have enough field stores to spill over the publication of
> >     the object. See also:
> >      http://shipilev.net/blog/2014/safe-public-construction/
> >
> >
> > Can you elaborate on what you mean by "enough field stores to spill over
> > the publication"?
>
> If you do quite a few non-final field stores in constructor, the
> instruction scheduler may decide to emit a few field stores, then
> publish the reference, then emit the rest of the field stores. I managed
> to observe this at least once back in JDK 7 and 32-bit VMs days on a
> real-life application.
>
> Out of curiosity, I slapped together a reproducer with jcstress:
>
> http://hg.openjdk.java.net/code-tools/jcstress/file/7cd3dfada49b/tests-custom/src/main/java/org/openjdk/jcstress/tests/unsafe/UnsafePublication.java
>
> On my i7-4790K, JDK 8u40 EA, Linux x86_64, it does:
>
> $ java -XX:-UseCompressedOops -jar tests-custom/target/jcstress.jar -t
> .*UnsafePublication.*  -v
>
> [OK] o.o.j.t.unsafe.UnsafePublication
> Observed state     Occurrences    Expectation   Interpretation
>           [-1] (   97,403,388)     ACCEPTABLE   Not published yet.
>            [0] (        2,731)     ACCEPTABLE   0 fields visible.
>            [1] (        6,099)     ACCEPTABLE   1 field visible.
>            [2] (        6,533)     ACCEPTABLE   2 fields visible.
>            [3] (       17,607)     ACCEPTABLE   3 fields visible.
>            [4] (   24,692,082)     ACCEPTABLE   All fields visible.
>
> Tell me something about "theoretical" again?
>
> Thanks,
> -Aleksey.
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/e2fe8604/attachment.html>

From aleksey.shipilev at oracle.com  Fri Jan 23 17:53:31 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 24 Jan 2015 01:53:31 +0300
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37HDAiaSswgx1-YmQL+3Kkm6YX1wRViCcgWbb1c7Kjo=6A@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<CAHjP37EnyjWWN_exMectkPa7t=XRFWZ8nJAV7kPbBv-OyDmgkQ@mail.gmail.com>	<CAO9V1MJiwyXXZ=cDPLRAe0wZu0FWYMuq7uWdr=dF2b-Lxr7rpg@mail.gmail.com>	<54C297EE.5050101@oracle.com>	<CAHjP37GzHRBc4wPPmNz+9GQj5h+qvR2XSNmR4OGxu6Of=vqv6Q@mail.gmail.com>	<54C2CD32.5020404@oracle.com>
	<CAHjP37HDAiaSswgx1-YmQL+3Kkm6YX1wRViCcgWbb1c7Kjo=6A@mail.gmail.com>
Message-ID: <54C2D0EB.2000601@oracle.com>

On 01/24/2015 01:42 AM, Vitaly Davidovich wrote:
> Yes, I know compiler is allowed to do that (without final and
> volatile*), but I was having a hard time imagining where it would
> actually make that type of transformation because it doesn't seem
> profitable at first thought.

> In the case you observed, did it actually make sense for it to have done
> that? I.e. was there some tangible optimization or was it just some
> artifact of implementation?

Well, who knows? Instruction scheduling depends on many things,
including, at times, at pure random when breaking the ties. As long as
spec is not violated, compilers are free to do stuff. *That* should be
the concern for this mailing list.

-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/6db10705/attachment.bin>

From jsampson at guidewire.com  Fri Jan 23 18:37:30 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 23 Jan 2015 23:37:30 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>

Howdy,

I'm curious if any proposals are being floated to somehow deal with
the problem of StackOverflowError arising inside concurrent code.
While writing a new synchronizer I keep obsessing over possible
error conditions, and this is the one I can't flush out.

I can carefully code around OutOfMemoryError by only allocating at
safe places, and I can brush off ThreadDeath just by yelling at
users to never call Thread.stop(). But StackOverflowError is the one
kind of error that I really can't code around, because it can happen
legitimately at any method call. The best I can do at the moment is
to reassure myself that it's not my fault, because ReentrantLock
has the same issue: http://bugs.java.com/view_bug.do?bug_id=7011862

For the sake of discussion, here's one solution that comes to mind.

First, define a "stack-limited" method to be a method that is not
recursive and that only calls other stack-limited methods, so that
it is possible for the JVM to determine an upper bound on its stack
usage.

Next, introduce two new standard annotations:

@StackLimited on a method declaration indicates that the method is
explicitly stack-limited. It is a compile error for any such method
to call any other method that is not also explicitly stack-limited
or to introduce any possibility of recursive invocation.

@StackSensitive on a method declaration indicates that the method is
explicitly stack-limited, with all the constraints of @StackLimited,
and also tells the JVM to proactively check the available stack
space and throw StackOverflowError immediately on entry to the
method if there is not enough stack for all possible executions of
the method.

The idea would be to annotate the minimum set of methods to ensure
that a synchronizer is never left in an invalid state due to stack
overflow. That would mostly be putting @StackLimited on the methods
of Unsafe, LockSupport, and the j.u.c.atomic package, and putting
@StackSensitive on just one or two methods in any given synchronizer
such as AQS.

Is something like this plausible, or even already in the works?

Cheers,
Justin


From vitalyd at gmail.com  Fri Jan 23 19:25:54 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 23 Jan 2015 19:25:54 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>

Ahhh, the noble goal of hardening a system against async exceptions like
OOM and SOE. :) Honestly, I've yet to see code that is 100% hardened
against OOM.  Frankly, I'm not sure this is possible in java given there's
a big machine that's running beneath you (i.e. JVM, which can throw OOM in
places you wouldn't necessary expect), and this goes for SOE as well.  I'd
love to be proven wrong though.



On Fri, Jan 23, 2015 at 6:37 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> Howdy,
>
> I'm curious if any proposals are being floated to somehow deal with
> the problem of StackOverflowError arising inside concurrent code.
> While writing a new synchronizer I keep obsessing over possible
> error conditions, and this is the one I can't flush out.
>
> I can carefully code around OutOfMemoryError by only allocating at
> safe places, and I can brush off ThreadDeath just by yelling at
> users to never call Thread.stop(). But StackOverflowError is the one
> kind of error that I really can't code around, because it can happen
> legitimately at any method call. The best I can do at the moment is
> to reassure myself that it's not my fault, because ReentrantLock
> has the same issue: http://bugs.java.com/view_bug.do?bug_id=7011862
>
> For the sake of discussion, here's one solution that comes to mind.
>
> First, define a "stack-limited" method to be a method that is not
> recursive and that only calls other stack-limited methods, so that
> it is possible for the JVM to determine an upper bound on its stack
> usage.
>
> Next, introduce two new standard annotations:
>
> @StackLimited on a method declaration indicates that the method is
> explicitly stack-limited. It is a compile error for any such method
> to call any other method that is not also explicitly stack-limited
> or to introduce any possibility of recursive invocation.
>
> @StackSensitive on a method declaration indicates that the method is
> explicitly stack-limited, with all the constraints of @StackLimited,
> and also tells the JVM to proactively check the available stack
> space and throw StackOverflowError immediately on entry to the
> method if there is not enough stack for all possible executions of
> the method.
>
> The idea would be to annotate the minimum set of methods to ensure
> that a synchronizer is never left in an invalid state due to stack
> overflow. That would mostly be putting @StackLimited on the methods
> of Unsafe, LockSupport, and the j.u.c.atomic package, and putting
> @StackSensitive on just one or two methods in any given synchronizer
> such as AQS.
>
> Is something like this plausible, or even already in the works?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150123/5687bf1b/attachment.html>

From dl at cs.oswego.edu  Fri Jan 23 19:32:42 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Jan 2015 19:32:42 -0500
Subject: [concurrency-interest] Varieties of CAS semantics (another doc
 fix request)
In-Reply-To: <54C26208.7090407@oracle.com>
References: <CAPUmR1ajjjt9f1PAqU9KJ-XjKpWnQ_XoZj-adaroJn58W0NRRg@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCMEMHKMAA.davidcholmes@aapt.net.au>	<CAPUmR1Y2aVUC4KywiLQQhEbpUB1FrS7hmZenYEnTABkLfhqC8w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8CC8A@sm-ex-01-vm.guidewire.com>
	<54C26208.7090407@oracle.com>
Message-ID: <54C2E82A.2010800@cs.oswego.edu>


I've been diverted with other things during most of this discussion,
but here are some notes about context.

The specs for CAS and all other atomics-related operations are
best-effort approximations; none of them are covered in the JSR133 JMM
(current JLS) spec, so we did the best we could at the time using
constructs available in the JLS. But we also knew that we'd someday
need to do better after spotting issues including some discussed on
this list.

Doing so is not as easy as you might think, because there are some
known technical bugs with the core model. So to fix these problems
perfectly, we need to fix the model too.  If we could, we'd then adapt
some of the wordings for some atomics-related methods from
corresponding parts of the C/C++ 11/14 specs. The intended semantics
are conceptually the same (in part because overlapping sets of people
were involved in defining both). It might be nice if they were all
exactly the same. But they cannot quite be, because Java provides
minimal no-out-of-thin-air guarantees to racy programs that C/C++ do
not.

About a year ago, we started a JMM revision JEP, hoping to both come up
with a revised core model and also to develop more credible specs for
"enhanced volatile" operations that for the most part correspond to
C/C++ memory_orders, and should replace the ad-hoc collection of Atomic
and Unsafe methods currently used to get these effects. See

JEP 188: Java Memory Model Update http://openjdk.java.net/jeps/188
JEP 193: Enhanced Volatiles http://openjdk.java.net/jeps/193

as well as my intro post to jmm-dev:
http://mail.openjdk.java.net/pipermail/jmm-dev/2014-February/000000.html

The JEP 188 core model efforts have been going very slowly. So far, no
one has proposed a viable candidate that correctly handles "Out of
thin air" issues while also not disallowing behaviors seen on actual
processors that seem intuitively OK. See the list archives for
details http://mail.openjdk.java.net/mailman/listinfo/jmm-dev

On the other hand, implementation efforts for JEP 193 (mainly by Paul
Sandoz) are proceeding (as they must, because VarHandles are needed in
jdk9). Worst case, the specs for these methods will have some of the
same problems that current CAS specs have now. If so, we'll at least
ask for clarity improvements.

-Doug


From dl at cs.oswego.edu  Fri Jan 23 20:25:17 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Jan 2015 20:25:17 -0500
Subject: [concurrency-interest] Using relaxed reads and writes
In-Reply-To: <1538772756.3228485.1421972627053.JavaMail.yahoo@jws10035.mail.ne1.yahoo.com>
References: <1538772756.3228485.1421972627053.JavaMail.yahoo@jws10035.mail.ne1.yahoo.com>
Message-ID: <54C2F47D.7010206@cs.oswego.edu>

On 01/22/2015 07:23 PM, Ben Manes wrote:
> I think the following is a safe optimization for my use-case, but I'd appreciate
> any feedback / concerns.
>
> Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is an entry
> in an LRU cache with miscellaneous metadata fields such as the queue links, the
> value, expiration timestamps, etc. When updating the value the node must be
> synchronized on in order to block if created by a concurrent computation. This
> means the more optimal CAS loop isn't used, which may have affect on simplifying
> the following discussion. Lets say that the Node's value and accessTime fields
> are volatile.
>
> When writing the value the node's lock must be held (or implicitly safe due to
> Node construction). In that case I think that a relaxed write
> (Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for
> visibility (or publishing via insert if created).

Yes, in fact hotspot often does this optimization for you,
at least on x86 and sparc, although not if biased locking
is enabled. It may not be worthwhile to hand-optimize.

-Doug




From TEREKHOV at de.ibm.com  Fri Jan 23 20:30:07 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 24 Jan 2015 02:30:07 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
Message-ID: <OF91C28984.E6E1EC0B-ONC1257DD7.00081A9D-C1257DD7.000840CE@de.ibm.com>

dejavu

http://www.programd.com/41_f4661b62296395b6_1.htm
http://lists.boost.org/Archives/boost/2003/10/54449.php

regards,
alexander.

Justin Sampson <jsampson at guidewire.com>@cs.oswego.edu on 24.01.2015
00:37:30

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
cc:
Subject:	[concurrency-interest] JVM support to avoid StackOverflowError


Howdy,

I'm curious if any proposals are being floated to somehow deal with
the problem of StackOverflowError arising inside concurrent code.
While writing a new synchronizer I keep obsessing over possible
error conditions, and this is the one I can't flush out.

I can carefully code around OutOfMemoryError by only allocating at
safe places, and I can brush off ThreadDeath just by yelling at
users to never call Thread.stop(). But StackOverflowError is the one
kind of error that I really can't code around, because it can happen
legitimately at any method call. The best I can do at the moment is
to reassure myself that it's not my fault, because ReentrantLock
has the same issue: http://bugs.java.com/view_bug.do?bug_id=7011862

For the sake of discussion, here's one solution that comes to mind.

First, define a "stack-limited" method to be a method that is not
recursive and that only calls other stack-limited methods, so that
it is possible for the JVM to determine an upper bound on its stack
usage.

Next, introduce two new standard annotations:

@StackLimited on a method declaration indicates that the method is
explicitly stack-limited. It is a compile error for any such method
to call any other method that is not also explicitly stack-limited
or to introduce any possibility of recursive invocation.

@StackSensitive on a method declaration indicates that the method is
explicitly stack-limited, with all the constraints of @StackLimited,
and also tells the JVM to proactively check the available stack
space and throw StackOverflowError immediately on entry to the
method if there is not enough stack for all possible executions of
the method.

The idea would be to annotate the minimum set of methods to ensure
that a synchronizer is never left in an invalid state due to stack
overflow. That would mostly be putting @StackLimited on the methods
of Unsafe, LockSupport, and the j.u.c.atomic package, and putting
@StackSensitive on just one or two methods in any given synchronizer
such as AQS.

Is something like this plausible, or even already in the works?

Cheers,
Justin

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From ben_manes at yahoo.com  Fri Jan 23 22:13:56 2015
From: ben_manes at yahoo.com (Ben Manes)
Date: Sat, 24 Jan 2015 03:13:56 +0000 (UTC)
Subject: [concurrency-interest] Using relaxed reads and writes
In-Reply-To: <54C2F47D.7010206@cs.oswego.edu>
References: <54C2F47D.7010206@cs.oswego.edu>
Message-ID: <632168398.607734.1422069236869.JavaMail.yahoo@mail.yahoo.com>

Thanks Doug!
That's really nice to know, since it bugged me for a long time but I wasn't sure if it was a safe enough optimization for a Hotspot transformation. Since there are some relaxed read/write opportunities outside of a lock I think its worth doing by hand. The relaxed operations are done within code generated classes so the extra effort to hand optimize turned out to be minor (72 node types to minimize per-node footprint based on the cache's configuration). 

     On Friday, January 23, 2015 5:50 PM, Doug Lea <dl at cs.oswego.edu> wrote:
   

 On 01/22/2015 07:23 PM, Ben Manes wrote:
> I think the following is a safe optimization for my use-case, but I'd appreciate
> any feedback / concerns.
>
> Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is an entry
> in an LRU cache with miscellaneous metadata fields such as the queue links, the
> value, expiration timestamps, etc. When updating the value the node must be
> synchronized on in order to block if created by a concurrent computation. This
> means the more optimal CAS loop isn't used, which may have affect on simplifying
> the following discussion. Lets say that the Node's value and accessTime fields
> are volatile.
>
> When writing the value the node's lock must be held (or implicitly safe due to
> Node construction). In that case I think that a relaxed write
> (Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for
> visibility (or publishing via insert if created).

Yes, in fact hotspot often does this optimization for you,
at least on x86 and sparc, although not if biased locking
is enabled. It may not be worthwhile to hand-optimize.

-Doug



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


    
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/f82bd43b/attachment.html>

From jsampson at guidewire.com  Sat Jan 24 00:44:33 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 24 Jan 2015 05:44:33 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Ahhh, the noble goal of hardening a system against async
> exceptions like OOM and SOE. :) Honestly, I've yet to see code
> that is 100% hardened against OOM. Frankly, I'm not sure this is
> possible in java given there's a big machine that's running
> beneath you (i.e. JVM, which can throw OOM in places you wouldn't
> necessary expect), and this goes for SOE as well. I'd love to be
> proven wrong though.

To be clear, I'm not talking about hardening the whole system
against arbitrary async exceptions. OOME and SOE _can_ be thrown
asynchronously, but there are also well-defined places that they
happen synchronously. It seems at least plausible to be able to
harden certain low-level library code against such synchronous
exceptions, especially certain blocks of code within synchronizers
where there's a risk of leaving the synchronizer locked.

I've searched a little in the archives and I can see the issue at
least mentioned a couple of times, but wanted to get a sense of
whether any solutions were being considered. The links that
Alexander sent are very interesting history! -- but they're about
C++ and I couldn't quite ascertain the final outcome.

To make the idea even more constrained, imagine a method that only
calls other final or static Java methods or intrinsified native
methods, whether directly or indirectly, without any recursion, none
of which do any heap allocation (no new objects, arrays, boxing,
lambdas, varargs, etc.). Conceptually, the JIT could inline the
whole call tree into a single method body with a single stack frame.

Any call to such a method would throw SOE right up front if the
stack has been exhausted, avoiding any SOE's in the middle. (Any
class loading and initialization that might be triggered by the body
of the method would have to be executed immediately on entry to the
method as well, since they can trigger arbitrary exceptions and
errors.)

The strawman proposal in my prior email was simply to provide an
annotation that would request for the JVM to implement such
fail-fast behavior for the annotated methods, with compile-time
verification that it should be possible for the runtime to do so.

Cheers,
Justin


From sitnikov.vladimir at gmail.com  Sat Jan 24 12:58:31 2015
From: sitnikov.vladimir at gmail.com (Vladimir Sitnikov)
Date: Sat, 24 Jan 2015 21:58:31 +0400
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
Message-ID: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>

Hi,

I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
it looks strange to see ReentrantLock there.
According to jol (see [1]), COWAL consumes 88 bytes.

It looks like excessive garbage for no apparent reason.

There are two questions:
1) Can ReentrantLock be eliminated in favor of synchronized(something)?
I believe a change of "ReentrantLock lock" to "Object lock=new
Object()" would reduce the footprint without loss of existing
semantics.

2) Can initial state (new Object[0]) be reused instead of creating new
Object[0] for each COWAL?
Plain old ArrayList does that, so I wonder why COWList misses the optimization.

[1]: https://gist.github.com/vlsi/3afe92a7f6449cc1692d

-- 
Regards,
Vladimir Sitnikov

From akarnokd at gmail.com  Sat Jan 24 13:46:08 2015
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sat, 24 Jan 2015 19:46:08 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54C002A6.9060204@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
	<54BD0ED7.50907@cs.oswego.edu>
	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
	<54BD2585.5040903@cs.oswego.edu>
	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
	<54BD74C7.3020500@cs.oswego.edu> <54C002A6.9060204@cs.oswego.edu>
Message-ID: <CAAWwtm914bGV3HjZ0T1NXNtnk1rx1kGQeD5yx0_MqmBwOx905g@mail.gmail.com>

The example of the one-shot publisher looks almost good, but I think the
creation of the IllegalArgumentException inside the scheduled action will
give misleading stacktrace: I'd be interested in who called request with
such negative value (without setting breakpoints). Therefore, I see two
options: throw the IAE directly in request or create the IAE and schedule
it separately from the value emission.

Doug Lea <dl at cs.oswego.edu> ezt ?rta (2015. janu?r 21., szerda):

>
> Thanks to a few people putting up with daily API and functionality
> changes to the in-progress SubmissionPublisher class, which is
> slowly stabilizing enough that others might want to try it out
> as well. Current javadoc at
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
> SubmissionPublisher.html
> And you can run with jdk8+ with -Xbootclasspath/p:jsr166.jar from
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
> For fun, I also placed a snapshot of one of my little internal
> development perf tests at http://gee.cs.oswego.edu/dl/wwwtmp/SPL4.java
>
> It shows throughput of a bunch of 3-stage flows. (The test doesn't
> use JMH, to simplify transiently adding and removing internal
> instrumentation while developing.)
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/50ef96c8/attachment.html>

From oleksandr.otenko at oracle.com  Sat Jan 24 13:49:50 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Sat, 24 Jan 2015 18:49:50 +0000
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2AEEE.3060607@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>	<54C2A085.5090808@oracle.com>	<CAO9V1M+HFGti_q-9BL4+VuL=VrctE23Oq-xu_fqCtZWky16+BQ@mail.gmail.com>
	<54C2AEEE.3060607@oracle.com>
Message-ID: <54C3E94E.3050405@oracle.com>

Can you explain how you get 'boom'?

Is this because "h0 = w.holder;" is not meant to be treated as a 
volatile read?

Alex


On 23/01/2015 20:28, Aleksey Shipilev wrote:
> On 01/23/2015 10:52 PM, Luke Sandberg wrote:
>> what about this other 'classic situation'
>> class Holder {
>>    Object field;
>> }
>>
>> class Wrapper {
>>    final Holder holder;
>>    Wrapper(Holder holder) {
>>      this.holder = holder;
>>    }
>> }
>>
>> T1:
>> unsafe = new Wrapper(new Holder(new Object());
>>
>> T2:
>> Holder h;
>> while ((h = unsafe) == null) {} // spin
>> h.holder.field.toString();  // can there be an NPE?
>>
>> So this case is fine because Wrapper is 'safely published' according to
>> final field semantics and 'Holder' gets to piggy back on this.  Right?
> Well, first, neither "safely published" nor "safely constructed" are
> defined in final field semantics. Second, Wrapper is not safely
> published, it is safely constructed, let's not conflate the two. Third,
> in your example, since you are putting the new unpublished Holder, it's
> okay, and no NPE is expected.
>
> However, if your wrapper does wrap already exposed object, then:
> (I fixed the types in your example, unsafe should be Wrapper)
>
> class Holder {
>    Object field;
> }
>
> class Wrapper {
>    final Holder holder;
>    Wrapper(Holder holder) {
>      this.holder = holder;
>    }
> }
>
> T1:
>   Holder h1 = UnsafeSingletonFactory.getOneAndOnlyHolder();
>   unsafe = new Wrapper(h1);
>
> T2:
>   Holder h2 = UnsafeSingletonFactory.getOneAndOnlyHolder();
>
>   Wrapper w;
>   while ((w = unsafe) == null) {} // spin
>   Holder h0 = w.holder;
>
>   // at this point, runtime realizes (h0 == h2), boom
>   h2.field.toString();  // potential NPE
>
> That's why I'm saying while the final wrapper trick is cute, it has some
> preconditions to satisfy first. In terms of your ListenableFuture
> example, you cannot guarantee the Future-to-be-wrapped is not exposed
> yet, right? That means the Future-that-had-been-wrapped fields are not
> guaranteed to be seen properly.
>
>
>> So how can i get those 'freeze' semantics without introducing additional
>> boxes around every field?
> You can get freeze only with final fields. I don't know what you want me
> to say. There is no other way, as I see it: only finals [*] properly
> survive the racy publication, as far as the specification is concerned.
> So, if you cannot use finals, you have to avoid races. Scratch that,
> avoid races, and use finals where possible.
>
> Until spec mandates the final-field-like semantics for all
> initializations in constructors, there is no general solution. Once it
> does, the issue will dissolve itself.
>
> -Aleksey.
>
> [*] ...and the objects reachable *only* through them -- spec is
> complicated enough there, but it seems to mean exactly this for objects
> reachable through final fields.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/2062fc9f/attachment.html>

From oleksandr.otenko at oracle.com  Sat Jan 24 14:00:04 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Sat, 24 Jan 2015 19:00:04 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
Message-ID: <54C3EBB4.30406@oracle.com>

> It is a compile error for any such method
> to ... introduce any possibility of recursive invocation.

How would you do that?


Alex


On 23/01/2015 23:37, Justin Sampson wrote:
> Howdy,
>
> I'm curious if any proposals are being floated to somehow deal with
> the problem of StackOverflowError arising inside concurrent code.
> While writing a new synchronizer I keep obsessing over possible
> error conditions, and this is the one I can't flush out.
>
> I can carefully code around OutOfMemoryError by only allocating at
> safe places, and I can brush off ThreadDeath just by yelling at
> users to never call Thread.stop(). But StackOverflowError is the one
> kind of error that I really can't code around, because it can happen
> legitimately at any method call. The best I can do at the moment is
> to reassure myself that it's not my fault, because ReentrantLock
> has the same issue: http://bugs.java.com/view_bug.do?bug_id=7011862
>
> For the sake of discussion, here's one solution that comes to mind.
>
> First, define a "stack-limited" method to be a method that is not
> recursive and that only calls other stack-limited methods, so that
> it is possible for the JVM to determine an upper bound on its stack
> usage.
>
> Next, introduce two new standard annotations:
>
> @StackLimited on a method declaration indicates that the method is
> explicitly stack-limited. It is a compile error for any such method
> to call any other method that is not also explicitly stack-limited
> or to introduce any possibility of recursive invocation.
>
> @StackSensitive on a method declaration indicates that the method is
> explicitly stack-limited, with all the constraints of @StackLimited,
> and also tells the JVM to proactively check the available stack
> space and throw StackOverflowError immediately on entry to the
> method if there is not enough stack for all possible executions of
> the method.
>
> The idea would be to annotate the minimum set of methods to ensure
> that a synchronizer is never left in an invalid state due to stack
> overflow. That would mostly be putting @StackLimited on the methods
> of Unsafe, LockSupport, and the j.u.c.atomic package, and putting
> @StackSensitive on just one or two methods in any given synchronizer
> such as AQS.
>
> Is something like this plausible, or even already in the works?
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Sat Jan 24 15:48:48 2015
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Jan 2015 12:48:48 -0800
Subject: [concurrency-interest] Using relaxed reads and writes
In-Reply-To: <54C2F47D.7010206@cs.oswego.edu>
References: <1538772756.3228485.1421972627053.JavaMail.yahoo@jws10035.mail.ne1.yahoo.com>
	<54C2F47D.7010206@cs.oswego.edu>
Message-ID: <CA+kOe0_3i_c3CPAScVmO8gX-Fkajq5Ncw4P3H3ocX9_+u2w-Xg@mail.gmail.com>

We've been doing these kinds of optimizations in j.u.c., especially the
obvious one of using relaxed write before publishing via CAS, but we
haven't observed compelling performance improvements.  Both hotspot and the
hardware conspire to make two successive volatile writes to a single cache
line much less than twice as expensive as one?!

On Fri, Jan 23, 2015 at 5:25 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/22/2015 07:23 PM, Ben Manes wrote:
>
>> I think the following is a safe optimization for my use-case, but I'd
>> appreciate
>> any feedback / concerns.
>>
>> Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is an
>> entry
>> in an LRU cache with miscellaneous metadata fields such as the queue
>> links, the
>> value, expiration timestamps, etc. When updating the value the node must
>> be
>> synchronized on in order to block if created by a concurrent computation.
>> This
>> means the more optimal CAS loop isn't used, which may have affect on
>> simplifying
>> the following discussion. Lets say that the Node's value and accessTime
>> fields
>> are volatile.
>>
>> When writing the value the node's lock must be held (or implicitly safe
>> due to
>> Node construction). In that case I think that a relaxed write
>> (Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for
>> visibility (or publishing via insert if created).
>>
>
> Yes, in fact hotspot often does this optimization for you,
> at least on x86 and sparc, although not if biased locking
> is enabled. It may not be worthwhile to hand-optimize.
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/19559afb/attachment-0001.html>

From martinrb at google.com  Sat Jan 24 15:56:11 2015
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 Jan 2015 12:56:11 -0800
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
Message-ID: <CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>

I think COWAL (and other classes in j.u.c.) could/should use builtin
synchronization as you suggest, to save 32 bytes (Doug, do you agree?).

The current situation is historical:
- j.u.c. classes used their own ReentrantLock as "dogfood" exercise before
integration into the JDK
- ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not in
later releases
- many j.u.c. classes use features of ReentrantLock that builtin monitors
do not provide, like condition objects.

On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
sitnikov.vladimir at gmail.com> wrote:

> Hi,
>
> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
> it looks strange to see ReentrantLock there.
> According to jol (see [1]), COWAL consumes 88 bytes.
>
> It looks like excessive garbage for no apparent reason.
>
> There are two questions:
> 1) Can ReentrantLock be eliminated in favor of synchronized(something)?
> I believe a change of "ReentrantLock lock" to "Object lock=new
> Object()" would reduce the footprint without loss of existing
> semantics.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/30213bbc/attachment.html>

From vitalyd at gmail.com  Sat Jan 24 16:31:31 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Jan 2015 16:31:31 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>

I'm having difficulty imagining how JIT, upon entry into a function, could
verify that all call chains within the body will not blow the stack.
There's control flow, unreachable code that turns reachable, etc that it
needs to account for.  It may have to load classes that it wouldn't have to
otherwise in order to peek at methods' bytecode, and so on.

If you want to harden your lib, SOE should be easier to tackle as it's at
least local to your thread of execution; OOM is a bit more challenging.

Common wisdom is you restart your app if you hit OOM/SOE as even if your
lib handles it, it's unlikely callers do.

sent from my phone
On Jan 24, 2015 12:44 AM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> Vitaly Davidovich wrote:
>
> > Ahhh, the noble goal of hardening a system against async
> > exceptions like OOM and SOE. :) Honestly, I've yet to see code
> > that is 100% hardened against OOM. Frankly, I'm not sure this is
> > possible in java given there's a big machine that's running
> > beneath you (i.e. JVM, which can throw OOM in places you wouldn't
> > necessary expect), and this goes for SOE as well. I'd love to be
> > proven wrong though.
>
> To be clear, I'm not talking about hardening the whole system
> against arbitrary async exceptions. OOME and SOE _can_ be thrown
> asynchronously, but there are also well-defined places that they
> happen synchronously. It seems at least plausible to be able to
> harden certain low-level library code against such synchronous
> exceptions, especially certain blocks of code within synchronizers
> where there's a risk of leaving the synchronizer locked.
>
> I've searched a little in the archives and I can see the issue at
> least mentioned a couple of times, but wanted to get a sense of
> whether any solutions were being considered. The links that
> Alexander sent are very interesting history! -- but they're about
> C++ and I couldn't quite ascertain the final outcome.
>
> To make the idea even more constrained, imagine a method that only
> calls other final or static Java methods or intrinsified native
> methods, whether directly or indirectly, without any recursion, none
> of which do any heap allocation (no new objects, arrays, boxing,
> lambdas, varargs, etc.). Conceptually, the JIT could inline the
> whole call tree into a single method body with a single stack frame.
>
> Any call to such a method would throw SOE right up front if the
> stack has been exhausted, avoiding any SOE's in the middle. (Any
> class loading and initialization that might be triggered by the body
> of the method would have to be executed immediately on entry to the
> method as well, since they can trigger arbitrary exceptions and
> errors.)
>
> The strawman proposal in my prior email was simply to provide an
> annotation that would request for the JVM to implement such
> fail-fast behavior for the annotated methods, with compile-time
> verification that it should be possible for the runtime to do so.
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/1c88039f/attachment.html>

From sub at laerad.com  Sat Jan 24 16:59:09 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Sat, 24 Jan 2015 21:59:09 +0000
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
Message-ID: <CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>

For lists smaller than a (low) preconfigured limit, why not use a simple
CAS-loop? When above this size a lock (of either variety) could be
inflated, at which point the memory cost is much less significant since it
is no longer dwarfing the other state.

It may even yield superior performance to boot.

On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com> wrote:

> I think COWAL (and other classes in j.u.c.) could/should use builtin
> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>
> The current situation is historical:
> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise before
> integration into the JDK
> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not
> in later releases
> - many j.u.c. classes use features of ReentrantLock that builtin monitors
> do not provide, like condition objects.
>
> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
> sitnikov.vladimir at gmail.com> wrote:
>
>> Hi,
>>
>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>> it looks strange to see ReentrantLock there.
>> According to jol (see [1]), COWAL consumes 88 bytes.
>>
>> It looks like excessive garbage for no apparent reason.
>>
>> There are two questions:
>> 1) Can ReentrantLock be eliminated in favor of synchronized(something)?
>> I believe a change of "ReentrantLock lock" to "Object lock=new
>> Object()" would reduce the footprint without loss of existing
>> semantics.
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/bef3de84/attachment.html>

From TEREKHOV at de.ibm.com  Sat Jan 24 17:46:11 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 24 Jan 2015 23:46:11 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
Message-ID: <OFE55E2C31.ABD99A68-ONC1257DD7.007CF2B5-C1257DD7.007D149A@de.ibm.com>

> I couldn't quite ascertain the final outcome.

Well, C++ still has no support for no_cancel { } to suppress thread
cancelation inside the scope (and restore previous state by exiting the
scope), no support for no_throw { } being the same as no_cancel { } and in
addition making that any unexpected/unhandled exception immediately results
in abort() at throw point without stack unwinding, no support for
async_cancel_safe { } to enable async cancel (if cancel was not disabled
upstack) and expressing that only async cancel safe operations can be coded
inside the scope, no support for meaningful exception specifications
providing fencing from unexpected exceptions, etc.

regards,
alexander.

Justin Sampson <jsampson at guidewire.com> on 24.01.2015 06:44:33

To:	Vitaly Davidovich <vitalyd at gmail.com>, Alexander
       Terekhov/Germany/IBM at IBMDE
cc:	"concurrency-interest at cs.oswego.edu"
       <concurrency-interest at cs.oswego.edu>
Subject:	RE: [concurrency-interest] JVM support to avoid
       StackOverflowError


Vitaly Davidovich wrote:

> Ahhh, the noble goal of hardening a system against async
> exceptions like OOM and SOE. :) Honestly, I've yet to see code
> that is 100% hardened against OOM. Frankly, I'm not sure this is
> possible in java given there's a big machine that's running
> beneath you (i.e. JVM, which can throw OOM in places you wouldn't
> necessary expect), and this goes for SOE as well. I'd love to be
> proven wrong though.

To be clear, I'm not talking about hardening the whole system
against arbitrary async exceptions. OOME and SOE _can_ be thrown
asynchronously, but there are also well-defined places that they
happen synchronously. It seems at least plausible to be able to
harden certain low-level library code against such synchronous
exceptions, especially certain blocks of code within synchronizers
where there's a risk of leaving the synchronizer locked.

I've searched a little in the archives and I can see the issue at
least mentioned a couple of times, but wanted to get a sense of
whether any solutions were being considered. The links that
Alexander sent are very interesting history! -- but they're about
C++ and I couldn't quite ascertain the final outcome.

To make the idea even more constrained, imagine a method that only
calls other final or static Java methods or intrinsified native
methods, whether directly or indirectly, without any recursion, none
of which do any heap allocation (no new objects, arrays, boxing,
lambdas, varargs, etc.). Conceptually, the JIT could inline the
whole call tree into a single method body with a single stack frame.

Any call to such a method would throw SOE right up front if the
stack has been exhausted, avoiding any SOE's in the middle. (Any
class loading and initialization that might be triggered by the body
of the method would have to be executed immediately on entry to the
method as well, since they can trigger arbitrary exceptions and
errors.)

The strawman proposal in my prior email was simply to provide an
annotation that would request for the JVM to implement such
fail-fast behavior for the annotated methods, with compile-time
verification that it should be possible for the runtime to do so.

Cheers,
Justin



From vitalyd at gmail.com  Sat Jan 24 19:05:06 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Jan 2015 19:05:06 -0500
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
Message-ID: <CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>

Why not *always* use a CAS loop? The use case for COWAL is such that
mutation is fairly rare and should not have contention.

sent from my phone
On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:

> For lists smaller than a (low) preconfigured limit, why not use a simple
> CAS-loop? When above this size a lock (of either variety) could be
> inflated, at which point the memory cost is much less significant since it
> is no longer dwarfing the other state.
>
> It may even yield superior performance to boot.
>
> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com> wrote:
>
>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>
>> The current situation is historical:
>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>> before integration into the JDK
>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not
>> in later releases
>> - many j.u.c. classes use features of ReentrantLock that builtin monitors
>> do not provide, like condition objects.
>>
>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>> sitnikov.vladimir at gmail.com> wrote:
>>
>>> Hi,
>>>
>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>> it looks strange to see ReentrantLock there.
>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>
>>> It looks like excessive garbage for no apparent reason.
>>>
>>> There are two questions:
>>> 1) Can ReentrantLock be eliminated in favor of synchronized(something)?
>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>> Object()" would reduce the footprint without loss of existing
>>> semantics.
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/be4320b7/attachment-0001.html>

From sub at laerad.com  Sat Jan 24 19:16:38 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Sun, 25 Jan 2015 00:16:38 +0000
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
Message-ID: <CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>

Changing this under the feet of existing users could be problematic in some
scenarios. As the list grows the cost under contention scales
disproportionately, as both the incidence and cost of failed CAS increases.
There's not a lot to be gained from CAS-spinning on modification to a list
with tens of thousands of elements, and for even larger lists there could
be significant costs. There are likely users with such lists. Below that
size, there's still the potential for significant garbage allocation, which
may be undesirable by comparison to accepting lower modification speed.


On 25 January 2015 at 00:05, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Why not *always* use a CAS loop? The use case for COWAL is such that
> mutation is fairly rare and should not have contention.
>
> sent from my phone
> On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:
>
>> For lists smaller than a (low) preconfigured limit, why not use a simple
>> CAS-loop? When above this size a lock (of either variety) could be
>> inflated, at which point the memory cost is much less significant since it
>> is no longer dwarfing the other state.
>>
>> It may even yield superior performance to boot.
>>
>> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com> wrote:
>>
>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>
>>> The current situation is historical:
>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>> before integration into the JDK
>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not
>>> in later releases
>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>> monitors do not provide, like condition objects.
>>>
>>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>>> sitnikov.vladimir at gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>>> it looks strange to see ReentrantLock there.
>>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>>
>>>> It looks like excessive garbage for no apparent reason.
>>>>
>>>> There are two questions:
>>>> 1) Can ReentrantLock be eliminated in favor of synchronized(something)?
>>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>>> Object()" would reduce the footprint without loss of existing
>>>> semantics.
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/61189d44/attachment.html>

From vitalyd at gmail.com  Sat Jan 24 19:25:00 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Jan 2015 19:25:00 -0500
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
Message-ID: <CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>

As the list grows, CAS only becomes a problem if you have lots of mutator
threads involved.  Yes, the window during which someone can come in and
change the list while you're doing a copy is expanded, but this is a
memcpy-like operation behind the scenes.  There may be users out there
mutating these things extensively, but they're probably already feeling the
GC effects.

The upside of CAS is there's no additional storage cost for any type of
lock, and if one were to use builtin monitor and it inflates, I think that
has some effect on GC where inflated monitors are reclaimed.

sent from my phone
On Jan 24, 2015 7:16 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:

> Changing this under the feet of existing users could be problematic in
> some scenarios. As the list grows the cost under contention scales
> disproportionately, as both the incidence and cost of failed CAS increases.
> There's not a lot to be gained from CAS-spinning on modification to a list
> with tens of thousands of elements, and for even larger lists there could
> be significant costs. There are likely users with such lists. Below that
> size, there's still the potential for significant garbage allocation, which
> may be undesirable by comparison to accepting lower modification speed.
>
>
> On 25 January 2015 at 00:05, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>> Why not *always* use a CAS loop? The use case for COWAL is such that
>> mutation is fairly rare and should not have contention.
>>
>> sent from my phone
>> On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:
>>
>>> For lists smaller than a (low) preconfigured limit, why not use a simple
>>> CAS-loop? When above this size a lock (of either variety) could be
>>> inflated, at which point the memory cost is much less significant since it
>>> is no longer dwarfing the other state.
>>>
>>> It may even yield superior performance to boot.
>>>
>>> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com>
>>> wrote:
>>>
>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>
>>>> The current situation is historical:
>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>> before integration into the JDK
>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>> not in later releases
>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>> monitors do not provide, like condition objects.
>>>>
>>>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>>>> sitnikov.vladimir at gmail.com> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>>>> it looks strange to see ReentrantLock there.
>>>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>>>
>>>>> It looks like excessive garbage for no apparent reason.
>>>>>
>>>>> There are two questions:
>>>>> 1) Can ReentrantLock be eliminated in favor of synchronized(something)?
>>>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>>>> Object()" would reduce the footprint without loss of existing
>>>>> semantics.
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/529bbc76/attachment.html>

From sub at laerad.com  Sat Jan 24 19:30:35 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Sun, 25 Jan 2015 00:30:35 +0000
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
Message-ID: <CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>

Right. My point is not everyone uses the API "correctly" (i.e. in
situations with mutator counts approaching zero), and these users could be
badly affected by such a change. Since once you get above a few tens of
thousands of items the costs are on the same order of magnitude as
parking/unparking, but without any risk associated with many competing
updates, it seems reasonable to leave the current behaviour in place at
least in this situation.

On 25 January 2015 at 00:25, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> As the list grows, CAS only becomes a problem if you have lots of mutator
> threads involved.  Yes, the window during which someone can come in and
> change the list while you're doing a copy is expanded, but this is a
> memcpy-like operation behind the scenes.  There may be users out there
> mutating these things extensively, but they're probably already feeling the
> GC effects.
>
> The upside of CAS is there's no additional storage cost for any type of
> lock, and if one were to use builtin monitor and it inflates, I think that
> has some effect on GC where inflated monitors are reclaimed.
>
> sent from my phone
> On Jan 24, 2015 7:16 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:
>
>> Changing this under the feet of existing users could be problematic in
>> some scenarios. As the list grows the cost under contention scales
>> disproportionately, as both the incidence and cost of failed CAS increases.
>> There's not a lot to be gained from CAS-spinning on modification to a list
>> with tens of thousands of elements, and for even larger lists there could
>> be significant costs. There are likely users with such lists. Below that
>> size, there's still the potential for significant garbage allocation, which
>> may be undesirable by comparison to accepting lower modification speed.
>>
>>
>> On 25 January 2015 at 00:05, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>>> Why not *always* use a CAS loop? The use case for COWAL is such that
>>> mutation is fairly rare and should not have contention.
>>>
>>> sent from my phone
>>> On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com>
>>> wrote:
>>>
>>>> For lists smaller than a (low) preconfigured limit, why not use a
>>>> simple CAS-loop? When above this size a lock (of either variety) could be
>>>> inflated, at which point the memory cost is much less significant since it
>>>> is no longer dwarfing the other state.
>>>>
>>>> It may even yield superior performance to boot.
>>>>
>>>> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com>
>>>> wrote:
>>>>
>>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>>
>>>>> The current situation is historical:
>>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>>> before integration into the JDK
>>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>>> not in later releases
>>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>>> monitors do not provide, like condition objects.
>>>>>
>>>>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>>>>> sitnikov.vladimir at gmail.com> wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>>>>> it looks strange to see ReentrantLock there.
>>>>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>>>>
>>>>>> It looks like excessive garbage for no apparent reason.
>>>>>>
>>>>>> There are two questions:
>>>>>> 1) Can ReentrantLock be eliminated in favor of
>>>>>> synchronized(something)?
>>>>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>>>>> Object()" would reduce the footprint without loss of existing
>>>>>> semantics.
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/1f1f69e5/attachment-0001.html>

From vitalyd at gmail.com  Sat Jan 24 19:41:01 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 24 Jan 2015 19:41:01 -0500
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
	<CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
Message-ID: <CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>

Well, I guess I'm less sympathetic to misusers :) I was thinking for the
intended (and majority, I'd hope) use cases, you get the best memory wise
outcome by not having any lock object.

The issue with CAS below some threshold and lock above is you still need
the lock object; if you allocate it lazily (with CAS!), you'd still pay
price of reserving (possibly compressed) size of pointer.  And if you
shrink the list back down below threshold, do you null out the lock or keep
it? You'd also introduce a branch into these methods.

Also, I haven't verified this, but at current memory bandwidths, I doubt
memcpy cost of a few tens of thousands of pointers is equal to park and
unpark costs.

Anyway, I'm +1 on at least replacing lock with builtin monitor.

sent from my phone
On Jan 24, 2015 7:30 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:

> Right. My point is not everyone uses the API "correctly" (i.e. in
> situations with mutator counts approaching zero), and these users could be
> badly affected by such a change. Since once you get above a few tens of
> thousands of items the costs are on the same order of magnitude as
> parking/unparking, but without any risk associated with many competing
> updates, it seems reasonable to leave the current behaviour in place at
> least in this situation.
>
> On 25 January 2015 at 00:25, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>> As the list grows, CAS only becomes a problem if you have lots of mutator
>> threads involved.  Yes, the window during which someone can come in and
>> change the list while you're doing a copy is expanded, but this is a
>> memcpy-like operation behind the scenes.  There may be users out there
>> mutating these things extensively, but they're probably already feeling the
>> GC effects.
>>
>> The upside of CAS is there's no additional storage cost for any type of
>> lock, and if one were to use builtin monitor and it inflates, I think that
>> has some effect on GC where inflated monitors are reclaimed.
>>
>> sent from my phone
>> On Jan 24, 2015 7:16 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:
>>
>>> Changing this under the feet of existing users could be problematic in
>>> some scenarios. As the list grows the cost under contention scales
>>> disproportionately, as both the incidence and cost of failed CAS increases.
>>> There's not a lot to be gained from CAS-spinning on modification to a list
>>> with tens of thousands of elements, and for even larger lists there could
>>> be significant costs. There are likely users with such lists. Below that
>>> size, there's still the potential for significant garbage allocation, which
>>> may be undesirable by comparison to accepting lower modification speed.
>>>
>>>
>>> On 25 January 2015 at 00:05, Vitaly Davidovich <vitalyd at gmail.com>
>>> wrote:
>>>
>>>> Why not *always* use a CAS loop? The use case for COWAL is such that
>>>> mutation is fairly rare and should not have contention.
>>>>
>>>> sent from my phone
>>>> On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com>
>>>> wrote:
>>>>
>>>>> For lists smaller than a (low) preconfigured limit, why not use a
>>>>> simple CAS-loop? When above this size a lock (of either variety) could be
>>>>> inflated, at which point the memory cost is much less significant since it
>>>>> is no longer dwarfing the other state.
>>>>>
>>>>> It may even yield superior performance to boot.
>>>>>
>>>>> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com>
>>>>> wrote:
>>>>>
>>>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>>>
>>>>>> The current situation is historical:
>>>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>>>> before integration into the JDK
>>>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>>>> not in later releases
>>>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>>>> monitors do not provide, like condition objects.
>>>>>>
>>>>>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>>>>>> sitnikov.vladimir at gmail.com> wrote:
>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>>>>>> it looks strange to see ReentrantLock there.
>>>>>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>>>>>
>>>>>>> It looks like excessive garbage for no apparent reason.
>>>>>>>
>>>>>>> There are two questions:
>>>>>>> 1) Can ReentrantLock be eliminated in favor of
>>>>>>> synchronized(something)?
>>>>>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>>>>>> Object()" would reduce the footprint without loss of existing
>>>>>>> semantics.
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150124/236d7b74/attachment.html>

From sub at laerad.com  Sat Jan 24 20:24:03 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Sun, 25 Jan 2015 01:24:03 +0000
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
	<CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
	<CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>
Message-ID: <CACr06N1+KJZB=3VSbVw3RWuM07wC-CbfGs1wndp1VywDsjp_vA@mail.gmail.com>

For modifications by index, quite likely. But since that's not going to be
atomic, the only such operation commonly used is likely to be append,
surely? Many of the other common operations likely entail less efficient
work.

Due to object padding, the reference for the unallocated lock is free on
many VMs, but even if not it's pretty insignificant by comparison to the
other fixed costs.
On 25 Jan 2015 00:41, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Well, I guess I'm less sympathetic to misusers :) I was thinking for the
> intended (and majority, I'd hope) use cases, you get the best memory wise
> outcome by not having any lock object.
>
> The issue with CAS below some threshold and lock above is you still need
> the lock object; if you allocate it lazily (with CAS!), you'd still pay
> price of reserving (possibly compressed) size of pointer.  And if you
> shrink the list back down below threshold, do you null out the lock or keep
> it? You'd also introduce a branch into these methods.
>
> Also, I haven't verified this, but at current memory bandwidths, I doubt
> memcpy cost of a few tens of thousands of pointers is equal to park and
> unpark costs.
>
> Anyway, I'm +1 on at least replacing lock with builtin monitor.
>
> sent from my phone
> On Jan 24, 2015 7:30 PM, "Benedict Elliott Smith" <sub at laerad.com> wrote:
>
>> Right. My point is not everyone uses the API "correctly" (i.e. in
>> situations with mutator counts approaching zero), and these users could be
>> badly affected by such a change. Since once you get above a few tens of
>> thousands of items the costs are on the same order of magnitude as
>> parking/unparking, but without any risk associated with many competing
>> updates, it seems reasonable to leave the current behaviour in place at
>> least in this situation.
>>
>> On 25 January 2015 at 00:25, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>>> As the list grows, CAS only becomes a problem if you have lots of
>>> mutator threads involved.  Yes, the window during which someone can come in
>>> and change the list while you're doing a copy is expanded, but this is a
>>> memcpy-like operation behind the scenes.  There may be users out there
>>> mutating these things extensively, but they're probably already feeling the
>>> GC effects.
>>>
>>> The upside of CAS is there's no additional storage cost for any type of
>>> lock, and if one were to use builtin monitor and it inflates, I think that
>>> has some effect on GC where inflated monitors are reclaimed.
>>>
>>> sent from my phone
>>> On Jan 24, 2015 7:16 PM, "Benedict Elliott Smith" <sub at laerad.com>
>>> wrote:
>>>
>>>> Changing this under the feet of existing users could be problematic in
>>>> some scenarios. As the list grows the cost under contention scales
>>>> disproportionately, as both the incidence and cost of failed CAS increases.
>>>> There's not a lot to be gained from CAS-spinning on modification to a list
>>>> with tens of thousands of elements, and for even larger lists there could
>>>> be significant costs. There are likely users with such lists. Below that
>>>> size, there's still the potential for significant garbage allocation, which
>>>> may be undesirable by comparison to accepting lower modification speed.
>>>>
>>>>
>>>> On 25 January 2015 at 00:05, Vitaly Davidovich <vitalyd at gmail.com>
>>>> wrote:
>>>>
>>>>> Why not *always* use a CAS loop? The use case for COWAL is such that
>>>>> mutation is fairly rare and should not have contention.
>>>>>
>>>>> sent from my phone
>>>>> On Jan 24, 2015 5:22 PM, "Benedict Elliott Smith" <sub at laerad.com>
>>>>> wrote:
>>>>>
>>>>>> For lists smaller than a (low) preconfigured limit, why not use a
>>>>>> simple CAS-loop? When above this size a lock (of either variety) could be
>>>>>> inflated, at which point the memory cost is much less significant since it
>>>>>> is no longer dwarfing the other state.
>>>>>>
>>>>>> It may even yield superior performance to boot.
>>>>>>
>>>>>> On 24 January 2015 at 20:56, Martin Buchholz <martinrb at google.com>
>>>>>> wrote:
>>>>>>
>>>>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>>>>
>>>>>>> The current situation is historical:
>>>>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>>>>> before integration into the JDK
>>>>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>>>>> not in later releases
>>>>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>>>>> monitors do not provide, like condition objects.
>>>>>>>
>>>>>>> On Sat, Jan 24, 2015 at 9:58 AM, Vladimir Sitnikov <
>>>>>>> sitnikov.vladimir at gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> I am looking into sources of CopyOnWriteArrayList in java 1.8u40 and
>>>>>>>> it looks strange to see ReentrantLock there.
>>>>>>>> According to jol (see [1]), COWAL consumes 88 bytes.
>>>>>>>>
>>>>>>>> It looks like excessive garbage for no apparent reason.
>>>>>>>>
>>>>>>>> There are two questions:
>>>>>>>> 1) Can ReentrantLock be eliminated in favor of
>>>>>>>> synchronized(something)?
>>>>>>>> I believe a change of "ReentrantLock lock" to "Object lock=new
>>>>>>>> Object()" would reduce the footprint without loss of existing
>>>>>>>> semantics.
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/6e91cf8b/attachment-0001.html>

From ben_manes at yahoo.com  Sat Jan 24 21:49:24 2015
From: ben_manes at yahoo.com (Ben Manes)
Date: Sun, 25 Jan 2015 02:49:24 +0000 (UTC)
Subject: [concurrency-interest] Using relaxed reads and writes
In-Reply-To: <CA+kOe0_3i_c3CPAScVmO8gX-Fkajq5Ncw4P3H3ocX9_+u2w-Xg@mail.gmail.com>
References: <CA+kOe0_3i_c3CPAScVmO8gX-Fkajq5Ncw4P3H3ocX9_+u2w-Xg@mail.gmail.com>
Message-ID: <155566267.18978.1422154164529.JavaMail.yahoo@mail.yahoo.com>

Yes, your work there is what made me consider adopting that optimization to a greater extent in this rewrite project. Similarly your thread on memory barriers got me to finally read up on the specifics to wrap my head around how to use those constructs as a model for reasoning about code. While I don't think I'll go down the route of using explicit Unsafe fence instructions anytime soon, its been valuable insight for using relaxed reads and writes more broadly.
There are a lot of tricks in j.u.c. that I happily steal, like JDK7 Exchanger's spinning heuristics were helpful when writing a elimination/combining arenas (stacks and queues). Similarly, I have plans for adapting Stripe64's resizing when contention is detected technique in a few key areas, such as arenas and cache buffer management. When the project is near completion, I'll probably try to see if you and Doug?would be gracious enough to review the algorithms at a high level to see if there are any further insights that I can take advantage of.
So thanks a lot for all the great ideas! =) 

     On Saturday, January 24, 2015 1:13 PM, Martin Buchholz <martinrb at google.com> wrote:
   

 We've been doing these kinds of optimizations in j.u.c., especially the obvious one of using relaxed write before publishing via CAS, but we haven't observed compelling performance improvements.? Both hotspot and the hardware conspire to make two successive volatile writes to a single cache line much less than twice as expensive as one?!

On Fri, Jan 23, 2015 at 5:25 PM, Doug Lea <dl at cs.oswego.edu> wrote:

On 01/22/2015 07:23 PM, Ben Manes wrote:

I think the following is a safe optimization for my use-case, but I'd appreciate
any feedback / concerns.

Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is an entry
in an LRU cache with miscellaneous metadata fields such as the queue links, the
value, expiration timestamps, etc. When updating the value the node must be
synchronized on in order to block if created by a concurrent computation. This
means the more optimal CAS loop isn't used, which may have affect on simplifying
the following discussion. Lets say that the Node's value and accessTime fields
are volatile.

When writing the value the node's lock must be held (or implicitly safe due to
Node construction). In that case I think that a relaxed write
(Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for
visibility (or publishing via insert if created).


Yes, in fact hotspot often does this optimization for you,
at least on x86 and sparc, although not if biased locking
is enabled. It may not be worthwhile to hand-optimize.

-Doug



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


   
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/19848dc2/attachment.html>

From sitnikov.vladimir at gmail.com  Sun Jan 25 02:51:10 2015
From: sitnikov.vladimir at gmail.com (Vladimir Sitnikov)
Date: Sun, 25 Jan 2015 11:51:10 +0400
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CACr06N1+KJZB=3VSbVw3RWuM07wC-CbfGs1wndp1VywDsjp_vA@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
	<CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
	<CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>
	<CACr06N1+KJZB=3VSbVw3RWuM07wC-CbfGs1wndp1VywDsjp_vA@mail.gmail.com>
Message-ID: <CAB=Je-GoewSVKcnVSaqM9guJzYrOddZg358oz9cCpJbdHHs-0Q@mail.gmail.com>

> I was thinking for the intended (and majority, I'd hope) use cases, you get the best memory wise outcome by not having any lock object.

At this point I agree with Vitaly. I can hardly imagine a valid use
case for highly contented COWAL.
Not having any lock object would be nice.

> For modifications by index, quite likely

Even modification by index requires a copy, so it is quite odd if it
is used frequently on a large COWAL.

>Due to object padding, the reference for the unallocated lock is free on many VMs

It is free only on 32bit VM. According to jol, all the other (64,
64coop, 64coop + 16byte alignment) do not have "spare" space if a
class has just a single reference field.

Vladimir

From akarnokd at gmail.com  Sun Jan 25 03:57:08 2015
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Sun, 25 Jan 2015 09:57:08 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54C002A6.9060204@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
	<54BD0ED7.50907@cs.oswego.edu>
	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
	<54BD2585.5040903@cs.oswego.edu>
	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
	<54BD74C7.3020500@cs.oswego.edu> <54C002A6.9060204@cs.oswego.edu>
Message-ID: <CAAWwtm9YWn5Dix=jp851tX7m8tMX7LVLZaLGgQMtyQCppOO3Bw@mail.gmail.com>

Hello again. I'm reviewing the SubmissionPublisher and found the following
problems:

- Subscribing a subscriber again: the code emits an onError asynchronously
to the already running subscriber which violates the Subscriber contract. I
suggest throwing the ISE in subscribe directly.

- getSubscribers: I'm not sure if this is a good idea because calling onXXX
methods on these active and async subscribers will encourage violating the
Subscriber contract.

-David

Doug Lea <dl at cs.oswego.edu> ezt ?rta (2015. janu?r 21., szerda):

>
> Thanks to a few people putting up with daily API and functionality
> changes to the in-progress SubmissionPublisher class, which is
> slowly stabilizing enough that others might want to try it out
> as well. Current javadoc at
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
> SubmissionPublisher.html
> And you can run with jdk8+ with -Xbootclasspath/p:jsr166.jar from
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
> For fun, I also placed a snapshot of one of my little internal
> development perf tests at http://gee.cs.oswego.edu/dl/wwwtmp/SPL4.java
>
> It shows throughput of a bunch of 3-stage flows. (The test doesn't
> use JMH, to simplify transiently adding and removing internal
> instrumentation while developing.)
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/d64ad97c/attachment.html>

From sub at laerad.com  Sun Jan 25 05:02:57 2015
From: sub at laerad.com (Benedict Elliott Smith)
Date: Sun, 25 Jan 2015 10:02:57 +0000
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAB=Je-GoewSVKcnVSaqM9guJzYrOddZg358oz9cCpJbdHHs-0Q@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
	<CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
	<CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>
	<CACr06N1+KJZB=3VSbVw3RWuM07wC-CbfGs1wndp1VywDsjp_vA@mail.gmail.com>
	<CAB=Je-GoewSVKcnVSaqM9guJzYrOddZg358oz9cCpJbdHHs-0Q@mail.gmail.com>
Message-ID: <CACr06N0GAC0Em3p+MsWur8UKdzKxmS+LpOq=QKSziH3z7k7b8w@mail.gmail.com>

>
> Even modification by index requires a copy, so it is quite odd if it is
> used frequently on a large COWAL.


All modifications to a large COWAL are expensive. My point is that many
will be more expensive than a simple copy.

It is free only on 32bit VM.


You are correct; my mistake. This is amounts to 25% of an empty COWAL, and
20% of a list with a single element. The REL is 200% and 160% respectively,
so the majority of the cost is saved. By the time there are a handful of
elements it's not really that significant.

That said, I don't have a strong opinion either way. It just seems it would
be one of those changes that could create headaches for users unexpectedly,
given its behaviour under contention would be significantly different. The
size of the affected cohort is tough to predict, as APIs are abused
universally. Since this is an internal characteristic not explicitly
defined by the API, but with wildly different behaviour, it seems
especially prone to the problem. The small cost necessary to ensure it
never degrades too severely from the existing behaviour might be warranted.



On 25 January 2015 at 07:51, Vladimir Sitnikov <sitnikov.vladimir at gmail.com>
wrote:

> > I was thinking for the intended (and majority, I'd hope) use cases, you
> get the best memory wise outcome by not having any lock object.
>
> At this point I agree with Vitaly. I can hardly imagine a valid use
> case for highly contented COWAL.
> Not having any lock object would be nice.
>
> > For modifications by index, quite likely
>
> Even modification by index requires a copy, so it is quite odd if it
> is used frequently on a large COWAL.
>
> >Due to object padding, the reference for the unallocated lock is free on
> many VMs
>
> It is free only on 32bit VM. According to jol, all the other (64,
> 64coop, 64coop + 16byte alignment) do not have "spare" space if a
> class has just a single reference field.
>
> Vladimir
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/02f03398/attachment-0001.html>

From viktor.klang at gmail.com  Sun Jan 25 06:08:23 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Sun, 25 Jan 2015 12:08:23 +0100
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CACr06N0GAC0Em3p+MsWur8UKdzKxmS+LpOq=QKSziH3z7k7b8w@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<CACr06N2L-9g9729vTdp8RLmycgsJH4JKtHPeKfw=VmST7De+cQ@mail.gmail.com>
	<CAHjP37HHd+atP74XNsbmOyNsPGKFCo90rDzWmJ4mrzvZWE=fkg@mail.gmail.com>
	<CACr06N1VyvY+g-7xLmnHmeX3hVrNb9H-iAVJySjrAHiA84U-RQ@mail.gmail.com>
	<CAHjP37F-M2tL2jBW6SqC9ugffAsU1z_4Wcn8407WjJf_Bz+h2w@mail.gmail.com>
	<CACr06N2Tpe=8HTsvqpY1OB0TtyN2oWrkc8=uHmUwLLb69pZ1Ew@mail.gmail.com>
	<CAHjP37FekWuap5zgk3_i5Rr8Fxgi6Ujefb39dtz526cuv3zXmQ@mail.gmail.com>
	<CACr06N1+KJZB=3VSbVw3RWuM07wC-CbfGs1wndp1VywDsjp_vA@mail.gmail.com>
	<CAB=Je-GoewSVKcnVSaqM9guJzYrOddZg358oz9cCpJbdHHs-0Q@mail.gmail.com>
	<CACr06N0GAC0Em3p+MsWur8UKdzKxmS+LpOq=QKSziH3z7k7b8w@mail.gmail.com>
Message-ID: <CANPzfU-SvxKYN5k9Mbd134CD9EVtszMHDqDYo_RccNTPD7NUKQ@mail.gmail.com>

Not suggesting it but there's also the option of using its own monitor.

-- 
Cheers,
?
On 25 Jan 2015 11:25, "Benedict Elliott Smith" <sub at laerad.com> wrote:

> Even modification by index requires a copy, so it is quite odd if it is
>> used frequently on a large COWAL.
>
>
> All modifications to a large COWAL are expensive. My point is that many
> will be more expensive than a simple copy.
>
> It is free only on 32bit VM.
>
>
> You are correct; my mistake. This is amounts to 25% of an empty COWAL, and
> 20% of a list with a single element. The REL is 200% and 160% respectively,
> so the majority of the cost is saved. By the time there are a handful of
> elements it's not really that significant.
>
> That said, I don't have a strong opinion either way. It just seems it
> would be one of those changes that could create headaches for users
> unexpectedly, given its behaviour under contention would be significantly
> different. The size of the affected cohort is tough to predict, as APIs are
> abused universally. Since this is an internal characteristic not explicitly
> defined by the API, but with wildly different behaviour, it seems
> especially prone to the problem. The small cost necessary to ensure it
> never degrades too severely from the existing behaviour might be warranted.
>
>
>
> On 25 January 2015 at 07:51, Vladimir Sitnikov <
> sitnikov.vladimir at gmail.com> wrote:
>
>> > I was thinking for the intended (and majority, I'd hope) use cases, you
>> get the best memory wise outcome by not having any lock object.
>>
>> At this point I agree with Vitaly. I can hardly imagine a valid use
>> case for highly contented COWAL.
>> Not having any lock object would be nice.
>>
>> > For modifications by index, quite likely
>>
>> Even modification by index requires a copy, so it is quite odd if it
>> is used frequently on a large COWAL.
>>
>> >Due to object padding, the reference for the unallocated lock is free on
>> many VMs
>>
>> It is free only on 32bit VM. According to jol, all the other (64,
>> 64coop, 64coop + 16byte alignment) do not have "spare" space if a
>> class has just a single reference field.
>>
>> Vladimir
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/9951c921/attachment.html>

From jsampson at guidewire.com  Sun Jan 25 07:06:51 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sun, 25 Jan 2015 12:06:51 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>

Consider this incredibly simple lock implementation:

public final class SimpleLock extends AbstractOwnableSynchronizer {
  private final AtomicBoolean state = new AtomicBoolean();
  public final boolean tryLock() {
    if (state.compareAndSet(false, true)) {
      setExclusiveOwnerThread(Thread.currentThread());
      return true;
    } else {
      return false;
    }
  }
  public final void unlock() {
    if (Thread.currentThread() != getExclusiveOwnerThread()) {
      throw new IllegalMonitorStateException();
    } else {
      setExclusiveOwnerThread(null);
      state.set(false);
    }
  }
}

It's possible for the CAS in tryLock() to succeed and then for the
very next line to blow up with a synchronous StackOverflowError,
leaving the state variable true even though tryLock() is throwing.
This is the only kind of case that I'm looking for advice about,
nothing more general.

So far the only responses have been "it's impossible to do anything
so don't even try." If that's really the end of it then I'll rest
easy knowing I've done my due diligence.

But it seems so trivial to recognize that this tryLock() method
doesn't do any heap allocation, isn't recursive, and only calls
static or final methods, most of them being intrinsic. So I was just
wondering if it might be plausible to annotate such a method, as a
request to the compiler to confirm those facts and as a request to
the JVM to fail fast on entry by allocating enough stack space up
front.

I wasn't imagining any kind of suppression of errors or more general
handling of arbitrarily complicated code, just avoidance of common
errors by failing fast when it's easy to do so.

Cheers,
Justin


From dl at cs.oswego.edu  Sun Jan 25 10:25:19 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Jan 2015 10:25:19 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <CAAWwtm9YWn5Dix=jp851tX7m8tMX7LVLZaLGgQMtyQCppOO3Bw@mail.gmail.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<54B8465B.4020001@univ-mlv.fr>	<54B85243.90403@cs.oswego.edu>
	<54B85FCA.4010005@univ-mlv.fr>	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>	<54B92843.8080100@univ-mlv.fr>	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>	<54BA4B3A.7020809@univ-mlv.fr>	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>	<54BD0ED7.50907@cs.oswego.edu>	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>	<54BD2585.5040903@cs.oswego.edu>	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>	<54BD74C7.3020500@cs.oswego.edu>
	<54C002A6.9060204@cs.oswego.edu>
	<CAAWwtm9YWn5Dix=jp851tX7m8tMX7LVLZaLGgQMtyQCppOO3Bw@mail.gmail.com>
Message-ID: <54C50ADF.9060507@cs.oswego.edu>

On 01/25/2015 03:57 AM, D?vid Karnok wrote:

> - Subscribing a subscriber again: the code emits an onError asynchronously to
>  the already running subscriber which violates the Subscriber contract.

Thanks. Clarified/changed to ...

      * Adds the given Subscriber unless already subscribed.  If
      * already subscribed, the Subscriber's onError method is invoked
      * on the existing subscription with an IllegalStateException.

>
> - getSubscribers: I'm not sure if this is a good idea because calling onXXX
> methods on these active and async subscribers will encourage violating the
> Subscriber contract.

We can discourage it:

      * Returns a list of current subscribers for monitoring and
      * tracking purposes, not for invoking {@link Flow.Subscriber}
      * methods on the subscribers.


> The example of the one-shot publisher looks almost good, but I think the
> creation of the IllegalArgumentException inside the scheduled action will
> give misleading stacktrace: I'd be interested in who called request with such
> negative value (without setting breakpoints). Therefore, I see two options:
> throw the IAE directly in request or create the IAE and schedule it
> separately from the value emission.

OK. Now illustrated with the latter.

-Doug

>http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/SubmissionPublisher.html
> And you can run with jdk8+ with -Xbootclasspath/p:jsr166.jar from
> http://gee.cs.oswego.edu/dl/__jsr166/dist/jsr166.jar
> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar> For fun, I also placed a
> snapshot of one of my little internal development perf tests at
> http://gee.cs.oswego.edu/dl/__wwwtmp/SPL4.java
> <http://gee.cs.oswego.edu/dl/wwwtmp/SPL4.java>
>



From dl at cs.oswego.edu  Sun Jan 25 10:40:42 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Jan 2015 10:40:42 -0500
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
Message-ID: <54C50E7A.30003@cs.oswego.edu>

On 01/24/2015 03:56 PM, Martin Buchholz wrote:
> I think COWAL (and other classes in j.u.c.) could/should use builtin
> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>
> The current situation is historical:
> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise before
> integration into the JDK
> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not in
> later releases
> - many j.u.c. classes use features of ReentrantLock that builtin monitors do not
> provide, like condition objects.

Right.

One disadvantage of converting to builtin locks is that users
become more dependent on quality of JVM decisions about when to
use biased locking, sometimes encountering surprising delays on
de-biasing. We do live with this risk in some other cases, so
it is probably an OK decision here as well. Using a combination
of locks and CAS seems less defensible. There are some very
wide race windows in some List operations.

-Doug



From dl at cs.oswego.edu  Sun Jan 25 10:51:35 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Jan 2015 10:51:35 -0500
Subject: [concurrency-interest] Using relaxed reads and writes
In-Reply-To: <CA+kOe0_3i_c3CPAScVmO8gX-Fkajq5Ncw4P3H3ocX9_+u2w-Xg@mail.gmail.com>
References: <1538772756.3228485.1421972627053.JavaMail.yahoo@jws10035.mail.ne1.yahoo.com>	<54C2F47D.7010206@cs.oswego.edu>
	<CA+kOe0_3i_c3CPAScVmO8gX-Fkajq5Ncw4P3H3ocX9_+u2w-Xg@mail.gmail.com>
Message-ID: <54C51107.5010707@cs.oswego.edu>

On 01/24/2015 03:48 PM, Martin Buchholz wrote:
> We've been doing these kinds of optimizations in j.u.c., especially the obvious
> one of using relaxed write before publishing via CAS, but we haven't observed
> compelling performance improvements.

Some of the mystique here is because I know most of the
cases hotspot can optimize (in part because I once helped write
some of the optimizations (for example Matcher::post_store_load_barrier).
On the other hand, these internals do change (the introduction of
biased locking removed some cases that were handled.) So some cases
are manually optimized anyway just out of caution, and so don't
much impact observed performance.

-Doug

> On Fri, Jan 23, 2015 at 5:25 PM, Doug Lea <dl at cs.oswego.edu
> <mailto:dl at cs.oswego.edu>> wrote:
>
>     On 01/22/2015 07:23 PM, Ben Manes wrote:
>
>         I think the following is a safe optimization for my use-case, but I'd
>         appreciate
>         any feedback / concerns.
>
>         Lets say that I have a ConcurrentHashMap<K, Node<V>>, where Node<V> is
>         an entry
>         in an LRU cache with miscellaneous metadata fields such as the queue
>         links, the
>         value, expiration timestamps, etc. When updating the value the node must be
>         synchronized on in order to block if created by a concurrent
>         computation. This
>         means the more optimal CAS loop isn't used, which may have affect on
>         simplifying
>         the following discussion. Lets say that the Node's value and accessTime
>         fields
>         are volatile.
>
>         When writing the value the node's lock must be held (or implicitly safe
>         due to
>         Node construction). In that case I think that a relaxed write
>         (Unsafe#putOrderedObject) is safe by piggybacking on the mutex unlock for
>         visibility (or publishing via insert if created).
>
>
>     Yes, in fact hotspot often does this optimization for you,
>     at least on x86 and sparc, although not if biased locking
>     is enabled. It may not be worthwhile to hand-optimize.
>
>     -Doug
>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From dl at cs.oswego.edu  Sun Jan 25 11:06:10 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 25 Jan 2015 11:06:10 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
Message-ID: <54C51472.8070408@cs.oswego.edu>

On 01/25/2015 07:06 AM, Justin Sampson wrote:

> So far the only responses have been "it's impossible to do anything
> so don't even try." If that's really the end of it then I'll rest
> easy knowing I've done my due diligence.

There have been JVM-level proposals to implement bandaids
for dealing with StackOverflow for j.u.c Locks, but as far
as I know, they are at best pointwise incomplete fixes.
As people have mentioned, it is a hard problem, in part because
the JVM can/does use the stack for class-loading, compilation,
etc, that can happen at any time.

If you can live with this uncertainty, you might also be able
to live with a hypothetical method
   boolean stackSpaceAvailable(int bytes)
that returns true if the stack could accommodate a
frame of the given size. Because this would require some
estimation uncertainty on top of other problems, I don't
think anyone has proposed it lately.

-Doug



From aaron.grunthal at infinite-source.de  Sun Jan 25 11:46:42 2015
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Sun, 25 Jan 2015 17:46:42 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C51472.8070408@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C51472.8070408@cs.oswego.edu>
Message-ID: <54C51DF2.7010809@infinite-source.de>

On 25.01.2015 17:06, Doug Lea wrote:
> On 01/25/2015 07:06 AM, Justin Sampson wrote:
>
>> So far the only responses have been "it's impossible to do anything
>> so don't even try." If that's really the end of it then I'll rest
>> easy knowing I've done my due diligence.
>
> There have been JVM-level proposals to implement bandaids
> for dealing with StackOverflow for j.u.c Locks, but as far
> as I know, they are at best pointwise incomplete fixes.
> As people have mentioned, it is a hard problem, in part because
> the JVM can/does use the stack for class-loading, compilation,
> etc, that can happen at any time.
>
> If you can live with this uncertainty, you might also be able
> to live with a hypothetical method
>    boolean stackSpaceAvailable(int bytes)

In the absence of such a method, wouldn't it be possible to design a 
canary to execute before the method in question? Basically call a method 
designed to use up a significant amount of stack space, larger than the 
protected method itself, in a way that the compiler can't optimize away?

It would be trading performance for safety in that case.

> that returns true if the stack could accommodate a
> frame of the given size. Because this would require some
> estimation uncertainty on top of other problems, I don't
> think anyone has proposed it lately.
>
> -Doug
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From peter.levart at gmail.com  Sun Jan 25 11:47:31 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 25 Jan 2015 17:47:31 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C51472.8070408@cs.oswego.edu>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C51472.8070408@cs.oswego.edu>
Message-ID: <54C51E23.6050503@gmail.com>


On 01/25/2015 05:06 PM, Doug Lea wrote:
> On 01/25/2015 07:06 AM, Justin Sampson wrote:
>
>> So far the only responses have been "it's impossible to do anything
>> so don't even try." If that's really the end of it then I'll rest
>> easy knowing I've done my due diligence.
>
> There have been JVM-level proposals to implement bandaids
> for dealing with StackOverflow for j.u.c Locks, but as far
> as I know, they are at best pointwise incomplete fixes.
> As people have mentioned, it is a hard problem, in part because
> the JVM can/does use the stack for class-loading, compilation,
> etc, that can happen at any time.
>
> If you can live with this uncertainty, you might also be able
> to live with a hypothetical method
>   boolean stackSpaceAvailable(int bytes)
> that returns true if the stack could accommodate a
> frame of the given size. Because this would require some
> estimation uncertainty on top of other problems, I don't
> think anyone has proposed it lately.
>
> -Doug

So this would tipically be used before critical sections of code that 
should complete normally like:

if (!stackSpaceAvailable(whatWeMightNeedPlusSome))
     throw new StackOverflowError();

// critical code...



I don't know if stack space used is the same regardless of mode of 
execution of particular code (interpreted, compiled, inlined, ...), but 
if it is comparable in each case, one could do such "probing" before 
critical piece of code with simple recursive no-op method(s) like:


public static probeStack(int frames) throws StackOverflowError {
     if (frames > 0) probeStack(frames - 1);
}

public static probeStack(int frames, Object arg1) throws 
StackOverflowError {
     if (frames > 0) probeStack(frames - 1, arg1);
}

public static probeStack(int frames, Object arg1, Object arg2) throws 
StackOverflowError {
     if (frames > 0) probeStack(frames - 1, arg1, arg2);
}

...
...

Is JIT free to optimize away such code?

Peter

>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/4ae5af96/attachment.html>

From vitalyd at gmail.com  Sun Jan 25 12:34:35 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 25 Jan 2015 12:34:35 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C51E23.6050503@gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C51472.8070408@cs.oswego.edu> <54C51E23.6050503@gmail.com>
Message-ID: <CAHjP37FvF0FmYWpRcjOTCsajmMuY3iz+BS8hncvk1fihiZjrng@mail.gmail.com>

I believe JIT would be free to optimize this out, e.g. tail recursion
turned into loop.

sent from my phone
On Jan 25, 2015 12:16 PM, "Peter Levart" <peter.levart at gmail.com> wrote:

>
> On 01/25/2015 05:06 PM, Doug Lea wrote:
>
> On 01/25/2015 07:06 AM, Justin Sampson wrote:
>
> So far the only responses have been "it's impossible to do anything
> so don't even try." If that's really the end of it then I'll rest
> easy knowing I've done my due diligence.
>
>
> There have been JVM-level proposals to implement bandaids
> for dealing with StackOverflow for j.u.c Locks, but as far
> as I know, they are at best pointwise incomplete fixes.
> As people have mentioned, it is a hard problem, in part because
> the JVM can/does use the stack for class-loading, compilation,
> etc, that can happen at any time.
>
> If you can live with this uncertainty, you might also be able
> to live with a hypothetical method
>   boolean stackSpaceAvailable(int bytes)
> that returns true if the stack could accommodate a
> frame of the given size. Because this would require some
> estimation uncertainty on top of other problems, I don't
> think anyone has proposed it lately.
>
> -Doug
>
>
> So this would tipically be used before critical sections of code that
> should complete normally like:
>
> if (!stackSpaceAvailable(whatWeMightNeedPlusSome))
>     throw new StackOverflowError();
>
> // critical code...
>
>
>
> I don't know if stack space used is the same regardless of mode of
> execution of particular code (interpreted, compiled, inlined, ...), but if
> it is comparable in each case, one could do such "probing" before critical
> piece of code with simple recursive no-op method(s) like:
>
>
> public static probeStack(int frames) throws StackOverflowError {
>     if (frames > 0) probeStack(frames - 1);
> }
>
> public static probeStack(int frames, Object arg1) throws
> StackOverflowError {
>     if (frames > 0) probeStack(frames - 1, arg1);
> }
>
> public static probeStack(int frames, Object arg1, Object arg2) throws
> StackOverflowError {
>     if (frames > 0) probeStack(frames - 1, arg1, arg2);
> }
>
> ...
> ...
>
> Is JIT free to optimize away such code?
>
> Peter
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/fd00a6c6/attachment.html>

From vitalyd at gmail.com  Sun Jan 25 12:41:55 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 25 Jan 2015 12:41:55 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C51DF2.7010809@infinite-source.de>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C51472.8070408@cs.oswego.edu>
	<54C51DF2.7010809@infinite-source.de>
Message-ID: <CAHjP37GbsCvG163hV5xeh_-m+Hj5EYbKnfdk1=gMb4=g35sgfw@mail.gmail.com>

Suppose you had such method - how would one use it in a reliable manner?
Somehow obtain current stack pointer, compute the distance to stack end
zone and pass that in? How do you know how much stack space you'll need?
Will you have to tune that as code is changed? Doesn't sound all that
appealing.

Currently, modulo bugs, JVM actually allows unwinding (java) stack overflow
exceptions reliably (well, just like other exceptions).  It installs
special pages at the end of allocated stack which allow it to have some
room of its own to do this.  Of course, this doesn't quite help with the
"it can be thrown anywhere" scenario.

sent from my phone
On Jan 25, 2015 12:08 PM, "Aaron Grunthal" <
aaron.grunthal at infinite-source.de> wrote:

> On 25.01.2015 17:06, Doug Lea wrote:
>
>> On 01/25/2015 07:06 AM, Justin Sampson wrote:
>>
>>  So far the only responses have been "it's impossible to do anything
>>> so don't even try." If that's really the end of it then I'll rest
>>> easy knowing I've done my due diligence.
>>>
>>
>> There have been JVM-level proposals to implement bandaids
>> for dealing with StackOverflow for j.u.c Locks, but as far
>> as I know, they are at best pointwise incomplete fixes.
>> As people have mentioned, it is a hard problem, in part because
>> the JVM can/does use the stack for class-loading, compilation,
>> etc, that can happen at any time.
>>
>> If you can live with this uncertainty, you might also be able
>> to live with a hypothetical method
>>    boolean stackSpaceAvailable(int bytes)
>>
>
> In the absence of such a method, wouldn't it be possible to design a
> canary to execute before the method in question? Basically call a method
> designed to use up a significant amount of stack space, larger than the
> protected method itself, in a way that the compiler can't optimize away?
>
> It would be trading performance for safety in that case.
>
>  that returns true if the stack could accommodate a
>> frame of the given size. Because this would require some
>> estimation uncertainty on top of other problems, I don't
>> think anyone has proposed it lately.
>>
>> -Doug
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/91f78d2d/attachment.html>

From vitalyd at gmail.com  Sun Jan 25 12:47:42 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sun, 25 Jan 2015 12:47:42 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37GbsCvG163hV5xeh_-m+Hj5EYbKnfdk1=gMb4=g35sgfw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C51472.8070408@cs.oswego.edu>
	<54C51DF2.7010809@infinite-source.de>
	<CAHjP37GbsCvG163hV5xeh_-m+Hj5EYbKnfdk1=gMb4=g35sgfw@mail.gmail.com>
Message-ID: <CAHjP37FTFu53CinkG-aM3PXTgaP2iDO6zHR6R6oXhMKQDTt9Bw@mail.gmail.com>

By the way, just remembered that .NET has something akin to this, called
constrained execution regions:
https://msdn.microsoft.com/en-us/library/ms228973(v=vs.110).aspx

sent from my phone
On Jan 25, 2015 12:41 PM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Suppose you had such method - how would one use it in a reliable manner?
> Somehow obtain current stack pointer, compute the distance to stack end
> zone and pass that in? How do you know how much stack space you'll need?
> Will you have to tune that as code is changed? Doesn't sound all that
> appealing.
>
> Currently, modulo bugs, JVM actually allows unwinding (java) stack
> overflow exceptions reliably (well, just like other exceptions).  It
> installs special pages at the end of allocated stack which allow it to have
> some room of its own to do this.  Of course, this doesn't quite help with
> the "it can be thrown anywhere" scenario.
>
> sent from my phone
> On Jan 25, 2015 12:08 PM, "Aaron Grunthal" <
> aaron.grunthal at infinite-source.de> wrote:
>
>> On 25.01.2015 17:06, Doug Lea wrote:
>>
>>> On 01/25/2015 07:06 AM, Justin Sampson wrote:
>>>
>>>  So far the only responses have been "it's impossible to do anything
>>>> so don't even try." If that's really the end of it then I'll rest
>>>> easy knowing I've done my due diligence.
>>>>
>>>
>>> There have been JVM-level proposals to implement bandaids
>>> for dealing with StackOverflow for j.u.c Locks, but as far
>>> as I know, they are at best pointwise incomplete fixes.
>>> As people have mentioned, it is a hard problem, in part because
>>> the JVM can/does use the stack for class-loading, compilation,
>>> etc, that can happen at any time.
>>>
>>> If you can live with this uncertainty, you might also be able
>>> to live with a hypothetical method
>>>    boolean stackSpaceAvailable(int bytes)
>>>
>>
>> In the absence of such a method, wouldn't it be possible to design a
>> canary to execute before the method in question? Basically call a method
>> designed to use up a significant amount of stack space, larger than the
>> protected method itself, in a way that the compiler can't optimize away?
>>
>> It would be trading performance for safety in that case.
>>
>>  that returns true if the stack could accommodate a
>>> frame of the given size. Because this would require some
>>> estimation uncertainty on top of other problems, I don't
>>> think anyone has proposed it lately.
>>>
>>> -Doug
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150125/dabd6e1d/attachment.html>

From aaron.grunthal at infinite-source.de  Sun Jan 25 13:12:30 2015
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Sun, 25 Jan 2015 19:12:30 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37GbsCvG163hV5xeh_-m+Hj5EYbKnfdk1=gMb4=g35sgfw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>	<54C51472.8070408@cs.oswego.edu>	<54C51DF2.7010809@infinite-source.de>
	<CAHjP37GbsCvG163hV5xeh_-m+Hj5EYbKnfdk1=gMb4=g35sgfw@mail.gmail.com>
Message-ID: <54C5320E.60605@infinite-source.de>

Reliable? Not in the sense of hard guarantees or anything. It would be 
more of an extra safety measure

Justin was asking for an extremely simple case where one could probably 
deduce all the leaf methods by hand / with a debugger, add a factor 2 
safety margin and hope for the best.

It's a fickle workaround, using the tools that are currently available 
and not a proper solution.

Relying on a stack canary that survives a sufficiently advanced? and 
presumed hostile compiler already puts it on shaky ground.


If we want something that can be used for opportunistic recursion that 
will abort and put things on a queue or simply wait if it runs out of 
stack space (e.g. fork join pool?) then we would need JVM support.

On 25.01.2015 18:41, Vitaly Davidovich wrote:
> Suppose you had such method - how would one use it in a reliable manner?
> Somehow obtain current stack pointer, compute the distance to stack end
> zone and pass that in? How do you know how much stack space you'll need?
> Will you have to tune that as code is changed? Doesn't sound all that
> appealing.
>
> Currently, modulo bugs, JVM actually allows unwinding (java) stack
> overflow exceptions reliably (well, just like other exceptions).  It
> installs special pages at the end of allocated stack which allow it to
> have some room of its own to do this.  Of course, this doesn't quite
> help with the "it can be thrown anywhere" scenario.
>
> sent from my phone
>
> On Jan 25, 2015 12:08 PM, "Aaron Grunthal"
> <aaron.grunthal at infinite-source.de
> <mailto:aaron.grunthal at infinite-source.de>> wrote:
>
>     On 25.01.2015 17:06, Doug Lea wrote:
>
>         On 01/25/2015 07:06 AM, Justin Sampson wrote:
>
>             So far the only responses have been "it's impossible to do
>             anything
>             so don't even try." If that's really the end of it then I'll
>             rest
>             easy knowing I've done my due diligence.
>
>
>         There have been JVM-level proposals to implement bandaids
>         for dealing with StackOverflow for j.u.c Locks, but as far
>         as I know, they are at best pointwise incomplete fixes.
>         As people have mentioned, it is a hard problem, in part because
>         the JVM can/does use the stack for class-loading, compilation,
>         etc, that can happen at any time.
>
>         If you can live with this uncertainty, you might also be able
>         to live with a hypothetical method
>             boolean stackSpaceAvailable(int bytes)
>
>
>     In the absence of such a method, wouldn't it be possible to design a
>     canary to execute before the method in question? Basically call a
>     method designed to use up a significant amount of stack space,
>     larger than the protected method itself, in a way that the compiler
>     can't optimize away?
>
>     It would be trading performance for safety in that case.
>
>         that returns true if the stack could accommodate a
>         frame of the given size. Because this would require some
>         estimation uncertainty on top of other problems, I don't
>         think anyone has proposed it lately.
>
>         -Doug
>
>
>         _________________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.__oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>


From viktor.klang at gmail.com  Mon Jan 26 05:25:06 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 26 Jan 2015 11:25:06 +0100
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54C50ADF.9060507@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu> <54B8465B.4020001@univ-mlv.fr>
	<54B85243.90403@cs.oswego.edu> <54B85FCA.4010005@univ-mlv.fr>
	<CANPzfU_dtexbsmTd_qbmU_ySfi0_PrvgU3oB3+ti95wSiYiq=w@mail.gmail.com>
	<54B92843.8080100@univ-mlv.fr>
	<CANPzfU9U6vt2=AJQ6EhJGDwu48LwcoRQ8tsY6TKcYPXgq5x6nw@mail.gmail.com>
	<54BA4B3A.7020809@univ-mlv.fr>
	<CANPzfU9UA+R6hYZGg+W+E88v89YmBL2jqVjvncL-MX0v5g6eJw@mail.gmail.com>
	<54BD0ED7.50907@cs.oswego.edu>
	<CAAWwtm95Vdr9STiMtgLsaz+0Sr=Lr34AgiKOoFKNt5ErzROwfQ@mail.gmail.com>
	<54BD2585.5040903@cs.oswego.edu>
	<CAAWwtm_XUL3XNFZ_Nw5Fi9oU_GdFViAEdkYk2n0==i9g65oU=w@mail.gmail.com>
	<54BD74C7.3020500@cs.oswego.edu> <54C002A6.9060204@cs.oswego.edu>
	<CAAWwtm9YWn5Dix=jp851tX7m8tMX7LVLZaLGgQMtyQCppOO3Bw@mail.gmail.com>
	<54C50ADF.9060507@cs.oswego.edu>
Message-ID: <CANPzfU9t_UQjjNjLQ8VthJ7RsQdS_h8YSjGH=KT3bPNKz8gQCA@mail.gmail.com>

Great progress! :)

On Sun, Jan 25, 2015 at 4:25 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/25/2015 03:57 AM, D?vid Karnok wrote:
>
>  - Subscribing a subscriber again: the code emits an onError
>> asynchronously to
>>  the already running subscriber which violates the Subscriber contract.
>>
>
> Thanks. Clarified/changed to ...
>
>      * Adds the given Subscriber unless already subscribed.  If
>      * already subscribed, the Subscriber's onError method is invoked
>      * on the existing subscription with an IllegalStateException.
>
>
>> - getSubscribers: I'm not sure if this is a good idea because calling
>> onXXX
>> methods on these active and async subscribers will encourage violating the
>> Subscriber contract.
>>
>
> We can discourage it:
>
>      * Returns a list of current subscribers for monitoring and
>      * tracking purposes, not for invoking {@link Flow.Subscriber}
>      * methods on the subscribers.
>
>
>  The example of the one-shot publisher looks almost good, but I think the
>> creation of the IllegalArgumentException inside the scheduled action will
>> give misleading stacktrace: I'd be interested in who called request with
>> such
>> negative value (without setting breakpoints). Therefore, I see two
>> options:
>> throw the IAE directly in request or create the IAE and schedule it
>> separately from the value emission.
>>
>
> OK. Now illustrated with the latter.
>
> -Doug
>
>  http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/
>> SubmissionPublisher.html
>> And you can run with jdk8+ with -Xbootclasspath/p:jsr166.jar from
>> http://gee.cs.oswego.edu/dl/__jsr166/dist/jsr166.jar
>> <http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar> For fun, I also
>> placed a
>> snapshot of one of my little internal development perf tests at
>> http://gee.cs.oswego.edu/dl/__wwwtmp/SPL4.java
>> <http://gee.cs.oswego.edu/dl/wwwtmp/SPL4.java>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/a133e53f/attachment.html>

From oleksandr.otenko at oracle.com  Mon Jan 26 06:42:36 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 26 Jan 2015 11:42:36 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
Message-ID: <54C6282C.3080509@oracle.com>

So?

How about throwing SOE in unlock? And what do you do if JVM says nuh-uh, 
you don't have enough stack to run unlock, even if you checked?

Alex


On 25/01/2015 12:06, Justin Sampson wrote:
> Consider this incredibly simple lock implementation:
>
> public final class SimpleLock extends AbstractOwnableSynchronizer {
>    private final AtomicBoolean state = new AtomicBoolean();
>    public final boolean tryLock() {
>      if (state.compareAndSet(false, true)) {
>        setExclusiveOwnerThread(Thread.currentThread());
>        return true;
>      } else {
>        return false;
>      }
>    }
>    public final void unlock() {
>      if (Thread.currentThread() != getExclusiveOwnerThread()) {
>        throw new IllegalMonitorStateException();
>      } else {
>        setExclusiveOwnerThread(null);
>        state.set(false);
>      }
>    }
> }
>
> It's possible for the CAS in tryLock() to succeed and then for the
> very next line to blow up with a synchronous StackOverflowError,
> leaving the state variable true even though tryLock() is throwing.
> This is the only kind of case that I'm looking for advice about,
> nothing more general.
>
> So far the only responses have been "it's impossible to do anything
> so don't even try." If that's really the end of it then I'll rest
> easy knowing I've done my due diligence.
>
> But it seems so trivial to recognize that this tryLock() method
> doesn't do any heap allocation, isn't recursive, and only calls
> static or final methods, most of them being intrinsic. So I was just
> wondering if it might be plausible to annotate such a method, as a
> request to the compiler to confirm those facts and as a request to
> the JVM to fail fast on entry by allocating enough stack space up
> front.
>
> I wasn't imagining any kind of suppression of errors or more general
> handling of arbitrarily complicated code, just avoidance of common
> errors by failing fast when it's easy to do so.
>
> Cheers,
> Justin


From oleksandr.otenko at oracle.com  Mon Jan 26 08:49:36 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 26 Jan 2015 13:49:36 +0000
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>
	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>
Message-ID: <54C645F0.2020704@oracle.com>

It seems you miss a very important restriction. "subsequent reads" only 
make the "preceding writes" "visible" to the thread that does the read. 
There is nothing in the spec that requires any writes preceding a 
volatile read preceding a normal write that you observe through a normal 
read, to be visible.

So even though the volatile writes may be implemented as barriers 
(therefore making the writes visible to all threads), they don't have to 
be; and, as explained further down this thread, there are cases where 
they can get eliminated.

Alex


On 23/01/2015 17:50, Luke Sandberg wrote:
> "A write to a volatile field (?8.3.1.4) happens-before every 
> subsequent read of that field."
>
> so by pulling the value out of the volatile field we get an HB edge 
> and program order supplies the rest.  The JMM doesn't require the 
> 'subsequent read' to be on another thread, it just still works if it 
> it.  I think in practice this means that no writes will get reordered 
> past this volatile write.
>
> So that write/read should force all subsequent reads to see all the 
> writes that happened earlier (notably our constructor field assignments).
>
> On Fri, Jan 23, 2015 at 9:36 AM, Aleksey Shipilev 
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>
>     On 23.01.2015 18:10, Luke Sandberg wrote:
>     > The only 'state transition' we require in this case is 'fully
>     > constructed' which doesn't seem like a complicated thing to
>     achieve but
>     > all the ways i can think of doing it add complexity and/or overhead.
>     >  e.g. we could add a method like this:
>     >
>     > static final class Wrapper<T> {
>     >   volatile T t;
>     > }
>     > static <T> T safelyPublish(T t) {
>     >  Wrapper wrapper = new Wrapper<T> ();
>     >  wrapper.t = t;
>     >  return wrapper.t;
>     > }
>     >
>     > Now, i think, that passing any 'unsafely constructed' object through
>     > this helper would create sufficient happens-before edges that
>     this code
>     > would be JMM compliant.
>
>     Erm, JMM compliant how? If you want a happens-before edge between the
>     write in one thread, and the read in another thread, you have to have
>     inter-thread synchronizes-with edge. That is, in this case, you
>     have to
>     write to volatile in publisher thread, and read from volatile in
>     consumer thread.
>
>     Swizzling the value through this magic safelyPublish method does not
>     introduce any inter-thread edges. If you still have to leak either the
>     argument or returned t to another thread through the data race,
>     all bets
>     are off. If you *received* the t from another thread via the data
>     race,
>     it is again too late to "sanitize" it with safelyPublish.
>
>     In other words, once a racy write had happened, there is NO WAY TO
>     RECOVER. That ship had sailed. JMM-wise, once you have an unordered
>     write, any read can see it in any happens-before consistent execution
>     (modulo causality requirements). There are also happens-before
>     consistent executions where you don't see that racy write. It's in
>     limbo, it may or may not come.
>
>     Spec-wise, the only escape-hatch way to be resilient in the face of
>     unsafe publication is final. No final -- no guarantees.
>
>
>     > All of these seem pretty complex/high overhead to solve this
>     (afaik only
>     > theoretical) problem.  Does anyone have other ideas? How bad
>     would it
>     > be to just rely on our callers not to unsafely publish?  Is
>     there any
>     > precedent for documenting something like this?
>
>     This is not a theoretical problem. Well, I think the consensus is
>     educating users that publishing via data race is very wrong, and
>     should
>     be avoided at all costs. Only a carefully constructed class can
>     survive
>     unsafe publication, but one should not generally rely on this because
>     classes are constructed by humans, and humans do mistakes all the
>     time.
>     Defense in depth here: protect your classes with finals to recover
>     from
>     accidents, but don't suggest users to unsafely publish them
>     because of that.
>
>     Thanks,
>     -Aleksey.
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/3dfb1fdb/attachment.html>

From vitalyd at gmail.com  Mon Jan 26 09:18:42 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 09:18:42 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C6282C.3080509@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
Message-ID: <CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>

Yeah, this is a losing proposition, I suspect.

But to play along, you could (a) run the code anyway with a catch block and
hope that you overestimated how much stack is needed or (b) instruct the VM
to shutdown, possibly a halt.  The additional complexity is how do you
ensure your error handling code won't then bump up against the same
situation, especially considering that JIT will most likely haven't
compiled the handling code as it's cold.  You'd need to have some
annotations to tell JIT to compile entire method, which would at least
solve that problem, but wouldn't help in the general case.

sent from my phone
On Jan 26, 2015 6:42 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

> So?
>
> How about throwing SOE in unlock? And what do you do if JVM says nuh-uh,
> you don't have enough stack to run unlock, even if you checked?
>
> Alex
>
>
> On 25/01/2015 12:06, Justin Sampson wrote:
>
>> Consider this incredibly simple lock implementation:
>>
>> public final class SimpleLock extends AbstractOwnableSynchronizer {
>>    private final AtomicBoolean state = new AtomicBoolean();
>>    public final boolean tryLock() {
>>      if (state.compareAndSet(false, true)) {
>>        setExclusiveOwnerThread(Thread.currentThread());
>>        return true;
>>      } else {
>>        return false;
>>      }
>>    }
>>    public final void unlock() {
>>      if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>        throw new IllegalMonitorStateException();
>>      } else {
>>        setExclusiveOwnerThread(null);
>>        state.set(false);
>>      }
>>    }
>> }
>>
>> It's possible for the CAS in tryLock() to succeed and then for the
>> very next line to blow up with a synchronous StackOverflowError,
>> leaving the state variable true even though tryLock() is throwing.
>> This is the only kind of case that I'm looking for advice about,
>> nothing more general.
>>
>> So far the only responses have been "it's impossible to do anything
>> so don't even try." If that's really the end of it then I'll rest
>> easy knowing I've done my due diligence.
>>
>> But it seems so trivial to recognize that this tryLock() method
>> doesn't do any heap allocation, isn't recursive, and only calls
>> static or final methods, most of them being intrinsic. So I was just
>> wondering if it might be plausible to annotate such a method, as a
>> request to the compiler to confirm those facts and as a request to
>> the JVM to fail fast on entry by allocating enough stack space up
>> front.
>>
>> I wasn't imagining any kind of suppression of errors or more general
>> handling of arbitrarily complicated code, just avoidance of common
>> errors by failing fast when it's easy to do so.
>>
>> Cheers,
>> Justin
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/2015cbb7/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Jan 26 10:01:46 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 26 Jan 2015 15:01:46 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
Message-ID: <54C656DA.4070807@oracle.com>

Yes.

What I meant, is when do you check that unlock has enough stack? If it 
is just before unlock, then what do you do if you can't. Not sure if you 
need to halt - consequences of SOE are already unspecified.

And if you check whether you will be able to unlock at the time you 
attempt to lock, then how do you tell JVM to check the entire code path 
up to unlock to see if there *will* be enough stack by the time the code 
gets there.

Alex

On 26/01/2015 14:18, Vitaly Davidovich wrote:
>
> Yeah, this is a losing proposition, I suspect.
>
> But to play along, you could (a) run the code anyway with a catch 
> block and hope that you overestimated how much stack is needed or (b) 
> instruct the VM to shutdown, possibly a halt.  The additional 
> complexity is how do you ensure your error handling code won't then 
> bump up against the same situation, especially considering that JIT 
> will most likely haven't compiled the handling code as it's cold.  
> You'd need to have some annotations to tell JIT to compile entire 
> method, which would at least solve that problem, but wouldn't help in 
> the general case.
>
> sent from my phone
>
> On Jan 26, 2015 6:42 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     So?
>
>     How about throwing SOE in unlock? And what do you do if JVM says
>     nuh-uh, you don't have enough stack to run unlock, even if you
>     checked?
>
>     Alex
>
>
>     On 25/01/2015 12:06, Justin Sampson wrote:
>
>         Consider this incredibly simple lock implementation:
>
>         public final class SimpleLock extends
>         AbstractOwnableSynchronizer {
>            private final AtomicBoolean state = new AtomicBoolean();
>            public final boolean tryLock() {
>              if (state.compareAndSet(false, true)) {
>                setExclusiveOwnerThread(Thread.currentThread());
>                return true;
>              } else {
>                return false;
>              }
>            }
>            public final void unlock() {
>              if (Thread.currentThread() != getExclusiveOwnerThread()) {
>                throw new IllegalMonitorStateException();
>              } else {
>                setExclusiveOwnerThread(null);
>                state.set(false);
>              }
>            }
>         }
>
>         It's possible for the CAS in tryLock() to succeed and then for the
>         very next line to blow up with a synchronous StackOverflowError,
>         leaving the state variable true even though tryLock() is throwing.
>         This is the only kind of case that I'm looking for advice about,
>         nothing more general.
>
>         So far the only responses have been "it's impossible to do
>         anything
>         so don't even try." If that's really the end of it then I'll rest
>         easy knowing I've done my due diligence.
>
>         But it seems so trivial to recognize that this tryLock() method
>         doesn't do any heap allocation, isn't recursive, and only calls
>         static or final methods, most of them being intrinsic. So I
>         was just
>         wondering if it might be plausible to annotate such a method, as a
>         request to the compiler to confirm those facts and as a request to
>         the JVM to fail fast on entry by allocating enough stack space up
>         front.
>
>         I wasn't imagining any kind of suppression of errors or more
>         general
>         handling of arbitrarily complicated code, just avoidance of common
>         errors by failing fast when it's easy to do so.
>
>         Cheers,
>         Justin
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/7ce8d22d/attachment.html>

From vitalyd at gmail.com  Mon Jan 26 10:12:43 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 10:12:43 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C656DA.4070807@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
Message-ID: <CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>

Yeah, I think you'd check right before calling unlock (but, how do you
ensure the caller could reach that far and not hit SOE - turtles all the
way down!).

The reason I mention halt is that you may not want to even attempt going
any further, e.g. not risk handler code mucking things up further.  But
yeah, this is so murky I don't think it's tractable  (nevermind how one
would even test all the various failure cases reliably).

Having come across various coding standards for safety critical software
(not java though), there seems to be a lot of focus on static analysis
verification,  such as prohibiting recursion.  Otherwise, I think you let
things fail fast and backups (or some failover mechanism) take over.

sent from my phone
On Jan 26, 2015 10:02 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Yes.
>
> What I meant, is when do you check that unlock has enough stack? If it is
> just before unlock, then what do you do if you can't. Not sure if you need
> to halt - consequences of SOE are already unspecified.
>
> And if you check whether you will be able to unlock at the time you
> attempt to lock, then how do you tell JVM to check the entire code path up
> to unlock to see if there *will* be enough stack by the time the code
> gets there.
>
> Alex
>
> On 26/01/2015 14:18, Vitaly Davidovich wrote:
>
> Yeah, this is a losing proposition, I suspect.
>
> But to play along, you could (a) run the code anyway with a catch block
> and hope that you overestimated how much stack is needed or (b) instruct
> the VM to shutdown, possibly a halt.  The additional complexity is how do
> you ensure your error handling code won't then bump up against the same
> situation, especially considering that JIT will most likely haven't
> compiled the handling code as it's cold.  You'd need to have some
> annotations to tell JIT to compile entire method, which would at least
> solve that problem, but wouldn't help in the general case.
>
> sent from my phone
> On Jan 26, 2015 6:42 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>> So?
>>
>> How about throwing SOE in unlock? And what do you do if JVM says nuh-uh,
>> you don't have enough stack to run unlock, even if you checked?
>>
>> Alex
>>
>>
>> On 25/01/2015 12:06, Justin Sampson wrote:
>>
>>> Consider this incredibly simple lock implementation:
>>>
>>> public final class SimpleLock extends AbstractOwnableSynchronizer {
>>>    private final AtomicBoolean state = new AtomicBoolean();
>>>    public final boolean tryLock() {
>>>      if (state.compareAndSet(false, true)) {
>>>        setExclusiveOwnerThread(Thread.currentThread());
>>>        return true;
>>>      } else {
>>>        return false;
>>>      }
>>>    }
>>>    public final void unlock() {
>>>      if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>>        throw new IllegalMonitorStateException();
>>>      } else {
>>>        setExclusiveOwnerThread(null);
>>>        state.set(false);
>>>      }
>>>    }
>>> }
>>>
>>> It's possible for the CAS in tryLock() to succeed and then for the
>>> very next line to blow up with a synchronous StackOverflowError,
>>> leaving the state variable true even though tryLock() is throwing.
>>> This is the only kind of case that I'm looking for advice about,
>>> nothing more general.
>>>
>>> So far the only responses have been "it's impossible to do anything
>>> so don't even try." If that's really the end of it then I'll rest
>>> easy knowing I've done my due diligence.
>>>
>>> But it seems so trivial to recognize that this tryLock() method
>>> doesn't do any heap allocation, isn't recursive, and only calls
>>> static or final methods, most of them being intrinsic. So I was just
>>> wondering if it might be plausible to annotate such a method, as a
>>> request to the compiler to confirm those facts and as a request to
>>> the JVM to fail fast on entry by allocating enough stack space up
>>> front.
>>>
>>> I wasn't imagining any kind of suppression of errors or more general
>>> handling of arbitrarily complicated code, just avoidance of common
>>> errors by failing fast when it's easy to do so.
>>>
>>> Cheers,
>>> Justin
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/1910e972/attachment.html>

From martinrb at google.com  Mon Jan 26 13:20:38 2015
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Jan 2015 10:20:38 -0800
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <54C50E7A.30003@cs.oswego.edu>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<54C50E7A.30003@cs.oswego.edu>
Message-ID: <CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>

Latest version of COWAL in jsr166 CVS now uses builtin monitor locks.

On Sun, Jan 25, 2015 at 7:40 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/24/2015 03:56 PM, Martin Buchholz wrote:
>
>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>
>> The current situation is historical:
>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise before
>> integration into the JDK
>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not
>> in
>> later releases
>> - many j.u.c. classes use features of ReentrantLock that builtin monitors
>> do not
>> provide, like condition objects.
>>
>
> Right.
>
> One disadvantage of converting to builtin locks is that users
> become more dependent on quality of JVM decisions about when to
> use biased locking, sometimes encountering surprising delays on
> de-biasing. We do live with this risk in some other cases, so
> it is probably an OK decision here as well. Using a combination
> of locks and CAS seems less defensible. There are some very
> wide race windows in some List operations.
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/aa5bd077/attachment.html>

From joe.bowbeer at gmail.com  Mon Jan 26 14:25:26 2015
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 26 Jan 2015 11:25:26 -0800
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<54C50E7A.30003@cs.oswego.edu>
	<CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>
Message-ID: <CAHzJPErhyc9w6AUOZh4046psBuRy3QtEyQT9LHRxae97nfMhLA@mail.gmail.com>

Has anyone encountered a situation where the memory footprint of COWAL
matters?

I wouldn't expect it to, and for I'm curious to know if this request is
only motivated by code inspection or if there is some existing/anticipated
problem.

In defense of the status quo, I'll point out that a consistent
implementation throughout j.u.c also has some advantages.

On Mon, Jan 26, 2015 at 10:20 AM, Martin Buchholz <martinrb at google.com>
wrote:

> Latest version of COWAL in jsr166 CVS now uses builtin monitor locks.
>
> On Sun, Jan 25, 2015 at 7:40 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 01/24/2015 03:56 PM, Martin Buchholz wrote:
>>
>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>
>>> The current situation is historical:
>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>> before
>>> integration into the JDK
>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but not
>>> in
>>> later releases
>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>> monitors do not
>>> provide, like condition objects.
>>>
>>
>> Right.
>>
>> One disadvantage of converting to builtin locks is that users
>> become more dependent on quality of JVM decisions about when to
>> use biased locking, sometimes encountering surprising delays on
>> de-biasing. We do live with this risk in some other cases, so
>> it is probably an OK decision here as well. Using a combination
>> of locks and CAS seems less defensible. There are some very
>> wide race windows in some List operations.
>>
>> -Doug
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/6741ebce/attachment.html>

From vitalyd at gmail.com  Mon Jan 26 15:03:07 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 15:03:07 -0500
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAHzJPErhyc9w6AUOZh4046psBuRy3QtEyQT9LHRxae97nfMhLA@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<54C50E7A.30003@cs.oswego.edu>
	<CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>
	<CAHzJPErhyc9w6AUOZh4046psBuRy3QtEyQT9LHRxae97nfMhLA@mail.gmail.com>
Message-ID: <CAHjP37GwLbux09McjDdeoK_x+xHjE4G2qeC-aNBWEfSWhKKfAg@mail.gmail.com>

This question is a bit of a slippery slope because, at the end of the day,
one could say this about a lot of classes.  Surely it's used less
frequently than, say, HashMap or ArrayList, but it's a *library* class and
therefore should be as frugal as possible when it doesn't detract from
performance, maintainability, and function.

On Mon, Jan 26, 2015 at 2:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Has anyone encountered a situation where the memory footprint of COWAL
> matters?
>
> I wouldn't expect it to, and for I'm curious to know if this request is
> only motivated by code inspection or if there is some existing/anticipated
> problem.
>
> In defense of the status quo, I'll point out that a consistent
> implementation throughout j.u.c also has some advantages.
>
> On Mon, Jan 26, 2015 at 10:20 AM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> Latest version of COWAL in jsr166 CVS now uses builtin monitor locks.
>>
>> On Sun, Jan 25, 2015 at 7:40 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>> On 01/24/2015 03:56 PM, Martin Buchholz wrote:
>>>
>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>
>>>> The current situation is historical:
>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>> before
>>>> integration into the JDK
>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>> not in
>>>> later releases
>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>> monitors do not
>>>> provide, like condition objects.
>>>>
>>>
>>> Right.
>>>
>>> One disadvantage of converting to builtin locks is that users
>>> become more dependent on quality of JVM decisions about when to
>>> use biased locking, sometimes encountering surprising delays on
>>> de-biasing. We do live with this risk in some other cases, so
>>> it is probably an OK decision here as well. Using a combination
>>> of locks and CAS seems less defensible. There are some very
>>> wide race windows in some List operations.
>>>
>>> -Doug
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/079626c1/attachment.html>

From jsampson at guidewire.com  Mon Jan 26 15:07:09 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Mon, 26 Jan 2015 20:07:09 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> Yeah, I think you'd check right before calling unlock (but, how do
> you ensure the caller could reach that far and not hit SOE -
> turtles all the way down!).

You'd really have to check before calling lock() to make sure
there's enough stack for unlock()... So basically, the futility in
any attempt to harden lock() and unlock() individually is that
they're called in pairs from higher-level code. We'd have to find a
way to make lock() fail-fast if there's not enough stack for the
corresponding unlock().

I'm satisfied with that explanation for now, though I may revisit
the discussion later in regard to the specific code I'm writing. :)

Cheers,
Justin


From vitalyd at gmail.com  Mon Jan 26 15:16:53 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 15:16:53 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>

That's true: if you do this via " lock(); try {...} finally {unlock()}",
then you may as well check right before lock.  But since this would be a
library exposed to others, it's not necessarily the case that they pair it
up like that.  Anyway, this is yet another wrinkle in here.

On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com>
wrote:

> Vitaly Davidovich wrote:
>
> > Yeah, I think you'd check right before calling unlock (but, how do
> > you ensure the caller could reach that far and not hit SOE -
> > turtles all the way down!).
>
> You'd really have to check before calling lock() to make sure
> there's enough stack for unlock()... So basically, the futility in
> any attempt to harden lock() and unlock() individually is that
> they're called in pairs from higher-level code. We'd have to find a
> way to make lock() fail-fast if there's not enough stack for the
> corresponding unlock().
>
> I'm satisfied with that explanation for now, though I may revisit
> the discussion later in regard to the specific code I'm writing. :)
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/f0a49a60/attachment-0001.html>

From vitalyd at gmail.com  Mon Jan 26 15:19:40 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 15:19:40 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
Message-ID: <CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>

Also, I know I posted a link to .NET's CER earlier, but found this article,
which goes into a bit more detail (including a section on their way to
harden SOE cases): https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.
Might be of general interest on how a comparable managed environment
attempts to handle this.

On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> That's true: if you do this via " lock(); try {...} finally {unlock()}",
> then you may as well check right before lock.  But since this would be a
> library exposed to others, it's not necessarily the case that they pair it
> up like that.  Anyway, this is yet another wrinkle in here.
>
> On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com>
> wrote:
>
>> Vitaly Davidovich wrote:
>>
>> > Yeah, I think you'd check right before calling unlock (but, how do
>> > you ensure the caller could reach that far and not hit SOE -
>> > turtles all the way down!).
>>
>> You'd really have to check before calling lock() to make sure
>> there's enough stack for unlock()... So basically, the futility in
>> any attempt to harden lock() and unlock() individually is that
>> they're called in pairs from higher-level code. We'd have to find a
>> way to make lock() fail-fast if there's not enough stack for the
>> corresponding unlock().
>>
>> I'm satisfied with that explanation for now, though I may revisit
>> the discussion later in regard to the specific code I'm writing. :)
>>
>> Cheers,
>> Justin
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/7c036a24/attachment.html>

From sitnikov.vladimir at gmail.com  Mon Jan 26 15:32:32 2015
From: sitnikov.vladimir at gmail.com (Vladimir Sitnikov)
Date: Tue, 27 Jan 2015 00:32:32 +0400
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAHjP37GwLbux09McjDdeoK_x+xHjE4G2qeC-aNBWEfSWhKKfAg@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<54C50E7A.30003@cs.oswego.edu>
	<CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>
	<CAHzJPErhyc9w6AUOZh4046psBuRy3QtEyQT9LHRxae97nfMhLA@mail.gmail.com>
	<CAHjP37GwLbux09McjDdeoK_x+xHjE4G2qeC-aNBWEfSWhKKfAg@mail.gmail.com>
Message-ID: <CAB=Je-HieUvK8vPM6dtn5o2mdXXYc7Z2gmL+_ajRtx3SXV=_9A@mail.gmail.com>

> Has anyone encountered a situation where the memory footprint of COWAL matters?

Well, the story began when I found heavy traffic on COWALs in our
application (as per JFR).
The "root" cause is the application (ab-)used commons-configuration,
and each commons-configuration "AbstractConfiguration" (as of 1.x)
always created a couple of COWALs (see [1]).

I opened a ticket for commons-configuration to "avoid creating of
empty COWALS" (see [2], jfr screenshot attached).
Then I opened COWAL.java and realized it could be improved (somewhere
near JDK 15:), so I raised a question in c-i.

[1]: http://grepcode.com/file/repo1.maven.org/maven2/commons-configuration/commons-configuration/1.10/org/apache/commons/configuration/event/EventSource.java#EventSource.initListeners%28%29

[2]: https://issues.apache.org/jira/browse/CONFIGURATION-596

Vladimir

2015-01-26 23:03 GMT+03:00 Vitaly Davidovich <vitalyd at gmail.com>:
> This question is a bit of a slippery slope because, at the end of the day,
> one could say this about a lot of classes.  Surely it's used less frequently
> than, say, HashMap or ArrayList, but it's a *library* class and therefore
> should be as frugal as possible when it doesn't detract from performance,
> maintainability, and function.
>
> On Mon, Jan 26, 2015 at 2:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
>>
>> Has anyone encountered a situation where the memory footprint of COWAL
>> matters?
>>
>> I wouldn't expect it to, and for I'm curious to know if this request is
>> only motivated by code inspection or if there is some existing/anticipated
>> problem.
>>
>> In defense of the status quo, I'll point out that a consistent
>> implementation throughout j.u.c also has some advantages.
>>
>> On Mon, Jan 26, 2015 at 10:20 AM, Martin Buchholz <martinrb at google.com>
>> wrote:
>>>
>>> Latest version of COWAL in jsr166 CVS now uses builtin monitor locks.
>>>
>>> On Sun, Jan 25, 2015 at 7:40 AM, Doug Lea <dl at cs.oswego.edu> wrote:
>>>>
>>>> On 01/24/2015 03:56 PM, Martin Buchholz wrote:
>>>>>
>>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
>>>>> synchronization as you suggest, to save 32 bytes (Doug, do you agree?).
>>>>>
>>>>> The current situation is historical:
>>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
>>>>> before
>>>>> integration into the JDK
>>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
>>>>> not in
>>>>> later releases
>>>>> - many j.u.c. classes use features of ReentrantLock that builtin
>>>>> monitors do not
>>>>> provide, like condition objects.
>>>>
>>>>
>>>> Right.
>>>>
>>>> One disadvantage of converting to builtin locks is that users
>>>> become more dependent on quality of JVM decisions about when to
>>>> use biased locking, sometimes encountering surprising delays on
>>>> de-biasing. We do live with this risk in some other cases, so
>>>> it is probably an OK decision here as well. Using a combination
>>>> of locks and CAS seems less defensible. There are some very
>>>> wide race windows in some List operations.
>>>>
>>>> -Doug
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Regards,
Vladimir Sitnikov

From joe.bowbeer at gmail.com  Mon Jan 26 17:03:27 2015
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 26 Jan 2015 14:03:27 -0800
Subject: [concurrency-interest] CopyOnWriteArrayList vs ReentrantLock
In-Reply-To: <CAB=Je-HieUvK8vPM6dtn5o2mdXXYc7Z2gmL+_ajRtx3SXV=_9A@mail.gmail.com>
References: <CAB=Je-HHJ6UZvKs1Re4nqXWQ0E-TQw7veY-x3Ox8x0gcbkhD=Q@mail.gmail.com>
	<CA+kOe08tJfn3pqa+mfQbT2=X69oY6RhXX7LC19FC2uOu=LPVeA@mail.gmail.com>
	<54C50E7A.30003@cs.oswego.edu>
	<CA+kOe0-vOBYjRs5Wkjk6vLnkpGjL9F7g2XGfE5kCkuri0Sgbng@mail.gmail.com>
	<CAHzJPErhyc9w6AUOZh4046psBuRy3QtEyQT9LHRxae97nfMhLA@mail.gmail.com>
	<CAHjP37GwLbux09McjDdeoK_x+xHjE4G2qeC-aNBWEfSWhKKfAg@mail.gmail.com>
	<CAB=Je-HieUvK8vPM6dtn5o2mdXXYc7Z2gmL+_ajRtx3SXV=_9A@mail.gmail.com>
Message-ID: <CAHzJPEr=urPQhSCvAac2zZ5yTE5WLB+ZzHtQg8HoDVNuEf0wkA@mail.gmail.com>

Thanks for adding the back story.

On Mon, Jan 26, 2015 at 12:32 PM, Vladimir Sitnikov <
sitnikov.vladimir at gmail.com> wrote:

> > Has anyone encountered a situation where the memory footprint of COWAL
> matters?
>
> Well, the story began when I found heavy traffic on COWALs in our
> application (as per JFR).
> The "root" cause is the application (ab-)used commons-configuration,
> and each commons-configuration "AbstractConfiguration" (as of 1.x)
> always created a couple of COWALs (see [1]).
>
> I opened a ticket for commons-configuration to "avoid creating of
> empty COWALS" (see [2], jfr screenshot attached).
> Then I opened COWAL.java and realized it could be improved (somewhere
> near JDK 15:), so I raised a question in c-i.
>
> [1]:
> http://grepcode.com/file/repo1.maven.org/maven2/commons-configuration/commons-configuration/1.10/org/apache/commons/configuration/event/EventSource.java#EventSource.initListeners%28%29
>
> [2]: https://issues.apache.org/jira/browse/CONFIGURATION-596
>
> Vladimir
>
> 2015-01-26 23:03 GMT+03:00 Vitaly Davidovich <vitalyd at gmail.com>:
> > This question is a bit of a slippery slope because, at the end of the
> day,
> > one could say this about a lot of classes.  Surely it's used less
> frequently
> > than, say, HashMap or ArrayList, but it's a *library* class and therefore
> > should be as frugal as possible when it doesn't detract from performance,
> > maintainability, and function.
> >
> > On Mon, Jan 26, 2015 at 2:25 PM, Joe Bowbeer <joe.bowbeer at gmail.com>
> wrote:
> >>
> >> Has anyone encountered a situation where the memory footprint of COWAL
> >> matters?
> >>
> >> I wouldn't expect it to, and for I'm curious to know if this request is
> >> only motivated by code inspection or if there is some
> existing/anticipated
> >> problem.
> >>
> >> In defense of the status quo, I'll point out that a consistent
> >> implementation throughout j.u.c also has some advantages.
> >>
> >> On Mon, Jan 26, 2015 at 10:20 AM, Martin Buchholz <martinrb at google.com>
> >> wrote:
> >>>
> >>> Latest version of COWAL in jsr166 CVS now uses builtin monitor locks.
> >>>
> >>> On Sun, Jan 25, 2015 at 7:40 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> >>>>
> >>>> On 01/24/2015 03:56 PM, Martin Buchholz wrote:
> >>>>>
> >>>>> I think COWAL (and other classes in j.u.c.) could/should use builtin
> >>>>> synchronization as you suggest, to save 32 bytes (Doug, do you
> agree?).
> >>>>>
> >>>>> The current situation is historical:
> >>>>> - j.u.c. classes used their own ReentrantLock as "dogfood" exercise
> >>>>> before
> >>>>> integration into the JDK
> >>>>> - ReentrantLock outperformed builtin locks with hotspot in JDK 5 but
> >>>>> not in
> >>>>> later releases
> >>>>> - many j.u.c. classes use features of ReentrantLock that builtin
> >>>>> monitors do not
> >>>>> provide, like condition objects.
> >>>>
> >>>>
> >>>> Right.
> >>>>
> >>>> One disadvantage of converting to builtin locks is that users
> >>>> become more dependent on quality of JVM decisions about when to
> >>>> use biased locking, sometimes encountering surprising delays on
> >>>> de-biasing. We do live with this risk in some other cases, so
> >>>> it is probably an OK decision here as well. Using a combination
> >>>> of locks and CAS seems less defensible. There are some very
> >>>> wide race windows in some List operations.
> >>>>
> >>>> -Doug
> >>>>
> >>>>
> >>>>
> >>>> _______________________________________________
> >>>> Concurrency-interest mailing list
> >>>> Concurrency-interest at cs.oswego.edu
> >>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
>
>
> --
> Regards,
> Vladimir Sitnikov
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/2212ac9c/attachment.html>

From martinrb at google.com  Mon Jan 26 20:22:47 2015
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 26 Jan 2015 17:22:47 -0800
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
Message-ID: <CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>

I think it's the VM's job to prevent SOE when executing pure Java code.
But it's not easy.
http://agis.io/2014/03/25/contiguous-stacks-in-go.html

On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Also, I know I posted a link to .NET's CER earlier, but found this
> article, which goes into a bit more detail (including a section on their
> way to harden SOE cases):
> https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might be of
> general interest on how a comparable managed environment attempts to handle
> this.
>
> On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> That's true: if you do this via " lock(); try {...} finally {unlock()}",
>> then you may as well check right before lock.  But since this would be a
>> library exposed to others, it's not necessarily the case that they pair it
>> up like that.  Anyway, this is yet another wrinkle in here.
>>
>> On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>>
>>> Vitaly Davidovich wrote:
>>>
>>> > Yeah, I think you'd check right before calling unlock (but, how do
>>> > you ensure the caller could reach that far and not hit SOE -
>>> > turtles all the way down!).
>>>
>>> You'd really have to check before calling lock() to make sure
>>> there's enough stack for unlock()... So basically, the futility in
>>> any attempt to harden lock() and unlock() individually is that
>>> they're called in pairs from higher-level code. We'd have to find a
>>> way to make lock() fail-fast if there's not enough stack for the
>>> corresponding unlock().
>>>
>>> I'm satisfied with that explanation for now, though I may revisit
>>> the discussion later in regard to the specific code I'm writing. :)
>>>
>>> Cheers,
>>> Justin
>>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/3c4f12eb/attachment-0001.html>

From vitalyd at gmail.com  Mon Jan 26 22:48:18 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 26 Jan 2015 22:48:18 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
Message-ID: <CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>

Correct me if I'm wrong, but aren't segmented/contiguous stacks really
meant for runtimes supporting lightweight threads (e.g. goroutines for Go)?
In those cases, you can't have full fledged native stacks for them as mem
consumption (at least virtual memory) would go through the roof.  So,
they're not really meant for preventing SOE.  Moreover, even if you segment
or reallocate stacks on heap, you could still ultimately run out of space -
not sure how either of these approaches solve that.

Also, it's less than ideal to have each function prologue code checking
whether a new stack needs to be allocated.

sent from my phone
On Jan 26, 2015 8:22 PM, "Martin Buchholz" <martinrb at google.com> wrote:

> I think it's the VM's job to prevent SOE when executing pure Java code.
> But it's not easy.
> http://agis.io/2014/03/25/contiguous-stacks-in-go.html
>
> On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> Also, I know I posted a link to .NET's CER earlier, but found this
>> article, which goes into a bit more detail (including a section on their
>> way to harden SOE cases):
>> https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might be of
>> general interest on how a comparable managed environment attempts to handle
>> this.
>>
>> On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> That's true: if you do this via " lock(); try {...} finally {unlock()}",
>>> then you may as well check right before lock.  But since this would be a
>>> library exposed to others, it's not necessarily the case that they pair it
>>> up like that.  Anyway, this is yet another wrinkle in here.
>>>
>>> On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com>
>>> wrote:
>>>
>>>> Vitaly Davidovich wrote:
>>>>
>>>> > Yeah, I think you'd check right before calling unlock (but, how do
>>>> > you ensure the caller could reach that far and not hit SOE -
>>>> > turtles all the way down!).
>>>>
>>>> You'd really have to check before calling lock() to make sure
>>>> there's enough stack for unlock()... So basically, the futility in
>>>> any attempt to harden lock() and unlock() individually is that
>>>> they're called in pairs from higher-level code. We'd have to find a
>>>> way to make lock() fail-fast if there's not enough stack for the
>>>> corresponding unlock().
>>>>
>>>> I'm satisfied with that explanation for now, though I may revisit
>>>> the discussion later in regard to the specific code I'm writing. :)
>>>>
>>>> Cheers,
>>>> Justin
>>>>
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150126/988fb0b6/attachment.html>

From peter.levart at gmail.com  Tue Jan 27 03:25:16 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Jan 2015 09:25:16 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>	<54C6282C.3080509@oracle.com>	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>	<54C656DA.4070807@oracle.com>	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
Message-ID: <54C74B6C.30302@gmail.com>

On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
> Correct me if I'm wrong, but aren't segmented/contiguous stacks really
> meant for runtimes supporting lightweight threads (e.g. goroutines for Go)?
> In those cases, you can't have full fledged native stacks for them as mem
> consumption (at least virtual memory) would go through the roof.  So,
> they're not really meant for preventing SOE.  Moreover, even if you segment
> or reallocate stacks on heap, you could still ultimately run out of space -
> not sure how either of these approaches solve that.
>
> Also, it's less than ideal to have each function prologue code checking
> whether a new stack needs to be allocated.

There's no overhead in checking whether StackOverflowError has to be 
thrown. The same kind of trap could be used to reallocate the stack instead.

Peter

>
> sent from my phone
> On Jan 26, 2015 8:22 PM, "Martin Buchholz" <martinrb at google.com> wrote:
>
>> I think it's the VM's job to prevent SOE when executing pure Java code.
>> But it's not easy.
>> http://agis.io/2014/03/25/contiguous-stacks-in-go.html
>>
>> On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> Also, I know I posted a link to .NET's CER earlier, but found this
>>> article, which goes into a bit more detail (including a section on their
>>> way to harden SOE cases):
>>> https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might be of
>>> general interest on how a comparable managed environment attempts to handle
>>> this.
>>>
>>> On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich <vitalyd at gmail.com>
>>> wrote:
>>>
>>>> That's true: if you do this via " lock(); try {...} finally {unlock()}",
>>>> then you may as well check right before lock.  But since this would be a
>>>> library exposed to others, it's not necessarily the case that they pair it
>>>> up like that.  Anyway, this is yet another wrinkle in here.
>>>>
>>>> On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com>
>>>> wrote:
>>>>
>>>>> Vitaly Davidovich wrote:
>>>>>
>>>>>> Yeah, I think you'd check right before calling unlock (but, how do
>>>>>> you ensure the caller could reach that far and not hit SOE -
>>>>>> turtles all the way down!).
>>>>> You'd really have to check before calling lock() to make sure
>>>>> there's enough stack for unlock()... So basically, the futility in
>>>>> any attempt to harden lock() and unlock() individually is that
>>>>> they're called in pairs from higher-level code. We'd have to find a
>>>>> way to make lock() fail-fast if there's not enough stack for the
>>>>> corresponding unlock().
>>>>>
>>>>> I'm satisfied with that explanation for now, though I may revisit
>>>>> the discussion later in regard to the specific code I'm writing. :)
>>>>>
>>>>> Cheers,
>>>>> Justin
>>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/488f218e/attachment.html>

From vitalyd at gmail.com  Tue Jan 27 09:27:45 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 27 Jan 2015 09:27:45 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C74B6C.30302@gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
	<54C74B6C.30302@gmail.com>
Message-ID: <CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>

Wouldn't that require minimum stack size to effectively be equal to page
size? For systems with large pages enabled, not sure that's workable in
this context.

On Tue, Jan 27, 2015 at 3:25 AM, Peter Levart <peter.levart at gmail.com>
wrote:

>  On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
>
> Correct me if I'm wrong, but aren't segmented/contiguous stacks really
> meant for runtimes supporting lightweight threads (e.g. goroutines for Go)?
> In those cases, you can't have full fledged native stacks for them as mem
> consumption (at least virtual memory) would go through the roof.  So,
> they're not really meant for preventing SOE.  Moreover, even if you segment
> or reallocate stacks on heap, you could still ultimately run out of space -
> not sure how either of these approaches solve that.
>
> Also, it's less than ideal to have each function prologue code checking
> whether a new stack needs to be allocated.
>
>
> There's no overhead in checking whether StackOverflowError has to be
> thrown. The same kind of trap could be used to reallocate the stack instead.
>
> Peter
>
>
>
> sent from my phone
> On Jan 26, 2015 8:22 PM, "Martin Buchholz" <martinrb at google.com> <martinrb at google.com> wrote:
>
>
>  I think it's the VM's job to prevent SOE when executing pure Java code.
> But it's not easy.http://agis.io/2014/03/25/contiguous-stacks-in-go.html
>
> On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich <vitalyd at gmail.com> <vitalyd at gmail.com>
> wrote:
>
>
>  Also, I know I posted a link to .NET's CER earlier, but found this
> article, which goes into a bit more detail (including a section on their
> way to harden SOE cases):https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might be of
> general interest on how a comparable managed environment attempts to handle
> this.
>
> On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich <vitalyd at gmail.com> <vitalyd at gmail.com>
> wrote:
>
>
>  That's true: if you do this via " lock(); try {...} finally {unlock()}",
> then you may as well check right before lock.  But since this would be a
> library exposed to others, it's not necessarily the case that they pair it
> up like that.  Anyway, this is yet another wrinkle in here.
>
> On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson <jsampson at guidewire.com> <jsampson at guidewire.com>
> wrote:
>
>
>  Vitaly Davidovich wrote:
>
>
>  Yeah, I think you'd check right before calling unlock (but, how do
> you ensure the caller could reach that far and not hit SOE -
> turtles all the way down!).
>
>  You'd really have to check before calling lock() to make sure
> there's enough stack for unlock()... So basically, the futility in
> any attempt to harden lock() and unlock() individually is that
> they're called in pairs from higher-level code. We'd have to find a
> way to make lock() fail-fast if there's not enough stack for the
> corresponding unlock().
>
> I'm satisfied with that explanation for now, though I may revisit
> the discussion later in regard to the specific code I'm writing. :)
>
> Cheers,
> Justin
>
>
>   _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/c2a6d2b3/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Tue Jan 27 09:55:30 2015
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 27 Jan 2015 15:55:30 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>	<54C6282C.3080509@oracle.com>	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>	<54C656DA.4070807@oracle.com>	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>	<54C74B6C.30302@gmail.com>
	<CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>
Message-ID: <54C7A6E2.1060308@infinite-source.de>

Aren't mixed page allocations an option?

On 27.01.2015 15:27, Vitaly Davidovich wrote:
> Wouldn't that require minimum stack size to effectively be equal to page
> size? For systems with large pages enabled, not sure that's workable in
> this context.
>
> On Tue, Jan 27, 2015 at 3:25 AM, Peter Levart <peter.levart at gmail.com
> <mailto:peter.levart at gmail.com>> wrote:
>
>     On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
>>     Correct me if I'm wrong, but aren't segmented/contiguous stacks really
>>     meant for runtimes supporting lightweight threads (e.g. goroutines for Go)?
>>     In those cases, you can't have full fledged native stacks for them as mem
>>     consumption (at least virtual memory) would go through the roof.  So,
>>     they're not really meant for preventing SOE.  Moreover, even if you segment
>>     or reallocate stacks on heap, you could still ultimately run out of space -
>>     not sure how either of these approaches solve that.
>>
>>     Also, it's less than ideal to have each function prologue code checking
>>     whether a new stack needs to be allocated.
>
>     There's no overhead in checking whether StackOverflowError has to be
>     thrown. The same kind of trap could be used to reallocate the stack
>     instead.
>
>     Peter
>
>
>>     sent from my phone
>>     On Jan 26, 2015 8:22 PM, "Martin Buchholz"<martinrb at google.com>  <mailto:martinrb at google.com>  wrote:
>>
>>>     I think it's the VM's job to prevent SOE when executing pure Java code.
>>>     But it's not easy.
>>>     http://agis.io/2014/03/25/contiguous-stacks-in-go.html
>>>
>>>     On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>
>>>     wrote:
>>>
>>>>     Also, I know I posted a link to .NET's CER earlier, but found this
>>>>     article, which goes into a bit more detail (including a section on their
>>>>     way to harden SOE cases):
>>>>     https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might be of
>>>>     general interest on how a comparable managed environment attempts to handle
>>>>     this.
>>>>
>>>>     On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>
>>>>     wrote:
>>>>
>>>>>     That's true: if you do this via " lock(); try {...} finally {unlock()}",
>>>>>     then you may as well check right before lock.  But since this would be a
>>>>>     library exposed to others, it's not necessarily the case that they pair it
>>>>>     up like that.  Anyway, this is yet another wrinkle in here.
>>>>>
>>>>>     On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson<jsampson at guidewire.com>  <mailto:jsampson at guidewire.com>
>>>>>     wrote:
>>>>>
>>>>>>     Vitaly Davidovich wrote:
>>>>>>
>>>>>>>     Yeah, I think you'd check right before calling unlock (but, how do
>>>>>>>     you ensure the caller could reach that far and not hit SOE -
>>>>>>>     turtles all the way down!).
>>>>>>     You'd really have to check before calling lock() to make sure
>>>>>>     there's enough stack for unlock()... So basically, the futility in
>>>>>>     any attempt to harden lock() and unlock() individually is that
>>>>>>     they're called in pairs from higher-level code. We'd have to find a
>>>>>>     way to make lock() fail-fast if there's not enough stack for the
>>>>>>     corresponding unlock().
>>>>>>
>>>>>>     I'm satisfied with that explanation for now, though I may revisit
>>>>>>     the discussion later in regard to the specific code I'm writing. :)
>>>>>>
>>>>>>     Cheers,
>>>>>>     Justin
>>>>>>
>>>>     _______________________________________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From lev.priima at oracle.com  Tue Jan 27 10:43:18 2015
From: lev.priima at oracle.com (Lev Priima)
Date: Tue, 27 Jan 2015 18:43:18 +0300
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in endless
 thread creation loop if workQueue.take() throws RuntimeException
Message-ID: <54C7B216.90404@oracle.com>

Using TPE w/ custom BlockingQueue and if RuntimeException happens in 
blocking BlockingQueue.take() method then this code

new ThreadPoolExecutor(1, 1, 0, TimeUnit.NANOSECONDS,
     new ArrayBlockingQueue<Runnable>(1) {
         public Runnable take() throws InterruptedException {
             throw new RuntimeException();
         }
     }
).prestartAllCoreThreads();

has an unbounded thread creation loop.

As a result there are many created unbounded threads in RUNNING state 
after printing stack trace to stderr by default 
UncaughtExceptionHandler. And these thread will be cleaned only when 
whole TPE finished.

Is this "Not an Issue"?

-- 
Lev

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/d895f460/attachment.html>

From vitalyd at gmail.com  Tue Jan 27 10:47:36 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 27 Jan 2015 10:47:36 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C7A6E2.1060308@infinite-source.de>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
	<54C74B6C.30302@gmail.com>
	<CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>
	<54C7A6E2.1060308@infinite-source.de>
Message-ID: <CAHjP37FWbzsg-DcPS1PaMmcxmCe9B8mpwBzLrOYKw0mv91D1ZQ@mail.gmail.com>

Sorry, I was thinking of when larger page size becomes the default, not 4k
pages + optional huge page allocations today.

On Tue, Jan 27, 2015 at 9:55 AM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Aren't mixed page allocations an option?
>
> On 27.01.2015 15:27, Vitaly Davidovich wrote:
>
>> Wouldn't that require minimum stack size to effectively be equal to page
>> size? For systems with large pages enabled, not sure that's workable in
>> this context.
>>
>> On Tue, Jan 27, 2015 at 3:25 AM, Peter Levart <peter.levart at gmail.com
>> <mailto:peter.levart at gmail.com>> wrote:
>>
>>     On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
>>
>>>     Correct me if I'm wrong, but aren't segmented/contiguous stacks
>>> really
>>>     meant for runtimes supporting lightweight threads (e.g. goroutines
>>> for Go)?
>>>     In those cases, you can't have full fledged native stacks for them
>>> as mem
>>>     consumption (at least virtual memory) would go through the roof.  So,
>>>     they're not really meant for preventing SOE.  Moreover, even if you
>>> segment
>>>     or reallocate stacks on heap, you could still ultimately run out of
>>> space -
>>>     not sure how either of these approaches solve that.
>>>
>>>     Also, it's less than ideal to have each function prologue code
>>> checking
>>>     whether a new stack needs to be allocated.
>>>
>>
>>     There's no overhead in checking whether StackOverflowError has to be
>>     thrown. The same kind of trap could be used to reallocate the stack
>>     instead.
>>
>>     Peter
>>
>>
>>      sent from my phone
>>>     On Jan 26, 2015 8:22 PM, "Martin Buchholz"<martinrb at google.com>
>>> <mailto:martinrb at google.com>  wrote:
>>>
>>>      I think it's the VM's job to prevent SOE when executing pure Java
>>>> code.
>>>>     But it's not easy.
>>>>     http://agis.io/2014/03/25/contiguous-stacks-in-go.html
>>>>
>>>>     On Mon, Jan 26, 2015 at 12:19 PM, Vitaly Davidovich<
>>>> vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>
>>>>     wrote:
>>>>
>>>>      Also, I know I posted a link to .NET's CER earlier, but found this
>>>>>     article, which goes into a bit more detail (including a section on
>>>>> their
>>>>>     way to harden SOE cases):
>>>>>     https://msdn.microsoft.com/en-us/magazine/cc163716.aspx.  Might
>>>>> be of
>>>>>     general interest on how a comparable managed environment attempts
>>>>> to handle
>>>>>     this.
>>>>>
>>>>>     On Mon, Jan 26, 2015 at 3:16 PM, Vitaly Davidovich<
>>>>> vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>
>>>>>     wrote:
>>>>>
>>>>>      That's true: if you do this via " lock(); try {...} finally
>>>>>> {unlock()}",
>>>>>>     then you may as well check right before lock.  But since this
>>>>>> would be a
>>>>>>     library exposed to others, it's not necessarily the case that
>>>>>> they pair it
>>>>>>     up like that.  Anyway, this is yet another wrinkle in here.
>>>>>>
>>>>>>     On Mon, Jan 26, 2015 at 3:07 PM, Justin Sampson<
>>>>>> jsampson at guidewire.com>  <mailto:jsampson at guidewire.com>
>>>>>>     wrote:
>>>>>>
>>>>>>      Vitaly Davidovich wrote:
>>>>>>>
>>>>>>>      Yeah, I think you'd check right before calling unlock (but, how
>>>>>>>> do
>>>>>>>>     you ensure the caller could reach that far and not hit SOE -
>>>>>>>>     turtles all the way down!).
>>>>>>>>
>>>>>>>     You'd really have to check before calling lock() to make sure
>>>>>>>     there's enough stack for unlock()... So basically, the futility
>>>>>>> in
>>>>>>>     any attempt to harden lock() and unlock() individually is that
>>>>>>>     they're called in pairs from higher-level code. We'd have to
>>>>>>> find a
>>>>>>>     way to make lock() fail-fast if there's not enough stack for the
>>>>>>>     corresponding unlock().
>>>>>>>
>>>>>>>     I'm satisfied with that explanation for now, though I may revisit
>>>>>>>     the discussion later in regard to the specific code I'm writing.
>>>>>>> :)
>>>>>>>
>>>>>>>     Cheers,
>>>>>>>     Justin
>>>>>>>
>>>>>>>      _______________________________________________
>>>>>     Concurrency-interest mailing list
>>>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest@
>>>>> cs.oswego.edu>
>>>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest@
>>> cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/9e04fae8/attachment-0001.html>

From aaron.grunthal at infinite-source.de  Tue Jan 27 11:46:35 2015
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 27 Jan 2015 17:46:35 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37FWbzsg-DcPS1PaMmcxmCe9B8mpwBzLrOYKw0mv91D1ZQ@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>	<54C6282C.3080509@oracle.com>	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>	<54C656DA.4070807@oracle.com>	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>	<54C74B6C.30302@gmail.com>	<CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>	<54C7A6E2.1060308@infinite-source.de>
	<CAHjP37FWbzsg-DcPS1PaMmcxmCe9B8mpwBzLrOYKw0mv91D1ZQ@mail.gmail.com>
Message-ID: <54C7C0EB.5010302@infinite-source.de>

Hum, yes, that makes things more complicated. A global or a per-thread 
option to toggle growable stacks might be necessary then. E.g. a 
threadpool of limited size could choose growable stacks while threads 
that mostly just wait on IO in shallow loops can share pages and use 
smaller, fixed-size stacks.

Instead of attacking the problem from the "JVM should provide the 
illusion of infinite stack space" angle there is another option: 
Avoiding uncontrolled growth in the first place, e.g. through tail call 
optimization.

It wouldn't allow the critical section inside library code to protect 
itself against SOE, but if the most common causes of exploding stacks 
could be flattened it would simply become a problem one has to worry 
less about.
Or at least there would be the option to yell at people and tell them to 
fix their code once they have the necessary tools to do it.

On 27.01.2015 16:47, Vitaly Davidovich wrote:
> Sorry, I was thinking of when larger page size becomes the default, not
> 4k pages + optional huge page allocations today.
>
> On Tue, Jan 27, 2015 at 9:55 AM, Aaron Grunthal
> <aaron.grunthal at infinite-source.de
> <mailto:aaron.grunthal at infinite-source.de>> wrote:
>
>     Aren't mixed page allocations an option?
>
>     On 27.01.2015 15:27, Vitaly Davidovich wrote:
>
>         Wouldn't that require minimum stack size to effectively be equal
>         to page
>         size? For systems with large pages enabled, not sure that's
>         workable in
>         this context.
>
>         On Tue, Jan 27, 2015 at 3:25 AM, Peter Levart
>         <peter.levart at gmail.com <mailto:peter.levart at gmail.com>
>         <mailto:peter.levart at gmail.com
>         <mailto:peter.levart at gmail.com>__>> wrote:
>
>              On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
>
>                  Correct me if I'm wrong, but aren't
>             segmented/contiguous stacks really
>                  meant for runtimes supporting lightweight threads (e.g.
>             goroutines for Go)?
>                  In those cases, you can't have full fledged native
>             stacks for them as mem
>                  consumption (at least virtual memory) would go through
>             the roof.  So,
>                  they're not really meant for preventing SOE.  Moreover,
>             even if you segment
>                  or reallocate stacks on heap, you could still
>             ultimately run out of space -
>                  not sure how either of these approaches solve that.
>
>                  Also, it's less than ideal to have each function
>             prologue code checking
>                  whether a new stack needs to be allocated.
>
>
>              There's no overhead in checking whether StackOverflowError
>         has to be
>              thrown. The same kind of trap could be used to reallocate
>         the stack
>              instead.
>
>              Peter
>
>
>                  sent from my phone
>                  On Jan 26, 2015 8:22 PM, "Martin
>             Buchholz"<martinrb at google.com <mailto:martinrb at google.com>>
>             <mailto:martinrb at google.com <mailto:martinrb at google.com>>
>             wrote:
>
>                      I think it's the VM's job to prevent SOE when
>                 executing pure Java code.
>                      But it's not easy.
>                 http://agis.io/2014/03/25/__contiguous-stacks-in-go.html
>                 <http://agis.io/2014/03/25/contiguous-stacks-in-go.html>
>
>                      On Mon, Jan 26, 2015 at 12:19 PM, Vitaly
>                 Davidovich<vitalyd at gmail.com
>                 <mailto:vitalyd at gmail.com>>  <mailto:vitalyd at gmail.com
>                 <mailto:vitalyd at gmail.com>>
>                      wrote:
>
>                          Also, I know I posted a link to .NET's CER
>                     earlier, but found this
>                          article, which goes into a bit more detail
>                     (including a section on their
>                          way to harden SOE cases):
>                     https://msdn.microsoft.com/en-__us/magazine/cc163716.aspx
>                     <https://msdn.microsoft.com/en-us/magazine/cc163716.aspx>.
>                     Might be of
>                          general interest on how a comparable managed
>                     environment attempts to handle
>                          this.
>
>                          On Mon, Jan 26, 2015 at 3:16 PM, Vitaly
>                     Davidovich<vitalyd at gmail.com
>                     <mailto:vitalyd at gmail.com>>
>                     <mailto:vitalyd at gmail.com <mailto:vitalyd at gmail.com>>
>                          wrote:
>
>                              That's true: if you do this via " lock();
>                         try {...} finally {unlock()}",
>                              then you may as well check right before
>                         lock.  But since this would be a
>                              library exposed to others, it's not
>                         necessarily the case that they pair it
>                              up like that.  Anyway, this is yet another
>                         wrinkle in here.
>
>                              On Mon, Jan 26, 2015 at 3:07 PM, Justin
>                         Sampson<jsampson at guidewire.com
>                         <mailto:jsampson at guidewire.com>__>
>                         <mailto:jsampson at guidewire.com
>                         <mailto:jsampson at guidewire.com>__>
>                              wrote:
>
>                                  Vitaly Davidovich wrote:
>
>                                      Yeah, I think you'd check right
>                                 before calling unlock (but, how do
>                                      you ensure the caller could reach
>                                 that far and not hit SOE -
>                                      turtles all the way down!).
>
>                                  You'd really have to check before
>                             calling lock() to make sure
>                                  there's enough stack for unlock()... So
>                             basically, the futility in
>                                  any attempt to harden lock() and
>                             unlock() individually is that
>                                  they're called in pairs from
>                             higher-level code. We'd have to find a
>                                  way to make lock() fail-fast if there's
>                             not enough stack for the
>                                  corresponding unlock().
>
>                                  I'm satisfied with that explanation for
>                             now, though I may revisit
>                                  the discussion later in regard to the
>                             specific code I'm writing. :)
>
>                                  Cheers,
>                                  Justin
>
>                          _________________________________________________
>                          Concurrency-interest mailing list
>                     Concurrency-interest at cs.__oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>
>                     <mailto:Concurrency-interest at __cs.oswego.edu
>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>                     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>                     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>                  _________________________________________________
>                  Concurrency-interest mailing list
>             Concurrency-interest at cs.__oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at __cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>
>         _________________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.__oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From godmar at gmail.com  Tue Jan 27 12:22:06 2015
From: godmar at gmail.com (Godmar Back)
Date: Tue, 27 Jan 2015 12:22:06 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
Message-ID: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>

Hi,

may I ask two clarifying questions about the FJ design?

Q1: Is my understanding correct that FJ is designed for arbitrary DAGs
where there are no restrictions on which task may join which other, as long
as the resulting dependency graph is a DAG?

Q2: If a computation is "fully strict," which (I believe) means that each
task is always joined by the task that created it, and if a stack-based
implementation like the one in FJ is used, then is it safe to steal during
calls to join() that would otherwise block? E.g. if a task being joined is
already being processed by some other worker, the calling thread can steal
any task from the tail end of other worker's queues? By "tail end" I mean
where the oldest task was pushed that a worker spawned, assuming a FIFO
order for stealing and a LIFO order for the worker's processing.

Or, does the task scheduler need to impose constraints on which tasks can
be stolen even if the computation is fully strict?

Thanks.

 - Godmar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/098accb8/attachment.html>

From vitalyd at gmail.com  Tue Jan 27 13:56:28 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 27 Jan 2015 13:56:28 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C7C0EB.5010302@infinite-source.de>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
	<54C74B6C.30302@gmail.com>
	<CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>
	<54C7A6E2.1060308@infinite-source.de>
	<CAHjP37FWbzsg-DcPS1PaMmcxmCe9B8mpwBzLrOYKw0mv91D1ZQ@mail.gmail.com>
	<54C7C0EB.5010302@infinite-source.de>
Message-ID: <CAHjP37EuxxsQzxVGgYRaTqhYRXqoSwXPs=s987a02E2Tv=NFCg@mail.gmail.com>

So I think the type of SOE that Justin was originally concerned about
(Justin, correct me if I'm wrong) was the type not directly related to your
java code, but rather internal VM code executing "behind your back" when
your code triggers certain VM actions (e.g. class loading, JIT compilation,
etc).  In those cases, the .NET CER concept and having the ability to
"prepare" a section of code (i.e. make sure all requisite classes are
loaded, make sure to either JIT compile upfront or disable compilation of
the handler code, etc) is not bad.  It's still not a hard guarantee, but
provides a bit more assurance that code will succeed.  The CER article
briefly mentions that preparing a region also probes for some amount of
stack space available, although it doesn't go into detail on what this
actually entails.  But on the other hand, if this doesn't guarantee
anything anyway, is there really sufficient benefit to justify implementing
this at the VM level?

On Tue, Jan 27, 2015 at 11:46 AM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Hum, yes, that makes things more complicated. A global or a per-thread
> option to toggle growable stacks might be necessary then. E.g. a threadpool
> of limited size could choose growable stacks while threads that mostly just
> wait on IO in shallow loops can share pages and use smaller, fixed-size
> stacks.
>
> Instead of attacking the problem from the "JVM should provide the illusion
> of infinite stack space" angle there is another option: Avoiding
> uncontrolled growth in the first place, e.g. through tail call optimization.
>
> It wouldn't allow the critical section inside library code to protect
> itself against SOE, but if the most common causes of exploding stacks could
> be flattened it would simply become a problem one has to worry less about.
> Or at least there would be the option to yell at people and tell them to
> fix their code once they have the necessary tools to do it.
>
> On 27.01.2015 16:47, Vitaly Davidovich wrote:
>
>> Sorry, I was thinking of when larger page size becomes the default, not
>> 4k pages + optional huge page allocations today.
>>
>> On Tue, Jan 27, 2015 at 9:55 AM, Aaron Grunthal
>> <aaron.grunthal at infinite-source.de
>> <mailto:aaron.grunthal at infinite-source.de>> wrote:
>>
>>     Aren't mixed page allocations an option?
>>
>>     On 27.01.2015 15:27, Vitaly Davidovich wrote:
>>
>>         Wouldn't that require minimum stack size to effectively be equal
>>         to page
>>         size? For systems with large pages enabled, not sure that's
>>         workable in
>>         this context.
>>
>>         On Tue, Jan 27, 2015 at 3:25 AM, Peter Levart
>>         <peter.levart at gmail.com <mailto:peter.levart at gmail.com>
>>         <mailto:peter.levart at gmail.com
>>
>>         <mailto:peter.levart at gmail.com>__>> wrote:
>>
>>              On 01/27/2015 04:48 AM, Vitaly Davidovich wrote:
>>
>>                  Correct me if I'm wrong, but aren't
>>             segmented/contiguous stacks really
>>                  meant for runtimes supporting lightweight threads (e.g.
>>             goroutines for Go)?
>>                  In those cases, you can't have full fledged native
>>             stacks for them as mem
>>                  consumption (at least virtual memory) would go through
>>             the roof.  So,
>>                  they're not really meant for preventing SOE.  Moreover,
>>             even if you segment
>>                  or reallocate stacks on heap, you could still
>>             ultimately run out of space -
>>                  not sure how either of these approaches solve that.
>>
>>                  Also, it's less than ideal to have each function
>>             prologue code checking
>>                  whether a new stack needs to be allocated.
>>
>>
>>              There's no overhead in checking whether StackOverflowError
>>         has to be
>>              thrown. The same kind of trap could be used to reallocate
>>         the stack
>>              instead.
>>
>>              Peter
>>
>>
>>                  sent from my phone
>>                  On Jan 26, 2015 8:22 PM, "Martin
>>             Buchholz"<martinrb at google.com <mailto:martinrb at google.com>>
>>             <mailto:martinrb at google.com <mailto:martinrb at google.com>>
>>             wrote:
>>
>>                      I think it's the VM's job to prevent SOE when
>>                 executing pure Java code.
>>                      But it's not easy.
>>                 http://agis.io/2014/03/25/__contiguous-stacks-in-go.html
>>                 <http://agis.io/2014/03/25/contiguous-stacks-in-go.html>
>>
>>                      On Mon, Jan 26, 2015 at 12:19 PM, Vitaly
>>                 Davidovich<vitalyd at gmail.com
>>                 <mailto:vitalyd at gmail.com>>  <mailto:vitalyd at gmail.com
>>                 <mailto:vitalyd at gmail.com>>
>>                      wrote:
>>
>>                          Also, I know I posted a link to .NET's CER
>>                     earlier, but found this
>>                          article, which goes into a bit more detail
>>                     (including a section on their
>>                          way to harden SOE cases):
>>                     https://msdn.microsoft.com/en-
>> __us/magazine/cc163716.aspx
>>                     <https://msdn.microsoft.com/
>> en-us/magazine/cc163716.aspx>.
>>                     Might be of
>>                          general interest on how a comparable managed
>>                     environment attempts to handle
>>                          this.
>>
>>                          On Mon, Jan 26, 2015 at 3:16 PM, Vitaly
>>                     Davidovich<vitalyd at gmail.com
>>                     <mailto:vitalyd at gmail.com>>
>>                     <mailto:vitalyd at gmail.com <mailto:vitalyd at gmail.com>>
>>                          wrote:
>>
>>                              That's true: if you do this via " lock();
>>                         try {...} finally {unlock()}",
>>                              then you may as well check right before
>>                         lock.  But since this would be a
>>                              library exposed to others, it's not
>>                         necessarily the case that they pair it
>>                              up like that.  Anyway, this is yet another
>>                         wrinkle in here.
>>
>>                              On Mon, Jan 26, 2015 at 3:07 PM, Justin
>>                         Sampson<jsampson at guidewire.com
>>                         <mailto:jsampson at guidewire.com>__>
>>                         <mailto:jsampson at guidewire.com
>>                         <mailto:jsampson at guidewire.com>__>
>>                              wrote:
>>
>>                                  Vitaly Davidovich wrote:
>>
>>                                      Yeah, I think you'd check right
>>                                 before calling unlock (but, how do
>>                                      you ensure the caller could reach
>>                                 that far and not hit SOE -
>>                                      turtles all the way down!).
>>
>>                                  You'd really have to check before
>>                             calling lock() to make sure
>>                                  there's enough stack for unlock()... So
>>                             basically, the futility in
>>                                  any attempt to harden lock() and
>>                             unlock() individually is that
>>                                  they're called in pairs from
>>                             higher-level code. We'd have to find a
>>                                  way to make lock() fail-fast if there's
>>                             not enough stack for the
>>                                  corresponding unlock().
>>
>>                                  I'm satisfied with that explanation for
>>                             now, though I may revisit
>>                                  the discussion later in regard to the
>>                             specific code I'm writing. :)
>>
>>                                  Cheers,
>>                                  Justin
>>
>>                          ______________________________
>> ___________________
>>                          Concurrency-interest mailing list
>>                     Concurrency-interest at cs.__oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     <mailto:Concurrency-interest at __cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>>
>>                     http://cs.oswego.edu/mailman/_
>> _listinfo/concurrency-interest
>>                     <http://cs.oswego.edu/mailman/
>> listinfo/concurrency-interest>
>>
>>
>>
>>
>>                  _________________________________________________
>>                  Concurrency-interest mailing list
>>             Concurrency-interest at cs.__oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest at __cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>>
>>             http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>             <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>>         _________________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.__oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>     _________________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.__oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/082a3549/attachment-0001.html>

From dl at cs.oswego.edu  Tue Jan 27 15:16:08 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 27 Jan 2015 15:16:08 -0500
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <54C7B216.90404@oracle.com>
References: <54C7B216.90404@oracle.com>
Message-ID: <54C7F208.7050704@cs.oswego.edu>

On 01/27/2015 10:43 AM, Lev Priima wrote:
> Using TPE w/ custom BlockingQueue and if RuntimeException happens in blocking
> BlockingQueue.take() method then this code
>
> new ThreadPoolExecutor(1, 1, 0, TimeUnit.NANOSECONDS,
>      new ArrayBlockingQueue<Runnable>(1) {
>          public Runnable take() throws InterruptedException {
>              throw new RuntimeException();
>          }
>      }
> ).prestartAllCoreThreads();
>
> has an unbounded thread creation loop.
>
> As a result there are many created unbounded threads in RUNNING state after
> printing stack trace to stderr by default UncaughtExceptionHandler. And these
> thread will be cleaned only when whole TPE finished.
>
> Is this "Not an Issue"?
>

It is apparently an issue for someone! But I don't see any spec
violation: If a supplied BlockingQueue not only does not block but also
continuously throws exceptions, then threads using it will be
continuously replaced, given the other TPE construction arguments
here and the call to prestartAllCoreThreads.

Any attempt to change TPE to somehow guess that it should stop trying
to replace threads seems unlikely to solve any actual problem, since
the TPE remains unusable either way.

-Doug


From peter.levart at gmail.com  Tue Jan 27 15:30:04 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 27 Jan 2015 21:30:04 +0100
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C2AEEE.3060607@oracle.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E4CB@sm-ex-01-vm.guidewire.com>	<54C2A085.5090808@oracle.com>	<CAO9V1M+HFGti_q-9BL4+VuL=VrctE23Oq-xu_fqCtZWky16+BQ@mail.gmail.com>
	<54C2AEEE.3060607@oracle.com>
Message-ID: <54C7F54C.9060401@gmail.com>

Hi,

Since you are all hot on this subject, I thought I would ask a related 
question. The JLS in the following paragraph:

http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.5.3

Says:

"In some cases, such as deserialization, the system will need to change 
the final fields of an object after construction. final fields can be 
changed via reflection and other implementation-dependent means. The 
only pattern in which this has reasonable semantics is one in which an 
object is constructed and then the final fields of the object are 
updated. The object should not be made visible to other threads, nor 
should the final fields be read, until all updates to the final fields 
of the object are complete. Freezes of a final field occur both at the 
end of the constructor in which the final field is set, and immediately 
after each modification of a final field via reflection or other special 
mechanism."

I'm interested in last paragraph which says: "Freezes of a final field 
occur both at the end of the constructor in which the final field is 
set, and immediately after each modification of a final field via 
reflection or other special mechanism."

But we know that "freeze" at the end of constructor includes a 
StoreStore fence, which prevents subsequent relaxed stores (of a 
reference to the constructed object for example) to be reordered before 
stores to final fields in constructor, but writing to final fields with 
reflection is implemented with simple Unsafe.putXXXVolatile().

So the question is: Does Unsafe.putVolatileXXX() provide a grater 
guarantee (that subsequent relaxed stores are not reordered before it), 
or does it behave the same as normal Java bytecode volatile field store, 
where no such guarantee is provided and consequently the implementation 
of reflection is not following JLS?

Regards, Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/dc3ff8c7/attachment.html>

From martinrb at google.com  Tue Jan 27 15:31:23 2015
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 27 Jan 2015 12:31:23 -0800
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <54C7B216.90404@oracle.com>
References: <54C7B216.90404@oracle.com>
Message-ID: <CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>

On Tue, Jan 27, 2015 at 7:43 AM, Lev Priima <lev.priima at oracle.com> wrote:

> And these thread will be cleaned only when whole TPE finished.
>
>
Is this really true?  Each thread should be replaced while running and so
the total number of threads retained by the TPE at any one time should be
no more than core pool size.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/ea768a18/attachment.html>

From dl at cs.oswego.edu  Tue Jan 27 15:43:46 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 27 Jan 2015 15:43:46 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
Message-ID: <54C7F882.7000500@cs.oswego.edu>

On 01/27/2015 12:22 PM, Godmar Back wrote:

> Q1: Is my understanding correct that FJ is designed for arbitrary DAGs where
> there are no restrictions on which task may join which other, as long as the
> resulting dependency graph is a DAG?

Yes; it is possible to create antagonistic non-strict DAGS that run
inefficiently, but these are not very commonly programmed by accident.

>
> Q2: If a computation is "fully strict," which (I believe) means that each task
> is always joined by the task that created it, and if a stack-based
> implementation like the one in FJ is used, then is it safe to steal during calls
> to join() that would otherwise block?

At least in the case you describe and a couple of other
special cases mentioned in ForkJoinPool internal documentation.

> E.g. if a task being joined is already
> being processed by some other worker, the calling thread can steal any task from
> the tail end of other worker's queues? By "tail end" I mean where the oldest
> task was pushed that a worker spawned, assuming a FIFO order for stealing and a
> LIFO order for the worker's processing.
>
> Or, does the task scheduler need to impose constraints on which tasks can be
> stolen even if the computation is fully strict?

The only additional constraints imposed are limits to bound
the length of potential ping-pong dependency chains. This is done
to force eventual blocking in case of misuses with cyclic
dependencies, as well as to bound some internal raciness.

-Doug



From lev.priima at oracle.com  Tue Jan 27 16:03:36 2015
From: lev.priima at oracle.com (Lev Priima)
Date: Wed, 28 Jan 2015 00:03:36 +0300
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>
References: <54C7B216.90404@oracle.com>
	<CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>
Message-ID: <54C7FD28.8070708@oracle.com>

Yes. And if we have BlockingQueue w/ some amount of tasks which fail 
with exceptions, same amount of threads(not limited by neither 
maximumPoolSize/corePoolSize) will hang under TPE which takes tasks from 
this queue.

It may cause problems if queue has a big percentage of exception-fail 
tasks and we eventually get OOME while unable to create new native thread.

Lev

On 01/27/2015 11:31 PM, Martin Buchholz wrote:
>
>
> On Tue, Jan 27, 2015 at 7:43 AM, Lev Priima <lev.priima at oracle.com 
> <mailto:lev.priima at oracle.com>> wrote:
>
>     And these thread will be cleaned only when whole TPE finished.
>
>
> Is this really true?  Each thread should be replaced while running and 
> so the total number of threads retained by the TPE at any one time 
> should be no more than core pool size.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/af55924e/attachment.html>

From godmar at gmail.com  Tue Jan 27 16:27:03 2015
From: godmar at gmail.com (Godmar Back)
Date: Tue, 27 Jan 2015 16:27:03 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <54C7F882.7000500@cs.oswego.edu>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
	<54C7F882.7000500@cs.oswego.edu>
Message-ID: <CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>

On Tue, Jan 27, 2015 at 3:43 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 01/27/2015 12:22 PM, Godmar Back wrote:
>
>  Q1: Is my understanding correct that FJ is designed for arbitrary DAGs
>> where
>> there are no restrictions on which task may join which other, as long as
>> the
>> resulting dependency graph is a DAG?
>>
>
> Yes; it is possible to create antagonistic non-strict DAGS that run
> inefficiently, but these are not very commonly programmed by accident.
>
>
>> Q2: If a computation is "fully strict," which (I believe) means that each
>> task
>> is always joined by the task that created it, and if a stack-based
>> implementation like the one in FJ is used, then is it safe to steal
>> during calls
>> to join() that would otherwise block?
>>
>
> At least in the case you describe and a couple of other
> special cases mentioned in ForkJoinPool internal documentation.


What exactly do you mean by ForkJoinPool internal documentation - the
comments in the source code that aren't displayed in the API/HTML
documentation?
If so, which classes specifically would discuss this?  Should I look at
src.zip that comes with JDK 1.8 or something newer?

 - Godmar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/c918959f/attachment.html>

From dl at cs.oswego.edu  Tue Jan 27 18:01:41 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 27 Jan 2015 18:01:41 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>	<54C7F882.7000500@cs.oswego.edu>
	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
Message-ID: <54C818D5.6050108@cs.oswego.edu>

On 01/27/2015 04:27 PM, Godmar Back wrote:

>     At least in the case you describe and a couple of other
>     special cases mentioned in ForkJoinPool internal documentation.
>
>
> What exactly do you mean by ForkJoinPool internal documentation - the comments
> in the source code that aren't displayed in the API/HTML documentation?

The base source of ForkJoinPool is at

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=log

We don't place internal algorithm documentation in public javadocs,
because it is always subject to change, while still complying with
public APIs.

-Doug


From martinrb at google.com  Tue Jan 27 18:08:40 2015
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 27 Jan 2015 15:08:40 -0800
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
	<54C7F882.7000500@cs.oswego.edu>
	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
Message-ID: <CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>

Master source is here:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=markup

On Tue, Jan 27, 2015 at 1:27 PM, Godmar Back <godmar at gmail.com> wrote:

> On Tue, Jan 27, 2015 at 3:43 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 01/27/2015 12:22 PM, Godmar Back wrote:
>>
>>  Q1: Is my understanding correct that FJ is designed for arbitrary DAGs
>>> where
>>> there are no restrictions on which task may join which other, as long as
>>> the
>>> resulting dependency graph is a DAG?
>>>
>>
>> Yes; it is possible to create antagonistic non-strict DAGS that run
>> inefficiently, but these are not very commonly programmed by accident.
>>
>>
>>> Q2: If a computation is "fully strict," which (I believe) means that
>>> each task
>>> is always joined by the task that created it, and if a stack-based
>>> implementation like the one in FJ is used, then is it safe to steal
>>> during calls
>>> to join() that would otherwise block?
>>>
>>
>> At least in the case you describe and a couple of other
>> special cases mentioned in ForkJoinPool internal documentation.
>
>
> What exactly do you mean by ForkJoinPool internal documentation - the
> comments in the source code that aren't displayed in the API/HTML
> documentation?
> If so, which classes specifically would discuss this?  Should I look at
> src.zip that comes with JDK 1.8 or something newer?
>
>  - Godmar
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/8aefd773/attachment.html>

From jsampson at guidewire.com  Tue Jan 27 18:11:07 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Tue, 27 Jan 2015 23:11:07 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37EuxxsQzxVGgYRaTqhYRXqoSwXPs=s987a02E2Tv=NFCg@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E8FF@sm-ex-01-vm.guidewire.com>
	<54C6282C.3080509@oracle.com>
	<CAHjP37FSVHq==GVaLVLMd-hNJoTOmzOfm5=7P1yT+Yx5CPwyLQ@mail.gmail.com>
	<54C656DA.4070807@oracle.com>
	<CAHjP37GY6sOkAnCrn+gCcHd3p+KhuBmNX3HEwZti052ws0Pqkg@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8EB82@sm-ex-01-vm.guidewire.com>
	<CAHjP37Erpz2JjN84Qks5OdDU0=sAdr+p-cT2qdYEbaYR1_ZD4A@mail.gmail.com>
	<CAHjP37Ht3rb9gwvrJV19uvv+Nn4zjZkBHERb7r6U5Bcj8J6sdA@mail.gmail.com>
	<CA+kOe08kXm0ry-PTNaBt648i-QtNYomUFi03PsrBtq8btLLusA@mail.gmail.com>
	<CAHjP37HerzGgfUOWC_MhTuSfV7g7q=F2Vkx1r7C4TVobf3Tbdw@mail.gmail.com>
	<54C74B6C.30302@gmail.com>
	<CAHjP37GAE66HrOn-NMmrMctd3=8q30Cy_0Shkm8UEy3VkCXdXA@mail.gmail.com>
	<54C7A6E2.1060308@infinite-source.de>
	<CAHjP37FWbzsg-DcPS1PaMmcxmCe9B8mpwBzLrOYKw0mv91D1ZQ@mail.gmail.com>
	<54C7C0EB.5010302@infinite-source.de>
	<CAHjP37EuxxsQzxVGgYRaTqhYRXqoSwXPs=s987a02E2Tv=NFCg@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D95349@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> So I think the type of SOE that Justin was originally concerned
> about (Justin, correct me if I'm wrong) was the type not directly
> related to your java code, but rather internal VM code executing
> "behind your back" when your code triggers certain VM actions
> (e.g. class loading, JIT compilation, etc).

I'm really interested in anything that can be reasonably expected
and practically prevented. :)

That is, I explicitly don't care about something like ThreadDeath or
InternalError being thrown between arbitrary instructions. There's
no reasonable model for dealing with such extreme examples of
asynchronous exceptions.

I'm also not thinking about supporting arbitrarily deep, dynamic
stacks, though that would slightly mitigate the issue.

But my intuition (not necessarily accurate) starting this discussion
was that things like OutOfMemoryError and StackOverflowError are
much more reasonable to expect and practical to prevent. If my code
explicitly allocates a new object or array, I know that OOME can
happen at that point. And I know that there are specific things that
a JVM does, like JIT and class loading, that can also trigger OOME.
And similarly for SOE, possibly triggered by any method call plus
those same JVM-internal things.

> In those cases, the .NET CER concept and having the ability to
> "prepare" a section of code (i.e. make sure all requisite classes
> are loaded, make sure to either JIT compile upfront or disable
> compilation of the handler code, etc) is not bad. It's still not a
> hard guarantee, but provides a bit more assurance that code will
> succeed.

Yes, that was very interesting to read about. The notion of
"preparing" a block of code is very close to what I was imagining.
If the JVM were to pre-JIT certain methods _and_ aggressively inline
any embedded method calls, such methods would fail-fast with an SOE
immediately on entry rather than anywhere in the middle.

The .NET mechanism looks too flexible and ugly for my tastes,
though. :) I'd rather see a simple method annotation.

> The CER article briefly mentions that preparing a region also
> probes for some amount of stack space available, although it
> doesn't go into detail on what this actually entails. But on the
> other hand, if this doesn't guarantee anything anyway, is there
> really sufficient benefit to justify implementing this at the VM
> level?

JIT'ing with aggressive inlining would be pretty close to a
guarantee, but it still leaves open the practical issue that Alex
brought up with matched lock()/unlock() pairs.

Come to think of it, though, what if this hypothetical annotation
didn't merely ask the JVM to pre-JIT and inline the body of the
annotated method itself, but _also_ to inline any _call_ to such a
method? Then any code anywhere that does 'lock(); try { ... }
finally { unlock(); }' would have both lock() and unlock() inlined
into it, such that the _calling_ method would fail-fast with SOE if
there's not enough stack for both lock() and unlock().

That's a lot more drastic than I was originally imagining (which was
to annotate some minimal tiny scope for inlining), but it's very
interesting to contemplate. The drawback then becomes one of code
bloat, with all the logic of ReentrantLock and AQS being inlined all
over the place. I don't know whether that is or isn't worthwhile,
but based on the discussion so far I do believe it would be pretty
bullet-proof.

Cheers,
Justin


From peter.levart at gmail.com  Tue Jan 27 18:28:19 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 28 Jan 2015 00:28:19 +0100
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <CAHjP37F8CXwmMRykVwy0gGMzYNFdeRfX3Ttsrg3FBXmhWb3PRw@mail.gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<54C28EE6.4050303@oracle.com>	<CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>	<54C2947C.6090205@oracle.com>
	<CAHjP37F8CXwmMRykVwy0gGMzYNFdeRfX3Ttsrg3FBXmhWb3PRw@mail.gmail.com>
Message-ID: <54C81F13.2090404@gmail.com>

If you want to have writable fields in an object and still construct it 
safely (so that unsafe publication of it to other threads would observe 
it fully initialized), you can merge the functionality of a "wrapper 
reference with final field" and the object into the same class and 
object like:

public class SafeClass {
     private /* volatile or not */ int x, y, z;
     public final SafeClass self;

     public SafeClass(...) {
         x = ...;
         y = ...;
         z = ...;
         self = this;
     }

     public getX() {
         return self.x; // access to x, y or z should go through 'self'.
     }

     public getY() {
         return self.y;
     }
}


So this has the space overhead of a single reference field in existing 
object vs. the overhead of separate object instance with a reference 
field. Another de-reference through 'self' is mandatory before accessing 
the field(s), like with wrapper object, but might be exhibiting better 
memory locality since 'self' is in the same object as other fields.

Just an idea. I don't know if it's practical though.


Peter



On 01/23/2015 07:44 PM, Vitaly Davidovich wrote:
>
> This is why, I believe, (revised) JMM is going to give volatile same 
> treatment as final.
>
> Doug, since you're here, please confirm (or deny!) this.  But this has 
> been discussed in the context of SAP ppc port, IIRC, and all known 
> JVMs already do that.
>
> sent from my phone
>
> On Jan 23, 2015 1:35 PM, "Aleksey Shipilev" 
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>
>     On 23.01.2015 21:20, Vitaly Davidovich wrote:
>     > He's just trying to emulate final field semantics using volatile
>     store +
>     > load in same thread to prevent compiler reordering.  At any rate, as
>     > mentioned before, it looks like JMM will be revised to specify that
>     > volatile writes in a constructor are treated the same way as final
>     > fields, and current JVMs already do that anyway which means you
>     wouldn't
>     > need the "fake" load of wrapper.t.
>
>     Well, could you please stop (trying to) cheat with concurrent things
>     when your libraries are going to be used in my banking and medical
>     software? I will appreciate that. The language has a set of rules,
>     follow them, please, use finals and don't let friends piggyback on
>     data
>     races.
>
>     Thank you,
>     -Aleksey.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/8ab37bcb/attachment.html>

From godmar at gmail.com  Tue Jan 27 18:29:55 2015
From: godmar at gmail.com (Godmar Back)
Date: Tue, 27 Jan 2015 18:29:55 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
	<54C7F882.7000500@cs.oswego.edu>
	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
	<CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>
Message-ID: <CAB4+JYK1mdoNi5L-k+UaDyfMXDhN2SoALpZcn0a=xaQApGSm9w@mail.gmail.com>

I'm having trouble locating the discussion of the fully-strict case in the
comments of ForkJoinPool.java.

My doubts are, specifically - "helping" is described as a more
"conservative" form of leapfrogging. I read "conservative" as that it might
not decide to steal/help whereas fully fledged leap-frogging implementation
would.

Looking at leap frogging, and the leap frog depth rule (Section 3.1) in
particular, I cannot tell whether the leap frog depth rule is necessary for
fully strict computations also or only in the general DAG case. Earlier in
the leap frogging paper (Section 2.4) the authors seem to assume a
fully-strict computation, and imply that the leap frog depth rule is
required. (I don't know when this term was coined, historically, it may be
after the leap frogging paper was published.)  This would mean that even
for fully-strict computations worker threads couldn't steal willy-nilly
when attempting to help. But I'm quite frankly confused about this.

Of course, this mailing list is about FJ, not my troubles reading old
papers; but I would still like to understand better how FJ implements
constraints on its task scheduling and how those depend on the structure of
the underlying computational DAG.
Also, are there any unit tests that attempt to present adverse DAGs to the
FJ framework?

 - Godmar

On Tue, Jan 27, 2015 at 6:08 PM, Martin Buchholz <martinrb at google.com>
wrote:

> Master source is here:
>
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=markup
>
> On Tue, Jan 27, 2015 at 1:27 PM, Godmar Back <godmar at gmail.com> wrote:
>
>> On Tue, Jan 27, 2015 at 3:43 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>> On 01/27/2015 12:22 PM, Godmar Back wrote:
>>>
>>>  Q1: Is my understanding correct that FJ is designed for arbitrary DAGs
>>>> where
>>>> there are no restrictions on which task may join which other, as long
>>>> as the
>>>> resulting dependency graph is a DAG?
>>>>
>>>
>>> Yes; it is possible to create antagonistic non-strict DAGS that run
>>> inefficiently, but these are not very commonly programmed by accident.
>>>
>>>
>>>> Q2: If a computation is "fully strict," which (I believe) means that
>>>> each task
>>>> is always joined by the task that created it, and if a stack-based
>>>> implementation like the one in FJ is used, then is it safe to steal
>>>> during calls
>>>> to join() that would otherwise block?
>>>>
>>>
>>> At least in the case you describe and a couple of other
>>> special cases mentioned in ForkJoinPool internal documentation.
>>
>>
>> What exactly do you mean by ForkJoinPool internal documentation - the
>> comments in the source code that aren't displayed in the API/HTML
>> documentation?
>> If so, which classes specifically would discuss this?  Should I look at
>> src.zip that comes with JDK 1.8 or something newer?
>>
>>  - Godmar
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150127/0108afe8/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Jan 27 19:03:18 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 00:03:18 +0000
Subject: [concurrency-interest] Safe publishing strategy
In-Reply-To: <54C81F13.2090404@gmail.com>
References: <CAO9V1MLuybVESz6Z=FC=Whs8zXypF_eixK49=KLRKxQG_motPQ@mail.gmail.com>	<CA+kOe08APW2kLwdrOvgiGxMtzEwLUP-nv22fS3ks-OiXgnJs-Q@mail.gmail.com>	<CAHjP37G+b_vEtM3Xwg2nvZfo_LSapTECoCKO_6DwCX7-XqQ2=A@mail.gmail.com>	<CAO9V1MJ3NMvcttR1GRCFYF+6J=bKwDnJqsD9+4AGJZHi5yaKxg@mail.gmail.com>	<54C28686.60306@oracle.com>	<CAO9V1MJyNrayWrn5krvQVTZ53ks1xKaJh6hEToDFAaj4nfpL9Q@mail.gmail.com>	<54C28BEE.8010201@oracle.com>	<CAO9V1MK-LNj8oWDBJgACUOVBssF2SoCedsHdp2PQVPufrGFM_g@mail.gmail.com>	<54C28EE6.4050303@oracle.com>	<CAHjP37F7YOVmFP+=OHvVRp64wsK4rzO13o3wucUsfeiR8ZcLxg@mail.gmail.com>	<54C2947C.6090205@oracle.com>	<CAHjP37F8CXwmMRykVwy0gGMzYNFdeRfX3Ttsrg3FBXmhWb3PRw@mail.gmail.com>
	<54C81F13.2090404@gmail.com>
Message-ID: <54C82746.6020200@oracle.com>

It should be possible to throw away unused final field read.

Alex

On 27/01/2015 23:28, Peter Levart wrote:
> If you want to have writable fields in an object and still construct 
> it safely (so that unsafe publication of it to other threads would 
> observe it fully initialized), you can merge the functionality of a 
> "wrapper reference with final field" and the object into the same 
> class and object like:
>
> public class SafeClass {
>     private /* volatile or not */ int x, y, z;
>     public final SafeClass self;
>
>     public SafeClass(...) {
>         x = ...;
>         y = ...;
>         z = ...;
>         self = this;
>     }
>
>     public getX() {
>         return self.x; // access to x, y or z should go through 'self'.
>     }
>
>     public getY() {
>         return self.y;
>     }
> }
>
>
> So this has the space overhead of a single reference field in existing 
> object vs. the overhead of separate object instance with a reference 
> field. Another de-reference through 'self' is mandatory before 
> accessing the field(s), like with wrapper object, but might be 
> exhibiting better memory locality since 'self' is in the same object 
> as other fields.
>
> Just an idea. I don't know if it's practical though.
>
>
> Peter
>
>
>
> On 01/23/2015 07:44 PM, Vitaly Davidovich wrote:
>>
>> This is why, I believe, (revised) JMM is going to give volatile same 
>> treatment as final.
>>
>> Doug, since you're here, please confirm (or deny!) this.  But this 
>> has been discussed in the context of SAP ppc port, IIRC, and all 
>> known JVMs already do that.
>>
>> sent from my phone
>>
>> On Jan 23, 2015 1:35 PM, "Aleksey Shipilev" 
>> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>>     On 23.01.2015 21:20, Vitaly Davidovich wrote:
>>     > He's just trying to emulate final field semantics using
>>     volatile store +
>>     > load in same thread to prevent compiler reordering.  At any
>>     rate, as
>>     > mentioned before, it looks like JMM will be revised to specify that
>>     > volatile writes in a constructor are treated the same way as final
>>     > fields, and current JVMs already do that anyway which means you
>>     wouldn't
>>     > need the "fake" load of wrapper.t.
>>
>>     Well, could you please stop (trying to) cheat with concurrent things
>>     when your libraries are going to be used in my banking and medical
>>     software? I will appreciate that. The language has a set of rules,
>>     follow them, please, use finals and don't let friends piggyback
>>     on data
>>     races.
>>
>>     Thank you,
>>     -Aleksey.
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/9fb86d2f/attachment.html>

From lev.priima at oracle.com  Wed Jan 28 07:27:34 2015
From: lev.priima at oracle.com (Lev Priima)
Date: Wed, 28 Jan 2015 15:27:34 +0300
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <54C862ED.3040404@oracle.com>
References: <54C7B216.90404@oracle.com>	<CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>
	<54C7FD28.8070708@oracle.com> <54C862ED.3040404@oracle.com>
Message-ID: <54C8D5B6.6070504@oracle.com>

Thanks Doug, David, Martin, especially Martin.
Is it worth to update javadoc of  ThreadPoolExecutor#Queuing section 
with this caveat ?

The original confusion in custom queue implementation raise up from 
javadoc, because BlockingQueue.take() interface specification does not 
prohibit explicitly to throw uncaught runtime exception/errors (as any 
other casual java code). But using this method in an exhaustive resource 
allocation loop obliges to deal with exceptional situations in 
work-producing methods more carefully.

Best Regards,
Lev

On 28.01.2015 7:17, David Holmes wrote:
> On 28/01/2015 7:03 AM, Lev Priima wrote:
>> Yes. And if we have BlockingQueue w/ some amount of tasks which fail
>> with exceptions, same amount of threads(not limited by neither
>> maximumPoolSize/corePoolSize) will hang under TPE which takes tasks from
>> this queue.
>>
>> It may cause problems if queue has a big percentage of exception-fail
>> tasks and we eventually get OOME while unable to create new native 
>> thread.
>
> If you use your pathological example then of course you can get into a 
> situation where the thread creation outpaces the thread termination - 
> it takes time and CPU cycles for a thread to actually complete.
>
> A BlockingQueue implementation should not have an expected failure 
> mode that results in regularly throwing Errors or RuntimeExceptions. 
> Such a BQ implementation would need to be fixed in my opinion.
>
> The TPE is working as designed - if errors/runtime-exceptions are 
> encountered the worker thread will terminate and be replaced by a 
> fresh worker. If you keep feeding the worker threads such exceptions 
> then you incur a high rate of thread churn. So don't do that. :)
>
> Cheers,
> David
>
>> Lev
>>
>> On 01/27/2015 11:31 PM, Martin Buchholz wrote:
>>>
>>>
>>> On Tue, Jan 27, 2015 at 7:43 AM, Lev Priima <lev.priima at oracle.com
>>> <mailto:lev.priima at oracle.com>> wrote:
>>>
>>>     And these thread will be cleaned only when whole TPE finished.
>>>
>>>
>>> Is this really true?  Each thread should be replaced while running and
>>> so the total number of threads retained by the TPE at any one time
>>> should be no more than core pool size.
>>


From oleksandr.otenko at oracle.com  Wed Jan 28 08:04:06 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 13:04:06 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1420904890038-11835.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<1420904890038-11835.post@n7.nabble.com>
Message-ID: <54C8DE46.1050900@oracle.com>

You are forgetting, that JIT is free to fuse a pattern like:

if (!v.cas(old,new)) { tmp = v.get();}

into CMPXCHG.

Your atomicity argument is flawed.

Here:

> if ((problem = ar.cas(X, Y)) != X)
>      log.error("BUG - expected X but was $problem")

you expect the atomicity to catch your problem. But atomicity of CAS 
does not matter in this case - because for the assertion to be correct, 
it must hold for all the time until this thread sets the value.

In other words, CAS always succeeding like that means you need to have a 
proof of mutually exclusive access to ar. In which case if (ar.get() != 
X) is just as good a test.


Alex


On 10/01/2015 15:48, thurstonn wrote:
> So this conversation digressed in unexpected ways . . . .
>
> Perhaps, because I wasn't clear.
> I'm not suggesting that the extant
> boolean compareAndSet(T expected, T new) and
> T getAndSet(T new)
>
> be changed/removed anything.
> If you don't want to use the new proposed:
> T cas(T expected, T new)
> then you can just ignore it.
>
>
> But the proposed cas would be faster, simpler, and **has no equivalent** in
> the currrent AtomicReference (because it would be **atomic**).
>
> And I really don't understand the "wouldn't be useful" comments - there are
> many circumstances (state machines, code/runtime visibility) in which it
> comes in handy (I was looking for it and was surprised it wasn't there while
> writing YACQ)
>
> Take the simplest case:
> you can use it as a sort of surrogate runtime assert;
> you're at a place in your code where you expect some shared state to be X,
> but you're lost and want to make sure:
>
> T problem;
> if ((problem = ar.cas(X, Y)) != X)
>      log.error("BUG - expected X but was $problem")
>
> Is any other justification needed (and there are many more potential
> justifications)?
>
> Now, if it's truly not possible/practical to implement cross-platform,
> that's one thing . . .
>
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p11835.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From dl at cs.oswego.edu  Wed Jan 28 08:45:41 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 28 Jan 2015 08:45:41 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <CAB4+JYK1mdoNi5L-k+UaDyfMXDhN2SoALpZcn0a=xaQApGSm9w@mail.gmail.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>	<54C7F882.7000500@cs.oswego.edu>	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>	<CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>
	<CAB4+JYK1mdoNi5L-k+UaDyfMXDhN2SoALpZcn0a=xaQApGSm9w@mail.gmail.com>
Message-ID: <54C8E805.1010402@cs.oswego.edu>

On 01/27/2015 06:29 PM, Godmar Back wrote:

> My doubts are, specifically - "helping" is described as a more "conservative"
>  form of leapfrogging. I read "conservative" as that it might not decide to
> steal/help whereas fully fledged leap-frogging implementation would.

Yes. The original formulations of leap-frogging and some other
work-stealing techniques allowed indefinite spinning
in cases including when the stealer will eventually produce tasks
that can be helped but hasn't. FJ gives up after a while and
blocks/compensates.

(Aside: We also provide a purely continuation-based
form of ForkJoinTask, CountedCompleter, that people can use to
avoid dependent blocking, at the price of having to think in
terms of CPS transforms. We also use this internally in cases where
we don't trust tasks to not indefinitely block. It's unfortunate
that this API is so user-hostile that we didn't even include it
initially.)

> Earlier in the leap frogging paper (Section 2.4) the authors seem to assume a
> fully-strict computation, and imply that the leap frog depth rule is
> required. (I don't know when this term was coined, historically, it may be
> after the leap frogging paper was published.)

Yes, it was.

> This would mean that even for fully-strict computations worker threads
> couldn't steal willy-nilly when attempting to help. But I'm quite frankly
> confused about this.

There are only a few scenarios that must be avoided, that
don't differ much in strict vs non-strict cases. You cannot
create a dependency cycle that could cause
(A waitsFor B) and (B waitsFor A). In some pure trees, this can't happen
anyway under the usual steal rules (so the very first non-JDK versions
of FJ, supporting only these case did not even need to track/check).

>  Also, are there any unit tests that attempt to present
> adverse DAGs to the FJ framework?
>

A few are in the tck tests (although not highlighted as such). Others
aren't integrated yet into any repository. One of these days...

-Doug


From gergg at cox.net  Wed Jan 28 09:58:15 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 28 Jan 2015 08:58:15 -0600
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <kCEf1p00S02hR0p01CEiM3>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<kCEf1p00S02hR0p01CEiM3>
Message-ID: <0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>

I personally try very hard not to catch Error/Throwable, ever, and count on the program terminating and restarting.  I use transactional behavior where needed, to make state survive across restarts, and try and design operations of my software systems to be as stateless as possible so that restarts are in general, meaningless to the outcome of the software systems work.  If StackOverflowError is caught someplace. I would consider that a big problem for the design of the software system.

Gregg Wonderly

> On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com> wrote:
> 
> Consider this incredibly simple lock implementation:
> 
> public final class SimpleLock extends AbstractOwnableSynchronizer {
>  private final AtomicBoolean state = new AtomicBoolean();
>  public final boolean tryLock() {
>    if (state.compareAndSet(false, true)) {
>      setExclusiveOwnerThread(Thread.currentThread());
>      return true;
>    } else {
>      return false;
>    }
>  }
>  public final void unlock() {
>    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>      throw new IllegalMonitorStateException();
>    } else {
>      setExclusiveOwnerThread(null);
>      state.set(false);
>    }
>  }
> }
> 
> It's possible for the CAS in tryLock() to succeed and then for the
> very next line to blow up with a synchronous StackOverflowError,
> leaving the state variable true even though tryLock() is throwing.
> This is the only kind of case that I'm looking for advice about,
> nothing more general.
> 
> So far the only responses have been "it's impossible to do anything
> so don't even try." If that's really the end of it then I'll rest
> easy knowing I've done my due diligence.
> 
> But it seems so trivial to recognize that this tryLock() method
> doesn't do any heap allocation, isn't recursive, and only calls
> static or final methods, most of them being intrinsic. So I was just
> wondering if it might be plausible to annotate such a method, as a
> request to the compiler to confirm those facts and as a request to
> the JVM to fail fast on entry by allocating enough stack space up
> front.
> 
> I wasn't imagining any kind of suppression of errors or more general
> handling of arbitrarily complicated code, just avoidance of common
> errors by failing fast when it's easy to do so.
> 
> Cheers,
> Justin
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From viktor.klang at gmail.com  Wed Jan 28 10:24:51 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 28 Jan 2015 16:24:51 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
Message-ID: <CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>

Is there a canonical definition of which Throwables should never be caught?

-- 
Cheers,
?
On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:

> I personally try very hard not to catch Error/Throwable, ever, and count
> on the program terminating and restarting.  I use transactional behavior
> where needed, to make state survive across restarts, and try and design
> operations of my software systems to be as stateless as possible so that
> restarts are in general, meaningless to the outcome of the software systems
> work.  If StackOverflowError is caught someplace. I would consider that a
> big problem for the design of the software system.
>
> Gregg Wonderly
>
> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
> wrote:
> >
> > Consider this incredibly simple lock implementation:
> >
> > public final class SimpleLock extends AbstractOwnableSynchronizer {
> >  private final AtomicBoolean state = new AtomicBoolean();
> >  public final boolean tryLock() {
> >    if (state.compareAndSet(false, true)) {
> >      setExclusiveOwnerThread(Thread.currentThread());
> >      return true;
> >    } else {
> >      return false;
> >    }
> >  }
> >  public final void unlock() {
> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
> >      throw new IllegalMonitorStateException();
> >    } else {
> >      setExclusiveOwnerThread(null);
> >      state.set(false);
> >    }
> >  }
> > }
> >
> > It's possible for the CAS in tryLock() to succeed and then for the
> > very next line to blow up with a synchronous StackOverflowError,
> > leaving the state variable true even though tryLock() is throwing.
> > This is the only kind of case that I'm looking for advice about,
> > nothing more general.
> >
> > So far the only responses have been "it's impossible to do anything
> > so don't even try." If that's really the end of it then I'll rest
> > easy knowing I've done my due diligence.
> >
> > But it seems so trivial to recognize that this tryLock() method
> > doesn't do any heap allocation, isn't recursive, and only calls
> > static or final methods, most of them being intrinsic. So I was just
> > wondering if it might be plausible to annotate such a method, as a
> > request to the compiler to confirm those facts and as a request to
> > the JVM to fail fast on entry by allocating enough stack space up
> > front.
> >
> > I wasn't imagining any kind of suppression of errors or more general
> > handling of arbitrarily complicated code, just avoidance of common
> > errors by failing fast when it's easy to do so.
> >
> > Cheers,
> > Justin
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/ab8f6a47/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 11:15:48 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 16:15:48 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
Message-ID: <54C90B34.6020108@oracle.com>

Won't that mean it is not a throwable, but process termination?

Alex


On 28/01/2015 15:24, Viktor Klang wrote:
>
> Is there a canonical definition of which Throwables should never be 
> caught?
>
> -- 
> Cheers,
> ?
>
> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net 
> <mailto:gergg at cox.net>> wrote:
>
>     I personally try very hard not to catch Error/Throwable, ever, and
>     count on the program terminating and restarting.  I use
>     transactional behavior where needed, to make state survive across
>     restarts, and try and design operations of my software systems to
>     be as stateless as possible so that restarts are in general,
>     meaningless to the outcome of the software systems work.  If
>     StackOverflowError is caught someplace. I would consider that a
>     big problem for the design of the software system.
>
>     Gregg Wonderly
>
>     > On Jan 25, 2015, at 6:06 AM, Justin Sampson
>     <jsampson at guidewire.com <mailto:jsampson at guidewire.com>> wrote:
>     >
>     > Consider this incredibly simple lock implementation:
>     >
>     > public final class SimpleLock extends AbstractOwnableSynchronizer {
>     >  private final AtomicBoolean state = new AtomicBoolean();
>     >  public final boolean tryLock() {
>     >    if (state.compareAndSet(false, true)) {
>     >      setExclusiveOwnerThread(Thread.currentThread());
>     >      return true;
>     >    } else {
>     >      return false;
>     >    }
>     >  }
>     >  public final void unlock() {
>     >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>     >      throw new IllegalMonitorStateException();
>     >    } else {
>     >      setExclusiveOwnerThread(null);
>     >      state.set(false);
>     >    }
>     >  }
>     > }
>     >
>     > It's possible for the CAS in tryLock() to succeed and then for the
>     > very next line to blow up with a synchronous StackOverflowError,
>     > leaving the state variable true even though tryLock() is throwing.
>     > This is the only kind of case that I'm looking for advice about,
>     > nothing more general.
>     >
>     > So far the only responses have been "it's impossible to do anything
>     > so don't even try." If that's really the end of it then I'll rest
>     > easy knowing I've done my due diligence.
>     >
>     > But it seems so trivial to recognize that this tryLock() method
>     > doesn't do any heap allocation, isn't recursive, and only calls
>     > static or final methods, most of them being intrinsic. So I was just
>     > wondering if it might be plausible to annotate such a method, as a
>     > request to the compiler to confirm those facts and as a request to
>     > the JVM to fail fast on entry by allocating enough stack space up
>     > front.
>     >
>     > I wasn't imagining any kind of suppression of errors or more general
>     > handling of arbitrarily complicated code, just avoidance of common
>     > errors by failing fast when it's easy to do so.
>     >
>     > Cheers,
>     > Justin
>     >
>     > _______________________________________________
>     > Concurrency-interest mailing list
>     > Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/53ac0725/attachment.html>

From vitalyd at gmail.com  Wed Jan 28 11:48:20 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 11:48:20 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C90B34.6020108@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
Message-ID: <CAHjP37FSh4ygd908RKFHEPQeP2rP41L7i1w-TjhTDbkGtJzuKA@mail.gmail.com>

Right.  At the very least, you're going to have (I hope, for operators'
sake at least) some catch handler that's going to leave a trace in the log
of an exception occurring with any pertinent details (stack trace, at
minimum).  I think the key point is to exit/terminate the process after
doing error reporting, and not continue in unknown state.

On Wed, Jan 28, 2015 at 11:15 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Won't that mean it is not a throwable, but process termination?
>
> Alex
>
>
>
> On 28/01/2015 15:24, Viktor Klang wrote:
>
> Is there a canonical definition of which Throwables should never be caught?
>
> --
> Cheers,
> ?
> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:
>
>> I personally try very hard not to catch Error/Throwable, ever, and count
>> on the program terminating and restarting.  I use transactional behavior
>> where needed, to make state survive across restarts, and try and design
>> operations of my software systems to be as stateless as possible so that
>> restarts are in general, meaningless to the outcome of the software systems
>> work.  If StackOverflowError is caught someplace. I would consider that a
>> big problem for the design of the software system.
>>
>> Gregg Wonderly
>>
>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>> >
>> > Consider this incredibly simple lock implementation:
>> >
>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>> >  private final AtomicBoolean state = new AtomicBoolean();
>> >  public final boolean tryLock() {
>> >    if (state.compareAndSet(false, true)) {
>> >      setExclusiveOwnerThread(Thread.currentThread());
>> >      return true;
>> >    } else {
>> >      return false;
>> >    }
>> >  }
>> >  public final void unlock() {
>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>> >      throw new IllegalMonitorStateException();
>> >    } else {
>> >      setExclusiveOwnerThread(null);
>> >      state.set(false);
>> >    }
>> >  }
>> > }
>> >
>> > It's possible for the CAS in tryLock() to succeed and then for the
>> > very next line to blow up with a synchronous StackOverflowError,
>> > leaving the state variable true even though tryLock() is throwing.
>> > This is the only kind of case that I'm looking for advice about,
>> > nothing more general.
>> >
>> > So far the only responses have been "it's impossible to do anything
>> > so don't even try." If that's really the end of it then I'll rest
>> > easy knowing I've done my due diligence.
>> >
>> > But it seems so trivial to recognize that this tryLock() method
>> > doesn't do any heap allocation, isn't recursive, and only calls
>> > static or final methods, most of them being intrinsic. So I was just
>> > wondering if it might be plausible to annotate such a method, as a
>> > request to the compiler to confirm those facts and as a request to
>> > the JVM to fail fast on entry by allocating enough stack space up
>> > front.
>> >
>> > I wasn't imagining any kind of suppression of errors or more general
>> > handling of arbitrarily complicated code, just avoidance of common
>> > errors by failing fast when it's easy to do so.
>> >
>> > Cheers,
>> > Justin
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/19eaf900/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 11:48:01 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 16:48:01 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54B0FB69.7080009@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com>
Message-ID: <54C912C1.3030208@oracle.com>

Will ARM guarantee that if CAS fails, it never returns expected as the 
old value?

Eg, two concurrent CAS read the same expected value, but one of CASes 
fails, because the other one succeeded and invalidated the cache line 
the other CAS has already read. Will the failing CAS re-read it and 
reiterate until successful, or fail quietly returning expected?

I think this is an important distinction: not just returning old value, 
but also guaranteeing that old == expected means CAS succeeded.

Alex


On 10/01/2015 10:14, Andrew Haley wrote:
> On 10/01/15 00:20, Doug Lea wrote:
>> On 01/09/2015 04:57 PM, thurstonn wrote:
>>> I was curious as to why there isn't an AtomicReference CAS:
>>>
>>> T cas(T expectedValue, T newValue)
>>>
>>> where the returned T represents the oldValue (if it == expectedValue, then
>>> CAS succeeded).
>> Because there is no equivalent on processors (including ARM, POWER)
>> implementing compareAndSet using load-linked/store-conditional.
>> (C++11/C11 make the same choice for the same reason.)
> I don't understand what you mean.  Writing a CAS with this signature
> would seem to me to be pretty straightforward on ARM.  I'm wondering
> if you're referring to some specific atomicity or ordering guarantees,
> but I'm not aware of any problem with that either.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Wed Jan 28 11:54:15 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 28 Jan 2015 17:54:15 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C90B34.6020108@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
Message-ID: <CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>

So you're saying that all Throwables are safe (as in "the JVM is still
fine") to catch?

Perm-gen OOME?
SOE?
UnknownError?

On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Won't that mean it is not a throwable, but process termination?
>
> Alex
>
>
>
> On 28/01/2015 15:24, Viktor Klang wrote:
>
> Is there a canonical definition of which Throwables should never be caught?
>
> --
> Cheers,
> ?
> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:
>
>> I personally try very hard not to catch Error/Throwable, ever, and count
>> on the program terminating and restarting.  I use transactional behavior
>> where needed, to make state survive across restarts, and try and design
>> operations of my software systems to be as stateless as possible so that
>> restarts are in general, meaningless to the outcome of the software systems
>> work.  If StackOverflowError is caught someplace. I would consider that a
>> big problem for the design of the software system.
>>
>> Gregg Wonderly
>>
>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
>> wrote:
>> >
>> > Consider this incredibly simple lock implementation:
>> >
>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>> >  private final AtomicBoolean state = new AtomicBoolean();
>> >  public final boolean tryLock() {
>> >    if (state.compareAndSet(false, true)) {
>> >      setExclusiveOwnerThread(Thread.currentThread());
>> >      return true;
>> >    } else {
>> >      return false;
>> >    }
>> >  }
>> >  public final void unlock() {
>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>> >      throw new IllegalMonitorStateException();
>> >    } else {
>> >      setExclusiveOwnerThread(null);
>> >      state.set(false);
>> >    }
>> >  }
>> > }
>> >
>> > It's possible for the CAS in tryLock() to succeed and then for the
>> > very next line to blow up with a synchronous StackOverflowError,
>> > leaving the state variable true even though tryLock() is throwing.
>> > This is the only kind of case that I'm looking for advice about,
>> > nothing more general.
>> >
>> > So far the only responses have been "it's impossible to do anything
>> > so don't even try." If that's really the end of it then I'll rest
>> > easy knowing I've done my due diligence.
>> >
>> > But it seems so trivial to recognize that this tryLock() method
>> > doesn't do any heap allocation, isn't recursive, and only calls
>> > static or final methods, most of them being intrinsic. So I was just
>> > wondering if it might be plausible to annotate such a method, as a
>> > request to the compiler to confirm those facts and as a request to
>> > the JVM to fail fast on entry by allocating enough stack space up
>> > front.
>> >
>> > I wasn't imagining any kind of suppression of errors or more general
>> > handling of arbitrarily complicated code, just avoidance of common
>> > errors by failing fast when it's easy to do so.
>> >
>> > Cheers,
>> > Justin
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/35e0f718/attachment.html>

From aph at redhat.com  Wed Jan 28 12:11:59 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Jan 2015 17:11:59 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C912C1.3030208@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com> <54C912C1.3030208@oracle.com>
Message-ID: <54C9185F.4080700@redhat.com>

On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
> Will ARM guarantee that if CAS fails, it never returns expected as the 
> old value?

Yes.

> Eg, two concurrent CAS read the same expected value, but one of CASes 
> fails, because the other one succeeded and invalidated the cache line 
> the other CAS has already read. Will the failing CAS re-read it and 
> reiterate until successful, or fail quietly returning expected?
> 
> I think this is an important distinction: not just returning old value, 
> but also guaranteeing that old == expected means CAS succeeded.

That's up to you.  The code is

retry:	ldarx r1, [address]	
	cmp r1, expected
	bne fail
	stlrx rtmp, r1, [address]
	cbnz tmp, retry	// try again if there was a race
fail:

Andrew.


From oleksandr.otenko at oracle.com  Wed Jan 28 12:20:51 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 17:20:51 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C9185F.4080700@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com> <54C912C1.3030208@oracle.com>
	<54C9185F.4080700@redhat.com>
Message-ID: <54C91A73.2000207@oracle.com>

On 28/01/2015 17:11, Andrew Haley wrote:
> On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
>> Will ARM guarantee that if CAS fails, it never returns expected as the
>> old value?
> Yes.

Thank you. But the code below implies that no, it doesn't, and one has 
to actually write the code written in Java in another branch of this 
discussion.

In other words, you can't write a useful T compareAndSwap(T prev, T 
next), where by comparing the return value to prev one can tell if the 
new value was set to next, without a loop somewhere. (Except 
architectures that already have instruction like x86's CMPXCHG that does 
provide that strong guarantee)

Alex

>
>> Eg, two concurrent CAS read the same expected value, but one of CASes
>> fails, because the other one succeeded and invalidated the cache line
>> the other CAS has already read. Will the failing CAS re-read it and
>> reiterate until successful, or fail quietly returning expected?
>>
>> I think this is an important distinction: not just returning old value,
>> but also guaranteeing that old == expected means CAS succeeded.
> That's up to you.  The code is
>
> retry:	ldarx r1, [address]	
> 	cmp r1, expected
> 	bne fail
> 	stlrx rtmp, r1, [address]
> 	cbnz tmp, retry	// try again if there was a race
> fail:
>
> Andrew.
>


From martinrb at google.com  Wed Jan 28 12:35:25 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 28 Jan 2015 09:35:25 -0800
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <54C8D5B6.6070504@oracle.com>
References: <54C7B216.90404@oracle.com>
	<CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>
	<54C7FD28.8070708@oracle.com> <54C862ED.3040404@oracle.com>
	<54C8D5B6.6070504@oracle.com>
Message-ID: <CA+kOe0-evOfrT7L-3ngY18UL8r5LMydU6CDHn3+SfjUr8TOv1A@mail.gmail.com>

It's hard for me to think of something we could add to the javadoc that
would actually help future users enough to offset the confusion of adding
subtleties of rarely encountered behavior.  I also don't see an easy way to
improve the pool's reaction to exceptions coming from the queue.  Right now
the reporting mechanism is the uncaught exception handler, which is under
the user's control, although it is not obvious.

On Wed, Jan 28, 2015 at 4:27 AM, Lev Priima <lev.priima at oracle.com> wrote:

> Thanks Doug, David, Martin, especially Martin.
> Is it worth to update javadoc of  ThreadPoolExecutor#Queuing section with
> this caveat ?
>
> The original confusion in custom queue implementation raise up from
> javadoc, because BlockingQueue.take() interface specification does not
> prohibit explicitly to throw uncaught runtime exception/errors (as any
> other casual java code). But using this method in an exhaustive resource
> allocation loop obliges to deal with exceptional situations in
> work-producing methods more carefully.
>
> Best Regards,
> Lev
>
>
> On 28.01.2015 7:17, David Holmes wrote:
>
>> On 28/01/2015 7:03 AM, Lev Priima wrote:
>>
>>> Yes. And if we have BlockingQueue w/ some amount of tasks which fail
>>> with exceptions, same amount of threads(not limited by neither
>>> maximumPoolSize/corePoolSize) will hang under TPE which takes tasks from
>>> this queue.
>>>
>>> It may cause problems if queue has a big percentage of exception-fail
>>> tasks and we eventually get OOME while unable to create new native
>>> thread.
>>>
>>
>> If you use your pathological example then of course you can get into a
>> situation where the thread creation outpaces the thread termination - it
>> takes time and CPU cycles for a thread to actually complete.
>>
>> A BlockingQueue implementation should not have an expected failure mode
>> that results in regularly throwing Errors or RuntimeExceptions. Such a BQ
>> implementation would need to be fixed in my opinion.
>>
>> The TPE is working as designed - if errors/runtime-exceptions are
>> encountered the worker thread will terminate and be replaced by a fresh
>> worker. If you keep feeding the worker threads such exceptions then you
>> incur a high rate of thread churn. So don't do that. :)
>>
>> Cheers,
>> David
>>
>>  Lev
>>>
>>> On 01/27/2015 11:31 PM, Martin Buchholz wrote:
>>>
>>>>
>>>>
>>>> On Tue, Jan 27, 2015 at 7:43 AM, Lev Priima <lev.priima at oracle.com
>>>> <mailto:lev.priima at oracle.com>> wrote:
>>>>
>>>>     And these thread will be cleaned only when whole TPE finished.
>>>>
>>>>
>>>> Is this really true?  Each thread should be replaced while running and
>>>> so the total number of threads retained by the TPE at any one time
>>>> should be no more than core pool size.
>>>>
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/11c9368a/attachment.html>

From aph at redhat.com  Wed Jan 28 12:35:48 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Jan 2015 17:35:48 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C91A73.2000207@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com> <54C912C1.3030208@oracle.com>
	<54C9185F.4080700@redhat.com> <54C91A73.2000207@oracle.com>
Message-ID: <54C91DF4.1010607@redhat.com>

On 01/28/2015 05:20 PM, Oleksandr Otenko wrote:
> On 28/01/2015 17:11, Andrew Haley wrote:
>> On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
>>> Will ARM guarantee that if CAS fails, it never returns expected as the
>>> old value?
>> Yes.
> 
> Thank you. But the code below implies that no, it doesn't, and one has 
> to actually write the code written in Java in another branch of this 
> discussion.
>
> In other words, you can't write a useful T compareAndSwap(T prev, T 
> next), where by comparing the return value to prev one can tell if the 
> new value was set to next, without a loop somewhere. (Except
> architectures that already have instruction like x86's CMPXCHG that does 
> provide that strong guarantee)

Why does this matter if there's a loop somewhere?  You have to retry
if there is a race, whether that gets done automagically by the
hardware or by the software.

Andrew.

From rk at rkuhn.info  Wed Jan 28 12:39:27 2015
From: rk at rkuhn.info (Roland Kuhn)
Date: Wed, 28 Jan 2015 18:39:27 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C90B34.6020108@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
Message-ID: <DFCE9230-40A2-4DFD-8CD7-1494F148BDE5@rkuhn.info>

?Throwable? and ?Catchable? are different words, it is a pity that not both concepts have been implemented.

Regards,

Roland

> 28 jan 2015 kl. 17:15 skrev Oleksandr Otenko <oleksandr.otenko at oracle.com>:
> 
> Won't that mean it is not a throwable, but process termination?
> 
> Alex
> 
> 
> On 28/01/2015 15:24, Viktor Klang wrote:
>> Is there a canonical definition of which Throwables should never be caught?
>> 
>> -- 
>> Cheers,
>> ?
>> 
>> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net <mailto:gergg at cox.net>> wrote:
>> I personally try very hard not to catch Error/Throwable, ever, and count on the program terminating and restarting.  I use transactional behavior where needed, to make state survive across restarts, and try and design operations of my software systems to be as stateless as possible so that restarts are in general, meaningless to the outcome of the software systems work.  If StackOverflowError is caught someplace. I would consider that a big problem for the design of the software system.
>> 
>> Gregg Wonderly
>> 
>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com <mailto:jsampson at guidewire.com>> wrote:
>> >
>> > Consider this incredibly simple lock implementation:
>> >
>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>> >  private final AtomicBoolean state = new AtomicBoolean();
>> >  public final boolean tryLock() {
>> >    if (state.compareAndSet(false, true)) {
>> >      setExclusiveOwnerThread(Thread.currentThread());
>> >      return true;
>> >    } else {
>> >      return false;
>> >    }
>> >  }
>> >  public final void unlock() {
>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>> >      throw new IllegalMonitorStateException();
>> >    } else {
>> >      setExclusiveOwnerThread(null);
>> >      state.set(false);
>> >    }
>> >  }
>> > }
>> >
>> > It's possible for the CAS in tryLock() to succeed and then for the
>> > very next line to blow up with a synchronous StackOverflowError,
>> > leaving the state variable true even though tryLock() is throwing.
>> > This is the only kind of case that I'm looking for advice about,
>> > nothing more general.
>> >
>> > So far the only responses have been "it's impossible to do anything
>> > so don't even try." If that's really the end of it then I'll rest
>> > easy knowing I've done my due diligence.
>> >
>> > But it seems so trivial to recognize that this tryLock() method
>> > doesn't do any heap allocation, isn't recursive, and only calls
>> > static or final methods, most of them being intrinsic. So I was just
>> > wondering if it might be plausible to annotate such a method, as a
>> > request to the compiler to confirm those facts and as a request to
>> > the JVM to fail fast on entry by allocating enough stack space up
>> > front.
>> >
>> > I wasn't imagining any kind of suppression of errors or more general
>> > handling of arbitrarily complicated code, just avoidance of common
>> > errors by failing fast when it's easy to do so.
>> >
>> > Cheers,
>> > Justin
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--
Simplicity and elegance are unpopular because they require hard work and discipline to achieve and education to be appreciated.
  -- Dijkstra

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/29d5b476/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 12:40:45 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 17:40:45 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C91DF4.1010607@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com> <54C912C1.3030208@oracle.com>
	<54C9185F.4080700@redhat.com> <54C91A73.2000207@oracle.com>
	<54C91DF4.1010607@redhat.com>
Message-ID: <54C91F1D.80600@oracle.com>

Well, it doesn't - except for the sake of discussion, adding the 
intrinsic isn't adding anything on top of what one could write in Java.

Alex

On 28/01/2015 17:35, Andrew Haley wrote:
> On 01/28/2015 05:20 PM, Oleksandr Otenko wrote:
>> On 28/01/2015 17:11, Andrew Haley wrote:
>>> On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
>>>> Will ARM guarantee that if CAS fails, it never returns expected as the
>>>> old value?
>>> Yes.
>> Thank you. But the code below implies that no, it doesn't, and one has
>> to actually write the code written in Java in another branch of this
>> discussion.
>>
>> In other words, you can't write a useful T compareAndSwap(T prev, T
>> next), where by comparing the return value to prev one can tell if the
>> new value was set to next, without a loop somewhere. (Except
>> architectures that already have instruction like x86's CMPXCHG that does
>> provide that strong guarantee)
> Why does this matter if there's a loop somewhere?  You have to retry
> if there is a race, whether that gets done automagically by the
> hardware or by the software.
>
> Andrew.


From vitalyd at gmail.com  Wed Jan 28 12:41:41 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 12:41:41 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C9185F.4080700@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
Message-ID: <CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>

Well, is it really "up to you" if you were to have the signature that
returns T instead of boolean? Caller would not have a way to distinguish
success vs failure.  This also implies that CAS using LL/SC would have to
spin and delay the operation for indeterminate amount of time.

On Wed, Jan 28, 2015 at 12:11 PM, Andrew Haley <aph at redhat.com> wrote:

> On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
> > Will ARM guarantee that if CAS fails, it never returns expected as the
> > old value?
>
> Yes.
>
> > Eg, two concurrent CAS read the same expected value, but one of CASes
> > fails, because the other one succeeded and invalidated the cache line
> > the other CAS has already read. Will the failing CAS re-read it and
> > reiterate until successful, or fail quietly returning expected?
> >
> > I think this is an important distinction: not just returning old value,
> > but also guaranteeing that old == expected means CAS succeeded.
>
> That's up to you.  The code is
>
> retry:  ldarx r1, [address]
>         cmp r1, expected
>         bne fail
>         stlrx rtmp, r1, [address]
>         cbnz tmp, retry // try again if there was a race
> fail:
>
> Andrew.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/f295382e/attachment.html>

From aph at redhat.com  Wed Jan 28 12:51:22 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Jan 2015 17:51:22 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>
	<CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>
Message-ID: <54C9219A.90408@redhat.com>

On 01/28/2015 05:41 PM, Vitaly Davidovich wrote:
> Well, is it really "up to you" if you were to have the signature
> that returns T instead of boolean? Caller would not have a way to
> distinguish success vs failure.

Indeed, that's true if you write it wrong.  "That's up to you" in this
case means "You're free to shoot yourself in the foot if you wish."
With the code I wrote, the only way that you're going to get
expected == old is if the CAS succeeded.  It would be unwise to write
it any other way.

> This also implies that CAS using LL/SC would have to
> spin and delay the operation for indeterminate amount of time.

But... even if you have a CMPXCHG instruction that's still possible.
There's no guarantee of fairness with CMPXCHG as far as I know.

Andrew.

From vitalyd at gmail.com  Wed Jan 28 12:51:50 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 12:51:50 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
Message-ID: <CAHjP37GEKczEpQ7voPRDnEggJLwRdYhNoCvaQPV8wzGw=+06kA@mail.gmail.com>

That depends on the definition of "safe to catch".  In the case of
top-level error reporting, such as would likely be the case here, you make
a *best-effort* attempt to report the error via some means, and then likely
exit.  To me, that scenario is "safe to catch", fully being aware that
error reporting may hit secondary error conditions, depending on the
severity of underlying cause.

For a library, you could do the same thing -- best-effort attempt to clean
up in known places; e.g. try/finally to release a lock in the finally
block, without checking what type of exception occurred.  It's
theoretically possible that the error will actually prevent success of the
finally block.

On Wed, Jan 28, 2015 at 11:54 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:

> So you're saying that all Throwables are safe (as in "the JVM is still
> fine") to catch?
>
> Perm-gen OOME?
> SOE?
> UnknownError?
>
> On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  Won't that mean it is not a throwable, but process termination?
>>
>> Alex
>>
>>
>>
>> On 28/01/2015 15:24, Viktor Klang wrote:
>>
>> Is there a canonical definition of which Throwables should never be
>> caught?
>>
>> --
>> Cheers,
>> ?
>> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:
>>
>>> I personally try very hard not to catch Error/Throwable, ever, and count
>>> on the program terminating and restarting.  I use transactional behavior
>>> where needed, to make state survive across restarts, and try and design
>>> operations of my software systems to be as stateless as possible so that
>>> restarts are in general, meaningless to the outcome of the software systems
>>> work.  If StackOverflowError is caught someplace. I would consider that a
>>> big problem for the design of the software system.
>>>
>>> Gregg Wonderly
>>>
>>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
>>> wrote:
>>> >
>>> > Consider this incredibly simple lock implementation:
>>> >
>>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>>> >  private final AtomicBoolean state = new AtomicBoolean();
>>> >  public final boolean tryLock() {
>>> >    if (state.compareAndSet(false, true)) {
>>> >      setExclusiveOwnerThread(Thread.currentThread());
>>> >      return true;
>>> >    } else {
>>> >      return false;
>>> >    }
>>> >  }
>>> >  public final void unlock() {
>>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>> >      throw new IllegalMonitorStateException();
>>> >    } else {
>>> >      setExclusiveOwnerThread(null);
>>> >      state.set(false);
>>> >    }
>>> >  }
>>> > }
>>> >
>>> > It's possible for the CAS in tryLock() to succeed and then for the
>>> > very next line to blow up with a synchronous StackOverflowError,
>>> > leaving the state variable true even though tryLock() is throwing.
>>> > This is the only kind of case that I'm looking for advice about,
>>> > nothing more general.
>>> >
>>> > So far the only responses have been "it's impossible to do anything
>>> > so don't even try." If that's really the end of it then I'll rest
>>> > easy knowing I've done my due diligence.
>>> >
>>> > But it seems so trivial to recognize that this tryLock() method
>>> > doesn't do any heap allocation, isn't recursive, and only calls
>>> > static or final methods, most of them being intrinsic. So I was just
>>> > wondering if it might be plausible to annotate such a method, as a
>>> > request to the compiler to confirm those facts and as a request to
>>> > the JVM to fail fast on entry by allocating enough stack space up
>>> > front.
>>> >
>>> > I wasn't imagining any kind of suppression of errors or more general
>>> > handling of arbitrarily complicated code, just avoidance of common
>>> > errors by failing fast when it's easy to do so.
>>> >
>>> > Cheers,
>>> > Justin
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
> --
> Cheers,
> ?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/25d5164d/attachment.html>

From vitalyd at gmail.com  Wed Jan 28 12:55:53 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 12:55:53 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C9219A.90408@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>
	<54C9219A.90408@redhat.com>
Message-ID: <CAHjP37FnFn=Z547zmAxD_BJk4ttiNWdWNwyRbJYpyd0CqTad_A@mail.gmail.com>

>
> But... even if you have a CMPXCHG instruction that's still possible.

There's no guarantee of fairness with CMPXCHG as far as I know.


No, that's not possible with cmpxchg.  The difference is that cmpxchg
allows you to return the current value and be guaranteed that it wasn't
equal to expected -- there's no case where the operation fails because of
some other thread stealing the cacheline and installing the same value and
causing the operation to fail (i.e. it's not LL/SC :)).

It's possible for someone to write their own CAS loop, which can spin for
indeterminate amount of time.  But that's a different scenario than a
"standalone" CAS operation; I may have a case where I CAS, and can proceed
with failure without retrying.

On Wed, Jan 28, 2015 at 12:51 PM, Andrew Haley <aph at redhat.com> wrote:

> On 01/28/2015 05:41 PM, Vitaly Davidovich wrote:
> > Well, is it really "up to you" if you were to have the signature
> > that returns T instead of boolean? Caller would not have a way to
> > distinguish success vs failure.
>
> Indeed, that's true if you write it wrong.  "That's up to you" in this
> case means "You're free to shoot yourself in the foot if you wish."
> With the code I wrote, the only way that you're going to get
> expected == old is if the CAS succeeded.  It would be unwise to write
> it any other way.
>
> > This also implies that CAS using LL/SC would have to
> > spin and delay the operation for indeterminate amount of time.
>
> But... even if you have a CMPXCHG instruction that's still possible.
> There's no guarantee of fairness with CMPXCHG as far as I know.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/7086c612/attachment-0001.html>

From aph at redhat.com  Wed Jan 28 13:06:36 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Jan 2015 18:06:36 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37FnFn=Z547zmAxD_BJk4ttiNWdWNwyRbJYpyd0CqTad_A@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>	<54C9219A.90408@redhat.com>
	<CAHjP37FnFn=Z547zmAxD_BJk4ttiNWdWNwyRbJYpyd0CqTad_A@mail.gmail.com>
Message-ID: <54C9252C.3030007@redhat.com>

On 01/28/2015 05:55 PM, Vitaly Davidovich wrote:
>>
>> But... even if you have a CMPXCHG instruction that's still possible.
> 
> There's no guarantee of fairness with CMPXCHG as far as I know.
> 
> No, that's not possible with cmpxchg.  The difference is that cmpxchg
> allows you to return the current value and be guaranteed that it wasn't
> equal to expected -- there's no case where the operation fails because of
> some other thread stealing the cacheline and installing the same value and
> causing the operation to fail (i.e. it's not LL/SC :)).

So what?  CMPXCHG won't fail, but it may have to wait while some bus
transactions complete.  It's perfectly possible for CMPXCHG
instructions to race, and the cache coherence logic has to work it all
out somehow.  How it does that is an implementation detail of the
processor and its bus interface.

Andrew.

From vitalyd at gmail.com  Wed Jan 28 13:21:44 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 13:21:44 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C91DF4.1010607@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
Message-ID: <CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>

There's no loop for a CAS with cmpxchg -- there's latency associated with
the microcode execution, sure, but that's no different than any other
instruction.  The key distinction is that cmpxchg allows a CAS to make
"forward" progress with fewer constraints, where forward progress simply
means "give me an answer to the CAS operation".  Using LL/SC, it seems like
you may have to loop just to get the right semantics for a failing CAS just
to get the cacheline to stabilize, which is a big difference from cmpxchg.


On Wed, Jan 28, 2015 at 12:35 PM, Andrew Haley <aph at redhat.com> wrote:

> On 01/28/2015 05:20 PM, Oleksandr Otenko wrote:
> > On 28/01/2015 17:11, Andrew Haley wrote:
> >> On 01/28/2015 04:48 PM, Oleksandr Otenko wrote:
> >>> Will ARM guarantee that if CAS fails, it never returns expected as the
> >>> old value?
> >> Yes.
> >
> > Thank you. But the code below implies that no, it doesn't, and one has
> > to actually write the code written in Java in another branch of this
> > discussion.
> >
> > In other words, you can't write a useful T compareAndSwap(T prev, T
> > next), where by comparing the return value to prev one can tell if the
> > new value was set to next, without a loop somewhere. (Except
> > architectures that already have instruction like x86's CMPXCHG that does
> > provide that strong guarantee)
>
> Why does this matter if there's a loop somewhere?  You have to retry
> if there is a race, whether that gets done automagically by the
> hardware or by the software.
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/80155ae3/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 13:34:34 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 18:34:34 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C9252C.3030007@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>	<54C9219A.90408@redhat.com>
	<CAHjP37FnFn=Z547zmAxD_BJk4ttiNWdWNwyRbJYpyd0CqTad_A@mail.gmail.com>
	<54C9252C.3030007@redhat.com>
Message-ID: <54C92BBA.4040302@oracle.com>

To my mind, CMPXCHG boils down to arbitration of who gets to lock the 
bus fairly. So CMPXCHG can be made to get the bus lock in N cycles for N 
CPUs. It is not so for LL/SC, because there is a gap between LL and SC 
and harder to control arbitration - how do they bias success of SC to 
one CPU over the other? Do they even do that?


Alex

On 28/01/2015 18:06, Andrew Haley wrote:
> On 01/28/2015 05:55 PM, Vitaly Davidovich wrote:
>>> But... even if you have a CMPXCHG instruction that's still possible.
>> There's no guarantee of fairness with CMPXCHG as far as I know.
>>
>> No, that's not possible with cmpxchg.  The difference is that cmpxchg
>> allows you to return the current value and be guaranteed that it wasn't
>> equal to expected -- there's no case where the operation fails because of
>> some other thread stealing the cacheline and installing the same value and
>> causing the operation to fail (i.e. it's not LL/SC :)).
> So what?  CMPXCHG won't fail, but it may have to wait while some bus
> transactions complete.  It's perfectly possible for CMPXCHG
> instructions to race, and the cache coherence logic has to work it all
> out somehow.  How it does that is an implementation detail of the
> processor and its bus interface.
>
> Andrew.


From vitalyd at gmail.com  Wed Jan 28 13:52:31 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 13:52:31 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C92BBA.4040302@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<CAHjP37Ehd9e=9wGh9j00JcGthCbz37+GcxKe7tWE6UQb4kfb1Q@mail.gmail.com>
	<54C9219A.90408@redhat.com>
	<CAHjP37FnFn=Z547zmAxD_BJk4ttiNWdWNwyRbJYpyd0CqTad_A@mail.gmail.com>
	<54C9252C.3030007@redhat.com> <54C92BBA.4040302@oracle.com>
Message-ID: <CAHjP37H7mqpW+DJdWLDapfKq9kbWE-Vks=GhKOX_BtO72+Bedw@mail.gmail.com>

As an aside, AFAIK, modern (and by this I mean > P6) Intel CPUs often avoid
locking the bus, and instead do cache locking (for cacheable memory and
locations that don't cross cachelines), making the impact local to the
cores that are contending for the cacheline rather than system-wide.

On Wed, Jan 28, 2015 at 1:34 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> To my mind, CMPXCHG boils down to arbitration of who gets to lock the bus
> fairly. So CMPXCHG can be made to get the bus lock in N cycles for N CPUs.
> It is not so for LL/SC, because there is a gap between LL and SC and harder
> to control arbitration - how do they bias success of SC to one CPU over the
> other? Do they even do that?
>
>
> Alex
>
>
> On 28/01/2015 18:06, Andrew Haley wrote:
>
>> On 01/28/2015 05:55 PM, Vitaly Davidovich wrote:
>>
>>> But... even if you have a CMPXCHG instruction that's still possible.
>>>>
>>> There's no guarantee of fairness with CMPXCHG as far as I know.
>>>
>>> No, that's not possible with cmpxchg.  The difference is that cmpxchg
>>> allows you to return the current value and be guaranteed that it wasn't
>>> equal to expected -- there's no case where the operation fails because of
>>> some other thread stealing the cacheline and installing the same value
>>> and
>>> causing the operation to fail (i.e. it's not LL/SC :)).
>>>
>> So what?  CMPXCHG won't fail, but it may have to wait while some bus
>> transactions complete.  It's perfectly possible for CMPXCHG
>> instructions to race, and the cache coherence logic has to work it all
>> out somehow.  How it does that is an implementation detail of the
>> processor and its bus interface.
>>
>> Andrew.
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/2e6971d7/attachment.html>

From viktor.klang at gmail.com  Wed Jan 28 14:10:31 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 28 Jan 2015 20:10:31 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CAHjP37GEKczEpQ7voPRDnEggJLwRdYhNoCvaQPV8wzGw=+06kA@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
	<CAHjP37GEKczEpQ7voPRDnEggJLwRdYhNoCvaQPV8wzGw=+06kA@mail.gmail.com>
Message-ID: <CANPzfU-gKtbfo9dM2xh2vUpeytfVG3-=VvrJ8nBWjWZpP1RPnA@mail.gmail.com>

On Wed, Jan 28, 2015 at 6:51 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> That depends on the definition of "safe to catch".
>

I mean it in a sense: "Not going to attempt to exit"


> In the case of top-level error reporting, such as would likely be the case
> here, you make a *best-effort* attempt to report the error via some means,
> and then likely exit.  To me, that scenario is "safe to catch", fully being
> aware that error reporting may hit secondary error conditions, depending on
> the severity of underlying cause.
>
> For a library, you could do the same thing -- best-effort attempt to clean
> up in known places; e.g. try/finally to release a lock in the finally
> block, without checking what type of exception occurred.  It's
> theoretically possible that the error will actually prevent success of the
> finally block.
>
> On Wed, Jan 28, 2015 at 11:54 AM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> So you're saying that all Throwables are safe (as in "the JVM is still
>> fine") to catch?
>>
>> Perm-gen OOME?
>> SOE?
>> UnknownError?
>>
>> On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  Won't that mean it is not a throwable, but process termination?
>>>
>>> Alex
>>>
>>>
>>>
>>> On 28/01/2015 15:24, Viktor Klang wrote:
>>>
>>> Is there a canonical definition of which Throwables should never be
>>> caught?
>>>
>>> --
>>> Cheers,
>>> ?
>>> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:
>>>
>>>> I personally try very hard not to catch Error/Throwable, ever, and
>>>> count on the program terminating and restarting.  I use transactional
>>>> behavior where needed, to make state survive across restarts, and try and
>>>> design operations of my software systems to be as stateless as possible so
>>>> that restarts are in general, meaningless to the outcome of the software
>>>> systems work.  If StackOverflowError is caught someplace. I would consider
>>>> that a big problem for the design of the software system.
>>>>
>>>> Gregg Wonderly
>>>>
>>>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
>>>> wrote:
>>>> >
>>>> > Consider this incredibly simple lock implementation:
>>>> >
>>>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>>>> >  private final AtomicBoolean state = new AtomicBoolean();
>>>> >  public final boolean tryLock() {
>>>> >    if (state.compareAndSet(false, true)) {
>>>> >      setExclusiveOwnerThread(Thread.currentThread());
>>>> >      return true;
>>>> >    } else {
>>>> >      return false;
>>>> >    }
>>>> >  }
>>>> >  public final void unlock() {
>>>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>>> >      throw new IllegalMonitorStateException();
>>>> >    } else {
>>>> >      setExclusiveOwnerThread(null);
>>>> >      state.set(false);
>>>> >    }
>>>> >  }
>>>> > }
>>>> >
>>>> > It's possible for the CAS in tryLock() to succeed and then for the
>>>> > very next line to blow up with a synchronous StackOverflowError,
>>>> > leaving the state variable true even though tryLock() is throwing.
>>>> > This is the only kind of case that I'm looking for advice about,
>>>> > nothing more general.
>>>> >
>>>> > So far the only responses have been "it's impossible to do anything
>>>> > so don't even try." If that's really the end of it then I'll rest
>>>> > easy knowing I've done my due diligence.
>>>> >
>>>> > But it seems so trivial to recognize that this tryLock() method
>>>> > doesn't do any heap allocation, isn't recursive, and only calls
>>>> > static or final methods, most of them being intrinsic. So I was just
>>>> > wondering if it might be plausible to annotate such a method, as a
>>>> > request to the compiler to confirm those facts and as a request to
>>>> > the JVM to fail fast on entry by allocating enough stack space up
>>>> > front.
>>>> >
>>>> > I wasn't imagining any kind of suppression of errors or more general
>>>> > handling of arbitrarily complicated code, just avoidance of common
>>>> > errors by failing fast when it's easy to do so.
>>>> >
>>>> > Cheers,
>>>> > Justin
>>>> >
>>>> > _______________________________________________
>>>> > Concurrency-interest mailing list
>>>> > Concurrency-interest at cs.oswego.edu
>>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>>
>> --
>> Cheers,
>> ?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/56057a65/attachment-0001.html>

From aph at redhat.com  Wed Jan 28 13:37:33 2015
From: aph at redhat.com (Andrew Haley)
Date: Wed, 28 Jan 2015 18:37:33 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
Message-ID: <54C92C6D.5030404@redhat.com>

On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
> There's no loop for a CAS with cmpxchg -- there's latency associated
> with the microcode execution, sure, but that's no different than any
> other instruction.

The latency I'm thinking of is associated with the bus transactions.

> The key distinction is that cmpxchg allows a CAS to make "forward"
> progress with fewer constraints, where forward progress simply means
> "give me an answer to the CAS operation".  Using LL/SC, it seems
> like you may have to loop just to get the right semantics for a
> failing CAS just to get the cacheline to stabilize, which is a big
> difference from cmpxchg.

Let's say there's a network of processors, all communicating their
memory states using a MESI-like protocol.  Somehow you have to
implement CAS.  Let's also say that the target address is not in the
local processor's cache.  There are essentially two ways of doing it:
do the CAS on the remote processor and communicate the result or try
to acquire the cache line which contains the target memory and do the
CAS locally.  Either way, if you're racing with other processors, you
may have to wait for some time.

Andrew.

From oleksandr.otenko at oracle.com  Wed Jan 28 14:34:01 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 19:34:01 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <DFCE9230-40A2-4DFD-8CD7-1494F148BDE5@rkuhn.info>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
	<DFCE9230-40A2-4DFD-8CD7-1494F148BDE5@rkuhn.info>
Message-ID: <54C939A9.6050102@oracle.com>

Hmmm. Whether there is a throwable that's not catchable, the program can 
never know = can't distinguish from termination.

Alex

On 28/01/2015 17:39, Roland Kuhn wrote:
> ?Throwable? and ?Catchable? are different words, it is a pity that not 
> both concepts have been implemented.
>
> Regards,
>
> Roland
>
>> 28 jan 2015 kl. 17:15 skrev Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>>:
>>
>> Won't that mean it is not a throwable, but process termination?
>>
>> Alex
>>
>>
>> On 28/01/2015 15:24, Viktor Klang wrote:
>>>
>>> Is there a canonical definition of which Throwables should never be 
>>> caught?
>>>
>>> -- 
>>> Cheers,
>>> ?
>>>
>>> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net 
>>> <mailto:gergg at cox.net>> wrote:
>>>
>>>     I personally try very hard not to catch Error/Throwable, ever,
>>>     and count on the program terminating and restarting.  I use
>>>     transactional behavior where needed, to make state survive
>>>     across restarts, and try and design operations of my software
>>>     systems to be as stateless as possible so that restarts are in
>>>     general, meaningless to the outcome of the software systems
>>>     work.  If StackOverflowError is caught someplace. I would
>>>     consider that a big problem for the design of the software system.
>>>
>>>     Gregg Wonderly
>>>
>>>     > On Jan 25, 2015, at 6:06 AM, Justin Sampson
>>>     <jsampson at guidewire.com <mailto:jsampson at guidewire.com>> wrote:
>>>     >
>>>     > Consider this incredibly simple lock implementation:
>>>     >
>>>     > public final class SimpleLock extends
>>>     AbstractOwnableSynchronizer {
>>>     >  private final AtomicBoolean state = new AtomicBoolean();
>>>     >  public final boolean tryLock() {
>>>     >    if (state.compareAndSet(false, true)) {
>>>     > setExclusiveOwnerThread(Thread.currentThread());
>>>     >      return true;
>>>     >    } else {
>>>     >      return false;
>>>     >    }
>>>     >  }
>>>     >  public final void unlock() {
>>>     >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>>     >      throw new IllegalMonitorStateException();
>>>     >    } else {
>>>     >      setExclusiveOwnerThread(null);
>>>     >      state.set(false);
>>>     >    }
>>>     >  }
>>>     > }
>>>     >
>>>     > It's possible for the CAS in tryLock() to succeed and then for the
>>>     > very next line to blow up with a synchronous StackOverflowError,
>>>     > leaving the state variable true even though tryLock() is throwing.
>>>     > This is the only kind of case that I'm looking for advice about,
>>>     > nothing more general.
>>>     >
>>>     > So far the only responses have been "it's impossible to do
>>>     anything
>>>     > so don't even try." If that's really the end of it then I'll rest
>>>     > easy knowing I've done my due diligence.
>>>     >
>>>     > But it seems so trivial to recognize that this tryLock() method
>>>     > doesn't do any heap allocation, isn't recursive, and only calls
>>>     > static or final methods, most of them being intrinsic. So I
>>>     was just
>>>     > wondering if it might be plausible to annotate such a method, as a
>>>     > request to the compiler to confirm those facts and as a request to
>>>     > the JVM to fail fast on entry by allocating enough stack space up
>>>     > front.
>>>     >
>>>     > I wasn't imagining any kind of suppression of errors or more
>>>     general
>>>     > handling of arbitrarily complicated code, just avoidance of common
>>>     > errors by failing fast when it's easy to do so.
>>>     >
>>>     > Cheers,
>>>     > Justin
>>>     >
>>>     > _______________________________________________
>>>     > Concurrency-interest mailing list
>>>     > Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Simplicity and elegance are unpopular because they require hard work 
> and discipline to achieve and education to be appreciated.
>   -- Dijkstra
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/fbaae2c1/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 14:36:19 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 19:36:19 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>	<54C90B34.6020108@oracle.com>
	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
Message-ID: <54C93A33.1070606@oracle.com>

By the way, "finally" catches all Throwables.

Alex

On 28/01/2015 16:54, Viktor Klang wrote:
> So you're saying that all Throwables are safe (as in "the JVM is still 
> fine") to catch?
>
> Perm-gen OOME?
> SOE?
> UnknownError?
>
> On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Won't that mean it is not a throwable, but process termination?
>
>     Alex
>
>
>
>     On 28/01/2015 15:24, Viktor Klang wrote:
>>
>>     Is there a canonical definition of which Throwables should never
>>     be caught?
>>
>>     -- 
>>     Cheers,
>>     ?
>>
>>     On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net
>>     <mailto:gergg at cox.net>> wrote:
>>
>>         I personally try very hard not to catch Error/Throwable,
>>         ever, and count on the program terminating and restarting.  I
>>         use transactional behavior where needed, to make state
>>         survive across restarts, and try and design operations of my
>>         software systems to be as stateless as possible so that
>>         restarts are in general, meaningless to the outcome of the
>>         software systems work.  If StackOverflowError is caught
>>         someplace. I would consider that a big problem for the design
>>         of the software system.
>>
>>         Gregg Wonderly
>>
>>         > On Jan 25, 2015, at 6:06 AM, Justin Sampson
>>         <jsampson at guidewire.com <mailto:jsampson at guidewire.com>> wrote:
>>         >
>>         > Consider this incredibly simple lock implementation:
>>         >
>>         > public final class SimpleLock extends
>>         AbstractOwnableSynchronizer {
>>         >  private final AtomicBoolean state = new AtomicBoolean();
>>         >  public final boolean tryLock() {
>>         >    if (state.compareAndSet(false, true)) {
>>         > setExclusiveOwnerThread(Thread.currentThread());
>>         >      return true;
>>         >    } else {
>>         >      return false;
>>         >    }
>>         >  }
>>         >  public final void unlock() {
>>         >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>         >      throw new IllegalMonitorStateException();
>>         >    } else {
>>         >      setExclusiveOwnerThread(null);
>>         >      state.set(false);
>>         >    }
>>         >  }
>>         > }
>>         >
>>         > It's possible for the CAS in tryLock() to succeed and then
>>         for the
>>         > very next line to blow up with a synchronous
>>         StackOverflowError,
>>         > leaving the state variable true even though tryLock() is
>>         throwing.
>>         > This is the only kind of case that I'm looking for advice
>>         about,
>>         > nothing more general.
>>         >
>>         > So far the only responses have been "it's impossible to do
>>         anything
>>         > so don't even try." If that's really the end of it then
>>         I'll rest
>>         > easy knowing I've done my due diligence.
>>         >
>>         > But it seems so trivial to recognize that this tryLock() method
>>         > doesn't do any heap allocation, isn't recursive, and only calls
>>         > static or final methods, most of them being intrinsic. So I
>>         was just
>>         > wondering if it might be plausible to annotate such a
>>         method, as a
>>         > request to the compiler to confirm those facts and as a
>>         request to
>>         > the JVM to fail fast on entry by allocating enough stack
>>         space up
>>         > front.
>>         >
>>         > I wasn't imagining any kind of suppression of errors or
>>         more general
>>         > handling of arbitrarily complicated code, just avoidance of
>>         common
>>         > errors by failing fast when it's easy to do so.
>>         >
>>         > Cheers,
>>         > Justin
>>         >
>>         > _______________________________________________
>>         > Concurrency-interest mailing list
>>         > Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> Cheers,
> ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/27df1e4f/attachment-0001.html>

From viktor.klang at gmail.com  Wed Jan 28 14:39:57 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 28 Jan 2015 20:39:57 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C93A33.1070606@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
	<54C93A33.1070606@oracle.com>
Message-ID: <CANPzfU-0k2-m=bYWe+LWhKaApbYjXcYK2d8rB2UT4FuicfTAgQ@mail.gmail.com>

Sure, but that was not my question :)

On Wed, Jan 28, 2015 at 8:36 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  By the way, "finally" catches all Throwables.
>
> Alex
>
>
> On 28/01/2015 16:54, Viktor Klang wrote:
>
> So you're saying that all Throwables are safe (as in "the JVM is still
> fine") to catch?
>
> Perm-gen OOME?
> SOE?
> UnknownError?
>
> On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  Won't that mean it is not a throwable, but process termination?
>>
>> Alex
>>
>>
>>
>> On 28/01/2015 15:24, Viktor Klang wrote:
>>
>> Is there a canonical definition of which Throwables should never be
>> caught?
>>
>> --
>> Cheers,
>> ?
>> On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net> wrote:
>>
>>> I personally try very hard not to catch Error/Throwable, ever, and count
>>> on the program terminating and restarting.  I use transactional behavior
>>> where needed, to make state survive across restarts, and try and design
>>> operations of my software systems to be as stateless as possible so that
>>> restarts are in general, meaningless to the outcome of the software systems
>>> work.  If StackOverflowError is caught someplace. I would consider that a
>>> big problem for the design of the software system.
>>>
>>> Gregg Wonderly
>>>
>>> > On Jan 25, 2015, at 6:06 AM, Justin Sampson <jsampson at guidewire.com>
>>> wrote:
>>> >
>>> > Consider this incredibly simple lock implementation:
>>> >
>>> > public final class SimpleLock extends AbstractOwnableSynchronizer {
>>> >  private final AtomicBoolean state = new AtomicBoolean();
>>> >  public final boolean tryLock() {
>>> >    if (state.compareAndSet(false, true)) {
>>> >      setExclusiveOwnerThread(Thread.currentThread());
>>> >      return true;
>>> >    } else {
>>> >      return false;
>>> >    }
>>> >  }
>>> >  public final void unlock() {
>>> >    if (Thread.currentThread() != getExclusiveOwnerThread()) {
>>> >      throw new IllegalMonitorStateException();
>>> >    } else {
>>> >      setExclusiveOwnerThread(null);
>>> >      state.set(false);
>>> >    }
>>> >  }
>>> > }
>>> >
>>> > It's possible for the CAS in tryLock() to succeed and then for the
>>> > very next line to blow up with a synchronous StackOverflowError,
>>> > leaving the state variable true even though tryLock() is throwing.
>>> > This is the only kind of case that I'm looking for advice about,
>>> > nothing more general.
>>> >
>>> > So far the only responses have been "it's impossible to do anything
>>> > so don't even try." If that's really the end of it then I'll rest
>>> > easy knowing I've done my due diligence.
>>> >
>>> > But it seems so trivial to recognize that this tryLock() method
>>> > doesn't do any heap allocation, isn't recursive, and only calls
>>> > static or final methods, most of them being intrinsic. So I was just
>>> > wondering if it might be plausible to annotate such a method, as a
>>> > request to the compiler to confirm those facts and as a request to
>>> > the JVM to fail fast on entry by allocating enough stack space up
>>> > front.
>>> >
>>> > I wasn't imagining any kind of suppression of errors or more general
>>> > handling of arbitrarily complicated code, just avoidance of common
>>> > errors by failing fast when it's easy to do so.
>>> >
>>> > Cheers,
>>> > Justin
>>> >
>>> > _______________________________________________
>>> > Concurrency-interest mailing list
>>> > Concurrency-interest at cs.oswego.edu
>>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
>  --
>   Cheers,
> ?
>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/74e22b7b/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 14:49:59 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Wed, 28 Jan 2015 19:49:59 +0000
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <CANPzfU-0k2-m=bYWe+LWhKaApbYjXcYK2d8rB2UT4FuicfTAgQ@mail.gmail.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>	<54C90B34.6020108@oracle.com>	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>	<54C93A33.1070606@oracle.com>
	<CANPzfU-0k2-m=bYWe+LWhKaApbYjXcYK2d8rB2UT4FuicfTAgQ@mail.gmail.com>
Message-ID: <54C93D67.3070107@oracle.com>

Well, if someone hopes to recover from all Throwables, then someone may 
find a reason to attempt a recovery from any throwables in catch.

Alex

On 28/01/2015 19:39, Viktor Klang wrote:
> Sure, but that was not my question :)
>
> On Wed, Jan 28, 2015 at 8:36 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     By the way, "finally" catches all Throwables.
>
>     Alex
>
>
>     On 28/01/2015 16:54, Viktor Klang wrote:
>>     So you're saying that all Throwables are safe (as in "the JVM is
>>     still fine") to catch?
>>
>>     Perm-gen OOME?
>>     SOE?
>>     UnknownError?
>>
>>     On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         Won't that mean it is not a throwable, but process termination?
>>
>>         Alex
>>
>>
>>
>>         On 28/01/2015 15:24, Viktor Klang wrote:
>>>
>>>         Is there a canonical definition of which Throwables should
>>>         never be caught?
>>>
>>>         -- 
>>>         Cheers,
>>>         ?
>>>
>>>         On 28 Jan 2015 16:22, "Gregg Wonderly" <gergg at cox.net
>>>         <mailto:gergg at cox.net>> wrote:
>>>
>>>             I personally try very hard not to catch Error/Throwable,
>>>             ever, and count on the program terminating and
>>>             restarting.  I use transactional behavior where needed,
>>>             to make state survive across restarts, and try and
>>>             design operations of my software systems to be as
>>>             stateless as possible so that restarts are in general,
>>>             meaningless to the outcome of the software systems
>>>             work.  If StackOverflowError is caught someplace. I
>>>             would consider that a big problem for the design of the
>>>             software system.
>>>
>>>             Gregg Wonderly
>>>
>>>             > On Jan 25, 2015, at 6:06 AM, Justin Sampson
>>>             <jsampson at guidewire.com <mailto:jsampson at guidewire.com>>
>>>             wrote:
>>>             >
>>>             > Consider this incredibly simple lock implementation:
>>>             >
>>>             > public final class SimpleLock extends
>>>             AbstractOwnableSynchronizer {
>>>             >  private final AtomicBoolean state = new AtomicBoolean();
>>>             >  public final boolean tryLock() {
>>>             >    if (state.compareAndSet(false, true)) {
>>>             > setExclusiveOwnerThread(Thread.currentThread());
>>>             >      return true;
>>>             >    } else {
>>>             >      return false;
>>>             >    }
>>>             >  }
>>>             >  public final void unlock() {
>>>             >    if (Thread.currentThread() !=
>>>             getExclusiveOwnerThread()) {
>>>             >      throw new IllegalMonitorStateException();
>>>             >    } else {
>>>             > setExclusiveOwnerThread(null);
>>>             >      state.set(false);
>>>             >    }
>>>             >  }
>>>             > }
>>>             >
>>>             > It's possible for the CAS in tryLock() to succeed and
>>>             then for the
>>>             > very next line to blow up with a synchronous
>>>             StackOverflowError,
>>>             > leaving the state variable true even though tryLock()
>>>             is throwing.
>>>             > This is the only kind of case that I'm looking for
>>>             advice about,
>>>             > nothing more general.
>>>             >
>>>             > So far the only responses have been "it's impossible
>>>             to do anything
>>>             > so don't even try." If that's really the end of it
>>>             then I'll rest
>>>             > easy knowing I've done my due diligence.
>>>             >
>>>             > But it seems so trivial to recognize that this
>>>             tryLock() method
>>>             > doesn't do any heap allocation, isn't recursive, and
>>>             only calls
>>>             > static or final methods, most of them being intrinsic.
>>>             So I was just
>>>             > wondering if it might be plausible to annotate such a
>>>             method, as a
>>>             > request to the compiler to confirm those facts and as
>>>             a request to
>>>             > the JVM to fail fast on entry by allocating enough
>>>             stack space up
>>>             > front.
>>>             >
>>>             > I wasn't imagining any kind of suppression of errors
>>>             or more general
>>>             > handling of arbitrarily complicated code, just
>>>             avoidance of common
>>>             > errors by failing fast when it's easy to do so.
>>>             >
>>>             > Cheers,
>>>             > Justin
>>>             >
>>>             > _______________________________________________
>>>             > Concurrency-interest mailing list
>>>             > Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     -- 
>>     Cheers,
>>     ?
>
>
>
>
> -- 
> Cheers,
> ?

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/fc49e245/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 28 15:08:14 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 15:08:14 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C92C6D.5030404@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
Message-ID: <CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>

>
> Let's say there's a network of processors, all communicating their
> memory states using a MESI-like protocol.  Somehow you have to
> implement CAS.  Let's also say that the target address is not in the
> local processor's cache.  There are essentially two ways of doing it:
> do the CAS on the remote processor and communicate the result or try
> to acquire the cache line which contains the target memory and do the
> CAS locally.  Either way, if you're racing with other processors, you
> may have to wait for some time.


Ok, Intel uses the latter approach, and that aspect is no different than
when you want to perform a simple write to a cacheline.  There're also
benefits to having an architectural instruction that clearly indicates that
you're intending to write to the cacheline; cmpxchg is effectively read +
write, so processor can request the cacheline for exclusive access (if it
doesn't yet have it) upfront rather than first fetch the line in
non-exclusive mode and later request owernship.  If you first read and then
later write as separate instructions, you potentially lose that knowledge
of intent.

I also find the need to spin (to let the cacheline to stabilize) in "user
mode" to report CAS *failures* a bit odd; you're basically spinning for
failures and success under contention.


On Wed, Jan 28, 2015 at 1:37 PM, Andrew Haley <aph at redhat.com> wrote:

> On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
> > There's no loop for a CAS with cmpxchg -- there's latency associated
> > with the microcode execution, sure, but that's no different than any
> > other instruction.
>
> The latency I'm thinking of is associated with the bus transactions.
>
> > The key distinction is that cmpxchg allows a CAS to make "forward"
> > progress with fewer constraints, where forward progress simply means
> > "give me an answer to the CAS operation".  Using LL/SC, it seems
> > like you may have to loop just to get the right semantics for a
> > failing CAS just to get the cacheline to stabilize, which is a big
> > difference from cmpxchg.
>
> Let's say there's a network of processors, all communicating their
> memory states using a MESI-like protocol.  Somehow you have to
> implement CAS.  Let's also say that the target address is not in the
> local processor's cache.  There are essentially two ways of doing it:
> do the CAS on the remote processor and communicate the result or try
> to acquire the cache line which contains the target memory and do the
> CAS locally.  Either way, if you're racing with other processors, you
> may have to wait for some time.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/10b08d82/attachment.html>

From TEREKHOV at de.ibm.com  Wed Jan 28 15:33:06 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 28 Jan 2015 21:33:06 +0100
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <54C93D67.3070107@oracle.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
	<54C93A33.1070606@oracle.com>
	<CANPzfU-0k2-m=bYWe+LWhKaApbYjXcYK2d8rB2UT4FuicfTAgQ@mail.gmail.com>
	<54C93D67.3070107@oracle.com>
Message-ID: <OFCDC28800.6130446D-ONC1257DDB.006DDA2C-C1257DDB.0070E61A@de.ibm.com>

In C++ it is implementation defined whether throwing unexpected (without
matching handler found) exception causes stack unwinding (execution of
destructors for objects on the stack).

IOW sane implementations with two phase
(first-search-for-matching-handler-then-unwind-if-handler-found) are at
least allowed and programs without silly use of catch(...) can terminate at
throw point (leaving a dump for analysis what went wrong) and allowing
recovery on higher level not hindering it by bad things like
deadlocks/hangs due to unwinding.

In Java with its finally-catching-exceptions-aka-catch(...) insanity
rules... IMHO.

regards,
alexander.

Oleksandr Otenko <oleksandr.otenko at oracle.com> on 28.01.2015 20:49:59

To:	Viktor Klang <viktor.klang at gmail.com>
cc:	Gregg Wonderly <gergg at cox.net>, concurrency-interest
       <concurrency-interest at cs.oswego.edu>, Alexander
       Terekhov/Germany/IBM at IBMDE
Subject:	Re: [concurrency-interest] JVM support to avoid
       StackOverflowError


Well, if someone hopes to recover from all Throwables, then someone may
find a reason to attempt a recovery from any throwables in catch.

Alex

On 28/01/2015 19:39, Viktor Klang wrote:
      Sure, but that was not my question :)

      On Wed, Jan 28, 2015 at 8:36 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
            By the way, "finally" catches all Throwables.

            Alex


            On 28/01/2015 16:54, Viktor Klang wrote:
                  So you're saying that all Throwables are safe (as in "the
                  JVM is still fine") to catch?

                  Perm-gen OOME?
                  SOE?
                  UnknownError?

                  On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
                  oleksandr.otenko at oracle.com> wrote:
                        Won't that mean it is not a throwable, but process
                        termination?

                        Alex



                        On 28/01/2015 15:24, Viktor Klang wrote:


                              Is there a canonical definition of which
                              Throwables should never be caught?


                              --
                              Cheers,
                              ?


                              On 28 Jan 2015 16:22, "Gregg Wonderly" <
                              gergg at cox.net> wrote:
                                    I personally try very hard not to catch
                                    Error/Throwable, ever, and count on the
                                    program terminating and restarting.? I
                                    use transactional behavior where
                                    needed, to make state survive across
                                    restarts, and try and design operations
                                    of my software systems to be as
                                    stateless as possible so that restarts
                                    are in general, meaningless to the
                                    outcome of the software systems work.
                                    If StackOverflowError is caught
                                    someplace. I would consider that a big
                                    problem for the design of the software
                                    system.

                                    Gregg Wonderly

                                    > On Jan 25, 2015, at 6:06 AM, Justin
                                    Sampson <jsampson at guidewire.com> wrote:
                                    >
                                    > Consider this incredibly simple lock
                                    implementation:
                                    >
                                    > public final class SimpleLock extends
                                    AbstractOwnableSynchronizer {
                                    >? private final AtomicBoolean state =
                                    new AtomicBoolean();
                                    >? public final boolean tryLock() {
                                    >? ? if (state.compareAndSet(false,
                                    true)) {
                                    >? ? ? setExclusiveOwnerThread
                                    (Thread.currentThread());
                                    >? ? ? return true;
                                    >? ? } else {
                                    >? ? ? return false;
                                    >? ? }
                                    >? }
                                    >? public final void unlock() {
                                    >? ? if (Thread.currentThread() !=
                                    getExclusiveOwnerThread()) {
                                    >? ? ? throw new
                                    IllegalMonitorStateException();
                                    >? ? } else {
                                    >? ? ? setExclusiveOwnerThread(null);
                                    >? ? ? state.set(false);
                                    >? ? }
                                    >? }
                                    > }
                                    >
                                    > It's possible for the CAS in tryLock
                                    () to succeed and then for the
                                    > very next line to blow up with a
                                    synchronous StackOverflowError,
                                    > leaving the state variable true even
                                    though tryLock() is throwing.
                                    > This is the only kind of case that
                                    I'm looking for advice about,
                                    > nothing more general.
                                    >
                                    > So far the only responses have been
                                    "it's impossible to do anything
                                    > so don't even try." If that's really
                                    the end of it then I'll rest
                                    > easy knowing I've done my due
                                    diligence.
                                    >
                                    > But it seems so trivial to recognize
                                    that this tryLock() method
                                    > doesn't do any heap allocation, isn't
                                    recursive, and only calls
                                    > static or final methods, most of them
                                    being intrinsic. So I was just
                                    > wondering if it might be plausible to
                                    annotate such a method, as a
                                    > request to the compiler to confirm
                                    those facts and as a request to
                                    > the JVM to fail fast on entry by
                                    allocating enough stack space up
                                    > front.
                                    >
                                    > I wasn't imagining any kind of
                                    suppression of errors or more general
                                    > handling of arbitrarily complicated
                                    code, just avoidance of common
                                    > errors by failing fast when it's easy
                                    to do so.
                                    >
                                    > Cheers,
                                    > Justin
                                    >
                                    >
                                    _______________________________________________

                                    > Concurrency-interest mailing list
                                    > Concurrency-interest at cs.oswego.edu
                                    >
                                    http://cs.oswego.edu/mailman/listinfo/concurrency-interest



                                    _______________________________________________

                                    Concurrency-interest mailing list
                                    Concurrency-interest at cs.oswego.edu
                                    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


                              _______________________________________________

                              Concurrency-interest mailing list
                              Concurrency-interest at cs.oswego.edu
                              http://cs.oswego.edu/mailman/listinfo/concurrency-interest






                  --
                  Cheers,
                  ?




      --
      Cheers,
      ?


From stephan.diestelhorst at gmail.com  Wed Jan 28 16:43:50 2015
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Wed, 28 Jan 2015 21:43:50 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
Message-ID: <CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>

A few topics.  Firstly, everybody assumes some form of non-blocking (often
wait-free) execution of single ISA instructions, without this ever being
properly guaranteed.

Secondly, the ARM ARM says that an implementation must ensure that forward
progress is made by at least one processor for repeatedly-contending
ldrex/strex sequences from multiple processors.  Which AFAICS at least
ensures lock-free execution of these CAS loops.

Stephan
On 28 Jan 2015 20:37, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> Let's say there's a network of processors, all communicating their
>> memory states using a MESI-like protocol.  Somehow you have to
>> implement CAS.  Let's also say that the target address is not in the
>> local processor's cache.  There are essentially two ways of doing it:
>> do the CAS on the remote processor and communicate the result or try
>> to acquire the cache line which contains the target memory and do the
>> CAS locally.  Either way, if you're racing with other processors, you
>> may have to wait for some time.
>
>
> Ok, Intel uses the latter approach, and that aspect is no different than
> when you want to perform a simple write to a cacheline.  There're also
> benefits to having an architectural instruction that clearly indicates that
> you're intending to write to the cacheline; cmpxchg is effectively read +
> write, so processor can request the cacheline for exclusive access (if it
> doesn't yet have it) upfront rather than first fetch the line in
> non-exclusive mode and later request owernship.  If you first read and then
> later write as separate instructions, you potentially lose that knowledge
> of intent.
>
> I also find the need to spin (to let the cacheline to stabilize) in "user
> mode" to report CAS *failures* a bit odd; you're basically spinning for
> failures and success under contention.
>
>
> On Wed, Jan 28, 2015 at 1:37 PM, Andrew Haley <aph at redhat.com> wrote:
>
>> On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
>> > There's no loop for a CAS with cmpxchg -- there's latency associated
>> > with the microcode execution, sure, but that's no different than any
>> > other instruction.
>>
>> The latency I'm thinking of is associated with the bus transactions.
>>
>> > The key distinction is that cmpxchg allows a CAS to make "forward"
>> > progress with fewer constraints, where forward progress simply means
>> > "give me an answer to the CAS operation".  Using LL/SC, it seems
>> > like you may have to loop just to get the right semantics for a
>> > failing CAS just to get the cacheline to stabilize, which is a big
>> > difference from cmpxchg.
>>
>> Let's say there's a network of processors, all communicating their
>> memory states using a MESI-like protocol.  Somehow you have to
>> implement CAS.  Let's also say that the target address is not in the
>> local processor's cache.  There are essentially two ways of doing it:
>> do the CAS on the remote processor and communicate the result or try
>> to acquire the cache line which contains the target memory and do the
>> CAS locally.  Either way, if you're racing with other processors, you
>> may have to wait for some time.
>>
>> Andrew.
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/3d757645/attachment-0001.html>

From stephan.diestelhorst at gmail.com  Wed Jan 28 16:45:01 2015
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Wed, 28 Jan 2015 21:45:01 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
Message-ID: <CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>

Also... The ldrex instruction has marked the intent of being followed by a
store.

Stephan
On 28 Jan 2015 21:43, "Stephan Diestelhorst" <stephan.diestelhorst at gmail.com>
wrote:

> A few topics.  Firstly, everybody assumes some form of non-blocking (often
> wait-free) execution of single ISA instructions, without this ever being
> properly guaranteed.
>
> Secondly, the ARM ARM says that an implementation must ensure that forward
> progress is made by at least one processor for repeatedly-contending
> ldrex/strex sequences from multiple processors.  Which AFAICS at least
> ensures lock-free execution of these CAS loops.
>
> Stephan
> On 28 Jan 2015 20:37, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>
>> Let's say there's a network of processors, all communicating their
>>> memory states using a MESI-like protocol.  Somehow you have to
>>> implement CAS.  Let's also say that the target address is not in the
>>> local processor's cache.  There are essentially two ways of doing it:
>>> do the CAS on the remote processor and communicate the result or try
>>> to acquire the cache line which contains the target memory and do the
>>> CAS locally.  Either way, if you're racing with other processors, you
>>> may have to wait for some time.
>>
>>
>> Ok, Intel uses the latter approach, and that aspect is no different than
>> when you want to perform a simple write to a cacheline.  There're also
>> benefits to having an architectural instruction that clearly indicates that
>> you're intending to write to the cacheline; cmpxchg is effectively read +
>> write, so processor can request the cacheline for exclusive access (if it
>> doesn't yet have it) upfront rather than first fetch the line in
>> non-exclusive mode and later request owernship.  If you first read and then
>> later write as separate instructions, you potentially lose that knowledge
>> of intent.
>>
>> I also find the need to spin (to let the cacheline to stabilize) in "user
>> mode" to report CAS *failures* a bit odd; you're basically spinning for
>> failures and success under contention.
>>
>>
>> On Wed, Jan 28, 2015 at 1:37 PM, Andrew Haley <aph at redhat.com> wrote:
>>
>>> On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
>>> > There's no loop for a CAS with cmpxchg -- there's latency associated
>>> > with the microcode execution, sure, but that's no different than any
>>> > other instruction.
>>>
>>> The latency I'm thinking of is associated with the bus transactions.
>>>
>>> > The key distinction is that cmpxchg allows a CAS to make "forward"
>>> > progress with fewer constraints, where forward progress simply means
>>> > "give me an answer to the CAS operation".  Using LL/SC, it seems
>>> > like you may have to loop just to get the right semantics for a
>>> > failing CAS just to get the cacheline to stabilize, which is a big
>>> > difference from cmpxchg.
>>>
>>> Let's say there's a network of processors, all communicating their
>>> memory states using a MESI-like protocol.  Somehow you have to
>>> implement CAS.  Let's also say that the target address is not in the
>>> local processor's cache.  There are essentially two ways of doing it:
>>> do the CAS on the remote processor and communicate the result or try
>>> to acquire the cache line which contains the target memory and do the
>>> CAS locally.  Either way, if you're racing with other processors, you
>>> may have to wait for some time.
>>>
>>> Andrew.
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/580bd0f9/attachment.html>

From vitalyd at gmail.com  Wed Jan 28 17:02:45 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 17:02:45 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
Message-ID: <CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>

>
> Also... The ldrex instruction has marked the intent of being followed by a
> store.


Ok, thanks for clarifying that.  Although thinking about that now, I don't
know what benefit that entails for LL/SC since the cacheline is unlocked.

A few topics.  Firstly, everybody assumes some form of non-blocking (often
> wait-free) execution of single ISA instructions, without this ever being
> properly guaranteed.


Do they? I think it's fairly known that a plain store does not scale when
many cores are writing to the cacheline; this typically comes up either
when someone is comparing plain stores to volatile stores, or when
discussing false sharing effects.  So I think most people that care about
things at this level know that any form of memory sharing is not going to
scale.


On Wed, Jan 28, 2015 at 4:45 PM, Stephan Diestelhorst <
stephan.diestelhorst at gmail.com> wrote:

> Also... The ldrex instruction has marked the intent of being followed by a
> store.
>
> Stephan
> On 28 Jan 2015 21:43, "Stephan Diestelhorst" <
> stephan.diestelhorst at gmail.com> wrote:
>
>> A few topics.  Firstly, everybody assumes some form of non-blocking
>> (often wait-free) execution of single ISA instructions, without this ever
>> being properly guaranteed.
>>
>> Secondly, the ARM ARM says that an implementation must ensure that
>> forward progress is made by at least one processor for
>> repeatedly-contending ldrex/strex sequences from multiple processors.
>> Which AFAICS at least ensures lock-free execution of these CAS loops.
>>
>> Stephan
>> On 28 Jan 2015 20:37, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>>
>>> Let's say there's a network of processors, all communicating their
>>>> memory states using a MESI-like protocol.  Somehow you have to
>>>> implement CAS.  Let's also say that the target address is not in the
>>>> local processor's cache.  There are essentially two ways of doing it:
>>>> do the CAS on the remote processor and communicate the result or try
>>>> to acquire the cache line which contains the target memory and do the
>>>> CAS locally.  Either way, if you're racing with other processors, you
>>>> may have to wait for some time.
>>>
>>>
>>> Ok, Intel uses the latter approach, and that aspect is no different than
>>> when you want to perform a simple write to a cacheline.  There're also
>>> benefits to having an architectural instruction that clearly indicates that
>>> you're intending to write to the cacheline; cmpxchg is effectively read +
>>> write, so processor can request the cacheline for exclusive access (if it
>>> doesn't yet have it) upfront rather than first fetch the line in
>>> non-exclusive mode and later request owernship.  If you first read and then
>>> later write as separate instructions, you potentially lose that knowledge
>>> of intent.
>>>
>>> I also find the need to spin (to let the cacheline to stabilize) in
>>> "user mode" to report CAS *failures* a bit odd; you're basically spinning
>>> for failures and success under contention.
>>>
>>>
>>> On Wed, Jan 28, 2015 at 1:37 PM, Andrew Haley <aph at redhat.com> wrote:
>>>
>>>> On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
>>>> > There's no loop for a CAS with cmpxchg -- there's latency associated
>>>> > with the microcode execution, sure, but that's no different than any
>>>> > other instruction.
>>>>
>>>> The latency I'm thinking of is associated with the bus transactions.
>>>>
>>>> > The key distinction is that cmpxchg allows a CAS to make "forward"
>>>> > progress with fewer constraints, where forward progress simply means
>>>> > "give me an answer to the CAS operation".  Using LL/SC, it seems
>>>> > like you may have to loop just to get the right semantics for a
>>>> > failing CAS just to get the cacheline to stabilize, which is a big
>>>> > difference from cmpxchg.
>>>>
>>>> Let's say there's a network of processors, all communicating their
>>>> memory states using a MESI-like protocol.  Somehow you have to
>>>> implement CAS.  Let's also say that the target address is not in the
>>>> local processor's cache.  There are essentially two ways of doing it:
>>>> do the CAS on the remote processor and communicate the result or try
>>>> to acquire the cache line which contains the target memory and do the
>>>> CAS locally.  Either way, if you're racing with other processors, you
>>>> may have to wait for some time.
>>>>
>>>> Andrew.
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/2e17d9cc/attachment.html>

From oleksandr.otenko at oracle.com  Wed Jan 28 20:17:31 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 01:17:31 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>
	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>
	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>
	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
Message-ID: <54C98A2B.4010807@oracle.com>

On 28/01/2015 22:02, Vitaly Davidovich wrote:
>
>     Also... The ldrex instruction has marked the intent of being
>     followed by a store.
>
>
> Ok, thanks for clarifying that.  Although thinking about that now, I 
> don't know what benefit that entails for LL/SC since the cacheline is 
> unlocked.
>
>     A few topics.  Firstly, everybody assumes some form of
>     non-blocking (often wait-free) execution of single ISA
>     instructions, without this ever being properly guaranteed.
>
>
> Do they? I think it's fairly known that a plain store does not scale 
> when many cores are writing to the cacheline; this typically comes up 
> either when someone is comparing plain stores to volatile stores, or 
> when discussing false sharing effects.  So I think most people that 
> care about things at this level know that any form of memory sharing 
> is not going to scale.

Well, I think it at least shows how the multicores might bias SC success 
after all.


Alex

>
>
> On Wed, Jan 28, 2015 at 4:45 PM, Stephan Diestelhorst 
> <stephan.diestelhorst at gmail.com 
> <mailto:stephan.diestelhorst at gmail.com>> wrote:
>
>     Also... The ldrex instruction has marked the intent of being
>     followed by a store.
>
>     Stephan
>
>     On 28 Jan 2015 21:43, "Stephan Diestelhorst"
>     <stephan.diestelhorst at gmail.com
>     <mailto:stephan.diestelhorst at gmail.com>> wrote:
>
>         A few topics.  Firstly, everybody assumes some form of
>         non-blocking (often wait-free) execution of single ISA
>         instructions, without this ever being properly guaranteed.
>
>         Secondly, the ARM ARM says that an implementation must ensure
>         that forward progress is made by at least one processor for
>         repeatedly-contending ldrex/strex sequences from multiple
>         processors.  Which AFAICS at least ensures lock-free execution
>         of these CAS loops.
>
>         Stephan
>
>         On 28 Jan 2015 20:37, "Vitaly Davidovich" <vitalyd at gmail.com
>         <mailto:vitalyd at gmail.com>> wrote:
>
>                 Let's say there's a network of processors, all
>                 communicating their
>                 memory states using a MESI-like protocol. Somehow you
>                 have to
>                 implement CAS.  Let's also say that the target address
>                 is not in the
>                 local processor's cache.  There are essentially two
>                 ways of doing it:
>                 do the CAS on the remote processor and communicate the
>                 result or try
>                 to acquire the cache line which contains the target
>                 memory and do the
>                 CAS locally.  Either way, if you're racing with other
>                 processors, you
>                 may have to wait for some time.
>
>
>             Ok, Intel uses the latter approach, and that aspect is no
>             different than when you want to perform a simple write to
>             a cacheline.  There're also benefits to having an
>             architectural instruction that clearly indicates that
>             you're intending to write to the cacheline; cmpxchg is
>             effectively read + write, so processor can request the
>             cacheline for exclusive access (if it doesn't yet have it)
>             upfront rather than first fetch the line in non-exclusive
>             mode and later request owernship.  If you first read and
>             then later write as separate instructions, you potentially
>             lose that knowledge of intent.
>
>             I also find the need to spin (to let the cacheline to
>             stabilize) in "user mode" to report CAS *failures* a bit
>             odd; you're basically spinning for failures and success
>             under contention.
>
>             On Wed, Jan 28, 2015 at 1:37 PM, Andrew Haley
>             <aph at redhat.com <mailto:aph at redhat.com>> wrote:
>
>                 On 01/28/2015 06:21 PM, Vitaly Davidovich wrote:
>                 > There's no loop for a CAS with cmpxchg -- there's
>                 latency associated
>                 > with the microcode execution, sure, but that's no
>                 different than any
>                 > other instruction.
>
>                 The latency I'm thinking of is associated with the bus
>                 transactions.
>
>                 > The key distinction is that cmpxchg allows a CAS to
>                 make "forward"
>                 > progress with fewer constraints, where forward
>                 progress simply means
>                 > "give me an answer to the CAS operation".  Using
>                 LL/SC, it seems
>                 > like you may have to loop just to get the right
>                 semantics for a
>                 > failing CAS just to get the cacheline to stabilize,
>                 which is a big
>                 > difference from cmpxchg.
>
>                 Let's say there's a network of processors, all
>                 communicating their
>                 memory states using a MESI-like protocol. Somehow you
>                 have to
>                 implement CAS.  Let's also say that the target address
>                 is not in the
>                 local processor's cache.  There are essentially two
>                 ways of doing it:
>                 do the CAS on the remote processor and communicate the
>                 result or try
>                 to acquire the cache line which contains the target
>                 memory and do the
>                 CAS locally.  Either way, if you're racing with other
>                 processors, you
>                 may have to wait for some time.
>
>                 Andrew.
>
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/1c4cfb44/attachment-0001.html>

From vitalyd at gmail.com  Wed Jan 28 23:05:40 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 28 Jan 2015 23:05:40 -0500
Subject: [concurrency-interest] JVM support to avoid StackOverflowError
In-Reply-To: <OFCDC28800.6130446D-ONC1257DDB.006DDA2C-C1257DDB.0070E61A@de.ibm.com>
References: <0FD072A166C6DC4C851F6115F37DDD2783D8E6BD@sm-ex-01-vm.guidewire.com>
	<CAHjP37FAc936BW+mzYzDn11=bq7moVVHPRaL4TOrtV51dRxF9w@mail.gmail.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D8E80A@sm-ex-01-vm.guidewire.com>
	<CAHjP37HAyNdmYWdLf5=qoaiA-T=iPHrF8PhbUO6q9Qn4xF1jMQ@mail.gmail.com>
	<0D7AED99-32E1-4997-BC22-1BBD660DCA95@cox.net>
	<CANPzfU98qT_OY_gsJT43CGammcbbvHQWcYhFbn-r6yZQOe0-Pw@mail.gmail.com>
	<54C90B34.6020108@oracle.com>
	<CANPzfU-H=R_ENKEUqXeH9qGEzOFycfPQKoVuDeUVZsP83z9Fhw@mail.gmail.com>
	<54C93A33.1070606@oracle.com>
	<CANPzfU-0k2-m=bYWe+LWhKaApbYjXcYK2d8rB2UT4FuicfTAgQ@mail.gmail.com>
	<54C93D67.3070107@oracle.com>
	<OFCDC28800.6130446D-ONC1257DDB.006DDA2C-C1257DDB.0070E61A@de.ibm.com>
Message-ID: <CAHjP37GXWGwX851qTddg+mwGWFY_-9A4tYNExRh8-E=38rKVyQ@mail.gmail.com>

This comes with its own problems: (a) implementation defined, so not
portable, (b) whether to hard abort should be a policy decided by user and
not baked in.

Robust error handling is one of the toughest aspects of software
engineering.

sent from my phone
On Jan 28, 2015 3:56 PM, "Alexander Terekhov" <TEREKHOV at de.ibm.com> wrote:

> In C++ it is implementation defined whether throwing unexpected (without
> matching handler found) exception causes stack unwinding (execution of
> destructors for objects on the stack).
>
> IOW sane implementations with two phase
> (first-search-for-matching-handler-then-unwind-if-handler-found) are at
> least allowed and programs without silly use of catch(...) can terminate at
> throw point (leaving a dump for analysis what went wrong) and allowing
> recovery on higher level not hindering it by bad things like
> deadlocks/hangs due to unwinding.
>
> In Java with its finally-catching-exceptions-aka-catch(...) insanity
> rules... IMHO.
>
> regards,
> alexander.
>
> Oleksandr Otenko <oleksandr.otenko at oracle.com> on 28.01.2015 20:49:59
>
> To:     Viktor Klang <viktor.klang at gmail.com>
> cc:     Gregg Wonderly <gergg at cox.net>, concurrency-interest
>        <concurrency-interest at cs.oswego.edu>, Alexander
>        Terekhov/Germany/IBM at IBMDE
> Subject:        Re: [concurrency-interest] JVM support to avoid
>        StackOverflowError
>
>
> Well, if someone hopes to recover from all Throwables, then someone may
> find a reason to attempt a recovery from any throwables in catch.
>
> Alex
>
> On 28/01/2015 19:39, Viktor Klang wrote:
>       Sure, but that was not my question :)
>
>       On Wed, Jan 28, 2015 at 8:36 PM, Oleksandr Otenko <
>       oleksandr.otenko at oracle.com> wrote:
>             By the way, "finally" catches all Throwables.
>
>             Alex
>
>
>             On 28/01/2015 16:54, Viktor Klang wrote:
>                   So you're saying that all Throwables are safe (as in "the
>                   JVM is still fine") to catch?
>
>                   Perm-gen OOME?
>                   SOE?
>                   UnknownError?
>
>                   On Wed, Jan 28, 2015 at 5:15 PM, Oleksandr Otenko <
>                   oleksandr.otenko at oracle.com> wrote:
>                         Won't that mean it is not a throwable, but process
>                         termination?
>
>                         Alex
>
>
>
>                         On 28/01/2015 15:24, Viktor Klang wrote:
>
>
>                               Is there a canonical definition of which
>                               Throwables should never be caught?
>
>
>                               --
>                               Cheers,
>                               ?
>
>
>                               On 28 Jan 2015 16:22, "Gregg Wonderly" <
>                               gergg at cox.net> wrote:
>                                     I personally try very hard not to catch
>                                     Error/Throwable, ever, and count on the
>                                     program terminating and restarting.  I
>                                     use transactional behavior where
>                                     needed, to make state survive across
>                                     restarts, and try and design operations
>                                     of my software systems to be as
>                                     stateless as possible so that restarts
>                                     are in general, meaningless to the
>                                     outcome of the software systems work.
>                                     If StackOverflowError is caught
>                                     someplace. I would consider that a big
>                                     problem for the design of the software
>                                     system.
>
>                                     Gregg Wonderly
>
>                                     > On Jan 25, 2015, at 6:06 AM, Justin
>                                     Sampson <jsampson at guidewire.com>
> wrote:
>                                     >
>                                     > Consider this incredibly simple lock
>                                     implementation:
>                                     >
>                                     > public final class SimpleLock extends
>                                     AbstractOwnableSynchronizer {
>                                     >  private final AtomicBoolean state =
>                                     new AtomicBoolean();
>                                     >  public final boolean tryLock() {
>                                     >    if (state.compareAndSet(false,
>                                     true)) {
>                                     >      setExclusiveOwnerThread
>                                     (Thread.currentThread());
>                                     >      return true;
>                                     >    } else {
>                                     >      return false;
>                                     >    }
>                                     >  }
>                                     >  public final void unlock() {
>                                     >    if (Thread.currentThread() !=
>                                     getExclusiveOwnerThread()) {
>                                     >      throw new
>                                     IllegalMonitorStateException();
>                                     >    } else {
>                                     >      setExclusiveOwnerThread(null);
>                                     >      state.set(false);
>                                     >    }
>                                     >  }
>                                     > }
>                                     >
>                                     > It's possible for the CAS in tryLock
>                                     () to succeed and then for the
>                                     > very next line to blow up with a
>                                     synchronous StackOverflowError,
>                                     > leaving the state variable true even
>                                     though tryLock() is throwing.
>                                     > This is the only kind of case that
>                                     I'm looking for advice about,
>                                     > nothing more general.
>                                     >
>                                     > So far the only responses have been
>                                     "it's impossible to do anything
>                                     > so don't even try." If that's really
>                                     the end of it then I'll rest
>                                     > easy knowing I've done my due
>                                     diligence.
>                                     >
>                                     > But it seems so trivial to recognize
>                                     that this tryLock() method
>                                     > doesn't do any heap allocation, isn't
>                                     recursive, and only calls
>                                     > static or final methods, most of them
>                                     being intrinsic. So I was just
>                                     > wondering if it might be plausible to
>                                     annotate such a method, as a
>                                     > request to the compiler to confirm
>                                     those facts and as a request to
>                                     > the JVM to fail fast on entry by
>                                     allocating enough stack space up
>                                     > front.
>                                     >
>                                     > I wasn't imagining any kind of
>                                     suppression of errors or more general
>                                     > handling of arbitrarily complicated
>                                     code, just avoidance of common
>                                     > errors by failing fast when it's easy
>                                     to do so.
>                                     >
>                                     > Cheers,
>                                     > Justin
>                                     >
>                                     >
>
> _______________________________________________
>
>                                     > Concurrency-interest mailing list
>                                     > Concurrency-interest at cs.oswego.edu
>                                     >
>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
>
>                                     Concurrency-interest mailing list
>                                     Concurrency-interest at cs.oswego.edu
>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
>
>                               Concurrency-interest mailing list
>                               Concurrency-interest at cs.oswego.edu
>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>
>                   --
>                   Cheers,
>                   ?
>
>
>
>
>       --
>       Cheers,
>       ?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150128/e9941af2/attachment-0001.html>

From peter.levart at gmail.com  Thu Jan 29 04:38:01 2015
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 29 Jan 2015 10:38:01 +0100
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <CA+kOe0-evOfrT7L-3ngY18UL8r5LMydU6CDHn3+SfjUr8TOv1A@mail.gmail.com>
References: <54C7B216.90404@oracle.com>	<CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>	<54C7FD28.8070708@oracle.com>
	<54C862ED.3040404@oracle.com>	<54C8D5B6.6070504@oracle.com>
	<CA+kOe0-evOfrT7L-3ngY18UL8r5LMydU6CDHn3+SfjUr8TOv1A@mail.gmail.com>
Message-ID: <54C9FF79.3020303@gmail.com>

On 01/28/2015 06:35 PM, Martin Buchholz wrote:
> It's hard for me to think of something we could add to the javadoc that
> would actually help future users enough to offset the confusion of adding
> subtleties of rarely encountered behavior.  I also don't see an easy way to
> improve the pool's reaction to exceptions coming from the queue.  Right now
> the reporting mechanism is the uncaught exception handler, which is under
> the user's control, although it is not obvious.

Well, there already exists these paragraphs in the class-level 
ThreadPoolExecutor javadoc:

  * <dt>Hook methods</dt>
  *
  * <dd>This class provides {@code protected} overridable
  * {@link #beforeExecute(Thread, Runnable)} and
  * {@link #afterExecute(Runnable, Throwable)} methods that are called
  * before and after execution of each task.  These can be used to
  * manipulate the execution environment; for example, reinitializing
  * ThreadLocals, gathering statistics, or adding log entries.
  * Additionally, method {@link #terminated} can be overridden to perform
  * any special processing that needs to be done once the Executor has
  * fully terminated.
  *
  * <p>If hook or callback methods throw exceptions, internal worker
  * threads may in turn fail and abruptly terminate.</dd>


The last paragraph could explicitly spell-out what are the "callback" 
methods. That would be enough, I think.


Regards, peter

>
> On Wed, Jan 28, 2015 at 4:27 AM, Lev Priima <lev.priima at oracle.com> wrote:
>
>> Thanks Doug, David, Martin, especially Martin.
>> Is it worth to update javadoc of  ThreadPoolExecutor#Queuing section with
>> this caveat ?
>>
>> The original confusion in custom queue implementation raise up from
>> javadoc, because BlockingQueue.take() interface specification does not
>> prohibit explicitly to throw uncaught runtime exception/errors (as any
>> other casual java code). But using this method in an exhaustive resource
>> allocation loop obliges to deal with exceptional situations in
>> work-producing methods more carefully.
>>
>> Best Regards,
>> Lev
>>
>>
>> On 28.01.2015 7:17, David Holmes wrote:
>>
>>> On 28/01/2015 7:03 AM, Lev Priima wrote:
>>>
>>>> Yes. And if we have BlockingQueue w/ some amount of tasks which fail
>>>> with exceptions, same amount of threads(not limited by neither
>>>> maximumPoolSize/corePoolSize) will hang under TPE which takes tasks from
>>>> this queue.
>>>>
>>>> It may cause problems if queue has a big percentage of exception-fail
>>>> tasks and we eventually get OOME while unable to create new native
>>>> thread.
>>>>
>>> If you use your pathological example then of course you can get into a
>>> situation where the thread creation outpaces the thread termination - it
>>> takes time and CPU cycles for a thread to actually complete.
>>>
>>> A BlockingQueue implementation should not have an expected failure mode
>>> that results in regularly throwing Errors or RuntimeExceptions. Such a BQ
>>> implementation would need to be fixed in my opinion.
>>>
>>> The TPE is working as designed - if errors/runtime-exceptions are
>>> encountered the worker thread will terminate and be replaced by a fresh
>>> worker. If you keep feeding the worker threads such exceptions then you
>>> incur a high rate of thread churn. So don't do that. :)
>>>
>>> Cheers,
>>> David
>>>
>>>   Lev
>>>> On 01/27/2015 11:31 PM, Martin Buchholz wrote:
>>>>
>>>>>
>>>>> On Tue, Jan 27, 2015 at 7:43 AM, Lev Priima <lev.priima at oracle.com
>>>>> <mailto:lev.priima at oracle.com>> wrote:
>>>>>
>>>>>      And these thread will be cleaned only when whole TPE finished.
>>>>>
>>>>>
>>>>> Is this really true?  Each thread should be replaced while running and
>>>>> so the total number of threads retained by the TPE at any one time
>>>>> should be no more than core pool size.
>>>>>
>>>>


From aph at redhat.com  Thu Jan 29 04:42:22 2015
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Jan 2015 09:42:22 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
Message-ID: <54CA007E.6010607@redhat.com>

On 28/01/15 22:02, Vitaly Davidovich wrote:
>>
>> Also... The ldrex instruction has marked the intent of being followed by a
>> store.
> 
> Ok, thanks for clarifying that.  Although thinking about that now, I
> don't know what benefit that entails for LL/SC since the cacheline
> is unlocked.

The purpose of LDARX is to load a line in the Exclusive state, so that
we know there are no other copies of it elsewhere.  Once we have a
line in E state, anybody else who wants it must ask us for it.  It's
neither locked nor unlocked, but it is the only copy of that in the
system, and it is clean.  (A simple load would have put the line into
Shared state.)  AIUI, the LDARX instruction will not complete until
the line is loaded and in E state.  Then we can do a STLRX in the
local cache, and as long as the line is still in E state that STLRX
will succeed.

Note that the delay here is not due to the STLRX loop, but due to the
time it takes the bus arbitration logic to acquire the line in E
state.  A CMPXCHG instruction must do the same thing.  If we wait for
too long (e.g. if our process gets descheduled) some other core may
acquire the line from us and the STLRX will subsequently fail.

(NB: I'm assuming simple MESI here, and the real protocol is probably
more complicated.  But this is close enough, I think.)

Andrew.

From thurston at nomagicsoftware.com  Thu Jan 29 08:03:34 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 29 Jan 2015 06:03:34 -0700 (MST)
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <54C8E805.1010402@cs.oswego.edu>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>
	<54C7F882.7000500@cs.oswego.edu>
	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>
	<CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>
	<CAB4+JYK1mdoNi5L-k+UaDyfMXDhN2SoALpZcn0a=xaQApGSm9w@mail.gmail.com>
	<54C8E805.1010402@cs.oswego.edu>
Message-ID: <1422536614543-12302.post@n7.nabble.com>

Doug Lea wrote
> On 01/27/2015 06:29 PM, Godmar Back wrote:
> 
>> My doubts are, specifically - "helping" is described as a more
>> "conservative"
>>  form of leapfrogging. I read "conservative" as that it might not decide
>> to
>> steal/help whereas fully fledged leap-frogging implementation would.
> 
> Yes. The original formulations of leap-frogging and some other
> work-stealing techniques allowed indefinite spinning
> in cases including when the stealer will eventually produce tasks
> that can be helped but hasn't. FJ gives up after a while and
> blocks/compensates.

But my understanding of your response to an earlier question I  posed
<http://jsr166-concurrency.10961.n7.nabble.com/FJ-stack-bounds-td11552.html>  
regarding the max stack height of a FJWT was:
msh + (msh - 1) + . . . + 1, where msh := max sequential stack height, which
is more aggressive (less conservative) than leapfrogging, where msh's are ==
(P2 in the leapfrogging paper)




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Q-Stealing-in-fully-strict-applications-tp12250p12302.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Thu Jan 29 08:20:44 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 13:20:44 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA007E.6010607@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com>
Message-ID: <54CA33AC.5000300@oracle.com>

Yes, and the objection is that CMPXCHG will delay any other core 
obtaining E state, until CMPXCHG finishes. Yet, there is nothing in ARM 
instruction set that will prevent others obtaining the line in E state - 
unless the core can predict STLRX is coming, and can delay response to 
the challenge about the line until STLRX completes. The presence of the 
loop means they don't always do this prediction, but Stephen's comment 
seems to imply that if the same sequence appears to fail frequently, 
then the core might bias its decision towards completing STLRX first and 
delaying the other cores in obtaining the line in E state.


Alex

On 29/01/2015 09:42, Andrew Haley wrote:
> On 28/01/15 22:02, Vitaly Davidovich wrote:
>>> Also... The ldrex instruction has marked the intent of being followed by a
>>> store.
>> Ok, thanks for clarifying that.  Although thinking about that now, I
>> don't know what benefit that entails for LL/SC since the cacheline
>> is unlocked.
> The purpose of LDARX is to load a line in the Exclusive state, so that
> we know there are no other copies of it elsewhere.  Once we have a
> line in E state, anybody else who wants it must ask us for it.  It's
> neither locked nor unlocked, but it is the only copy of that in the
> system, and it is clean.  (A simple load would have put the line into
> Shared state.)  AIUI, the LDARX instruction will not complete until
> the line is loaded and in E state.  Then we can do a STLRX in the
> local cache, and as long as the line is still in E state that STLRX
> will succeed.
>
> Note that the delay here is not due to the STLRX loop, but due to the
> time it takes the bus arbitration logic to acquire the line in E
> state.  A CMPXCHG instruction must do the same thing.  If we wait for
> too long (e.g. if our process gets descheduled) some other core may
> acquire the line from us and the STLRX will subsequently fail.
>
> (NB: I'm assuming simple MESI here, and the real protocol is probably
> more complicated.  But this is close enough, I think.)
>
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Thu Jan 29 08:42:31 2015
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Jan 2015 13:42:31 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA33AC.5000300@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
Message-ID: <54CA38C7.8090106@redhat.com>

On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:

> Yes, and the objection is that CMPXCHG will delay any other core
> obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
> ARM instruction set that will prevent others obtaining the line in E
> state - unless the core can predict STLRX is coming,

It's not at all unreasonable to predict that a STLRX follows an LDARX.
That's what I'd predict.

> and can delay response to the challenge about the line until STLRX
> completes.

There's nothing in the instruction set, but I don't think there needs
to be.  Any other core wanting to get that line in E state is going to
have to do a memory transaction, and by the time that transaction
complete the STLRX will probably have been finished for some time.

> The presence of the loop means they don't always do this prediction,

Not necessarily, no: the loop has to be there in case the core is
interrupted or blocked for some other reason.

> but Stephen's comment seems to imply that if the same sequence
> appears to fail frequently, then the core might bias its decision
> towards completing STLRX first and delaying the other cores in
> obtaining the line in E state.

Andrew.

From vitalyd at gmail.com  Thu Jan 29 10:57:51 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 10:57:51 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA38C7.8090106@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
Message-ID: <CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>

>
> Not necessarily, no: the loop has to be there in case the core is
> interrupted or blocked for some other reason.


You mean the *thread* requesting that cacheline is interrupted/blocked,
rather than the core, right? I think that's what you meant, given your
earlier answer, but just want to make sure I understand.

So how does the core know that some OS level thread of execution that
requested the line is no longer "available"? Does this happen as part of
context switch implementation on ARM, or is there something else?

As an aside, do any of you guys know why ARM decided to provide this form
of LL/SC and not offer a cmpxchg type of instruction? It seems like all the
cache coherence machinery is already there to get exclusive access to the
line, so why not "go all the way" and couple the load + store into 1
instruction since I'm guessing LDARX is almost always coupled with a STLRX
anyway.

On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:

> On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>
> > Yes, and the objection is that CMPXCHG will delay any other core
> > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
> > ARM instruction set that will prevent others obtaining the line in E
> > state - unless the core can predict STLRX is coming,
>
> It's not at all unreasonable to predict that a STLRX follows an LDARX.
> That's what I'd predict.
>
> > and can delay response to the challenge about the line until STLRX
> > completes.
>
> There's nothing in the instruction set, but I don't think there needs
> to be.  Any other core wanting to get that line in E state is going to
> have to do a memory transaction, and by the time that transaction
> complete the STLRX will probably have been finished for some time.
>
> > The presence of the loop means they don't always do this prediction,
>
> Not necessarily, no: the loop has to be there in case the core is
> interrupted or blocked for some other reason.
>
> > but Stephen's comment seems to imply that if the same sequence
> > appears to fail frequently, then the core might bias its decision
> > towards completing STLRX first and delaying the other cores in
> > obtaining the line in E state.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/527f5cee/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 11:00:18 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 16:00:18 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
Message-ID: <54CA5912.6020509@oracle.com>

Because with LL/SC you can do more than just swap.

LDARX
do your magic f(x)
STLRX - it's atomic!

Alex

On 29/01/2015 15:57, Vitaly Davidovich wrote:
>
>     Not necessarily, no: the loop has to be there in case the core is
>     interrupted or blocked for some other reason.
>
>
> You mean the *thread* requesting that cacheline is 
> interrupted/blocked, rather than the core, right? I think that's what 
> you meant, given your earlier answer, but just want to make sure I 
> understand.
>
> So how does the core know that some OS level thread of execution that 
> requested the line is no longer "available"? Does this happen as part 
> of context switch implementation on ARM, or is there something else?
>
> As an aside, do any of you guys know why ARM decided to provide this 
> form of LL/SC and not offer a cmpxchg type of instruction? It seems 
> like all the cache coherence machinery is already there to get 
> exclusive access to the line, so why not "go all the way" and couple 
> the load + store into 1 instruction since I'm guessing LDARX is almost 
> always coupled with a STLRX anyway.
>
> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com 
> <mailto:aph at redhat.com>> wrote:
>
>     On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>
>     > Yes, and the objection is that CMPXCHG will delay any other core
>     > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>     > ARM instruction set that will prevent others obtaining the line in E
>     > state - unless the core can predict STLRX is coming,
>
>     It's not at all unreasonable to predict that a STLRX follows an LDARX.
>     That's what I'd predict.
>
>     > and can delay response to the challenge about the line until STLRX
>     > completes.
>
>     There's nothing in the instruction set, but I don't think there needs
>     to be.  Any other core wanting to get that line in E state is going to
>     have to do a memory transaction, and by the time that transaction
>     complete the STLRX will probably have been finished for some time.
>
>     > The presence of the loop means they don't always do this prediction,
>
>     Not necessarily, no: the loop has to be there in case the core is
>     interrupted or blocked for some other reason.
>
>     > but Stephen's comment seems to imply that if the same sequence
>     > appears to fail frequently, then the core might bias its decision
>     > towards completing STLRX first and delaying the other cores in
>     > obtaining the line in E state.
>
>     Andrew.
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/6066b091/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 11:03:08 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 16:03:08 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA5912.6020509@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA5912.6020509@oracle.com>
Message-ID: <54CA59BC.7090802@oracle.com>

By the way, Intel's transactional extension will do something similar to 
LDARX/STLRX, probably:

transaction start - all loads are considered LDARX
transaction end - as long as nothing touched so far has changed its 
status from E, success.

Alex

On 29/01/2015 16:00, Oleksandr Otenko wrote:
> Because with LL/SC you can do more than just swap.
>
> LDARX
> do your magic f(x)
> STLRX - it's atomic!
>
> Alex
>
> On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>
>>     Not necessarily, no: the loop has to be there in case the core is
>>     interrupted or blocked for some other reason.
>>
>>
>> You mean the *thread* requesting that cacheline is 
>> interrupted/blocked, rather than the core, right? I think that's what 
>> you meant, given your earlier answer, but just want to make sure I 
>> understand.
>>
>> So how does the core know that some OS level thread of execution that 
>> requested the line is no longer "available"? Does this happen as part 
>> of context switch implementation on ARM, or is there something else?
>>
>> As an aside, do any of you guys know why ARM decided to provide this 
>> form of LL/SC and not offer a cmpxchg type of instruction? It seems 
>> like all the cache coherence machinery is already there to get 
>> exclusive access to the line, so why not "go all the way" and couple 
>> the load + store into 1 instruction since I'm guessing LDARX is 
>> almost always coupled with a STLRX anyway.
>>
>> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com 
>> <mailto:aph at redhat.com>> wrote:
>>
>>     On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>
>>     > Yes, and the objection is that CMPXCHG will delay any other core
>>     > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>>     > ARM instruction set that will prevent others obtaining the line
>>     in E
>>     > state - unless the core can predict STLRX is coming,
>>
>>     It's not at all unreasonable to predict that a STLRX follows an
>>     LDARX.
>>     That's what I'd predict.
>>
>>     > and can delay response to the challenge about the line until STLRX
>>     > completes.
>>
>>     There's nothing in the instruction set, but I don't think there needs
>>     to be.  Any other core wanting to get that line in E state is
>>     going to
>>     have to do a memory transaction, and by the time that transaction
>>     complete the STLRX will probably have been finished for some time.
>>
>>     > The presence of the loop means they don't always do this
>>     prediction,
>>
>>     Not necessarily, no: the loop has to be there in case the core is
>>     interrupted or blocked for some other reason.
>>
>>     > but Stephen's comment seems to imply that if the same sequence
>>     > appears to fail frequently, then the core might bias its decision
>>     > towards completing STLRX first and delaying the other cores in
>>     > obtaining the line in E state.
>>
>>     Andrew.
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/7dac373d/attachment.html>

From vitalyd at gmail.com  Thu Jan 29 11:21:49 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 11:21:49 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA5912.6020509@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA5912.6020509@oracle.com>
Message-ID: <CAHjP37GsJduc8D+7ojgMYPmhL7-gqY62V1HuNyXcbC+25rQ6eA@mail.gmail.com>

Sure, but can you give a concrete example of where this buys you something?
Is this supposed to allow detecting ABA issues?

sent from my phone
On Jan 29, 2015 11:00 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  Because with LL/SC you can do more than just swap.
>
> LDARX
> do your magic f(x)
> STLRX - it's atomic!
>
> Alex
>
> On 29/01/2015 15:57, Vitaly Davidovich wrote:
>
>  Not necessarily, no: the loop has to be there in case the core is
>> interrupted or blocked for some other reason.
>
>
>  You mean the *thread* requesting that cacheline is interrupted/blocked,
> rather than the core, right? I think that's what you meant, given your
> earlier answer, but just want to make sure I understand.
>
>  So how does the core know that some OS level thread of execution that
> requested the line is no longer "available"? Does this happen as part of
> context switch implementation on ARM, or is there something else?
>
>  As an aside, do any of you guys know why ARM decided to provide this
> form of LL/SC and not offer a cmpxchg type of instruction? It seems like
> all the cache coherence machinery is already there to get exclusive access
> to the line, so why not "go all the way" and couple the load + store into 1
> instruction since I'm guessing LDARX is almost always coupled with a STLRX
> anyway.
>
> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:
>
>> On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>
>> > Yes, and the objection is that CMPXCHG will delay any other core
>> > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>> > ARM instruction set that will prevent others obtaining the line in E
>> > state - unless the core can predict STLRX is coming,
>>
>> It's not at all unreasonable to predict that a STLRX follows an LDARX.
>> That's what I'd predict.
>>
>> > and can delay response to the challenge about the line until STLRX
>> > completes.
>>
>> There's nothing in the instruction set, but I don't think there needs
>> to be.  Any other core wanting to get that line in E state is going to
>> have to do a memory transaction, and by the time that transaction
>> complete the STLRX will probably have been finished for some time.
>>
>> > The presence of the loop means they don't always do this prediction,
>>
>> Not necessarily, no: the loop has to be there in case the core is
>> interrupted or blocked for some other reason.
>>
>> > but Stephen's comment seems to imply that if the same sequence
>> > appears to fail frequently, then the core might bias its decision
>> > towards completing STLRX first and delaying the other cores in
>> > obtaining the line in E state.
>>
>>  Andrew.
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/8ead3890/attachment-0001.html>

From aph at redhat.com  Thu Jan 29 11:42:19 2015
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Jan 2015 16:42:19 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
Message-ID: <54CA62EB.4050305@redhat.com>

On 01/29/2015 03:57 PM, Vitaly Davidovich wrote:
>>
>> Not necessarily, no: the loop has to be there in case the core is
>> interrupted or blocked for some other reason.
> 
> You mean the *thread* requesting that cacheline is interrupted/blocked,
> rather than the core, right? I think that's what you meant, given your
> earlier answer, but just want to make sure I understand.

The line is owned by a core, not a thread, and it's a core that gets
interrupted.

> So how does the core know that some OS level thread of execution that
> requested the line is no longer "available"? Does this happen as part of
> context switch implementation on ARM, or is there something else?

It doesn't need to do anything.  If the core doesn't do the STLRX fast
enough it'll lose: there's nothing AFAIK blocking another core from
acquiring that cache line.  It's up to the local core to do whatever
it needs to do quickly.

> As an aside, do any of you guys know why ARM decided to provide this form
> of LL/SC and not offer a cmpxchg type of instruction? It seems like all the
> cache coherence machinery is already there to get exclusive access to the
> line, so why not "go all the way" and couple the load + store into 1
> instruction since I'm guessing LDARX is almost always coupled with a STLRX
> anyway.

Because LDARX and STLRX are much more flexible.  You can do CAS, but
you can also do a locked load/add/store, load/xor/store, and so on,
without having to provide special instructions for all the
possibilities.

Andrew.

From vitalyd at gmail.com  Thu Jan 29 11:53:50 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 11:53:50 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA62EB.4050305@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA62EB.4050305@redhat.com>
Message-ID: <CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>

>
> The line is owned by a core, not a thread, and it's a core that gets
> interrupted.


Yes, I understand who owns the line, but your previous email mentioned
process.  So, to confirm, you're talking about a processor interrupt
occurring, such as when OS sends an IPI as part of context switching?

It doesn't need to do anything.  If the core doesn't do the STLRX fast
> enough it'll lose: there's nothing AFAIK blocking another core from
> acquiring that cache line.  It's up to the local core to do whatever
> it needs to do quickly.


Ok.


> Because LDARX and STLRX are much more flexible.  You can do CAS, but
> you can also do a locked load/add/store, load/xor/store, and so on,
> without having to provide special instructions for all the
> possibilities.


You can do all this with cmpxchg as well - read the value, add/xor/whatever
it, and then cmpxchg it back in if it hasn't been modified since the read.


On Thu, Jan 29, 2015 at 11:42 AM, Andrew Haley <aph at redhat.com> wrote:

> On 01/29/2015 03:57 PM, Vitaly Davidovich wrote:
> >>
> >> Not necessarily, no: the loop has to be there in case the core is
> >> interrupted or blocked for some other reason.
> >
> > You mean the *thread* requesting that cacheline is interrupted/blocked,
> > rather than the core, right? I think that's what you meant, given your
> > earlier answer, but just want to make sure I understand.
>
> The line is owned by a core, not a thread, and it's a core that gets
> interrupted.
>
> > So how does the core know that some OS level thread of execution that
> > requested the line is no longer "available"? Does this happen as part of
> > context switch implementation on ARM, or is there something else?
>
> It doesn't need to do anything.  If the core doesn't do the STLRX fast
> enough it'll lose: there's nothing AFAIK blocking another core from
> acquiring that cache line.  It's up to the local core to do whatever
> it needs to do quickly.
>
> > As an aside, do any of you guys know why ARM decided to provide this form
> > of LL/SC and not offer a cmpxchg type of instruction? It seems like all
> the
> > cache coherence machinery is already there to get exclusive access to the
> > line, so why not "go all the way" and couple the load + store into 1
> > instruction since I'm guessing LDARX is almost always coupled with a
> STLRX
> > anyway.
>
> Because LDARX and STLRX are much more flexible.  You can do CAS, but
> you can also do a locked load/add/store, load/xor/store, and so on,
> without having to provide special instructions for all the
> possibilities.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/08ab80e4/attachment.html>

From vitalyd at gmail.com  Thu Jan 29 11:57:52 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 11:57:52 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA59BC.7090802@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA5912.6020509@oracle.com> <54CA59BC.7090802@oracle.com>
Message-ID: <CAHjP37FcHiBgo0FbC+Vu0LFfxLwXM_dHmLJwfwTs69rygmF5YQ@mail.gmail.com>

Yes, but Intel's TSX has an actual transaction around the whole set of
accesses.  With LL/SC, you really have that only for 1 cacheline at a time;
you cannot LL/SC two cachelines, and abort both atomically if either of
them fails.

On Thu, Jan 29, 2015 at 11:03 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  By the way, Intel's transactional extension will do something similar to
> LDARX/STLRX, probably:
>
> transaction start - all loads are considered LDARX
> transaction end - as long as nothing touched so far has changed its status
> from E, success.
>
> Alex
>
>
> On 29/01/2015 16:00, Oleksandr Otenko wrote:
>
> Because with LL/SC you can do more than just swap.
>
> LDARX
> do your magic f(x)
> STLRX - it's atomic!
>
> Alex
>
> On 29/01/2015 15:57, Vitaly Davidovich wrote:
>
>  Not necessarily, no: the loop has to be there in case the core is
>> interrupted or blocked for some other reason.
>
>
>  You mean the *thread* requesting that cacheline is interrupted/blocked,
> rather than the core, right? I think that's what you meant, given your
> earlier answer, but just want to make sure I understand.
>
>  So how does the core know that some OS level thread of execution that
> requested the line is no longer "available"? Does this happen as part of
> context switch implementation on ARM, or is there something else?
>
>  As an aside, do any of you guys know why ARM decided to provide this
> form of LL/SC and not offer a cmpxchg type of instruction? It seems like
> all the cache coherence machinery is already there to get exclusive access
> to the line, so why not "go all the way" and couple the load + store into 1
> instruction since I'm guessing LDARX is almost always coupled with a STLRX
> anyway.
>
> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:
>
>> On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>
>> > Yes, and the objection is that CMPXCHG will delay any other core
>> > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>> > ARM instruction set that will prevent others obtaining the line in E
>> > state - unless the core can predict STLRX is coming,
>>
>> It's not at all unreasonable to predict that a STLRX follows an LDARX.
>> That's what I'd predict.
>>
>> > and can delay response to the challenge about the line until STLRX
>> > completes.
>>
>> There's nothing in the instruction set, but I don't think there needs
>> to be.  Any other core wanting to get that line in E state is going to
>> have to do a memory transaction, and by the time that transaction
>> complete the STLRX will probably have been finished for some time.
>>
>> > The presence of the loop means they don't always do this prediction,
>>
>> Not necessarily, no: the loop has to be there in case the core is
>> interrupted or blocked for some other reason.
>>
>> > but Stephen's comment seems to imply that if the same sequence
>> > appears to fail frequently, then the core might bias its decision
>> > towards completing STLRX first and delaying the other cores in
>> > obtaining the line in E state.
>>
>>  Andrew.
>>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/e8a497eb/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 11:57:52 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 16:57:52 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37GsJduc8D+7ojgMYPmhL7-gqY62V1HuNyXcbC+25rQ6eA@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>	<54CA5912.6020509@oracle.com>
	<CAHjP37GsJduc8D+7ojgMYPmhL7-gqY62V1HuNyXcbC+25rQ6eA@mail.gmail.com>
Message-ID: <54CA6690.6000105@oracle.com>

Not necessarily.

node.next=head;
head=node;

Alex

On 29/01/2015 16:21, Vitaly Davidovich wrote:
>
> Sure, but can you give a concrete example of where this buys you 
> something? Is this supposed to allow detecting ABA issues?
>
> sent from my phone
>
> On Jan 29, 2015 11:00 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Because with LL/SC you can do more than just swap.
>
>     LDARX
>     do your magic f(x)
>     STLRX - it's atomic!
>
>     Alex
>
>     On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>
>>         Not necessarily, no: the loop has to be there in case the core is
>>         interrupted or blocked for some other reason.
>>
>>
>>     You mean the *thread* requesting that cacheline is
>>     interrupted/blocked, rather than the core, right? I think that's
>>     what you meant, given your earlier answer, but just want to make
>>     sure I understand.
>>
>>     So how does the core know that some OS level thread of execution
>>     that requested the line is no longer "available"? Does this
>>     happen as part of context switch implementation on ARM, or is
>>     there something else?
>>
>>     As an aside, do any of you guys know why ARM decided to provide
>>     this form of LL/SC and not offer a cmpxchg type of instruction?
>>     It seems like all the cache coherence machinery is already there
>>     to get exclusive access to the line, so why not "go all the way"
>>     and couple the load + store into 1 instruction since I'm guessing
>>     LDARX is almost always coupled with a STLRX anyway.
>>
>>     On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com
>>     <mailto:aph at redhat.com>> wrote:
>>
>>         On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>
>>         > Yes, and the objection is that CMPXCHG will delay any other
>>         core
>>         > obtaining E state, until CMPXCHG finishes. Yet, there is
>>         nothing in
>>         > ARM instruction set that will prevent others obtaining the
>>         line in E
>>         > state - unless the core can predict STLRX is coming,
>>
>>         It's not at all unreasonable to predict that a STLRX follows
>>         an LDARX.
>>         That's what I'd predict.
>>
>>         > and can delay response to the challenge about the line
>>         until STLRX
>>         > completes.
>>
>>         There's nothing in the instruction set, but I don't think
>>         there needs
>>         to be.  Any other core wanting to get that line in E state is
>>         going to
>>         have to do a memory transaction, and by the time that transaction
>>         complete the STLRX will probably have been finished for some
>>         time.
>>
>>         > The presence of the loop means they don't always do this
>>         prediction,
>>
>>         Not necessarily, no: the loop has to be there in case the core is
>>         interrupted or blocked for some other reason.
>>
>>         > but Stephen's comment seems to imply that if the same sequence
>>         > appears to fail frequently, then the core might bias its
>>         decision
>>         > towards completing STLRX first and delaying the other cores in
>>         > obtaining the line in E state.
>>
>>         Andrew.
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/2dcff701/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 11:59:57 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 16:59:57 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37FcHiBgo0FbC+Vu0LFfxLwXM_dHmLJwfwTs69rygmF5YQ@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>	<54CA5912.6020509@oracle.com>	<54CA59BC.7090802@oracle.com>
	<CAHjP37FcHiBgo0FbC+Vu0LFfxLwXM_dHmLJwfwTs69rygmF5YQ@mail.gmail.com>
Message-ID: <54CA670D.4090704@oracle.com>

Correct, but you get the same semantics, if you use it to update just 
one variable.

Alex

On 29/01/2015 16:57, Vitaly Davidovich wrote:
> Yes, but Intel's TSX has an actual transaction around the whole set of 
> accesses.  With LL/SC, you really have that only for 1 cacheline at a 
> time; you cannot LL/SC two cachelines, and abort both atomically if 
> either of them fails.
>
> On Thu, Jan 29, 2015 at 11:03 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     By the way, Intel's transactional extension will do something
>     similar to LDARX/STLRX, probably:
>
>     transaction start - all loads are considered LDARX
>     transaction end - as long as nothing touched so far has changed
>     its status from E, success.
>
>     Alex
>
>
>     On 29/01/2015 16:00, Oleksandr Otenko wrote:
>>     Because with LL/SC you can do more than just swap.
>>
>>     LDARX
>>     do your magic f(x)
>>     STLRX - it's atomic!
>>
>>     Alex
>>
>>     On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>>
>>>         Not necessarily, no: the loop has to be there in case the
>>>         core is
>>>         interrupted or blocked for some other reason.
>>>
>>>
>>>     You mean the *thread* requesting that cacheline is
>>>     interrupted/blocked, rather than the core, right? I think that's
>>>     what you meant, given your earlier answer, but just want to make
>>>     sure I understand.
>>>
>>>     So how does the core know that some OS level thread of execution
>>>     that requested the line is no longer "available"? Does this
>>>     happen as part of context switch implementation on ARM, or is
>>>     there something else?
>>>
>>>     As an aside, do any of you guys know why ARM decided to provide
>>>     this form of LL/SC and not offer a cmpxchg type of instruction?
>>>     It seems like all the cache coherence machinery is already there
>>>     to get exclusive access to the line, so why not "go all the way"
>>>     and couple the load + store into 1 instruction since I'm
>>>     guessing LDARX is almost always coupled with a STLRX anyway.
>>>
>>>     On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com
>>>     <mailto:aph at redhat.com>> wrote:
>>>
>>>         On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>>
>>>         > Yes, and the objection is that CMPXCHG will delay any
>>>         other core
>>>         > obtaining E state, until CMPXCHG finishes. Yet, there is
>>>         nothing in
>>>         > ARM instruction set that will prevent others obtaining the
>>>         line in E
>>>         > state - unless the core can predict STLRX is coming,
>>>
>>>         It's not at all unreasonable to predict that a STLRX follows
>>>         an LDARX.
>>>         That's what I'd predict.
>>>
>>>         > and can delay response to the challenge about the line
>>>         until STLRX
>>>         > completes.
>>>
>>>         There's nothing in the instruction set, but I don't think
>>>         there needs
>>>         to be.  Any other core wanting to get that line in E state
>>>         is going to
>>>         have to do a memory transaction, and by the time that
>>>         transaction
>>>         complete the STLRX will probably have been finished for some
>>>         time.
>>>
>>>         > The presence of the loop means they don't always do this
>>>         prediction,
>>>
>>>         Not necessarily, no: the loop has to be there in case the
>>>         core is
>>>         interrupted or blocked for some other reason.
>>>
>>>         > but Stephen's comment seems to imply that if the same sequence
>>>         > appears to fail frequently, then the core might bias its
>>>         decision
>>>         > towards completing STLRX first and delaying the other cores in
>>>         > obtaining the line in E state.
>>>
>>>         Andrew.
>>>
>>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/e67e1d9f/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 12:01:17 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 17:01:17 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>	<54CA62EB.4050305@redhat.com>
	<CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>
Message-ID: <54CA675D.5020909@oracle.com>

Yes, but you do end up with CMPXCHG doing the second read. With STRLX 
you only check the cache wasn't hit by anyone else since the last read.

Alex

On 29/01/2015 16:53, Vitaly Davidovich wrote:
>
>     The line is owned by a core, not a thread, and it's a core that gets
>     interrupted.
>
>
> Yes, I understand who owns the line, but your previous email mentioned 
> process.  So, to confirm, you're talking about a processor interrupt 
> occurring, such as when OS sends an IPI as part of context switching?
>
>     It doesn't need to do anything.  If the core doesn't do the STLRX fast
>     enough it'll lose: there's nothing AFAIK blocking another core from
>     acquiring that cache line.  It's up to the local core to do whatever
>     it needs to do quickly.
>
>
> Ok.
>
>     Because LDARX and STLRX are much more flexible.  You can do CAS, but
>     you can also do a locked load/add/store, load/xor/store, and so on,
>     without having to provide special instructions for all the
>     possibilities.
>
>
> You can do all this with cmpxchg as well - read the value, 
> add/xor/whatever it, and then cmpxchg it back in if it hasn't been 
> modified since the read.
>
> On Thu, Jan 29, 2015 at 11:42 AM, Andrew Haley <aph at redhat.com 
> <mailto:aph at redhat.com>> wrote:
>
>     On 01/29/2015 03:57 PM, Vitaly Davidovich wrote:
>     >>
>     >> Not necessarily, no: the loop has to be there in case the core is
>     >> interrupted or blocked for some other reason.
>     >
>     > You mean the *thread* requesting that cacheline is
>     interrupted/blocked,
>     > rather than the core, right? I think that's what you meant,
>     given your
>     > earlier answer, but just want to make sure I understand.
>
>     The line is owned by a core, not a thread, and it's a core that gets
>     interrupted.
>
>     > So how does the core know that some OS level thread of execution
>     that
>     > requested the line is no longer "available"? Does this happen as
>     part of
>     > context switch implementation on ARM, or is there something else?
>
>     It doesn't need to do anything.  If the core doesn't do the STLRX fast
>     enough it'll lose: there's nothing AFAIK blocking another core from
>     acquiring that cache line.  It's up to the local core to do whatever
>     it needs to do quickly.
>
>     > As an aside, do any of you guys know why ARM decided to provide
>     this form
>     > of LL/SC and not offer a cmpxchg type of instruction? It seems
>     like all the
>     > cache coherence machinery is already there to get exclusive
>     access to the
>     > line, so why not "go all the way" and couple the load + store into 1
>     > instruction since I'm guessing LDARX is almost always coupled
>     with a STLRX
>     > anyway.
>
>     Because LDARX and STLRX are much more flexible.  You can do CAS, but
>     you can also do a locked load/add/store, load/xor/store, and so on,
>     without having to provide special instructions for all the
>     possibilities.
>
>     Andrew.
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/8e1babdc/attachment.html>

From vitalyd at gmail.com  Thu Jan 29 12:02:51 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 12:02:51 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA6690.6000105@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA5912.6020509@oracle.com>
	<CAHjP37GsJduc8D+7ojgMYPmhL7-gqY62V1HuNyXcbC+25rQ6eA@mail.gmail.com>
	<54CA6690.6000105@oracle.com>
Message-ID: <CAHjP37E97WZri_gRzm_GDCBWbG7qy1BPWmr8ZDVoEEafwbz1uw@mail.gmail.com>

Sorry, what is this supposed to show? That could be two separate cacheline
writes here.

On Thu, Jan 29, 2015 at 11:57 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Not necessarily.
>
> node.next=head;
> head=node;
>
> Alex
>
>
> On 29/01/2015 16:21, Vitaly Davidovich wrote:
>
> Sure, but can you give a concrete example of where this buys you
> something? Is this supposed to allow detecting ABA issues?
>
> sent from my phone
> On Jan 29, 2015 11:00 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  Because with LL/SC you can do more than just swap.
>>
>> LDARX
>> do your magic f(x)
>> STLRX - it's atomic!
>>
>> Alex
>>
>> On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>
>>  Not necessarily, no: the loop has to be there in case the core is
>>> interrupted or blocked for some other reason.
>>
>>
>>  You mean the *thread* requesting that cacheline is interrupted/blocked,
>> rather than the core, right? I think that's what you meant, given your
>> earlier answer, but just want to make sure I understand.
>>
>>  So how does the core know that some OS level thread of execution that
>> requested the line is no longer "available"? Does this happen as part of
>> context switch implementation on ARM, or is there something else?
>>
>>  As an aside, do any of you guys know why ARM decided to provide this
>> form of LL/SC and not offer a cmpxchg type of instruction? It seems like
>> all the cache coherence machinery is already there to get exclusive access
>> to the line, so why not "go all the way" and couple the load + store into 1
>> instruction since I'm guessing LDARX is almost always coupled with a STLRX
>> anyway.
>>
>> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:
>>
>>> On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>>
>>> > Yes, and the objection is that CMPXCHG will delay any other core
>>> > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>>> > ARM instruction set that will prevent others obtaining the line in E
>>> > state - unless the core can predict STLRX is coming,
>>>
>>> It's not at all unreasonable to predict that a STLRX follows an LDARX.
>>> That's what I'd predict.
>>>
>>> > and can delay response to the challenge about the line until STLRX
>>> > completes.
>>>
>>> There's nothing in the instruction set, but I don't think there needs
>>> to be.  Any other core wanting to get that line in E state is going to
>>> have to do a memory transaction, and by the time that transaction
>>> complete the STLRX will probably have been finished for some time.
>>>
>>> > The presence of the loop means they don't always do this prediction,
>>>
>>> Not necessarily, no: the loop has to be there in case the core is
>>> interrupted or blocked for some other reason.
>>>
>>> > but Stephen's comment seems to imply that if the same sequence
>>> > appears to fail frequently, then the core might bias its decision
>>> > towards completing STLRX first and delaying the other cores in
>>> > obtaining the line in E state.
>>>
>>>  Andrew.
>>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/9e4235d2/attachment-0001.html>

From aph at redhat.com  Thu Jan 29 12:04:33 2015
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Jan 2015 17:04:33 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>	<54CA62EB.4050305@redhat.com>
	<CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>
Message-ID: <54CA6821.9050304@redhat.com>

On 01/29/2015 04:53 PM, Vitaly Davidovich wrote:
>>
>> The line is owned by a core, not a thread, and it's a core that gets
>> interrupted.
> 
> 
> Yes, I understand who owns the line, but your previous email mentioned
> process.  So, to confirm, you're talking about a processor interrupt
> occurring, such as when OS sends an IPI as part of context switching?

Yes.


>> > Because LDARX and STLRX are much more flexible.  You can do CAS, but
>> > you can also do a locked load/add/store, load/xor/store, and so on,
>> > without having to provide special instructions for all the
>> > possibilities.
>
> You can do all this with cmpxchg as well - read the value, add/xor/whatever
> it, and then cmpxchg it back in if it hasn't been modified since the read.

Sure, but you have to do more memory transactions, and there's a
greater probability that you'll fail.  The first read loads the line
in Shared state, then to do the cmpxchg you'd have to upgrade the line
to Exclusive state then do the CAS.  Changing the line to Exclusive
state takes time, and by the time it succeeds the original value may
have changed.  It's much cleaner to get the line into Exclusive state
straight away.

Andrew.

From vitalyd at gmail.com  Thu Jan 29 12:21:03 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 12:21:03 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA6821.9050304@redhat.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
	<54CA62EB.4050305@redhat.com>
	<CAHjP37GzrYyDV5tb40v9_2q3yaa9eJTF05TBKF91kfXTYu0TRA@mail.gmail.com>
	<54CA6821.9050304@redhat.com>
Message-ID: <CAHjP37GNXuYikZ9jubFWp8i9Fx7MMQSh+9qRLccSZsdzc23t6w@mail.gmail.com>

>
> Sure, but you have to do more memory transactions, and there's a
> greater probability that you'll fail.  The first read loads the line
> in Shared state, then to do the cmpxchg you'd have to upgrade the line
> to Exclusive state then do the CAS.  Changing the line to Exclusive
> state takes time, and by the time it succeeds the original value may
> have changed.  It's much cleaner to get the line into Exclusive state
> straight away.


Ok, fair enough on the extra mem transaction if doing a plain load.  I
think newer Intel cpus support prefetchw instruction (load cacheline with
intent to write), which a compiler could emit if it sees that there's an
unconditional write attempt soon thereafter.


On Thu, Jan 29, 2015 at 12:04 PM, Andrew Haley <aph at redhat.com> wrote:

> On 01/29/2015 04:53 PM, Vitaly Davidovich wrote:
> >>
> >> The line is owned by a core, not a thread, and it's a core that gets
> >> interrupted.
> >
> >
> > Yes, I understand who owns the line, but your previous email mentioned
> > process.  So, to confirm, you're talking about a processor interrupt
> > occurring, such as when OS sends an IPI as part of context switching?
>
> Yes.
>
>
> >> > Because LDARX and STLRX are much more flexible.  You can do CAS, but
> >> > you can also do a locked load/add/store, load/xor/store, and so on,
> >> > without having to provide special instructions for all the
> >> > possibilities.
> >
> > You can do all this with cmpxchg as well - read the value,
> add/xor/whatever
> > it, and then cmpxchg it back in if it hasn't been modified since the
> read.
>
> Sure, but you have to do more memory transactions, and there's a
> greater probability that you'll fail.  The first read loads the line
> in Shared state, then to do the cmpxchg you'd have to upgrade the line
> to Exclusive state then do the CAS.  Changing the line to Exclusive
> state takes time, and by the time it succeeds the original value may
> have changed.  It's much cleaner to get the line into Exclusive state
> straight away.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/d52e6fcb/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 12:45:39 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 17:45:39 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37E97WZri_gRzm_GDCBWbG7qy1BPWmr8ZDVoEEafwbz1uw@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>	<54B07067.30103@cs.oswego.edu>	<54B0FB69.7080009@redhat.com>	<54C912C1.3030208@oracle.com>	<54C9185F.4080700@redhat.com>	<54C91A73.2000207@oracle.com>	<54C91DF4.1010607@redhat.com>	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>	<54C92C6D.5030404@redhat.com>	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>	<54CA007E.6010607@redhat.com>	<54CA33AC.5000300@oracle.com>	<54CA38C7.8090106@redhat.com>	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>	<54CA5912.6020509@oracle.com>	<CAHjP37GsJduc8D+7ojgMYPmhL7-gqY62V1HuNyXcbC+25rQ6eA@mail.gmail.com>	<54CA6690.6000105@oracle.com>
	<CAHjP37E97WZri_gRzm_GDCBWbG7qy1BPWmr8ZDVoEEafwbz1uw@mail.gmail.com>
Message-ID: <54CA71C3.8070008@oracle.com>

That's an example of what is possible with LL/SC, and not possible with 
CMPXCHG - you can't insert an arbitrary function between the load and 
store inside CMPXCHG.

Alex

On 29/01/2015 17:02, Vitaly Davidovich wrote:
> Sorry, what is this supposed to show? That could be two separate 
> cacheline writes here.
>
> On Thu, Jan 29, 2015 at 11:57 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Not necessarily.
>
>     node.next=head;
>     head=node;
>
>     Alex
>
>
>     On 29/01/2015 16:21, Vitaly Davidovich wrote:
>>
>>     Sure, but can you give a concrete example of where this buys you
>>     something? Is this supposed to allow detecting ABA issues?
>>
>>     sent from my phone
>>
>>     On Jan 29, 2015 11:00 AM, "Oleksandr Otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         Because with LL/SC you can do more than just swap.
>>
>>         LDARX
>>         do your magic f(x)
>>         STLRX - it's atomic!
>>
>>         Alex
>>
>>         On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>>
>>>             Not necessarily, no: the loop has to be there in case
>>>             the core is
>>>             interrupted or blocked for some other reason.
>>>
>>>
>>>         You mean the *thread* requesting that cacheline is
>>>         interrupted/blocked, rather than the core, right? I think
>>>         that's what you meant, given your earlier answer, but just
>>>         want to make sure I understand.
>>>
>>>         So how does the core know that some OS level thread of
>>>         execution that requested the line is no longer "available"?
>>>         Does this happen as part of context switch implementation on
>>>         ARM, or is there something else?
>>>
>>>         As an aside, do any of you guys know why ARM decided to
>>>         provide this form of LL/SC and not offer a cmpxchg type of
>>>         instruction? It seems like all the cache coherence machinery
>>>         is already there to get exclusive access to the line, so why
>>>         not "go all the way" and couple the load + store into 1
>>>         instruction since I'm guessing LDARX is almost always
>>>         coupled with a STLRX anyway.
>>>
>>>         On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley
>>>         <aph at redhat.com <mailto:aph at redhat.com>> wrote:
>>>
>>>             On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>>
>>>             > Yes, and the objection is that CMPXCHG will delay any
>>>             other core
>>>             > obtaining E state, until CMPXCHG finishes. Yet, there
>>>             is nothing in
>>>             > ARM instruction set that will prevent others obtaining
>>>             the line in E
>>>             > state - unless the core can predict STLRX is coming,
>>>
>>>             It's not at all unreasonable to predict that a STLRX
>>>             follows an LDARX.
>>>             That's what I'd predict.
>>>
>>>             > and can delay response to the challenge about the line
>>>             until STLRX
>>>             > completes.
>>>
>>>             There's nothing in the instruction set, but I don't
>>>             think there needs
>>>             to be.  Any other core wanting to get that line in E
>>>             state is going to
>>>             have to do a memory transaction, and by the time that
>>>             transaction
>>>             complete the STLRX will probably have been finished for
>>>             some time.
>>>
>>>             > The presence of the loop means they don't always do
>>>             this prediction,
>>>
>>>             Not necessarily, no: the loop has to be there in case
>>>             the core is
>>>             interrupted or blocked for some other reason.
>>>
>>>             > but Stephen's comment seems to imply that if the same
>>>             sequence
>>>             > appears to fail frequently, then the core might bias
>>>             its decision
>>>             > towards completing STLRX first and delaying the other
>>>             cores in
>>>             > obtaining the line in E state.
>>>
>>>             Andrew.
>>>
>>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/bca05e38/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Jan 29 15:56:18 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 30 Jan 2015 06:56:18 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54CA71C3.8070008@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>

These hardware discussions are getting somewhat OT ... :)

You can't insert an arbitrary function between the ll and sc - Any** other store within the ll/sc block can invalidate the sc.

ll/sc can be immune to ABA problem but not once you code the CAS up in the loop form - that makes it also susceptible to ABA.

** Which stores can impact the sc may, or may not be specified by the architecture.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Oleksandr Otenko
  Sent: Friday, 30 January 2015 3:46 AM
  To: Vitaly Davidovich
  Cc: Doug Lea; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] AtomicReference CAS signatures


  That's an example of what is possible with LL/SC, and not possible with CMPXCHG - you can't insert an arbitrary function between the load and store inside CMPXCHG.

  Alex


  On 29/01/2015 17:02, Vitaly Davidovich wrote:

    Sorry, what is this supposed to show? That could be two separate cacheline writes here.


    On Thu, Jan 29, 2015 at 11:57 AM, Oleksandr Otenko <oleksandr.otenko at oracle.com> wrote:

      Not necessarily.

      node.next=head;
      head=node;

      Alex 



      On 29/01/2015 16:21, Vitaly Davidovich wrote:

        Sure, but can you give a concrete example of where this buys you something? Is this supposed to allow detecting ABA issues?

        sent from my phone

        On Jan 29, 2015 11:00 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com> wrote:

          Because with LL/SC you can do more than just swap.

          LDARX
          do your magic f(x)
          STLRX - it's atomic!

          Alex


          On 29/01/2015 15:57, Vitaly Davidovich wrote:

              Not necessarily, no: the loop has to be there in case the core is
              interrupted or blocked for some other reason.


            You mean the *thread* requesting that cacheline is interrupted/blocked, rather than the core, right? I think that's what you meant, given your earlier answer, but just want to make sure I understand.


            So how does the core know that some OS level thread of execution that requested the line is no longer "available"? Does this happen as part of context switch implementation on ARM, or is there something else?


            As an aside, do any of you guys know why ARM decided to provide this form of LL/SC and not offer a cmpxchg type of instruction? It seems like all the cache coherence machinery is already there to get exclusive access to the line, so why not "go all the way" and couple the load + store into 1 instruction since I'm guessing LDARX is almost always coupled with a STLRX anyway.


            On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:

              On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:

              > Yes, and the objection is that CMPXCHG will delay any other core
              > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
              > ARM instruction set that will prevent others obtaining the line in E
              > state - unless the core can predict STLRX is coming,

              It's not at all unreasonable to predict that a STLRX follows an LDARX.
              That's what I'd predict.

              > and can delay response to the challenge about the line until STLRX
              > completes.

              There's nothing in the instruction set, but I don't think there needs
              to be.  Any other core wanting to get that line in E state is going to
              have to do a memory transaction, and by the time that transaction
              complete the STLRX will probably have been finished for some time.

              > The presence of the loop means they don't always do this prediction,

              Not necessarily, no: the loop has to be there in case the core is
              interrupted or blocked for some other reason.


              > but Stephen's comment seems to imply that if the same sequence
              > appears to fail frequently, then the core might bias its decision
              > towards completing STLRX first and delaying the other cores in
              > obtaining the line in E state.


              Andrew.










-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150130/9f8ff787/attachment.html>

From oleksandr.otenko at oracle.com  Thu Jan 29 16:31:02 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 29 Jan 2015 21:31:02 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
Message-ID: <54CAA696.3040803@oracle.com>

ok, next time I'll put a disclaimer that I don't know exactly how the 
hardware works :-)

Alex

On 29/01/2015 20:56, David Holmes wrote:
> These hardware discussions are getting somewhat OT ... :)
> You can't insert an arbitrary function between the ll and sc - Any** 
> other store within the ll/sc block can invalidate the sc.
> ll/sc can be immune to ABA problem but not once you code the CAS up in 
> the loop form - that makes it also susceptible to ABA.
> ** Which stores can impact the sc may, or may not be specified by the 
> architecture.
> David
>
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Oleksandr Otenko
>     *Sent:* Friday, 30 January 2015 3:46 AM
>     *To:* Vitaly Davidovich
>     *Cc:* Doug Lea; concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] AtomicReference CAS signatures
>
>     That's an example of what is possible with LL/SC, and not possible
>     with CMPXCHG - you can't insert an arbitrary function between the
>     load and store inside CMPXCHG.
>
>     Alex
>
>     On 29/01/2015 17:02, Vitaly Davidovich wrote:
>>     Sorry, what is this supposed to show? That could be two separate
>>     cacheline writes here.
>>
>>     On Thu, Jan 29, 2015 at 11:57 AM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         Not necessarily.
>>
>>         node.next=head;
>>         head=node;
>>
>>         Alex
>>
>>
>>         On 29/01/2015 16:21, Vitaly Davidovich wrote:
>>>
>>>         Sure, but can you give a concrete example of where this buys
>>>         you something? Is this supposed to allow detecting ABA issues?
>>>
>>>         sent from my phone
>>>
>>>         On Jan 29, 2015 11:00 AM, "Oleksandr Otenko"
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             Because with LL/SC you can do more than just swap.
>>>
>>>             LDARX
>>>             do your magic f(x)
>>>             STLRX - it's atomic!
>>>
>>>             Alex
>>>
>>>             On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>>>
>>>>                 Not necessarily, no: the loop has to be there in
>>>>                 case the core is
>>>>                 interrupted or blocked for some other reason.
>>>>
>>>>
>>>>             You mean the *thread* requesting that cacheline is
>>>>             interrupted/blocked, rather than the core, right? I
>>>>             think that's what you meant, given your earlier answer,
>>>>             but just want to make sure I understand.
>>>>
>>>>             So how does the core know that some OS level thread of
>>>>             execution that requested the line is no longer
>>>>             "available"? Does this happen as part of context switch
>>>>             implementation on ARM, or is there something else?
>>>>
>>>>             As an aside, do any of you guys know why ARM decided to
>>>>             provide this form of LL/SC and not offer a cmpxchg type
>>>>             of instruction? It seems like all the cache coherence
>>>>             machinery is already there to get exclusive access to
>>>>             the line, so why not "go all the way" and couple the
>>>>             load + store into 1 instruction since I'm guessing
>>>>             LDARX is almost always coupled with a STLRX anyway.
>>>>
>>>>             On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley
>>>>             <aph at redhat.com <mailto:aph at redhat.com>> wrote:
>>>>
>>>>                 On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>>>
>>>>                 > Yes, and the objection is that CMPXCHG will delay
>>>>                 any other core
>>>>                 > obtaining E state, until CMPXCHG finishes. Yet,
>>>>                 there is nothing in
>>>>                 > ARM instruction set that will prevent others
>>>>                 obtaining the line in E
>>>>                 > state - unless the core can predict STLRX is coming,
>>>>
>>>>                 It's not at all unreasonable to predict that a
>>>>                 STLRX follows an LDARX.
>>>>                 That's what I'd predict.
>>>>
>>>>                 > and can delay response to the challenge about the
>>>>                 line until STLRX
>>>>                 > completes.
>>>>
>>>>                 There's nothing in the instruction set, but I don't
>>>>                 think there needs
>>>>                 to be.  Any other core wanting to get that line in
>>>>                 E state is going to
>>>>                 have to do a memory transaction, and by the time
>>>>                 that transaction
>>>>                 complete the STLRX will probably have been finished
>>>>                 for some time.
>>>>
>>>>                 > The presence of the loop means they don't always
>>>>                 do this prediction,
>>>>
>>>>                 Not necessarily, no: the loop has to be there in
>>>>                 case the core is
>>>>                 interrupted or blocked for some other reason.
>>>>
>>>>                 > but Stephen's comment seems to imply that if the
>>>>                 same sequence
>>>>                 > appears to fail frequently, then the core might
>>>>                 bias its decision
>>>>                 > towards completing STLRX first and delaying the
>>>>                 other cores in
>>>>                 > obtaining the line in E state.
>>>>
>>>>                 Andrew.
>>>>
>>>>
>>>
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/c460ae5d/attachment-0001.html>

From vitalyd at gmail.com  Thu Jan 29 16:37:14 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 16:37:14 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
References: <54CA71C3.8070008@oracle.com>
	<NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37H+a7sw2WYY4kMc4A-JL6Qi1+Vtwd9i4bv8F7s58nzufQ@mail.gmail.com>

Oh c'mon David, the core discussion has simmered down long ago, we're in
"bonus" territory here :)

I doubt another store to same cacheline from same core invidates SC, but I
was going to say it's unlikely you want to introduce extra memory ops in
between purely because you expand the race window further (e.g. imagine you
take a LLC miss and it's data dependent - you're looking at 100+ cycle
delay).

sent from my phone
On Jan 29, 2015 3:56 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

>  These hardware discussions are getting somewhat OT ... :)
>
> You can't insert an arbitrary function between the ll and sc - Any** other
> store within the ll/sc block can invalidate the sc.
>
> ll/sc can be immune to ABA problem but not once you code the CAS up in the
> loop form - that makes it also susceptible to ABA.
>
> ** Which stores can impact the sc may, or may not be specified by the
> architecture.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Oleksandr Otenko
> *Sent:* Friday, 30 January 2015 3:46 AM
> *To:* Vitaly Davidovich
> *Cc:* Doug Lea; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] AtomicReference CAS signatures
>
> That's an example of what is possible with LL/SC, and not possible with
> CMPXCHG - you can't insert an arbitrary function between the load and store
> inside CMPXCHG.
>
> Alex
>
> On 29/01/2015 17:02, Vitaly Davidovich wrote:
>
> Sorry, what is this supposed to show? That could be two separate cacheline
> writes here.
>
> On Thu, Jan 29, 2015 at 11:57 AM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>> Not necessarily.
>>
>> node.next=head;
>> head=node;
>>
>> Alex
>>
>>
>> On 29/01/2015 16:21, Vitaly Davidovich wrote:
>>
>> Sure, but can you give a concrete example of where this buys you
>> something? Is this supposed to allow detecting ABA issues?
>>
>> sent from my phone
>> On Jan 29, 2015 11:00 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>> Because with LL/SC you can do more than just swap.
>>>
>>> LDARX
>>> do your magic f(x)
>>> STLRX - it's atomic!
>>>
>>> Alex
>>>
>>> On 29/01/2015 15:57, Vitaly Davidovich wrote:
>>>
>>>  Not necessarily, no: the loop has to be there in case the core is
>>>> interrupted or blocked for some other reason.
>>>
>>>
>>> You mean the *thread* requesting that cacheline is interrupted/blocked,
>>> rather than the core, right? I think that's what you meant, given your
>>> earlier answer, but just want to make sure I understand.
>>>
>>> So how does the core know that some OS level thread of execution that
>>> requested the line is no longer "available"? Does this happen as part of
>>> context switch implementation on ARM, or is there something else?
>>>
>>> As an aside, do any of you guys know why ARM decided to provide this
>>> form of LL/SC and not offer a cmpxchg type of instruction? It seems like
>>> all the cache coherence machinery is already there to get exclusive access
>>> to the line, so why not "go all the way" and couple the load + store into 1
>>> instruction since I'm guessing LDARX is almost always coupled with a STLRX
>>> anyway.
>>>
>>> On Thu, Jan 29, 2015 at 8:42 AM, Andrew Haley <aph at redhat.com> wrote:
>>>
>>>> On 01/29/2015 01:20 PM, Oleksandr Otenko wrote:
>>>>
>>>> > Yes, and the objection is that CMPXCHG will delay any other core
>>>> > obtaining E state, until CMPXCHG finishes. Yet, there is nothing in
>>>> > ARM instruction set that will prevent others obtaining the line in E
>>>> > state - unless the core can predict STLRX is coming,
>>>>
>>>> It's not at all unreasonable to predict that a STLRX follows an LDARX.
>>>> That's what I'd predict.
>>>>
>>>> > and can delay response to the challenge about the line until STLRX
>>>> > completes.
>>>>
>>>> There's nothing in the instruction set, but I don't think there needs
>>>> to be.  Any other core wanting to get that line in E state is going to
>>>> have to do a memory transaction, and by the time that transaction
>>>> complete the STLRX will probably have been finished for some time.
>>>>
>>>> > The presence of the loop means they don't always do this prediction,
>>>>
>>>> Not necessarily, no: the loop has to be there in case the core is
>>>> interrupted or blocked for some other reason.
>>>>
>>>> > but Stephen's comment seems to imply that if the same sequence
>>>> > appears to fail frequently, then the core might bias its decision
>>>> > towards completing STLRX first and delaying the other cores in
>>>> > obtaining the line in E state.
>>>>
>>>> Andrew.
>>>>
>>>
>>>
>>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/c7ddb55e/attachment.html>

From jsampson at guidewire.com  Thu Jan 29 16:37:42 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 29 Jan 2015 21:37:42 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu> <54B0FB69.7080009@redhat.com>
	<54C912C1.3030208@oracle.com> <54C9185F.4080700@redhat.com>
	<54C91A73.2000207@oracle.com> <54C91DF4.1010607@redhat.com>
	<CAHjP37FmmaU5xawi7D1AO3L3W3u_b3XE=4miUr72rfsfXYt8OA@mail.gmail.com>
	<54C92C6D.5030404@redhat.com>
	<CAHjP37HaEwdQn4n3LpG2dK1KJqebTF4-kGN1uuJ5Dz5JjTg0kw@mail.gmail.com>
	<CAJR39EzmTjiyPQ5vs9VAgZSNXtsaVXSt1_wZ8YyO693Zg6OaqQ@mail.gmail.com>
	<CAJR39ExHYo7iR7pviXjv3MqEM56bqkump_7Lq-Rrxfxd3DfHXQ@mail.gmail.com>
	<CAHjP37E=dsJ1t8sdCZhxwcC1nPdMbRZQiqDypyxGispvGOo=-w@mail.gmail.com>
	<54CA007E.6010607@redhat.com> <54CA33AC.5000300@oracle.com>
	<54CA38C7.8090106@redhat.com>
	<CAHjP37HQ37uf_T1ncjv5LWp48vO+TtgQv-GADVBrzW38iQHcAg@mail.gmail.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D960A9@sm-ex-01-vm.guidewire.com>

Vitaly Davidovich wrote:

> So how does the core know that some OS level thread of execution
> that requested the line is no longer "available"? Does this happen
> as part of context switch implementation on ARM, or is there
> something else?

Apparently that's the recommendation from ARM:

"When an operating system performs a context switch, it must reset
the local monitor to open state, to prevent false positives
occurring. ARMv6K introduced the Clear-Exclusive instruction, CLREX,
to reset the local monitor."

http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dht0008a/CJAGCFAF.html

Apropos of other questions on this thread, at the end of the same
section it says:

"For these reasons ARM recommends that:
* the Load-Exclusive and Store-Exclusive are no more than 128 bytes
  apart
* no explicit cache maintenance operations or data accesses are
  performed between the Load-Exclusive and the Store-Exclusive."

Cheers,
Justin


From jsampson at guidewire.com  Thu Jan 29 16:46:16 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 29 Jan 2015 21:46:16 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
References: <54CA71C3.8070008@oracle.com>
	<NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D960D5@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> ll/sc can be immune to ABA problem but not once you code the CAS
> up in the loop form - that makes it also susceptible to ABA.

Does it really? Once you start looping, each earlier failed LL/SC is
irrelevant. Once the final SC succeeds, isn't it still immune to ABA
situations between the most recent LL and that SC?

Cheers,
Justin


From aph at redhat.com  Thu Jan 29 17:02:45 2015
From: aph at redhat.com (Andrew Haley)
Date: Thu, 29 Jan 2015 22:02:45 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEBKKNAA.davidcholmes@aapt.net.au>
Message-ID: <54CAAE05.3090506@redhat.com>

On 01/29/2015 08:56 PM, David Holmes wrote:
> These hardware discussions are getting somewhat OT ... :)

Well, OK, but I don't think they are really OT.  There are many
misconceptions about how all this stuff we talk about really works,
and IMO they are harmful because they cause people to do wrong things.
(e.g. the old "avoid memory barriers because they flush the cache"
meme which I seem to have a lifelong task trying to squash.)  A
little injection of reality now and then is good for the soul!

Andrew.

From davidcholmes at aapt.net.au  Thu Jan 29 17:02:56 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 30 Jan 2015 08:02:56 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D960D5@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEBMKNAA.davidcholmes@aapt.net.au>

Justin writes:
> 
> David Holmes wrote:
> 
> > ll/sc can be immune to ABA problem but not once you code the CAS
> > up in the loop form - that makes it also susceptible to ABA.
> 
> Does it really? Once you start looping, each earlier failed LL/SC is
> irrelevant. Once the final SC succeeds, isn't it still immune to ABA
> situations between the most recent LL and that SC?

If a straight-through:

ll addr
cmp addr, expected
bne fail
sc addr, newval

succeeds then you know that the value at addr has not been changed in anyway since the ll - not even a store with the same value.

Once you add the loop all you know is that by the end of the loop *addr==expected at the final ll stage, but addr could have gone through an arbitrary number of changes in value since the first ll.

The CAS semantics, by definition are susceptible to the ABA problem. Hence algorithms based on CAS have to avoid the ABA problem by design.

David



From jsampson at guidewire.com  Thu Jan 29 18:26:12 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 29 Jan 2015 23:26:12 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEBMKNAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D960D5@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCGEBMKNAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D96195@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> Once you add the loop all you know is that by the end of the loop
> *addr==expected at the final ll stage, but addr could have gone
> through an arbitrary number of changes in value since the first
> ll.
>
> The CAS semantics, by definition are susceptible to the ABA
> problem. Hence algorithms based on CAS have to avoid the ABA
> problem by design.

I think that I understand the need to deal with the ABA problem in
higher-level code, for example calling get() followed later by
calling compareAndSet() on an AtomicX. Most of the provided atomic
operations like getAndIncrement() don't care, because the update is
purely numerical: if get() returns 5 and then compareAndSet(5, 6)
succeeds, it doesn't matter whether the value was changed to 42 and
back to 5 in the meantime; the increment still appears atomic. It's
more of an issue for getAndUpdate() because the update function only
benefits from the happens-before ordering of the get(), and may miss
other changes that happen-before the compareAndSet(); if the update
function cares about anything at all besides the atomic value
itself, that could lead to inconsistencies.

But I'm having trouble seeing how it could matter at all within the
low-level implementation of CAS itself. Isn't a loop of several
failed LL/SC pairs followed by a successful LL/SC pair
indistinguishable from spinning for several cycles before a single
successful LL/SC pair? How are the observable semantics any
different? The only concern I can think of is similar to the
getAndUpdate() situation where the memory barriers are too loose. If
only the very first LL prevents reordering of later reads, then you
could have reads pulled up above the final successful LL/SC. But
that seems too obviously wrong to consider. It doesn't matter if
later reads are pulled up above the final SC, as long as they don't
cross the final LL. Aren't the memory ordering effects at least that
strong?

I was pondering this question myself recently, and was too shy to
ask it for fear of exposing my ignorance on the matter, but as long
as someone else brought it up, I may as well expose away. :)

Cheers,
Justin


From davidcholmes at aapt.net.au  Thu Jan 29 18:37:14 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 30 Jan 2015 09:37:14 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D96195@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEBOKNAA.davidcholmes@aapt.net.au>

The ABA issue was mentioned and one of the benefits of ll/sc is that you can use it in a way that doesn't suffer from the ABA problem**. I was simply clarifying that once you use ll/sc in the loop to form the CAS then the use of the CAS within an algorithm is still subject to ABA.

Sorry that was unclear.

** Though with the constraints on what can be done between the ll and sc there are few examples of this.

David
-----

Justin writes:
> 
> David Holmes wrote:
> 
> > Once you add the loop all you know is that by the end of the loop
> > *addr==expected at the final ll stage, but addr could have gone
> > through an arbitrary number of changes in value since the first
> > ll.
> >
> > The CAS semantics, by definition are susceptible to the ABA
> > problem. Hence algorithms based on CAS have to avoid the ABA
> > problem by design.
> 
> I think that I understand the need to deal with the ABA problem in
> higher-level code, for example calling get() followed later by
> calling compareAndSet() on an AtomicX. Most of the provided atomic
> operations like getAndIncrement() don't care, because the update is
> purely numerical: if get() returns 5 and then compareAndSet(5, 6)
> succeeds, it doesn't matter whether the value was changed to 42 and
> back to 5 in the meantime; the increment still appears atomic. It's
> more of an issue for getAndUpdate() because the update function only
> benefits from the happens-before ordering of the get(), and may miss
> other changes that happen-before the compareAndSet(); if the update
> function cares about anything at all besides the atomic value
> itself, that could lead to inconsistencies.
> 
> But I'm having trouble seeing how it could matter at all within the
> low-level implementation of CAS itself. Isn't a loop of several
> failed LL/SC pairs followed by a successful LL/SC pair
> indistinguishable from spinning for several cycles before a single
> successful LL/SC pair? How are the observable semantics any
> different? The only concern I can think of is similar to the
> getAndUpdate() situation where the memory barriers are too loose. If
> only the very first LL prevents reordering of later reads, then you
> could have reads pulled up above the final successful LL/SC. But
> that seems too obviously wrong to consider. It doesn't matter if
> later reads are pulled up above the final SC, as long as they don't
> cross the final LL. Aren't the memory ordering effects at least that
> strong?
> 
> I was pondering this question myself recently, and was too shy to
> ask it for fear of exposing my ignorance on the matter, but as long
> as someone else brought it up, I may as well expose away. :)
> 
> Cheers,
> Justin
> 



From jsampson at guidewire.com  Thu Jan 29 18:56:00 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Thu, 29 Jan 2015 23:56:00 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEBOKNAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D96195@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCCEBOKNAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D96283@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> The ABA issue was mentioned and one of the benefits of ll/sc is
> that you can use it in a way that doesn't suffer from the ABA
> problem**. I was simply clarifying that once you use ll/sc in the
> loop to form the CAS then the use of the CAS within an algorithm
> is still subject to ABA.
>
> Sorry that was unclear.
>
> ** Though with the constraints on what can be done between the ll
> and sc there are few examples of this.

Sorry, still unclear. :)

I believe the following to be true:

* An LL/SC pair that contains anything other than simple comparison
  or arithmetic on the observed value is subject to the ABA problem,
  regardless of whether it's done in a loop to retry spurious
  failures.

* An LL/SC pair that only contains simple comparison or arithmetic
  on the observed value (like CAS) is _not_ subject to the ABA
  problem, regardless of whether it's done in a loop to retry
  spurious failures.

* Any higher-level algorithm using CAS must be designed with an
  awareness of the ABA problem, even though the CAS operation itself
  is _not_ subject to the ABA problem, since the underlying value
  may change back and forth between any two get() or compareAndSet()
  calls.

Am I getting close?

Cheers,
Justin


From davidcholmes at aapt.net.au  Thu Jan 29 19:07:38 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 30 Jan 2015 10:07:38 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D96283@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEBPKNAA.davidcholmes@aapt.net.au>

Justin writes:
> 
> David Holmes wrote:
> 
> > The ABA issue was mentioned and one of the benefits of ll/sc is
> > that you can use it in a way that doesn't suffer from the ABA
> > problem**. I was simply clarifying that once you use ll/sc in the
> > loop to form the CAS then the use of the CAS within an algorithm
> > is still subject to ABA.
> >
> > Sorry that was unclear.
> >
> > ** Though with the constraints on what can be done between the ll
> > and sc there are few examples of this.
> 
> Sorry, still unclear. :)
> 
> I believe the following to be true:
> 
> * An LL/SC pair that contains anything other than simple comparison
>   or arithmetic on the observed value is subject to the ABA problem,
>   regardless of whether it's done in a loop to retry spurious
>   failures.
>
> * An LL/SC pair that only contains simple comparison or arithmetic
>   on the observed value (like CAS) is _not_ subject to the ABA
>   problem, regardless of whether it's done in a loop to retry
>   spurious failures.

I don't understand your distinction above. Given the practical limits on what can occur between the ll and sc, your first case will likely always induce spurious failures.

In the second case if you add in the retry loop then you can not detect the A-B-A transition. It is the absence of a loop that allows ll/sc to fail if the A-B-A transition occurs.

 
> * Any higher-level algorithm using CAS must be designed with an
>   awareness of the ABA problem, even though the CAS operation itself
>   is _not_ subject to the ABA problem, since the underlying value
>   may change back and forth between any two get() or compareAndSet()
>   calls.

It doesn't make sense to me to talk about ABA in relation to the CAS operation itself. ABA comes from the context in which the CAS is used.

David
-----
 
> Am I getting close?
> 
> Cheers,
> Justin
> 



From vitalyd at gmail.com  Thu Jan 29 19:36:58 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 29 Jan 2015 19:36:58 -0500
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEBPKNAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D96283@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCGEBPKNAA.davidcholmes@aapt.net.au>
Message-ID: <CAHjP37GDhUq9f7Y=J0nbKwAKUEh2Z0gCwzGOHNEkxGSJ-_3bvg@mail.gmail.com>

I think an intuitive way to describe LL/SC and ABA is as follows.  With a
single LL/SC, the load part "marks" the value (that's your A in ABA) - if
the underlying memory transitions to another and then back to initial (i.e.
BA), this is detected and SC fails.

Once you start looping (CAS), each individual iteration avoids its own ABA
but not if you consider A to be the very first read done on 1st iteration,
so to speak.

sent from my phone
On Jan 29, 2015 7:32 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

> Justin writes:
> >
> > David Holmes wrote:
> >
> > > The ABA issue was mentioned and one of the benefits of ll/sc is
> > > that you can use it in a way that doesn't suffer from the ABA
> > > problem**. I was simply clarifying that once you use ll/sc in the
> > > loop to form the CAS then the use of the CAS within an algorithm
> > > is still subject to ABA.
> > >
> > > Sorry that was unclear.
> > >
> > > ** Though with the constraints on what can be done between the ll
> > > and sc there are few examples of this.
> >
> > Sorry, still unclear. :)
> >
> > I believe the following to be true:
> >
> > * An LL/SC pair that contains anything other than simple comparison
> >   or arithmetic on the observed value is subject to the ABA problem,
> >   regardless of whether it's done in a loop to retry spurious
> >   failures.
> >
> > * An LL/SC pair that only contains simple comparison or arithmetic
> >   on the observed value (like CAS) is _not_ subject to the ABA
> >   problem, regardless of whether it's done in a loop to retry
> >   spurious failures.
>
> I don't understand your distinction above. Given the practical limits on
> what can occur between the ll and sc, your first case will likely always
> induce spurious failures.
>
> In the second case if you add in the retry loop then you can not detect
> the A-B-A transition. It is the absence of a loop that allows ll/sc to fail
> if the A-B-A transition occurs.
>
>
> > * Any higher-level algorithm using CAS must be designed with an
> >   awareness of the ABA problem, even though the CAS operation itself
> >   is _not_ subject to the ABA problem, since the underlying value
> >   may change back and forth between any two get() or compareAndSet()
> >   calls.
>
> It doesn't make sense to me to talk about ABA in relation to the CAS
> operation itself. ABA comes from the context in which the CAS is used.
>
> David
> -----
>
> > Am I getting close?
> >
> > Cheers,
> > Justin
> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150129/38109742/attachment.html>

From jsampson at guidewire.com  Thu Jan 29 20:23:09 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Fri, 30 Jan 2015 01:23:09 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEBPKNAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD2783D96283@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCGEBPKNAA.davidcholmes@aapt.net.au>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D96390@sm-ex-01-vm.guidewire.com>

David Holmes wrote:

> I don't understand your distinction above. Given the practical
> limits on what can occur between the ll and sc, your first case
> will likely always induce spurious failures.

Oops! Yes, blame it on lack of sleep. I started talking about ll/sc
as if they were get/cas. The first two bullet points were inspired
by the distinction between getAndUpdate and getAndIncrement, which
have different ABA-related semantics.

So, right, any ABA transition simply causes LL/SC to fail, so
there's never any problem. My larger point is that wrapping it in a
loop merely eliminates spurious failure due to ABA, it doesn't
introduce new problems due to ABA.

> In the second case if you add in the retry loop then you can not
> detect the A-B-A transition. It is the absence of a loop that
> allows ll/sc to fail if the A-B-A transition occurs.

Why would you care about "detecting" it? LL/SC can fail spuriously
for other reasons, so you're not actually detecting a definite ABA
transition. Wrapping it in a loop just means that you're throwing
away all the failed LL/SC's until you get one that succeeds. That's
semantically no different from just doing one LL/SC slightly later,
so there's no observable semantic effect, right?

> It doesn't make sense to me to talk about ABA in relation to the
> CAS operation itself. ABA comes from the context in which the CAS
> is used.

I agree with that, but my confusion was triggered by your earlier
statement, "ll/sc can be immune to ABA problem but not once you code
the CAS up in the loop form - that makes it also susceptible to
ABA."

Since ABA is really a matter of the _context_ in which CAS is used,
the user has to be concerned with arbitrary A-B-A-B-A transitions
happening before the CAS anyway. If one LL/SC fails due to an A-B-A
transition and then a retried LL/SC succeeds, that can't be
distinguished semantically from an A-B-A transition that happened
right before the first LL/SC.

Cheers,
Justin


From davidcholmes at aapt.net.au  Thu Jan 29 21:23:36 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 30 Jan 2015 12:23:36 +1000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD2783D96390@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGECAKNAA.davidcholmes@aapt.net.au>

Justin writes:
> Why would you care about "detecting" it? LL/SC can fail spuriously
> for other reasons, so you're not actually detecting a definite ABA

http://en.wikipedia.org/wiki/Load-link/store-conditional

In practical terms, given the limits on what can occur between the ll and sc
on some architectures, there are few algorithms that might take advantage of
ll/sc's ability to detect A-B-A transitions. But as per link above, PPC is
more tolerant than ARM and so can support some such algorithms.

Cheers,
David


Justin writes:
>
> David Holmes wrote:
>
> > I don't understand your distinction above. Given the practical
> > limits on what can occur between the ll and sc, your first case
> > will likely always induce spurious failures.
>
> Oops! Yes, blame it on lack of sleep. I started talking about ll/sc
> as if they were get/cas. The first two bullet points were inspired
> by the distinction between getAndUpdate and getAndIncrement, which
> have different ABA-related semantics.
>
> So, right, any ABA transition simply causes LL/SC to fail, so
> there's never any problem. My larger point is that wrapping it in a
> loop merely eliminates spurious failure due to ABA, it doesn't
> introduce new problems due to ABA.
>
> > In the second case if you add in the retry loop then you can not
> > detect the A-B-A transition. It is the absence of a loop that
> > allows ll/sc to fail if the A-B-A transition occurs.
>
> Why would you care about "detecting" it? LL/SC can fail spuriously
> for other reasons, so you're not actually detecting a definite ABA
> transition. Wrapping it in a loop just means that you're throwing
> away all the failed LL/SC's until you get one that succeeds. That's
> semantically no different from just doing one LL/SC slightly later,
> so there's no observable semantic effect, right?
>
> > It doesn't make sense to me to talk about ABA in relation to the
> > CAS operation itself. ABA comes from the context in which the CAS
> > is used.
>
> I agree with that, but my confusion was triggered by your earlier
> statement, "ll/sc can be immune to ABA problem but not once you code
> the CAS up in the loop form - that makes it also susceptible to
> ABA."
>
> Since ABA is really a matter of the _context_ in which CAS is used,
> the user has to be concerned with arbitrary A-B-A-B-A transitions
> happening before the CAS anyway. If one LL/SC fails due to an A-B-A
> transition and then a retried LL/SC succeeds, that can't be
> distinguished semantically from an A-B-A transition that happened
> right before the first LL/SC.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gergg at cox.net  Fri Jan 30 01:33:24 2015
From: gergg at cox.net (Gregg Wonderly)
Date: Fri, 30 Jan 2015 00:33:24 -0600
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <iaaH1p00P02hR0p01aaJlj>
References: <1420743543652-11812.post@n7.nabble.com>
	<CAHjP37Fp-0Ms5s=LCjJvkC_tm5NBd3sf-yfpiZb6h9G0YcEUXA@mail.gmail.com>
	<54B7CD9B.1070908@oracle.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<hqKe1p00m02hR0p01qKjmt>
	<129D2A38-A415-4A10-AD3D-6D77FF055651@cox.ne! t>
	<iaaH1p00P02hR0p01aaJlj>
Message-ID: <B8AD1A1F-A978-45D4-82A8-7BD602926370@cox.net>


> On Jan 21, 2015, at 4:23 AM, Andrew Haley <aph at redhat.com> wrote:
> 
> On 20/01/15 22:44, Gregg Wonderly wrote:
> 
>> There are just countless places where I have had to deal with all
>> aspects of the different Java versions. I never deployed anything on
>> 1.2 due to the huge number of problems in that release (that release
>> pretty much killed the momentum of Java in the market place as a
>> desktop environment, and that was a disastrous outcome for Java
>> overall).  I?ve dealt with very careful tuning of multi-threaded
>> applications, such as my broker, which had hundreds of interacting
>> threads around a handful of data structures.  There are places where
>> I was a bit reckless with synchronization, because the early JVMs
>> didn?t show signs of caring about that.  This whole loop hoisting
>> bit is one of the places where I knew that the values were not
>> strictly synchronized.  But, I did not care, because eventually the
>> change in the loop control variable was always visible.
> 
> Loop hoisting in the compiler is the least of your problems: the
> hardware also does it for you.  You might have been able to get away
> with it on x86, which goes to great lengths to automagically handle
> cache coherency for you.

Yes.  And the problem is that we let it drive failure, rather than have the software structures make all of that immaterial.  Java is a sequential language by structure.  Developers expect that.  Rather than continuing to mess with Java the language, we should be working on a new language design that is all about exploiting concurrency, rather than making concurrency implementations a hazard to software systems that we have today.

> We're trying to write the next generation of high-performance software
> for the next generation of high-performance hardware.  Newer
> processors buffer writes to memory (sometimes for a very long time)
> and need explicit synchronization for correctness.  They do this for
> very good reasons to do with power consumption and performance: if you
> have a many-core processor you need to minimize traffic associated
> with managing cache coherency.  Ergo, volatile.  If we were to make
> all field accesses sequentially consistent a many-core machine might
> have so much traffic that it'd slow down to the speed of a few cores.
> This would make Java unsuitable for programming those machines.

Java is becoming unusable for programming on those machines because it?s constructs don?t allow adequate abstraction of concurrency concepts so that developers can create flow control that always works.  Instead, we have to carefully construct relationships between threads in ways that are not about concurrency but rather about convincing the hardware to perform operations that allow the most performant concurrency between threads, in a way that meets some form of appropriate, sequential consistency to boot.

Again, it comes back to JIT optimization of non-volatile variables.  It is just completely impossible for developers to ?guess? what optimization will happen next to break their software.  Since volatile was the ?trigger?, why would you expect that there would not be panic and ignorance driving them to just drop volatile everywhere since that fixes one predominate problem that breaks software, completely?

> Sure, those of us who have been around the block a few times sometimes
> wish that the world were as simple as we think it used to be.  But
> people who do not know where to make variables volatile are unlikely
> to write correct multi-threaded code which accesses shared state in
> memory, even without needing volatile annotations.  Sure, they might
> test it on x86 and declare it to be correct, but that's not the same
> thing at all.

This is the statement that demonstrates the issue for me.  Why are we using a language full of sequential flow control constructs and statements, and trying to artificially fit concurrency into it, in a way that continuous to be full of failure modes that create problematic software systems?

Concurrency is hard when expressed in terms of sequential software constructs that promote ?observation? instead of demanding rendezvous for sharing.  The lambda work and some other things, like the callback/messaging designs that Doug is working on, are the ways to promote software systems that work in concurrent environments.

We need concurrency constructs that promote sound software systems.

All of the focus and interest in micro optimizing the software around the ?behavior? of the hardware, which is orthogonal to the language?s sequential software constructs seems problematic to me, and appears to becoming very hard to get ?right? for developers and implementations both.  Why not do something easier?

Gregg Wonderly

> Andrew.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From aph at redhat.com  Fri Jan 30 03:44:19 2015
From: aph at redhat.com (Andrew Haley)
Date: Fri, 30 Jan 2015 08:44:19 +0000
Subject: [concurrency-interest] unpark/park memory visibility
In-Reply-To: <B8AD1A1F-A978-45D4-82A8-7BD602926370@cox.net>
References: <1420743543652-11812.post@n7.nabble.com>
	<CAHjP37GP=E8Z5rkvk8HiB9JM9MVhkS7cnuNDmc93exOGaCX0hA@mail.gmail.com>
	<54B7D57A.6000703@oracle.com>
	<CAHjP37F2Z0t9o-=5TfcT6AGBd2YLb-K0AEbFtbwUTvDQM22RRg@mail.gmail.com>
	<54B7EC2A.1040409@oracle.com>
	<CAHjP37HWzWnGpVvN0GV4nkwt4K_6QLutWRGyTtM6-WNn1LuR7g@mail.gmail.com>
	<54B7FC50.7020108@oracle.com> <54B80729.7030609@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D83956@sm-ex-01-vm.guidewire.com>
	<54B8399F.6000106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83A4C@sm-ex-01-vm.guidewire.com>
	<54B84055.2000701@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83B20@sm-ex-01-vm.guidewire.com>
	<54B928DF.80106@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD2783D83EA6@sm-ex-01-vm.guidewire.com>
	<gmPL1p01B02hR! 0p01mPNPG>
	<F350C472-C399-444A-84E1-C3D1308B39F6@cox.net>
	<hqKe1p00m02hR0p01qKjmt>
	<129D2A38-A415-4A10-AD3D-6D77FF055651@cox.ne! ! t>
	<iaaH1p00P02hR0p01aaJlj>
	<B8AD1A1F-A978-45D4-82A8-7BD602926370@cox.net>
Message-ID: <54CB4463.7070308@redhat.com>

On 30/01/15 06:33, Gregg Wonderly wrote:
> 
>> Sure, those of us who have been around the block a few times
>> sometimes wish that the world were as simple as we think it used to
>> be.  But people who do not know where to make variables volatile
>> are unlikely to write correct multi-threaded code which accesses
>> shared state in memory, even without needing volatile annotations.
>> Sure, they might test it on x86 and declare it to be correct, but
>> that's not the same thing at all.
> 
> This is the statement that demonstrates the issue for me.  Why are
> we using a language full of sequential flow control constructs and
> statements, and trying to artificially fit concurrency into it, in a
> way that continuous to be full of failure modes that create
> problematic software systems?

There's nothing artificial about concurrency in Java.  It is one of the
first mainstream languages to make a real stab at supporting
concurrency.

You're making a fundamental category error: the structures we're
discussing are precisely those that any language designer must use in
order to create secure abstractions for application programmers to
use.  Shared variables are very difficult to manage in large-scale
software, so we are providing the essential tools people need.  To do
that we need to deal with the low-level reality of synchronization.

> Concurrency is hard when expressed in terms of sequential software
> constructs that promote ?observation? instead of demanding
> rendezvous for sharing.  The lambda work and some other things, like
> the callback/messaging designs that Doug is working on, are the ways
> to promote software systems that work in concurrent environments.
> 
> We need concurrency constructs that promote sound software systems.

Of course.  That's what we're creating.  Where have you been?

> All of the focus and interest in micro optimizing the software
> around the ?behavior? of the hardware, which is orthogonal to the
> language?s sequential software constructs seems problematic to me,
> and appears to becoming very hard to get ?right? for developers and
> implementations both.

Yes.  It's hard.  But people need better and higher-level constructs,
so we need to write them.  And we cannot do that without getting the
low-level details exactly right.

Andrew.

From dl at cs.oswego.edu  Fri Jan 30 07:39:14 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 30 Jan 2015 07:39:14 -0500
Subject: [concurrency-interest] Q. Stealing in fully strict applications
In-Reply-To: <1422536614543-12302.post@n7.nabble.com>
References: <CAB4+JYJS1EO6aPfOa6Oku5U6etsjOSVO1cKaUHsk3iSseV7vgQ@mail.gmail.com>	<54C7F882.7000500@cs.oswego.edu>	<CAB4+JYJ-WGmrYXVx--a1_oSB=3MuRvigf-ijxbEajdOV-O5DcQ@mail.gmail.com>	<CA+kOe09rkamQN2Uehk=Kbw815RQHRhqZHM8ODYR=VZfgJGFk+w@mail.gmail.com>	<CAB4+JYK1mdoNi5L-k+UaDyfMXDhN2SoALpZcn0a=xaQApGSm9w@mail.gmail.com>	<54C8E805.1010402@cs.oswego.edu>
	<1422536614543-12302.post@n7.nabble.com>
Message-ID: <54CB7B72.6030700@cs.oswego.edu>

On 01/29/2015 08:03 AM, thurstonn wrote:

> But my understanding of your response to an earlier question I  posed
> <http://jsr166-concurrency.10961.n7.nabble.com/FJ-stack-bounds-td11552.html>
> regarding the max stack height of a FJWT was:
> msh + (msh - 1) + . . . + 1, where msh := max sequential stack height, which
> is more aggressive (less conservative) than leapfrogging, where msh's are ==
> (P2 in the leapfrogging paper)

When FJ uses leap-frogging, it has the same msh bound. It
also handles a few additional cases that amount to
stealing a sibling in a local deque, at the cost of
more stack frames but fewer threads. Almost none of these
cases are good programming practice, but are not illegal.
For example, the infamously bad out-of-order-join case:
   a.fork(); b.fork(); a.join(); b.join();

-Doug




From dl at cs.oswego.edu  Fri Jan 30 08:44:14 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 30 Jan 2015 08:44:14 -0500
Subject: [concurrency-interest] Q: 8071326: ThreadPoolExecutor in
 endless thread creation loop if workQueue.take() throws RuntimeException
In-Reply-To: <54C9FF79.3020303@gmail.com>
References: <54C7B216.90404@oracle.com>	<CA+kOe09bPb9WnGD6mMw_w2dg0nyN_edYAmD+xeThqtpRGP3FpA@mail.gmail.com>	<54C7FD28.8070708@oracle.com>	<54C862ED.3040404@oracle.com>	<54C8D5B6.6070504@oracle.com>	<CA+kOe0-evOfrT7L-3ngY18UL8r5LMydU6CDHn3+SfjUr8TOv1A@mail.gmail.com>
	<54C9FF79.3020303@gmail.com>
Message-ID: <54CB8AAE.3040801@cs.oswego.edu>

On 01/29/2015 04:38 AM, Peter Levart wrote:

>   *
>   * <p>If hook or callback methods throw exceptions, internal worker
>   * threads may in turn fail and abruptly terminate.</dd>
>
>
> The last paragraph could explicitly spell-out what are the "callback" methods.
> That would be enough, I think.

Good idea; thanks. Changed to:

!  * <p>If hook, callback, or BlockingQueue methods throw exceptions,
!  * internal worker threads may in turn fail, abruptly terminate, and
!  * possibly be replaced.</dd>

-Doug


From oleksandr.otenko at oracle.com  Fri Jan 30 09:46:43 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 30 Jan 2015 14:46:43 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGECAKNAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCGECAKNAA.davidcholmes@aapt.net.au>
Message-ID: <54CB9953.1040206@oracle.com>

I think Justin is trying to point at the meaning of ABA.

For example, for atomic values, like integers, how can one tell apart a 
42 two cycles ago from 42 now? It is not easy to define ABA there - 
because the update is relative to 42, /not the context/ that defines 42 
/when/. The hardware can tell that 42 has changed twice, but can the 
software tell the difference, if it didn't see 42 two cycles ago?

But ABA is easy to define as a problem for data structures: reusing a 
node in a tree, list, queue, an index in an array, etc creates a 
problem, if the contents of the node may change. Then the atomic update 
can only ensure a modification of the reference under assumption of what 
the content of the node was (which determines the /meaning/ of A in 
ABA). In this respect the LL/SC that does not allow other loads/stores 
between them, cannot also ensure absence of ABA problem - because you 
have to code the check of the contents (to make a decision how to 
update) outside LL/SC, and then it doesn't matter what LL/SC can do, 
because it can't detect the change in the /meaning/ of A.

Alex

On 30/01/2015 02:23, David Holmes wrote:
> Justin writes:
>> Why would you care about "detecting" it? LL/SC can fail spuriously
>> for other reasons, so you're not actually detecting a definite ABA
> http://en.wikipedia.org/wiki/Load-link/store-conditional
>
> In practical terms, given the limits on what can occur between the ll and sc
> on some architectures, there are few algorithms that might take advantage of
> ll/sc's ability to detect A-B-A transitions. But as per link above, PPC is
> more tolerant than ARM and so can support some such algorithms.
>
> Cheers,
> David
>
>
> Justin writes:
>> David Holmes wrote:
>>
>>> I don't understand your distinction above. Given the practical
>>> limits on what can occur between the ll and sc, your first case
>>> will likely always induce spurious failures.
>> Oops! Yes, blame it on lack of sleep. I started talking about ll/sc
>> as if they were get/cas. The first two bullet points were inspired
>> by the distinction between getAndUpdate and getAndIncrement, which
>> have different ABA-related semantics.
>>
>> So, right, any ABA transition simply causes LL/SC to fail, so
>> there's never any problem. My larger point is that wrapping it in a
>> loop merely eliminates spurious failure due to ABA, it doesn't
>> introduce new problems due to ABA.
>>
>>> In the second case if you add in the retry loop then you can not
>>> detect the A-B-A transition. It is the absence of a loop that
>>> allows ll/sc to fail if the A-B-A transition occurs.
>> Why would you care about "detecting" it? LL/SC can fail spuriously
>> for other reasons, so you're not actually detecting a definite ABA
>> transition. Wrapping it in a loop just means that you're throwing
>> away all the failed LL/SC's until you get one that succeeds. That's
>> semantically no different from just doing one LL/SC slightly later,
>> so there's no observable semantic effect, right?
>>
>>> It doesn't make sense to me to talk about ABA in relation to the
>>> CAS operation itself. ABA comes from the context in which the CAS
>>> is used.
>> I agree with that, but my confusion was triggered by your earlier
>> statement, "ll/sc can be immune to ABA problem but not once you code
>> the CAS up in the loop form - that makes it also susceptible to
>> ABA."
>>
>> Since ABA is really a matter of the _context_ in which CAS is used,
>> the user has to be concerned with arbitrary A-B-A-B-A transitions
>> happening before the CAS anyway. If one LL/SC fails due to an A-B-A
>> transition and then a retried LL/SC succeeds, that can't be
>> distinguished semantically from an A-B-A transition that happened
>> right before the first LL/SC.
>>
>> Cheers,
>> Justin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150130/299226a4/attachment-0001.html>

From thurston at nomagicsoftware.com  Sat Jan 31 12:33:17 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Sat, 31 Jan 2015 10:33:17 -0700 (MST)
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <54C8DE46.1050900@oracle.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<1420904890038-11835.post@n7.nabble.com>
	<54C8DE46.1050900@oracle.com>
Message-ID: <1422725597589-12338.post@n7.nabble.com>

Sigh.
You must not be understanding my point; usually I find (pseudo) code to be
the most effective way,but I'll try to put it in English.

a native compare-and-swap:  (T, T) -> T

is the **only** way to answer this question:

"What was the value of the memory location that caused the conditional-set
to *fail*"?

I repeat, the only way.


The following two lines of code are **not semantically equivalent**

assert compare-and-set(..., ...) : "oops - should have succeeded but failed
due to " + get()

assert (T result = compare-and-swap(..., ...)) == (T) expected : "oops -
should have succeeded but failed due to " + result

That should be obvious, and no presumption of a CAS-spin or some other loop
is relevant.


A compare-and-swap **failure** is semantically equivalent to a
**standalone** get().  In fact, a semantically equivalent implementation of
#get would be:

T get()
{
     return compare-and-swap(new Object(), new Object()) //guaranteed to
'fail'

}
 

The designers of CMPXCHG understand this (as well as C++11, and openjdk's
Atomic::cmpxchg(),et al), after all, it could just update some boolean flag
in a register (indeed it does this), but in addition it places the value of
the memory location in a register - it is wasteful to have to issue a
subsequent load of the same memory location, when in some circumstances, a
user knows (from context) that the same value will be returned (think state
machine).

You need to free yourself from the assumption that CAS is only useful as
part of a user-defined CAS spin. Yes, in most cases that is true, but not in
all







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/AtomicReference-CAS-signatures-tp11820p12338.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jsampson at guidewire.com  Sat Jan 31 17:13:54 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 31 Jan 2015 22:13:54 +0000
Subject: [concurrency-interest] AtomicReference CAS signatures
In-Reply-To: <1422725597589-12338.post@n7.nabble.com>
References: <1420840643988-11820.post@n7.nabble.com>
	<54B07067.30103@cs.oswego.edu>
	<0FD072A166C6DC4C851F6115F37DDD2783D818DA@sm-ex-01-vm.guidewire.com>
	<1420904890038-11835.post@n7.nabble.com>	<54C8DE46.1050900@oracle.com>
	<1422725597589-12338.post@n7.nabble.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD2783D969A7@sm-ex-01-vm.guidewire.com>

thurstonn wrote:

> Sigh.
> You must not be understanding my point; usually I find (pseudo)
> code to be the most effective way, but I'll try to put it in
> English.

I think you and Alex are talking past each other, not so much due to
lack of understanding but more due to lack of alignment about what's
useful or not.

> a native compare-and-swap:  (T, T) -> T
>
> is the **only** way to answer this question:
>
> "What was the value of the memory location that caused the
> conditional-set to *fail*"?
>
> I repeat, the only way.

That kind of depends on what you mean by "native." It could be:

* An assembly-language routine written with a single machine
  instruction (cmpxchg).
* An assembly-language routine written with a pair of machine
  instructions wrapped in a retry loop (ll/sc).
* A Java-language library method calling the native get() and
  compareAndSet() wrapped in a retry loop.

Any of those can produce the semantics you want. Your examples may
motivate the existence of a compareAndSwap() method, but they don't
really show why it has to be a single machine instruction.

> The following two lines of code are **not semantically
> equivalent**
>
> assert compare-and-set(..., ...) : "oops - should have succeeded
> but failed due to " + get()
>
> assert (T result = compare-and-swap(..., ...)) == (T) expected :
> "oops - should have succeeded but failed due to " + result
>
> That should be obvious, and no presumption of a CAS-spin or some
> other loop is relevant.
> [...]
> You need to free yourself from the assumption that CAS is only
> useful as part of a user-defined CAS spin. Yes, in most cases that
> is true, but not in all

So far you've been focusing on this assertion use case. I think
Alex's point is that if you're asserting that the CAS has succeeded,
that must mean that your algorithm is assuming that no other threads
are updating the value concurrently. For example, if the current
thread holds a lock and is going to release it, you can assume that
no other thread is modifying the lock state. But typically the
unlock would just be coded as state.set(false) rather than
state.compareAndSet(true, false), because it's simpler, faster, and
semantically equivalent given the algorithm's design. If your
algorithm design is such that the value might be getting updated by
other threads, then you shouldn't be asserting success, and you will
probably have your own retry loop around the CAS anyway.

To put it bluntly, if your primary motivation for having a
compare-and-swap intrinsic is to improve the error message that gets
displayed if it turns out that your algorithm is incorrect (the only
way an assert should ever fail), I don't think you have much chance
of sympathy from someone like Alex. :)

Cheers,
Justin


