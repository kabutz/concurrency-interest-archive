From chakravarthy.varaga at wipro.com  Thu May  1 05:33:18 2008
From: chakravarthy.varaga at wipro.com (chakravarthy.varaga at wipro.com)
Date: Thu, 1 May 2008 15:03:18 +0530
Subject: [concurrency-interest] Garbage Collection - SingleThreadedExecutor
Message-ID: <DC91351678E31E4FB24B551221F507EC0660DB74@blr-itp-msg.wipro.com>

Hi,
 
    I have used an executor SingleThreadExecutor from the
ExecutorService. Basically my tasks are update tasks in a CORBA server
and the tasks use java instances locally. 
    I was wondering how would GC happen in a SingleThreadExecutor? What
queue is used internally in this ExecutorService and how does the queue
releases the references to the Java instances?
 
    Any help here is highly appreciated?
 
Thanks & Regards
/Varaga

Please do not print this email unless it is absolutely necessary. 

The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s) and may contain proprietary, confidential or privileged information. If you are not the intended recipient, you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately and destroy all copies of this message and any attachments. 

WARNING: Computer viruses can be transmitted via email. The recipient should check this email and any attachments for the presence of viruses. The company accepts no liability for any damage caused by any virus transmitted by this email. 

www.wipro.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080501/880ea00b/attachment.html 

From dcholmes at optusnet.com.au  Thu May  1 15:18:33 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 02 May 2008 05:18:33 +1000
Subject: [concurrency-interest] Garbage Collection -
	SingleThreadedExecutor
In-Reply-To: <DC91351678E31E4FB24B551221F507EC0660DB74@blr-itp-msg.wipro.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHFHLAA.dcholmes@optusnet.com.au>

It uses a LinkedBlockingQueue. When an element is extracted from the queue
(by the executors thread) the queue no longer references it. When the thread
finishes execution of the Runnable/Callable it stops referencing it too.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
chakravarthy.varaga at wipro.com
  Sent: Thursday, 1 May 2008 7:33 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Garbage Collection -
SingleThreadedExecutor


  Hi,

      I have used an executor SingleThreadExecutor from the ExecutorService.
Basically my tasks are update tasks in a CORBA server and the tasks use java
instances locally.
      I was wondering how would GC happen in a SingleThreadExecutor? What
queue is used internally in this ExecutorService and how does the queue
releases the references to the Java instances?

      Any help here is highly appreciated?

  Thanks & Regards
  /Varaga
  Please do not print this email unless it is absolutely necessary.

  The information contained in this electronic message and any attachments
to this message are intended for the exclusive use of the addressee(s) and
may contain proprietary, confidential or privileged information. If you are
not the intended recipient, you should not disseminate, distribute or copy
this e-mail. Please notify the sender immediately and destroy all copies of
this message and any attachments.

  WARNING: Computer viruses can be transmitted via email. The recipient
should check this email and any attachments for the presence of viruses. The
company accepts no liability for any damage caused by any virus transmitted
by this email.

  www.wipro.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/6cd67dfd/attachment.html 

From chakravarthy.varaga at wipro.com  Fri May  2 02:14:28 2008
From: chakravarthy.varaga at wipro.com (chakravarthy.varaga at wipro.com)
Date: Fri, 2 May 2008 11:44:28 +0530
Subject: [concurrency-interest] Garbage Collection -
	SingleThreadedExecutor
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHFHLAA.dcholmes@optusnet.com.au>
References: <DC91351678E31E4FB24B551221F507EC0660DB74@blr-itp-msg.wipro.com>
	<NFBBKALFDCPFIDBNKAPCGEHFHLAA.dcholmes@optusnet.com.au>
Message-ID: <DC91351678E31E4FB24B551221F507EC0660DB78@blr-itp-msg.wipro.com>

Hi,
 
    Thanks for the response.
    Yet another query,
        Basically I have a CORBA server and client requests pumps on my
server APIs asynchronously. One of the API throws a customized
exception. Lets call this API removeJob() throws InvalidJobException. If
I have to use a SingleThread executor, the API returns quickly, since
the remove activity is assigned as a task in the thread.
The scenario is when the Thread executes the task, it might throw an
exception. 
How could we use the concurrent packages to throw this exception back to
the client or Is there a way to throw the exception back to the client.
Please be noted. that the server is not aware of the client.
 
    Request experts' advice here.
 
Thanks & Regards
/Varaga

________________________________

From: David.Holmes at Sun.COM [mailto:David.Holmes at Sun.COM] On Behalf Of
David Holmes
Sent: Friday, May 02, 2008 12:49 AM
To: Chakravarthy Varaga (WT01 - Telecom Applications and Solutions);
concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Garbage Collection -
SingleThreadedExecutor


It uses a LinkedBlockingQueue. When an element is extracted from the
queue (by the executors thread) the queue no longer references it. When
the thread finishes execution of the Runnable/Callable it stops
referencing it too.
 
David Holmes

	-----Original Message-----
	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
chakravarthy.varaga at wipro.com
	Sent: Thursday, 1 May 2008 7:33 PM
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] Garbage Collection -
SingleThreadedExecutor
	
	
	Hi,
	 
	    I have used an executor SingleThreadExecutor from the
ExecutorService. Basically my tasks are update tasks in a CORBA server
and the tasks use java instances locally. 
	    I was wondering how would GC happen in a
SingleThreadExecutor? What queue is used internally in this
ExecutorService and how does the queue releases the references to the
Java instances?
	 
	    Any help here is highly appreciated?
	 
	Thanks & Regards
	/Varaga

	Please do not print this email unless it is absolutely
necessary. 

	The information contained in this electronic message and any
attachments to this message are intended for the exclusive use of the
addressee(s) and may contain proprietary, confidential or privileged
information. If you are not the intended recipient, you should not
disseminate, distribute or copy this e-mail. Please notify the sender
immediately and destroy all copies of this message and any attachments. 

	WARNING: Computer viruses can be transmitted via email. The
recipient should check this email and any attachments for the presence
of viruses. The company accepts no liability for any damage caused by
any virus transmitted by this email. 

	www.wipro.com 


Please do not print this email unless it is absolutely necessary. 

The information contained in this electronic message and any attachments to this message are intended for the exclusive use of the addressee(s) and may contain proprietary, confidential or privileged information. If you are not the intended recipient, you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately and destroy all copies of this message and any attachments. 

WARNING: Computer viruses can be transmitted via email. The recipient should check this email and any attachments for the presence of viruses. The company accepts no liability for any damage caused by any virus transmitted by this email. 

www.wipro.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/e90e7563/attachment.html 

From hanson.char at gmail.com  Fri May  2 03:10:58 2008
From: hanson.char at gmail.com (Hanson Char)
Date: Fri, 2 May 2008 00:10:58 -0700
Subject: [concurrency-interest] Javolution ?
Message-ID: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>

Hi,

What is this forum's thought on using the concurrent utilities of the
Javolution library on a standard (non-realtime) JVM ?

  http://javolution.org/api/javolution/util/package-summary.html

Is it a good/bad thing ?  For example, should one seriously consider using
FastMap instead of ConcurrentHashMap for improving performance and reducing
the load on GC ?

  http://javolution.org/api/javolution/util/FastMap.html

ConcurrentContext also looks interesting

  http://javolution.org/api/javolution/context/ConcurrentContext.html

etc.

Or are there some good reasons why one should probably stay away from using
it in general ?

Regards,
Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/5142fa0f/attachment.html 

From kasper at kav.dk  Fri May  2 06:11:19 2008
From: kasper at kav.dk (Kasper Nielsen)
Date: Fri, 02 May 2008 12:11:19 +0200
Subject: [concurrency-interest] Javolution ?
In-Reply-To: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>
References: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>
Message-ID: <481AE8C7.80701@kav.dk>

Hanson Char wrote:
> 
> Or are there some good reasons why one should probably stay away from 
> using it in general ?
> 
Yes, despite attempts to tell him otherwise. The author still believes 
it is only necessary to synchronize on structural updates such as put/clear.

See, for example, FastMap
https://javolution.dev.java.net/source/browse/javolution/src/javolution/util/FastMap.java?rev=1.17&view=markup


Or it could just be me that doesn't properly understand how FastMap 
should be used?

--------From the documentation--------
# Are shared maps valid substitutes for ConcurrentHashMap?

Unlike ConcurrentHashMap access to a shared FastMap never blocks. 
Retrieval reflects the map state not older than the last time the 
accessing threads have been synchronized* (for multi-processors systems 
synchronizing ensures that the CPU internal cache is not stale).

In practice, it means that most well-written concurrent programs should 
be able to use shared FastMap in place of ConcurrentHashMap as threads 
are already synchronized to ensure proper behavior.

* It is important for both threads to synchronize on the same monitor in 
order to set up the happens-before relationship properly. It is not the 
case that everything visible to thread A when it synchronizes on object 
X becomes visible to thread B after it synchronizes on object Y. The 
release and acquire have to "match" (i.e., be performed on the same 
monitor) to have the right semantics. Otherwise, the code has a data race.
----..................................

Cheers
   Kasper

From mthornton at optrak.co.uk  Fri May  2 06:58:29 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Fri, 02 May 2008 11:58:29 +0100
Subject: [concurrency-interest] Javolution ?
In-Reply-To: <481AE8C7.80701@kav.dk>
References: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>
	<481AE8C7.80701@kav.dk>
Message-ID: <481AF3D5.6020308@optrak.co.uk>

Kasper Nielsen wrote:
> Hanson Char wrote:
>   
>> Or are there some good reasons why one should probably stay away from 
>> using it in general ?
>>
>>     
> Yes, despite attempts to tell him otherwise. The author still believes 
> it is only necessary to synchronize on structural updates such as put/clear.
>
> See, for example, FastMap
> https://javolution.dev.java.net/source/browse/javolution/src/javolution/util/FastMap.java?rev=1.17&view=markup
>
>
> Or it could just be me that doesn't properly understand how FastMap 
> should be used?
>
> --------From the documentation--------
> # Are shared maps valid substitutes for ConcurrentHashMap?
>
> Unlike ConcurrentHashMap access to a shared FastMap never blocks. 
> Retrieval reflects the map state not older than the last time the 
> accessing threads have been synchronized* (for multi-processors systems 
> synchronizing ensures that the CPU internal cache is not stale).
>   
Is his assumption here, that if the reader sees any more recent changes 
(i.e. since last synchronization), that those changes will be in the 
order they were made on the mutating thread? I think that is true on 
some processors, and in that case FastMap might work on such processors.

Mark Thornton


From hanson.char at gmail.com  Fri May  2 12:39:01 2008
From: hanson.char at gmail.com (Hanson Char)
Date: Fri, 2 May 2008 09:39:01 -0700
Subject: [concurrency-interest] Javolution ?
In-Reply-To: <481AF3D5.6020308@optrak.co.uk>
References: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>
	<481AE8C7.80701@kav.dk> <481AF3D5.6020308@optrak.co.uk>
Message-ID: <ca53c8f80805020939l564c01a8l22e90a0984b1e039@mail.gmail.com>

Hi Jean,

Would you mind clarifying some of the questions asked about FastMap ?  (See
below.)

Thanks,
Hanson

On Fri, May 2, 2008 at 3:58 AM, Mark Thornton <mthornton at optrak.co.uk>
wrote:

> Kasper Nielsen wrote:
> > Hanson Char wrote:
> >
> >> Or are there some good reasons why one should probably stay away from
> >> using it in general ?
> >>
> >>
> > Yes, despite attempts to tell him otherwise. The author still believes
> > it is only necessary to synchronize on structural updates such as
> put/clear.
> >
> > See, for example, FastMap
> >
> https://javolution.dev.java.net/source/browse/javolution/src/javolution/util/FastMap.java?rev=1.17&view=markup
> >
> >
> > Or it could just be me that doesn't properly understand how FastMap
> > should be used?
> >
> > --------From the documentation--------
> > # Are shared maps valid substitutes for ConcurrentHashMap?
> >
> > Unlike ConcurrentHashMap access to a shared FastMap never blocks.
> > Retrieval reflects the map state not older than the last time the
> > accessing threads have been synchronized* (for multi-processors systems
> > synchronizing ensures that the CPU internal cache is not stale).
> >
> Is his assumption here, that if the reader sees any more recent changes
> (i.e. since last synchronization), that those changes will be in the
> order they were made on the mutating thread? I think that is true on
> some processors, and in that case FastMap might work on such processors.
>
> Mark Thornton
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/56e2174a/attachment.html 

From gregg at cytetech.com  Fri May  2 14:07:33 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Fri, 02 May 2008 13:07:33 -0500
Subject: [concurrency-interest] Javolution ?
In-Reply-To: <481AF3D5.6020308@optrak.co.uk>
References: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>
	<481AE8C7.80701@kav.dk> <481AF3D5.6020308@optrak.co.uk>
Message-ID: <481B5865.50005@cytetech.com>

Mark Thornton wrote:
> Kasper Nielsen wrote:
>> Hanson Char wrote:
>>   
>>> Or are there some good reasons why one should probably stay away from 
>>> using it in general ?
>>>
>>>     
>> Yes, despite attempts to tell him otherwise. The author still believes 
>> it is only necessary to synchronize on structural updates such as put/clear.
>>
>> See, for example, FastMap
>> https://javolution.dev.java.net/source/browse/javolution/src/javolution/util/FastMap.java?rev=1.17&view=markup
>>
>>
>> Or it could just be me that doesn't properly understand how FastMap 
>> should be used?
>>
>> --------From the documentation--------
>> # Are shared maps valid substitutes for ConcurrentHashMap?
>>
>> Unlike ConcurrentHashMap access to a shared FastMap never blocks. 
>> Retrieval reflects the map state not older than the last time the 
>> accessing threads have been synchronized* (for multi-processors systems 
>> synchronizing ensures that the CPU internal cache is not stale).
>>   
> Is his assumption here, that if the reader sees any more recent changes 
> (i.e. since last synchronization), that those changes will be in the 
> order they were made on the mutating thread? I think that is true on 
> some processors, and in that case FastMap might work on such processors.

It seems to me that he is saying:

	If you want to have visibility, then somewhere in YOUR code, you
	need to have "synchronized" on something that ensures that visiblity.

It seems to me that he is suggesting that you need to treat it as a HashMap for 
synchronizing writes vs the NEXT read after that write.  If you are just doing 
mostly reads, then there is no reason to do locking all the time.

Perhaps a delegate pattern such as shown here is what he's trying to describe?

public class FastHashMap<K,V> implements Map<K,V> {
	volatile boolean didWrite;
	Map<K,V> del;

	public FashHashMap( Map<K,V> delegate ) {
		del = delegate;
	}

	public void put( K key, V value ) {
		synchronized( this ) {
			didWrite = true;
			del.put( key, value );
		}
	}

	public V get( K key ) {
		if( didWrite ) {
			synchronized(this) {
				didWrite = false;
				return del.get(key);
			}
		}
		return del.get(key);
	}

	.... other methods ...
}

Gregg Wonderly

From dcholmes at optusnet.com.au  Fri May  2 14:08:21 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 03 May 2008 04:08:21 +1000
Subject: [concurrency-interest] Garbage Collection
	-SingleThreadedExecutor
In-Reply-To: <DC91351678E31E4FB24B551221F507EC0660DB78@blr-itp-msg.wipro.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEHMHLAA.dcholmes@optusnet.com.au>

The way exceptions are handled with asynchronous invocations in j.u.c is to
use a Future. If the code that performs the async operation encounters an
exception then this is recorded in the FutureTask object. The client code
that makes the async call is handed back the Future and can (later) query if
the computation was successful.

The other option - which you'll have to manually construct - is to have an
exception "callback".

The details really depend on the semantics of your API and how you want to
communicate the fact that the exception happend.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
chakravarthy.varaga at wipro.com
  Sent: Friday, 2 May 2008 4:14 PM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Garbage
Collection -SingleThreadedExecutor


  Hi,

      Thanks for the response.
      Yet another query,
          Basically I have a CORBA server and client requests pumps on my
server APIs asynchronously. One of the API throws a customized exception.
Lets call this API removeJob() throws InvalidJobException. If I have to use
a SingleThread executor, the API returns quickly, since the remove activity
is assigned as a task in the thread.
  The scenario is when the Thread executes the task, it might throw an
exception.
  How could we use the concurrent packages to throw this exception back to
the client or Is there a way to throw the exception back to the client.
Please be noted. that the server is not aware of the client.

      Request experts' advice here.

  Thanks & Regards
  /Varaga



----------------------------------------------------------------------------
--
  From: David.Holmes at Sun.COM [mailto:David.Holmes at Sun.COM] On Behalf Of
David Holmes
  Sent: Friday, May 02, 2008 12:49 AM
  To: Chakravarthy Varaga (WT01 - Telecom Applications and Solutions);
concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] Garbage Collection -
SingleThreadedExecutor


  It uses a LinkedBlockingQueue. When an element is extracted from the queue
(by the executors thread) the queue no longer references it. When the thread
finishes execution of the Runnable/Callable it stops referencing it too.

  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
chakravarthy.varaga at wipro.com
    Sent: Thursday, 1 May 2008 7:33 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] Garbage Collection -
SingleThreadedExecutor


    Hi,

        I have used an executor SingleThreadExecutor from the
ExecutorService. Basically my tasks are update tasks in a CORBA server and
the tasks use java instances locally.
        I was wondering how would GC happen in a SingleThreadExecutor? What
queue is used internally in this ExecutorService and how does the queue
releases the references to the Java instances?

        Any help here is highly appreciated?

    Thanks & Regards
    /Varaga
    Please do not print this email unless it is absolutely necessary.

    The information contained in this electronic message and any attachments
to this message are intended for the exclusive use of the addressee(s) and
may contain proprietary, confidential or privileged information. If you are
not the intended recipient, you should not disseminate, distribute or copy
this e-mail. Please notify the sender immediately and destroy all copies of
this message and any attachments.

    WARNING: Computer viruses can be transmitted via email. The recipient
should check this email and any attachments for the presence of viruses. The
company accepts no liability for any damage caused by any virus transmitted
by this email.

    www.wipro.com

  Please do not print this email unless it is absolutely necessary.

  The information contained in this electronic message and any attachments
to this message are intended for the exclusive use of the addressee(s) and
may contain proprietary, confidential or privileged information. If you are
not the intended recipient, you should not disseminate, distribute or copy
this e-mail. Please notify the sender immediately and destroy all copies of
this message and any attachments.

  WARNING: Computer viruses can be transmitted via email. The recipient
should check this email and any attachments for the presence of viruses. The
company accepts no liability for any damage caused by any virus transmitted
by this email.

  www.wipro.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080503/7e4abd22/attachment.html 

From dl at cs.oswego.edu  Fri May  2 14:24:58 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 02 May 2008 14:24:58 -0400
Subject: [concurrency-interest] Javolution ?
In-Reply-To: <481AF3D5.6020308@optrak.co.uk>
References: <ca53c8f80805020010w2522a457q7ecc1a2f256c6011@mail.gmail.com>	<481AE8C7.80701@kav.dk>
	<481AF3D5.6020308@optrak.co.uk>
Message-ID: <481B5C7A.9050102@cs.oswego.edu>

Mark Thornton wrote:

> Is his assumption here, that if the reader sees any more recent changes 
> (i.e. since last synchronization), that those changes will be in the 
> order they were made on the mutating thread? I think that is true on 
> some processors, and in that case FastMap might work on such processors.
> 

It might hypothetically work despite not being correct with respect
to the memory model if (JIT) compilers didn't reorder any of the reads
and writes in the various threads, but all do so these days, mainly
to lessen memory stalls.

-Doug

From joe.bowbeer at gmail.com  Fri May  2 16:17:25 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 2 May 2008 13:17:25 -0700
Subject: [concurrency-interest] Garbage Collection
	-SingleThreadedExecutor
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEHMHLAA.dcholmes@optusnet.com.au>
References: <DC91351678E31E4FB24B551221F507EC0660DB78@blr-itp-msg.wipro.com>
	<NFBBKALFDCPFIDBNKAPCKEHMHLAA.dcholmes@optusnet.com.au>
Message-ID: <31f2a7bd0805021317p2bba43fcsaed18edf73ad4a75@mail.gmail.com>

Elaborating on what David wrote:

In your CORBA example, you could add a callback to your removeJob() method:

  void removeJob(data, callback);

On the server side, instead of executing a Runnable, you execute a
FutureTask that overrides its done() method in order to invoke the client's
callback:


http://java.sun.com/javase/6/docs/api/java/util/concurrent/FutureTask.html#done()

Example: javax.swing.SwingWoker overrides done() in order to callback on the
event-dispatch thread.

--Joe

On Fri, May 2, 2008 at 11:08 AM, David Holmes wrote:

>  The way exceptions are handled with asynchronous invocations in j.u.c is
> to use a Future. If the code that performs the async operation encounters an
> exception then this is recorded in the FutureTask object. The client code
> that makes the async call is handed back the Future and can (later) query if
> the computation was successful.
>
> The other option - which you'll have to manually construct - is to have an
> exception "callback".
>
> The details really depend on the semantics of your API and how you want to
> communicate the fact that the exception happend.
>
> David Holmes
>
> -----Original Message-----
> **
>
> Hi,
>
>     Thanks for the response.
>     Yet another query,
>         Basically I have a CORBA server and client requests pumps on my
> server APIs asynchronously. One of the API throws a customized exception.
> Lets call this API removeJob() throws InvalidJobException. If I have to use
> a SingleThread executor, the API returns quickly, since the remove activity
> is assigned as a task in the thread.
> The scenario is when the Thread executes the task, it might throw an
> exception.
> How could we use the concurrent packages to throw this exception back to
> the client or Is there a way to throw the exception back to the client.
> Please be noted. that the server is not aware of the client.
>
>     Request experts' advice here.
>
> Thanks & Regards
> /Varaga
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/def6ef0b/attachment-0001.html 

From crazybob at crazybob.org  Fri May  2 19:44:45 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 2 May 2008 16:44:45 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <480796BF.1020606@sun.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<48067201.8090202@redhat.com>
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>
	<48075F96.2050604@redhat.com>
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>
	<480796BF.1020606@sun.com>
Message-ID: <a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>

Sorry for the delayed response...

On Thu, Apr 17, 2008 at 11:28 AM, Tom Hawtin <Thomas.Hawtin at sun.com> wrote:

> Standard equality doesn't make sense for weak/soft. Weak/soft references
>> are fundamentally identity based.
>>
>
> Weak certainly. But for soft?


I've been thinking about this, and I think it's perhaps even more true for
soft references. Presumably you're using a soft reference because you want
to save memory--why waste memory with two equivalent copies of the same
piece of data?

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/995c1a5e/attachment.html 

From jason.greene at redhat.com  Fri May  2 20:41:21 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Fri, 02 May 2008 19:41:21 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<48067201.8090202@redhat.com>
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>
	<48075F96.2050604@redhat.com>
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
Message-ID: <481BB4B1.1040701@redhat.com>

Bob Lee wrote:
> Sorry for the delayed response...
> 
> On Thu, Apr 17, 2008 at 11:28 AM, Tom Hawtin <Thomas.Hawtin at sun.com 
> <mailto:Thomas.Hawtin at sun.com>> wrote:
> 
>         Standard equality doesn't make sense for weak/soft. Weak/soft
>         references are fundamentally identity based.
> 
> 
>     Weak certainly. But for soft?
> 
> 
> I've been thinking about this, and I think it's perhaps even more true 
> for soft references. Presumably you're using a soft reference because 
> you want to save memory--why waste memory with two equivalent copies of 
> the same piece of data?

IMO, the typical case of a soft-key map is a simple cache, where you 
want the table entry (combined space consumed by the key and value) to 
stay around for as long as possible, until the VM actually needs the space.

Code that uses this cache, might not have the instance of the key 
available to it, especially if the key data is read from some other 
source (perhaps over i/o). While the construction of a duplicate key 
object from that data is consuming extra memory, its lifespan is most 
likely much shorter (it might only exist for a single operation).

-- 
Jason T. Greene
JBoss, a division of Red Hat

From crazybob at crazybob.org  Fri May  2 20:47:04 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 2 May 2008 17:47:04 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481BB4B1.1040701@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<48067201.8090202@redhat.com>
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>
	<48075F96.2050604@redhat.com>
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
	<481BB4B1.1040701@redhat.com>
Message-ID: <a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>

On Fri, May 2, 2008 at 5:41 PM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> IMO, the typical case of a soft-key map is a simple cache, where you want
> the table entry (combined space consumed by the key and value) to stay
> around for as long as possible, until the VM actually needs the space.
>
> Code that uses this cache, might not have the instance of the key available
> to it, especially if the key data is read from some other source (perhaps
> over i/o). While the construction of a duplicate key object from that data
> is consuming extra memory, its lifespan is most likely much shorter (it
> might only exist for a single operation).


Can you provide a use case for this? In the example you provided before, it
made more sense to have strongly referenced keys and soft values.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/1e21195b/attachment.html 

From jason.greene at redhat.com  Fri May  2 21:21:29 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Fri, 02 May 2008 20:21:29 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>	
	<48067201.8090202@redhat.com>	
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>	
	<48075F96.2050604@redhat.com>	
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>	
	<480796BF.1020606@sun.com>	
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>	
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
Message-ID: <481BBE19.401@redhat.com>

Bob Lee wrote:
> On Fri, May 2, 2008 at 5:41 PM, Jason T. Greene <jason.greene at redhat.com 
> <mailto:jason.greene at redhat.com>> wrote:
> 
>     IMO, the typical case of a soft-key map is a simple cache, where you
>     want the table entry (combined space consumed by the key and value)
>     to stay around for as long as possible, until the VM actually needs
>     the space.
> 
>     Code that uses this cache, might not have the instance of the key
>     available to it, especially if the key data is read from some other
>     source (perhaps over i/o). While the construction of a duplicate key
>     object from that data is consuming extra memory, its lifespan is
>     most likely much shorter (it might only exist for a single operation).
> 
> 
> Can you provide a use case for this? In the example you provided before, 
> it made more sense to have strongly referenced keys and soft values.
> 
So I had mentioned string keys as just a simple example, although it 
could be any serializable type. As an example an application receives a 
request for data over the network (RMI/HTTP/whatever), it then checks 
its local cache for the data. On a cache-miss it does a semi-expensive 
process to construct a value (possibly large) and stores it on the 
cache. On a hit, it can easily calculate and return a response from the 
value.

The problem with using strong keys and a soft value, is that the table 
will grow without bounds. Unless you periodically scan it for null 
values, and remove the corresponding key, but that would be inefficient.

-- 
Jason T. Greene
JBoss, a division of Red Hat

From crazybob at crazybob.org  Fri May  2 21:31:14 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 2 May 2008 18:31:14 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481BBE19.401@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<48067201.8090202@redhat.com>
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>
	<48075F96.2050604@redhat.com>
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
	<481BBE19.401@redhat.com>
Message-ID: <a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>

On Fri, May 2, 2008 at 6:21 PM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> So I had mentioned string keys as just a simple example, although it could
> be any serializable type. As an example an application receives a request
> for data over the network (RMI/HTTP/whatever), it then checks its local
> cache for the data. On a cache-miss it does a semi-expensive process to
> construct a value (possibly large) and stores it on the cache. On a hit, it
> can easily calculate and return a response from the value.
>

As you say below, you should use strong keys and soft values here.


> The problem with using strong keys and a soft value, is that the table will
> grow without bounds. Unless you periodically scan it for null values, and
> remove the corresponding key, but that would be inefficient.


I don't see the problem here. In our ReferenceMap, if either the key or
value is reclaimed by the garbage collector, we purge the entire entry. We
certainly don't "periodically scan for null values." I'm kind of surprised
you still haven't looked at our impl--all of this is clearly documented
there.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/ce255a15/attachment.html 

From dcholmes at optusnet.com.au  Sat May  3 01:06:17 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 03 May 2008 15:06:17 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <48175B0A.1080700@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEICHLAA.dcholmes@optusnet.com.au>

Gregg,

Thank you for the annual "security is broken in TPE" reminder :-) There's a
long history here that started with J2EE security issues and was convoluted
by the fact that the default security manager implementation actually
relaxed security rather than strengthening it - ie it doesn't implement the
installed security policy. We were in a no-win situation and originally used
a direct AccessController.checkPermission call. That upset some folks and it
was later changed to a SecurityManager.checkPermission call - which seemed
to address main concerns.

The bottom line is that the security architecture, as defined, is just
inadequate when it comes to thread creation, control and termination - it's
pretty much all or nothing, so there's no way to discern which threads you
should be allowed to control/modify (though ThreadGroup membership could be
used). There had to be a way to prevent TPE shutdown if the security policy
disallowed it (and in hindsight we perhaps should have gone through the
painful process of trying to get an explicit permission for this - the
reason we didn't was that to do the interrupt() you need the "modifyThread"
permission anyway so we just left it at that). But we couldn't trust the
(default) SecurityManager to enforce that policy hence we made the explicit
permission check.

The consequence of this is that you have the perverse situation where you
can create a TPE (and its threads) but not shut it down. If anyone can see a
resolution to this conundrum we'd be happy to hear it - though simply
disabling any security check is probably not an option (though maybe the
whole J2EE issue is moot now?)

BTW applets thread should be required to have certain permissions for
"modifying" threads outside the applet thread group. Eg you shouldn't be
able to find all threads in the system an interrupt them - but it's been
years since I checked that :)

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Wednesday, 30 April 2008 3:30 AM
> To: concurrency-interest
> Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
> shutdownpermission check.
>
>
> I have a STPE that I use in an applet.  At applet shutdown, I'd
> like to shutdown
> the executor, but alas it requests a Permission check for a
> RuntimePermission
> that I don't have in an unsigned applet.  It seems kind of silly
> to demand this
> permission when no other thread management permission exist in
> the JDK for
> applets.  Am I missing something in consideration of this issue?
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From jason.greene at redhat.com  Sat May  3 02:09:19 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Sat, 03 May 2008 01:09:19 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>	
	<48067201.8090202@redhat.com>	
	<aa067ea10804162340u3be7765ax884b5adc3a95e498@mail.gmail.com>	
	<48075F96.2050604@redhat.com>	
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>	
	<480796BF.1020606@sun.com>	
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>	
	<481BB4B1.1040701@redhat.com>	
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>	
	<481BBE19.401@redhat.com>
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
Message-ID: <481C018F.7090504@redhat.com>

Bob Lee wrote:
> On Fri, May 2, 2008 at 6:21 PM, Jason T. Greene <jason.greene at redhat.com 
> <mailto:jason.greene at redhat.com>> wrote:
> 
>     So I had mentioned string keys as just a simple example, although it
>     could be any serializable type. As an example an application
>     receives a request for data over the network (RMI/HTTP/whatever), it
>     then checks its local cache for the data. On a cache-miss it does a
>     semi-expensive process to construct a value (possibly large) and
>     stores it on the cache. On a hit, it can easily calculate and return
>     a response from the value.
> 
> 
> As you say below, you should use strong keys and soft values here.
>  
> 
>     The problem with using strong keys and a soft value, is that the
>     table will grow without bounds. Unless you periodically scan it for
>     null values, and remove the corresponding key, but that would be
>     inefficient.
> 
>  
> I don't see the problem here. In our ReferenceMap, if either the key or 
> value is reclaimed by the garbage collector, we purge the entire entry. 
> We certainly don't "periodically scan for null values." I'm kind of 
> surprised you still haven't looked at our impl--all of this is clearly 
> documented there.
> 

Ah, yes of course. I stand corrected. Well, in any case it might be 
desired for both the key and the value to be soft, to allow for faster 
reclamation. Although ideally, with the addition of ephemerons, the key 
would be soft, and the value would be a dependant reference.

-- 
Jason T. Greene
JBoss, a division of Red Hat

From crazybob at crazybob.org  Sat May  3 02:19:24 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Fri, 2 May 2008 23:19:24 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481C018F.7090504@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<48075F96.2050604@redhat.com>
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
	<481BBE19.401@redhat.com>
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
	<481C018F.7090504@redhat.com>
Message-ID: <a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>

On Fri, May 2, 2008 at 11:09 PM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> Ah, yes of course. I stand corrected. Well, in any case it might be desired
> for both the key and the value to be soft, to allow for faster reclamation.


Soft keys and soft values would create more (undesirable) work for the
garbage collector, but I don't see how it would speed up reclamation.
Another problem with soft keys is that their last use time can get
erroneously reset when you look up a different key with the same hash code.


> Although ideally, with the addition of ephemerons, the key would be soft,
> and the value would be a dependant reference.


With ephemerons, you'd still have a strong key and soft value but a strong
reference from the key to the value wouldn't prevent the value from getting
reclaimed.

I can't recall if we've ever used soft keys; it may make sense to not
support them at all.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080502/0e28e4ed/attachment.html 

From jason.greene at redhat.com  Sat May  3 23:05:13 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Sat, 03 May 2008 22:05:13 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>	
	<48075F96.2050604@redhat.com>	
	<a74683f90804170906p7a55df0ck47deb44cdfc2cbe0@mail.gmail.com>	
	<480796BF.1020606@sun.com>	
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>	
	<481BB4B1.1040701@redhat.com>	
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>	
	<481BBE19.401@redhat.com>	
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>	
	<481C018F.7090504@redhat.com>
	<a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>
Message-ID: <481D27E9.2000902@redhat.com>

Bob Lee wrote:
> On Fri, May 2, 2008 at 11:09 PM, Jason T. Greene 
> <jason.greene at redhat.com <mailto:jason.greene at redhat.com>> wrote:
> 
>     Ah, yes of course. I stand corrected. Well, in any case it might be
>     desired for both the key and the value to be soft, to allow for
>     faster reclamation.
> 
> 
> Soft keys and soft values would create more (undesirable) work for the 
> garbage collector, but I don't see how it would speed up reclamation. 

Reclamation is faster because the collector can clear the reference 
without waiting on the reference queue and table lock.

> Another problem with soft keys is that their last use time can get 
> erroneously reset when you look up a different key with the same hash code.

Sure this is a problem for all reference types. Having identical 
hashcodes for different keys is bad in many ways.

> 
>     Although ideally, with the addition of ephemerons, the key would be
>     soft, and the value would be a dependant reference.
> 
> 
> With ephemerons, you'd still have a strong key and soft value but a 
> strong reference from the key to the value wouldn't prevent the value 
> from getting reclaimed.

Thats just one aspect. With an ephemeron, the dependent value is 
collectable as soon as its corresponding key has been reclaimed. So this 
gives us the desired logical outcome of the entire entry being purged 
before the table has to clean it (by entry i mean the key-value 
combination, the actual defunct table entry object (small) would have to 
be purged by the table) .


-- 
Jason T. Greene
JBoss, a division of Red Hat

From crazybob at crazybob.org  Sat May  3 23:11:41 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Sat, 3 May 2008 20:11:41 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481D27E9.2000902@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
	<481BBE19.401@redhat.com>
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
	<481C018F.7090504@redhat.com>
	<a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>
	<481D27E9.2000902@redhat.com>
Message-ID: <a74683f90805032011h683da92ao3b4b20c8208cb98b@mail.gmail.com>

On Sat, May 3, 2008 at 8:05 PM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> Another problem with soft keys is that their last use time can get
>> erroneously reset when you look up a different key with the same hash code.
>>
>
> Sure this is a problem for all reference types. Having identical hashcodes
> for different keys is bad in many ways.


I meant to say "a different key in the same bucket."

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080503/2ce8ed16/attachment.html 

From gregg at cytetech.com  Sun May  4 22:52:16 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sun, 04 May 2008 21:52:16 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEICHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCOEICHLAA.dcholmes@optusnet.com.au>
Message-ID: <481E7660.4050601@cytetech.com>

David Holmes wrote:
> Thank you for the annual "security is broken in TPE" reminder :-) There's a
> long history here that started with J2EE security issues and was convoluted
> by the fact that the default security manager implementation actually
> relaxed security rather than strengthening it - ie it doesn't implement the
> installed security policy. We were in a no-win situation and originally used
> a direct AccessController.checkPermission call. That upset some folks and it
> was later changed to a SecurityManager.checkPermission call - which seemed
> to address main concerns.

I still don't understand why you did anything to assert authorization 
requirements.  Calls to Thread.interrupt(), Thread.stop(), or manipulation of 
the ThreadGroup or Thread objects is still always controlled, explicitly by the 
SecurityManager, in some way, where appropriate.  If I have a reference to a 
TPE, why can't I call shutdown on that reference and it work?

> The consequence of this is that you have the perverse situation where you
> can create a TPE (and its threads) but not shut it down. If anyone can see a
> resolution to this conundrum we'd be happy to hear it - though simply
> disabling any security check is probably not an option (though maybe the
> whole J2EE issue is moot now?)

I don't pretend to understand the J2EE issue.  I don't use J2EE, at all.  But, 
I'd venture to say that a platform class which uses existing APIs should in no 
way enforce any authorization which does not represent a new resource that needs 
different protection schemes than what the platform (and there are several 
related to the type of security manager used) provides.

> BTW applets thread should be required to have certain permissions for
> "modifying" threads outside the applet thread group. Eg you shouldn't be
> able to find all threads in the system an interrupt them - but it's been
> years since I checked that :)

Yes, and that's all the protections that should be enforced.

I'm still struggling to see what use case possibly drives this.  I just can't 
think of a reason that I might have a reference to a TPE and it not be right for 
me to be able to shut it down.

If I can get into the execution stream, I can throw RuntimeException to stop 
threads etc.  Keeping that kind of thing from happening is just part of 
designing software that uses classloaders to extend the application in ways that 
would allow those other classes to be malicious to your application.

Gregg Wonderly.

From dcholmes at optusnet.com.au  Mon May  5 03:18:00 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 5 May 2008 17:18:00 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
	shutdownpermission check.
In-Reply-To: <481E7660.4050601@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>

Gregg,

The use case is that you've been given a reference to an executor that you
are allowed to submit tasks to, but that doesn't give you the right to shut
it down (or arguably to control it in any way). I believe that was the J2EE
scenario.

The problem we had was that if the security policy said that you did not
have permission to "modifyThread", the default SecurityManager
implementation doesn't adhere to that policy - it ignores it unless the
thread is in the root threadgroup - and that's plain wrong. Hence the
explicit permission check.

The thread related permissions are simply far too coarse-grained to provide
any level of fine-grained access control. The SecurityManager should have
provided ThreadGroup based access control but it didn't - and even that
isn't enough for Executor management. So we were left with either no
security, or more security that we really wanted. And the latter won because
at the time security was something that was wanted.

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Monday, 5 May 2008 12:52 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest; gregg.wonderly at pobox.com
> Subject: Re: [concurrency-interest] ScheduledThreadPoolExecutor and
> shutdownpermission check.
>
>
> David Holmes wrote:
> > Thank you for the annual "security is broken in TPE" reminder
> :-) There's a
> > long history here that started with J2EE security issues and
> was convoluted
> > by the fact that the default security manager implementation actually
> > relaxed security rather than strengthening it - ie it doesn't
> implement the
> > installed security policy. We were in a no-win situation and
> originally used
> > a direct AccessController.checkPermission call. That upset some
> folks and it
> > was later changed to a SecurityManager.checkPermission call -
> which seemed
> > to address main concerns.
>
> I still don't understand why you did anything to assert authorization
> requirements.  Calls to Thread.interrupt(), Thread.stop(), or
> manipulation of
> the ThreadGroup or Thread objects is still always controlled,
> explicitly by the
> SecurityManager, in some way, where appropriate.  If I have a
> reference to a
> TPE, why can't I call shutdown on that reference and it work?
>
> > The consequence of this is that you have the perverse situation
> where you
> > can create a TPE (and its threads) but not shut it down. If
> anyone can see a
> > resolution to this conundrum we'd be happy to hear it - though simply
> > disabling any security check is probably not an option (though maybe the
> > whole J2EE issue is moot now?)
>
> I don't pretend to understand the J2EE issue.  I don't use J2EE,
> at all.  But,
> I'd venture to say that a platform class which uses existing APIs
> should in no
> way enforce any authorization which does not represent a new
> resource that needs
> different protection schemes than what the platform (and there
> are several
> related to the type of security manager used) provides.
>
> > BTW applets thread should be required to have certain permissions for
> > "modifying" threads outside the applet thread group. Eg you shouldn't be
> > able to find all threads in the system an interrupt them - but it's been
> > years since I checked that :)
>
> Yes, and that's all the protections that should be enforced.
>
> I'm still struggling to see what use case possibly drives this.
> I just can't
> think of a reason that I might have a reference to a TPE and it
> not be right for
> me to be able to shut it down.
>
> If I can get into the execution stream, I can throw
> RuntimeException to stop
> threads etc.  Keeping that kind of thing from happening is just part of
> designing software that uses classloaders to extend the
> application in ways that
> would allow those other classes to be malicious to your application.
>
> Gregg Wonderly.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From neal at gafter.com  Mon May  5 10:25:24 2008
From: neal at gafter.com (Neal Gafter)
Date: Mon, 5 May 2008 07:25:24 -0700
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
Message-ID: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>

Cay Horstmann noted at

http://weblogs.java.net/blog/cayhorstmann/archive/2008/05/on_bluecollar_l.html

an incompatible change between SE 5 and SE 6 in the executors framework,
described in the bug report

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6267833

Funny thing is, the new signature is not quite correct either.

In SE 5 it was

*<T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)*

In SE 6 it was

*<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)*

The correct signature should be

*<T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends T>>
tasks)*

This lets me pass in a *Collection* that contains instances of
implementations of both *Callable<String>* and *Callable<Integer>*, and *
invokeAll* returns a *List<Future<Object>>*.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080505/593da410/attachment.html 

From crazybob at crazybob.org  Mon May  5 11:19:52 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Mon, 5 May 2008 08:19:52 -0700
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
In-Reply-To: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
References: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
Message-ID: <a74683f90805050819y49e6962hc62fe0a509c808ea@mail.gmail.com>

For future reference, JDiff absolutely rocks:
http://javadiff.sourceforge.net/

Bob

On Mon, May 5, 2008 at 7:25 AM, Neal Gafter <neal at gafter.com> wrote:

> Cay Horstmann noted at
>
>
> http://weblogs.java.net/blog/cayhorstmann/archive/2008/05/on_bluecollar_l.html
>
> an incompatible change between SE 5 and SE 6 in the executors framework,
> described in the bug report
>
> http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6267833
>
> Funny thing is, the new signature is not quite correct either.
>
> In SE 5 it was
>
> *<T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)*
>
> In SE 6 it was
>
> *<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)*
>
> The correct signature should be
>
> *<T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends T>>
> tasks)*
>
> This lets me pass in a *Collection* that contains instances of
> implementations of both *Callable<String>* and *Callable<Integer>*, and *
> invokeAll* returns a *List<Future<Object>>*.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080505/279148b9/attachment.html 

From Thomas.Hawtin at Sun.COM  Mon May  5 11:56:43 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Mon, 05 May 2008 16:56:43 +0100
Subject: [concurrency-interest] ScheduledThreadPoolExecutor
	and	shutdownpermission check.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
Message-ID: <481F2E3B.4080208@sun.com>

David Holmes wrote:
> 
> The use case is that you've been given a reference to an executor that you
> are allowed to submit tasks to, but that doesn't give you the right to shut
> it down (or arguably to control it in any way). I believe that was the J2EE
> scenario.

Particularly in the case of ThreadPoolExecutor, you can do all sorts, 
such as purge and getQueue.

> The problem we had was that if the security policy said that you did not
> have permission to "modifyThread", the default SecurityManager
> implementation doesn't adhere to that policy - it ignores it unless the
> thread is in the root threadgroup - and that's plain wrong. Hence the
> explicit permission check.

If you want to give code access to a Thread instance but not allow 
modification, then you should change the checkAccess policy in your 
SecurityManager.

Tom Hawtin

From jason.greene at redhat.com  Mon May  5 12:01:52 2008
From: jason.greene at redhat.com (Jason T. Greene)
Date: Mon, 05 May 2008 11:01:52 -0500
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
 166 - Feedback Requested
In-Reply-To: <a74683f90805032011h683da92ao3b4b20c8208cb98b@mail.gmail.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>	
	<480796BF.1020606@sun.com>	
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>	
	<481BB4B1.1040701@redhat.com>	
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>	
	<481BBE19.401@redhat.com>	
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>	
	<481C018F.7090504@redhat.com>	
	<a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>	
	<481D27E9.2000902@redhat.com>
	<a74683f90805032011h683da92ao3b4b20c8208cb98b@mail.gmail.com>
Message-ID: <481F2F70.4020501@redhat.com>

Bob Lee wrote:
> On Sat, May 3, 2008 at 8:05 PM, Jason T. Greene <jason.greene at redhat.com 
> <mailto:jason.greene at redhat.com>> wrote:
> 
>         Another problem with soft keys is that their last use time can
>         get erroneously reset when you look up a different key with the
>         same hash code.
> 
> 
>     Sure this is a problem for all reference types. Having identical
>     hashcodes for different keys is bad in many ways.
> 
> 
> I meant to say "a different key in the same bucket."

This shouldn't be a problem with any CHM base implementation since it 
stores a copy of the hashcode on the entry object. This is checked 
before accessing the key, so it should short circuit before touching the 
ref.

-- 
Jason T. Greene
JBoss, a division of Red Hat

From crazybob at crazybob.org  Mon May  5 12:06:21 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Mon, 5 May 2008 09:06:21 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481F2F70.4020501@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
	<481BBE19.401@redhat.com>
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
	<481C018F.7090504@redhat.com>
	<a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>
	<481D27E9.2000902@redhat.com>
	<a74683f90805032011h683da92ao3b4b20c8208cb98b@mail.gmail.com>
	<481F2F70.4020501@redhat.com>
Message-ID: <a74683f90805050906r999b451i72c99f067928e958@mail.gmail.com>

On Mon, May 5, 2008 at 9:01 AM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> This shouldn't be a problem with any CHM base implementation since it
> stores a copy of the hashcode on the entry object. This is checked before
> accessing the key, so it should short circuit before touching the ref.
>

Oops. Had it right the first time.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080505/6c849f9d/attachment.html 

From crazybob at crazybob.org  Mon May  5 12:39:02 2008
From: crazybob at crazybob.org (Bob Lee)
Date: Mon, 5 May 2008 09:39:02 -0700
Subject: [concurrency-interest] ConcurrentReferenceMap enhancement to
	166 - Feedback Requested
In-Reply-To: <481D27E9.2000902@redhat.com>
References: <BLU134-W31F80D3ABE77F37AD7BC8383EA0@phx.gbl>
	<480796BF.1020606@sun.com>
	<a74683f90805021644xf894c68k5e1184a29975453a@mail.gmail.com>
	<481BB4B1.1040701@redhat.com>
	<a74683f90805021747g6b2f7011xf7bd82a0ef035cef@mail.gmail.com>
	<481BBE19.401@redhat.com>
	<a74683f90805021831w75d3545do1d40d36cfd9332e3@mail.gmail.com>
	<481C018F.7090504@redhat.com>
	<a74683f90805022319q621ac279hd67a7d76be847a03@mail.gmail.com>
	<481D27E9.2000902@redhat.com>
Message-ID: <a74683f90805050939w76ddb840oce3dfe893f8298e7@mail.gmail.com>

On Sat, May 3, 2008 at 8:05 PM, Jason T. Greene <jason.greene at redhat.com>
wrote:

> Reclamation is faster because the collector can clear the reference without
> waiting on the reference queue and table lock.


This is an interesting point, and I've been thinking about it. First, you
wouldn't need to touch a reference queue. We trust the ReferenceMap code, so
we can clear entries directly. You still have the segment locking, but if
this becomes an issue, wouldn't that mean CHM isn't quite concurrent enough?

In other words, if the the value is cleared during a collection but the key
is not, the entry cleanup will actually happen sooner than clearing the key
ref. Either way, the entry cleanup happens plenty soon enough.

Bob
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080505/8b6ef084/attachment.html 

From dcholmes at optusnet.com.au  Mon May  5 13:42:57 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 06 May 2008 03:42:57 +1000
Subject: [concurrency-interest] ScheduledThreadPoolExecutorand
 shutdownpermission check.
In-Reply-To: <481F2E3B.4080208@sun.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIJHLAA.dcholmes@optusnet.com.au>

Tom Hawtin wrote:
> David Holmes wrote:
> > The problem we had was that if the security policy said that you did not
> > have permission to "modifyThread", the default SecurityManager
> > implementation doesn't adhere to that policy - it ignores it unless the
> > thread is in the root threadgroup - and that's plain wrong. Hence the
> > explicit permission check.
>
> If you want to give code access to a Thread instance but not allow
> modification, then you should change the checkAccess policy in your
> SecurityManager.

I don't want to start debating the pros/cons of the current security
architecture, but if you've installed the desired security policy files then
you shouldn't have to define a custom SecurityManager as well to get that
policy enforced. The security policy can be defined by a system adminstrator
who has no control over the actual execution of the JVM and so can't force
the use of any particular security manager at runtime.

At a minimum the default SecurityManger should have used ThreadGroups to
determine access. For a long time (maybe its even still there) the
ThreadGroup docs would talk about not being allowed to do things unless in
the same ThreadGroup - but in practice that depended on the security manager
and the default one never enforced such a policy.

Anyway, long bridge and lots of water ... if there's a way to fix this to
the satifaction of all then we're open to suggestions.

Cheers,
David Holmes


From Thomas.Hawtin at Sun.COM  Mon May  5 14:04:38 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Mon, 05 May 2008 19:04:38 +0100
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
In-Reply-To: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
References: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
Message-ID: <481F4C36.7010904@sun.com>

Neal Gafter wrote:
> 
> *<T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)*
> 
> In SE 6 it was
> 
> *<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)*
> 
> The correct signature should be
> 
> *<T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends 
> T>> tasks)*

There are lots of places where something like that could be done. For 
instance in CompletionService<V>:

   Future<V> submit(Callable<? extends V> task);

How does one determine whether inserting a wildcard is worth it?

Tom Hawtin

From neal at gafter.com  Mon May  5 14:15:13 2008
From: neal at gafter.com (Neal Gafter)
Date: Mon, 5 May 2008 11:15:13 -0700
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
In-Reply-To: <481F4C36.7010904@sun.com>
References: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
	<481F4C36.7010904@sun.com>
Message-ID: <15e8b9d20805051115m363fb6e9va7eca9b0f26a1614@mail.gmail.com>

In an argument position it is almost always worth it.

On Mon, May 5, 2008 at 11:04 AM, Tom Hawtin <Thomas.Hawtin at sun.com> wrote:

> Neal Gafter wrote:
>
> >
> > *<T> List<Future<T>> invokeAll(Collection<Callable<T>> tasks)*
> >
> > In SE 6 it was
> >
> > *<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks)*
> >
> > The correct signature should be
> >
> > *<T> List<Future<T>> invokeAll(Collection<? extends Callable<? extends
> > T>> tasks)*
> >
>
> There are lots of places where something like that could be done. For
> instance in CompletionService<V>:
>
>  Future<V> submit(Callable<? extends V> task);
>
> How does one determine whether inserting a wildcard is worth it?
>
> Tom Hawtin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080505/a440b1ba/attachment.html 

From gregg at cytetech.com  Mon May  5 14:36:49 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 05 May 2008 13:36:49 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
Message-ID: <481F53C1.2030300@cytetech.com>

David Holmes wrote:
> Gregg,
> 
> The use case is that you've been given a reference to an executor that you
> are allowed to submit tasks to, but that doesn't give you the right to shut
> it down (or arguably to control it in any way). I believe that was the J2EE
> scenario.

That seems to indicate to me that the J2EE use of the API was not correct for 
it's intended users and that there should be a submission point that is not the 
TPE, so that the sync delegates to a TPE and thus does not expose the TPE object.

> The problem we had was that if the security policy said that you did not
> have permission to "modifyThread", the default SecurityManager
> implementation doesn't adhere to that policy - it ignores it unless the
> thread is in the root threadgroup - and that's plain wrong. Hence the
> explicit permission check.
> 
> The thread related permissions are simply far too coarse-grained to provide
> any level of fine-grained access control. The SecurityManager should have
> provided ThreadGroup based access control but it didn't - and even that
> isn't enough for Executor management. So we were left with either no
> security, or more security that we really wanted. And the latter won because
> at the time security was something that was wanted.

And that is where a security control API object would come into play it seems to 
me.  The security control object, was selected to be the TPE class, and my 
complaint is that doing that, created barriers for the other Java platforms by 
demanding a specific security model, which is not applicable to all.

J2EE should have a context object somewhere already, where an appropriate design 
could be achieved where the platforms use of thread control could be implemented 
and TPEs could be used without exposing the reference to them directly.  It 
might just be that J2EE should just have a TPE delegate that does the existing 
permission checks using a base TPE that has no security implementation.

Gregg Wonderly

From dl at cs.oswego.edu  Mon May  5 14:39:31 2008
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 05 May 2008 14:39:31 -0400
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
In-Reply-To: <481F4C36.7010904@sun.com>
References: <15e8b9d20805050725t2141e060xe93df3652a430276@mail.gmail.com>
	<481F4C36.7010904@sun.com>
Message-ID: <481F5463.5060003@cs.oswego.edu>

Tom Hawtin wrote:

> There are lots of places where something like that could be done. For 
> instance in CompletionService<V>:
> 
>    Future<V> submit(Callable<? extends V> task);
> 
> How does one determine whether inserting a wildcard is worth it?
> 

Cay's cited point by Dmitriy Setrakyan is a good start:

   "Why in the world do I need to specify <? extends SomeInterface>
   for generics? What else am I going to do with an interface other than
   *extend* it. Well... I guess I can stare at it, but I don't think it counts."

Which when you say it like that, makes it easier to obey the rule in anger.

(We're sorry of course about the signature bug. And about
the signature fix. Hard to win here...)

-Doug

From gregg at cytetech.com  Mon May  5 15:10:15 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 05 May 2008 14:10:15 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <481F2E3B.4080208@sun.com>
References: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
	<481F2E3B.4080208@sun.com>
Message-ID: <481F5B97.2000905@cytetech.com>

Tom Hawtin wrote:
> David Holmes wrote:
>> The problem we had was that if the security policy said that you did not
>> have permission to "modifyThread", the default SecurityManager
>> implementation doesn't adhere to that policy - it ignores it unless the
>> thread is in the root threadgroup - and that's plain wrong. Hence the
>> explicit permission check.
> 
> If you want to give code access to a Thread instance but not allow 
> modification, then you should change the checkAccess policy in your 
> SecurityManager.

This is not a realistic position because users of libraries or other packaged 
Java code, are free to assert whatever security they want the SecurityManager to 
enforce, from the command line, at application startup (applets too if you sign 
the jar).  Netbeans, for example, asserts a SecurityManager to make RMI work, 
but which actually provides no security.  Libraries should really not assert any 
particular security model.  Instead, there should be delegate classes that 
provide a security model and do not expose the underlying objects for exploit.

Gregg Wonderly

From Thomas.Hawtin at Sun.COM  Mon May  5 16:31:29 2008
From: Thomas.Hawtin at Sun.COM (Tom Hawtin)
Date: Mon, 05 May 2008 21:31:29 +0100
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <481F5B97.2000905@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
	<481F2E3B.4080208@sun.com> <481F5B97.2000905@cytetech.com>
Message-ID: <481F6EA1.1070603@sun.com>

Gregg Wonderly wrote:
> Tom Hawtin wrote:
>> If you want to give code access to a Thread instance but not allow 
>> modification, then you should change the checkAccess policy in your 
>> SecurityManager.
> 
> This is not a realistic position because users of libraries or other packaged 
> Java code, are free to assert whatever security they want the SecurityManager to 
> enforce, from the command line, at application startup (applets too if you sign 
> the jar).  Netbeans, for example, asserts a SecurityManager to make RMI work, 
> but which actually provides no security.  Libraries should really not assert any 
> particular security model.  Instead, there should be delegate classes that 
> provide a security model and do not expose the underlying objects for exploit.

Such libraries should not share sensitive objects[1], between untrusted 
contexts. If you share ThreadPoolExecutors (or even just threads), then 
contexts can, to some extent, interfere with one another.

Tom Hawtin

[1] In the case of Threads, this include those reachable the 
Thread.currentThread or similar.

From gregg at cytetech.com  Tue May  6 00:10:16 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 05 May 2008 23:10:16 -0500
Subject: [concurrency-interest] ScheduledThreadPoolExecutor and
 shutdownpermission check.
In-Reply-To: <481F6EA1.1070603@sun.com>
References: <NFBBKALFDCPFIDBNKAPCKEIGHLAA.dcholmes@optusnet.com.au>
	<481F2E3B.4080208@sun.com> <481F5B97.2000905@cytetech.com>
	<481F6EA1.1070603@sun.com>
Message-ID: <481FDA28.2060601@cytetech.com>

Tom Hawtin wrote:
> Gregg Wonderly wrote:
>> Tom Hawtin wrote:
>>> If you want to give code access to a Thread instance but not allow 
>>> modification, then you should change the checkAccess policy in your 
>>> SecurityManager.
>>
>> This is not a realistic position because users of libraries or other 
>> packaged Java code, are free to assert whatever security they want the 
>> SecurityManager to enforce, from the command line, at application 
>> startup (applets too if you sign the jar).  Netbeans, for example, 
>> asserts a SecurityManager to make RMI work, but which actually 
>> provides no security.  Libraries should really not assert any 
>> particular security model.  Instead, there should be delegate classes 
>> that provide a security model and do not expose the underlying objects 
>> for exploit.
> 
> Such libraries should not share sensitive objects[1], between untrusted 
> contexts. If you share ThreadPoolExecutors (or even just threads), then 
> contexts can, to some extent, interfere with one another.
> 
> Tom Hawtin
> 
> [1] In the case of Threads, this include those reachable the 
> Thread.currentThread or similar.

And this is exactly my position.  TPE should not be exposed if all of its 
methods are not intended to be used.  It doesn't provide anything, any different 
from what I can "write" to manage threads myself, literally, so there is really 
no need to "add" an authorization layer, as a library class.  If I want to have 
security around a TPE, I, as the application designer should be in charge of 
making that decision.

The Thread class already controls how threads are created and destroyed for each 
  platform type.  What each platform does, is already the right security, 
because it's what has been defined for Threads, since the start.  TPE just 
provides some convenience for how Threads are reused and managed.  So again, my 
feeling is that any code that has a reference to a TPE should not be presented 
with any more authorization requirements than what Thread and the 
SecurityManager already dictate for the platform that TPE is used on.

Gregg Wonderly

From kasper at kav.dk  Wed May 14 18:10:46 2008
From: kasper at kav.dk (Kasper Nielsen)
Date: Thu, 15 May 2008 00:10:46 +0200
Subject: [concurrency-interest] High resolution timers and JSR 310
Message-ID: <482B6366.9010306@kav.dk>

Hi,

There is some ongoing talk about adding a high resolution timer 
System.currentTimeNanos() over at JSR 310. Which acts just like 
System.currentTimeMillis() but with nanosecond resolution.
(https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)

In my opinion this is practically impossible to implement correctly 
without proper os/hardware support. For example, using combinations of 
QueryPerformanceCounter and GetSystemTimeAsFileTime on Windows is just a 
disaster waiting to happen.

Maybe someone on this list has some input.

- Kasper


From osvaldo at visionnaire.com.br  Sun May 18 10:25:17 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Sun, 18 May 2008 11:25:17 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <482B6366.9010306@kav.dk>
References: <482B6366.9010306@kav.dk>
Message-ID: <48303C4D.70507@visionnaire.com.br>

Hi,

I understand the problem, but I wonder if an alternative solution would 
be feasible, like a millisecond-precision timing that's actually 
millisecond-accurate (instead of the current APIs currentTimeMillis(), 
new Date() etc., that update values only each ~16ms, at least in the 
implementations where I tested this). Right now I am forced to write a 
hacked utility class that emulates System.currentTimeMillis() (including 
its epoch - so the value can be used to create dates), but relying on 
currentTimeNanos(), and I patch libraries like Log4J because my 
application logs ABSOLUTELY require 1ms-accurate timestamps in the logs. 
16ms is not good enough in modern multi-GHz / multicore systems, when I 
want to collect performance data from logs and even a simple query in 
Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of 
application operations executing in the same timestamp; but that's good 
enough (for the time being) as any operation taking <1ms is fast enough 
so I don't care.

Perhaps we could reimplement currentTimeMillis() with the same JVM 
tricks used by the nano timer (intrinsification), so we can get at least 
1ms accuracy, without the risks imposed by additional techniques 
required for nanosecond timers.

A+
Osvaldo
> Hi,
>
> There is some ongoing talk about adding a high resolution timer
> System.currentTimeNanos() over at JSR 310. Which acts just like
> System.currentTimeMillis() but with nanosecond resolution.
> (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
>
> In my opinion this is practically impossible to implement correctly
> without proper os/hardware support. For example, using combinations of
> QueryPerformanceCounter and GetSystemTimeAsFileTime on Windows is just a
> disaster waiting to happen.
>
> Maybe someone on this list has some input.
>
> - Kasper
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>    


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223


From dcholmes at optusnet.com.au  Sun May 18 18:04:03 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 19 May 2008 08:04:03 +1000
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48303C4D.70507@visionnaire.com.br>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>

Osvaldo,

As Kasper alluded the problem is that the OS doesn't necessarily give you a
high-precision time source that is tied to "wall clock time". Hence on
Windows the update resolution is typically 10ms (15ms for some versions), on
Solaris/Linux it is also typically 10ms but can be 1ms depending on system
configuration.

nanoTime doesn't use JVM tricks to get better resolution, it uses a
completely different time-source to currentTimeMillis - such as TSC
(rarely), HPET or ACPI Power Management timer - exposed via some specific OS
API such as QueryPerformanceCounter/QueryPerformanceFrequency on Windows;
gethrtime() on Solaris and clock_gettime(CLOCK_MONOTONIC) on linux. These
higher resolution time sources are not tied to wall-clock time.

See my blog for some details on Windows:

http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks

Trying to get a high resolution time source that is tied to changes in
wall-clock time will be a real problem to implement as you would have to
effectively synchronize the two different time sources in some way. This
would be far from trivial.

What you can do is to define the high resolution clock to be based on the
epoch, but not require that subsequent changes in system time be reflected
in the value. The real-time spec for Java does this for its real-time clock
source. For this all you have to do is determine the system time at VM
startup and then return a (base + offset) value. Would that suffice or must
ntp adjustments and DST changes also be reflected?

David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
> Pinali Doederlein
> Sent: Monday, 19 May 2008 12:25 AM
> To: Kasper Nielsen
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>
>
> Hi,
>
> I understand the problem, but I wonder if an alternative solution would
> be feasible, like a millisecond-precision timing that's actually
> millisecond-accurate (instead of the current APIs currentTimeMillis(),
> new Date() etc., that update values only each ~16ms, at least in the
> implementations where I tested this). Right now I am forced to write a
> hacked utility class that emulates System.currentTimeMillis() (including
> its epoch - so the value can be used to create dates), but relying on
> currentTimeNanos(), and I patch libraries like Log4J because my
> application logs ABSOLUTELY require 1ms-accurate timestamps in the logs.
> 16ms is not good enough in modern multi-GHz / multicore systems, when I
> want to collect performance data from logs and even a simple query in
> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
> application operations executing in the same timestamp; but that's good
> enough (for the time being) as any operation taking <1ms is fast enough
> so I don't care.
>
> Perhaps we could reimplement currentTimeMillis() with the same JVM
> tricks used by the nano timer (intrinsification), so we can get at least
> 1ms accuracy, without the risks imposed by additional techniques
> required for nanosecond timers.
>
> A+
> Osvaldo
> > Hi,
> >
> > There is some ongoing talk about adding a high resolution timer
> > System.currentTimeNanos() over at JSR 310. Which acts just like
> > System.currentTimeMillis() but with nanosecond resolution.
> > (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
> >
> > In my opinion this is practically impossible to implement correctly
> > without proper os/hardware support. For example, using combinations of
> > QueryPerformanceCounter and GetSystemTimeAsFileTime on Windows is just a
> > disaster waiting to happen.
> >
> > Maybe someone on this list has some input.
> >
> > - Kasper
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
>
>
> --
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From mthornton at optrak.co.uk  Mon May 19 04:43:41 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Mon, 19 May 2008 09:43:41 +0100
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48303C4D.70507@visionnaire.com.br>
References: <482B6366.9010306@kav.dk> <48303C4D.70507@visionnaire.com.br>
Message-ID: <48313DBD.8070809@optrak.co.uk>

Osvaldo Pinali Doederlein wrote:
> Hi,
>
> I understand the problem, but I wonder if an alternative solution would 
> be feasible, like a millisecond-precision timing that's actually 
> millisecond-accurate (instead of the current APIs currentTimeMillis(), 
> new Date() etc., that update values only each ~16ms, at least in the 
> implementations where I tested this). Right now I am forced to write a 
> hacked utility class that emulates System.currentTimeMillis() (including 
> its epoch - so the value can be used to create dates), but relying on 
> currentTimeNanos(), and I patch libraries like Log4J because my 
> application logs ABSOLUTELY require 1ms-accurate timestamps in the logs. 
> 16ms is not good enough in modern multi-GHz / multicore systems, when I 
> want to collect performance data from logs and even a simple query in 
> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of 
>   
Are the logs all on the same machine or are you trying to reconcile logs 
from several machines (and thus also have to accurately synchronize the 
clocks)? As an aside, how accurately are system clocks synchronized in 
clusters?

Mark Thornton

From osvaldo at visionnaire.com.br  Mon May 19 10:01:54 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Mon, 19 May 2008 11:01:54 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
Message-ID: <48318852.7070800@visionnaire.com.br>

Hi David,

Thanks for the clarifications, I wasn't aware of the article in your 
blog. I knew that the nano timer used other techniques as well, like 
different OS calls, but I didn't have these details. Anyway, I remember 
reading somewhere, long ago, that HotSpot also uses intrinsification of 
currentTimeNanos() (and I think the presence of this call even forces a 
method to be compiled eagerly?) because the overheads of JNI etc. would 
otherwise spoil its precision. But even if this is true, it's just a 
minor detail. I didn't know that typical OSes are so limited in their 
timing-related APIs; my days of C/C++/Win32 are long gone (for good!) 
and I'd think that modern OSes, which are packed with advanced features 
requiring precise timing (multimedia, advanced networking etc.), would 
offer better public APIs...

Just for the record, my hack is like this:

public class JDKUtil {
     private static long milliOffset;
     private static long nanoOffset;
     public static Date newDate () {
         return new Date(currentTimeMillis());
     }
     public static Calendar newCalendar () {
         final Calendar cal = Calendar.getInstance();
         cal.setTime(newDate());
         return cal;
     }
     public static long currentTimeMillis () {
         return milliOffset + System.nanoTime() / 1000000;
     }
     protected static void calibrateTimer () {
         long millis = 0, nanos = 0;
         for (int attempts = 0; attempts < 10; ++attempts) {
             millis = System.currentTimeMillis();
             nanos = System.nanoTime();
             final long millis2 = System.currentTimeMillis();
             final long nanos2 = System.nanoTime();
             if (millis == millis2 && nanos == nanos2)
                 break;
         }
         milliOffset = millis - nanos / 1000000;
         nanoOffset = milliOffset * 1000000;
     }
     static { calibrateTimer(); }
}

The trick is in the calibrate method, which is executed at classinit 
time. I read both timers in sucession, and then I re-read them just to 
prevent the bad luck of reading the nano timer just after the 
millisecond timer bumped to a new value. This gives me some assurance 
that the calculated offset (the epoch delta for using the nano timer as 
a wall-clock timer) will be synchronized with the millisecond timer - at 
least, as much as reasonably possible with pure-Java code.

I know that the millisecond timer can be affected by events like DST 
changes, but I don't care, because I use this utility exclusively for 
logging and ad-hoc benchmarking instrumentation. I keep using standard 
APIs like new Date() for any other need, especially business-related 
operations. I could make the code above a litte better, e.g. arranging 
for the calibration operation to be repeated every few hours, or having 
a more complex calibration algorithm that attempts to detect the exact 
moment when the millisecond timer changes and then calculates the 
offset. But I know that this complexity wouldn't pay off, as my 
wallclock timestamps would always be affected by the accuracy of the 
baseline offset, taken from the millisecond timer. My purpose was just 
having a /relative/ accuracy of 1ns between several timestamps, obtainer 
either sequentially or concurrently (within a single machine) - yes, I 
use this in logs that are written to by concurrent threads (using 
Log4J's features like the async appender and NDC).

A+
Osvaldo
> Osvaldo,
>
> As Kasper alluded the problem is that the OS doesn't necessarily give you a
> high-precision time source that is tied to "wall clock time". Hence on
> Windows the update resolution is typically 10ms (15ms for some versions), on
> Solaris/Linux it is also typically 10ms but can be 1ms depending on system
> configuration.
>
> nanoTime doesn't use JVM tricks to get better resolution, it uses a
> completely different time-source to currentTimeMillis - such as TSC
> (rarely), HPET or ACPI Power Management timer - exposed via some specific OS
> API such as QueryPerformanceCounter/QueryPerformanceFrequency on Windows;
> gethrtime() on Solaris and clock_gettime(CLOCK_MONOTONIC) on linux. These
> higher resolution time sources are not tied to wall-clock time.
>
> See my blog for some details on Windows:
>
> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks
>
> Trying to get a high resolution time source that is tied to changes in
> wall-clock time will be a real problem to implement as you would have to
> effectively synchronize the two different time sources in some way. This
> would be far from trivial.
>
> What you can do is to define the high resolution clock to be based on the
> epoch, but not require that subsequent changes in system time be reflected
> in the value. The real-time spec for Java does this for its real-time clock
> source. For this all you have to do is determine the system time at VM
> startup and then return a (base + offset) value. Would that suffice or must
> ntp adjustments and DST changes also be reflected?
>
> David Holmes
>
>    
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
>> Pinali Doederlein
>> Sent: Monday, 19 May 2008 12:25 AM
>> To: Kasper Nielsen
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>
>>
>> Hi,
>>
>> I understand the problem, but I wonder if an alternative solution would
>> be feasible, like a millisecond-precision timing that's actually
>> millisecond-accurate (instead of the current APIs currentTimeMillis(),
>> new Date() etc., that update values only each ~16ms, at least in the
>> implementations where I tested this). Right now I am forced to write a
>> hacked utility class that emulates System.currentTimeMillis() (including
>> its epoch - so the value can be used to create dates), but relying on
>> currentTimeNanos(), and I patch libraries like Log4J because my
>> application logs ABSOLUTELY require 1ms-accurate timestamps in the logs.
>> 16ms is not good enough in modern multi-GHz / multicore systems, when I
>> want to collect performance data from logs and even a simple query in
>> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
>> application operations executing in the same timestamp; but that's good
>> enough (for the time being) as any operation taking<1ms is fast enough
>> so I don't care.
>>
>> Perhaps we could reimplement currentTimeMillis() with the same JVM
>> tricks used by the nano timer (intrinsification), so we can get at least
>> 1ms accuracy, without the risks imposed by additional techniques
>> required for nanosecond timers.
>>
>> A+
>> Osvaldo
>>      
>>> Hi,
>>>
>>> There is some ongoing talk about adding a high resolution timer
>>> System.currentTimeNanos() over at JSR 310. Which acts just like
>>> System.currentTimeMillis() but with nanosecond resolution.
>>> (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
>>>
>>> In my opinion this is practically impossible to implement correctly
>>> without proper os/hardware support. For example, using combinations of
>>> QueryPerformanceCounter and GetSystemTimeAsFileTime on Windows is just a
>>> disaster waiting to happen.
>>>
>>> Maybe someone on this list has some input.
>>>
>>> - Kasper
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>        
>> --
>> -----------------------------------------------------------------------
>> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>      
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>    


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080519/ba0e0c31/attachment-0001.html 

From osvaldo at visionnaire.com.br  Mon May 19 10:26:53 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Mon, 19 May 2008 11:26:53 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48313DBD.8070809@optrak.co.uk>
References: <482B6366.9010306@kav.dk> <48303C4D.70507@visionnaire.com.br>
	<48313DBD.8070809@optrak.co.uk>
Message-ID: <48318E2D.10403@visionnaire.com.br>

Hi Mark,
> Osvaldo Pinali Doederlein wrote:
>    
>> I understand the problem, but I wonder if an alternative solution would
>> be feasible, like a millisecond-precision timing that's actually
>> millisecond-accurate (instead of the current APIs currentTimeMillis(),
>> new Date() etc., that update values only each ~16ms, at least in the
>> implementations where I tested this). Right now I am forced to write a
>> hacked utility class that emulates System.currentTimeMillis() (including
>> its epoch - so the value can be used to create dates), but relying on
>> currentTimeNanos(), and I patch libraries like Log4J because my
>> application logs ABSOLUTELY require 1ms-accurate timestamps in the logs.
>> 16ms is not good enough in modern multi-GHz / multicore systems, when I
>> want to collect performance data from logs and even a simple query in
>> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
>>      
> Are the logs all on the same machine or are you trying to reconcile logs
> from several machines (and thus also have to accurately synchronize the
> clocks)? As an aside, how accurately are system clocks synchronized in
> clusters?
>    
I used this only for machine-specific configs. The application does run 
in a cluster (WebSphere, on partitions of IBM gear), but I set up each 
node to produce its own logs. Because each transacion is completely 
processed by a single node, I shouldn't suffer of difficulties to 
correlate different logs, but in practice my busineds transactions are 
composed of two or more system transactions, often separated by several 
seconds (ISO8583 banking messages...); add this to a load of a few 
dozens of transactions per second in each node, and the logs are already 
an insane mess even at INFO level. So, my 1ms-accurate timestamps don't 
help a lot to reconcile data; but they don't make that worse either. ;-)

With the hacked wallclock/nanosecond timer, I can use Log4J to produce a 
simple performance logs like this (in the transaction boundaries - MDBs):

2007-08-28 16:12:58,416;TEST;ORDER;TIME;LATENCY1;WAIT1;LATENCY2;WAIT2;MEMORY
2007-08-28 
16:12:58,440;0;0;22,790326;7,014858;2,013384;2,188267;1,423644;125747752
2007-08-28 
16:12:58,453;0;1;34,385864;15,796281;1,718095;2,195461;1,789403;126482096
2007-08-28 
16:12:58,460;0;2;40,76363;23,922803;1,903664;2,315658;1,338508;127001976
2007-08-28 
16:12:58,470;0;3;48,569022;32,525085;2,164939;2,236668;1,380342;127612512
2007-08-28 
16:12:58,478;0;4;53,362438;38,620202;1,806165;1,850026;0,89753;127875728
2007-08-28 
16:12:58,541;0;7;22,12013;5,800248;3,735251;2,169131;1,339276;129197152

Nanosecond precision is used not only in the timestamps, but also in 
latency and I/O-wait counters that the app also tracks. For the latter, 
notice how the individual times are very small, sometimes even below 1ms 
(which is the unit of all values from TIME to WAIT2). I can load this 
text file in a spreadsheet and produce some pretty useful and accurate 
performance analysis. All this without complex code, or management tools 
that can't be used in production at peak time (high cost and/or probe 
effect - no DTrace on AIX...).

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080519/f62013e2/attachment.html 

From jmanson at cs.umd.edu  Mon May 19 18:10:22 2008
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Mon, 19 May 2008 15:10:22 -0700
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
Message-ID: <4831FACE.7050203@cs.umd.edu>

When I tried to implement a high speed timer based on the cycle counter 
on Linux, I had some luck with reading the cycle counter every few 
seconds and coordinating that with the actual time (repeating the 
coordination until you were sure that it was accurate to within a given 
interval by checking the cycle counter both before and after the check 
of the current time).

Then, when people actually wanted the time, we could base it off of the 
cycle counter (base_time + cycles_since_base_time * 
number_of_cycles_per_unit_of_time).  It was lightning-fast, compared to 
currentTimeMillis and nanoTime on Linux.

The problem was that this wasn't high-precision (or even particularly 
accurate), because of clock drift and the fact that the cycle counter 
can be different on different processors.  Could something like this 
that could be done with clock_gettime() instead of the cycle counter?

					Jeremy


David Holmes wrote:
> Osvaldo,
> 
> As Kasper alluded the problem is that the OS doesn't necessarily give you a
> high-precision time source that is tied to "wall clock time". Hence on
> Windows the update resolution is typically 10ms (15ms for some versions), on
> Solaris/Linux it is also typically 10ms but can be 1ms depending on system
> configuration.
> 
> nanoTime doesn't use JVM tricks to get better resolution, it uses a
> completely different time-source to currentTimeMillis - such as TSC
> (rarely), HPET or ACPI Power Management timer - exposed via some specific OS
> API such as QueryPerformanceCounter/QueryPerformanceFrequency on Windows;
> gethrtime() on Solaris and clock_gettime(CLOCK_MONOTONIC) on linux. These
> higher resolution time sources are not tied to wall-clock time.
> 
> See my blog for some details on Windows:
> 
> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks
> 
> Trying to get a high resolution time source that is tied to changes in
> wall-clock time will be a real problem to implement as you would have to
> effectively synchronize the two different time sources in some way. This
> would be far from trivial.
> 
> What you can do is to define the high resolution clock to be based on the
> epoch, but not require that subsequent changes in system time be reflected
> in the value. The real-time spec for Java does this for its real-time clock
> source. For this all you have to do is determine the system time at VM
> startup and then return a (base + offset) value. Would that suffice or must
> ntp adjustments and DST changes also be reflected?
> 
> David Holmes
> 
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
>> Pinali Doederlein
>> Sent: Monday, 19 May 2008 12:25 AM
>> To: Kasper Nielsen
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>
>>
>> Hi,
>>
>> I understand the problem, but I wonder if an alternative solution would
>> be feasible, like a millisecond-precision timing that's actually
>> millisecond-accurate (instead of the current APIs currentTimeMillis(),
>> new Date() etc., that update values only each ~16ms, at least in the
>> implementations where I tested this). Right now I am forced to write a
>> hacked utility class that emulates System.currentTimeMillis() (including
>> its epoch - so the value can be used to create dates), but relying on
>> currentTimeNanos(), and I patch libraries like Log4J because my
>> application logs ABSOLUTELY require 1ms-accurate timestamps in the logs.
>> 16ms is not good enough in modern multi-GHz / multicore systems, when I
>> want to collect performance data from logs and even a simple query in
>> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
>> application operations executing in the same timestamp; but that's good
>> enough (for the time being) as any operation taking <1ms is fast enough
>> so I don't care.
>>
>> Perhaps we could reimplement currentTimeMillis() with the same JVM
>> tricks used by the nano timer (intrinsification), so we can get at least
>> 1ms accuracy, without the risks imposed by additional techniques
>> required for nanosecond timers.
>>
>> A+
>> Osvaldo
>>> Hi,
>>>
>>> There is some ongoing talk about adding a high resolution timer
>>> System.currentTimeNanos() over at JSR 310. Which acts just like
>>> System.currentTimeMillis() but with nanosecond resolution.
>>> (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
>>>
>>> In my opinion this is practically impossible to implement correctly
>>> without proper os/hardware support. For example, using combinations of
>>> QueryPerformanceCounter and GetSystemTimeAsFileTime on Windows is just a
>>> disaster waiting to happen.
>>>
>>> Maybe someone on this list has some input.
>>>
>>> - Kasper
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> --
>> -----------------------------------------------------------------------
>> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Mon May 19 19:07:08 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 20 May 2008 09:07:08 +1000
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <4831FACE.7050203@cs.umd.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>

Jeremy,

There are no doubt techniques for trying to synchronize two clock sources.
It becomes an issue of the cost involved and the accuracy achievable. I
don't claim to know what these techniques are nor how to characterise their
effectiveness.

I always come back to wondering why you need a high resolution value that is
tied to wall-clock time? A timestamp that simply needs to look similar to
wall-clock time does not need to be synchronized with wall-clock time after
startup. And a timestamp that is guaranteed monotonic increasing is far more
useful than one that is not.

Cheers,
David

> -----Original Message-----
> From: Jeremy Manson [mailto:jmanson at cs.umd.edu]
> Sent: Tuesday, 20 May 2008 8:10 AM
> To: dholmes at ieee.org
> Cc: Osvaldo Pinali Doederlein; Kasper Nielsen; concurrency-interest
> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>
>
> When I tried to implement a high speed timer based on the cycle counter
> on Linux, I had some luck with reading the cycle counter every few
> seconds and coordinating that with the actual time (repeating the
> coordination until you were sure that it was accurate to within a given
> interval by checking the cycle counter both before and after the check
> of the current time).
>
> Then, when people actually wanted the time, we could base it off of the
> cycle counter (base_time + cycles_since_base_time *
> number_of_cycles_per_unit_of_time).  It was lightning-fast, compared to
> currentTimeMillis and nanoTime on Linux.
>
> The problem was that this wasn't high-precision (or even particularly
> accurate), because of clock drift and the fact that the cycle counter
> can be different on different processors.  Could something like this
> that could be done with clock_gettime() instead of the cycle counter?
>
> 					Jeremy
>
>
> David Holmes wrote:
> > Osvaldo,
> >
> > As Kasper alluded the problem is that the OS doesn't
> necessarily give you a
> > high-precision time source that is tied to "wall clock time". Hence on
> > Windows the update resolution is typically 10ms (15ms for some
> versions), on
> > Solaris/Linux it is also typically 10ms but can be 1ms
> depending on system
> > configuration.
> >
> > nanoTime doesn't use JVM tricks to get better resolution, it uses a
> > completely different time-source to currentTimeMillis - such as TSC
> > (rarely), HPET or ACPI Power Management timer - exposed via
> some specific OS
> > API such as QueryPerformanceCounter/QueryPerformanceFrequency
> on Windows;
> > gethrtime() on Solaris and clock_gettime(CLOCK_MONOTONIC) on
> linux. These
> > higher resolution time sources are not tied to wall-clock time.
> >
> > See my blog for some details on Windows:
> >
> > http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks
> >
> > Trying to get a high resolution time source that is tied to changes in
> > wall-clock time will be a real problem to implement as you would have to
> > effectively synchronize the two different time sources in some way. This
> > would be far from trivial.
> >
> > What you can do is to define the high resolution clock to be
> based on the
> > epoch, but not require that subsequent changes in system time
> be reflected
> > in the value. The real-time spec for Java does this for its
> real-time clock
> > source. For this all you have to do is determine the system time at VM
> > startup and then return a (base + offset) value. Would that
> suffice or must
> > ntp adjustments and DST changes also be reflected?
> >
> > David Holmes
> >
> >> -----Original Message-----
> >> From: concurrency-interest-bounces at cs.oswego.edu
> >> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
> >> Pinali Doederlein
> >> Sent: Monday, 19 May 2008 12:25 AM
> >> To: Kasper Nielsen
> >> Cc: concurrency-interest
> >> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
> >>
> >>
> >> Hi,
> >>
> >> I understand the problem, but I wonder if an alternative solution would
> >> be feasible, like a millisecond-precision timing that's actually
> >> millisecond-accurate (instead of the current APIs currentTimeMillis(),
> >> new Date() etc., that update values only each ~16ms, at least in the
> >> implementations where I tested this). Right now I am forced to write a
> >> hacked utility class that emulates System.currentTimeMillis()
> (including
> >> its epoch - so the value can be used to create dates), but relying on
> >> currentTimeNanos(), and I patch libraries like Log4J because my
> >> application logs ABSOLUTELY require 1ms-accurate timestamps in
> the logs.
> >> 16ms is not good enough in modern multi-GHz / multicore systems, when I
> >> want to collect performance data from logs and even a simple query in
> >> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
> >> application operations executing in the same timestamp; but that's good
> >> enough (for the time being) as any operation taking <1ms is fast enough
> >> so I don't care.
> >>
> >> Perhaps we could reimplement currentTimeMillis() with the same JVM
> >> tricks used by the nano timer (intrinsification), so we can
> get at least
> >> 1ms accuracy, without the risks imposed by additional techniques
> >> required for nanosecond timers.
> >>
> >> A+
> >> Osvaldo
> >>> Hi,
> >>>
> >>> There is some ongoing talk about adding a high resolution timer
> >>> System.currentTimeNanos() over at JSR 310. Which acts just like
> >>> System.currentTimeMillis() but with nanosecond resolution.
> >>> (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
> >>>
> >>> In my opinion this is practically impossible to implement correctly
> >>> without proper os/hardware support. For example, using combinations of
> >>> QueryPerformanceCounter and GetSystemTimeAsFileTime on
> Windows is just a
> >>> disaster waiting to happen.
> >>>
> >>> Maybe someone on this list has some input.
> >>>
> >>> - Kasper
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at altair.cs.oswego.edu
> >>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >>
> >> --
> >> -----------------------------------------------------------------------
> >> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> >> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> >> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at altair.cs.oswego.edu
> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From jmanson at cs.umd.edu  Mon May 19 19:24:56 2008
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Mon, 19 May 2008 16:24:56 -0700
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
Message-ID: <48320C48.8000006@cs.umd.edu>

Well, in our case, we actually didn't want that; we just wanted 
something that was cheap to execute.  Both System.nanoTime() and 
System.currentTimeMillis() are CPU hogs on Linux, and this approach 
solved that problem.  I was thinking that a similar approach could be 
used to solve the other problem (I'm sure it would be much more 
expensive).  However, if it isn't worth solving the problem, then it 
sounds like it wouldn't be a very valuable use of anyone's time.

					Jeremy

David Holmes wrote:
> Jeremy,
> 
> There are no doubt techniques for trying to synchronize two clock sources.
> It becomes an issue of the cost involved and the accuracy achievable. I
> don't claim to know what these techniques are nor how to characterise their
> effectiveness.
> 
> I always come back to wondering why you need a high resolution value that is
> tied to wall-clock time? A timestamp that simply needs to look similar to
> wall-clock time does not need to be synchronized with wall-clock time after
> startup. And a timestamp that is guaranteed monotonic increasing is far more
> useful than one that is not.
> 
> Cheers,
> David
> 
>> -----Original Message-----
>> From: Jeremy Manson [mailto:jmanson at cs.umd.edu]
>> Sent: Tuesday, 20 May 2008 8:10 AM
>> To: dholmes at ieee.org
>> Cc: Osvaldo Pinali Doederlein; Kasper Nielsen; concurrency-interest
>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>
>>
>> When I tried to implement a high speed timer based on the cycle counter
>> on Linux, I had some luck with reading the cycle counter every few
>> seconds and coordinating that with the actual time (repeating the
>> coordination until you were sure that it was accurate to within a given
>> interval by checking the cycle counter both before and after the check
>> of the current time).
>>
>> Then, when people actually wanted the time, we could base it off of the
>> cycle counter (base_time + cycles_since_base_time *
>> number_of_cycles_per_unit_of_time).  It was lightning-fast, compared to
>> currentTimeMillis and nanoTime on Linux.
>>
>> The problem was that this wasn't high-precision (or even particularly
>> accurate), because of clock drift and the fact that the cycle counter
>> can be different on different processors.  Could something like this
>> that could be done with clock_gettime() instead of the cycle counter?
>>
>> 					Jeremy
>>
>>
>> David Holmes wrote:
>>> Osvaldo,
>>>
>>> As Kasper alluded the problem is that the OS doesn't
>> necessarily give you a
>>> high-precision time source that is tied to "wall clock time". Hence on
>>> Windows the update resolution is typically 10ms (15ms for some
>> versions), on
>>> Solaris/Linux it is also typically 10ms but can be 1ms
>> depending on system
>>> configuration.
>>>
>>> nanoTime doesn't use JVM tricks to get better resolution, it uses a
>>> completely different time-source to currentTimeMillis - such as TSC
>>> (rarely), HPET or ACPI Power Management timer - exposed via
>> some specific OS
>>> API such as QueryPerformanceCounter/QueryPerformanceFrequency
>> on Windows;
>>> gethrtime() on Solaris and clock_gettime(CLOCK_MONOTONIC) on
>> linux. These
>>> higher resolution time sources are not tied to wall-clock time.
>>>
>>> See my blog for some details on Windows:
>>>
>>> http://blogs.sun.com/dholmes/entry/inside_the_hotspot_vm_clocks
>>>
>>> Trying to get a high resolution time source that is tied to changes in
>>> wall-clock time will be a real problem to implement as you would have to
>>> effectively synchronize the two different time sources in some way. This
>>> would be far from trivial.
>>>
>>> What you can do is to define the high resolution clock to be
>> based on the
>>> epoch, but not require that subsequent changes in system time
>> be reflected
>>> in the value. The real-time spec for Java does this for its
>> real-time clock
>>> source. For this all you have to do is determine the system time at VM
>>> startup and then return a (base + offset) value. Would that
>> suffice or must
>>> ntp adjustments and DST changes also be reflected?
>>>
>>> David Holmes
>>>
>>>> -----Original Message-----
>>>> From: concurrency-interest-bounces at cs.oswego.edu
>>>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
>>>> Pinali Doederlein
>>>> Sent: Monday, 19 May 2008 12:25 AM
>>>> To: Kasper Nielsen
>>>> Cc: concurrency-interest
>>>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>>>
>>>>
>>>> Hi,
>>>>
>>>> I understand the problem, but I wonder if an alternative solution would
>>>> be feasible, like a millisecond-precision timing that's actually
>>>> millisecond-accurate (instead of the current APIs currentTimeMillis(),
>>>> new Date() etc., that update values only each ~16ms, at least in the
>>>> implementations where I tested this). Right now I am forced to write a
>>>> hacked utility class that emulates System.currentTimeMillis()
>> (including
>>>> its epoch - so the value can be used to create dates), but relying on
>>>> currentTimeNanos(), and I patch libraries like Log4J because my
>>>> application logs ABSOLUTELY require 1ms-accurate timestamps in
>> the logs.
>>>> 16ms is not good enough in modern multi-GHz / multicore systems, when I
>>>> want to collect performance data from logs and even a simple query in
>>>> Oracle runs in ~2ms. Even with 1ms accuracy, I observe dozens of
>>>> application operations executing in the same timestamp; but that's good
>>>> enough (for the time being) as any operation taking <1ms is fast enough
>>>> so I don't care.
>>>>
>>>> Perhaps we could reimplement currentTimeMillis() with the same JVM
>>>> tricks used by the nano timer (intrinsification), so we can
>> get at least
>>>> 1ms accuracy, without the risks imposed by additional techniques
>>>> required for nanosecond timers.
>>>>
>>>> A+
>>>> Osvaldo
>>>>> Hi,
>>>>>
>>>>> There is some ongoing talk about adding a high resolution timer
>>>>> System.currentTimeNanos() over at JSR 310. Which acts just like
>>>>> System.currentTimeMillis() but with nanosecond resolution.
>>>>> (https://jsr-310.dev.java.net/servlets/ReadMsg?list=dev&msgNo=1282)
>>>>>
>>>>> In my opinion this is practically impossible to implement correctly
>>>>> without proper os/hardware support. For example, using combinations of
>>>>> QueryPerformanceCounter and GetSystemTimeAsFileTime on
>> Windows is just a
>>>>> disaster waiting to happen.
>>>>>
>>>>> Maybe someone on this list has some input.
>>>>>
>>>>> - Kasper
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at altair.cs.oswego.edu
>>>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> --
>>>> -----------------------------------------------------------------------
>>>> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>>>> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>>>> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at altair.cs.oswego.edu
>>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From mthornton at optrak.co.uk  Tue May 20 03:56:19 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 20 May 2008 08:56:19 +0100
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48318852.7070800@visionnaire.com.br>
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
	<48318852.7070800@visionnaire.com.br>
Message-ID: <48328423.8010206@optrak.co.uk>

Osvaldo Pinali Doederlein wrote:
>     protected static void calibrateTimer () {
>         long millis = 0, nanos = 0;
>         for (int attempts = 0; attempts < 10; ++attempts) {
>             millis = System.currentTimeMillis();
>             nanos = System.nanoTime();
>             final long millis2 = System.currentTimeMillis();
>             final long nanos2 = System.nanoTime();
>             if (millis == millis2 && nanos == nanos2)
>                 break;
>         }
>         milliOffset = millis - nanos / 1000000;
>         nanoOffset = milliOffset * 1000000;
>     }

You are comparing successing currentTimeMillis() and nanoTime results. 
On my system successive nanoTime results are always different, so your 
code would always return the result from the tenth attempt. A slightly 
better result (in my experience) is obtained by calling 
currentTimeMillis and nanoTime immediately after sleeping for 30ms.

Mark Thornton


From mthornton at optrak.co.uk  Tue May 20 04:01:53 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 20 May 2008 09:01:53 +0100
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48320C48.8000006@cs.umd.edu>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
	<48320C48.8000006@cs.umd.edu>
Message-ID: <48328571.2060601@optrak.co.uk>

Jeremy Manson wrote:
> Well, in our case, we actually didn't want that; we just wanted 
> something that was cheap to execute.  Both System.nanoTime() and 
> System.currentTimeMillis() are CPU hogs on Linux, and this approach 
>   
I measured System.nanoTime at 150ns on Ubuntu (Hardy Heron), but around 
800ns under Vista SP1. CPU is an Intel quad core Q6600. Under Ubuntu 
currentTimeMillis and nanoTime appeared to be locked together, at least 
over a 1000s interval, while under Vista the drift amounted to 100ms 
over the same period (and with significant noise ~30ms in comparisons 
between the two).

Mark Thornton


From fw at deneb.enyo.de  Tue May 20 05:59:11 2008
From: fw at deneb.enyo.de (Florian Weimer)
Date: Tue, 20 May 2008 11:59:11 +0200
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au> (David
	Holmes's message of "Mon, 19 May 2008 08:04:03 +1000")
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
Message-ID: <878wy5b8k0.fsf@mid.deneb.enyo.de>

* David Holmes:

> As Kasper alluded the problem is that the OS doesn't necessarily give you a
> high-precision time source that is tied to "wall clock time". Hence on
> Windows the update resolution is typically 10ms (15ms for some versions), on
> Solaris/Linux it is also typically 10ms but can be 1ms depending on system
> configuration.

Linux will offer sub-1ms resolution in the future.  Current kernels do,
but it'll take some time until this has propagated widely.

Somewhat related is that Thread.sleep in the Sun JVM currently can't
handle sub-millisecond precision, it's all rounded up to the next
millisecond.  This is an internal API issue in the Hotspot VM.  Calling
nanosleep directly is an option, but won't give you interruptible
sleeps.

From mthornton at optrak.co.uk  Tue May 20 06:34:20 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Tue, 20 May 2008 11:34:20 +0100
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
Message-ID: <4832A92C.4050007@optrak.co.uk>

David Holmes wrote:
> I always come back to wondering why you need a high resolution value that is
> tied to wall-clock time?
Convenience. Why do we need multiple clocks, why can't we have a single 
clock with nanosecond precision? Well we could if every computer was 
fitted with an atomic clock. So we have multiple clocks because that is 
easier to implement with available hardware. Yet a single high 
resolution clock synchronized to real time does appear to be possible 
(that appears to be what I get on the latest Ubuntu). Using the 
techniques applied by NTP it would also seem possible to synchronize 
System.nanoTime() to real time to provide typically microsecond 
resolution and a delta from real time kept below 0.1s or better.

If you are an engineer familiar with the internal implementation, you 
will be happy to choose an appropriate clock for each purpose. Other 
people just want THE time, and don't bother them with all those details, 
UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding about why 
they can't have (sub)microsecond resolution when the processors have 
clock rates in the GHz.

;-)

Mark Thornton


From osvaldo at visionnaire.com.br  Tue May 20 11:04:57 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Tue, 20 May 2008 12:04:57 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48328423.8010206@optrak.co.uk>
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>	<48318852.7070800@visionnaire.com.br>
	<48328423.8010206@optrak.co.uk>
Message-ID: <4832E899.8030601@visionnaire.com.br>

Mark Thornton wrote:
> Osvaldo Pinali Doederlein wrote:
>    
>>      protected static void calibrateTimer () {
>>          long millis = 0, nanos = 0;
>>          for (int attempts = 0; attempts<  10; ++attempts) {
>>              millis = System.currentTimeMillis();
>>              nanos = System.nanoTime();
>>              final long millis2 = System.currentTimeMillis();
>>              final long nanos2 = System.nanoTime();
>>              if (millis == millis2&&  nanos == nanos2)
>>                  break;
>>          }
>>          milliOffset = millis - nanos / 1000000;
>>          nanoOffset = milliOffset * 1000000;
>>      }
>>      
>
> You are comparing successing currentTimeMillis() and nanoTime results.
> On my system successive nanoTime results are always different, so your
> code would always return the result from the tenth attempt. A slightly
> better result (in my experience) is obtained by calling
> currentTimeMillis and nanoTime immediately after sleeping for 30ms.
>    
Oops, thanks for poiting that bug. I wrote this code quite a long ago 
(by J2SE5.0's beta) so I don't even remember if/how I tested it, perhaps 
I just assumed that nanoTime()'s return value would be truncated to some 
accuracy factor - I vaguely remember that it's ~200ns accurate - but 
this doesn't happen, it returns different values every time. I dropped 
the nanos2 read&check, so the rest makes sense.

After this fix, I created a test that calls the calibration in a loop 
and then builds a histogram of the number of iterations required to 
reach calibration. Result after 1 million iterations:

HotSpot Server 1.6.0u10b24: 1 iter: 99.920454%;  2 iters: 0.079645%; >= 
2 iters: 0%
HotSpot Client 1.6.0u10b24: 1 iter: 99.919153%;  2 iters: 0.080946%; >= 
2 iters: 0%
IBM JDK 6.0SR1            : 1 iter: 99.925357%;  2 iters: 0.074743%; >= 
2 iters: 0%

These results are very consistent, even in the sub-optimized HS Client, 
so it seems that the calibration algorithm is worth the very small effort.

A+
Osvaldo

-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/5dcd1f60/attachment.html 

From osvaldo at visionnaire.com.br  Tue May 20 11:30:17 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Tue, 20 May 2008 12:30:17 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <4832A92C.4050007@optrak.co.uk>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
	<4832A92C.4050007@optrak.co.uk>
Message-ID: <4832EE89.1080406@visionnaire.com.br>

Hi,

Now I'll fully expose my ignorance on the subject of computer time 
measurement: Why can't a system rely on the CPU, which supposedly 
contains a high frequency, stable oscillator for its clock? For example, 
on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns 
precision, and I can read this counter with the RDRTSC opcode. I should 
be able to adjust this value to nanoseconds (in this case just a divide 
by 2), add it to the boot-time wallclock provided by the motherboard 
(since the RTSC is zeroed at boot), and be done with it - using only the 
RTSC as the source for all clock needs afterwards. I know there are 
problems like energy-efficient CPUs that change their clock frequency 
dynamically, and synchronization with NTP or Windows Time servers, but 
the system should be able to monitor these factors and adjust the 
nano-clock, just like it does for the low-precision clock.

BTW, I don't need an atomic-quality clock that's extremely accurate with 
the real time of my timezone... even in distributed systems, I only need 
a reasonably good accuracy within a cluster or the local network. I need 
nanosecond-level accuracy, though, within a single computer system or 
application node, so I can measure the duration, order, and correlation 
of events, with extremely good precision.

A+
Osvaldo
> David Holmes wrote:
>    
>> I always come back to wondering why you need a high resolution value that is
>> tied to wall-clock time?
>>      
> Convenience. Why do we need multiple clocks, why can't we have a single
> clock with nanosecond precision? Well we could if every computer was
> fitted with an atomic clock. So we have multiple clocks because that is
> easier to implement with available hardware. Yet a single high
> resolution clock synchronized to real time does appear to be possible
> (that appears to be what I get on the latest Ubuntu). Using the
> techniques applied by NTP it would also seem possible to synchronize
> System.nanoTime() to real time to provide typically microsecond
> resolution and a delta from real time kept below 0.1s or better.
>
> If you are an engineer familiar with the internal implementation, you
> will be happy to choose an appropriate clock for each purpose. Other
> people just want THE time, and don't bother them with all those details,
> UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding about why
> they can't have (sub)microsecond resolution when the processors have
> clock rates in the GHz.
>
> ;-)
>
> Mark Thornton
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>    


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/125f83b8/attachment.html 

From jim.andreou at gmail.com  Tue May 20 11:40:25 2008
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Tue, 20 May 2008 18:40:25 +0300
Subject: [concurrency-interest] List#toArray question
Message-ID: <4832F0E9.4090803@ics.forth.gr>

I'm reading the 5th chapther of the 2nd version of Effective Java 
(available here: 
http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf)
and I'm confused at page 121.

It talks about reducing a list with a function that is applied to its 
elementsm and gives this code:

// Reduction without generics or concurrency flaw
static Object reduce(List list, Function f, Object initVal) {
    Object[] snapshot = list.toArray(); // Locks list internally
    Object result = initVal;
    for (Object o : list)
        result = f.apply(result, o);
    return result;
}

(I hope the formatting is preserved).
What is this "Locks list internally" comment? I can't find any such 
locking (looking Sun's Java 6 implementation), neither in AbstractList 
and above, nor say in ArrayList (which delegates to 
Arrays#copyOf(Object[], int) - could that imply that System#arraycopy 
locks its arrays? But then still, that affects only ArrayLists...). And 
I would be confused if there was any. Did I miss anything obvious?

Dimitris Andreou

From jim.andreou at gmail.com  Tue May 20 11:42:55 2008
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Tue, 20 May 2008 18:42:55 +0300
Subject: [concurrency-interest] List#toArray question
Message-ID: <4832F17F.7020801@gmail.com>

I'm reading the 5th chapther of the 2nd version of Effective Java 
(available here: 
http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf) 

and I'm confused at page 121.

It talks about reducing a list with a function that is applied to its 
elementsm and gives this code:

// Reduction without generics or concurrency flaw
static Object reduce(List list, Function f, Object initVal) {
   Object[] snapshot = list.toArray(); // Locks list internally
   Object result = initVal;
   for (Object o : list)
       result = f.apply(result, o);
   return result;
}

(I hope the formatting is preserved).
What is this "Locks list internally" comment? I can't find any such 
locking (looking Sun's Java 6 implementation), neither in AbstractList 
and above, nor say in ArrayList (which delegates to 
Arrays#copyOf(Object[], int) - could that imply that System#arraycopy 
locks its arrays? But then still, that affects only ArrayLists...). And 
I would be confused if there was any. Did I miss anything obvious?

Dimitris Andreou


From holger at wizards.de  Tue May 20 12:40:46 2008
From: holger at wizards.de (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Tue, 20 May 2008 18:40:46 +0200
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <4832F17F.7020801@gmail.com>
References: <4832F17F.7020801@gmail.com>
Message-ID: <4832FF0E.3070601@wizards.de>

Dimitris Andreou wrote:
> // Reduction without generics or concurrency flaw
> static Object reduce(List list, Function f, Object initVal) {
>    Object[] snapshot = list.toArray(); // Locks list internally
>    Object result = initVal;
>    for (Object o : list)
>        result = f.apply(result, o);
>    return result;
> }
> 
> (I hope the formatting is preserved).
> What is this "Locks list internally" comment? I can't find any such 

Seems like a bug (how ironic). The loop should iterate over the 
snapshot[], not the list - but the compiler did not complain because 
foreach handles both arrays and collections.
Any decent IDE should warn that snapshot is never read. A great showcase 
for the dangers of ignoring compiler warnings..

Or, as a wise man once said:
"-Wall -Werror..you may hate it now, but you'll learn to love it later" :-)

-h

From jim.andreou at gmail.com  Tue May 20 12:45:40 2008
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Tue, 20 May 2008 19:45:40 +0300
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <4832FF0E.3070601@wizards.de>
References: <4832F17F.7020801@gmail.com> <4832FF0E.3070601@wizards.de>
Message-ID: <48330034.2000508@gmail.com>

O/H Holger Hoffst?tte ??????:
> Dimitris Andreou wrote:
>> // Reduction without generics or concurrency flaw
>> static Object reduce(List list, Function f, Object initVal) {
>> Object[] snapshot = list.toArray(); // Locks list internally
>> Object result = initVal;
>> for (Object o : list)
>> result = f.apply(result, o);
>> return result;
>> }
>>
>> (I hope the formatting is preserved).
>> What is this "Locks list internally" comment? I can't find any such 
>
> Seems like a bug (how ironic). The loop should iterate over the 
> snapshot[], not the list - but the compiler did not complain because 
> foreach handles both arrays and collections.
> Any decent IDE should warn that snapshot is never read. A great 
> showcase for the dangers of ignoring compiler warnings..
>
> Or, as a wise man once said:
> "-Wall -Werror..you may hate it now, but you'll learn to love it 
> later" :-)
>
> -h
>

Heh, I didn't notice that at all, it definitely looks like a typo. But 
my question regarded the first line of the method (and specifically, 
Bloch's comment), not the iteration below.

Dimitris

From holger at wizards.de  Tue May 20 12:56:32 2008
From: holger at wizards.de (=?UTF-8?B?SG9sZ2VyIEhvZmZzdMOkdHRl?=)
Date: Tue, 20 May 2008 18:56:32 +0200
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <48330034.2000508@gmail.com>
References: <4832F17F.7020801@gmail.com> <4832FF0E.3070601@wizards.de>
	<48330034.2000508@gmail.com>
Message-ID: <483302C0.3090902@wizards.de>

Dimitris Andreou wrote:
> O/H Holger Hoffst?tte ??????:
>> Dimitris Andreou wrote:
>>> // Reduction without generics or concurrency flaw
>>> static Object reduce(List list, Function f, Object initVal) {
>>> Object[] snapshot = list.toArray(); // Locks list internally
>>> Object result = initVal;
>>> for (Object o : list)
>>> result = f.apply(result, o);
>>> return result;
>>> }
>>>
>>> (I hope the formatting is preserved).
>>> What is this "Locks list internally" comment? I can't find any such 
>>
>> Seems like a bug (how ironic). The loop should iterate over the 
>> snapshot[], not the list - but the compiler did not complain because 
>> foreach handles both arrays and collections.
>> Any decent IDE should warn that snapshot is never read. A great 
>> showcase for the dangers of ignoring compiler warnings..
>>
>> Or, as a wise man once said:
>> "-Wall -Werror..you may hate it now, but you'll learn to love it 
>> later" :-)
>>
>> -h
>>
> 
> Heh, I didn't notice that at all, it definitely looks like a typo. But 
> my question regarded the first line of the method (and specifically, 
> Bloch's comment), not the iteration below.

That's the point: when you iterate over the method-local snapshot, the 
original list is not touched anymore and therefore the iteration is "safe" 
from side effects (strictly speaking the computation as a whole is not).
Other than that I don't think there is any other "locking" going on, at 
least not in the j.u.c.Lock sense of the word.

-h


From joe.bowbeer at gmail.com  Tue May 20 12:58:31 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 20 May 2008 09:58:31 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <4832F17F.7020801@gmail.com>
References: <4832F17F.7020801@gmail.com>
Message-ID: <31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>

On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:

> I'm reading the 5th chapther of the 2nd version of Effective Java
> (available here:
>
> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
> )
>
> and I'm confused at page 121.
>
> It talks about reducing a list with a function that is applied to its
> elements and gives this code:
>
> // Reduction without generics or concurrency flaw
> static Object reduce(List list, Function f, Object initVal) {
>   Object[] snapshot = list.toArray(); // Locks list internally
>   Object result = initVal;
>   for (Object o : list)
>       result = f.apply(result, o);
>   return result;
> }
>
> (I hope the formatting is preserved).
> What is this "Locks list internally" comment? I can't find any such
> locking (looking Sun's Java 6 implementation), neither in AbstractList
> and above, nor say in ArrayList (which delegates to
> Arrays#copyOf(Object[], int) - could that imply that System#arraycopy
> locks its arrays? But then still, that affects only ArrayLists...). And
> I would be confused if there was any. Did I miss anything obvious?
>
> Dimitris Andreou
>
>

I see what you mean now that you point it out.

An alternative you didn't mention is replacing "List" with "Vector", whose
methods are locked internally.

--
Joe Bowbeer
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/76d7c97f/attachment.html 

From nbronson at cs.stanford.edu  Tue May 20 13:02:48 2008
From: nbronson at cs.stanford.edu (Nathan Bronson)
Date: Tue, 20 May 2008 10:02:48 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <48330034.2000508@gmail.com>
References: <4832F17F.7020801@gmail.com> <4832FF0E.3070601@wizards.de>
	<48330034.2000508@gmail.com>
Message-ID: <1211302968.19941.5.camel@astroman>

The snapshot code is definitely not safe for an arbitrary List, since
any implementation that inherits AbstractCollection.toArray() will just
perform an iteration internally (with no lock).  Perhaps this code
originally used a Vector, for which toArray() is synchronized?

Nathan

On Tue, 2008-05-20 at 19:45 +0300, Dimitris Andreou wrote:
> O/H Holger Hoffst?tte ??????:
> > Dimitris Andreou wrote:
> >> // Reduction without generics or concurrency flaw
> >> static Object reduce(List list, Function f, Object initVal) {
> >> Object[] snapshot = list.toArray(); // Locks list internally
> >> Object result = initVal;
> >> for (Object o : list)
> >> result = f.apply(result, o);
> >> return result;
> >> }
> >>
> >> (I hope the formatting is preserved).
> >> What is this "Locks list internally" comment? I can't find any such 
> >
> > Seems like a bug (how ironic). The loop should iterate over the 
> > snapshot[], not the list - but the compiler did not complain because 
> > foreach handles both arrays and collections.
> > Any decent IDE should warn that snapshot is never read. A great 
> > showcase for the dangers of ignoring compiler warnings..
> >
> > Or, as a wise man once said:
> > "-Wall -Werror..you may hate it now, but you'll learn to love it 
> > later" :-)
> >
> > -h
> >
> 
> Heh, I didn't notice that at all, it definitely looks like a typo. But 
> my question regarded the first line of the method (and specifically, 
> Bloch's comment), not the iteration below.
> 
> Dimitris
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From joe.bowbeer at gmail.com  Tue May 20 13:12:36 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 20 May 2008 10:12:36 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
References: <4832F17F.7020801@gmail.com>
	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
Message-ID: <31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>

Ah. Here's what you're missing:

  ...suppose you have a synchronized list (of the sort returned by
Collections.synchronizedList) ...


On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:

> On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
>
>> I'm reading the 5th chapther of the 2nd version of Effective Java
>> (available here:
>>
>> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
>> )
>>
>> and I'm confused at page 121.
>>
>> It talks about reducing a list with a function that is applied to its
>> elements and gives this code:
>>
>> // Reduction without generics or concurrency flaw
>> static Object reduce(List list, Function f, Object initVal) {
>>   Object[] snapshot = list.toArray(); // Locks list internally
>>   Object result = initVal;
>>   for (Object o : list)
>>       result = f.apply(result, o);
>>   return result;
>> }
>>
>> (I hope the formatting is preserved).
>> What is this "Locks list internally" comment? I can't find any such
>> locking (looking Sun's Java 6 implementation), neither in AbstractList
>> and above, nor say in ArrayList (which delegates to
>> Arrays#copyOf(Object[], int) - could that imply that System#arraycopy
>> locks its arrays? But then still, that affects only ArrayLists...). And
>> I would be confused if there was any. Did I miss anything obvious?
>>
>> Dimitris Andreou
>>
>>
>
> I see what you mean now that you point it out.
>
> An alternative you didn't mention is replacing "List" with "Vector", whose
> methods are locked internally.
>
> --
> Joe Bowbeer
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/428d2a14/attachment.html 

From hans.boehm at hp.com  Tue May 20 13:30:15 2008
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 20 May 2008 17:30:15 +0000
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <878wy5b8k0.fsf@mid.deneb.enyo.de>
References: <NFBBKALFDCPFIDBNKAPCEEMBHLAA.dcholmes@optusnet.com.au>
	<878wy5b8k0.fsf@mid.deneb.enyo.de>
Message-ID: <238A96A773B3934685A7269CC8A8D0420599B0F4F4@GVW0436EXB.americas.hpqcorp.net>

Another issue that seems to be getting overlooked here is whether you need these clocks to be monotonic, in that a "later" call always returns a value no less than an "earlier" call.  That requirement may add significantly add to the implementation expense, and may be at odds with wanting "wall clock time", which someone might adjust.

There are at least two possible definitions of "earlier" that are interesting:

a) earlier in the execution of the same thread

b) earlier in the sense of "happens before"

A quick Google search brought up some complaints about non-monotonicity of nanoTime in sense (a), due presumably to thread migration and unsynchronized cpu clocks.  I think this is fixable at the OS level.  Getting monotonicity in the stronger sense (b) may not be practically feasible without the right hardware support.  The C++ committee has had some discussion of this, but it's still unclear to me whether it's safe to assume that hardware support is always present.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf
> Of Florian Weimer
> Sent: Tuesday, May 20, 2008 2:59 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest; Osvaldo Pinali Doederlein
> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>
> * David Holmes:
>
> > As Kasper alluded the problem is that the OS doesn't
> necessarily give
> > you a high-precision time source that is tied to "wall clock time".
> > Hence on Windows the update resolution is typically 10ms (15ms for
> > some versions), on Solaris/Linux it is also typically 10ms
> but can be
> > 1ms depending on system configuration.
>
> Linux will offer sub-1ms resolution in the future.  Current
> kernels do, but it'll take some time until this has propagated widely.
>
> Somewhat related is that Thread.sleep in the Sun JVM
> currently can't handle sub-millisecond precision, it's all
> rounded up to the next millisecond.  This is an internal API
> issue in the Hotspot VM.  Calling nanosleep directly is an
> option, but won't give you interruptible sleeps.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From josh at bloch.us  Tue May 20 13:58:57 2008
From: josh at bloch.us (Joshua Bloch)
Date: Tue, 20 May 2008 10:58:57 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
References: <4832F17F.7020801@gmail.com>
	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
	<31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
Message-ID: <b097ac510805201058x6f25e9b5tecb32efc04248cbf@mail.gmail.com>

Hi.  I was just about to say what Joe Bowbeer just said.  The first sentence
of the first paragraph on the page says:


> For example, suppose you have a synchronized list (of the sort returned by
> Collections.synchronizedList)


I hope people don't get confused by this:(

Keep in mind that the code above it ("*// Reduction without generics, and
with concurrency flaw!*") would be garbage if the list weren't synchronized:
it depends on the synchronized block to exclude concurrent updates.  (It is
intentionally broken, in that it calls an alien method inside the
synchronized block, which is what is fixed in the example below.)

      Regards,

      Josh

On Tue, May 20, 2008 at 10:12 AM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> Ah. Here's what you're missing:
>
>   ...suppose you have a synchronized list (of the sort returned by
> Collections.synchronizedList) ...
>
>
>
> On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:
>
>> On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
>>
>>> I'm reading the 5th chapther of the 2nd version of Effective Java
>>> (available here:
>>>
>>> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
>>> )
>>>
>>> and I'm confused at page 121.
>>>
>>> It talks about reducing a list with a function that is applied to its
>>> elements and gives this code:
>>>
>>> // Reduction without generics or concurrency flaw
>>> static Object reduce(List list, Function f, Object initVal) {
>>>   Object[] snapshot = list.toArray(); // Locks list internally
>>>   Object result = initVal;
>>>   for (Object o : list)
>>>       result = f.apply(result, o);
>>>   return result;
>>> }
>>>
>>> (I hope the formatting is preserved).
>>> What is this "Locks list internally" comment? I can't find any such
>>> locking (looking Sun's Java 6 implementation), neither in AbstractList
>>> and above, nor say in ArrayList (which delegates to
>>> Arrays#copyOf(Object[], int) - could that imply that System#arraycopy
>>> locks its arrays? But then still, that affects only ArrayLists...). And
>>> I would be confused if there was any. Did I miss anything obvious?
>>>
>>> Dimitris Andreou
>>>
>>>
>>
>> I see what you mean now that you point it out.
>>
>> An alternative you didn't mention is replacing "List" with "Vector", whose
>> methods are locked internally.
>>
>> --
>> Joe Bowbeer
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/96559ec6/attachment-0001.html 

From jim.andreou at gmail.com  Tue May 20 13:59:27 2008
From: jim.andreou at gmail.com (Dimitris Andreou)
Date: Tue, 20 May 2008 20:59:27 +0300
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
References: <4832F17F.7020801@gmail.com>	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
	<31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
Message-ID: <4833117F.3060505@gmail.com>

That's it, thanks! (That was too far to the top :) )

O/H Joe Bowbeer ??????:
> Ah. Here's what you're missing:
>
> ...suppose you have a synchronized list (of the sort returned by 
> Collections.synchronizedList) ...
>
>
> On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:
>
>     On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
>
>         I'm reading the 5th chapther of the 2nd version of Effective Java
>         (available here:
>         http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf)
>
>         and I'm confused at page 121.
>
>         It talks about reducing a list with a function that is applied
>         to its
>         elements and gives this code:
>
>
>         // Reduction without generics or concurrency flaw
>         static Object reduce(List list, Function f, Object initVal) {
>         Object[] snapshot = list.toArray(); // Locks list internally
>         Object result = initVal;
>         for (Object o : list)
>         result = f.apply(result, o);
>         return result;
>         }
>
>         (I hope the formatting is preserved).
>         What is this "Locks list internally" comment? I can't find any
>         such
>         locking (looking Sun's Java 6 implementation), neither in
>         AbstractList
>         and above, nor say in ArrayList (which delegates to
>         Arrays#copyOf(Object[], int) - could that imply that
>         System#arraycopy
>         locks its arrays? But then still, that affects only
>         ArrayLists...). And
>         I would be confused if there was any. Did I miss anything obvious?
>
>         Dimitris Andreou
>
>
>
>     I see what you mean now that you point it out.
>
>     An alternative you didn't mention is replacing "List" with
>     "Vector", whose methods are locked internally.
>
>     --
>     Joe Bowbeer
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From josh at bloch.us  Tue May 20 14:53:57 2008
From: josh at bloch.us (Joshua Bloch)
Date: Tue, 20 May 2008 11:53:57 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <4833117F.3060505@gmail.com>
References: <4832F17F.7020801@gmail.com>
	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
	<31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
	<4833117F.3060505@gmail.com>
Message-ID: <b097ac510805201153t17569aacxe9a9b8aeeecf39f9@mail.gmail.com>

Dimitris,

Thanks for telling me.  If too many people get confused by this, I'll have
to figure out some way to clarify it.

     Josh

On Tue, May 20, 2008 at 10:59 AM, Dimitris Andreou <jim.andreou at gmail.com>
wrote:

> That's it, thanks! (That was too far to the top :) )
>
> O/H Joe Bowbeer ??????:
>  > Ah. Here's what you're missing:
> >
> > ...suppose you have a synchronized list (of the sort returned by
> > Collections.synchronizedList) ...
> >
> >
> > On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:
> >
> >     On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
> >
> >         I'm reading the 5th chapther of the 2nd version of Effective Java
> >         (available here:
> >
> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
> )
> >
> >         and I'm confused at page 121.
> >
> >         It talks about reducing a list with a function that is applied
> >         to its
> >         elements and gives this code:
> >
> >
> >         // Reduction without generics or concurrency flaw
> >         static Object reduce(List list, Function f, Object initVal) {
> >         Object[] snapshot = list.toArray(); // Locks list internally
> >         Object result = initVal;
> >         for (Object o : list)
> >         result = f.apply(result, o);
> >         return result;
> >         }
> >
> >         (I hope the formatting is preserved).
> >         What is this "Locks list internally" comment? I can't find any
> >         such
> >         locking (looking Sun's Java 6 implementation), neither in
> >         AbstractList
> >         and above, nor say in ArrayList (which delegates to
> >         Arrays#copyOf(Object[], int) - could that imply that
> >         System#arraycopy
> >         locks its arrays? But then still, that affects only
> >         ArrayLists...). And
> >         I would be confused if there was any. Did I miss anything
> obvious?
> >
> >         Dimitris Andreou
> >
> >
> >
> >     I see what you mean now that you point it out.
> >
> >     An alternative you didn't mention is replacing "List" with
> >     "Vector", whose methods are locked internally.
> >
> >     --
> >     Joe Bowbeer
> >
> >
> > ------------------------------------------------------------------------
>  >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/089c890e/attachment.html 

From joe.bowbeer at gmail.com  Tue May 20 15:01:48 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 20 May 2008 12:01:48 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <b097ac510805201153t17569aacxe9a9b8aeeecf39f9@mail.gmail.com>
References: <4832F17F.7020801@gmail.com>
	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
	<31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
	<4833117F.3060505@gmail.com>
	<b097ac510805201153t17569aacxe9a9b8aeeecf39f9@mail.gmail.com>
Message-ID: <31f2a7bd0805201201y72624268qe04741809bd92330@mail.gmail.com>

Josh,

As Holger points out, there *is* a problem with this intermediate code:

  Prior to release 1.5, the natural way to do this would have been
  using List's toArray method (which locks the list internally):

  // Reduction without generics or concurrency flaw
  static Object reduce(List list, Function f, Object initVal) {
      Object[] snapshot = list.toArray(); // Locks list internally
      Object result = initVal;
      for (Object o : list)
          result = f.apply(result, o);
      return result;
  }

It should be iterating over *snapshot* not list, and using an old-style for
loop, because foreach didn't exist prior to Java 5.

--Joe


2008/5/20 Joshua Bloch:

> Dimitris,
>
> Thanks for telling me.  If too many people get confused by this, I'll have
> to figure out some way to clarify it.
>
>      Josh
>
> On Tue, May 20, 2008 at 10:59 AM, Dimitris Andreou wrote:
>
>> That's it, thanks! (That was too far to the top :) )
>>
>> O/H Joe Bowbeer ??????:
>>  > Ah. Here's what you're missing:
>> >
>> > ...suppose you have a synchronized list (of the sort returned by
>> > Collections.synchronizedList) ...
>> >
>> >
>> > On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:
>> >
>> >     On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
>> >
>> >         I'm reading the 5th chapther of the 2nd version of Effective
>> Java
>> >         (available here:
>> >
>> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
>> )
>> >
>> >         and I'm confused at page 121.
>> >
>> >         It talks about reducing a list with a function that is applied
>> >         to its
>> >         elements and gives this code:
>> >
>> >
>> >         // Reduction without generics or concurrency flaw
>> >         static Object reduce(List list, Function f, Object initVal) {
>> >         Object[] snapshot = list.toArray(); // Locks list internally
>> >         Object result = initVal;
>> >         for (Object o : list)
>> >         result = f.apply(result, o);
>> >         return result;
>> >         }
>> >
>> >         (I hope the formatting is preserved).
>> >         What is this "Locks list internally" comment? I can't find any
>> >         such
>> >         locking (looking Sun's Java 6 implementation), neither in
>> >         AbstractList
>> >         and above, nor say in ArrayList (which delegates to
>> >         Arrays#copyOf(Object[], int) - could that imply that
>> >         System#arraycopy
>> >         locks its arrays? But then still, that affects only
>> >         ArrayLists...). And
>> >         I would be confused if there was any. Did I miss anything
>> obvious?
>> >
>> >         Dimitris Andreou
>> >
>> >
>> >
>> >     I see what you mean now that you point it out.
>> >
>> >     An alternative you didn't mention is replacing "List" with
>> >     "Vector", whose methods are locked internally.
>> >
>> >     --
>> >     Joe Bowbeer
>> >
>> >
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/d79f0eac/attachment.html 

From josh at bloch.us  Tue May 20 15:06:15 2008
From: josh at bloch.us (Joshua Bloch)
Date: Tue, 20 May 2008 12:06:15 -0700
Subject: [concurrency-interest] List#toArray question
In-Reply-To: <31f2a7bd0805201201y72624268qe04741809bd92330@mail.gmail.com>
References: <4832F17F.7020801@gmail.com>
	<31f2a7bd0805200958m215fe17bo23a43b3d702a924d@mail.gmail.com>
	<31f2a7bd0805201012l722e5232kbf126ac6f8e3f18e@mail.gmail.com>
	<4833117F.3060505@gmail.com>
	<b097ac510805201153t17569aacxe9a9b8aeeecf39f9@mail.gmail.com>
	<31f2a7bd0805201201y72624268qe04741809bd92330@mail.gmail.com>
Message-ID: <b097ac510805201206n64072907w2bbb34c40327bd94@mail.gmail.com>

Joe,

Yes.  I have publically 'fessed up to this one:
http://www.infoq.com/articles/bloch-effective-java-2e .  I'm really unhappy
with myself: I found this before the book went to pressed, fixed it in the
code bundle, but somehow I don't fix it in the book.  Sigh...

       Josh

P.S.  I'll send a fixed PDF to infoQ today.

2008/5/20 Joe Bowbeer <joe.bowbeer at gmail.com>:

> Josh,
>
> As Holger points out, there *is* a problem with this intermediate code:
>
>   Prior to release 1.5, the natural way to do this would have been
>   using List's toArray method (which locks the list internally):
>
>   // Reduction without generics or concurrency flaw
>   static Object reduce(List list, Function f, Object initVal) {
>       Object[] snapshot = list.toArray(); // Locks list internally
>       Object result = initVal;
>       for (Object o : list)
>           result = f.apply(result, o);
>       return result;
>   }
>
> It should be iterating over *snapshot* not list, and using an old-style for
> loop, because foreach didn't exist prior to Java 5.
>
> --Joe
>
>
> 2008/5/20 Joshua Bloch:
>
>>  Dimitris,
>>
>> Thanks for telling me.  If too many people get confused by this, I'll have
>> to figure out some way to clarify it.
>>
>>      Josh
>>
>>    On Tue, May 20, 2008 at 10:59 AM, Dimitris Andreou wrote:
>>
>>> That's it, thanks! (That was too far to the top :) )
>>>
>>> O/H Joe Bowbeer ??????:
>>>  > Ah. Here's what you're missing:
>>> >
>>> > ...suppose you have a synchronized list (of the sort returned by
>>> > Collections.synchronizedList) ...
>>> >
>>> >
>>> > On Tue, May 20, 2008 at 9:58 AM, Joe Bowbeer wrote:
>>> >
>>> >     On Tue, May 20, 2008 at 8:42 AM, Dimitris Andreou wrote:
>>> >
>>> >         I'm reading the 5th chapther of the 2nd version of Effective
>>> Java
>>> >         (available here:
>>> >
>>> http://www.infoq.com/resource/articles/bloch-effective-java-2e/en/resources/Bloch_Ch05.pdf
>>> )
>>> >
>>> >         and I'm confused at page 121.
>>> >
>>> >         It talks about reducing a list with a function that is applied
>>> >         to its
>>> >         elements and gives this code:
>>> >
>>> >
>>> >         // Reduction without generics or concurrency flaw
>>> >         static Object reduce(List list, Function f, Object initVal) {
>>> >         Object[] snapshot = list.toArray(); // Locks list internally
>>> >         Object result = initVal;
>>> >         for (Object o : list)
>>> >         result = f.apply(result, o);
>>> >         return result;
>>> >         }
>>> >
>>> >         (I hope the formatting is preserved).
>>> >         What is this "Locks list internally" comment? I can't find any
>>> >         such
>>> >         locking (looking Sun's Java 6 implementation), neither in
>>> >         AbstractList
>>> >         and above, nor say in ArrayList (which delegates to
>>> >         Arrays#copyOf(Object[], int) - could that imply that
>>> >         System#arraycopy
>>> >         locks its arrays? But then still, that affects only
>>> >         ArrayLists...). And
>>> >         I would be confused if there was any. Did I miss anything
>>> obvious?
>>> >
>>> >         Dimitris Andreou
>>> >
>>> >
>>> >
>>> >     I see what you mean now that you point it out.
>>> >
>>> >     An alternative you didn't mention is replacing "List" with
>>> >     "Vector", whose methods are locked internally.
>>> >
>>> >     --
>>> >     Joe Bowbeer
>>> >
>>> >
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080520/da69fb09/attachment-0001.html 

From sankar_adabala at yahoo.com  Wed May 21 02:15:26 2008
From: sankar_adabala at yahoo.com (sankar adabala)
Date: Tue, 20 May 2008 23:15:26 -0700 (PDT)
Subject: [concurrency-interest] ScheduledThreadPoolExecutor long running
	tasks should not be Queued up
Message-ID: <957752.78815.qm@web58712.mail.re1.yahoo.com>


Iam using STE for polling a large set of devices.STE works fine and scales up well.But i noticed the following.

Q1)
Suppose a TaskA is scheduled repeatedly every 4 minutes and if the Task takes more than 4 minutes and in this case  takes 6 minutes,then the next scheduling is happening at 6th minute.I don't want this behaviour, what i wanted is the task has to be scheduled every 4minutes and the absolute scheduling time should be a multiple of 4.

Is there a mechanism in STE to specify a task to be executed every 4minutes or multiple of 4 in case the task takes more time.


Q2)
What is the tolerance in seconds we can expect in scheduling ,suppose if a task is scheduled every 4 minutes ,does it gets scheduled exactly every 4 minutes or in the range [3 minutes 59 secs to 4 minutes 1 sec].


I have a workaround for Q1.I skip the scheduling if it doesn't fall in the range[3 minutes 59 secs to 4 minutes 1 sec ie a smoothing interval of 1 sec].


please let me know if there are better ways to handle this situation.


Thanks,
Sankar Krishna




      __________________________________________________________
Sent from Yahoo! Mail.
A Smarter Email http://uk.docs.yahoo.com/nowyoucan.html


From jmanson at cs.umd.edu  Wed May 21 03:33:37 2008
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Wed, 21 May 2008 00:33:37 -0700
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48328571.2060601@optrak.co.uk>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
	<48320C48.8000006@cs.umd.edu> <48328571.2060601@optrak.co.uk>
Message-ID: <4833D051.8000002@cs.umd.edu>

Mark Thornton wrote:
> Jeremy Manson wrote:
>> Well, in our case, we actually didn't want that; we just wanted 
>> something that was cheap to execute.  Both System.nanoTime() and 
>> System.currentTimeMillis() are CPU hogs on Linux, and this approach   

> I measured System.nanoTime at 150ns on Ubuntu (Hardy Heron), but around 
> 800ns under Vista SP1. CPU is an Intel quad core Q6600. Under Ubuntu 
> currentTimeMillis and nanoTime appeared to be locked together, at least 
> over a 1000s interval, while under Vista the drift amounted to 100ms 
> over the same period (and with significant noise ~30ms in comparisons 
> between the two).
> 
> Mark Thornton

I had to page the details back in, because I had forgotten them...

The results end up being pretty system dependent.  Because 
System.currentTimeMillis requires a syscall, and because syscalls 
require you to clear the pipeline, the speed with which you can clear 
the pipeline becomes really important.

I was using a P4 to do my experiments; the P4 had a really long 
pipeline, and my hack made currentTimeMillis really fast.  Same thing 
with an Opteron (8214HE).  When I ran the same experiment with a 8-way 
Core2Duo (where the pipelines are shorter), the difference was a lot 
smaller.

Finally, when I ran the same experiment on a 64-bit VM (they're faster, 
folks), the difference between my hack and currentTimeMillis decreased 
to the point where I didn't care about my hack anymore.

					Jeremy

From jmanson at cs.umd.edu  Wed May 21 03:41:06 2008
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Wed, 21 May 2008 00:41:06 -0700
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <4832EE89.1080406@visionnaire.com.br>
References: <NFBBKALFDCPFIDBNKAPCEEMEHLAA.dcholmes@optusnet.com.au>
	<4832A92C.4050007@optrak.co.uk> <4832EE89.1080406@visionnaire.com.br>
Message-ID: <4833D212.6040608@cs.umd.edu>

Hi, Osvaldo,

This is what we tried.  There are two issues:

1) The CPU can slow down the cycle counter to save speed, and

2) The cycle counter can be different on each CPU on systems with 
multiple processors.

It's good enough for a lot of uses, although you can imagine the 
confusion it causes from time to time.

					Jeremy


Osvaldo Pinali Doederlein wrote:
> Hi,
> 
> Now I'll fully expose my ignorance on the subject of computer time 
> measurement: Why can't a system rely on the CPU, which supposedly 
> contains a high frequency, stable oscillator for its clock? For example, 
> on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns 
> precision, and I can read this counter with the RDRTSC opcode. I should 
> be able to adjust this value to nanoseconds (in this case just a divide 
> by 2), add it to the boot-time wallclock provided by the motherboard 
> (since the RTSC is zeroed at boot), and be done with it - using only the 
> RTSC as the source for all clock needs afterwards. I know there are 
> problems like energy-efficient CPUs that change their clock frequency 
> dynamically, and synchronization with NTP or Windows Time servers, but 
> the system should be able to monitor these factors and adjust the 
> nano-clock, just like it does for the low-precision clock.
> 
> BTW, I don't need an atomic-quality clock that's extremely accurate with 
> the real time of my timezone... even in distributed systems, I only need 
> a reasonably good accuracy within a cluster or the local network. I need 
> nanosecond-level accuracy, though, within a single computer system or 
> application node, so I can measure the duration, order, and correlation 
> of events, with extremely good precision.
> 
> A+
> Osvaldo
>> David Holmes wrote:
>>   
>>> I always come back to wondering why you need a high resolution value that is
>>> tied to wall-clock time?
>>>     
>> Convenience. Why do we need multiple clocks, why can't we have a single 
>> clock with nanosecond precision? Well we could if every computer was 
>> fitted with an atomic clock. So we have multiple clocks because that is 
>> easier to implement with available hardware. Yet a single high 
>> resolution clock synchronized to real time does appear to be possible 
>> (that appears to be what I get on the latest Ubuntu). Using the 
>> techniques applied by NTP it would also seem possible to synchronize 
>> System.nanoTime() to real time to provide typically microsecond 
>> resolution and a delta from real time kept below 0.1s or better.
>>
>> If you are an engineer familiar with the internal implementation, you 
>> will be happy to choose an appropriate clock for each purpose. Other 
>> people just want THE time, and don't bother them with all those details, 
>> UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding about why 
>> they can't have (sub)microsecond resolution when the processors have 
>> clock rates in the GHz.
>>
>> ;-)
>>
>> Mark Thornton
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>   
> 
> 
> -- 
> -----------------------------------------------------------------------
> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Wed May 21 04:30:36 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 21 May 2008 18:30:36 +1000
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <4833D212.6040608@cs.umd.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEMOHLAA.dcholmes@optusnet.com.au>

Hmm well the cycle counter is *bad* enough that it's not used as a timing
source by the major OSes anymore. The Linux folk have abandoned it until we
have cycle counters that are synchronized across cores. Windows swapped it
for the pmTimer back in the XP SP2 days and only uses it on systems where it
can't cause a problem. Of course you can also get utilities from Intel and
AMD to synchronize the TSC across processors if you really want to use it.
Solaris can still use the TSC on x86 but, as far as I know, performs its own
synchronization.

If you need a monotonic counter then the cycle counter is only useful on
uniprocessors/single-cores. And of couse you need to disable sped-step or
any other energy saving mechanism that alters the cycle counter frequency.

So caveat emptor. ;-)

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jeremy
> Manson
> Sent: Wednesday, 21 May 2008 5:41 PM
> To: Osvaldo Pinali Doederlein
> Cc: concurrency-interest
> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>
>
> Hi, Osvaldo,
>
> This is what we tried.  There are two issues:
>
> 1) The CPU can slow down the cycle counter to save speed, and
>
> 2) The cycle counter can be different on each CPU on systems with
> multiple processors.
>
> It's good enough for a lot of uses, although you can imagine the
> confusion it causes from time to time.
>
> 					Jeremy
>
>
> Osvaldo Pinali Doederlein wrote:
> > Hi,
> >
> > Now I'll fully expose my ignorance on the subject of computer time
> > measurement: Why can't a system rely on the CPU, which supposedly
> > contains a high frequency, stable oscillator for its clock? For
> example,
> > on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns
> > precision, and I can read this counter with the RDRTSC opcode. I should
> > be able to adjust this value to nanoseconds (in this case just a divide
> > by 2), add it to the boot-time wallclock provided by the motherboard
> > (since the RTSC is zeroed at boot), and be done with it - using
> only the
> > RTSC as the source for all clock needs afterwards. I know there are
> > problems like energy-efficient CPUs that change their clock frequency
> > dynamically, and synchronization with NTP or Windows Time servers, but
> > the system should be able to monitor these factors and adjust the
> > nano-clock, just like it does for the low-precision clock.
> >
> > BTW, I don't need an atomic-quality clock that's extremely
> accurate with
> > the real time of my timezone... even in distributed systems, I
> only need
> > a reasonably good accuracy within a cluster or the local
> network. I need
> > nanosecond-level accuracy, though, within a single computer system or
> > application node, so I can measure the duration, order, and correlation
> > of events, with extremely good precision.
> >
> > A+
> > Osvaldo
> >> David Holmes wrote:
> >>
> >>> I always come back to wondering why you need a high
> resolution value that is
> >>> tied to wall-clock time?
> >>>
> >> Convenience. Why do we need multiple clocks, why can't we have
> a single
> >> clock with nanosecond precision? Well we could if every computer was
> >> fitted with an atomic clock. So we have multiple clocks
> because that is
> >> easier to implement with available hardware. Yet a single high
> >> resolution clock synchronized to real time does appear to be possible
> >> (that appears to be what I get on the latest Ubuntu). Using the
> >> techniques applied by NTP it would also seem possible to synchronize
> >> System.nanoTime() to real time to provide typically microsecond
> >> resolution and a delta from real time kept below 0.1s or better.
> >>
> >> If you are an engineer familiar with the internal implementation, you
> >> will be happy to choose an appropriate clock for each purpose. Other
> >> people just want THE time, and don't bother them with all
> those details,
> >> UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding
> about why
> >> they can't have (sub)microsecond resolution when the processors have
> >> clock rates in the GHz.
> >>
> >> ;-)
> >>
> >> Mark Thornton
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at altair.cs.oswego.edu
> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >
> >
> > --
> > -----------------------------------------------------------------------
> > Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
> > osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
> > Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
> >
> >
> > ------------------------------------------------------------------------
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From joe.bowbeer at gmail.com  Wed May 21 04:56:32 2008
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 21 May 2008 01:56:32 -0700
Subject: [concurrency-interest] ScheduledThreadPoolExecutor long running
	tasks should not be Queued up
In-Reply-To: <957752.78815.qm@web58712.mail.re1.yahoo.com>
References: <957752.78815.qm@web58712.mail.re1.yahoo.com>
Message-ID: <31f2a7bd0805210156u1370f009jb2c14e545a96904e@mail.gmail.com>

On Tue, May 20, 2008 at 11:15 PM, sankar adabala wrote:

>
> I am using STE for polling a large set of devices.STE works fine and scales
> up well.But i noticed the following.
>
> Q1)
> Suppose a TaskA is scheduled repeatedly every 4 minutes and if the Task
> takes more than 4 minutes and in this case  takes 6 minutes,then the next
> scheduling is happening at 6th minute.I don't want this behaviour, what i
> wanted is the task has to be scheduled every 4minutes and the absolute
> scheduling time should be a multiple of 4.
>
> Is there a mechanism in STE to specify a task to be executed every 4minutes
> or multiple of 4 in case the task takes more time.
>
>
> Q2)
> What is the tolerance in seconds we can expect in scheduling ,suppose if a
> task is scheduled every 4 minutes ,does it gets scheduled exactly every 4
> minutes or in the range [3 minutes 59 secs to 4 minutes 1 sec].
>
>
> I have a workaround for Q1.I skip the scheduling if it doesn't fall in the
> range[3 minutes 59 secs to 4 minutes 1 sec ie a smoothing interval of 1
> sec].
>
>
> please let me know if there are better ways to handle this situation.
>
>
> Thanks,
> Sankar Krishna



Your workaround sounds OK to me.  It is similar, by the way, to the sample
code in the TimerTask doc that punts if the scheduled execution time was too
far in the past.

Concerning punctuality, the scheduled executor will have the best chance if
you allocate enough threads (see corePoolSize) such that there's always an
idle thread when the next task should begin.  One thread for each long
running task in your case.

--Joe
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080521/1218f488/attachment.html 

From harneetk12 at yahoo.com  Wed May 21 08:33:01 2008
From: harneetk12 at yahoo.com (harneet kaur)
Date: Wed, 21 May 2008 05:33:01 -0700 (PDT)
Subject: [concurrency-interest] Simulating infinite loop issue of
	Hashmap.containsKey()
Message-ID: <461738.27195.qm@web37107.mail.mud.yahoo.com>

Hi,
We want to simulate the infinite loop issue of HashMap
caused by containsKey(). 
What is the way to do that? 

We plan to do the following towards simulating it:
Initialize the Hashmap with a small size and have lots
of threads adding to it and also calling containsKey()
simulateously such that the hashmap resizing occurs
around the time conatinsKey() is getting called.
We assume that the above scenario would cause the
infinite loop to occur at some point.

Is that correct?

Regards,
Harneet


      

From mthornton at optrak.co.uk  Wed May 21 09:01:16 2008
From: mthornton at optrak.co.uk (Mark Thornton)
Date: Wed, 21 May 2008 14:01:16 +0100
Subject: [concurrency-interest] Simulating infinite loop issue
	of	Hashmap.containsKey()
In-Reply-To: <461738.27195.qm@web37107.mail.mud.yahoo.com>
References: <461738.27195.qm@web37107.mail.mud.yahoo.com>
Message-ID: <48341D1C.9080007@optrak.co.uk>

harneet kaur wrote:
> Hi,
> We want to simulate the infinite loop issue of HashMap
> caused by containsKey(). 
> What is the way to do that? 
>
> We plan to do the following towards simulating it:
> Initialize the Hashmap with a small size and have lots
> of threads adding to it and also calling containsKey()
> simulateously such that the hashmap resizing occurs
> around the time conatinsKey() is getting called.
> We assume that the above scenario would cause the
> infinite loop to occur at some point.
>
> Is that correct?
>
> Regards,
> Harneet
You shouldn't be trying to use HashMap from multiple threads without 
using a synchronizing wrapper.

Mark Thornton


From harneetk12 at yahoo.com  Wed May 21 09:26:55 2008
From: harneetk12 at yahoo.com (harneet kaur)
Date: Wed, 21 May 2008 06:26:55 -0700 (PDT)
Subject: [concurrency-interest] Simulating infinite loop issue of
	Hashmap.containsKey()
In-Reply-To: <48341D1C.9080007@optrak.co.uk>
Message-ID: <179557.51338.qm@web37104.mail.mud.yahoo.com>

Hi Mark,
  I understand your point. But here our intention is
to simulate the infinite loop issue of Hashmap
(basically for some internal testing purpose).
So for that we are not synchronizing over the Hashmap
and would like to know what else to do to simulate the
issue.

Regards,
Harneet
--- Mark Thornton <mthornton at optrak.co.uk> wrote:

> harneet kaur wrote:
> > Hi,
> > We want to simulate the infinite loop issue of
> HashMap
> > caused by containsKey(). 
> > What is the way to do that? 
> >
> > We plan to do the following towards simulating it:
> > Initialize the Hashmap with a small size and have
> lots
> > of threads adding to it and also calling
> containsKey()
> > simulateously such that the hashmap resizing
> occurs
> > around the time conatinsKey() is getting called.
> > We assume that the above scenario would cause the
> > infinite loop to occur at some point.
> >
> > Is that correct?
> >
> > Regards,
> > Harneet
> You shouldn't be trying to use HashMap from multiple
> threads without 
> using a synchronizing wrapper.
> 
> Mark Thornton
> 
> 



      

From osvaldo at visionnaire.com.br  Wed May 21 10:05:07 2008
From: osvaldo at visionnaire.com.br (Osvaldo Pinali Doederlein)
Date: Wed, 21 May 2008 11:05:07 -0300
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMOHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEMOHLAA.dcholmes@optusnet.com.au>
Message-ID: <48342C13.7050501@visionnaire.com.br>

Summarizing this thread, I'd conclude that the folks designing 
processors don't care enough for the problems of poor code bastards like 
us. :) To mention a single idea, maybe the CPU could have a dedicated 
oscillator that's used only for feeding the RTSC (or a similar 
register), not being affected by the clock/energy management policies. 
Considering the large amount of transistors that modern CPUs dedicate to 
media-oriented ISA extensions, I'm not asking a lot; a good timer would 
be very important for many applications including media, realtime, etc. 
Some CPUs even have dedicated hardware for real random number 
generation, why can't we have something much simpler like a stable 
high-res clock? It sucks. I don't see a big problem in SMP systems, you 
could elect a single CPU to be the source of timing, and if some code 
running in another CPU needs the realtime clock it would pay a penalty 
for a cross-call (IIRC this piggybacks on the FSB protocol just ike 
cache coherence and should have a cost similar to a CAS), and even with 
that penaly we'd have much higher precision and acuracy than the 
miserable currentTimeMillis(). Finally, in most computers (desktop to 
mobile/embedded) we're not going to see SMP (multi-socket) any time 
soon, because multicore technology is providing parallelism for 
everything below midrange servers... a few years ago I'd see some 
technophiles/enthusiasts/gamers buying horribly expensive dual-socket 
systems, but today this doesn't happen anymore, people who want a 'God 
Box' will just buy a quad-core CPU. So, even tricks that "work only for 
uniprocessors" should be good enough for most machines for a long while.

A+
Osvaldo
> Hmm well the cycle counter is *bad* enough that it's not used as a timing
> source by the major OSes anymore. The Linux folk have abandoned it until we
> have cycle counters that are synchronized across cores. Windows swapped it
> for the pmTimer back in the XP SP2 days and only uses it on systems where it
> can't cause a problem. Of course you can also get utilities from Intel and
> AMD to synchronize the TSC across processors if you really want to use it.
> Solaris can still use the TSC on x86 but, as far as I know, performs its own
> synchronization.
>
> If you need a monotonic counter then the cycle counter is only useful on
> uniprocessors/single-cores. And of couse you need to disable sped-step or
> any other energy saving mechanism that alters the cycle counter frequency.
>
> So caveat emptor. ;-)
>
> Cheers,
> David Holmes
>
>    
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jeremy
>> Manson
>> Sent: Wednesday, 21 May 2008 5:41 PM
>> To: Osvaldo Pinali Doederlein
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>
>>
>> Hi, Osvaldo,
>>
>> This is what we tried.  There are two issues:
>>
>> 1) The CPU can slow down the cycle counter to save speed, and
>>
>> 2) The cycle counter can be different on each CPU on systems with
>> multiple processors.
>>
>> It's good enough for a lot of uses, although you can imagine the
>> confusion it causes from time to time.
>>
>> 					Jeremy
>>
>>
>> Osvaldo Pinali Doederlein wrote:
>>      
>>> Hi,
>>>
>>> Now I'll fully expose my ignorance on the subject of computer time
>>> measurement: Why can't a system rely on the CPU, which supposedly
>>> contains a high frequency, stable oscillator for its clock? For
>>>        
>> example,
>>      
>>> on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns
>>> precision, and I can read this counter with the RDRTSC opcode. I should
>>> be able to adjust this value to nanoseconds (in this case just a divide
>>> by 2), add it to the boot-time wallclock provided by the motherboard
>>> (since the RTSC is zeroed at boot), and be done with it - using
>>>        
>> only the
>>      
>>> RTSC as the source for all clock needs afterwards. I know there are
>>> problems like energy-efficient CPUs that change their clock frequency
>>> dynamically, and synchronization with NTP or Windows Time servers, but
>>> the system should be able to monitor these factors and adjust the
>>> nano-clock, just like it does for the low-precision clock.
>>>
>>> BTW, I don't need an atomic-quality clock that's extremely
>>>        
>> accurate with
>>      
>>> the real time of my timezone... even in distributed systems, I
>>>        
>> only need
>>      
>>> a reasonably good accuracy within a cluster or the local
>>>        
>> network. I need
>>      
>>> nanosecond-level accuracy, though, within a single computer system or
>>> application node, so I can measure the duration, order, and correlation
>>> of events, with extremely good precision.
>>>
>>> A+
>>> Osvaldo
>>>        
>>>> David Holmes wrote:
>>>>
>>>>          
>>>>> I always come back to wondering why you need a high
>>>>>            
>> resolution value that is
>>      
>>>>> tied to wall-clock time?
>>>>>
>>>>>            
>>>> Convenience. Why do we need multiple clocks, why can't we have
>>>>          
>> a single
>>      
>>>> clock with nanosecond precision? Well we could if every computer was
>>>> fitted with an atomic clock. So we have multiple clocks
>>>>          
>> because that is
>>      
>>>> easier to implement with available hardware. Yet a single high
>>>> resolution clock synchronized to real time does appear to be possible
>>>> (that appears to be what I get on the latest Ubuntu). Using the
>>>> techniques applied by NTP it would also seem possible to synchronize
>>>> System.nanoTime() to real time to provide typically microsecond
>>>> resolution and a delta from real time kept below 0.1s or better.
>>>>
>>>> If you are an engineer familiar with the internal implementation, you
>>>> will be happy to choose an appropriate clock for each purpose. Other
>>>> people just want THE time, and don't bother them with all
>>>>          
>> those details,
>>      
>>>> UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding
>>>>          
>> about why
>>      
>>>> they can't have (sub)microsecond resolution when the processors have
>>>> clock rates in the GHz.
>>>>
>>>> ;-)
>>>>
>>>> Mark Thornton
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at altair.cs.oswego.edu
>>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>          
>>> --
>>> -----------------------------------------------------------------------
>>> Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>>> osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>>> Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at altair.cs.oswego.edu
>>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>        
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>      
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>    


-- 
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080521/9483608b/attachment.html 

From jdmarshall at gmail.com  Wed May 21 13:35:13 2008
From: jdmarshall at gmail.com (jason marshall)
Date: Wed, 21 May 2008 10:35:13 -0700
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEMOHLAA.dcholmes@optusnet.com.au>
References: <4833D212.6040608@cs.umd.edu>
	<NFBBKALFDCPFIDBNKAPCMEMOHLAA.dcholmes@optusnet.com.au>
Message-ID: <3cf41bb90805211035q60962a4bufef6f7857d55a33c@mail.gmail.com>

Speed-Step is only getting more complicated as time progresses.

I believe it was on the Penryn processors that Intel introduced a new
variant of their thermal management that can overclock a core when it
is saturated but the die is still running below the safe thermal
threshold.  This is intended to provide a little speedup to serial
tasks, and presumably also helps a little with spiky usage patterns.

If this actually works as advertised, they'll expand this behavior in
future chip designs.


(The irony of this design change is that the 'warmup phase' in
benchmarks has finally achieved the literal sense of the term.)

-Jason







On Wed, May 21, 2008 at 1:30 AM, David Holmes <dcholmes at optusnet.com.au> wrote:
> Hmm well the cycle counter is *bad* enough that it's not used as a timing
> source by the major OSes anymore. The Linux folk have abandoned it until we
> have cycle counters that are synchronized across cores. Windows swapped it
> for the pmTimer back in the XP SP2 days and only uses it on systems where it
> can't cause a problem. Of course you can also get utilities from Intel and
> AMD to synchronize the TSC across processors if you really want to use it.
> Solaris can still use the TSC on x86 but, as far as I know, performs its own
> synchronization.
>
> If you need a monotonic counter then the cycle counter is only useful on
> uniprocessors/single-cores. And of couse you need to disable sped-step or
> any other energy saving mechanism that alters the cycle counter frequency.
>
> So caveat emptor. ;-)
>
> Cheers,
> David Holmes
>
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu
>> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jeremy
>> Manson
>> Sent: Wednesday, 21 May 2008 5:41 PM
>> To: Osvaldo Pinali Doederlein
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] High resolution timers and JSR 310
>>
>>
>> Hi, Osvaldo,
>>
>> This is what we tried.  There are two issues:
>>
>> 1) The CPU can slow down the cycle counter to save speed, and
>>
>> 2) The cycle counter can be different on each CPU on systems with
>> multiple processors.
>>
>> It's good enough for a lot of uses, although you can imagine the
>> confusion it causes from time to time.
>>
>>                                       Jeremy
>>
>>
>> Osvaldo Pinali Doederlein wrote:
>> > Hi,
>> >
>> > Now I'll fully expose my ignorance on the subject of computer time
>> > measurement: Why can't a system rely on the CPU, which supposedly
>> > contains a high frequency, stable oscillator for its clock? For
>> example,
>> > on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns
>> > precision, and I can read this counter with the RDRTSC opcode. I should
>> > be able to adjust this value to nanoseconds (in this case just a divide
>> > by 2), add it to the boot-time wallclock provided by the motherboard
>> > (since the RTSC is zeroed at boot), and be done with it - using
>> only the
>> > RTSC as the source for all clock needs afterwards. I know there are
>> > problems like energy-efficient CPUs that change their clock frequency
>> > dynamically, and synchronization with NTP or Windows Time servers, but
>> > the system should be able to monitor these factors and adjust the
>> > nano-clock, just like it does for the low-precision clock.
>> >
>> > BTW, I don't need an atomic-quality clock that's extremely
>> accurate with
>> > the real time of my timezone... even in distributed systems, I
>> only need
>> > a reasonably good accuracy within a cluster or the local
>> network. I need
>> > nanosecond-level accuracy, though, within a single computer system or
>> > application node, so I can measure the duration, order, and correlation
>> > of events, with extremely good precision.
>> >
>> > A+
>> > Osvaldo
>> >> David Holmes wrote:
>> >>
>> >>> I always come back to wondering why you need a high
>> resolution value that is
>> >>> tied to wall-clock time?
>> >>>
>> >> Convenience. Why do we need multiple clocks, why can't we have
>> a single
>> >> clock with nanosecond precision? Well we could if every computer was
>> >> fitted with an atomic clock. So we have multiple clocks
>> because that is
>> >> easier to implement with available hardware. Yet a single high
>> >> resolution clock synchronized to real time does appear to be possible
>> >> (that appears to be what I get on the latest Ubuntu). Using the
>> >> techniques applied by NTP it would also seem possible to synchronize
>> >> System.nanoTime() to real time to provide typically microsecond
>> >> resolution and a delta from real time kept below 0.1s or better.
>> >>
>> >> If you are an engineer familiar with the internal implementation, you
>> >> will be happy to choose an appropriate clock for each purpose. Other
>> >> people just want THE time, and don't bother them with all
>> those details,
>> >> UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding
>> about why
>> >> they can't have (sub)microsecond resolution when the processors have
>> >> clock rates in the GHz.
>> >>
>> >> ;-)
>> >>
>> >> Mark Thornton
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at altair.cs.oswego.edu
>> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >
>> >
>> > --
>> > -----------------------------------------------------------------------
>> > Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
>> > osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
>> > Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
>> >
>> >
>> > ------------------------------------------------------------------------
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at altair.cs.oswego.edu
>> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
- Jason


From dcholmes at optusnet.com.au  Wed May 21 18:46:56 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 22 May 2008 08:46:56 +1000
Subject: [concurrency-interest] High resolution timers and JSR 310
In-Reply-To: <48342C13.7050501@visionnaire.com.br>
Message-ID: <NFBBKALFDCPFIDBNKAPCAENBHLAA.dcholmes@optusnet.com.au>

Hi Osvaldo,

I don't think it's a case of "the folks designing processors don't care
enough for the problems of poor code bastards like us" . I think it's more a
case of "those software folk using instructions for things they weren't
intended" :-)

<opinion>
The TSC is a per-core cycle counter that was intended to do simple low-level
measurements of the number of cycles it took to do things - simple
instruction execution costs, maybe measure costs of cache misses and the
like. I don't think it was intended to measure complex sequences that could
include context switches, and it wasn't a per-thread counter. But the
software folk took it and using (imprecise) frequency information they tried
to turn it into a general-purpose high resolution timing mechanism. And
guess what - they failed!.
</opinion>

The next generation of timer support - the High Performance Event Timer
(HPET) - is what was proposed as a solution for this. But it is nowhere near
as fast as a TSC access - order of a few microseconds.

BTW multi-core doesn't (yet) avoid the TSC issues of SMP. The TSC being
synchronized across cores is a feature of the next generation of processors,
not the millions already out there.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Osvaldo
Pinali Doederlein
  Sent: Thursday, 22 May 2008 12:05 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] High resolution timers and JSR 310


  Summarizing this thread, I'd conclude that the folks designing processors
don't care enough for the problems of poor code bastards like us. :) To
mention a single idea, maybe the CPU could have a dedicated oscillator
that's used only for feeding the RTSC (or a similar register), not being
affected by the clock/energy management policies. Considering the large
amount of transistors that modern CPUs dedicate to media-oriented ISA
extensions, I'm not asking a lot; a good timer would be very important for
many applications including media, realtime, etc. Some CPUs even have
dedicated hardware for real random number generation, why can't we have
something much simpler like a stable high-res clock? It sucks. I don't see a
big problem in SMP systems, you could elect a single CPU to be the source of
timing, and if some code running in another CPU needs the realtime clock it
would pay a penalty for a cross-call (IIRC this piggybacks on the FSB
protocol just ike cache coherence and should have a cost similar to a CAS),
and even with that penaly we'd have much higher precision and acuracy than
the miserable currentTimeMillis(). Finally, in most computers (desktop to
mobile/embedded) we're not going to see SMP (multi-socket) any time soon,
because multicore technology is providing parallelism for everything below
midrange servers... a few years ago I'd see some
technophiles/enthusiasts/gamers buying horribly expensive dual-socket
systems, but today this doesn't happen anymore, people who want a 'God Box'
will just buy a quad-core CPU. So, even tricks that "work only for
uniprocessors" should be good enough for most machines for a long while.

  A+
  Osvaldo

Hmm well the cycle counter is *bad* enough that it's not used as a timing
source by the major OSes anymore. The Linux folk have abandoned it until we
have cycle counters that are synchronized across cores. Windows swapped it
for the pmTimer back in the XP SP2 days and only uses it on systems where it
can't cause a problem. Of course you can also get utilities from Intel and
AMD to synchronize the TSC across processors if you really want to use it.
Solaris can still use the TSC on x86 but, as far as I know, performs its own
synchronization.

If you need a monotonic counter then the cycle counter is only useful on
uniprocessors/single-cores. And of couse you need to disable sped-step or
any other energy saving mechanism that alters the cycle counter frequency.

So caveat emptor. ;-)

Cheers,
David Holmes

  -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jeremy
Manson
Sent: Wednesday, 21 May 2008 5:41 PM
To: Osvaldo Pinali Doederlein
Cc: concurrency-interest
Subject: Re: [concurrency-interest] High resolution timers and JSR 310


Hi, Osvaldo,

This is what we tried.  There are two issues:

1) The CPU can slow down the cycle counter to save speed, and

2) The cycle counter can be different on each CPU on systems with
multiple processors.

It's good enough for a lot of uses, although you can imagine the
confusion it causes from time to time.

					Jeremy


Osvaldo Pinali Doederlein wrote:
    Hi,

Now I'll fully expose my ignorance on the subject of computer time
measurement: Why can't a system rely on the CPU, which supposedly
contains a high frequency, stable oscillator for its clock? For
      example,
    on a 2.0GHz CPU, the cycle counter provides a timer with 0,5ns
precision, and I can read this counter with the RDRTSC opcode. I should
be able to adjust this value to nanoseconds (in this case just a divide
by 2), add it to the boot-time wallclock provided by the motherboard
(since the RTSC is zeroed at boot), and be done with it - using
      only the
    RTSC as the source for all clock needs afterwards. I know there are
problems like energy-efficient CPUs that change their clock frequency
dynamically, and synchronization with NTP or Windows Time servers, but
the system should be able to monitor these factors and adjust the
nano-clock, just like it does for the low-precision clock.

BTW, I don't need an atomic-quality clock that's extremely
      accurate with
    the real time of my timezone... even in distributed systems, I
      only need
    a reasonably good accuracy within a cluster or the local
      network. I need
    nanosecond-level accuracy, though, within a single computer system or
application node, so I can measure the duration, order, and correlation
of events, with extremely good precision.

A+
Osvaldo
      David Holmes wrote:

        I always come back to wondering why you need a high
          resolution value that is
    tied to wall-clock time?

          Convenience. Why do we need multiple clocks, why can't we have
        a single
    clock with nanosecond precision? Well we could if every computer was
fitted with an atomic clock. So we have multiple clocks
        because that is
    easier to implement with available hardware. Yet a single high
resolution clock synchronized to real time does appear to be possible
(that appears to be what I get on the latest Ubuntu). Using the
techniques applied by NTP it would also seem possible to synchronize
System.nanoTime() to real time to provide typically microsecond
resolution and a delta from real time kept below 0.1s or better.

If you are an engineer familiar with the internal implementation, you
will be happy to choose an appropriate clock for each purpose. Other
people just want THE time, and don't bother them with all
        those details,
    UTC, UT1, NTP, HPET, TSC, ... Nor are they very understanding
        about why
    they can't have (sub)microsecond resolution when the processors have
clock rates in the GHz.

;-)

Mark Thornton

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


        --
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223


------------------------------------------------------------------------

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
      _______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




--
-----------------------------------------------------------------------
Osvaldo Pinali Doederlein                   Visionnaire Inform?tica S/A
osvaldo at visionnaire.com.br                http://www.visionnaire.com.br
Arquiteto de Tecnologia                          +55 (41) 337-1000 #223
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080522/572309c5/attachment-0001.html 

From martinrb at google.com  Sat May 24 18:50:27 2008
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 24 May 2008 15:50:27 -0700
Subject: [concurrency-interest] Executors change from JDK5 to JDK6
Message-ID: <1ccfd1c10805241550jf483f57paaee2acefc983f57@mail.gmail.com>

Doug wrote:

> (We're sorry of course about the signature bug. And about
> the signature fix. Hard to win here...)

As one of the guilty party, I'm sorry too.

And I have a sad story.

I was the host for the Silicon Valley Java Users Group last week,
and I noticed that one of the presenters was using eclipse with
JDK 5.  I gently teased him about not having upgraded to JDK 6 yet,
but he said his work project would not compile under JDK 6,
so he was "stuck" on JDK 5.  He demoed an attempt to switch
to compiling with JDK 6, and yup, there was invokeAll in the
list of compile errors.

Martin

From jed at atlassian.com  Sun May 25 21:45:22 2008
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 26 May 2008 11:45:22 +1000
Subject: [concurrency-interest] Simulating infinite loop issue
	of	Hashmap.containsKey()
In-Reply-To: <179557.51338.qm@web37104.mail.mud.yahoo.com>
References: <179557.51338.qm@web37104.mail.mud.yahoo.com>
Message-ID: <483A1632.5000903@atlassian.com>

harneet,

if it is just for testing why not simply replace the HashMap with an 
InfinitelyLoopingHashMap which you can easily control (and easily stop 
looping when finished testing). The conditions that cause a HashMap to 
corrupt are not particularly easy to evoke by hand.

http://lightbody.net/blog/2005/07/hashmapget_can_cause_an_infini.html

harneet kaur wrote:
> Hi Mark,
>   I understand your point. But here our intention is
> to simulate the infinite loop issue of Hashmap
> (basically for some internal testing purpose).
> So for that we are not synchronizing over the Hashmap
> and would like to know what else to do to simulate the
> issue.
>
> Regards,
> Harneet
> --- Mark Thornton <mthornton at optrak.co.uk> wrote:
>
>   
>> harneet kaur wrote:
>>     
>>> Hi,
>>> We want to simulate the infinite loop issue of
>>>       
>> HashMap
>>     
>>> caused by containsKey(). 
>>> What is the way to do that? 
>>>
>>> We plan to do the following towards simulating it:
>>> Initialize the Hashmap with a small size and have
>>>       
>> lots
>>     
>>> of threads adding to it and also calling
>>>       
>> containsKey()
>>     
>>> simulateously such that the hashmap resizing
>>>       
>> occurs
>>     
>>> around the time conatinsKey() is getting called.
>>> We assume that the above scenario would cause the
>>> infinite loop to occur at some point.
>>>
>>> Is that correct?
>>>
>>> Regards,
>>> Harneet
>>>       
>> You shouldn't be trying to use HashMap from multiple
>> threads without 
>> using a synchronizing wrapper.
>>     


From hallorant at gmail.com  Fri May 30 09:44:44 2008
From: hallorant at gmail.com (Tim Halloran)
Date: Fri, 30 May 2008 09:44:44 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
Message-ID: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>

On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" I
ran into this code which is a 2-thread solution for a Lock.  (I don't think
you need the book to understand my question.)

public class Peterson implements Lock {
    // thread-local index, 0 or 1
    private volatile boolean[] flag = new boolean[2];
    private volatile int victim;

    public void lock() {
        int i = ThreadID.get();
        int j = 1 - i;
        flag[i] = true; // I'm interested
        victim = i; // You go first
        while (flag[j] && victim == i) {
            // wait
        }
    }

    public void unlock() {
        int i = ThreadID.get();
        flag[i] = false;
    }
}

The issue is the declaration of the boolean array "flag" (not the lock
algorithm).  The authors note (on page 25) that the fields need to be
volatile in Java to work, however, I don't think "flag" is working the way
they expect.  Am I wrong?

First, I think "flag" should be declared "final" not "volatile" as the field
should never be mutated.

    private final boolean[] flag = new boolean[2];

Here, my confusion starts as I think this can't be fixed in Java.  I believe
there is no way to indicate that the primitive elements of an array are
volatile (e.g., give them the memory model semantics the authors desire).

I was hoping someone here would know for sure.  I was going to let the
authors know--but wanted to check my facts :-)

Best Regards,
Tim Halloran
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/703d088c/attachment.html 

From tim at peierls.net  Fri May 30 09:58:44 2008
From: tim at peierls.net (Tim Peierls)
Date: Fri, 30 May 2008 09:58:44 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
Message-ID: <63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>

I noticed that, too, but I figured the authors were skating (sloppily)
around the unfortunate fact that you can't have arrays of volatiles. They
should certainly have footnoted it as pseudo-code. They should have just
used two volatiles, flag1 and flag2.

--tim

On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com> wrote:

> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" I
> ran into this code which is a 2-thread solution for a Lock.  (I don't think
> you need the book to understand my question.)
>
> public class Peterson implements Lock {
>     // thread-local index, 0 or 1
>     private volatile boolean[] flag = new boolean[2];
>     private volatile int victim;
>
>     public void lock() {
>         int i = ThreadID.get();
>         int j = 1 - i;
>         flag[i] = true; // I'm interested
>         victim = i; // You go first
>         while (flag[j] && victim == i) {
>             // wait
>         }
>     }
>
>     public void unlock() {
>         int i = ThreadID.get();
>         flag[i] = false;
>     }
> }
>
> The issue is the declaration of the boolean array "flag" (not the lock
> algorithm).  The authors note (on page 25) that the fields need to be
> volatile in Java to work, however, I don't think "flag" is working the way
> they expect.  Am I wrong?
>
> First, I think "flag" should be declared "final" not "volatile" as the
> field should never be mutated.
>
>     private final boolean[] flag = new boolean[2];
>
> Here, my confusion starts as I think this can't be fixed in Java.  I
> believe there is no way to indicate that the primitive elements of an array
> are volatile (e.g., give them the memory model semantics the authors
> desire).
>
> I was hoping someone here would know for sure.  I was going to let the
> authors know--but wanted to check my facts :-)
>
> Best Regards,
> Tim Halloran
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/0f303a3f/attachment.html 

From hallorant at gmail.com  Fri May 30 10:13:44 2008
From: hallorant at gmail.com (Tim Halloran)
Date: Fri, 30 May 2008 10:13:44 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
	<63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>
Message-ID: <a36ab4bc0805300713x4f59e8e2pf2b0c3deff07ec65@mail.gmail.com>

Thanks Tim, I'll send the authors an errata...they can decide what to do.

On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net> wrote:

> I noticed that, too, but I figured the authors were skating (sloppily)
> around the unfortunate fact that you can't have arrays of volatiles. They
> should certainly have footnoted it as pseudo-code. They should have just
> used two volatiles, flag1 and flag2.
>
> --tim
>
> On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com> wrote:
>
>> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming" I
>> ran into this code which is a 2-thread solution for a Lock.  (I don't think
>> you need the book to understand my question.)
>>
>> public class Peterson implements Lock {
>>     // thread-local index, 0 or 1
>>     private volatile boolean[] flag = new boolean[2];
>>     private volatile int victim;
>>
>>     public void lock() {
>>         int i = ThreadID.get();
>>         int j = 1 - i;
>>         flag[i] = true; // I'm interested
>>         victim = i; // You go first
>>         while (flag[j] && victim == i) {
>>             // wait
>>         }
>>     }
>>
>>     public void unlock() {
>>         int i = ThreadID.get();
>>         flag[i] = false;
>>     }
>> }
>>
>> The issue is the declaration of the boolean array "flag" (not the lock
>> algorithm).  The authors note (on page 25) that the fields need to be
>> volatile in Java to work, however, I don't think "flag" is working the way
>> they expect.  Am I wrong?
>>
>> First, I think "flag" should be declared "final" not "volatile" as the
>> field should never be mutated.
>>
>>     private final boolean[] flag = new boolean[2];
>>
>> Here, my confusion starts as I think this can't be fixed in Java.  I
>> believe there is no way to indicate that the primitive elements of an array
>> are volatile (e.g., give them the memory model semantics the authors
>> desire).
>>
>> I was hoping someone here would know for sure.  I was going to let the
>> authors know--but wanted to check my facts :-)
>>
>> Best Regards,
>> Tim Halloran
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/67ef3da2/attachment.html 

From forax at univ-mlv.fr  Fri May 30 11:57:39 2008
From: forax at univ-mlv.fr (=?ISO-8859-1?Q?R=E9mi_Forax?=)
Date: Fri, 30 May 2008 17:57:39 +0200
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <a36ab4bc0805300713x4f59e8e2pf2b0c3deff07ec65@mail.gmail.com>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>	<63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>
	<a36ab4bc0805300713x4f59e8e2pf2b0c3deff07ec65@mail.gmail.com>
Message-ID: <484023F3.1060605@univ-mlv.fr>

Or use an AtomicIntegerArray.

R?mi

Tim Halloran a ?crit :
> Thanks Tim, I'll send the authors an errata...they can decide what to do.
>
> On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net 
> <mailto:tim at peierls.net>> wrote:
>
>     I noticed that, too, but I figured the authors were skating
>     (sloppily) around the unfortunate fact that you can't have arrays
>     of volatiles. They should certainly have footnoted it as
>     pseudo-code. They should have just used two volatiles, flag1 and
>     flag2.
>
>     --tim
>
>     On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com
>     <mailto:hallorant at gmail.com>> wrote:
>
>         On page 27 of Herlihy & Shavit's "The Art of Multiprocessor
>         Programming" I ran into this code which is a 2-thread solution
>         for a Lock.  (I don't think you need the book to understand my
>         question.)
>
>         public class Peterson implements Lock {
>             // thread-local index, 0 or 1
>             private volatile boolean[] flag = new boolean[2];
>             private volatile int victim;
>
>             public void lock() {
>                 int i = ThreadID.get();
>                 int j = 1 - i;
>                 flag[i] = true; // I'm interested
>                 victim = i; // You go first
>                 while (flag[j] && victim == i) {
>                     // wait
>                 }
>             }
>
>             public void unlock() {
>                 int i = ThreadID.get();
>                 flag[i] = false;
>             }
>         }
>
>         The issue is the declaration of the boolean array "flag" (not
>         the lock algorithm).  The authors note (on page 25) that the
>         fields need to be volatile in Java to work, however, I don't
>         think "flag" is working the way they expect.  Am I wrong?
>
>         First, I think "flag" should be declared "final" not
>         "volatile" as the field should never be mutated.
>
>             private final boolean[] flag = new boolean[2];
>
>         Here, my confusion starts as I think this can't be fixed in
>         Java.  I believe there is no way to indicate that the
>         primitive elements of an array are volatile (e.g., give them
>         the memory model semantics the authors desire).
>
>         I was hoping someone here would know for sure.  I was going to
>         let the authors know--but wanted to check my facts :-)
>
>         Best Regards,
>         Tim Halloran
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at altair.cs.oswego.edu
>         <mailto:Concurrency-interest at altair.cs.oswego.edu>
>         http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   


From kasirohan at yahoo.com  Fri May 30 12:54:45 2008
From: kasirohan at yahoo.com (kasi sankaralingam)
Date: Fri, 30 May 2008 09:54:45 -0700 (PDT)
Subject: [concurrency-interest] Concurrnet hash map reader and concurrent
	hash map performance
Message-ID: <164348.48258.qm@web52103.mail.re2.yahoo.com>

Any logic for removing the Concurrnet hash map reader in jdk 1.5 and replace it with
concurrent hashmap, the latter is much slower than the reader thanks
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/686a13d2/attachment.html 

From tim at peierls.net  Fri May 30 13:13:40 2008
From: tim at peierls.net (Tim Peierls)
Date: Fri, 30 May 2008 13:13:40 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <484023F3.1060605@univ-mlv.fr>
References: <a36ab4bc0805300644v319f527dt1f84ca6ca2d249da@mail.gmail.com>
	<63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>
	<a36ab4bc0805300713x4f59e8e2pf2b0c3deff07ec65@mail.gmail.com>
	<484023F3.1060605@univ-mlv.fr>
Message-ID: <63b4e4050805301013l346c9ca5s1e95a433939a755d@mail.gmail.com>

I imagine the authors didn't want to have to introduce and explain the use
of AtomicIntegerArray to implement two volatile booleans.

--tim

On Fri, May 30, 2008 at 11:57 AM, R?mi Forax <forax at univ-mlv.fr> wrote:

> Or use an AtomicIntegerArray.
>
> R?mi
>
> Tim Halloran a ?crit :
> > Thanks Tim, I'll send the authors an errata...they can decide what to do.
> >
> > On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net
> > <mailto:tim at peierls.net>> wrote:
> >
> >     I noticed that, too, but I figured the authors were skating
> >     (sloppily) around the unfortunate fact that you can't have arrays
> >     of volatiles. They should certainly have footnoted it as
> >     pseudo-code. They should have just used two volatiles, flag1 and
> >     flag2.
> >
> >     --tim
> >
> >     On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com
> >     <mailto:hallorant at gmail.com>> wrote:
> >
> >         On page 27 of Herlihy & Shavit's "The Art of Multiprocessor
> >         Programming" I ran into this code which is a 2-thread solution
> >         for a Lock.  (I don't think you need the book to understand my
> >         question.)
> >
> >         public class Peterson implements Lock {
> >             // thread-local index, 0 or 1
> >             private volatile boolean[] flag = new boolean[2];
> >             private volatile int victim;
> >
> >             public void lock() {
> >                 int i = ThreadID.get();
> >                 int j = 1 - i;
> >                 flag[i] = true; // I'm interested
> >                 victim = i; // You go first
> >                 while (flag[j] && victim == i) {
> >                     // wait
> >                 }
> >             }
> >
> >             public void unlock() {
> >                 int i = ThreadID.get();
> >                 flag[i] = false;
> >             }
> >         }
> >
> >         The issue is the declaration of the boolean array "flag" (not
> >         the lock algorithm).  The authors note (on page 25) that the
> >         fields need to be volatile in Java to work, however, I don't
> >         think "flag" is working the way they expect.  Am I wrong?
> >
> >         First, I think "flag" should be declared "final" not
> >         "volatile" as the field should never be mutated.
> >
> >             private final boolean[] flag = new boolean[2];
> >
> >         Here, my confusion starts as I think this can't be fixed in
> >         Java.  I believe there is no way to indicate that the
> >         primitive elements of an array are volatile (e.g., give them
> >         the memory model semantics the authors desire).
> >
> >         I was hoping someone here would know for sure.  I was going to
> >         let the authors know--but wanted to check my facts :-)
> >
> >         Best Regards,
> >         Tim Halloran
> >
> >         _______________________________________________
> >         Concurrency-interest mailing list
> >         Concurrency-interest at altair.cs.oswego.edu
> >         <mailto:Concurrency-interest at altair.cs.oswego.edu>
> >
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> >
> > ------------------------------------------------------------------------
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/9eaaa12b/attachment.html 

From dcholmes at optusnet.com.au  Fri May 30 16:16:43 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 31 May 2008 06:16:43 +1000
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <63b4e4050805300658w1a5da3a6s699f7c619645ac0e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEOIHLAA.dcholmes@optusnet.com.au>

They should have stuck with the original form of Peterson's algorithm and
gotten it right. ;-)

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Tim Peierls
  Sent: Friday, 30 May 2008 11:59 PM
  To: Tim Halloran
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Volatile and primitive arrays


  I noticed that, too, but I figured the authors were skating (sloppily)
around the unfortunate fact that you can't have arrays of volatiles. They
should certainly have footnoted it as pseudo-code. They should have just
used two volatiles, flag1 and flag2.

  --tim


  On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com> wrote:

    On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming"
I ran into this code which is a 2-thread solution for a Lock.  (I don't
think you need the book to understand my question.)

    public class Peterson implements Lock {
        // thread-local index, 0 or 1
        private volatile boolean[] flag = new boolean[2];
        private volatile int victim;

        public void lock() {
            int i = ThreadID.get();
            int j = 1 - i;
            flag[i] = true; // I'm interested
            victim = i; // You go first
            while (flag[j] && victim == i) {
                // wait
            }
        }

        public void unlock() {
            int i = ThreadID.get();
            flag[i] = false;
        }
    }

    The issue is the declaration of the boolean array "flag" (not the lock
algorithm).  The authors note (on page 25) that the fields need to be
volatile in Java to work, however, I don't think "flag" is working the way
they expect.  Am I wrong?

    First, I think "flag" should be declared "final" not "volatile" as the
field should never be mutated.

        private final boolean[] flag = new boolean[2];

    Here, my confusion starts as I think this can't be fixed in Java.  I
believe there is no way to indicate that the primitive elements of an array
are volatile (e.g., give them the memory model semantics the authors
desire).

    I was hoping someone here would know for sure.  I was going to let the
authors know--but wanted to check my facts :-)

    Best Regards,
    Tim Halloran

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at altair.cs.oswego.edu
    http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080531/63d61709/attachment.html 

From gkorland at gmail.com  Fri May 30 22:05:34 2008
From: gkorland at gmail.com (Guy Korland)
Date: Sat, 31 May 2008 05:05:34 +0300
Subject: [concurrency-interest] Volatile and primitive arrays
Message-ID: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>

Hi Tim

If you really think about it, since java 5, it's not an error.
Since all the author was trying to do is to promise that thread will read a
fresh value.

When a thread call flag[j] it really access a volatile reference "flag"
which flashes the all memory since java 5 and then read the place in the
array which in fact was already refreshed.

Guy


>Thanks Tim, I'll send the authors an errata...they can decide what to do.

>On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net> wrote:

> I noticed that, too, but I figured the authors were skating (sloppily)
> around the unfortunate fact that you can't have arrays of volatiles. They
> should certainly have footnoted it as pseudo-code. They should have just
> used two volatiles, flag1 and flag2.
>
> --tim
>
> On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com> wrote:
>
>> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming"
I
>> ran into this code which is a 2-thread solution for a Lock.  (I don't
think
>> you need the book to understand my question.)
>>
>> public class Peterson implements Lock {
>>     // thread-local index, 0 or 1
>>     private volatile boolean[] flag = new boolean[2];
>>     private volatile int victim;
>>
>>     public void lock() {
>>         int i = ThreadID.get();
>>         int j = 1 - i;
>>         flag[i] = true; // I'm interested
>>         victim = i; // You go first
>>         while (flag[j] && victim == i) {
>>             // wait
>>         }
>>     }
>>
>>     public void unlock() {
>>         int i = ThreadID.get();
>>         flag[i] = false;
>>     }
>> }
>>
>> The issue is the declaration of the boolean array "flag" (not the lock
>> algorithm).  The authors note (on page 25) that the fields need to be
>> volatile in Java to work, however, I don't think "flag" is working the
way
>> they expect.  Am I wrong?
>>
>> First, I think "flag" should be declared "final" not "volatile" as the
>> field should never be mutated.
>>
>>     private final boolean[] flag = new boolean[2];
>>
>> Here, my confusion starts as I think this can't be fixed in Java.  I
>> believe there is no way to indicate that the primitive elements of an
array
>> are volatile (e.g., give them the memory model semantics the authors
>> desire).
>>
>> I was hoping someone here would know for sure.  I was going to let the
>> authors know--but wanted to check my facts :-)
>>
>> Best Regards,
>> Tim Halloran
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080531/5c367fe4/attachment-0001.html 

From tim at peierls.net  Fri May 30 22:18:01 2008
From: tim at peierls.net (Tim Peierls)
Date: Fri, 30 May 2008 22:18:01 -0400
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>
References: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>
Message-ID: <63b4e4050805301918h35058125y88e030c2f3d85287@mail.gmail.com>

The visibility effects of volatile only apply when you have a read of a
field following a write of that field. Successive reads of a field don't
have any special properties.

--tim

On Fri, May 30, 2008 at 10:05 PM, Guy Korland <gkorland at gmail.com> wrote:

> Hi Tim
>
> If you really think about it, since java 5, it's not an error.
> Since all the author was trying to do is to promise that thread will read a
> fresh value.
>
> When a thread call flag[j] it really access a volatile reference "flag"
> which flashes the all memory since java 5 and then read the place in the
> array which in fact was already refreshed.
>
> Guy
>
>
> >Thanks Tim, I'll send the authors an errata...they can decide what to do.
>
> >On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net> wrote:
>
> > I noticed that, too, but I figured the authors were skating (sloppily)
> > around the unfortunate fact that you can't have arrays of volatiles. They
> > should certainly have footnoted it as pseudo-code. They should have just
> > used two volatiles, flag1 and flag2.
> >
> > --tim
> >
> > On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com>
> wrote:
> >
> >> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor Programming"
> I
> >> ran into this code which is a 2-thread solution for a Lock.  (I don't
> think
> >> you need the book to understand my question.)
> >>
> >> public class Peterson implements Lock {
> >>     // thread-local index, 0 or 1
> >>     private volatile boolean[] flag = new boolean[2];
> >>     private volatile int victim;
> >>
> >>     public void lock() {
> >>         int i = ThreadID.get();
> >>         int j = 1 - i;
> >>         flag[i] = true; // I'm interested
> >>         victim = i; // You go first
> >>         while (flag[j] && victim == i) {
> >>             // wait
> >>         }
> >>     }
> >>
> >>     public void unlock() {
> >>         int i = ThreadID.get();
> >>         flag[i] = false;
> >>     }
> >> }
> >>
> >> The issue is the declaration of the boolean array "flag" (not the lock
> >> algorithm).  The authors note (on page 25) that the fields need to be
> >> volatile in Java to work, however, I don't think "flag" is working the
> way
> >> they expect.  Am I wrong?
> >>
> >> First, I think "flag" should be declared "final" not "volatile" as the
> >> field should never be mutated.
> >>
> >>     private final boolean[] flag = new boolean[2];
> >>
> >> Here, my confusion starts as I think this can't be fixed in Java.  I
> >> believe there is no way to indicate that the primitive elements of an
> array
> >> are volatile (e.g., give them the memory model semantics the authors
> >> desire).
> >>
> >> I was hoping someone here would know for sure.  I was going to let the
> >> authors know--but wanted to check my facts :-)
> >>
> >> Best Regards,
> >> Tim Halloran
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at altair.cs.oswego.edu
> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080530/1fd5ba3a/attachment.html 

From gkorland at gmail.com  Fri May 30 22:24:12 2008
From: gkorland at gmail.com (Guy Korland)
Date: Sat, 31 May 2008 05:24:12 +0300
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <63b4e4050805301918h35058125y88e030c2f3d85287@mail.gmail.com>
References: <79be5fa30805301905i26296cb7y23976bfb55bbf4bf@mail.gmail.com>
	<63b4e4050805301918h35058125y88e030c2f3d85287@mail.gmail.com>
Message-ID: <79be5fa30805301924p50e3eacbsab575480de76befb@mail.gmail.com>

Are you sure? This will break the "Happens-before order" property that
volatile provides.

Guy

On Sat, May 31, 2008 at 5:18 AM, Tim Peierls <tim at peierls.net> wrote:

> The visibility effects of volatile only apply when you have a read of a
> field following a write of that field. Successive reads of a field don't
> have any special properties.
>
> --tim
>
>
> On Fri, May 30, 2008 at 10:05 PM, Guy Korland <gkorland at gmail.com> wrote:
>
>> Hi Tim
>>
>> If you really think about it, since java 5, it's not an error.
>> Since all the author was trying to do is to promise that thread will read
>> a fresh value.
>>
>> When a thread call flag[j] it really access a volatile reference "flag"
>> which flashes the all memory since java 5 and then read the place in the
>> array which in fact was already refreshed.
>>
>> Guy
>>
>>
>> >Thanks Tim, I'll send the authors an errata...they can decide what to do.
>>
>> >On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net> wrote:
>>
>> > I noticed that, too, but I figured the authors were skating (sloppily)
>> > around the unfortunate fact that you can't have arrays of volatiles.
>> They
>> > should certainly have footnoted it as pseudo-code. They should have just
>> > used two volatiles, flag1 and flag2.
>> >
>> > --tim
>> >
>> > On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com>
>> wrote:
>> >
>> >> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor
>> Programming" I
>> >> ran into this code which is a 2-thread solution for a Lock.  (I don't
>> think
>> >> you need the book to understand my question.)
>> >>
>> >> public class Peterson implements Lock {
>> >>     // thread-local index, 0 or 1
>> >>     private volatile boolean[] flag = new boolean[2];
>> >>     private volatile int victim;
>> >>
>> >>     public void lock() {
>> >>         int i = ThreadID.get();
>> >>         int j = 1 - i;
>> >>         flag[i] = true; // I'm interested
>> >>         victim = i; // You go first
>> >>         while (flag[j] && victim == i) {
>> >>             // wait
>> >>         }
>> >>     }
>> >>
>> >>     public void unlock() {
>> >>         int i = ThreadID.get();
>> >>         flag[i] = false;
>> >>     }
>> >> }
>> >>
>> >> The issue is the declaration of the boolean array "flag" (not the lock
>> >> algorithm).  The authors note (on page 25) that the fields need to be
>> >> volatile in Java to work, however, I don't think "flag" is working the
>> way
>> >> they expect.  Am I wrong?
>> >>
>> >> First, I think "flag" should be declared "final" not "volatile" as the
>> >> field should never be mutated.
>> >>
>> >>     private final boolean[] flag = new boolean[2];
>> >>
>> >> Here, my confusion starts as I think this can't be fixed in Java.  I
>> >> believe there is no way to indicate that the primitive elements of an
>> array
>> >> are volatile (e.g., give them the memory model semantics the authors
>> >> desire).
>> >>
>> >> I was hoping someone here would know for sure.  I was going to let the
>> >> authors know--but wanted to check my facts :-)
>> >>
>> >> Best Regards,
>> >> Tim Halloran
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at altair.cs.oswego.edu
>> >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at altair.cs.oswego.edu
>> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080531/f9d706bf/attachment.html 

From dcholmes at optusnet.com.au  Sat May 31 05:26:44 2008
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 31 May 2008 19:26:44 +1000
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <79be5fa30805301924p50e3eacbsab575480de76befb@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEOKHLAA.dcholmes@optusnet.com.au>

Yes he's sure :) The only "happens-before" property that volatile has is as
follows: a volatile write happens-before a subsequent volatile read of the
value written.

If you only have reads you don't have happens-before.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Guy Korland
  Sent: Saturday, 31 May 2008 12:24 PM
  To: Tim Peierls
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Volatile and primitive arrays


  Are you sure? This will break the "Happens-before order" property that
volatile provides.

  Guy


  On Sat, May 31, 2008 at 5:18 AM, Tim Peierls <tim at peierls.net> wrote:

    The visibility effects of volatile only apply when you have a read of a
field following a write of that field. Successive reads of a field don't
have any special properties.

    --tim



    On Fri, May 30, 2008 at 10:05 PM, Guy Korland <gkorland at gmail.com>
wrote:

      Hi Tim

      If you really think about it, since java 5, it's not an error.
      Since all the author was trying to do is to promise that thread will
read a fresh value.

      When a thread call flag[j] it really access a volatile reference
"flag" which flashes the all memory since java 5 and then read the place in
the array which in fact was already refreshed.

      Guy


      >Thanks Tim, I'll send the authors an errata...they can decide what to
do.
      >On Fri, May 30, 2008 at 9:58 AM, Tim Peierls <tim at peierls.net> wrote:

      > I noticed that, too, but I figured the authors were skating
(sloppily)
      > around the unfortunate fact that you can't have arrays of volatiles.
They
      > should certainly have footnoted it as pseudo-code. They should have
just
      > used two volatiles, flag1 and flag2.
      >
      > --tim
      >
      > On Fri, May 30, 2008 at 9:44 AM, Tim Halloran <hallorant at gmail.com>
wrote:
      >
      >> On page 27 of Herlihy & Shavit's "The Art of Multiprocessor
Programming" I
      >> ran into this code which is a 2-thread solution for a Lock.  (I
don't think
      >> you need the book to understand my question.)
      >>
      >> public class Peterson implements Lock {
      >>     // thread-local index, 0 or 1
      >>     private volatile boolean[] flag = new boolean[2];
      >>     private volatile int victim;
      >>
      >>     public void lock() {
      >>         int i = ThreadID.get();
      >>         int j = 1 - i;
      >>         flag[i] = true; // I'm interested
      >>         victim = i; // You go first
      >>         while (flag[j] && victim == i) {
      >>             // wait
      >>         }
      >>     }
      >>
      >>     public void unlock() {
      >>         int i = ThreadID.get();
      >>         flag[i] = false;
      >>     }
      >> }
      >>
      >> The issue is the declaration of the boolean array "flag" (not the
lock
      >> algorithm).  The authors note (on page 25) that the fields need to
be
      >> volatile in Java to work, however, I don't think "flag" is working
the way
      >> they expect.  Am I wrong?
      >>
      >> First, I think "flag" should be declared "final" not "volatile" as
the
      >> field should never be mutated.
      >>
      >>     private final boolean[] flag = new boolean[2];
      >>
      >> Here, my confusion starts as I think this can't be fixed in Java.
I
      >> believe there is no way to indicate that the primitive elements of
an array
      >> are volatile (e.g., give them the memory model semantics the
authors
      >> desire).
      >>
      >> I was hoping someone here would know for sure.  I was going to let
the
      >> authors know--but wanted to check my facts :-)
      >>
      >> Best Regards,
      >> Tim Halloran
      >>
      >> _______________________________________________
      >> Concurrency-interest mailing list
      >> Concurrency-interest at altair.cs.oswego.edu
      >> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
      >>
      >>
      >


      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at altair.cs.oswego.edu
      http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest





-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20080531/44a25275/attachment-0001.html 

From gregg at cytetech.com  Sat May 31 09:16:20 2008
From: gregg at cytetech.com (Gregg Wonderly)
Date: Sat, 31 May 2008 08:16:20 -0500
Subject: [concurrency-interest] Volatile and primitive arrays
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEOKHLAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCCEOKHLAA.dcholmes@optusnet.com.au>
Message-ID: <48414FA4.9030905@cytetech.com>

David Holmes wrote:
> Yes he's sure :) The only "happens-before" property that volatile has is 
> as follows: a volatile write happens-before a subsequent volatile read 
> of the value written.
>  
> If you only have reads you don't have happens-before.

The interesting point, of course, is that if the array value reference is not 
written, then the array dereferences for read, don't come before any write of 
the same dereferenced array value.

Those trying to be clever with volatiles and arrays (not me, but I can just see 
this question comming) might think that the next question of interest would be, 
if I code the following:

	volatile int []arr;
	
	public void setValue( int idx, int val ) {
		arr[idx] = val;
		arr = arr;
	}

	public int getValue( int idx ) {
		arr = arr;
		return arr[ idx ];
	}

Is there now a meaningful happens before for the elements of the array?  I'd 
guess that the compiler might remove the no-op assignment.

Gregg Wonderly

From robert.nicholson at gmail.com  Sat May 31 20:50:58 2008
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Sat, 31 May 2008 19:50:58 -0500
Subject: [concurrency-interest] Throttling the submission with
	ThreadPoolExecutor
Message-ID: <3DCE3824-6DF8-43BB-9EB3-6CE7FC3F904C@gmail.com>

I would like to ask how you ensure that order of submissions is  
maintained yet still be able to throttle those submissions when using  
a bounded queue? It seems that CallerRunsPolicy allows the Task to be  
run earlier than those currently in the working queue and that in our  
particular case is unacceptable. So what is the typical approach to  
guaranteeing order at the same time still being able to throttle the  
rate of submissions. Presumably this relys on an approach whereby you  
resubmit those resubmissions upon rejection after some interval to  
increase the likehood that the resubmission itself isn't rejected?

