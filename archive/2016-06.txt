From oleksandr.otenko at gmail.com  Wed Jun  1 03:02:26 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 1 Jun 2016 08:02:26 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464742768769-13481.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com>
Message-ID: <CANkgWKjJYpMshc4vxeg-paafzXTZ5yqwbLQsgZLKLgsq=yjupA@mail.gmail.com>

That's right, it can reify to that order, if nothing breaks. It can even
eliminate both writes if it can prove it corresponds to the same
specification and agrees with program order of both threads and their java
language semantics. In reality the complexity of the proof is the limit
that ensures nothing funny happens.
On 1 Jun 2016 02:09, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> I think most of the confusion rests with the A = B.
> Roland made it clear that = means "identity".
> I wasn't inventing anything, I simply misunderstood the meaning of "=".
>
> Now in the context of strict total order, you seem to suggest that there
> can
> be some adhoc equals relation (like "equal-by-name") that is associated
> with
> but distinct from the strict binary operator?
> If "=" means identity in the weak total order rules (b) antisymmetric),
> then
> why wouldn't it mean identity in d) Trichotomy ?
> I didn't follow that part.
>
> When I talk of the "definition of a total order", this is what I mean:
> You can think of a total order "definition" in two ways:
> 1.  Is just the enumeration of all of the pair-wise relations; so when
> evaluating x op y, simply search through the enumerated pair-wise
> relations.
> That's not what I meant by "defining a total order"
>
> 2.  Specify the definition of a "rule" (algorithm, whatever), e.g. the
> mathematical operator <= applied to the set of integers (I  shouldn't need
> to explain that); when evaluating x <= y, I can simply apply the rule - it
> is this that the JMM does not do - it leaves it up to each execution, again
> as long as it satisfies the formal mathematical rules and a few additional
> constraints (program order, memory sequential consistency).
>
> I hope that difference is clear, it is to me.
> Now I suppose the lack of a "rule" doesn't matter,  after all using a rule
> one can generate the enumerated pair-wise relations, and then just apply
> step 1.
>
> Upon thinking about it more deeply, what bothers me is the potential
> divergence between the "physical reality" of a CPU executing an instruction
> (at a time x) and the execution's total order.
>
> Specifically, the JMM allows the following for an execution:
> Time x:   A(x=1)
> Time x + 100:  B(y=1)
>
> yet the execution can reify its total order as {(B, A)}; this seems at odds
> with the (flawed) notion of a volatile write as entailing an "immediate
> flush to memory"
>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13481.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160601/61dae614/attachment.html>

From aleksey.shipilev at oracle.com  Wed Jun  1 04:27:37 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 1 Jun 2016 11:27:37 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464742768769-13481.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com> <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com>
Message-ID: <574E9C79.4010104@oracle.com>

On 06/01/2016 03:59 AM, thurstonn wrote:
> When I talk of the "definition of a total order", this is what I mean:
> You can think of a total order "definition" in two ways:
> 1.  Is just the enumeration of all of the pair-wise relations; so when
> evaluating x op y, simply search through the enumerated pair-wise relations. 
> That's not what I meant by "defining a total order"
> 
> 2.  Specify the definition of a "rule" (algorithm, whatever), e.g. the
> mathematical operator <= applied to the set of integers (I  shouldn't need
> to explain that); when evaluating x <= y, I can simply apply the rule - it
> is this that the JMM does not do - it leaves it up to each execution, again
> as long as it satisfies the formal mathematical rules and a few additional
> constraints (program order, memory sequential consistency).

The problematic part is that once you assign an integer to every SA,
allowing to SAs to have the same integer assignment derails the total
order of SAs, under "different SAs are not equal" equality rule. You
cannot reasonably say that "x = 1" and "r1 = x" are the
same/equal/interchangeable SAs if they have the same timestamps.

You can enumerate SAs with integers if you enforce that no two SAs have
the same integer: this will basically enumerate the linear order under SO.


> I hope that difference is clear, it is to me.
> Now I suppose the lack of a "rule" doesn't matter,  after all using a rule
> one can generate the enumerated pair-wise relations, and then just apply
> step 1.
> 
> Upon thinking about it more deeply, what bothers me is the potential
> divergence between the "physical reality" of a CPU executing an instruction
> (at a time x) and the execution's total order.

That's because the physical reality and the behavior of the abstract
machine which behavior JMM rules describe *are* different. You seem to
keep thinking that SAs are the physical machine actions, but they are
not! Those are abstract machine actions, and they don't have to be
manifested in physical reality. Everyone in this thread keep saying
that, in different forms.

It is completely plausible to ask abstract machine to do:

        volatile x, y;
  ---------------------------
     x = 1     |   y = 2
     r1 = y    |   r2 = x;

...and see the physical machine do:

 ---------------------------
     r1 = 2    |   r2 = 1;

Even though you have "x = 1" and "y = 2" in the SO. That does not oblige
actual physical machine to execute all exact actions in SO, as long as
it's not detectable by a program. That's what "as if"-ness in sequential
consistency gives you.

That "divergence" is a blessing in disguise: it is the wiggle room that
allows software and hardware to optimize execution (e.g. do something
that is not exactly spelled in the program, in exact order spelled in
the program, etc), while still adhering to abstract machine rules that
rule out unwanted outcomes.

> Specifically, the JMM allows the following for an execution:
> Time x:   A(x=1)
> Time x + 100:  B(y=1)
> 
> yet the execution can reify its total order as {(B, A)}; this seems at odds
> with the (flawed) notion of a volatile write as entailing an "immediate
> flush to memory"

That is why the notion of "immediate flush to memory" is *wrong* mental
model to begin with. Since Java 5, JMM carefully avoids this by avoiding
talking about "immediate" (and actually, about the time itself), "flush"
(and actually, about the implementation), "memory" (and other actual
hardware implementation).

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160601/51e1c37e/attachment.sig>

From peter.levart at gmail.com  Wed Jun  1 04:35:18 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 10:35:18 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <15269016.0KoCuDZ9Z7@tapsy>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy>
Message-ID: <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>

Hi Jens,


On 05/31/2016 08:48 PM, Jens Wilke wrote:
> On Tuesday 31 May 2016 18:50:53 Peter Levart wrote:
>
>> The only place where load factor is coded is in the capacity() method.
>> The '2' in if (2 * capacity(inserted + 1) > curLen) above is to convert
>> from capacity to table length (as two table slots are used for each
>> key/value pair in the table). The 3/2 factor in the next line is just
>> coincidentally equal to 1/loadFactor and has nothing to do with it. It
>> is used as a kind of "hysteresis loop" that prevents oscillating around
>> the tipping point (the point where capacity() function jumps by a factor
>> of 2). The table is rehashed when, as mentioned, the number of inserted
>> entries (including tombstoned entries) grows over loadFactor * capacity.
>> Rehashing also gets rid of tombstones.
>>
>> Imagine a table that contains just one tombstone when it is rehashed. if
>> the new table length was computed simply as newLen = 2 *
>> capacity(inserted - tombstoned + 1), the new table would have the same
>> length as the old one and we would end up with new rehashed table with
>> space for a single entry before another rehashing was necessary. The 3/2
>> factor in newLen = 2 * capacity((inserted - tombstoned) * 3 / 2 + 1)
>> guarantees that rehashing is not needed for at least another size()/2 +
>> 1 insertions. When this factor is 1 <= factor < 2, it does not affect
>> normal growth when entries are just inserted and never removed.
> Exactly, that is the strange thing. But, usually the load factor only determines _when_
> to extend, you do not need it to calculate the next size. The next expansion
> size is all the time  tab.length * 2, since your size is a power of two, and you
> do not insert more then one element at a time, so that there is a need to expand more.
>
> The load factor is a "magic number". It is inside expand(), in checkSize() and in capacity().

Just in checkSize() and capacity() - not in rehash(). It is hardcoded 
because of the specific nature of linear-probe hashtable that is very 
sensitive on its value. values larger than 2/3 are not sensible as 
"clustering" effects explode and values much lower than 2/3 make the 
table occupy to much space.

>
> I don't see a real need for the methods checkSize() and capacity() in that form. At least
> for the current functionality without shrink support.

There is shrink support in current functionality (see below).

>   Just an idea how this could look
> more obvious (hopefully...):
>
> static final int SLOTS_PER_ENTRY = 2;
> static final int LOAD_PERCENT = 66;
> . . .
> int maximumCapacity = tab.length / SLOTS_PER_ENTRY * LOAD_PERCENT / 100;
> if (inserted > maximumCapactiy) {
>    // needs expansion or tombstone cleanup!
>    int newLen = tab.length;
>    if (size() > maximumCapacity) {
>      // needs expansion!
>      newLen = tab.length * 2;
>      if (newLen < 0) { throw new OutOfMemoryException("...."); }
>    }
>    Object[] newTab = new Object[newLen];
>    // do the rehash
> }
>
> Interestingly your current code would also shrink the table, since
> you calculate the next table length based on the real size. But its in the put() path :)

That's right. The consequence of "lazy" removals. The reason it is only 
in put() path is that only put[IfAbsent] may insert new entry into table 
and therefore increase load past the limit of 2/3. But, as said, it may 
be beneficial to trigger rehashing (with expunging of tombstones and 
possible shrinking) also in remove() in order to more eagerly optimize 
lookup time by preventing building-up future "clusters".

I think the strategy of rehash() (whether to expand or shrink or not) 
should not depend on whether it is triggered by put() or remove() as 
this is just the last action on the table that pushed it into a state 
which needs rehashing while majority of actions preceding it could be 
just the opposite. We can/should not predict the behavior of the program 
from the past actions as it may be contra-productive. The most general 
strategy, I think, should assume nothing from past actions and only look 
at minimizing the worst-case effects considering all possible future 
actions.

>
> BTW:
>
>     public int size() {
>       synchronized (lock) {
>         return inserted - tombstoned;
>       }
>     }
>
> Why not have the size as variable, and calculate occupation = size + tombstone for the internal
> purposes?

It could be that way, yes. Then 'size' could be volatile and we would 
not need the lock in size(). But OTOH, size() method is not critical and 
computing the occupancy at each insertion in order to check for need to 
rehash is more important to be simple.

>
> Cheers,
>
> Jens
>

Thanks for your input.

Regards, Peter


From peter.levart at gmail.com  Wed Jun  1 05:05:28 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 11:05:28 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
 volatile read?
In-Reply-To: <2282396.Th39PTn6K5@tapsy>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <2282396.Th39PTn6K5@tapsy>
Message-ID: <02e87b3c-e2fc-70a1-5e46-e5121ef3d4f3@gmail.com>

Hi Jens,


On 05/31/2016 09:00 PM, Jens Wilke wrote:
> On Tuesday 31 May 2016 12:44:09 Peter Levart wrote:
>> Hi concurrency experts,
>>
>> As part of an attempt to optimize the ClassValue API for small number of
>> elements per Class in OpenJDK, I came up with a proposal to leverage
>> ConcurrentHashMap, which improves the footprint overhead considerably
>> . . .
>  From concurrency / performance point of view, I wonder whether it is
> worthwhile to remove the second volatile read in get().
>
> So it would be:
>
>     public V get(Object key) {
>       Object[] tab = table;
>       int len = tab.length;
>       int i = firstIndex(key, len);
>       while (true) {
>          Object item = tab[i]; // 1st read key...
>       ...
>
> To do this, additional stores to table are needed in put:
>
>       tab[i + 1] = value; // 1st write value...
>       table = tab;
>       tab[i] = key; // ...then publish the key
>       table = tab;
>
> Cheers,
>
> Jens
>

The 1st volatile read (tab = table) is needed for correctly observing 
the contents of the array as rehash() may have swapped it with newly 
constructed array which was filled with relaxed writes.

The 2nd volatile read (item = getVolatile(tab, i)) is needed for 
correctly observing the contents of the 'item' key (needed later in 
equals() computation) as put(), for example, may have installed new key 
into an empty slot. Your proposition for additional stores in put() are 
not enough. For example:

PutThread:

      tab[i + 1] = value;
      table = tab;
      tab[i] = key; // not a volatile write
      // preemption and GetThread kicks in...

GetThread (racing with PutThread):

     Object item = tab[i];
     // racy read may see the key reference written by PutThread but may 
also see the underlying object only partly initialized in following 
actions (equals())

PutThread:

     table = tab;
     // volatile write that came to late and was not accompanied by a 
volatile read of 'table' in GetThread.


The 3rd volatile read for the associated value (return (V) 
getVolatile(tab, i + 1)) is needed because of put() (but not 
putIfAbsent()) and replace() methods that only install new value for the 
existing key. If those methods were not needed, the 3rd read could be a 
normal read.

Regards, Peter


From concurrency at kuli.org  Wed Jun  1 05:11:25 2016
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 1 Jun 2016 11:11:25 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
Message-ID: <574EA6BD.7000900@kuli.org>

Am 01.06.2016 um 10:35 schrieb Peter Levart:
> But, as said, it may
> be beneficial to trigger rehashing (with expunging of tombstones and
> possible shrinking) also in remove() in order to more eagerly optimize
> lookup time by preventing building-up future "clusters".

I wouldn't do it. You risk a "pumping effect" when many elements are
added and removed frequently.

What would be the use case? First filling the map, then removing most of
the elements and never add many again? I've never seen that.

And it's even more unlikely in ClassValue where removals will definitely
be the exception.

HashMap doesn't shrink either for good reasons.

-Michael



From jw_list at headissue.com  Wed Jun  1 06:33:34 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Wed, 01 Jun 2016 12:33:34 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
	volatile read?
In-Reply-To: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
Message-ID: <1789153.I0ozaZQFfo@tapsy>

(re send from yesterday, since post did not make it to the list....)

On Tuesday 31 May 2016 12:44:09 Peter Levart wrote:
> Hi concurrency experts,
> 
> As part of an attempt to optimize the ClassValue API for small number of 
> elements per Class in OpenJDK, I came up with a proposal to leverage 
> ConcurrentHashMap, which improves the footprint overhead considerably 
> . . .

>From concurrency / performance point of view, I wonder whether it is
worthwhile to remove the second volatile read in get().

So it would be:

   public V get(Object key) {
     Object[] tab = table;
     int len = tab.length;
     int i = firstIndex(key, len);
     while (true) {
        Object item = tab[i]; // 1st read key...
     ...

To do this, additional stores to table are needed in put:

     tab[i + 1] = value; // 1st write value...
     table = tab;
     tab[i] = key; // ...then publish the key
     table = tab;

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From jw_list at headissue.com  Wed Jun  1 06:45:57 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Wed, 01 Jun 2016 12:45:57 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
	volatile read?
In-Reply-To: <02e87b3c-e2fc-70a1-5e46-e5121ef3d4f3@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <2282396.Th39PTn6K5@tapsy> <02e87b3c-e2fc-70a1-5e46-e5121ef3d4f3@gmail.com>
Message-ID: <13574269.23mmr6uzQ2@tapsy>

On Wednesday 01 June 2016 11:05:28 Peter Levart wrote:
> PutThread:
> 
>       tab[i + 1] = value;
>       table = tab;
>       tab[i] = key; // not a volatile write
>       // preemption and GetThread kicks in...
> 
> GetThread (racing with PutThread):
> 
>      Object item = tab[i];
>      // racy read may see the key reference written by PutThread but may 
> also see the underlying object only partly initialized in following 
> actions (equals())

Interesting area to explore.

In case your example is the real execution sequence, there will be no partially
initialized object data visible, since this "happened before" volatile 
store in line 2 (table = tab).

But maybe the non volatile store gets reorderd before the volatile store?
I doubt whether this is possible with the new JMM, but I cannot pinpoint the
clause that says so. Any comments on this from the JMM gurus?

Since you use unsafe anyway, you could forbid the reordering explicitly:

PutThread:

     tab[i + 1] = value; // 1st write value...
     table = tab;
     U.storeFence();
     tab[i] = key; // ...then publish the key
     table = tab;

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From peter.levart at gmail.com  Wed Jun  1 07:12:00 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 13:12:00 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
 volatile read?
In-Reply-To: <13574269.23mmr6uzQ2@tapsy>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <2282396.Th39PTn6K5@tapsy> <02e87b3c-e2fc-70a1-5e46-e5121ef3d4f3@gmail.com>
 <13574269.23mmr6uzQ2@tapsy>
Message-ID: <e5a7c1e2-223a-0fd6-711b-46a0f4fcee77@gmail.com>

Hi Jens,


On 06/01/2016 12:45 PM, Jens Wilke wrote:
> On Wednesday 01 June 2016 11:05:28 Peter Levart wrote:
>> PutThread:
>>
>>        tab[i + 1] = value;
>>        table = tab;
>>        tab[i] = key; // not a volatile write
>>        // preemption and GetThread kicks in...
>>
>> GetThread (racing with PutThread):
>>
>>       Object item = tab[i];
>>       // racy read may see the key reference written by PutThread but may
>> also see the underlying object only partly initialized in following
>> actions (equals())
> Interesting area to explore.
>
> In case your example is the real execution sequence, there will be no partially
> initialized object data visible, since this "happened before" volatile
> store in line 2 (table = tab).

I'm talking about the key object whose reference is written after the 
volatile store in line 2.

>
> But maybe the non volatile store gets reorderd before the volatile store?
> I doubt whether this is possible with the new JMM, but I cannot pinpoint the
> clause that says so. Any comments on this from the JMM gurus?

Normal store can reorder before a volatile store. The other way around 
is not possible. But that's not an issue here.

>
> Since you use unsafe anyway, you could forbid the reordering explicitly:
>
> PutThread:
>
>       tab[i + 1] = value; // 1st write value...
>       table = tab;
>       U.storeFence();
>       tab[i] = key; // ...then publish the key
>       table = tab;
>
> Cheers,
>
> Jens
>

As said, reordering of write of key reference before the write of the 
table reference is not an issue here. Reordering of writes to fields of 
the key object after the write of key reference is the issue. You might 
say that volatile write to table in line 2 "flushes" those writes. It 
does, but only if it is accompanied by a corresponding volatile read 
that must be placed before reading those flushed writes, which is not 
guaranteed to happen here.


Regards, Peter


From peter.levart at gmail.com  Wed Jun  1 07:28:56 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 13:28:56 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <574EA6BD.7000900@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org>
Message-ID: <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>

Hi Michael,


Thanks for joining the conversation.


On 06/01/2016 11:11 AM, Michael Kuhlmann wrote:
> Am 01.06.2016 um 10:35 schrieb Peter Levart:
>> But, as said, it may
>> be beneficial to trigger rehashing (with expunging of tombstones and
>> possible shrinking) also in remove() in order to more eagerly optimize
>> lookup time by preventing building-up future "clusters".
> I wouldn't do it. You risk a "pumping effect" when many elements are
> added and removed frequently.
>
> What would be the use case? First filling the map, then removing most of
> the elements and never add many again? I've never seen that.
>
> And it's even more unlikely in ClassValue where removals will definitely
> be the exception.
>
> HashMap doesn't shrink either for good reasons.
>
> -Michael

So you would never shrink the table once it grows? Maybe this is the 
best strategy as other general hash table implementations have adopted 
the same strategy. The specificity of this implementation is in that the 
rehashing is triggered not because of size() growing over loadFactor * 
capacity but because of size() + tombstoned growing over loadFactor * 
capacity, so doubling the capacity with each rehashing is not always 
appropriate. So maybe the rehash() strategy should just decide between 
keeping the old capacity or doubling it in a way that doesn't cause 
frequent rehashing.

Regards, Peter

>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jw_list at headissue.com  Wed Jun  1 08:06:41 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Wed, 01 Jun 2016 14:06:41 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
	volatile read?
In-Reply-To: <e5a7c1e2-223a-0fd6-711b-46a0f4fcee77@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <13574269.23mmr6uzQ2@tapsy> <e5a7c1e2-223a-0fd6-711b-46a0f4fcee77@gmail.com>
Message-ID: <1868016.s77k9abJjr@tapsy>

On Wednesday 01 June 2016 13:12:00 Peter Levart wrote:
> It does, but only if it is accompanied by a corresponding volatile read 
> that must be placed before reading those flushed writes, which is not 
> guaranteed to happen here.

But that is exactly what you do in all your code paths:

  Object[] tab = table;

You never can see a partially initialized key object in case the reference
never becomes visible before the rest.

As you step over the line above, either the object contents are visible, because of
the first volatile store, or the reference and the contents are visible, because of
the second volatile store.

Do I miss something?

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From concurrency at kuli.org  Wed Jun  1 09:02:53 2016
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 1 Jun 2016 15:02:53 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
Message-ID: <574EDCFD.2090506@kuli.org>

Hi Peter,

Am 01.06.2016 um 13:28 schrieb Peter Levart:
> So you would never shrink the table once it grows? Maybe this is the
> best strategy as other general hash table implementations have adopted
> the same strategy. The specificity of this implementation is in that the
> rehashing is triggered not because of size() growing over loadFactor *
> capacity but because of size() + tombstoned growing over loadFactor *
> capacity, so doubling the capacity with each rehashing is not always
> appropriate. So maybe the rehash() strategy should just decide between
> keeping the old capacity or doubling it in a way that doesn't cause
> frequent rehashing.

Usually I wouldn't. But in your case, it's different because the list
might get filled with tombstones over the time, if it's never rehashed.

Maybe this can be solved differently. I wonder whether it was a good
idea to invent the skip factor in the tombstones. If you stay away from
that, and just iterate over all tombstones like as with normal values in
the get() method, you gain one advantage:

In the put() method, if a tombstone is identified, you can simply
overwrite it with the new value. In that case, the number of elements +
tombstones never will get much higher than the maximum number of
elements over all time.

And on top, the tombstone instance can be a singleton enum value (and
identified by identity check).

The disadvantage is that get() probably needs to iterate over some more
tombstones if many values have been removed.

But on the other side, when the same key gets removed and added several
times (which might be a common use case), you won't get a large list of
tombstones which would block some area of your array, raising the chance
of hash collisions with other elements.

So, in total I would assume that a skip-less tombstone approach is a
performance gain.

Or do I miss something?

-Michael


From jw_list at headissue.com  Wed Jun  1 09:46:54 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Wed, 01 Jun 2016 15:46:54 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <574EDCFD.2090506@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com> <574EDCFD.2090506@kuli.org>
Message-ID: <3981128.2D6OyacQQl@tapsy>

On Wednesday 01 June 2016 15:02:53 Michael Kuhlmann wrote:
> In the put() method, if a tombstone is identified, you can simply
> overwrite it with the new value.

Be careful. See the read path:

 123     public V get(Object key) {
 124         Object[] tab = table;
 125         int len = tab.length;
 126         int i = firstIndex(key, len);
 127         while (true) {
 128             Object item = getVolatile(tab, i); // 1st read key...
 129             if (key == item || (item != null && key.equals(item))) {
 130                 // possible race with removing an entry can cause the value
 131                 // to be observed cleared. If this happens, we know the key
 132                 // has/will be tombstoned just after that so null is returned
 133                 return (V) getVolatile(tab, i + 1);  // ...then value

What happens if a thread stops before line 133? For removal you can safely write a null. If you write
another value, you assign it possibly to a key that was there once there a while ago.

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From peter.levart at gmail.com  Wed Jun  1 09:50:21 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 15:50:21 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <574EDCFD.2090506@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org>
Message-ID: <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>

Hi Michael,


On 06/01/2016 03:02 PM, Michael Kuhlmann wrote:
> Hi Peter,
>
> Am 01.06.2016 um 13:28 schrieb Peter Levart:
>> So you would never shrink the table once it grows? Maybe this is the
>> best strategy as other general hash table implementations have adopted
>> the same strategy. The specificity of this implementation is in that the
>> rehashing is triggered not because of size() growing over loadFactor *
>> capacity but because of size() + tombstoned growing over loadFactor *
>> capacity, so doubling the capacity with each rehashing is not always
>> appropriate. So maybe the rehash() strategy should just decide between
>> keeping the old capacity or doubling it in a way that doesn't cause
>> frequent rehashing.
> Usually I wouldn't. But in your case, it's different because the list
> might get filled with tombstones over the time, if it's never rehashed.
>
> Maybe this can be solved differently. I wonder whether it was a good
> idea to invent the skip factor in the tombstones. If you stay away from
> that, and just iterate over all tombstones like as with normal values in
> the get() method, you gain one advantage:
>
> In the put() method, if a tombstone is identified, you can simply
> overwrite it with the new value. In that case, the number of elements +
> tombstones never will get much higher than the maximum number of
> elements over all time.
>
> And on top, the tombstone instance can be a singleton enum value (and
> identified by identity check).
>
> The disadvantage is that get() probably needs to iterate over some more
> tombstones if many values have been removed.
>
> But on the other side, when the same key gets removed and added several
> times (which might be a common use case), you won't get a large list of
> tombstones which would block some area of your array, raising the chance
> of hash collisions with other elements.
>
> So, in total I would assume that a skip-less tombstone approach is a
> performance gain.
>
> Or do I miss something?

I wish I could find a way to reuse the slot occupied by a tombstone. 
Keep in mind that get() must be lock-free. For that to work, I would 
need an atomic read from two consecutive array slots. Otherwise I could 
read a key belonging to one mapping and until I get to read the value 
slot, it will contain the value from another mapping. This phenomena 
could be called "Entry tearing".

Regards, Peter

>
> -Michael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.levart at gmail.com  Wed Jun  1 09:56:27 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 15:56:27 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
Message-ID: <8de61bf7-3b7b-7433-d179-992d2902fc14@gmail.com>



On 06/01/2016 03:50 PM, Peter Levart wrote:
>> Or do I miss something?
>
> I wish I could find a way to reuse the slot occupied by a tombstone. 
> Keep in mind that get() must be lock-free. For that to work, I would 
> need an atomic read from two consecutive array slots. Otherwise I 
> could read a key belonging to one mapping and until I get to read the 
> value slot, it will contain the value from another mapping. This 
> phenomena could be called "Entry tearing".
>
> Regards, Peter 

If I could keep the keys in the table after the entries are removed 
(simply by clearing the value slots), then I could reuse such "removed" 
entry at least for re-insertion of the same (equals) key. But it is 
usually expected from a Map that it releases both key and value when the 
entry is removed, so that they can get GC-ed.

Peter

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160601/53856b9e/attachment.html>

From concurrency at kuli.org  Wed Jun  1 10:05:53 2016
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Wed, 1 Jun 2016 16:05:53 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <8de61bf7-3b7b-7433-d179-992d2902fc14@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <8de61bf7-3b7b-7433-d179-992d2902fc14@gmail.com>
Message-ID: <574EEBC1.6000700@kuli.org>

Am 01.06.2016 um 15:56 schrieb Peter Levart:
> 
> 
> On 06/01/2016 03:50 PM, Peter Levart wrote:
>>> Or do I miss something?
>>
>> I wish I could find a way to reuse the slot occupied by a tombstone.
>> Keep in mind that get() must be lock-free. For that to work, I would
>> need an atomic read from two consecutive array slots. Otherwise I
>> could read a key belonging to one mapping and until I get to read the
>> value slot, it will contain the value from another mapping. This
>> phenomena could be called "Entry tearing".
>>
>> Regards, Peter 
> 
> If I could keep the keys in the table after the entries are removed
> (simply by clearing the value slots), then I could reuse such "removed"
> entry at least for re-insertion of the same (equals) key. But it is
> usually expected from a Map that it releases both key and value when the
> entry is removed, so that they can get GC-ed.
> 
> Peter
> 


You're right. I missed the point that the sequence put(a, x); remove(a);
put(b, y) would make it possible to return y in get(a).

Adding some kind of Entry object with key and value combined, like in
HashMap, is not an option? This would help with that problem. But of
course, it would raise the memory footprint slightly.

Hmh...

-Michael

From peter.levart at gmail.com  Wed Jun  1 10:08:08 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 16:08:08 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
 volatile read?
In-Reply-To: <1868016.s77k9abJjr@tapsy>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <13574269.23mmr6uzQ2@tapsy> <e5a7c1e2-223a-0fd6-711b-46a0f4fcee77@gmail.com>
 <1868016.s77k9abJjr@tapsy>
Message-ID: <c3d08965-af0a-a187-284d-6f737b4a591e@gmail.com>



On 06/01/2016 02:06 PM, Jens Wilke wrote:
> On Wednesday 01 June 2016 13:12:00 Peter Levart wrote:
>> It does, but only if it is accompanied by a corresponding volatile read
>> that must be placed before reading those flushed writes, which is not
>> guaranteed to happen here.
> But that is exactly what you do in all your code paths:
>
>    Object[] tab = table;
>
> You never can see a partially initialized key object in case the reference
> never becomes visible before the rest.
>
> As you step over the line above, either the object contents are visible, because of
> the first volatile store, or the reference and the contents are visible, because of
> the second volatile store.
>
> Do I miss something?
>
> Cheers,
>
> Jens
>

Ok, let's write down some more context:

GetThread:

      tab = table;
      // preemption and PutThread kicks in...

PutThread:

      key = new Key(...);
      table.get(key);
      ...
      tab[i + 1] = value;
      table = tab;
      tab[i] = key; // not a volatile write
      // preemption and GetThread kicks in...

GetThread (racing with PutThread):

     Object item = tab[i];
     // racy read may see the key reference written by PutThread but may 
also see the underlying object only partly initialized in following 
actions (equals())

PutThread:

     table = tab;
     // volatile write that came to late and was not accompanied by a 
volatile read of 'table' in GetThread.


You see, volatile read of table in GetThread might be performed before 
the volatile write of table in PutThread.


Regards, Peter


From peter.levart at gmail.com  Wed Jun  1 10:15:29 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 16:15:29 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <574EEBC1.6000700@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <8de61bf7-3b7b-7433-d179-992d2902fc14@gmail.com> <574EEBC1.6000700@kuli.org>
Message-ID: <5cd69a65-1cc5-c801-62b8-20f87a7f34ab@gmail.com>



On 06/01/2016 04:05 PM, Michael Kuhlmann wrote:
> Am 01.06.2016 um 15:56 schrieb Peter Levart:
>>
>> On 06/01/2016 03:50 PM, Peter Levart wrote:
>>>> Or do I miss something?
>>> I wish I could find a way to reuse the slot occupied by a tombstone.
>>> Keep in mind that get() must be lock-free. For that to work, I would
>>> need an atomic read from two consecutive array slots. Otherwise I
>>> could read a key belonging to one mapping and until I get to read the
>>> value slot, it will contain the value from another mapping. This
>>> phenomena could be called "Entry tearing".
>>>
>>> Regards, Peter
>> If I could keep the keys in the table after the entries are removed
>> (simply by clearing the value slots), then I could reuse such "removed"
>> entry at least for re-insertion of the same (equals) key. But it is
>> usually expected from a Map that it releases both key and value when the
>> entry is removed, so that they can get GC-ed.
>>
>> Peter
>>
>
> You're right. I missed the point that the sequence put(a, x); remove(a);
> put(b, y) would make it possible to return y in get(a).
>
> Adding some kind of Entry object with key and value combined, like in
> HashMap, is not an option? This would help with that problem. But of
> course, it would raise the memory footprint slightly.

If I do that, then I might as well change the table to use chained 
nodes. But that's what we already have in ConcurrentHashMap.

The whole point of this implementation was to have a more direct access 
to key/value which improves cache-locality and with it, access time.

>
> Hmh...
>
> -Michael

Regards, Peter


From peter.levart at gmail.com  Wed Jun  1 10:56:38 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 1 Jun 2016 16:56:38 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
 volatile read?
In-Reply-To: <c3d08965-af0a-a187-284d-6f737b4a591e@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <13574269.23mmr6uzQ2@tapsy> <e5a7c1e2-223a-0fd6-711b-46a0f4fcee77@gmail.com>
 <1868016.s77k9abJjr@tapsy> <c3d08965-af0a-a187-284d-6f737b4a591e@gmail.com>
Message-ID: <8d787c1f-35a6-09cc-861d-5f9fe073b522@gmail.com>

Sorry, I just noticed I made a mistake here...


On 06/01/2016 04:08 PM, Peter Levart wrote:
>
>
> Ok, let's write down some more context:
>
> GetThread:
>
>      tab = table;
>      // preemption and PutThread kicks in...
>
> PutThread:
>
>      key = new Key(...);
>      table.get(key);

The above line should read:

         table.put(key, ...);

>      ...
>      tab[i + 1] = value;
>      table = tab;
>      tab[i] = key; // not a volatile write
>      // preemption and GetThread kicks in...
>
> GetThread (racing with PutThread):
>
>     Object item = tab[i];
>     // racy read may see the key reference written by PutThread but 
> may also see the underlying object only partly initialized in 
> following actions (equals())
>
> PutThread:
>
>     table = tab;
>     // volatile write that came to late and was not accompanied by a 
> volatile read of 'table' in GetThread.
>
>
> You see, volatile read of table in GetThread might be performed before 
> the volatile write of table in PutThread.
>
>
> Regards, Peter
>


From jw_list at headissue.com  Wed Jun  1 11:01:28 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Wed, 01 Jun 2016 17:01:28 +0200
Subject: [concurrency-interest] LinearProbeHashtable / Remove second
	volatile read?
In-Reply-To: <c3d08965-af0a-a187-284d-6f737b4a591e@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <1868016.s77k9abJjr@tapsy> <c3d08965-af0a-a187-284d-6f737b4a591e@gmail.com>
Message-ID: <5778413.T2KAYiroEm@tapsy>

On Wednesday 01 June 2016 16:08:08 Peter Levart wrote:
> You see, volatile read of table in GetThread might be performed before 
> the volatile write of table in PutThread.

Thanks for detailed write up, I see my mistake!

That's a very important detail for a general hash table.

For hashing class instances you do not need equals at all. ;)

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From nathanila at gmail.com  Wed Jun  1 12:27:08 2016
From: nathanila at gmail.com (Nathan & Ila Reynolds)
Date: Wed, 1 Jun 2016 09:27:08 -0700
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <574EA6BD.7000900@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org>
Message-ID: <00a401d1bc22$72441730$56cc4590$@gmail.com>

> First filling the map, then removing most of the elements and never add many again? I've never seen that.

I have.  Consider the following 2 data points.

Large Sparse Collections are collections with the following property: 2 * size ≤ capacity && capacity > default.  This means that the collection had enough elements to cause the capacity to grow beyond the default and then a lot of elements were removed.  0.8% of the bytes of an average Java heap is consumed with Large Sparse collections.  0.4% of the bytes of an average Java heap is consumed with Large Sparse HashMaps.  0.1% of the bytes of an average Java heap is consumed with Large Sparse ConcurrentHashMaps.

Empty Used Collections are collections that contained at least 1 element at some point in time but are empty at the time of the heap dump.  0.3% of an average Java heap is filled with Empty Used Collections split evenly among ArrayLists, HashMaps and ConcurrentHashMaps.

With only 0.8% and 0.3% of an average heap wasted, it probably isn't worth the human effort or CPU time to handle these cases.  However, these cases do exist.

-Nathan

-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Michael Kuhlmann
Sent: Wednesday, June 01, 2016 2:11 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LinearProbeHashtable

Am 01.06.2016 um 10:35 schrieb Peter Levart:
> But, as said, it may
> be beneficial to trigger rehashing (with expunging of tombstones and 
> possible shrinking) also in remove() in order to more eagerly optimize 
> lookup time by preventing building-up future "clusters".

I wouldn't do it. You risk a "pumping effect" when many elements are added and removed frequently.

What would be the use case? First filling the map, then removing most of the elements and never add many again? I've never seen that.

And it's even more unlikely in ClassValue where removals will definitely be the exception.

HashMap doesn't shrink either for good reasons.

-Michael


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From thurston at nomagicsoftware.com  Thu Jun  2 10:53:15 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 2 Jun 2016 07:53:15 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <574E9C79.4010104@oracle.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com> <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com> <574E9C79.4010104@oracle.com>
Message-ID: <1464879195590-13502.post@n7.nabble.com>

<<"That's what "as if"-ness in sequential
consistency gives you.

That "divergence" is a blessing in disguise: it is the wiggle room that
allows software and hardware to optimize execution (e.g. do something
that is not exactly spelled in the program, in exact order spelled in
the program, etc), while still adhering to abstract machine rules that
rule out unwanted outcomes. "
>>

"Synchronization actions are sequentially consistent" is not explicitly in
JLS$17, as I've pointed out.
Maybe it's implied, maybe someone can argue that it can be arrived at via
deduction .
I believe it should be added explicitly, with even more clarity

Section 17.4.4
Synchronization actions are sequentially consistent (and *not necessarily
strictly consistent, nor linearizable*)


Regarding SAs, I had more or less thought of them as a way for the developer
to command to the platform:
"No wiggle room!" or as you might phrase it ("I need to reason about them
without deep thinking").
The "as-if" "results" complicates that, but so be it.  If that's the
intention, why not state it clearly?






--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13502.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aph at redhat.com  Thu Jun  2 12:25:29 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 2 Jun 2016 17:25:29 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464879195590-13502.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com> <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com> <574E9C79.4010104@oracle.com>
 <1464879195590-13502.post@n7.nabble.com>
Message-ID: <57505DF9.6020604@redhat.com>

On 02/06/16 15:53, thurstonn wrote:
> Regarding SAs, I had more or less thought of them as a way for the developer
> to command to the platform:
> "No wiggle room!" or as you might phrase it ("I need to reason about them
> without deep thinking").

No: the hardware won't let you because it can't because processing
elements are asynchronous.  I guess you can disable all physical PEs
except one, and that would give you an actual physical total order of
memory accesses, as opposed to the illusion of one.  But ewww, why
bother?

Andrew.


From thurston at nomagicsoftware.com  Thu Jun  2 11:43:23 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 2 Jun 2016 08:43:23 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <57505DF9.6020604@redhat.com>
References: <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com> <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com> <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com> <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com> <1464879195590-13502.post@n7.nabble.com>
 <57505DF9.6020604@redhat.com>
Message-ID: <1464882203299-13504.post@n7.nabble.com>

Well, that's not exactly relevant to the prior post, but I can't help myself.

Can't all hardware platforms guarantee sequential access to the bus (if they
chose to?).
That would seem to do the trick



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13504.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aph at redhat.com  Thu Jun  2 12:59:24 2016
From: aph at redhat.com (Andrew Haley)
Date: Thu, 2 Jun 2016 17:59:24 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464882203299-13504.post@n7.nabble.com>
References: <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com> <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com> <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com> <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com> <1464879195590-13502.post@n7.nabble.com>
 <57505DF9.6020604@redhat.com> <1464882203299-13504.post@n7.nabble.com>
Message-ID: <575065EC.2010108@redhat.com>

On 02/06/16 16:43, thurstonn wrote:
> Well, that's not exactly relevant to the prior post, but I can't help myself.

It's the only sensible interpretation I can make from what you posted.
If you meant something else, feel free to explain what it was.

> Can't all hardware platforms guarantee sequential access to the bus (if they
> chose to?).

There may not even be a bus.  All sorts of topologies are possible.
And load-acquire/store-release operations do *not* (as we now) force
anything to be "flushed into memory".

Andrew.



From thurston at nomagicsoftware.com  Thu Jun  2 12:01:42 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 2 Jun 2016 09:01:42 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <575065EC.2010108@redhat.com>
References: <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com> <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com> <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com> <1464879195590-13502.post@n7.nabble.com>
 <57505DF9.6020604@redhat.com> <1464882203299-13504.post@n7.nabble.com>
 <575065EC.2010108@redhat.com>
Message-ID: <1464883302887-13506.post@n7.nabble.com>

Well, I didn't ask a yes/no question, unless your responding No to:

"If that's the intention, why not state it clearly?"

and advocating what? leaving it vague.


Or were you arguing against "synchronization actions are sequentially
consistent"?








--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13506.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From dragonsinth at gmail.com  Thu Jun  2 14:33:14 2016
From: dragonsinth at gmail.com (Scott Blum)
Date: Thu, 2 Jun 2016 14:33:14 -0400
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464883302887-13506.post@n7.nabble.com>
References: <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com>
 <1464879195590-13502.post@n7.nabble.com>
 <57505DF9.6020604@redhat.com>
 <1464882203299-13504.post@n7.nabble.com>
 <575065EC.2010108@redhat.com>
 <1464883302887-13506.post@n7.nabble.com>
Message-ID: <CALuNCpg24r==rLrhJrz3QFLfg2ApppAdMqqgoUcOWobxb8ghDw@mail.gmail.com>

Can you phrase your question as a Java program?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160602/9f1924ad/attachment.html>

From aleksey.shipilev at oracle.com  Thu Jun  2 17:13:13 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 3 Jun 2016 00:13:13 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1464879195590-13502.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1464688372978-13460.post@n7.nabble.com> <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com> <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com> <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com> <574E9C79.4010104@oracle.com>
 <1464879195590-13502.post@n7.nabble.com>
Message-ID: <5750A169.8070306@oracle.com>

On 02.06.2016 17:53, thurstonn wrote:
> "Synchronization actions are sequentially consistent" is not 
> explicitly in JLS$17, as I've pointed out. Maybe it's implied, maybe 
> someone can argue that it can be arrived at via deduction . I
> believe it should be added explicitly, with even more clarity

I *do* think that is trivially deducible from SO totality, SO
consistency and SO-PO consistency. In fact, it takes only a few minutes
to arrive there:
  http://shipilev.net/blog/2014/jmm-pragmatics/#_synchronization_order

Also, it is trivial from observing that a program consisting only of SAs
is still covered by SC-DRF, while only SO rules are applicable, and no
data races are present in the program. Therefore you can arrive to the
idea that SO rules over SA-only programs give you SC almost immediately.


> Regarding SAs, I had more or less thought of them as a way for the
> developer to command to the platform: "No wiggle room!" or as you
> might phrase it ("I need to reason about them without deep
> thinking"). The "as-if" "results" complicates that, but so be it.  If
> that's the intention, why not state it clearly?

The thing is, there is still wiggle room even in SO rules:
  a) Lock coarsening is doable, which strips away adjacent locks/unlocks
(SA by spec!);
  b) Synchronization/volatiles in thread-local contexts are removable
(e.g. in constructors, deserializers);
  c) Synchronization/volatiles on single-threaded machines are
completely redundant, and can be stripped away without remorse.


Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160603/1390b37c/attachment.sig>

From thurston at nomagicsoftware.com  Thu Jun  2 16:47:09 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 2 Jun 2016 13:47:09 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <5750A169.8070306@oracle.com>
References: <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com> <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com> <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com> <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com> <1464879195590-13502.post@n7.nabble.com>
 <5750A169.8070306@oracle.com>
Message-ID: <1464900429911-13509.post@n7.nabble.com>

Synchronization actions are sequentially consistent (and *not necessarily
strictly consistent, nor linearizable*)

The part within the parentheses is *very important*; I think if you
understand that part, you'll understand the potential confusion deriving
from the JMM spec; every strictly consistent execution is also sequentially
consistent; the reverse does not hold



I've read your link multiple times, chapeau.  



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13509.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at gmail.com  Fri Jun  3 03:26:21 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Fri, 3 Jun 2016 08:26:21 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464900429911-13509.post@n7.nabble.com>
References: <1464688372978-13460.post@n7.nabble.com>
 <574D942C.5060305@oracle.com>
 <1464713744101-13465.post@n7.nabble.com>
 <574DE420.1000502@oracle.com>
 <1464723440028-13471.post@n7.nabble.com>
 <574E1C76.3050106@oracle.com>
 <1464742768769-13481.post@n7.nabble.com>
 <574E9C79.4010104@oracle.com>
 <1464879195590-13502.post@n7.nabble.com>
 <5750A169.8070306@oracle.com>
 <1464900429911-13509.post@n7.nabble.com>
Message-ID: <CANkgWKg0JewudtrqMrTvMT0yb2yWerVO+6wy3ogAe=tLNjwu1g@mail.gmail.com>

I could find 16 occurrences of sequential consistency in the JMM spec. It
is definitely pointed out and discussed. However, JMM does not guarantee
sequential consistency on its own, it only enables you to construct
sequentially consistent programs.

Alex
On 2 Jun 2016 21:57, "thurstonn" <thurston at nomagicsoftware.com> wrote:

> Synchronization actions are sequentially consistent (and *not necessarily
> strictly consistent, nor linearizable*)
>
> The part within the parentheses is *very important*; I think if you
> understand that part, you'll understand the potential confusion deriving
> from the JMM spec; every strictly consistent execution is also sequentially
> consistent; the reverse does not hold
>
>
>
> I've read your link multiple times, chapeau.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13509.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160603/47e464ee/attachment-0001.html>

From dl at cs.oswego.edu  Fri Jun  3 12:40:13 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 3 Jun 2016 12:40:13 -0400
Subject: [concurrency-interest] current j.u.c updates
Message-ID: <5751B2ED.8090405@cs.oswego.edu>


The current versions of jsr166 classes are now updated to use VarHandles,
onSpinWait, and other jdk9 stuff, so are approaching their expected
jdk9 form.

If you'd like to help your future self, please try it out
and let us know of any problems.  It would be nice to hear
about issues before we integrate with openjdk (soon).

To use it, you need at  least jdk9 build 120. You can get it from
   https://jdk9.java.net/download/
and then get jsr166.jar at
   http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166.jar
and run using
   java -Xpatch:java.base="$DIR/jsr166.jar"
where DIR is the full file prefix.
(Note that these instructions are different than they were for
earlier jdk9 builds.)

Instead of getting the jar, you can get sources at build yourself
   http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166.tar.gz?view=tar
then cd into jsr166 and "ant jar" and "ant tck" to run basic unit tests.
Building javadocs will fail with assertion error that we think
will be fixed soon in openjdk.

For the curious, VarHandles replace all uses of Unsafe except
(1) those in ConcurrentHashMap, to avoid bootstrap circularities
when class loaders and method handles need CHM, and (2) those
accessing Thread class fields (in ThreadLocalRandom, Locks.LockSupport,
and atomic/Striped64).

Bear in mind whan comparing to jdk8 performance that the G1 garbage
collector is now default in jdk9, which tends cause slowdowns in some
concurrent applications (but may still be preferable because of
reduced latencies.) Use "-XX:+UseParallelGC", for jdk8-like GC,
with -XX:+UseCondCardMark to reduce false sharing.

Also, if you haven't yet, you might want to try out (jdk9)
Flow and SubmissionPublisher.


-Doug





From nitinchaudhary48 at gmail.com  Fri Jun  3 18:46:14 2016
From: nitinchaudhary48 at gmail.com (Nitin chaudhary)
Date: Sat, 4 Jun 2016 04:16:14 +0530
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <00a401d1bc22$72441730$56cc4590$@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <00a401d1bc22$72441730$56cc4590$@gmail.com>
Message-ID: <CACtNifbftDNj-oynB707j=heK1gWVzZwG4s0k05_rc_HnbeV5Q@mail.gmail.com>

Hi Concurrency Experts,

It might be a stupid question but I am unable to understand the behaviour
of join Method.

Attached two program 1 program 'JoinExample' in line number 24, calling
 join on thread t and waits until thread t finishes. Program finishes well.

But in Example Second 'NoVisibility' Line number 13, my Reader thread got
stuck and it seems like halt for ever. It should call a loop second time
but it did not. If possible kindly do let me know the reason.

Thank you in advance and pardon my English.

Regards,
Nitin Kumar
#:+91 9654151525
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/48d8cd82/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: JoinExample.java
Type: application/octet-stream
Size: 695 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/48d8cd82/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: NoVisibility.java
Type: application/octet-stream
Size: 836 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/48d8cd82/attachment-0001.obj>

From nitinchaudhary48 at gmail.com  Fri Jun  3 18:47:44 2016
From: nitinchaudhary48 at gmail.com (Nitin chaudhary)
Date: Sat, 4 Jun 2016 04:17:44 +0530
Subject: [concurrency-interest] Thread Join method
Message-ID: <CACtNifZOpQW4PF5q3YqAvyRXfptCoC5mtej96YaCjP8MX5-Axg@mail.gmail.com>

*[changed subject]*

On Sat, Jun 4, 2016 at 4:16 AM, Nitin chaudhary <nitinchaudhary48 at gmail.com>
wrote:

> Hi Concurrency Experts,
>
> It might be a stupid question but I am unable to understand the behaviour
> of join Method.
>
> Attached two program 1 program 'JoinExample' in line number 24, calling
>  join on thread t and waits until thread t finishes. Program finishes well.
>
> But in Example Second 'NoVisibility' Line number 13, my Reader thread got
> stuck and it seems like halt for ever. It should call a loop second time
> but it did not. If possible kindly do let me know the reason.
>
> Thank you in advance and pardon my English.
>
> Regards,
> Nitin Kumar
> #:+91 9654151525
>
>


-- 
Nitin Rohtash Singh Chaudhary
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/2980331b/attachment.html>

From nitinchaudhary48 at gmail.com  Fri Jun  3 18:50:42 2016
From: nitinchaudhary48 at gmail.com (Nitin chaudhary)
Date: Sat, 4 Jun 2016 04:20:42 +0530
Subject: [concurrency-interest] Thread Join Method
Message-ID: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>

Hi Concurrency Experts,

Apologize for filling your mail boxes with my stupidity.

It might be a stupid question but I am unable to understand the behaviour
of join Method.

Attached two program 1 program 'JoinExample' in line number 24, calling
 join on thread t and waits until thread t finishes. Program finishes well.

But in Example Second 'NoVisibility' Line number 13, my Reader thread got
stuck and it seems like halt for ever. It should call a loop second time
but it did not. If possible kindly do let me know the reason.

Thank you in advance and pardon my English.

Regards,
Nitin Kumar
#:+91 9654151525
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/319237d7/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: JoinExample.java
Type: application/octet-stream
Size: 695 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/319237d7/attachment.obj>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: NoVisibility.java
Type: application/octet-stream
Size: 836 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/319237d7/attachment-0001.obj>

From davidcholmes at aapt.net.au  Fri Jun  3 19:34:20 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 4 Jun 2016 09:34:20 +1000
Subject: [concurrency-interest] Thread Join Method
In-Reply-To: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
References: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
Message-ID: <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>

Hi,

 

You can’t join() the currentThread() – you are waiting for yourself to terminate.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitin chaudhary
Sent: Saturday, June 4, 2016 8:51 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Thread Join Method

 

 

Hi Concurrency Experts,

 

Apologize for filling your mail boxes with my stupidity.

 

It might be a stupid question but I am unable to understand the behaviour of join Method.

 

Attached two program 1 program 'JoinExample' in line number 24, calling  join on thread t and waits until thread t finishes. Program finishes well.

 

But in Example Second 'NoVisibility' Line number 13, my Reader thread got stuck and it seems like halt for ever. It should call a loop second time but it did not. If possible kindly do let me know the reason.

 

Thank you in advance and pardon my English.

 

Regards,

Nitin Kumar

#:+91 9654151525

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/c0465e7c/attachment-0001.html>

From forax at univ-mlv.fr  Sat Jun  4 04:34:38 2016
From: forax at univ-mlv.fr (Remi Forax)
Date: Sat, 4 Jun 2016 10:34:38 +0200 (CEST)
Subject: [concurrency-interest] i wait for myself to die
In-Reply-To: <CACtNifbftDNj-oynB707j=heK1gWVzZwG4s0k05_rc_HnbeV5Q@mail.gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <00a401d1bc22$72441730$56cc4590$@gmail.com>
 <CACtNifbftDNj-oynB707j=heK1gWVzZwG4s0k05_rc_HnbeV5Q@mail.gmail.com>
Message-ID: <1043624640.529001.1465029278182.JavaMail.zimbra@u-pem.fr>

Hi Nitin, 
i'm not really an expert by 
Thread.currentThread().join() 
means that you stop the current thread until the current thread finish the method run(), 
hence the wait forever. 

cheers, 
Rémi 

----- Mail original -----

> De: "Nitin chaudhary" <nitinchaudhary48 at gmail.com>
> À: concurrency-interest at cs.oswego.edu
> Envoyé: Samedi 4 Juin 2016 00:46:14
> Objet: Re: [concurrency-interest] LinearProbeHashtable

> Hi Concurrency Experts,

> It might be a stupid question but I am unable to understand the behaviour of
> join Method.

> Attached two program 1 program 'JoinExample' in line number 24, calling join
> on thread t and waits until thread t finishes. Program finishes well.

> But in Example Second 'NoVisibility' Line number 13, my Reader thread got
> stuck and it seems like halt for ever. It should call a loop second time but
> it did not. If possible kindly do let me know the reason.

> Thank you in advance and pardon my English.

> Regards,
> Nitin Kumar
> #:+91 9654151525

> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160604/d0c7229c/attachment.html>

From elizarov at devexperts.com  Mon Jun  6 06:56:51 2016
From: elizarov at devexperts.com (Roman Elizarov)
Date: Mon, 6 Jun 2016 10:56:51 +0000
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
Message-ID: <72697567e55848e381ce0a926308ca3a@devexperts.com>


> I wish I could find a way to reuse the slot occupied by a tombstone. 
> Keep in mind that get() must be lock-free. For that to work, I would need an atomic read from two consecutive array slots. Otherwise I could read a key belonging to one mapping > and until I get to read the value slot, it will contain the value from another mapping. This phenomena could be called "Entry tearing".

The irony is that it is possible (at least on x86 hardware) in very efficient way, yet Java does not provide a way for us to do so. 

Below, I believe, is the exhaustive classification of ways to reuse tomstones in a concurrent key-value hashmap (key and value are separated) with wait-free/lock-free reads:

1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).

2. Introduce Entry object to combine key & value into a single object. It is still pretty fast, but it considerably (a factor of many times) increases hashmap's memory consumption and decreases data structure memory locality (an extra memory dereference to wait for memory load on).

3. Coordinate reads and write, e.g. allow your reads to modify hashtable to signal that the read it in process. There are a lots of actual implementation approaches (see Harris's works for CASN, for example, may STMs will do the trick, too). It "solves" the problem even when all you have is a single-reference CAS. You still keep low memory consumption for your data, but it adds a lot of overhead to read and write operations. IMHO, it has practical use only if you have huge amount of data (some kind of in-memory database) which you read/write not that often.







From elizarov at devexperts.com  Mon Jun  6 07:02:30 2016
From: elizarov at devexperts.com (Roman Elizarov)
Date: Mon, 6 Jun 2016 11:02:30 +0000
Subject: [concurrency-interest] LinearProbeHashtable
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com> 
Message-ID: <6cd0c7dd21584ba6b4b04a0fda1aee5f@devexperts.com>

There's actually another approach that might do the trick. Just combine key and value into a StampedLock, e.g. add an int version to each key/value pair (you'll have to do it in a separate int array until we have project Valhalla value types in Java) This way you can do lock-free consistent reads of key/value pairs (by detecting entry tearing just like StampedLock does). It only adds 4 bytes per key/value pair (much less than a separate Entry object) and enables tomstones reuse.

-----Original Message-----
From: Roman Elizarov 
Sent: Monday, June 6, 2016 12:57 PM
To: 'Peter Levart' <peter.levart at gmail.com>; Michael Kuhlmann <concurrency at kuli.org>; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] LinearProbeHashtable


> I wish I could find a way to reuse the slot occupied by a tombstone. 
> Keep in mind that get() must be lock-free. For that to work, I would need an atomic read from two consecutive array slots. Otherwise I could read a key belonging to one mapping > and until I get to read the value slot, it will contain the value from another mapping. This phenomena could be called "Entry tearing".

The irony is that it is possible (at least on x86 hardware) in very efficient way, yet Java does not provide a way for us to do so. 

Below, I believe, is the exhaustive classification of ways to reuse tomstones in a concurrent key-value hashmap (key and value are separated) with wait-free/lock-free reads:

1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).

2. Introduce Entry object to combine key & value into a single object. It is still pretty fast, but it considerably (a factor of many times) increases hashmap's memory consumption and decreases data structure memory locality (an extra memory dereference to wait for memory load on).

3. Coordinate reads and write, e.g. allow your reads to modify hashtable to signal that the read it in process. There are a lots of actual implementation approaches (see Harris's works for CASN, for example, may STMs will do the trick, too). It "solves" the problem even when all you have is a single-reference CAS. You still keep low memory consumption for your data, but it adds a lot of overhead to read and write operations. IMHO, it has practical use only if you have huge amount of data (some kind of in-memory database) which you read/write not that often.







From concurrency at kuli.org  Mon Jun  6 08:11:44 2016
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Mon, 6 Jun 2016 14:11:44 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <72697567e55848e381ce0a926308ca3a@devexperts.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com>
Message-ID: <57556880.3040901@kuli.org>

Am 06.06.2016 um 12:56 schrieb Roman Elizarov:
> 
> 1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).

This would only work if the object reference is only 32 bit long, which
is not necessarily the case. (Imagine VMs with more than 32 gb heap.)

So it will never be supported by Java language features.

-Michael


From elizarov at devexperts.com  Mon Jun  6 08:51:30 2016
From: elizarov at devexperts.com (Roman Elizarov)
Date: Mon, 6 Jun 2016 12:51:30 +0000
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <57556880.3040901@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com> <57556880.3040901@kuli.org>
Message-ID: <e27bba57b1894a69b516d7685703c0ab@devexperts.com>

x86-64 has CMPXCHG16B instruction that can work with two 64-bit references at the same time. 

-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Michael Kuhlmann
Sent: Monday, June 6, 2016 2:12 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LinearProbeHashtable

Am 06.06.2016 um 12:56 schrieb Roman Elizarov:
> 
> 1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).

This would only work if the object reference is only 32 bit long, which is not necessarily the case. (Imagine VMs with more than 32 gb heap.)

So it will never be supported by Java language features.

-Michael

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From concurrency at kuli.org  Mon Jun  6 08:59:14 2016
From: concurrency at kuli.org (Michael Kuhlmann)
Date: Mon, 6 Jun 2016 14:59:14 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com> <57556880.3040901@kuli.org>
 <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
Message-ID: <575573A2.1090609@kuli.org>

Am 06.06.2016 um 14:51 schrieb Roman Elizarov:
> x86-64 has CMPXCHG16B instruction that can work with two 64-bit references at the same time. 
> 

Ah, I didn't know this. Thanks!

But then, again, there is the problem how to support other CPU types,
including 32-Bit X84.

-Michael


From viktor.klang at gmail.com  Mon Jun  6 09:31:43 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 6 Jun 2016 15:31:43 +0200
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <575573A2.1090609@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy>
 <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy>
 <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org>
 <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org>
 <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com>
 <57556880.3040901@kuli.org>
 <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
 <575573A2.1090609@kuli.org>
Message-ID: <CANPzfU9OcQrjrX4uF_3omC3A89XK9AVaHA+5J4T9D1YfdsLkxg@mail.gmail.com>

The downgrade route would fall back to critical sections?

-- 
Cheers,
√
On Jun 6, 2016 3:03 PM, "Michael Kuhlmann" <concurrency at kuli.org> wrote:

> Am 06.06.2016 um 14:51 schrieb Roman Elizarov:
> > x86-64 has CMPXCHG16B instruction that can work with two 64-bit
> references at the same time.
> >
>
> Ah, I didn't know this. Thanks!
>
> But then, again, there is the problem how to support other CPU types,
> including 32-Bit X84.
>
> -Michael
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160606/af6b1137/attachment.html>

From elizarov at devexperts.com  Mon Jun  6 10:31:38 2016
From: elizarov at devexperts.com (Roman Elizarov)
Date: Mon, 6 Jun 2016 14:31:38 +0000
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <575573A2.1090609@kuli.org>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com> <57556880.3040901@kuli.org>
 <e27bba57b1894a69b516d7685703c0ab@devexperts.com> <575573A2.1090609@kuli.org>
Message-ID: <648df44dccb74b52943725cce9b45e6c@devexperts.com>

32-bit x86 has CMPXCHG8B.

However, there could be an architecture that does not support this kind of instructions natively. Even reading two references atomically (yet alone CASing them) can be a problem on some (what?) architectures. However, some kind of fallback can always be found and implemented, just like it now happens with "volatile" 64 bit variables that are required to be atomic, but not all architectures support them. 

Critical section is always the last-resort fallback that JVM can implement in such cases, but you do much better than that. Since you are going to implement those fallbacks in C++ code (in assembly, actually), you can use CASN-like approach of Harris, for example, which gives you both lock-freedom and disjoint-access-parallelism.

/Roman

-----Original Message-----
From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Michael Kuhlmann
Sent: Monday, June 6, 2016 2:59 PM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LinearProbeHashtable

Am 06.06.2016 um 14:51 schrieb Roman Elizarov:
> x86-64 has CMPXCHG16B instruction that can work with two 64-bit references at the same time. 
> 

Ah, I didn't know this. Thanks!

But then, again, there is the problem how to support other CPU types, including 32-Bit X84.

-Michael

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From snfuchs at gmx.de  Mon Jun  6 17:07:46 2016
From: snfuchs at gmx.de (Stefan Fuchs)
Date: Mon, 6 Jun 2016 23:07:46 +0200
Subject: [concurrency-interest] Can parallelStream be used in Webstart
	applications?
Message-ID: <5755E622.2040701@gmx.de>

Hi,

a recent change in our webstart application triggered various 
PermissionExceptions all over the place.
 From injection failures in guice (tried to use reflection to update a 
private member) to loading resources with getResource().
This was odd given that our application is started with all-permissions.
Moreover the application worked fine (using various functions, which 
require elevated permissions) until a certain function was triggered.


Bisecting revealed that the problem was triggered by a call to 
|CompletableFuture 
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html>::||supplyAsync 
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#supplyAsync-java.util.function.Supplier->(Supplier 
<https://docs.oracle.com/javase/8/docs/api/java/util/function/Supplier.html><U> 
supplier).

Reading the fine-print in the class description of |ForkJoinPool, we 
found out that the threads created with no Permissions enabled.

"If a |SecurityManager| 
<https://docs.oracle.com/javase/8/docs/api/java/lang/SecurityManager.html> 
is present and no factory is specified, then the default pool uses a 
factory supplying threads that have no |Permissions| 
<https://docs.oracle.com/javase/8/docs/api/java/security/Permissions.html> 
enabled. The system class loader is used to load these classes."

So we think what, happens is that certain application classes are loaded 
in the context of the restricted thread and are therefor unable to 
function properly.

While we could fix the above problem by passing our own ExecutorService 
to the appropriate variant of the above method, we wonder what happens, 
if the common ForkJoinPool is used by some code, that is not under our 
control, say some linked framework or even the jdk itself.

Perhaps we could add the code mentioned here at are very early stage of 
our application?
http://stackoverflow.com/questions/34303094/is-it-not-possible-to-supply-a-thread-facory-or-name-pattern-to-forkjoinpool


The issue has been raised before in 
https://bugs.openjdk.java.net/browse/JDK-8143638 . The bug was however 
closed as not an issue.

So is every Webstart application using parallelStream and co. to 
implement the above workaround?


Any thoughts?


Stefan Fuchs

From oleksandr.otenko at gmail.com  Wed Jun  8 06:30:24 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 8 Jun 2016 11:30:24 +0100
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com> <57556880.3040901@kuli.org>
 <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
Message-ID: <FCB1B7B4-3053-4112-9D18-4CFA4BEEEEFD@gmail.com>

But does it have a way to surface in Java? (Direct)ByteBuffers?..

Alex

> On 6 Jun 2016, at 13:51, Roman Elizarov <elizarov at devexperts.com> wrote:
> 
> x86-64 has CMPXCHG16B instruction that can work with two 64-bit references at the same time. 
> 
> -----Original Message-----
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Michael Kuhlmann
> Sent: Monday, June 6, 2016 2:12 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] LinearProbeHashtable
> 
> Am 06.06.2016 um 12:56 schrieb Roman Elizarov:
>> 
>> 1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).
> 
> This would only work if the object reference is only 32 bit long, which is not necessarily the case. (Imagine VMs with more than 32 gb heap.)
> 
> So it will never be supported by Java language features.
> 
> -Michael
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at gmail.com  Wed Jun  8 06:32:28 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 8 Jun 2016 11:32:28 +0100
Subject: [concurrency-interest] Thread Join Method
In-Reply-To: <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>
References: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
 <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>
Message-ID: <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>

Surely this should throw an Exception

Alex

> On 4 Jun 2016, at 00:34, David Holmes <davidcholmes at aapt.net.au> wrote:
> 
> Hi,
>  
> You can’t join() the currentThread() – you are waiting for yourself to terminate.
>  
> David
>  
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitin chaudhary
> Sent: Saturday, June 4, 2016 8:51 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Thread Join Method
>  
>  
> Hi Concurrency Experts,
>  
> Apologize for filling your mail boxes with my stupidity.
>  
> It might be a stupid question but I am unable to understand the behaviour of join Method.
>  
> Attached two program 1 program 'JoinExample' in line number 24, calling  join on thread t and waits until thread t finishes. Program finishes well.
>  
> But in Example Second 'NoVisibility' Line number 13, my Reader thread got stuck and it seems like halt for ever. It should call a loop second time but it did not. If possible kindly do let me know the reason.
>  
> Thank you in advance and pardon my English.
>  
> Regards,
> Nitin Kumar
> #:+91 9654151525
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/0bde05c0/attachment.html>

From viktor.klang at gmail.com  Wed Jun  8 07:01:43 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 8 Jun 2016 13:01:43 +0200
Subject: [concurrency-interest] Thread Join Method
In-Reply-To: <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>
References: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
 <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>
 <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>
Message-ID: <CANPzfU_mj8gOQm=QZQ0rsoJSjmWKvETH51=pVrnR6Ow1z53=vg@mail.gmail.com>

Think of backwards compatibility!

;)

On Wed, Jun 8, 2016 at 12:32 PM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Surely this should throw an Exception
>
> Alex
>
> On 4 Jun 2016, at 00:34, David Holmes <davidcholmes at aapt.net.au> wrote:
>
> Hi,
>
> You can’t join() the currentThread() – you are waiting for yourself to
> terminate.
>
> David
>
> *From:* Concurrency-interest [
> mailto:concurrency-interest-bounces at cs.oswego.edu
> <concurrency-interest-bounces at cs.oswego.edu>] *On Behalf Of *Nitin
> chaudhary
> *Sent:* Saturday, June 4, 2016 8:51 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Thread Join Method
>
>
> Hi Concurrency Experts,
>
> Apologize for filling your mail boxes with my stupidity.
>
> It might be a stupid question but I am unable to understand the behaviour
> of join Method.
>
> Attached two program 1 program 'JoinExample' in line number 24, calling
>  join on thread t and waits until thread t finishes. Program finishes well.
>
> But in Example Second 'NoVisibility' Line number 13, my Reader thread got
> stuck and it seems like halt for ever. It should call a loop second time
> but it did not. If possible kindly do let me know the reason.
>
> Thank you in advance and pardon my English.
>
> Regards,
> Nitin Kumar
> #:+91 9654151525
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/ae3d1771/attachment-0001.html>

From elizarov at devexperts.com  Wed Jun  8 07:10:29 2016
From: elizarov at devexperts.com (Roman Elizarov)
Date: Wed, 8 Jun 2016 11:10:29 +0000
Subject: [concurrency-interest] LinearProbeHashtable
In-Reply-To: <FCB1B7B4-3053-4112-9D18-4CFA4BEEEEFD@gmail.com>
References: <699f7f44-2d36-e594-99bb-745d4799aa81@gmail.com>
 <3825850.COdVxmxUDJ@tapsy> <289b8636-0163-d3cb-40fd-df7ae2ab1104@gmail.com>
 <15269016.0KoCuDZ9Z7@tapsy> <ca2f16d2-1ec1-561d-8981-6688dd634b88@gmail.com>
 <574EA6BD.7000900@kuli.org> <3fb46ea7-9ad0-ea32-d374-1587b2c60007@gmail.com>
 <574EDCFD.2090506@kuli.org> <09bfb4e0-a8a5-b7f1-e991-8c2be6c9e788@gmail.com>
 <72697567e55848e381ce0a926308ca3a@devexperts.com> <57556880.3040901@kuli.org>
 <e27bba57b1894a69b516d7685703c0ab@devexperts.com>
 <FCB1B7B4-3053-4112-9D18-4CFA4BEEEEFD@gmail.com>
Message-ID: <79b9925eefb44edaaa4cb4acc181f112@devexperts.com>

Project Valhalla? E.g., define a value class with key and value references, use array of these value class in your hashmap implementation. Use VarHandle to volatile (atomically) read it and CAS it. Assuming that project Valhalla implementation uses platform-specific instructions when they are available, then it is all going to work as fast as the corresponding C/C++ code.

-----Original Message-----
From: Alex Otenko [mailto:oleksandr.otenko at gmail.com] 
Sent: Wednesday, June 8, 2016 12:30 PM
To: Roman Elizarov <elizarov at devexperts.com>
Cc: Michael Kuhlmann <concurrency at kuli.org>; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LinearProbeHashtable

But does it have a way to surface in Java? (Direct)ByteBuffers?..

Alex

> On 6 Jun 2016, at 13:51, Roman Elizarov <elizarov at devexperts.com> wrote:
> 
> x86-64 has CMPXCHG16B instruction that can work with two 64-bit references at the same time. 
> 
> -----Original Message-----
> From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Michael Kuhlmann
> Sent: Monday, June 6, 2016 2:12 PM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] LinearProbeHashtable
> 
> Am 06.06.2016 um 12:56 schrieb Roman Elizarov:
>> 
>> 1. Use double-word reads to read both  key and value and double-word CAS to update them at the same time. You can do it on x86, but not via existing or future (Java 9) Java APIs. It is the most efficient approach (both memory and CPU time). I wonder if we can hope to get this available via VarHandles in Java 10 (when we can define a value type with a pair of references).
> 
> This would only work if the object reference is only 32 bit long, which is not necessarily the case. (Imagine VMs with more than 32 gb heap.)
> 
> So it will never be supported by Java language features.
> 
> -Michael
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From davidcholmes at aapt.net.au  Wed Jun  8 08:20:40 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 8 Jun 2016 22:20:40 +1000
Subject: [concurrency-interest] Thread Join Method
In-Reply-To: <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>
References: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
 <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>
 <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>
Message-ID: <007501d1c180$2bea6010$83bf2030$@aapt.net.au>

Sure it could have been spec’d that way 20 years ago and every use burdened with a currentThread() check. But it wasn’t.

 

There a lots of ways to “shoot yourself in the foot”.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Alex Otenko
Sent: Wednesday, June 8, 2016 8:32 PM
To: dholmes at ieee.org
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Thread Join Method

 

Surely this should throw an Exception

 

Alex

 

On 4 Jun 2016, at 00:34, David Holmes <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au> > wrote:

 

Hi,

 

You can’t join() the currentThread() – you are waiting for yourself to terminate.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitin chaudhary
Sent: Saturday, June 4, 2016 8:51 AM
To: concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> 
Subject: [concurrency-interest] Thread Join Method

 

 

Hi Concurrency Experts,

 

Apologize for filling your mail boxes with my stupidity.

 

It might be a stupid question but I am unable to understand the behaviour of join Method.

 

Attached two program 1 program 'JoinExample' in line number 24, calling  join on thread t and waits until thread t finishes. Program finishes well.

 

But in Example Second 'NoVisibility' Line number 13, my Reader thread got stuck and it seems like halt for ever. It should call a loop second time but it did not. If possible kindly do let me know the reason.

 

Thank you in advance and pardon my English.

 

Regards,

Nitin Kumar

#:+91 9654151525

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/09016e78/attachment.html>

From forax at univ-mlv.fr  Wed Jun  8 08:45:59 2016
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 8 Jun 2016 14:45:59 +0200 (CEST)
Subject: [concurrency-interest] Thread Join Method
In-Reply-To: <007501d1c180$2bea6010$83bf2030$@aapt.net.au>
References: <CACtNifZVg24aMhq7T794S0eDM8=nW3ORs2fWdNrcQ922ffKyiA@mail.gmail.com>
 <04a601d1bdf0$74562b60$5d028220$@aapt.net.au>
 <9C2BE043-0915-482C-853F-F7D7B6BC2F35@gmail.com>
 <007501d1c180$2bea6010$83bf2030$@aapt.net.au>
Message-ID: <1357232714.1344067.1465389959584.JavaMail.zimbra@u-pem.fr>

Technically, this one doesn't allow to shout yourself into the foot because you're frozen before :) 

Rémi 

----- Mail original -----

> De: "David Holmes" <davidcholmes at aapt.net.au>
> À: "Alex Otenko" <oleksandr.otenko at gmail.com>, dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Envoyé: Mercredi 8 Juin 2016 14:20:40
> Objet: Re: [concurrency-interest] Thread Join Method

> Sure it could have been spec’d that way 20 years ago and every use burdened
> with a currentThread() check. But it wasn’t.

> There a lots of ways to “shoot yourself in the foot”.

> David

> From: Concurrency-interest
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Alex Otenko
> Sent: Wednesday, June 8, 2016 8:32 PM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Thread Join Method

> Surely this should throw an Exception

> Alex

> > On 4 Jun 2016, at 00:34, David Holmes < davidcholmes at aapt.net.au > wrote:
> 

> > Hi,
> 

> > You can’t join() the currentThread() – you are waiting for yourself to
> > terminate.
> 

> > David
> 

> > From: Concurrency-interest [
> > mailto:concurrency-interest-bounces at cs.oswego.edu ] On Behalf Of Nitin
> > chaudhary
> 
> > Sent: Saturday, June 4, 2016 8:51 AM
> 
> > To: concurrency-interest at cs.oswego.edu
> 
> > Subject: [concurrency-interest] Thread Join Method
> 

> > Hi Concurrency Experts,
> 

> > Apologize for filling your mail boxes with my stupidity.
> 

> > It might be a stupid question but I am unable to understand the behaviour
> > of
> > join Method.
> 

> > Attached two program 1 program 'JoinExample' in line number 24, calling
> > join
> > on thread t and waits until thread t finishes. Program finishes well.
> 

> > But in Example Second 'NoVisibility' Line number 13, my Reader thread got
> > stuck and it seems like halt for ever. It should call a loop second time
> > but
> > it did not. If possible kindly do let me know the reason.
> 

> > Thank you in advance and pardon my English.
> 

> > Regards,
> 

> > Nitin Kumar
> 

> > #:+91 9654151525
> 

> > _______________________________________________
> 
> > Concurrency-interest mailing list
> 
> > Concurrency-interest at cs.oswego.edu
> 
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/bbcbfdc3/attachment-0001.html>

From thurston at nomagicsoftware.com  Wed Jun  8 08:42:24 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 8 Jun 2016 05:42:24 -0700 (MST)
Subject: [concurrency-interest] Could park/unpark be changed to directly
	accommodate messages?
Message-ID: <1465389744357-13531.post@n7.nabble.com>

Here's what I had in mind:


unpark(Thread, Object message)

Object   park()


Of course I realize that there are many different ways to indirectly
accomplish the same thing presently (which is part of the problem).  But
isn't this the kind of thing that could be optimized for each platform,
using whatever intrinsics are natively available (not to mention
standardizing a "best approach" for developers)?

This would in effect, just be adding asynchronous messaging to Java, but is
that a bad thing?
I'm sure this isn't the first time that such a thing has been suggested, and
I'm just curious as to what the practical difficulties (spurious "deparks"
are clearly one problem, platform differences are surely another) are that
have prevented its adoption.  I've never seen a discussion directly
addressing the topic, so I thought I would try it here.

Thanks




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From aph at redhat.com  Wed Jun  8 10:24:54 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 8 Jun 2016 15:24:54 +0100
Subject: [concurrency-interest] Could park/unpark be changed to directly
 accommodate messages?
In-Reply-To: <1465389744357-13531.post@n7.nabble.com>
References: <1465389744357-13531.post@n7.nabble.com>
Message-ID: <57582AB6.6070002@redhat.com>

On 08/06/16 13:42, thurstonn wrote:
> Of course I realize that there are many different ways to indirectly
> accomplish the same thing presently (which is part of the problem).  But
> isn't this the kind of thing that could be optimized for each platform,
> using whatever intrinsics are natively available (not to mention
> standardizing a "best approach" for developers)?

park() is the lowest-level possible primitive.  It may be implemented
with very simple code: just a few instructions.  Of course a
message-passing layer can be put on top of park(), but then it
wouldn't be park(), it would be a blocking send and receive.

Andrew.

From thurston at nomagicsoftware.com  Wed Jun  8 09:25:13 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 8 Jun 2016 06:25:13 -0700 (MST)
Subject: [concurrency-interest] Could park/unpark be changed to directly
	accommodate messages?
In-Reply-To: <57582AB6.6070002@redhat.com>
References: <1465389744357-13531.post@n7.nabble.com>
 <57582AB6.6070002@redhat.com>
Message-ID: <1465392313487-13533.post@n7.nabble.com>

Andrew Haley wrote
> On 08/06/16 13:42, thurstonn wrote:
>> Of course I realize that there are many different ways to indirectly
>> accomplish the same thing presently (which is part of the problem).  But
>> isn't this the kind of thing that could be optimized for each platform,
>> using whatever intrinsics are natively available (not to mention
>> standardizing a "best approach" for developers)?
> 
> park() is the lowest-level possible primitive.  It may be implemented
> with very simple code: just a few instructions.  Of course a
> message-passing layer can be put on top of park(), but then it
> wouldn't be park(), it would be a blocking send and receive.
> 
> Andrew.
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

Fair enough, I guess what I'm really asking is isn't it possible to
implement platform-specific send and receive (I'm not talking about a Java
language facade - obviously those can and have been done), then.
And what are the practical difficulties in doing so?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531p13533.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at gmail.com  Wed Jun  8 10:36:37 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 8 Jun 2016 15:36:37 +0100
Subject: [concurrency-interest] Could park/unpark be changed to directly
	accommodate messages?
In-Reply-To: <1465389744357-13531.post@n7.nabble.com>
References: <1465389744357-13531.post@n7.nabble.com>
Message-ID: <5CA9B8E9-A5A6-447A-8DFA-7736841E9C2F@gmail.com>

No.

park() inherently can wake up spuriously. It is up to the designer of the concurrent algorithm to determine whether the wake up was spurious or intended.

Since you are going to solve that problem anyway, you may just as well pass the object at the same time.

Alex

> On 8 Jun 2016, at 13:42, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> Here's what I had in mind:
> 
> 
> unpark(Thread, Object message)
> 
> Object   park()
> 
> 
> Of course I realize that there are many different ways to indirectly
> accomplish the same thing presently (which is part of the problem).  But
> isn't this the kind of thing that could be optimized for each platform,
> using whatever intrinsics are natively available (not to mention
> standardizing a "best approach" for developers)?
> 
> This would in effect, just be adding asynchronous messaging to Java, but is
> that a bad thing?
> I'm sure this isn't the first time that such a thing has been suggested, and
> I'm just curious as to what the practical difficulties (spurious "deparks"
> are clearly one problem, platform differences are surely another) are that
> have prevented its adoption.  I've never seen a discussion directly
> addressing the topic, so I thought I would try it here.
> 
> Thanks
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aph at redhat.com  Wed Jun  8 10:42:48 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 8 Jun 2016 15:42:48 +0100
Subject: [concurrency-interest] Could park/unpark be changed to directly
 accommodate messages?
In-Reply-To: <1465392313487-13533.post@n7.nabble.com>
References: <1465389744357-13531.post@n7.nabble.com>
 <57582AB6.6070002@redhat.com> <1465392313487-13533.post@n7.nabble.com>
Message-ID: <57582EE8.9060002@redhat.com>

On 08/06/16 14:25, thurstonn wrote:
> Fair enough, I guess what I'm really asking is isn't it possible to
> implement platform-specific send and receive (I'm not talking about a Java
> language facade - obviously those can and have been done), then.
> And what are the practical difficulties in doing so?

There are no practical difficulties as far as I'm aware.  A simple JNI
version could be done in an afternoon.  A C2 intrinsic would take a
while longer.

Andrew.

From mr.chrisvest at gmail.com  Wed Jun  8 11:50:53 2016
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Wed, 8 Jun 2016 17:50:53 +0200
Subject: [concurrency-interest] Could park/unpark be changed to directly
	accommodate messages?
In-Reply-To: <1465389744357-13531.post@n7.nabble.com>
References: <1465389744357-13531.post@n7.nabble.com>
Message-ID: <C9255815-16D9-4318-A171-BDC632A45771@gmail.com>

You can store the message in a field on unpark(), then read and clear the field upon returning from park().
That would be the behaviour as far as I can see, except the field wouldn’t necessarily be in the Thread object - you can put it in an object with a bit more context of use, though that creates the problem that the field is now potentially shared across multiple threads.
Meanwhile park() can return in response to zero (spurious) or many unparks, and the reads of the field can race with yet more calls to unpark, so the field access will be racy and prone to lost updates in any case.
The park() and unpark() methods are also used by many different components, which is one of the sources of spurious wake-ups; unparks intended for one park call that ends up waking up a completely different park call in a different (or same) concurrency primitive.

I don’t see it as a problem that there are many different ways to pass information between threads. Different situations demand different properties of the concurrency primitives. We can choose and make trade-offs between properties of delivery, performance and safety guarantees.
Off the top of my head I don’t know of a primitive with such weak safety properties as what I outlined above, so maybe there isn’t very much need for such a thing. That would be my guess, assuming I’ve understood what you are asking.

At one point I had use for, and created, a sort of one-way single-use exchanger (a Promise of sorts) where one thread could wait for a value to appear, and another could with a single method set a field in this exchanger, and unpark the waiting thread. But this still did looping to account for spurious wake-ups, and they were single-use so I could check the field for null to determine if a value had arrived or not. So not quite general asynchronous messaging, and not a use case that would be solved by your API even though it at first glance might look like a good fit.

Cheers,
Chris

> On 08 Jun 2016, at 14:42, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> Here's what I had in mind:
> 
> 
> unpark(Thread, Object message)
> 
> Object   park()
> 
> 
> Of course I realize that there are many different ways to indirectly
> accomplish the same thing presently (which is part of the problem).  But
> isn't this the kind of thing that could be optimized for each platform,
> using whatever intrinsics are natively available (not to mention
> standardizing a "best approach" for developers)?
> 
> This would in effect, just be adding asynchronous messaging to Java, but is
> that a bad thing?
> I'm sure this isn't the first time that such a thing has been suggested, and
> I'm just curious as to what the practical difficulties (spurious "deparks"
> are clearly one problem, platform differences are surely another) are that
> have prevented its adoption.  I've never seen a discussion directly
> addressing the topic, so I thought I would try it here.
> 
> Thanks
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/2582f4f9/attachment.html>

From nathanila at gmail.com  Wed Jun  8 12:18:42 2016
From: nathanila at gmail.com (Nathan & Ila Reynolds)
Date: Wed, 8 Jun 2016 09:18:42 -0700
Subject: [concurrency-interest] Could park/unpark be changed to
	directly	accommodate messages?
In-Reply-To: <C9255815-16D9-4318-A171-BDC632A45771@gmail.com>
References: <1465389744357-13531.post@n7.nabble.com>
 <C9255815-16D9-4318-A171-BDC632A45771@gmail.com>
Message-ID: <026501d1c1a1$6d5aaed0$48100c70$@gmail.com>

If I understand correctly, some environments really can cause the thread to return from park() without a call to unpark().  This is a spurious return of park().

 

There is another source of “spurious” returns of park().  With so many concurrency constructs throughout a large project, there will be times when Thread A calls unpark() on Thread B and Thread B has long since left the associated code and is executing in some other area of the large project.  In fact, Thread B could be parked somewhere else and become unparked needlessly.

 

The same can happen with Thread.interrupt().  I have actually seen this case.  A thread called into some library and then came out of the library.  The thread then called Object.wait() on an unrelated object only to be erroneously interrupted by another thread in the library.

 

My point is that one has to always handle “spurious” returns of park().  park() should almost always be put in a loop.  When the thread wakes up from park(), it should check the condition inside the loop and then either exit the loop or go back to park().

 

Since the thread is almost always in a park() loop, one can easily pass a message to the thread which wakes up.  In fact, couldn’t one use an Exchanger to pass messages?

 

-Nathan

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Chris Vest
Sent: Wednesday, June 08, 2016 8:51 AM
To: thurstonn <thurston at nomagicsoftware.com>
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Could park/unpark be changed to directly accommodate messages?

 

You can store the message in a field on unpark(), then read and clear the field upon returning from park().

That would be the behaviour as far as I can see, except the field wouldn’t necessarily be in the Thread object - you can put it in an object with a bit more context of use, though that creates the problem that the field is now potentially shared across multiple threads.

Meanwhile park() can return in response to zero (spurious) or many unparks, and the reads of the field can race with yet more calls to unpark, so the field access will be racy and prone to lost updates in any case.

The park() and unpark() methods are also used by many different components, which is one of the sources of spurious wake-ups; unparks intended for one park call that ends up waking up a completely different park call in a different (or same) concurrency primitive.

 

I don’t see it as a problem that there are many different ways to pass information between threads. Different situations demand different properties of the concurrency primitives. We can choose and make trade-offs between properties of delivery, performance and safety guarantees.

Off the top of my head I don’t know of a primitive with such weak safety properties as what I outlined above, so maybe there isn’t very much need for such a thing. That would be my guess, assuming I’ve understood what you are asking.

 

At one point I had use for, and created, a sort of one-way single-use exchanger (a Promise of sorts) where one thread could wait for a value to appear, and another could with a single method set a field in this exchanger, and unpark the waiting thread. But this still did looping to account for spurious wake-ups, and they were single-use so I could check the field for null to determine if a value had arrived or not. So not quite general asynchronous messaging, and not a use case that would be solved by your API even though it at first glance might look like a good fit.


Cheers,

Chris

 

On 08 Jun 2016, at 14:42, thurstonn <thurston at nomagicsoftware.com <mailto:thurston at nomagicsoftware.com> > wrote:

 

Here's what I had in mind:


unpark(Thread, Object message)

Object   park()


Of course I realize that there are many different ways to indirectly
accomplish the same thing presently (which is part of the problem).  But
isn't this the kind of thing that could be optimized for each platform,
using whatever intrinsics are natively available (not to mention
standardizing a "best approach" for developers)?

This would in effect, just be adding asynchronous messaging to Java, but is
that a bad thing?
I'm sure this isn't the first time that such a thing has been suggested, and
I'm just curious as to what the practical difficulties (spurious "deparks"
are clearly one problem, platform differences are surely another) are that
have prevented its adoption.  I've never seen a discussion directly
addressing the topic, so I thought I would try it here.

Thanks




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com <http://nabble.com> .
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160608/4a4476ea/attachment.html>

From david.lloyd at redhat.com  Wed Jun  8 12:29:36 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 8 Jun 2016 11:29:36 -0500
Subject: [concurrency-interest] Could park/unpark be changed to directly
 accommodate messages?
In-Reply-To: <026501d1c1a1$6d5aaed0$48100c70$@gmail.com>
References: <1465389744357-13531.post@n7.nabble.com>
 <C9255815-16D9-4318-A171-BDC632A45771@gmail.com>
 <026501d1c1a1$6d5aaed0$48100c70$@gmail.com>
Message-ID: <575847F0.3050106@redhat.com>

This seems closely related to the occasionally-mentioned idea of parking 
relative to a change to a particular memory location, which might be the 
better primitive choice since it could probably be used to implement 
message passing (among other things).  I'm not sure that what we have 
for VarHandle support is sufficient to accomplish this though.

On 06/08/2016 11:18 AM, Nathan & Ila Reynolds wrote:
> If I understand correctly, some environments really can cause the thread
> to return from park() without a call to unpark().  This is a spurious
> return of park().
>
> There is another source of “spurious” returns of park().  With so many
> concurrency constructs throughout a large project, there will be times
> when Thread A calls unpark() on Thread B and Thread B has long since
> left the associated code and is executing in some other area of the
> large project.  In fact, Thread B could be parked somewhere else and
> become unparked needlessly.
>
> The same can happen with Thread.interrupt().  I have actually seen this
> case.  A thread called into some library and then came out of the
> library.  The thread then called Object.wait() on an unrelated object
> only to be erroneously interrupted by another thread in the library.
>
> My point is that one has to always handle “spurious” returns of park().
> park() should almost always be put in a loop.  When the thread wakes up
> from park(), it should check the condition inside the loop and then
> either exit the loop or go back to park().
>
> Since the thread is almost always in a park() loop, one can easily pass
> a message to the thread which wakes up.  In fact, couldn’t one use an
> Exchanger to pass messages?
>
> -Nathan
>
> *From:*Concurrency-interest
> [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Chris
> Vest
> *Sent:* Wednesday, June 08, 2016 8:51 AM
> *To:* thurstonn <thurston at nomagicsoftware.com>
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Could park/unpark be changed to
> directly accommodate messages?
>
> You can store the message in a field on unpark(), then read and clear
> the field upon returning from park().
>
> That would be the behaviour as far as I can see, except the field
> wouldn’t necessarily be in the Thread object - you can put it in an
> object with a bit more context of use, though that creates the problem
> that the field is now potentially shared across multiple threads.
>
> Meanwhile park() can return in response to zero (spurious) or many
> unparks, and the reads of the field can race with yet more calls to
> unpark, so the field access will be racy and prone to lost updates in
> any case.
>
> The park() and unpark() methods are also used by many different
> components, which is one of the sources of spurious wake-ups; unparks
> intended for one park call that ends up waking up a completely different
> park call in a different (or same) concurrency primitive.
>
> I don’t see it as a problem that there are many different ways to pass
> information between threads. Different situations demand different
> properties of the concurrency primitives. We can choose and make
> trade-offs between properties of delivery, performance and safety
> guarantees.
>
> Off the top of my head I don’t know of a primitive with such weak safety
> properties as what I outlined above, so maybe there isn’t very much need
> for such a thing. That would be my guess, assuming I’ve understood what
> you are asking.
>
> At one point I had use for, and created, a sort of one-way single-use
> exchanger (a Promise of sorts) where one thread could wait for a value
> to appear, and another could with a single method set a field in this
> exchanger, and unpark the waiting thread. But this still did looping to
> account for spurious wake-ups, and they were single-use so I could check
> the field for null to determine if a value had arrived or not. So not
> quite general asynchronous messaging, and not a use case that would be
> solved by your API even though it at first glance might look like a good
> fit.
>
>
> Cheers,
>
> Chris
>
>     On 08 Jun 2016, at 14:42, thurstonn <thurston at nomagicsoftware.com
>     <mailto:thurston at nomagicsoftware.com>> wrote:
>
>     Here's what I had in mind:
>
>
>     unpark(Thread, Object message)
>
>     Object   park()
>
>
>     Of course I realize that there are many different ways to indirectly
>     accomplish the same thing presently (which is part of the problem).  But
>     isn't this the kind of thing that could be optimized for each platform,
>     using whatever intrinsics are natively available (not to mention
>     standardizing a "best approach" for developers)?
>
>     This would in effect, just be adding asynchronous messaging to Java,
>     but is
>     that a bad thing?
>     I'm sure this isn't the first time that such a thing has been
>     suggested, and
>     I'm just curious as to what the practical difficulties (spurious
>     "deparks"
>     are clearly one problem, platform differences are surely another)
>     are that
>     have prevented its adoption.  I've never seen a discussion directly
>     addressing the topic, so I thought I would try it here.
>
>     Thanks
>
>
>
>
>     --
>     View this message in context:
>     http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com
>     <http://nabble.com>.
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-- 
- DML

From thurston at nomagicsoftware.com  Wed Jun  8 11:41:35 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 8 Jun 2016 08:41:35 -0700 (MST)
Subject: [concurrency-interest] Could park/unpark be changed to directly
	accommodate messages?
In-Reply-To: <575847F0.3050106@redhat.com>
References: <1465389744357-13531.post@n7.nabble.com>
 <C9255815-16D9-4318-A171-BDC632A45771@gmail.com>
 <026501d1c1a1$6d5aaed0$48100c70$@gmail.com> <575847F0.3050106@redhat.com>
Message-ID: <1465400495771-13539.post@n7.nabble.com>

Similar, but not quite the same; because in your posit, you'd need to define
the semantics of multiple threads parking on the same memory address (which
one of those threads gets "deparked"? it's very similar to notify()/wait()
in that respect)

It's often desirable that a program can select which thread it wants to
communicate with, using whatever program-specific criteria it needs (e.g.
thread waiting longest)


David M. Lloyd-3 wrote
> This seems closely related to the occasionally-mentioned idea of parking 
> relative to a change to a particular memory location, which might be the 
> better primitive choice since it could probably be used to implement 
> message passing (among other things).  I'm not sure that what we have 
> for VarHandle support is sufficient to accomplish this though.
> 
> On 06/08/2016 11:18 AM, Nathan & Ila Reynolds wrote:
>> If I understand correctly, some environments really can cause the thread
>> to return from park() without a call to unpark().  This is a spurious
>> return of park().
>>
>> There is another source of “spurious” returns of park().  With so many
>> concurrency constructs throughout a large project, there will be times
>> when Thread A calls unpark() on Thread B and Thread B has long since
>> left the associated code and is executing in some other area of the
>> large project.  In fact, Thread B could be parked somewhere else and
>> become unparked needlessly.
>>
>> The same can happen with Thread.interrupt().  I have actually seen this
>> case.  A thread called into some library and then came out of the
>> library.  The thread then called Object.wait() on an unrelated object
>> only to be erroneously interrupted by another thread in the library.
>>
>> My point is that one has to always handle “spurious” returns of park().
>> park() should almost always be put in a loop.  When the thread wakes up
>> from park(), it should check the condition inside the loop and then
>> either exit the loop or go back to park().
>>
>> Since the thread is almost always in a park() loop, one can easily pass
>> a message to the thread which wakes up.  In fact, couldn’t one use an
>> Exchanger to pass messages?
>>
>> -Nathan
>>
>> *From:*Concurrency-interest
>> [mailto:

> concurrency-interest-bounces at .oswego

> ] *On Behalf Of *Chris
>> Vest
>> *Sent:* Wednesday, June 08, 2016 8:51 AM
>> *To:* thurstonn &lt;

> thurston@

> &gt;
>> *Cc:* 

> concurrency-interest at .oswego

>> *Subject:* Re: [concurrency-interest] Could park/unpark be changed to
>> directly accommodate messages?
>>
>> You can store the message in a field on unpark(), then read and clear
>> the field upon returning from park().
>>
>> That would be the behaviour as far as I can see, except the field
>> wouldn’t necessarily be in the Thread object - you can put it in an
>> object with a bit more context of use, though that creates the problem
>> that the field is now potentially shared across multiple threads.
>>
>> Meanwhile park() can return in response to zero (spurious) or many
>> unparks, and the reads of the field can race with yet more calls to
>> unpark, so the field access will be racy and prone to lost updates in
>> any case.
>>
>> The park() and unpark() methods are also used by many different
>> components, which is one of the sources of spurious wake-ups; unparks
>> intended for one park call that ends up waking up a completely different
>> park call in a different (or same) concurrency primitive.
>>
>> I don’t see it as a problem that there are many different ways to pass
>> information between threads. Different situations demand different
>> properties of the concurrency primitives. We can choose and make
>> trade-offs between properties of delivery, performance and safety
>> guarantees.
>>
>> Off the top of my head I don’t know of a primitive with such weak safety
>> properties as what I outlined above, so maybe there isn’t very much need
>> for such a thing. That would be my guess, assuming I’ve understood what
>> you are asking.
>>
>> At one point I had use for, and created, a sort of one-way single-use
>> exchanger (a Promise of sorts) where one thread could wait for a value
>> to appear, and another could with a single method set a field in this
>> exchanger, and unpark the waiting thread. But this still did looping to
>> account for spurious wake-ups, and they were single-use so I could check
>> the field for null to determine if a value had arrived or not. So not
>> quite general asynchronous messaging, and not a use case that would be
>> solved by your API even though it at first glance might look like a good
>> fit.
>>
>>
>> Cheers,
>>
>> Chris
>>
>>     On 08 Jun 2016, at 14:42, thurstonn &lt;

> thurston@

> &gt;     &lt;mailto:

> thurston@

> &gt;> wrote:
>>
>>     Here's what I had in mind:
>>
>>
>>     unpark(Thread, Object message)
>>
>>     Object   park()
>>
>>
>>     Of course I realize that there are many different ways to indirectly
>>     accomplish the same thing presently (which is part of the problem). 
>> But
>>     isn't this the kind of thing that could be optimized for each
>> platform,
>>     using whatever intrinsics are natively available (not to mention
>>     standardizing a "best approach" for developers)?
>>
>>     This would in effect, just be adding asynchronous messaging to Java,
>>     but is
>>     that a bad thing?
>>     I'm sure this isn't the first time that such a thing has been
>>     suggested, and
>>     I'm just curious as to what the practical difficulties (spurious
>>     "deparks"
>>     are clearly one problem, platform differences are surely another)
>>     are that
>>     have prevented its adoption.  I've never seen a discussion directly
>>     addressing the topic, so I thought I would try it here.
>>
>>     Thanks
>>
>>
>>
>>
>>     --
>>     View this message in context:
>>    
>> http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531.html
>>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com
>>     &lt;http://nabble.com&gt;.
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     

> Concurrency-interest at .oswego

>>     &lt;mailto:

> Concurrency-interest at .oswego

> &gt;
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> 

> Concurrency-interest at .oswego

>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> 
> -- 
> - DML
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Could-park-unpark-be-changed-to-directly-accommodate-messages-tp13531p13539.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From nitsanw at azulsystems.com  Thu Jun  9 05:04:17 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Thu, 9 Jun 2016 10:04:17 +0100
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
Message-ID: <57593111.6030209@azulsystems.com>

Currently thread local resources can be managed using a ThreadLocal. 
ThreadLocals are initialized on 'discovery' and never explicitly discarded.
I would like to have Thread lifecycle aware thread resources. There is a 
use case for this already inside the JDK with thread local direct byte 
buffers. Currently the cleanup of the thread local byte buffers is 
driven by finalization, this can result in issues in high thread churn 
applications. With lifecycle awareness the buffers could be cleaned up 
on thread exit.
Here is a suggested enhancement to TL to allow this behavior:
   final static ThreadLocal<ByteBuffer> TL_BUFFER = new 
ThreadLocal<ByteBuffer>() {
     @Override
     protected ByteBuffer initialValue() {
       return ByteBuffer.allocateDirect(4096);
     }
     @Override
     protected void discard(ByteBuffer bb) {
       ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
     }
   };

The discard method will be called on thread exit.
Am I missing an already existing way of achieving the same? Does this 
functionality somehow not sit well with existing APIs?

From dawid.weiss at gmail.com  Thu Jun  9 05:21:01 2016
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Thu, 9 Jun 2016 11:21:01 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <57593111.6030209@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
Message-ID: <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>

Apache Lucene has a similar problem and a workaround here:
https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java

Dawid

On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com> wrote:
> Currently thread local resources can be managed using a ThreadLocal.
> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
> I would like to have Thread lifecycle aware thread resources. There is a use
> case for this already inside the JDK with thread local direct byte buffers.
> Currently the cleanup of the thread local byte buffers is driven by
> finalization, this can result in issues in high thread churn applications.
> With lifecycle awareness the buffers could be cleaned up on thread exit.
> Here is a suggested enhancement to TL to allow this behavior:
>   final static ThreadLocal<ByteBuffer> TL_BUFFER = new
> ThreadLocal<ByteBuffer>() {
>     @Override
>     protected ByteBuffer initialValue() {
>       return ByteBuffer.allocateDirect(4096);
>     }
>     @Override
>     protected void discard(ByteBuffer bb) {
>       ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>     }
>   };
>
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this
> functionality somehow not sit well with existing APIs?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From viktor.klang at gmail.com  Thu Jun  9 05:26:41 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 9 Jun 2016 11:26:41 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <57593111.6030209@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
Message-ID: <CANPzfU_Er=QmJpjP=GUOtrUVGHkfLN3-6cLWd5tsoP-Y9=6ENw@mail.gmail.com>

Speaking of ThreadLocals, especially for TL propagation it would be
splendid if one could get *ALL* TLs currently initialized for a given
Thread instance.

On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com>
wrote:

> Currently thread local resources can be managed using a ThreadLocal.
> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
> I would like to have Thread lifecycle aware thread resources. There is a
> use case for this already inside the JDK with thread local direct byte
> buffers. Currently the cleanup of the thread local byte buffers is driven
> by finalization, this can result in issues in high thread churn
> applications. With lifecycle awareness the buffers could be cleaned up on
> thread exit.
> Here is a suggested enhancement to TL to allow this behavior:
>   final static ThreadLocal<ByteBuffer> TL_BUFFER = new
> ThreadLocal<ByteBuffer>() {
>     @Override
>     protected ByteBuffer initialValue() {
>       return ByteBuffer.allocateDirect(4096);
>     }
>     @Override
>     protected void discard(ByteBuffer bb) {
>       ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>     }
>   };
>
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this
> functionality somehow not sit well with existing APIs?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/9cff42c0/attachment.html>

From davidcholmes at aapt.net.au  Thu Jun  9 05:37:31 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 9 Jun 2016 19:37:31 +1000
Subject: [concurrency-interest] Thread local resource management
	feature	request/discussion
In-Reply-To: <CANPzfU_Er=QmJpjP=GUOtrUVGHkfLN3-6cLWd5tsoP-Y9=6ENw@mail.gmail.com>
References: <57593111.6030209@azulsystems.com>
 <CANPzfU_Er=QmJpjP=GUOtrUVGHkfLN3-6cLWd5tsoP-Y9=6ENw@mail.gmail.com>
Message-ID: <00c701d1c232$8bc01630$a3404290$@aapt.net.au>

That would totally break encapsulation. The ThreadLocal variable belongs to the code in which it is declared and used. Some other piece of code should not be able to gather all such variables – just like you couldn’t (and shouldn’t) be able to gather a list of all (for example) static fields that a thread can access.

 

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Viktor Klang
Sent: Thursday, June 9, 2016 7:27 PM
To: Nitsan Wakart <nitsanw at azulsystems.com>
Cc: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] Thread local resource management feature request/discussion

 

Speaking of ThreadLocals, especially for TL propagation it would be splendid if one could get *ALL* TLs currently initialized for a given Thread instance.

 

On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com> wrote:

Currently thread local resources can be managed using a ThreadLocal. ThreadLocals are initialized on 'discovery' and never explicitly discarded.
I would like to have Thread lifecycle aware thread resources. There is a use case for this already inside the JDK with thread local direct byte buffers. Currently the cleanup of the thread local byte buffers is driven by finalization, this can result in issues in high thread churn applications. With lifecycle awareness the buffers could be cleaned up on thread exit.
Here is a suggested enhancement to TL to allow this behavior:
  final static ThreadLocal<ByteBuffer> TL_BUFFER = new ThreadLocal<ByteBuffer>() {
    @Override
    protected ByteBuffer initialValue() {
      return ByteBuffer.allocateDirect(4096);
    }
    @Override
    protected void discard(ByteBuffer bb) {
      ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
    }
  };

The discard method will be called on thread exit.
Am I missing an already existing way of achieving the same? Does this functionality somehow not sit well with existing APIs?
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





 

-- 

Cheers,

√

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/5b971163/attachment.html>

From viktor.klang at gmail.com  Thu Jun  9 05:48:12 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 9 Jun 2016 11:48:12 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <00c701d1c232$8bc01630$a3404290$@aapt.net.au>
References: <57593111.6030209@azulsystems.com>
 <CANPzfU_Er=QmJpjP=GUOtrUVGHkfLN3-6cLWd5tsoP-Y9=6ENw@mail.gmail.com>
 <00c701d1c232$8bc01630$a3404290$@aapt.net.au>
Message-ID: <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>

On Thu, Jun 9, 2016 at 11:37 AM, David Holmes <davidcholmes at aapt.net.au>
wrote:

> That would totally break encapsulation.
>

You mean like reflection and setAccessible(true)? ;)


> The ThreadLocal variable belongs to the code in which it is declared and
> used. Some other piece of code should not be able to gather all such
> variables – just like you couldn’t (and shouldn’t) be able to gather a list
> of all (for example) static fields that a thread can access.
>

You can already do this (collect which ThreadLocals are used) with
instrumentation though.

Right now it's a real pain point with async programs to propagate
ThreadLocals (since you don't know which thread locals to propagate).



>
>
> David
>
>
>
> *From:* Concurrency-interest [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Viktor Klang
> *Sent:* Thursday, June 9, 2016 7:27 PM
> *To:* Nitsan Wakart <nitsanw at azulsystems.com>
> *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Subject:* Re: [concurrency-interest] Thread local resource management
> feature request/discussion
>
>
>
> Speaking of ThreadLocals, especially for TL propagation it would be
> splendid if one could get *ALL* TLs currently initialized for a given
> Thread instance.
>
>
>
> On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com>
> wrote:
>
> Currently thread local resources can be managed using a ThreadLocal.
> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
> I would like to have Thread lifecycle aware thread resources. There is a
> use case for this already inside the JDK with thread local direct byte
> buffers. Currently the cleanup of the thread local byte buffers is driven
> by finalization, this can result in issues in high thread churn
> applications. With lifecycle awareness the buffers could be cleaned up on
> thread exit.
> Here is a suggested enhancement to TL to allow this behavior:
>   final static ThreadLocal<ByteBuffer> TL_BUFFER = new
> ThreadLocal<ByteBuffer>() {
>     @Override
>     protected ByteBuffer initialValue() {
>       return ByteBuffer.allocateDirect(4096);
>     }
>     @Override
>     protected void discard(ByteBuffer bb) {
>       ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>     }
>   };
>
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this
> functionality somehow not sit well with existing APIs?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> --
>
> Cheers,
>
> √
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/03307ba4/attachment-0001.html>

From jw_list at headissue.com  Thu Jun  9 06:06:50 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Thu, 09 Jun 2016 12:06:50 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>
References: <57593111.6030209@azulsystems.com>
 <00c701d1c232$8bc01630$a3404290$@aapt.net.au>
 <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>
Message-ID: <1666616.HKOpY9jgIp@tapsy>

On Thursday 09 June 2016 11:48:12 Viktor Klang wrote:
> Right now it's a real pain point with async programs to propagate ThreadLocals (since you don't know which thread locals to propagate).

If you have a list of all thread locals you don't know which one to propagate either. What is the meaning of _thread local_ again?!

You can define a thread factory and everyone wanting to propagate thread related things, can register there.

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From viktor.klang at gmail.com  Thu Jun  9 07:26:46 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 9 Jun 2016 13:26:46 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <1666616.HKOpY9jgIp@tapsy>
References: <57593111.6030209@azulsystems.com>
 <00c701d1c232$8bc01630$a3404290$@aapt.net.au>
 <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>
 <1666616.HKOpY9jgIp@tapsy>
Message-ID: <CANPzfU-3jD5Ag3Yhpq6zyfEDCEEXkGguGTFM-RGOn+FAsQA1kw@mail.gmail.com>

On Jun 9, 2016 12:06 PM, "Jens Wilke" <jw_list at headissue.com> wrote:
>
> On Thursday 09 June 2016 11:48:12 Viktor Klang wrote:
> > Right now it's a real pain point with async programs to propagate
ThreadLocals (since you don't know which thread locals to propagate).
>
> If you have a list of all thread locals you don't know which one to
propagate either. What is the meaning of _thread local_ again?!

Would you mind elaborating a bit there--what exactly wouldn't work?

>
> You can define a thread factory and everyone wanting to propagate thread
related things, can register there.

Doesn't work because A) legacy code/libraries and B) becomes end-user
concern and C) how does the end User find and access all TLs needed and
track whether new TLs have been introduced between lib upgrades?

Perhaps what is needed is a reboot: AsyncLocals and AsyncTraces to replace
ThreadLocal and StackTrace(Elements) (since the Async versions subsume the
thread locals if we can specify that ThreadLocal is an AsyncLocal which is
pinned to a specific Async context--a thread instance)

>
> Cheers,
>
> Jens
>
> --
> "Everything superfluous is wrong!"
>
>    // Jens Wilke - headissue GmbH - Germany
>  \//  https://headissue.com
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/8824ae61/attachment.html>

From david.lloyd at redhat.com  Thu Jun  9 07:37:35 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 9 Jun 2016 06:37:35 -0500
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <57593111.6030209@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
Message-ID: <575954FF.1060607@redhat.com>

On 06/09/2016 04:04 AM, Nitsan Wakart wrote:
> Currently thread local resources can be managed using a ThreadLocal.
> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
> I would like to have Thread lifecycle aware thread resources. There is a
> use case for this already inside the JDK with thread local direct byte
> buffers. Currently the cleanup of the thread local byte buffers is
> driven by finalization, this can result in issues in high thread churn
> applications. With lifecycle awareness the buffers could be cleaned up
> on thread exit.

A simple solution to this is to keep TLs private and only allow them to 
be set in combination with a function or action that clears it on exit:

MyContextual c = ...something that keeps an internal TL...
MyResult r = c.doWith(a, b, (aa, ab) -> { ...TL is set only inside of 
here... });

Lexical scoping means never leaking.  Adding support for recursive 
doWith is easy too.

Granted it doesn't really solve 100% of use cases but it does help with 
some.  I agree that *ThreadLocals should be allowed to self-clean on 
thread exit; it's a bit crazy that they don't.  We (JBoss) use some 
pretty hideous reflection hacks today to accomplish this.

> Here is a suggested enhancement to TL to allow this behavior:
>    final static ThreadLocal<ByteBuffer> TL_BUFFER = new
> ThreadLocal<ByteBuffer>() {
>      @Override
>      protected ByteBuffer initialValue() {
>        return ByteBuffer.allocateDirect(4096);
>      }
>      @Override
>      protected void discard(ByteBuffer bb) {
>        ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>      }
>    };
>
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this
> functionality somehow not sit well with existing APIs?

I like this idea but some may balk at adding a virtual method to classes 
which are meant to be extended.  Might make more sense to have a 
Runnable cleanup action (or Consumer or whatever) that could be supplied 
to the *ThreadLocal constructor.

-- 
- DML

From jw_list at headissue.com  Thu Jun  9 08:18:28 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Thu, 09 Jun 2016 14:18:28 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <CANPzfU-3jD5Ag3Yhpq6zyfEDCEEXkGguGTFM-RGOn+FAsQA1kw@mail.gmail.com>
References: <57593111.6030209@azulsystems.com> <1666616.HKOpY9jgIp@tapsy>
 <CANPzfU-3jD5Ag3Yhpq6zyfEDCEEXkGguGTFM-RGOn+FAsQA1kw@mail.gmail.com>
Message-ID: <23270352.7ASVzBT4LD@tapsy>

On Thursday 09 June 2016 13:26:46 Viktor Klang wrote:
> On Jun 9, 2016 12:06 PM, "Jens Wilke" <jw_list at headissue.com> wrote:
> >
> > On Thursday 09 June 2016 11:48:12 Viktor Klang wrote:
> > > Right now it's a real pain point with async programs to propagate ThreadLocals (since you don't know which thread locals to propagate).
> >
> > If you have a list of all thread locals you don't know which one to propagate either. What is the meaning of _thread local_ again?!
> 
> Would you mind elaborating a bit there--what exactly wouldn't work?

A thread local is local to one thread. Not local to one thread and others considering they have a common parent.

For example nested transactions: If you have a transaction context, you may want to propagate the transaction context. But you do do not necessarily want to copy the context instance and treat the two threads equally, because a tx.commit() in a child thread should not have the same effect as the tx.commit() in the parent thread.

Whether it needs to be really thread local, or whether it is context that needs to be propagated depends on the library and use case. Futhermore,
if you know it needs to be propagated, the "how" might differ as well.

> > You can define a thread factory and everyone wanting to propagate thread related things, can register there.
> 
> Doesn't work because

> A) legacy code/libraries

Yes, but nothing will, since there is no standard what should happen. Propagating every TL might just create a big memory leak.

> and B) becomes end-user concern and 

A thread local propagator can be a SPI and every library needing this can provide one.

> C) how does the end User find and access all TLs needed and track whether new TLs have been introduced between lib upgrades?

It does not and never should.

> Perhaps what is needed is a reboot: AsyncLocals and AsyncTraces to replace ThreadLocal and StackTrace(Elements) (since the Async versions subsume the thread locals if we can specify that ThreadLocal is an AsyncLocal which is pinned to a specific Async context--a thread instance)

What has async to do with thread locals? ;) Wheels can be used not just for cars.

But srsly, mind about the concepts you try to name. How can a local variable reference be asynchronous by itself?
Hope you don't mind the hint from someone who is not doing only async with TLs.

So maybe its an AutoPropagatedThreadLocal?!

But yes, stack traces is a pain point, too. Just trying to fix the JCache TCK because it is actually asserted that the identical exception instance from another thread is rethrown.

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From viktor.klang at gmail.com  Thu Jun  9 08:38:55 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 9 Jun 2016 14:38:55 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <23270352.7ASVzBT4LD@tapsy>
References: <57593111.6030209@azulsystems.com> <1666616.HKOpY9jgIp@tapsy>
 <CANPzfU-3jD5Ag3Yhpq6zyfEDCEEXkGguGTFM-RGOn+FAsQA1kw@mail.gmail.com>
 <23270352.7ASVzBT4LD@tapsy>
Message-ID: <CANPzfU_Uba2hCMTxYbU8Ne2AtPuryOQNnTUmFd1MuZ+gF_typg@mail.gmail.com>

On Thu, Jun 9, 2016 at 2:18 PM, Jens Wilke <jw_list at headissue.com> wrote:

> On Thursday 09 June 2016 13:26:46 Viktor Klang wrote:
> > On Jun 9, 2016 12:06 PM, "Jens Wilke" <jw_list at headissue.com> wrote:
> > >
> > > On Thursday 09 June 2016 11:48:12 Viktor Klang wrote:
> > > > Right now it's a real pain point with async programs to propagate
> ThreadLocals (since you don't know which thread locals to propagate).
> > >
> > > If you have a list of all thread locals you don't know which one to
> propagate either. What is the meaning of _thread local_ again?!
> >
> > Would you mind elaborating a bit there--what exactly wouldn't work?
>
> A thread local is local to one thread. Not local to one thread and others
> considering they have a common parent.
>
> For example nested transactions: If you have a transaction context, you
> may want to propagate the transaction context. But you do do not
> necessarily want to copy the context instance and treat the two threads
> equally, because a tx.commit() in a child thread should not have the same
> effect as the tx.commit() in the parent thread.
>

How would this differ with propagation?


>
> Whether it needs to be really thread local, or whether it is context that
> needs to be propagated depends on the library and use case. Futhermore,
> if you know it needs to be propagated, the "how" might differ as well.
>

Do you have a concrete example?


>
> > > You can define a thread factory and everyone wanting to propagate
> thread related things, can register there.
> >
> > Doesn't work because
>
> > A) legacy code/libraries
>
> Yes, but nothing will, since there is no standard what should happen.
> Propagating every TL might just create a big memory leak.
>

This is interesting that you mention, because this is the problem Nitsan is
trying to address with having "disposable TLs" (i.e. TLs already have
issues with memory leakage and undesired memory retention. ClassLoader
leakages most prominently)


>
> > and B) becomes end-user concern and
>
> A thread local propagator can be a SPI and every library needing this can
> provide one.
>

Absolutely. So by Java 25 we might see broad adoption and all old
dependencies will be phased out. Not really tracable :)


>
> > C) how does the end User find and access all TLs needed and track
> whether new TLs have been introduced between lib upgrades?
>
> It does not and never should.
>

I agree completely :)


>
> > Perhaps what is needed is a reboot: AsyncLocals and AsyncTraces to
> replace ThreadLocal and StackTrace(Elements) (since the Async versions
> subsume the thread locals if we can specify that ThreadLocal is an
> AsyncLocal which is pinned to a specific Async context--a thread instance)
>
> What has async to do with thread locals? ;) Wheels can be used not just
> for cars.
>

There *does* seem to be prior art here:
http://blog.cincura.net/233526-asynclocal-t-in-net-46/


>
> But srsly, mind about the concepts you try to name. How can a local
> variable reference be asynchronous by itself?
> Hope you don't mind the hint from someone who is not doing only async with
> TLs.
>

My power screwdriver was sold as a screwdriver but it's pretty good at
drilling too. :)


>
> So maybe its an AutoPropagatedThreadLocal?!
>

…ContextFactoryMetaProviderServiceLocatorBean :)


>
> But yes, stack traces is a pain point, too. Just trying to fix the JCache
> TCK because it is actually asserted that the identical exception instance
> from another thread is rethrown.
>

Don't get me started on StackTraceElement :)


>
> Cheers,
>
> Jens
>
> --
> "Everything superfluous is wrong!"
>
>    // Jens Wilke - headissue GmbH - Germany
>  \//  https://headissue.com
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/45499d57/attachment-0001.html>

From david.lloyd at redhat.com  Thu Jun  9 09:00:42 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Thu, 9 Jun 2016 08:00:42 -0500
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <73564C83-C610-41ED-8FF4-B23556C79F9B@googlemail.com>
References: <57593111.6030209@azulsystems.com> <575954FF.1060607@redhat.com>
 <73564C83-C610-41ED-8FF4-B23556C79F9B@googlemail.com>
Message-ID: <5759687A.80404@redhat.com>

On 06/09/2016 06:43 AM, Norman Maurer wrote:
> Hey David,
>
>
>> On 09 Jun 2016, at 13:37, David M. Lloyd <david.lloyd at redhat.com> wrote:
>>
>> On 06/09/2016 04:04 AM, Nitsan Wakart wrote:
>>> Currently thread local resources can be managed using a ThreadLocal.
>>> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
>>> I would like to have Thread lifecycle aware thread resources. There is a
>>> use case for this already inside the JDK with thread local direct byte
>>> buffers. Currently the cleanup of the thread local byte buffers is
>>> driven by finalization, this can result in issues in high thread churn
>>> applications. With lifecycle awareness the buffers could be cleaned up
>>> on thread exit.
>>
>> A simple solution to this is to keep TLs private and only allow them to be set in combination with a function or action that clears it on exit:
>>
>> MyContextual c = ...something that keeps an internal TL...
>> MyResult r = c.doWith(a, b, (aa, ab) -> { ...TL is set only inside of here... });
>>
>> Lexical scoping means never leaking.  Adding support for recursive doWith is easy too.
>>
>> Granted it doesn't really solve 100% of use cases but it does help with some.  I agree that *ThreadLocals should be allowed to self-clean on thread exit; it's a bit crazy that they don't.  We (JBoss) use some pretty hideous reflection hacks today to accomplish this.
>
> Do you have a link to one of such “pretty hideous reflection hacks” ;) ?

Sure (though this just unreferences, it doesn't actually call remove() - 
the idea being just to avoid memory leaks in the event that the Thread 
reference is not released for long periods of time):

https://github.com/jbossas/jboss-threads/blob/master/src/main/java/org/jboss/threads/ThreadLocalResetter.java#L29

-- 
- DML

From thurston at nomagicsoftware.com  Thu Jun  9 10:36:10 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 9 Jun 2016 07:36:10 -0700 (MST)
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <57593111.6030209@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
Message-ID: <1465482970379-13551.post@n7.nabble.com>

And how would you propose dealing with the case where the ThreadLocal
"referent" (ByteBuffer in your case) is strongly reachable outside the
exiting thread's ThreadLocalMap?


Nitsan Wakart-2 wrote
> Currently thread local resources can be managed using a ThreadLocal. 
> ThreadLocals are initialized on 'discovery' and never explicitly
> discarded.
> I would like to have Thread lifecycle aware thread resources. There is a 
> use case for this already inside the JDK with thread local direct byte 
> buffers. Currently the cleanup of the thread local byte buffers is 
> driven by finalization, this can result in issues in high thread churn 
> applications. With lifecycle awareness the buffers could be cleaned up 
> on thread exit.
> Here is a suggested enhancement to TL to allow this behavior:
>    final static ThreadLocal
> <ByteBuffer>
>  TL_BUFFER = new 
> ThreadLocal
> <ByteBuffer>
> () {
>      @Override
>      protected ByteBuffer initialValue() {
>        return ByteBuffer.allocateDirect(4096);
>      }
>      @Override
>      protected void discard(ByteBuffer bb) {
>        ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>      }
>    };
> 
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this 
> functionality somehow not sit well with existing APIs?
> _______________________________________________
> Concurrency-interest mailing list

> Concurrency-interest at .oswego

> http://cs.oswego.edu/mailman/listinfo/concurrency-interest





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Thread-local-resource-management-feature-request-discussion-tp13540p13551.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From jw_list at headissue.com  Thu Jun  9 15:27:28 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Thu, 09 Jun 2016 21:27:28 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <CANPzfU_Uba2hCMTxYbU8Ne2AtPuryOQNnTUmFd1MuZ+gF_typg@mail.gmail.com>
References: <57593111.6030209@azulsystems.com> <23270352.7ASVzBT4LD@tapsy>
 <CANPzfU_Uba2hCMTxYbU8Ne2AtPuryOQNnTUmFd1MuZ+gF_typg@mail.gmail.com>
Message-ID: <5645389.gl4sZXeyCn@tapsy>

On Thursday 09 June 2016 14:38:55 Viktor Klang wrote:
> > A thread local is local to one thread. Not local to one thread and others
> > considering they have a common parent.
> >
> > For example nested transactions: If you have a transaction context, you
> > may want to propagate the transaction context. But you do do not
> > necessarily want to copy the context instance and treat the two threads
> > equally, because a tx.commit() in a child thread should not have the same
> > effect as the tx.commit() in the parent thread.
> 
> How would this differ with propagation?

If it's nested you may want to add a nesting level when propagating.

> > Whether it needs to be really thread local, or whether it is context that
> > needs to be propagated depends on the library and use case. Futhermore,
> > if you know it needs to be propagated, the "how" might differ as well.
> >
> 
> Do you have a concrete example?

E.g. LongAdder, ConcurrentHashMap. Those use the thread local random's variables, which is
now directly in the thread, but conceptually its a thread local and backports exist using a thread local.
You don't want to be those values all the same....

> > But srsly, mind about the concepts you try to name. How can a local
> > variable reference be asynchronous by itself?
> > Hope you don't mind the hint from someone who is not doing only async with
> > TLs.
> 
> My power screwdriver was sold as a screwdriver but it's pretty good at
> drilling too. :)

My power driller makes awkward holes, when I try to use it for screw driving and
the noise is just terrible. :)

> Absolutely. So by Java 25 we might see broad adoption and all old dependencies 
> will be phased out. Not really tracable :)

Meanwhile I learned to be patient, Rome wasn't build in a day.

And every now and then I find out that something is fixed, where I did use workarounds
for decades.

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From peter.levart at gmail.com  Thu Jun  9 16:35:50 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 9 Jun 2016 22:35:50 +0200
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>
References: <57593111.6030209@azulsystems.com>
 <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>
Message-ID: <b5eb73dd-8701-3707-e7ee-cc7241b48f2d@gmail.com>



On 06/09/2016 11:21 AM, Dawid Weiss wrote:
> Apache Lucene has a similar problem and a workaround here:
> https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java
>
> Dawid

Hi Dawid,

As I understand this is a different problem. Lucene's 
CloseableThreadLocal tries to expedite cleanup of values associated with 
unreachable (and already GCed) ThreadLocal instances while the 
associated Thread(s) are still alive. If a thread exits and it's Thread 
object is GCed, then all thread local values associated with such thread 
can be GCed with the Thread in the same GC cycle. If OTOH a thread is 
still alive and a ThreadLocal object is GCed, then the value associated 
with such ThreadLocal object remains reachable until it is expunged from 
the ThreadLocalMap(s) reachable from live Thread objects, which happens 
only when they are accessed by some other ThreadLocal object. If they 
are not, such values can remain reachable until associated Thread(s) are 
GCed.

Fixing this (the case that lucene is trying to solve with 
CloseableThreadLocal) is not simple as it would require actions of some 
other (say cleanup) thread to do the expunging and that would require 
ThreadLocalMap to allow access from multiple threads which means it 
would require synchronization. Introducing synchronization in 
ThreadLocalMap would penalize normal access to ThreadLocalMap which 
currently does not need any synchronization.

I think the only "real" solution to this problem (and to some other 
problems with imprompt cleanup such as in ClassValue API) is adding 
Ephemeron support to JVM. I even created a prototype for that some time 
ago (with advice from Gil Tene who helped me specify the behavior):

http://cr.openjdk.java.net/~plevart/misc/Ephemeron/webrev.jdk.02/
http://cr.openjdk.java.net/~plevart/misc/Ephemeron/webrev.hotspot.02/

The interest to add such feature to JVM was not very big at that time. 
But that's probably just a result of people not being aware of what such 
feature could be used for. In ThreadLocal API, for example, an Ephemeron 
would be used as an Entry in ThreadLocalMap referenced from the Thread 
object as now. The key of such Ephemeron would be the ThreadLocal object 
and the value of such Ephemeron would be the associated value. If either 
of Thread or ThreadLocal object became unreachable, they would be GCed 
together with the associated value in the same GC cycle.

Regards, Peter

> On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com> wrote:
>> Currently thread local resources can be managed using a ThreadLocal.
>> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
>> I would like to have Thread lifecycle aware thread resources. There is a use
>> case for this already inside the JDK with thread local direct byte buffers.
>> Currently the cleanup of the thread local byte buffers is driven by
>> finalization, this can result in issues in high thread churn applications.
>> With lifecycle awareness the buffers could be cleaned up on thread exit.
>> Here is a suggested enhancement to TL to allow this behavior:
>>    final static ThreadLocal<ByteBuffer> TL_BUFFER = new
>> ThreadLocal<ByteBuffer>() {
>>      @Override
>>      protected ByteBuffer initialValue() {
>>        return ByteBuffer.allocateDirect(4096);
>>      }
>>      @Override
>>      protected void discard(ByteBuffer bb) {
>>        ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>>      }
>>    };
>>
>> The discard method will be called on thread exit.
>> Am I missing an already existing way of achieving the same? Does this
>> functionality somehow not sit well with existing APIs?
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/8f5c9421/attachment.html>

From perm at sics.se  Thu Jun  9 16:56:41 2016
From: perm at sics.se (Per Mildner)
Date: Thu, 9 Jun 2016 22:56:41 +0200
Subject: [concurrency-interest] How to implement task-local variables (like
	thread local variables but for Fork/Join tasks)
Message-ID: <ECB57D43-8964-42C8-99C9-81AA9E45930A@sics.se>

I need something that should behave like a thread-local variable but for a tree of RecursiveAction tasks.

That is, a task inherits the task-local variables from the task that created it. Looking up a task-local variable is done by traversing the chain of parent tasks, looking for special task instances that holds task local values.

I currently implement this by letting all tasks know their parent (the “current task”) which is passed to their constructor. This way I can traverse the chain of parents towards the root of the task tree.

Keeping track of the current task is done by making all tasks set and restore themselves as the “current task” in a thread-local variable in their RecursiveAction.compute() but that seems wasteful since it is done also when the the task is running in the same thread as its parent task.

So, any ideas how to implement this efficiently?

Even better, is there a way to make something like this work with the default pool?

Regards,


Per Mildner
Per.Mildner at sics.se

(I sent this earlier but it never seemed to arrive at the list. Sorry if it gets duplicated.)



From peter.levart at gmail.com  Thu Jun  9 17:16:50 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 9 Jun 2016 23:16:50 +0200
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>
References: <57593111.6030209@azulsystems.com>
 <CANPzfU_Er=QmJpjP=GUOtrUVGHkfLN3-6cLWd5tsoP-Y9=6ENw@mail.gmail.com>
 <00c701d1c232$8bc01630$a3404290$@aapt.net.au>
 <CANPzfU--5YJyia_TzcgkCjKpaAfSMSxCZoLWKqzOZjqhkaXuRA@mail.gmail.com>
Message-ID: <905e5e9c-bd9a-caad-b161-a85029e10d3b@gmail.com>



On 06/09/2016 11:48 AM, Viktor Klang wrote:
>
>
> On Thu, Jun 9, 2016 at 11:37 AM, David Holmes 
> <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     That would totally break encapsulation.
>
>
> You mean like reflection and setAccessible(true)? ;)
>
>     The ThreadLocal variable belongs to the code in which it is
>     declared and used. Some other piece of code should not be able to
>     gather all such variables – just like you couldn’t (and shouldn’t)
>     be able to gather a list of all (for example) static fields that a
>     thread can access.
>
>
> You can already do this (collect which ThreadLocals are used) with 
> instrumentation though.
>
> Right now it's a real pain point with async programs to propagate 
> ThreadLocals (since you don't know which thread locals to propagate).

An API that doesn't break encapsulation could be as follows:

public class Thread {
...
     /**
      * Exchanges thread-local values associated with current thread 
with the
      * values encapsulated in given {@link ThreadLocal.Context}.
      *
      * @param tlCtx the context with which to exchange thread-local values
      *              associated with current thread
      */
     public static void exchangeThreadLocalContext(ThreadLocal.Context 
tlCtx) {
         Objects.requireNonNull(tlCtx);
         Thread cur = Thread.currentThread();
         cur.threadLocals = tlCtx.mapRef.getAndSet(cur.threadLocals);
     }
}

public class ThreadLocal {
...
     /**
      * A snapshot of values that can be
      * {@link Thread#exchangeThreadLocalContext(Context) exchanged}
      * with thread-local values associated with current thread.
      */
     public static final class Context {
         final AtomicReference<ThreadLocalMap> mapRef = new 
AtomicReference<>();

         /**
          * Constructs an empty context with no values.
          */
         public Context() {
         }
     }
}



...but would require special permissions, as any code could otherwise 
"clear" thread-locals of current thread or install thread-locals 
exchanged with some other thread. The API makes sure that thread-local 
values remain accessible by a single thread.

Regards, Peter


>
>     David
>
>     *From:*Concurrency-interest
>     [mailto:concurrency-interest-bounces at cs.oswego.edu
>     <mailto:concurrency-interest-bounces at cs.oswego.edu>] *On Behalf Of
>     *Viktor Klang
>     *Sent:* Thursday, June 9, 2016 7:27 PM
>     *To:* Nitsan Wakart <nitsanw at azulsystems.com
>     <mailto:nitsanw at azulsystems.com>>
>     *Cc:* concurrency-interest <concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>>
>     *Subject:* Re: [concurrency-interest] Thread local resource
>     management feature request/discussion
>
>     Speaking of ThreadLocals, especially for TL propagation it would
>     be splendid if one could get *ALL* TLs currently initialized for a
>     given Thread instance.
>
>     On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart
>     <nitsanw at azulsystems.com <mailto:nitsanw at azulsystems.com>> wrote:
>
>         Currently thread local resources can be managed using a
>         ThreadLocal. ThreadLocals are initialized on 'discovery' and
>         never explicitly discarded.
>         I would like to have Thread lifecycle aware thread resources.
>         There is a use case for this already inside the JDK with
>         thread local direct byte buffers. Currently the cleanup of the
>         thread local byte buffers is driven by finalization, this can
>         result in issues in high thread churn applications. With
>         lifecycle awareness the buffers could be cleaned up on thread
>         exit.
>         Here is a suggested enhancement to TL to allow this behavior:
>           final static ThreadLocal<ByteBuffer> TL_BUFFER = new
>         ThreadLocal<ByteBuffer>() {
>             @Override
>             protected ByteBuffer initialValue() {
>               return ByteBuffer.allocateDirect(4096);
>             }
>             @Override
>             protected void discard(ByteBuffer bb) {
>         ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>             }
>           };
>
>         The discard method will be called on thread exit.
>         Am I missing an already existing way of achieving the same?
>         Does this functionality somehow not sit well with existing APIs?
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>     -- 
>
>     Cheers,
>
>     √
>
>
>
>
> -- 
> Cheers,
> √
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160609/451c78f5/attachment-0001.html>

From dawid.weiss at gmail.com  Fri Jun 10 02:39:23 2016
From: dawid.weiss at gmail.com (Dawid Weiss)
Date: Fri, 10 Jun 2016 08:39:23 +0200
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <b5eb73dd-8701-3707-e7ee-cc7241b48f2d@gmail.com>
References: <57593111.6030209@azulsystems.com>
 <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>
 <b5eb73dd-8701-3707-e7ee-cc7241b48f2d@gmail.com>
Message-ID: <CAM21Rt8eJJmhE_W0fdEcAXbw+ioF3BH7O8rf8wAvgbsSykTvKA@mail.gmail.com>

You are correct, Peter -- apologies for introducing some noise to the
thread. Your explanation and links are very helpful though.

Dawid

On Thu, Jun 9, 2016 at 10:35 PM, Peter Levart <peter.levart at gmail.com> wrote:
>
>
> On 06/09/2016 11:21 AM, Dawid Weiss wrote:
>
> Apache Lucene has a similar problem and a workaround here:
> https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/util/CloseableThreadLocal.java
>
> Dawid
>
>
> Hi Dawid,
>
> As I understand this is a different problem. Lucene's CloseableThreadLocal
> tries to expedite cleanup of values associated with unreachable (and already
> GCed) ThreadLocal instances while the associated Thread(s) are still alive.
> If a thread exits and it's Thread object is GCed, then all thread local
> values associated with such thread can be GCed with the Thread in the same
> GC cycle. If OTOH a thread is still alive and a ThreadLocal object is GCed,
> then the value associated with such ThreadLocal object remains reachable
> until it is expunged from the ThreadLocalMap(s) reachable from live Thread
> objects, which happens only when they are accessed by some other ThreadLocal
> object. If they are not, such values can remain reachable until associated
> Thread(s) are GCed.
>
> Fixing this (the case that lucene is trying to solve with
> CloseableThreadLocal) is not simple as it would require actions of some
> other (say cleanup) thread to do the expunging and that would require
> ThreadLocalMap to allow access from multiple threads which means it would
> require synchronization. Introducing synchronization in ThreadLocalMap would
> penalize normal access to ThreadLocalMap which currently does not need any
> synchronization.
>
> I think the only "real" solution to this problem (and to some other problems
> with imprompt cleanup such as in ClassValue API) is adding Ephemeron support
> to JVM. I even created a prototype for that some time ago (with advice from
> Gil Tene who helped me specify the behavior):
>
>     http://cr.openjdk.java.net/~plevart/misc/Ephemeron/webrev.jdk.02/
>     http://cr.openjdk.java.net/~plevart/misc/Ephemeron/webrev.hotspot.02/
>
> The interest to add such feature to JVM was not very big at that time. But
> that's probably just a result of people not being aware of what such feature
> could be used for. In ThreadLocal API, for example, an Ephemeron would be
> used as an Entry in ThreadLocalMap referenced from the Thread object as now.
> The key of such Ephemeron would be the ThreadLocal object and the value of
> such Ephemeron would be the associated value. If either of Thread or
> ThreadLocal object became unreachable, they would be GCed together with the
> associated value in the same GC cycle.
>
> Regards, Peter
>
>
> On Thu, Jun 9, 2016 at 11:04 AM, Nitsan Wakart <nitsanw at azulsystems.com>
> wrote:
>
> Currently thread local resources can be managed using a ThreadLocal.
> ThreadLocals are initialized on 'discovery' and never explicitly discarded.
> I would like to have Thread lifecycle aware thread resources. There is a use
> case for this already inside the JDK with thread local direct byte buffers.
> Currently the cleanup of the thread local byte buffers is driven by
> finalization, this can result in issues in high thread churn applications.
> With lifecycle awareness the buffers could be cleaned up on thread exit.
> Here is a suggested enhancement to TL to allow this behavior:
>   final static ThreadLocal<ByteBuffer> TL_BUFFER = new
> ThreadLocal<ByteBuffer>() {
>     @Override
>     protected ByteBuffer initialValue() {
>       return ByteBuffer.allocateDirect(4096);
>     }
>     @Override
>     protected void discard(ByteBuffer bb) {
>       ((sun.nio.ch.DirectBuffer)bb).cleaner().clean();
>     }
>   };
>
> The discard method will be called on thread exit.
> Am I missing an already existing way of achieving the same? Does this
> functionality somehow not sit well with existing APIs?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From nitsanw at azulsystems.com  Fri Jun 10 05:41:00 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Fri, 10 Jun 2016 10:41:00 +0100
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <CAM21Rt8eJJmhE_W0fdEcAXbw+ioF3BH7O8rf8wAvgbsSykTvKA@mail.gmail.com>
References: <57593111.6030209@azulsystems.com>
 <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>
 <b5eb73dd-8701-3707-e7ee-cc7241b48f2d@gmail.com>
 <CAM21Rt8eJJmhE_W0fdEcAXbw+ioF3BH7O8rf8wAvgbsSykTvKA@mail.gmail.com>
Message-ID: <575A8B2C.1040801@azulsystems.com>

 > I think the only "real" solution to this problem (and to some other 
problems with imprompt cleanup such as in ClassValue API) is adding 
Ephemeron support to JVM.
I particularly do not want to have this handled on the GC schedule. 
Thread exit is the right time to release a thread resources. A GC may 
take place in the very distant future, but especially for thread 
lifetime sort of resources this will typically mean waiting for the next 
old GC. The resources cleared may not be heap pressure related but 
perhaps lookup or external resource pools which would benefit from a 
prompt release.

From nitsanw at azulsystems.com  Fri Jun 10 05:47:14 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Fri, 10 Jun 2016 10:47:14 +0100
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <5645389.gl4sZXeyCn@tapsy>
References: <57593111.6030209@azulsystems.com> <23270352.7ASVzBT4LD@tapsy>
 <CANPzfU_Uba2hCMTxYbU8Ne2AtPuryOQNnTUmFd1MuZ+gF_typg@mail.gmail.com>
 <5645389.gl4sZXeyCn@tapsy>
Message-ID: <575A8CA2.6000508@azulsystems.com>

I appreciate the interest folks, but you have diverted to a somewhat 
different feature than the one I'm asking for.
The feature I'm after is simply a ThreadLocal exit hook. It is 
notionally similar to the init hook already in place and does not entail 
threads exposing their TL contents to other threads/code.

From nitsanw at azulsystems.com  Fri Jun 10 05:52:24 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Fri, 10 Jun 2016 10:52:24 +0100
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <1465482970379-13551.post@n7.nabble.com>
References: <57593111.6030209@azulsystems.com>
 <1465482970379-13551.post@n7.nabble.com>
Message-ID: <575A8DD8.8010507@azulsystems.com>

 > And how would you propose dealing with the case where the ThreadLocal 
"referent" (ByteBuffer in your case) is strongly reachable outside the 
exiting thread's ThreadLocalMap?

I suggest we don't deal with it. If we limited ourselves to features 
which cannot be abused we wouldn't have a programming language.
The method is optional, and the default implementation will be empty, 
the same way init is.
IMO this is no more risky than leaving it to the user discretion to use 
TL get() and at their discretion check/not check for null.

From peter.levart at gmail.com  Fri Jun 10 06:59:02 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 10 Jun 2016 12:59:02 +0200
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <575A8B2C.1040801@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
 <CAM21Rt_YpUw2KiZSE2446jhh2pJR8imv-2cxfwne6Td2dZshuA@mail.gmail.com>
 <b5eb73dd-8701-3707-e7ee-cc7241b48f2d@gmail.com>
 <CAM21Rt8eJJmhE_W0fdEcAXbw+ioF3BH7O8rf8wAvgbsSykTvKA@mail.gmail.com>
 <575A8B2C.1040801@azulsystems.com>
Message-ID: <306498fe-5864-ec65-c522-a1510ddff7c1@gmail.com>

Hi Nitsan,


On 06/10/2016 11:41 AM, Nitsan Wakart wrote:
> > I think the only "real" solution to this problem (and to some other 
> problems with imprompt cleanup such as in ClassValue API) is adding 
> Ephemeron support to JVM.
> I particularly do not want to have this handled on the GC schedule. 
> Thread exit is the right time to release a thread resources. A GC may 
> take place in the very distant future, but especially for thread 
> lifetime sort of resources this will typically mean waiting for the 
> next old GC. The resources cleared may not be heap pressure related 
> but perhaps lookup or external resource pools which would benefit from 
> a prompt release.

I was talking about the other issue that Lucene is trying to solve with 
CloseableThreadLocal - the case where ThreadLocal instance is GCed, but 
the Thread is still alive and keeps executing code. I agree with your 
point in extending the ThreadLocal API to allow for a discard action. If 
you have control over Thread creation (for example if you can supply 
your own ThreadFactory to some executor) then you can do that on-top of 
existing API. For example:


public abstract class DiscardableThreadLocal<T> extends ThreadLocal<T> {
     private static final ThreadLocal<Map<DiscardableThreadLocal<?>, 
Boolean>> DTLS =
         new ThreadLocal<>() {
             @Override
             protected Map<DiscardableThreadLocal<?>, Boolean> 
initialValue() {
                 return new IdentityHashMap<>();
             }
         };

     @Override
     protected final T initialValue() {
         T value = initialValueImpl();
         // register if not registered yet
         DTLS.get().putIfAbsent(this, Boolean.TRUE);
         return value;
     }

     @Override
     public void set(T value) {
         // register if not registered yet
         DTLS.get().putIfAbsent(this, Boolean.TRUE);
         super.set(value);
     }

     @Override
     public void remove() {
         super.remove();
         // deregister if registered
         DTLS.get().remove(this);
     }

     T getImpl() {
         return super.get();
     }

     // to be implemented by subclasses

     protected abstract T initialValueImpl();

     protected abstract void discardValue(T value);

     // a ThreadFactory that discards DiscardableThreadLocal(s) when 
threads exit
     public static class DiscardingThreadFactory implements ThreadFactory {
         private final ThreadFactory factory;

         public DiscardingThreadFactory(ThreadFactory factory) {
             this.factory = factory;
         }

         @Override
         public Thread newThread(Runnable r) {
             return factory.newThread(() -> {
                 try {
                     r.run();
                 } finally {
                     for (DiscardableThreadLocal<?> dtl : 
DTLS.get().keySet()) {
                         @SuppressWarnings("unchecked")
                         DiscardableThreadLocal<Object> dtlo = 
(DiscardableThreadLocal) dtl;
                         dtlo.discardValue(dtlo.getImpl());
                     }
                     // release from exiting thread
                     DTLS.remove();
                 }
             });
         }
     }

     // example usage
     public static void main(String[] args) throws InterruptedException {
         DiscardableThreadLocal<String> tl = new 
DiscardableThreadLocal<>() {
             @Override
             protected String initialValueImpl() {
                 return "garbage";
             }

             @Override
             protected void discardValue(String value) {
                 System.out.println("discarding: " + value);
             }
         };

         ExecutorService exe = Executors.newCachedThreadPool(
             new DiscardingThreadFactory(Executors.defaultThreadFactory()));

         exe.submit(() -> {
             System.out.println("observing: " + tl.get());
         });

         exe.shutdown();
         exe.awaitTermination(5, TimeUnit.SECONDS);
     }
}


... will print:

observing: garbage
discarding: garbage


Note that in order to guarantee that discardValue() gets called on 
thread-exit for all initialized/set/but-not-removed associated values, 
DiscardableThreadLocal(s) are retained for the lifetime of any thread, 
created by DiscardingThreadFactory that associated a value in 
combination with them. This is typically not a problem unless such 
DiscardableThreadLocal subclasses also retain a ClassLoader you want to 
be GCed before the mentioned threads exit.

This could be "fixed" by using a WeakHashMap instead of IdentityHashMap 
in above code and ensuring that DiscardableThreadLocal subclasses don't 
override equals/hashCode by making them final. This would nullify the 
guarantee that discardValues() is called though, if 
DiscardableThreadLocal instances are GCed before thread(s) exit.

Regards, Peter


From thurston at nomagicsoftware.com  Fri Jun 10 10:43:09 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Fri, 10 Jun 2016 07:43:09 -0700 (MST)
Subject: [concurrency-interest] Thread local resource management feature
	request/discussion
In-Reply-To: <575A8DD8.8010507@azulsystems.com>
References: <57593111.6030209@azulsystems.com>
 <1465482970379-13551.post@n7.nabble.com> <575A8DD8.8010507@azulsystems.com>
Message-ID: <1465569789458-13561.post@n7.nabble.com>

Nitsan Wakart-2 wrote
>> And how would you propose dealing with the case where the ThreadLocal 
> "referent" (ByteBuffer in your case) is strongly reachable outside the 
> exiting thread's ThreadLocalMap?
> 
> I suggest we don't deal with it. If we limited ourselves to features 
> which cannot be abused we wouldn't have a programming language.
> The method is optional, and the default implementation will be empty, 
> the same way init is.
> IMO this is no more risky than leaving it to the user discretion to use 
> TL get() and at their discretion check/not check for null.

I don't necessarily disagree with that, but as this "thread's" digression
(copying context) exposed, it could result in unexpected behavior, that
can't just facilely be ascribed to developer error.

It doesn't seem (at least from your example), that your real problem is with
a lack of a thread-local lifecycle; it's with a lack of *workable Java
finalization*.  Unreachable DirectBytebuffers need to be "cleaned up" before
being claimed, whether they be (small-t) thread local, or ThreadLocals, or
inter-thread.
That's what finalization is putatively for; it's a violation of SOC to add
to ThreadLocals what is a job for DirectByteBuffers, et al.

Questions of design purity aside, implementation has the merit of being very
simple.  There is already a (private) exit() method in j.l.Thread, which is
where all Thread variables are nulled (including #threadLocals)
It would be simple to add a method to ThreadLocalMap (discardAll?) that
would just walk through its entries and invoke discard(T) on each
ThreadLocal, as the entries are just {WeakReference<ThreadLocal&lt;T>>, T) }
tuples.  Then exit() could be amended to include a call to
#threadLocals.discardAll() before nulling. Note: it's unclear which thread
invokes Thread#exit(), I don't see why it wouldn't be the Thread's thread
(it's awful to write sentences like that!)

Now this wouldn't cover the cases where the ThreadLocal instances were
already GC'd (i.e. only weakly-reachable), so those T's wouldn't be
discarded, so a slightly different approach would be needed if you want to
cover those cases as well.

I don't really have a problem with this, I just wonder if it's plugging
thumbs into dikes as opposed to fixing Java finalization, which is the real
issue




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Thread-local-resource-management-feature-request-discussion-tp13540p13561.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From nitsanw at azulsystems.com  Fri Jun 10 14:28:12 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Fri, 10 Jun 2016 19:28:12 +0100
Subject: [concurrency-interest] Thread local resource management feature
 request/discussion
In-Reply-To: <1465569789458-13561.post@n7.nabble.com>
References: <57593111.6030209@azulsystems.com>
 <1465482970379-13551.post@n7.nabble.com> <575A8DD8.8010507@azulsystems.com>
 <1465569789458-13561.post@n7.nabble.com>
Message-ID: <575B06BC.1070501@azulsystems.com>

 > "I don't really have a problem with this, I just wonder if it's 
plugging thumbs into dikes as opposed to fixing Java finalization, which 
is the real issue"
While finalization is *an* issue I don't think it's the issue here. I 
personally think that relying on GC to trigger cleanup logic is not 
appropriate in most cases, but this is perhaps not the place to discuss 
my aversions. ThreadLocals have an obvious relationship to the thread 
lifecycle, that relationship is not well expressed by GC cycles. I would 
like to have that relationship better expressed.
An alternative solution, with broader application perhaps, would be to 
add a thread shutdown hook. This would solve the thread lifecycle 
visibility problem more generally. Using this option to express the 
original example:
   final static ThreadLocal<ByteBuffer> TL_BUFFER = new 
ThreadLocal<ByteBuffer>() {
     @Override
     protected ByteBuffer initialValue() {
       ByteBuffer bb = ByteBuffer.allocateDirect(4096);
       Thread.currentThread().onThreadExit(() -> 
((sun.nio.ch.DirectBuffer)bb).cleaner().clean());
       return bb;
     }
   };
Where onThreadExit (or maybe addThreadExitTask or something) would take 
a Runnable.

From martinrb at google.com  Wed Jun 15 21:53:48 2016
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 15 Jun 2016 18:53:48 -0700
Subject: [concurrency-interest] Does StampedLock need a releaseFence in
	theory?
Message-ID: <CA+kOe0-yucV7zTgb6hWythLg0Qg=L+sLHXDDj2jxvFm=v1gUEQ@mail.gmail.com>

Somehow I ended up re-reading Hans' excellent
Can Seqlocks Get Along With Programming Language Memory Models?
http://www.hpl.hp.com/techreports/2012/HPL-2012-68.pdf
and was wondering about why there wasn't a symmetrical releaseFence to pair
up with the acquireFence.  The proof in section 5 says

"""
The correctness argument is basically as above, but the
happens-before chain becomes:
Initial update of seq by w is sequenced before
Write to datan by w synchronizes with
The acquire fence in r (since preceding operation saw
write, 29.8 p4 in [11]) is sequenced before
final read of seq by r
"""

But if the write to datan is a relaxed write (as would be written by
programmers accustomed to using plain writes in a critical section), then I
don't see the "Write to datan by w synchronizes with ..."  and I wonder
whether for theoretical correctness one needs something like:

--- src/main/java/util/concurrent/locks/StampedLock.java 9 Jun 2016
00:32:02 -0000 1.60
+++ src/main/java/util/concurrent/locks/StampedLock.java 16 Jun 2016
01:30:09 -0000
@@ -349,9 +347,14 @@
     public long tryWriteLock() {
         long s, next;
-        return ((((s = state) & ABITS) == 0L &&
-                 STATE.compareAndSet(this, s, next = s + WBIT)) ?
-                next : 0L);
+        if (((s = state) & ABITS) == 0L &&
+            STATE.compareAndSet(this, s, next = s + WBIT)) {
+            // Necessary in theory, but not in practice?
+            VarHandle.releaseFence();
+            return next;
+        } else {
+            return 0L;
+        }
     }

In practice it's probably not an issue because CASes are implemented using
full fences, and the JIT cannot reorder the dependent write.

BTW, with jdk9 VarHandles seqlocks can be implemented without Unsafe, so a
"new release" of the Seqlocks paper might be useful.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160615/47b9e612/attachment.html>

From coyotesqrl at gmail.com  Sat Jun 18 17:33:35 2016
From: coyotesqrl at gmail.com (R.A. Porter)
Date: Sat, 18 Jun 2016 21:33:35 +0000
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
Message-ID: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>

I'm trying to write a multiplexed input stream where multiple consumers can
read a common IO stream concurrently without making multiple copies of the
stream in memory. My first pass through had used a CyclicBarrier, but my
friend pointed out to me that it wasn't flexible enough to allow one of the
consumers to drop out early by calling close on the stream.

I've updated to use Phaser and, while I have a lot of tests to write and
perform, it appears that almost everything is working as it should be. The
only issue I have is with the use of awaitAdvanceInterruptibly().

I don't want a single bad consumer blocking or otherwise taking too long
consuming the current in-memory buffer to cause all my other consumers to
block on phase advance. I thought I'd be able to manage that with the
await/Advance timeout, but either I've used it wrong or I completely
misunderstand its purpose and function.

Any guidance would be much appreciated. If I can't get this to work with
Phaser, I may need to drop to simpler concurrency controls and manage the
thread interaction with a lot more manual code. That's certainly doable,
but definitely not preferred.

N.B. This is a work in progress; it does not reflect my best, final efforts.

-R.A. Porter

===
/*
 Copyright (C) 2016 R.A. Porter
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Lesser General Public License as
published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Lesser General Public License for more details.

    You should have received a copy of the GNU Lesser General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package com.dreamloom.multiplex;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.UncheckedIOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.Phaser;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.function.Consumer;
import java.util.stream.Collectors;

/**
 * Creates a multiplexed input stream, capable of parallel, concurrent
reads by multiple consumers.
 * <p>
 * Input streams are depleted as they are consumed; this decorator allows
multiple consumers to read
 * from the same stream without making multiple copies and overfilling
memory. Because it operates
 * in parallel, it also provides the performance benefit of concurrent
stream reading without adding
 * complexity to client code.
 * <p>
 * If any of the {@code Consumer} threads throw an exception or take longer
than the configured time
 * to read a chunk of data from memory ({@link #AWAIT_SECONDS} seconds),
the multiplexer will fail
 * all the consumer threads.
 */
public class MultiplexInputStream extends InputStream {
    private static final int DEFAULT_BUFFER_SIZE = 2048;

    private static final int AWAIT_SECONDS = 10;

    private static final ThreadLocal<Integer> INDEX = new
ThreadLocal<Integer>() {
        @Override
        protected Integer initialValue() {
            return 0;
        }
    };

    private static final String MULTIPLEX_THREAD_PREFIX = "Multiplex";

    private final BufferedInputStream bis;

    private final byte[] buffer;

    private Phaser phaser;

    private int currentReadBytes;

    private ExecutorService service;

    private final AtomicInteger factoryIdx = new AtomicInteger(0);

    /**
     * Creates a new {@code MultiplexInputStream} instance with an
in-memory buffer of default
     * size, 2048 bytes.
     *
     * @param source the underlying input stream to decorate and multiplex
     * @throws IOException if there is a problem reading the underlying
stream
     */
    public MultiplexInputStream(InputStream source) throws IOException {
        this(source, DEFAULT_BUFFER_SIZE);
    }

    /**
     * Creates a new {@code MultiplexInputStream} instance with an
in-memory buffer of
     * {@code bufferSize)} bytes.
     *
     * @param source     the underlying input stream to decorate and
multiplex
     * @param bufferSize the size of the in-memory buffer
     * @throws IOException if there is a problem reading the underlying
stream
     */
    public MultiplexInputStream(InputStream source, int bufferSize) throws
IOException {
        buffer = new byte[bufferSize];
        bis = new BufferedInputStream(source);

        currentReadBytes = bis.read(buffer);
        if (currentReadBytes == -1) {
            throw new IllegalStateException(
                    "Error initializing stream; no content found in
source.");
        }
    }

    /**
     * Starts the multiplexer, running the list of provided {@code
Consumer}s to read the stream.
     * <p>
     * It is the responsibility of the caller to ensure that each of the
provided {@code Consumer}s
     * correctly ingests and consumes the {@code InputStream} and does not
block.
     *
     * @param consumers the list of processors reading the {@code
InputStream}
     * @throws ExecutionException
     * @throws InterruptedException
     */
    public void invoke(List<Consumer<InputStream>> consumers)
            throws ExecutionException, InterruptedException {
        phaser = new Phaser(consumers.size()) {
            @Override
            protected boolean onAdvance(int phase, int registeredParties) {
                if (registeredParties == 0) {
                    return true;
                }
                try {
                    currentReadBytes = bis.read(buffer);
                } catch (IOException e) {
                    // Failure to read the underlying stream should result
in a failure of all running
                    // consumer threads.
                    throw new UncheckedIOException(e);
                }

                return false;
            }
        };

        service = Executors.newFixedThreadPool(consumers.size(),
                r -> new Thread(r, getConsumerThreadName()));

        Set<Future> futures = consumers.stream()
                .map(consumer -> service.submit(() ->
consumer.accept(MultiplexInputStream.this)))
                .collect(Collectors.toSet());

        for (Future future : futures) {
            future.get();
        }
    }

    @Override
    public int read() throws IOException {
        Integer index = INDEX.get();
        // Block if this thread has gotten the last byte of the current
buffer
        if (index == currentReadBytes) {
            try {
                phaser.awaitAdvanceInterruptibly(phaser.arrive(),
AWAIT_SECONDS, TimeUnit.SECONDS);

                index = 0;
                INDEX.set(index);
            } catch (InterruptedException | TimeoutException e) {
                throw new IOException("Error processing multiplexed input",
e);
            }
        }

        if (currentReadBytes == -1) {
            return -1;
        }

        byte b = buffer[index++];
        INDEX.set(index);
        return b;
    }

    @Override
    public void close() throws IOException {
        // If caller is one of the consumer threads, arriveAndDeregister on
its behalf;
        // else, close down resources
        synchronized (MULTIPLEX_THREAD_PREFIX) {
            if (Thread.currentThread()
                    .getName()
                    .startsWith(MULTIPLEX_THREAD_PREFIX)) {
                phaser.arriveAndDeregister();
            } else {
                bis.close();
                service.shutdownNow();
            }
        }
    }

    private String getConsumerThreadName() {
        return String.format("%s-%d-%s", MULTIPLEX_THREAD_PREFIX,
factoryIdx.getAndIncrement(),
                UUID.randomUUID()
                        .toString());
    }
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160618/466a4919/attachment.html>

From jw_list at headissue.com  Sun Jun 19 06:30:57 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Sun, 19 Jun 2016 12:30:57 +0200
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
In-Reply-To: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
Message-ID: <6320984.cZEVs8j6Gn@tapsy>

On Saturday 18 June 2016 21:33:35 R.A. Porter wrote:
> I'm trying to write a multiplexed input stream where multiple consumers can read a common IO stream concurrently without making multiple copies of the stream in memory.

Multiplexing means combining multiple streams into one, e.g. by chunking and with each chunk having an identifier of the source stream. Reading a multiplexed stream, would be actually demultiplexing. I think that is not what you try to do here.

You just want to forward the identical inputstream data to multiple consumers, correct?

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From coyotesqrl at gmail.com  Sun Jun 19 08:55:08 2016
From: coyotesqrl at gmail.com (R.A. Porter)
Date: Sun, 19 Jun 2016 12:55:08 +0000
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
In-Reply-To: <6320984.cZEVs8j6Gn@tapsy>
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
 <6320984.cZEVs8j6Gn@tapsy>
Message-ID: <CAFFMuLP+mWWEOaFes3MhzfCusDXTzCodA=cX1zm5UfO3Oz4i6Q@mail.gmail.com>

Right you are, Jens. It indeed should be called something different.

-R.A. Porter

On Sun, Jun 19, 2016, 3:30 AM Jens Wilke <jw_list at headissue.com> wrote:

> On Saturday 18 June 2016 21:33:35 R.A. Porter wrote:
> > I'm trying to write a multiplexed input stream where multiple consumers
> can read a common IO stream concurrently without making multiple copies of
> the stream in memory.
>
> Multiplexing means combining multiple streams into one, e.g. by chunking
> and with each chunk having an identifier of the source stream. Reading a
> multiplexed stream, would be actually demultiplexing. I think that is not
> what you try to do here.
>
> You just want to forward the identical inputstream data to multiple
> consumers, correct?
>
> Cheers,
>
> Jens
>
> --
> "Everything superfluous is wrong!"
>
>    // Jens Wilke - headissue GmbH - Germany
>  \//  https://headissue.com
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160619/34ddf7b1/attachment-0001.html>

From oleksandr.otenko at gmail.com  Sun Jun 19 10:34:00 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sun, 19 Jun 2016 15:34:00 +0100
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
In-Reply-To: <CAFFMuLP+mWWEOaFes3MhzfCusDXTzCodA=cX1zm5UfO3Oz4i6Q@mail.gmail.com>
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
 <6320984.cZEVs8j6Gn@tapsy>
 <CAFFMuLP+mWWEOaFes3MhzfCusDXTzCodA=cX1zm5UfO3Oz4i6Q@mail.gmail.com>
Message-ID: <CANkgWKg0Ncj6tGLo9o_oPnhNzYQCLBfjeCNjRGQ70Buie4a2Qg@mail.gmail.com>

The crucial point here is how dynamic the reader count is. If it is fixed,
then any solution is not lock free, and the slowest reader problem will
manifest itself in some form. If it is not fixed,  then you need to define
the condition when the buffer can be freed.

Alex
On 19 Jun 2016 13:59, "R.A. Porter" <coyotesqrl at gmail.com> wrote:

> Right you are, Jens. It indeed should be called something different.
>
> -R.A. Porter
>
> On Sun, Jun 19, 2016, 3:30 AM Jens Wilke <jw_list at headissue.com> wrote:
>
>> On Saturday 18 June 2016 21:33:35 R.A. Porter wrote:
>> > I'm trying to write a multiplexed input stream where multiple consumers
>> can read a common IO stream concurrently without making multiple copies of
>> the stream in memory.
>>
>> Multiplexing means combining multiple streams into one, e.g. by chunking
>> and with each chunk having an identifier of the source stream. Reading a
>> multiplexed stream, would be actually demultiplexing. I think that is not
>> what you try to do here.
>>
>> You just want to forward the identical inputstream data to multiple
>> consumers, correct?
>>
>> Cheers,
>>
>> Jens
>>
>> --
>> "Everything superfluous is wrong!"
>>
>>    // Jens Wilke - headissue GmbH - Germany
>>  \//  https://headissue.com
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160619/6b031574/attachment.html>

From coyotesqrl at gmail.com  Sun Jun 19 10:51:37 2016
From: coyotesqrl at gmail.com (R.A. Porter)
Date: Sun, 19 Jun 2016 14:51:37 +0000
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
In-Reply-To: <CANkgWKg0Ncj6tGLo9o_oPnhNzYQCLBfjeCNjRGQ70Buie4a2Qg@mail.gmail.com>
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
 <6320984.cZEVs8j6Gn@tapsy>
 <CAFFMuLP+mWWEOaFes3MhzfCusDXTzCodA=cX1zm5UfO3Oz4i6Q@mail.gmail.com>
 <CANkgWKg0Ncj6tGLo9o_oPnhNzYQCLBfjeCNjRGQ70Buie4a2Qg@mail.gmail.com>
Message-ID: <CAFFMuLNgCU=FG_7kikog_6Bp6eFeNDna9nXR5tP3y1c0oPdrSQ@mail.gmail.com>

My preference for resolving the slowest reader problem in this case is to
timeout and fail all the readers. It's not completely fixed, as the
consumers can deregister mid-process; I'm just hoping to determine when
that happens. As I understood awaitAdvanceInterruptibly(), it should have
timed out my other threads and allowed me to terminate the process.

-r

On Sun, Jun 19, 2016, 7:34 AM Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> The crucial point here is how dynamic the reader count is. If it is fixed,
> then any solution is not lock free, and the slowest reader problem will
> manifest itself in some form. If it is not fixed,  then you need to define
> the condition when the buffer can be freed.
>
> Alex
> On 19 Jun 2016 13:59, "R.A. Porter" <coyotesqrl at gmail.com> wrote:
>
>> Right you are, Jens. It indeed should be called something different.
>>
>> -R.A. Porter
>>
>> On Sun, Jun 19, 2016, 3:30 AM Jens Wilke <jw_list at headissue.com> wrote:
>>
>>> On Saturday 18 June 2016 21:33:35 R.A. Porter wrote:
>>> > I'm trying to write a multiplexed input stream where multiple
>>> consumers can read a common IO stream concurrently without making multiple
>>> copies of the stream in memory.
>>>
>>> Multiplexing means combining multiple streams into one, e.g. by chunking
>>> and with each chunk having an identifier of the source stream. Reading a
>>> multiplexed stream, would be actually demultiplexing. I think that is not
>>> what you try to do here.
>>>
>>> You just want to forward the identical inputstream data to multiple
>>> consumers, correct?
>>>
>>> Cheers,
>>>
>>> Jens
>>>
>>> --
>>> "Everything superfluous is wrong!"
>>>
>>>    // Jens Wilke - headissue GmbH - Germany
>>>  \//  https://headissue.com
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160619/a136dec3/attachment.html>

From oleksandr.otenko at gmail.com  Sun Jun 19 13:52:44 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Sun, 19 Jun 2016 18:52:44 +0100
Subject: [concurrency-interest] Problem using
	Phaser.awaitAdvanceInterruptibly()
In-Reply-To: <CAFFMuLNgCU=FG_7kikog_6Bp6eFeNDna9nXR5tP3y1c0oPdrSQ@mail.gmail.com>
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
 <6320984.cZEVs8j6Gn@tapsy>
 <CAFFMuLP+mWWEOaFes3MhzfCusDXTzCodA=cX1zm5UfO3Oz4i6Q@mail.gmail.com>
 <CANkgWKg0Ncj6tGLo9o_oPnhNzYQCLBfjeCNjRGQ70Buie4a2Qg@mail.gmail.com>
 <CAFFMuLNgCU=FG_7kikog_6Bp6eFeNDna9nXR5tP3y1c0oPdrSQ@mail.gmail.com>
Message-ID: <CANkgWKiNp-3PHSOm3LRJHZRaw7hTWBz2MvzZKqcrpun2XHKObA@mail.gmail.com>

You are better off having a sequencer and a single linked list of
non-reusable buffers. Then you don't need to know how many readers are
looking at the stream.

You can reuse the buffers, if the reader can tell it is the last reader.
This is where the reader count is needed.
On 19 Jun 2016 15:51, "R.A. Porter" <coyotesqrl at gmail.com> wrote:

> My preference for resolving the slowest reader problem in this case is to
> timeout and fail all the readers. It's not completely fixed, as the
> consumers can deregister mid-process; I'm just hoping to determine when
> that happens. As I understood awaitAdvanceInterruptibly(), it should have
> timed out my other threads and allowed me to terminate the process.
>
> -r
>
> On Sun, Jun 19, 2016, 7:34 AM Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> The crucial point here is how dynamic the reader count is. If it is
>> fixed, then any solution is not lock free, and the slowest reader problem
>> will manifest itself in some form. If it is not fixed,  then you need to
>> define the condition when the buffer can be freed.
>>
>> Alex
>> On 19 Jun 2016 13:59, "R.A. Porter" <coyotesqrl at gmail.com> wrote:
>>
>>> Right you are, Jens. It indeed should be called something different.
>>>
>>> -R.A. Porter
>>>
>>> On Sun, Jun 19, 2016, 3:30 AM Jens Wilke <jw_list at headissue.com> wrote:
>>>
>>>> On Saturday 18 June 2016 21:33:35 R.A. Porter wrote:
>>>> > I'm trying to write a multiplexed input stream where multiple
>>>> consumers can read a common IO stream concurrently without making multiple
>>>> copies of the stream in memory.
>>>>
>>>> Multiplexing means combining multiple streams into one, e.g. by
>>>> chunking and with each chunk having an identifier of the source stream.
>>>> Reading a multiplexed stream, would be actually demultiplexing. I think
>>>> that is not what you try to do here.
>>>>
>>>> You just want to forward the identical inputstream data to multiple
>>>> consumers, correct?
>>>>
>>>> Cheers,
>>>>
>>>> Jens
>>>>
>>>> --
>>>> "Everything superfluous is wrong!"
>>>>
>>>>    // Jens Wilke - headissue GmbH - Germany
>>>>  \//  https://headissue.com
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160619/47c57a2b/attachment.html>

From alexei.kaigorodov at gmail.com  Sun Jun 19 14:29:47 2016
From: alexei.kaigorodov at gmail.com (Alexei)
Date: Sun, 19 Jun 2016 18:29:47 +0000 (UTC)
Subject: [concurrency-interest]
	=?utf-8?q?Problem_using=09Phaser=2EawaitAd?=
	=?utf-8?q?vanceInterruptibly=28=29?=
References: <CAFFMuLM7ixvSxV6x=8XbU9fCxkTAVvU8EHvGUQ3sU-E_34YCiA@mail.gmail.com>
Message-ID: <loom.20160619T200906-33@post.gmane.org>

R.A. Porter <coyotesqrl <at> gmail.com> writes:

> 
> I'm trying to write a multiplexed input stream where multiple consumers 
can read a common IO stream concurrently without making multiple copies of 
the stream in memory. My first pass through had used a CyclicBarrier, but my 
friend pointed out to me that it wasn't flexible enough to allow one of the 
consumers to drop out early by calling close on the stream.
> I've updated to use Phaser and, while I have a lot of tests to write and 
perform, it appears that almost everything is working as it should be. The 
only issue I have is with the use of awaitAdvanceInterruptibly().
> 
> I don't want a single bad consumer blocking or otherwise taking too long 
consuming the current in-memory buffer to cause all my other consumers to 
block on phase advance. I thought I'd be able to manage that with the 
await/Advance timeout, but either I've used it wrong or I completely 
misunderstand its purpose and function.
> 
> 
> Any guidance would be much appreciated. If I can't get this to work with 
Phaser, I may need to drop to simpler concurrency controls and manage the 
thread interaction with a lot more manual code. That's certainly doable, but 
definitely not preferred.
> 
> N.B. This is a work in progress; it does not reflect my best, final 
efforts.
> 
> -R.A. Porter
> 
> ===
> 
> /*
>  Copyright (C) 2016 R.A. Porter
>     This program is free software: you can redistribute it and/or modify
>     it under the terms of the GNU Lesser General Public License as 
published by
>     the Free Software Foundation, either version 3 of the License, or
>     (at your option) any later version.
> 
>     This program is distributed in the hope that it will be useful,
>     but WITHOUT ANY WARRANTY; without even the implied warranty of
>     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
>     GNU Lesser General Public License for more details.
> 
>     You should have received a copy of the GNU Lesser General Public 
License
>     along with this program.  If not, see <http://www.gnu.org/licenses/>.
>  */
> package com.dreamloom.multiplex;
> 
> import java.io.BufferedInputStream;
> import java.io.IOException;
> import java.io.InputStream;
> import java.io.UncheckedIOException;
> import java.util.List;
> import java.util.Set;
> import java.util.UUID;
> import java.util.concurrent.ExecutionException;
> import java.util.concurrent.ExecutorService;
> import java.util.concurrent.Executors;
> import java.util.concurrent.Future;
> import java.util.concurrent.Phaser;
> import java.util.concurrent.TimeUnit;
> import java.util.concurrent.TimeoutException;
> import java.util.concurrent.atomic.AtomicInteger;
> import java.util.function.Consumer;
> import java.util.stream.Collectors;
> 
> /**
>  * Creates a multiplexed input stream, capable of parallel, concurrent 
reads by multiple consumers.
>  * <p>
>  * Input streams are depleted as they are consumed; this decorator allows 
multiple consumers to read
>  * from the same stream without making multiple copies and overfilling 
memory. Because it operates
>  * in parallel, it also provides the performance benefit of concurrent 
stream reading without adding
>  * complexity to client code.
>  * <p>
>  * If any of the { <at> code Consumer} threads throw an exception or take 
longer than the configured time
>  * to read a chunk of data from memory ({ <at> link #AWAIT_SECONDS} 
seconds), the multiplexer will fail
>  * all the consumer threads.
>  */
> public class MultiplexInputStream extends InputStream {
>     private static final int DEFAULT_BUFFER_SIZE = 2048;
> 
>     private static final int AWAIT_SECONDS = 10;
> 
>     private static final ThreadLocal<Integer> INDEX = new 
ThreadLocal<Integer>() {
>          <at> Override
>         protected Integer initialValue() {
>             return 0;
>         }
>     };
> 
>     private static final String MULTIPLEX_THREAD_PREFIX = "Multiplex";
> 
>     private final BufferedInputStream bis;
> 
>     private final byte[] buffer;
> 
>     private Phaser phaser;
> 
>     private int currentReadBytes;
> 
>     private ExecutorService service;
> 
>     private final AtomicInteger factoryIdx = new AtomicInteger(0);
> 
>     /**
>      * Creates a new { <at> code MultiplexInputStream} instance with an 
in-memory buffer of default
>      * size, 2048 bytes.
>      *
>      *  <at> param source the underlying input stream to decorate and 
multiplex
>      *  <at> throws IOException if there is a problem reading the 
underlying stream
>      */
>     public MultiplexInputStream(InputStream source) throws IOException {
>         this(source, DEFAULT_BUFFER_SIZE);
>     }
> 
>     /**
>      * Creates a new { <at> code MultiplexInputStream} instance with an 
in-memory buffer of
>      * { <at> code bufferSize)} bytes.
>      *
>      *  <at> param source     the underlying input stream to decorate and 
multiplex
>      *  <at> param bufferSize the size of the in-memory buffer
>      *  <at> throws IOException if there is a problem reading the 
underlying stream
>      */
>     public MultiplexInputStream(InputStream source, int bufferSize) throws 
IOException {
>         buffer = new byte[bufferSize];
>         bis = new BufferedInputStream(source);
> 
>         currentReadBytes = bis.read(buffer);
>         if (currentReadBytes == -1) {
>             throw new IllegalStateException(
>                     "Error initializing stream; no content found in 
source.");
>         }
>     }
> 
>     /**
>      * Starts the multiplexer, running the list of provided { <at> code 
Consumer}s to read the stream.
>      * <p>
>      * It is the responsibility of the caller to ensure that each of the 
provided { <at> code Consumer}s
>      * correctly ingests and consumes the { <at> code InputStream} and 
does not block.
>      *
>      *  <at> param consumers the list of processors reading the { <at> 
code InputStream}
>      *  <at> throws ExecutionException
>      *  <at> throws InterruptedException
>      */
>     public void invoke(List<Consumer<InputStream>> consumers)
>             throws ExecutionException, InterruptedException {
>         phaser = new Phaser(consumers.size()) {
>              <at> Override
>             protected boolean onAdvance(int phase, int registeredParties) 
{
>                 if (registeredParties == 0) {
>                     return true;
>                 }
>                 try {
>                     currentReadBytes = bis.read(buffer);
>                 } catch (IOException e) {
>                     // Failure to read the underlying stream should result 
in a failure of all running
>                     // consumer threads.
>                     throw new UncheckedIOException(e);
>                 }
> 
>                 return false;
>             }
>         };
> 
>         service = Executors.newFixedThreadPool(consumers.size(),
>                 r -> new Thread(r, getConsumerThreadName()));
> 
>         Set<Future> futures = consumers.stream()
>                 .map(consumer -> service.submit(() -> 
consumer.accept(MultiplexInputStream.this)))
>                 .collect(Collectors.toSet());
> 
>         for (Future future : futures) {
>             future.get();
>         }
>     }
> 
>      <at> Override
>     public int read() throws IOException {
>         Integer index = INDEX.get();
>         // Block if this thread has gotten the last byte of the current 
buffer
>         if (index == currentReadBytes) {
>             try {
>                 phaser.awaitAdvanceInterruptibly(phaser.arrive(), 
AWAIT_SECONDS, TimeUnit.SECONDS);
> 
>                 index = 0;
>                 INDEX.set(index);
>             } catch (InterruptedException | TimeoutException e) {
>                 throw new IOException("Error processing multiplexed 
input", e);
>             }
>         }
> 
>         if (currentReadBytes == -1) {
>             return -1;
>         }
> 
>         byte b = buffer[index++];
>         INDEX.set(index);
>         return b;
>     }
> 
>      <at> Override
>     public void close() throws IOException {
>         // If caller is one of the consumer threads, arriveAndDeregister 
on its behalf;
>         // else, close down resources
>         synchronized (MULTIPLEX_THREAD_PREFIX) {
>             if (Thread.currentThread()
>                     .getName()
>                     .startsWith(MULTIPLEX_THREAD_PREFIX)) {
>                 phaser.arriveAndDeregister();
>             } else {
>                 bis.close();
>                 service.shutdownNow();
>             }
>         }
>     }
> 
>     private String getConsumerThreadName() {
>         return String.format("%s-%d-%s", MULTIPLEX_THREAD_PREFIX, 
factoryIdx.getAndIncrement(),
>                 UUID.randomUUID()
>                         .toString());
>     }
> }
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest <at> cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

You are trying to use the same instance of MultiplexInputStream for each 
consumer, while each consumer has its own reading pointer. You save that 
pointer in a thread local variable. As a result, that pointer is available 
for the MultiplexInputStream only when consumer calls read() method. 
Pointers of other consumers are not available. If they were available, 
MultiplexInputStream could know if some part of the buffer has been read by 
all consumers, fill it from the source, and allow fast consumers read more 
bytes, thus increasing the level of parallelism.

That is, my advice is a) split MultiplexInputStream in 2 classes, one 
representing the source and the buffer, and the second representing the 
state of consumer, b) use the buffer as a ring buffer c) do not use thread 
local. As a result, you'll need not to use Phaser at all - 
synchronized/wait/notify would be enough.




From Sebastian.Millies at softwareag.com  Mon Jun 20 10:46:00 2016
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Mon, 20 Jun 2016 14:46:00 +0000
Subject: [concurrency-interest] Session-based locking?
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>

Hello there,

is there anything in java.util.concurrent that supports locking based on sessions instead of threads?

Here’s what I have in mind: A client-server architecture in which the client initiates a logical unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. The client then proceeds to make any number of server requests, which will get serviced by different threads on the server. Which thread services which request basically being determined by chance. Finally, the client hits “save”, leaves edit mode and is finished.

The server requests must be processed in a cooperative way: Some require exclusive locks on specific resources, some require only read locks, some require both. I cannot use ReentrantReadWriteLock, because a) that does not permit me to unlock from a different thread, and b) the re-entrancy itself should also rather be based on the session id (a String) than on the thread.

In my case, the problem is simplified by the fact that upgrading a lock from read to write is never needed. Also, the server immediately knows for each request what locks to acquire, and if it can’t get them, there is no point in letting the client wait, the server can fail immediately, rollback any intermediate changes, and tell the client. (Something like try-lock with a minimal timeout would do the trick.) It follows that there is never any problem of finding and interrupting waiting any threads waiting on a lock.

I have googled a bit. The most common recommendation seems to be to roll my own Lock implementation internally using Semaphore to implement a “counting lock”. Or build directly on LockSupport. Both sound daunting. Does anyone perhaps know of a library that would offer support on a somewhat higher level ? Perhaps something that could be taken out of a web application framework? I imagine the problem must be common.

Best Regards,
Sebastian

Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160620/d042273c/attachment-0001.html>

From peter.levart at gmail.com  Mon Jun 20 11:23:36 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 20 Jun 2016 17:23:36 +0200
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
Message-ID: <95e8974b-458c-96de-a163-736036a6f6a4@gmail.com>

Hi Millies,


On 06/20/2016 04:46 PM, Millies, Sebastian wrote:
>
> Hello there,
>
> is there anything in java.util.concurrent that supports locking based 
> on sessions instead of threads?
>
> Here’s what I have in mind: A client-server architecture in which the 
> client initiates a logical unit of work (“LUW”, an SAP term), i. e. 
> basically enters edit mode. The client then proceeds to make any 
> number of server requests, which will get serviced by different 
> threads on the server. Which thread services which request basically 
> being determined by chance. Finally, the client hits “save”, leaves 
> edit mode and is finished.
>

...can client initiate concurrent requests in the same "session" ? A 
browser client can (for example when loading images in the html page) or 
when hitting reload button while server is still serving previous 
request. How would you want to treat such concurrent requests initiated 
from the same client "session" ?

Regards, Peter

> The server requests must be processed in a cooperative way: Some 
> require exclusive locks on specific resources, some require only read 
> locks, some require both. I cannot use ReentrantReadWriteLock, because 
> a) that does not permit me to unlock from a different thread, and b) 
> the re-entrancy itself should also rather be based on the session id 
> (a String) than on the thread.
>
> In my case, the problem is simplified by the fact that upgrading a 
> lock from read to write is never needed. Also, the server immediately 
> knows for each request what locks to acquire, and if it can’t get 
> them, there is no point in letting the client wait, the server can 
> fail immediately, rollback any intermediate changes, and tell the 
> client. (Something like try-lock with a minimal timeout would do the 
> trick.) It follows that there is never any problem of finding and 
> interrupting waiting any threads waiting on a lock.
>
> I have googled a bit. The most common recommendation seems to be to 
> roll my own Lock implementation internally using Semaphore to 
> implement a “counting lock”. Or build directly on LockSupport. Both 
> sound daunting. Does anyone perhaps know of a library that would offer 
> support on a somewhat higher level ? Perhaps something that could be 
> taken out of a web application framework? I imagine the problem must 
> be common.
>
> Best Regards,
>
> Sebastian
>
>
> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 
> Darmstadt, Germany – Registergericht/Commercial register: Darmstadt 
> HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich 
> (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd 
> Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory 
> Board: Dr. Andreas Bereczky - *http://www.softwareag.com*
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160620/c81db144/attachment.html>

From jw_list at headissue.com  Mon Jun 20 11:31:25 2016
From: jw_list at headissue.com (Jens Wilke)
Date: Mon, 20 Jun 2016 17:31:25 +0200
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
Message-ID: <14664953.G9tj5ehQzs@tapsy>

Millies,

On Monday 20 June 2016 14:46:00 Millies, Sebastian wrote:
> Does anyone perhaps know of a library that would offer support on a somewhat higher level ?

>From what you describe it seems to be not a "session" but a "transaction":

> A client-server architecture in which the client initiates a logical unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. The client then proceeds to make any number of server requests, which will get serviced by different threads on the server. 

At the higher level, there are (distributed) transaction managers.

> The most common recommendation seems to be to roll my own Lock implementation internally using Semaphore to implement a “counting lock”.

What resources you need to protect with a lock? You need a lock per resource, not per client session / transaction.

Why you need counting, when actually the client determines the start and end?

Cheers,

Jens

-- 
"Everything superfluous is wrong!"

   // Jens Wilke - headissue GmbH - Germany
 \//  https://headissue.com

From nathanila at gmail.com  Mon Jun 20 11:31:38 2016
From: nathanila at gmail.com (Nathan & Ila Reynolds)
Date: Mon, 20 Jun 2016 08:31:38 -0700
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
Message-ID: <052201d1cb08$d93bd910$8bb38b30$@gmail.com>

I don’t know how well this works.  I didn’t test it.  At least it compiles.  I release it to the public domain.  Use it at your own risk.

 

This code will ensure that the resource it protects is only used by a single session.  It does not ensure that multiple thread for the same session don’t access the resource concurrently.  For that, a simple synchronized block may suffice.

 

This code only handles a single writer case.  It doesn’t handle multiple readers.

 

import java.time.Duration;

import java.time.Instant;

 

public class SessionLock

{

   private String m_session;   // The session which owns the lock

   private int    m_count;

 

   public boolean tryLock(String session, Duration time)

   {

      Instant deadline;

      long sleep;

 

      if (session == null)

         throw new NullPointerException("session is null");

 

      deadline = Instant.

         now().

         plus(time);

 

      synchronized (this)

      {

         while (true)

         {

            if (m_session == null)

            {

               m_session = session;

               m_count   = 1;

 

               return(true);

            }

 

            if (m_session.equals(session))

            {

               m_count++;

               return(true);

            }

 

            sleep = Duration.

               between(Instant.now(), deadline).

               toMillis();

 

            if (sleep <= 0)

               return(false);

 

            try

            {

               wait(sleep);

            }

            catch (InterruptedException e)

            {

               deadline = Instant.EPOCH;    // Wake up and try to acquire the lock.  If fails, then exit immediately.

            }

         }

      }

   }

 

   public void unlock(String session)

   {

      synchronized (this)

      {

         if (m_session == null)

            throw new IllegalStateException("The lock is not acquired");

 

         if (!m_session.equals(session))

            throw new IllegalStateException("The lock is not owned by " + session + ".  It is owned by " + m_session);

 

         if (m_count <= 0)

            throw new IllegalStateException("The lock count is unexpected " + m_count);

 

         if (--m_count > 0)

            return;

 

         m_session = null;

 

         notify();

      }

   }

}

 

-Nathan

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Millies, Sebastian
Sent: Monday, June 20, 2016 7:46 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Session-based locking?

 

Hello there,

 

is there anything in java.util.concurrent that supports locking based on sessions instead of threads? 

 

Here’s what I have in mind: A client-server architecture in which the client initiates a logical unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. The client then proceeds to make any number of server requests, which will get serviced by different threads on the server. Which thread services which request basically being determined by chance. Finally, the client hits “save”, leaves edit mode and is finished.

 

The server requests must be processed in a cooperative way: Some require exclusive locks on specific resources, some require only read locks, some require both. I cannot use ReentrantReadWriteLock, because a) that does not permit me to unlock from a different thread, and b) the re-entrancy itself should also rather be based on the session id (a String) than on the thread.

 

In my case, the problem is simplified by the fact that upgrading a lock from read to write is never needed. Also, the server immediately knows for each request what locks to acquire, and if it can’t get them, there is no point in letting the client wait, the server can fail immediately, rollback any intermediate changes, and tell the client. (Something like try-lock with a minimal timeout would do the trick.) It follows that there is never any problem of finding and interrupting waiting any threads waiting on a lock.

 

I have googled a bit. The most common recommendation seems to be to roll my own Lock implementation internally using Semaphore to implement a “counting lock”. Or build directly on LockSupport. Both sound daunting. Does anyone perhaps know of a library that would offer support on a somewhat higher level ? Perhaps something that could be taken out of a web application framework? I imagine the problem must be common.

 

Best Regards,

Sebastian

 


Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky -  <http://www.softwareag.com> http://www.softwareag.com 

 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160620/24066986/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Tue Jun 21 03:59:47 2016
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Tue, 21 Jun 2016 07:59:47 +0000
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <14664953.G9tj5ehQzs@tapsy>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
 <14664953.G9tj5ehQzs@tapsy>
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E4900102E6844B@HQMBX5.eur.ad.sag>

thanks for the feed-back (and the code). I'll try to answer your questions all in one post.

@Jens: yes, it's similar to a transaction, except the resources I have to protect are not transactional: some are singleton objects in memory, some are files, one is actually a database. They don't support XA, of course, and introducing a full-fledged tx manager seems overkill. In fact, there is already a notion of exclusive locking in place, which needs to be relaxed by introducing read-write-locking. (@Reynoldses: in fact a key requirement).

 I thought of counting, because any number of sessions can acquire a read lock to the same resource, so perhaps having one AtomicInteger per resource with values -1 (write-locked), 0 (free) , or x > 0 (read-locked by x sessions) would already do the job. I wouldn't even need a notion of ownership if I don't offer upgrade, re-entrancy would be automatic, although there would be no lock-upgrade/downgrade functionality.

@Peter: It's not a browser client, I believe it's a swing client that communicates with the backend over a combination of SOAP and protobuf technology. Don't ask me more details, I don't know them. But they don't do concurrent requests within the same session.

-- Sebastian

-----Original Message-----
From: Jens Wilke [mailto:jw_list at headissue.com]
Sent: Monday, June 20, 2016 5:31 PM
To: concurrency-interest at cs.oswego.edu
Cc: Millies, Sebastian
Subject: Re: [concurrency-interest] Session-based locking?

Millies,

On Monday 20 June 2016 14:46:00 Millies, Sebastian wrote:
> Does anyone perhaps know of a library that would offer support on a somewhat higher level ?

From what you describe it seems to be not a "session" but a "transaction":

> A client-server architecture in which the client initiates a logical unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. The client then proceeds to make any number of server requests, which will get serviced by different threads on the server.

At the higher level, there are (distributed) transaction managers.

> The most common recommendation seems to be to roll my own Lock implementation internally using Semaphore to implement a “counting lock”.

What resources you need to protect with a lock? You need a lock per resource, not per client session / transaction.

Why you need counting, when actually the client determines the start and end?

Cheers,

Jens

-----Original Message----

From: Peter Levart [mailto:peter.levart at gmail.com]
Sent: Monday, June 20, 2016 5:24 PM
To: Millies, Sebastian; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Session-based locking?

Hi Millies,

...can client initiate concurrent requests in the same "session" ? A browser client can (for example when loading images in the html page) or when hitting reload button while server is still serving previous request. How would you want to treat such concurrent requests initiated from the same client "session" ?

Regards, Peter



Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com


From oleksandr.otenko at gmail.com  Wed Jun 22 02:53:08 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 22 Jun 2016 07:53:08 +0100
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E4900102E6844B@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
 <14664953.G9tj5ehQzs@tapsy>
 <32F15738E8E5524DA4F01A0FA4A8E4900102E6844B@HQMBX5.eur.ad.sag>
Message-ID: <9DB4CCDB-DBB3-45C5-994B-2E14134F9F0B@gmail.com>


> On 21 Jun 2016, at 08:59, Millies, Sebastian <Sebastian.Millies at softwareag.com> wrote:
> 
> thanks for the feed-back (and the code). I'll try to answer your questions all in one post.
> 
> @Jens: yes, it's similar to a transaction, except the resources I have to protect are not transactional: some are singleton objects in memory, some are files, one is actually a database. They don't support XA, of course, and introducing a full-fledged tx manager seems overkill. In fact, there is already a notion of exclusive locking in place, which needs to be relaxed by introducing read-write-locking. (@Reynoldses: in fact a key requirement).
> 
> I thought of counting, because any number of sessions can acquire a read lock to the same resource, so perhaps having one AtomicInteger per resource with values -1 (write-locked), 0 (free) , or x > 0 (read-locked by x sessions) would already do the job. I wouldn't even need a notion of ownership if I don't offer upgrade, re-entrancy would be automatic, although there would be no lock-upgrade/downgrade functionality.
> 
> @Peter: It's not a browser client, I believe it's a swing client that communicates with the backend over a combination of SOAP and protobuf technology. Don't ask me more details, I don't know them. But they don't do concurrent requests within the same session.

But are they known to terminate?

Eg something not releasing a lock would become a big deal. This is less of an issue in a local lock, as the communications are far more reliable.


Alex


> 
> -- Sebastian
> 
> -----Original Message-----
> From: Jens Wilke [mailto:jw_list at headissue.com <mailto:jw_list at headissue.com>]
> Sent: Monday, June 20, 2016 5:31 PM
> To: concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>
> Cc: Millies, Sebastian
> Subject: Re: [concurrency-interest] Session-based locking?
> 
> Millies,
> 
> On Monday 20 June 2016 14:46:00 Millies, Sebastian wrote:
>> Does anyone perhaps know of a library that would offer support on a somewhat higher level ?
> 
> From what you describe it seems to be not a "session" but a "transaction":
> 
>> A client-server architecture in which the client initiates a logical unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. The client then proceeds to make any number of server requests, which will get serviced by different threads on the server.
> 
> At the higher level, there are (distributed) transaction managers.
> 
>> The most common recommendation seems to be to roll my own Lock implementation internally using Semaphore to implement a “counting lock”.
> 
> What resources you need to protect with a lock? You need a lock per resource, not per client session / transaction.
> 
> Why you need counting, when actually the client determines the start and end?
> 
> Cheers,
> 
> Jens
> 
> -----Original Message----
> 
> From: Peter Levart [mailto:peter.levart at gmail.com <mailto:peter.levart at gmail.com>]
> Sent: Monday, June 20, 2016 5:24 PM
> To: Millies, Sebastian; concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Session-based locking?
> 
> Hi Millies,
> 
> ...can client initiate concurrent requests in the same "session" ? A browser client can (for example when loading images in the html page) or when hitting reload button while server is still serving previous request. How would you want to treat such concurrent requests initiated from the same client "session" ?
> 
> Regards, Peter
> 
> 
> 
> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 Darmstadt, Germany – Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com <http://www.softwareag.com/>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160622/b2c137d8/attachment-0001.html>

From peter.levart at gmail.com  Thu Jun 23 05:17:02 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 23 Jun 2016 11:17:02 +0200
Subject: [concurrency-interest] Session-based locking?
In-Reply-To: <9DB4CCDB-DBB3-45C5-994B-2E14134F9F0B@gmail.com>
References: <32F15738E8E5524DA4F01A0FA4A8E4900102E68208@HQMBX5.eur.ad.sag>
 <14664953.G9tj5ehQzs@tapsy>
 <32F15738E8E5524DA4F01A0FA4A8E4900102E6844B@HQMBX5.eur.ad.sag>
 <9DB4CCDB-DBB3-45C5-994B-2E14134F9F0B@gmail.com>
Message-ID: <e4b2f44d-2046-070a-b0e0-704c4424f80c@gmail.com>



On 06/22/2016 08:53 AM, Alex Otenko wrote:
>
>> On 21 Jun 2016, at 08:59, Millies, Sebastian 
>> <Sebastian.Millies at softwareag.com 
>> <mailto:Sebastian.Millies at softwareag.com>> wrote:
>>
>> thanks for the feed-back (and the code). I'll try to answer your 
>> questions all in one post.
>>
>> @Jens: yes, it's similar to a transaction, except the resources I 
>> have to protect are not transactional: some are singleton objects in 
>> memory, some are files, one is actually a database. They don't 
>> support XA, of course, and introducing a full-fledged tx manager 
>> seems overkill. In fact, there is already a notion of exclusive 
>> locking in place, which needs to be relaxed by introducing 
>> read-write-locking. (@Reynoldses: in fact a key requirement).
>>
>> I thought of counting, because any number of sessions can acquire a 
>> read lock to the same resource, so perhaps having one AtomicInteger 
>> per resource with values -1 (write-locked), 0 (free) , or x > 0 
>> (read-locked by x sessions) would already do the job. I wouldn't even 
>> need a notion of ownership if I don't offer upgrade, re-entrancy 
>> would be automatic, although there would be no lock-upgrade/downgrade 
>> functionality.
>>
>> @Peter: It's not a browser client, I believe it's a swing client that 
>> communicates with the backend over a combination of SOAP and protobuf 
>> technology. Don't ask me more details, I don't know them. But they 
>> don't do concurrent requests within the same session.
>
> But are they known to terminate?
>
> Eg something not releasing a lock would become a big deal. This is 
> less of an issue in a local lock, as the communications are far more 
> reliable.
>
>
> Alex

Then some kind of session-expiry server-side mechanism would have to be 
used to release the locks held by expired session(s). But those would be 
just locks - what about state that was changed as part of holding those 
locks which may be left inconsistent?

I played with the locking aspect only. Here's a simple 
ReentrantSessionRWLock with API similar to ReentrantReadWriteLock, 
supporting session expiry:

     http://cr.openjdk.java.net/~plevart/misc/ReentrantSessionRWLock/

Regards, Peter

>
>
>>
>> -- Sebastian
>>
>> -----Original Message-----
>> From: Jens Wilke [mailto:jw_list at headissue.com]
>> Sent: Monday, June 20, 2016 5:31 PM
>> To:concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>
>> Cc: Millies, Sebastian
>> Subject: Re: [concurrency-interest] Session-based locking?
>>
>> Millies,
>>
>> On Monday 20 June 2016 14:46:00 Millies, Sebastian wrote:
>>> Does anyone perhaps know of a library that would offer support on a 
>>> somewhat higher level ?
>>
>> From what you describe it seems to be not a "session" but a 
>> "transaction":
>>
>>> A client-server architecture in which the client initiates a logical 
>>> unit of work (“LUW”, an SAP term), i. e. basically enters edit mode. 
>>> The client then proceeds to make any number of server requests, 
>>> which will get serviced by different threads on the server.
>>
>> At the higher level, there are (distributed) transaction managers.
>>
>>> The most common recommendation seems to be to roll my own Lock 
>>> implementation internally using Semaphore to implement a “counting 
>>> lock”.
>>
>> What resources you need to protect with a lock? You need a lock per 
>> resource, not per client session / transaction.
>>
>> Why you need counting, when actually the client determines the start 
>> and end?
>>
>> Cheers,
>>
>> Jens
>>
>> -----Original Message----
>>
>> From: Peter Levart [mailto:peter.levart at gmail.com]
>> Sent: Monday, June 20, 2016 5:24 PM
>> To: Millies, Sebastian;concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] Session-based locking?
>>
>> Hi Millies,
>>
>> ...can client initiate concurrent requests in the same "session" ? A 
>> browser client can (for example when loading images in the html page) 
>> or when hitting reload button while server is still serving previous 
>> request. How would you want to treat such concurrent requests 
>> initiated from the same client "session" ?
>>
>> Regards, Peter
>>
>>
>>
>> Software AG – Sitz/Registered office: Uhlandstraße 12, 64297 
>> Darmstadt, Germany – Registergericht/Commercial register: Darmstadt 
>> HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich 
>> (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd 
>> Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory 
>> Board: Dr. Andreas Bereczky -http://www.softwareag.com 
>> <http://www.softwareag.com/>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160623/57effcd3/attachment-0001.html>

From thurston at nomagicsoftware.com  Thu Jun 23 08:10:08 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 23 Jun 2016 05:10:08 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1464619256580-13457.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
Message-ID: <1466683808391-13578.post@n7.nabble.com>

So here's a sketch of a proof that sequential consistency (SC) is an improper
correctness property for volatile memory.


volatile x

Thread I               |                Thread 2

a.    x = 3             |               
                           
                           |            c.    x = 1

b.  r1 = x // -> 3

Such an execution is SC, if counter-intuitive.

However, consider the execution below:


volatile x, y

Thread I                |                Thread 2

a.    x = 3              |               
                           
                           |            d.    y = 1
                           |            e.    x = 1

b.     y = 3             |

c.  r1 = x // -> 3     |

                            |           f.  r2 = y // -> 1

x & y are SC (their actions are just mirrors of the first, SC execution);
but it is trivial to see that the above execution is *not* SC (and therefore
proscribed by the JMM).

The problem is that SC is not compositional; 
A stronger correctness property for SAs is necessary, one that is
compositional (e.g. linearizability).
Linearizable volatile memory would not allow either the first or second
executions; and all linearizable executions are necessarily SC ones ;
thereby complying with the SC-DRF guarantee
        



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13578.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at gmail.com  Thu Jun 23 09:53:53 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 23 Jun 2016 14:53:53 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1466683808391-13578.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
Message-ID: <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>

When you use word ‘proof’, you need to expose axioms you base it on.

For example, you use “correctness property” and “volatile memory”. It is based on some assumptions that you did not disclose. From where I sit, “volatile memory” does not have “correctness properties” at all.

JMM does not guarantee sequential consistency of all conforming algorithms. JMM does not guarantee absence of races; in fact, it allows them. But one can design sequentially consistent algorithms based on JMM. And one can design linearizable algorithms just as well.


Then you make an assumption that the outcome is “not” SC - although not clear on what grounds you conclude that. The second outcome is certainly allowed by JMM - there is a total order of synchronization actions that will yield that outcome.

Alex

> On 23 Jun 2016, at 13:10, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> So here's a sketch of a proof that sequential consistency (SC) is an improper
> correctness property for volatile memory.
> 
> 
> volatile x
> 
> Thread I               |                Thread 2
> 
> a.    x = 3             |               
> 
>                           |            c.    x = 1
> 
> b.  r1 = x // -> 3
> 
> Such an execution is SC, if counter-intuitive.
> 
> However, consider the execution below:
> 
> 
> volatile x, y
> 
> Thread I                |                Thread 2
> 
> a.    x = 3              |               
> 
>                           |            d.    y = 1
>                           |            e.    x = 1
> 
> b.     y = 3             |
> 
> c.  r1 = x // -> 3     |
> 
>                            |           f.  r2 = y // -> 1
> 
> x & y are SC (their actions are just mirrors of the first, SC execution);
> but it is trivial to see that the above execution is *not* SC (and therefore
> proscribed by the JMM).
> 
> The problem is that SC is not compositional; 
> A stronger correctness property for SAs is necessary, one that is
> compositional (e.g. linearizability).
> Linearizable volatile memory would not allow either the first or second
> executions; and all linearizable executions are necessarily SC ones ;
> thereby complying with the SC-DRF guarantee
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13578.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aleksey.shipilev at oracle.com  Thu Jun 23 10:24:57 2016
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 23 Jun 2016 17:24:57 +0300
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
 Total Order?
In-Reply-To: <1466683808391-13578.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
Message-ID: <576BF139.8050101@oracle.com>

On 06/23/2016 03:10 PM, thurstonn wrote:
> So here's a sketch of a proof that sequential consistency (SC) is an improper
> correctness property for volatile memory.

Everything else about this "proof" is said by Alex in other thread.
This is my take on your examples:

> volatile x;
> 
> Thread I                |                Thread 2
> a.    x = 3             |               
>                         |            c.    x = 1
> b.  r1 = x // -> 3
> 
> Such an execution is SC, if counter-intuitive.

This notation is confusing: lower-case letters and line position must
mean something, but it is not obvious what. So, if we take a program:

   x = 3;     |     x = 1;
   r1 = x;    |

Then both (1) and (3) are SC results.


> However, consider the execution below:
> 
> volatile x, y
> 
> Thread I                |                Thread 2
> a.    x = 3             |               
>                         |            d.    y = 1
>                         |            e.    x = 1
> b.     y = 3            |
> c.  r1 = x // -> 3      |
>                         |           f.  r2 = y // -> 1

If we take the program:

    x = 3      |      y = 1
    y = 3      |      x = 1
    r1 = x     |      r2 = y

...then SC results are:
  (1, 1), for example: (x=3), (y=3), (y=1), (x=1), (r1=x), (r2=y)
  (1, 3), for example: (x=3), (y=1), (y=3), (x=1), (r1=x), (r2=y)
  (3, 1), for example: (x=3), (y=3), (r1=x), (y=1), (x=1), (r2=y)
  (3, 3), for example: (y=1), (x=1), (x=3), (y=3), (r1=x), (r2=y)

Note that as long as $x value set is concerned, it has all the SC
results from the first example. How's that not composable?

Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 836 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160623/047a6a23/attachment.sig>

From thurston at nomagicsoftware.com  Thu Jun 23 09:44:35 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 23 Jun 2016 06:44:35 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
Message-ID: <1466689475423-13581.post@n7.nabble.com>

"The second outcome is certainly allowed by JMM - there is a total order of
synchronization actions that will yield that outcome. "

No.  That is incorrect.
The program contains no data-races.  Why?  because *it only contains SAs*
(viz. volatile reads and writes).
There is no SO that produces the execution.

There is no sleight of hand going on.  I am using the terms as used in the
literature (Herlihy, et al).  It isn't necessary for me to define SC (and it
shouldn't be necessary for me to define "volatile memory" - are you really
unclear what that means?)

I encourage you to prove to yourself that the second execution isn't SC.

It is true that the language is awkward; the problem is that the following
expressions are used:

1.  SC is a memory model
2.  An execution is SC (or not)
3.  Some concurrent object (really type) is SC

It is in the latter sense that I describe volatile memory.  What I'm really
saying is something like  :
"The implementation of volatile memory" is SC.  Now if you replace volatile
memory with "concurrent queue", it's much more natural.
But we can think of (volatile) memory as a type, with two operations, read
and write (or load and store).
There is a "sequential specification" for memory, viz. every read (by
whichever thread) sees the immediately prior write (by whichever thread).
In the JLS, this is explicity described as "synchronization consistency"
(although I prefer "memory consistency" myself).

If we assume that volatile memory is implemented in an SC manner - then
execution #1 is clearly allowed; so far, no problem.

The problem comes when there are executions that involve multiple
"instances" of volatile memory (the volatile variables x and y).  If x and y
are SC, i.e. their "correctness property" is SC, then execution #2 is
possible/valid.  But it isn't SC!!  Note, this is a well known property of
SC objects - they aren't compositional (again, not my term); usually the
literature uses queues in place of memory, but non-compositionality is a
property of SC objects.

So, my proof is by way of contradiction:
Suppose volatile memory is SC,
then executions like #2 are possible,
but since #2 is not a SC execution, then it is proscribed by the JMM (since
the execution is DRF)







--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13581.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at gmail.com  Thu Jun 23 11:22:50 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 23 Jun 2016 16:22:50 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1466689475423-13581.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
 <1466689475423-13581.post@n7.nabble.com>
Message-ID: <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>


> On 23 Jun 2016, at 14:44, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> "The second outcome is certainly allowed by JMM - there is a total order of
> synchronization actions that will yield that outcome. "
> 
> No.  That is incorrect.
> The program contains no data-races.  Why?  because *it only contains SAs*
> (viz. volatile reads and writes).
> There is no SO that produces the execution.

I don’t know what reading of JMM produces that conclusion.

The synchronization order producing the outcome was given in another thread, but here it is for completeness:

(everything Thread 1 does first, followed by everything Thread 2 does)

x=3, y=3, r1=x, y=1, x=1, r2=y

A few other synchronization orders are also possible.

> There is no sleight of hand going on.  I am using the terms as used in the
> literature (Herlihy, et al).  It isn't necessary for me to define SC (and it
> shouldn't be necessary for me to define "volatile memory" - are you really
> unclear what that means?)
> 
> I encourage you to prove to yourself that the second execution isn't SC.


JMM is not about SC specifically. It is about seeing whether something is SC, if you need it to be SC. But you could just as well check if it is linearizable. But linearization makes sense only in the presence of some “correctness properties” that are out of scope of JMM. Like “a queue X is linearizable” means that any total order of operations preserve the queue’s properties (queue remains FIFO, queue preserves priority of elements, etc).

ABA problem is possible in JMM, but shows something is not SC.



> 
> It is true that the language is awkward; the problem is that the following
> expressions are used:
> 
> 1.  SC is a memory model
> 2.  An execution is SC (or not)
> 3.  Some concurrent object (really type) is SC
> 
> It is in the latter sense that I describe volatile memory.  What I'm really
> saying is something like  :
> "The implementation of volatile memory" is SC.  Now if you replace volatile
> memory with "concurrent queue", it's much more natural.
> But we can think of (volatile) memory as a type, with two operations, read
> and write (or load and store).
> There is a "sequential specification" for memory, viz. every read (by
> whichever thread) sees the immediately prior write (by whichever thread).
> In the JLS, this is explicity described as "synchronization consistency"
> (although I prefer "memory consistency" myself).
> 
> If we assume that volatile memory is implemented in an SC manner - then
> execution #1 is clearly allowed; so far, no problem.
> 
> The problem comes when there are executions that involve multiple
> "instances" of volatile memory (the volatile variables x and y).  If x and y
> are SC, i.e. their "correctness property" is SC, then execution #2 is
> possible/valid.  But it isn't SC!!  Note, this is a well known property of
> SC objects - they aren't compositional (again, not my term); usually the
> literature uses queues in place of memory, but non-compositionality is a
> property of SC objects.

You cannot replace queues and memory and use the same reasoning as in literature. A queue is an object with consistency requirements that have to be enforced in a particular way.



> So, my proof is by way of contradiction:
> Suppose volatile memory is SC,
> then executions like #2 are possible,
> but since #2 is not a SC execution, then it is proscribed by the JMM (since
> the execution is DRF)

Also, since JMM clearly allows execution #2, you have a problem in your proof - I suggest you check assumptions about what volatile memory is, and what JMM allows.


Alex


> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13581.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160623/f10b5f76/attachment-0001.html>

From thurston at nomagicsoftware.com  Thu Jun 23 10:58:50 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 23 Jun 2016 07:58:50 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
 <1466689475423-13581.post@n7.nabble.com>
 <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
Message-ID: <1466693930412-13583.post@n7.nabble.com>

Ah, yes.  My bad

(a, b) (b, c), (c,d), (d, e) (e, f) is one obvious SC execution that
produces the result; my example does not illustrate the problem (brain
cramp).

On your other points:
Memory has a  "sequential specification" as well, and it is specifically
mentioned in the JLS (as I explained); and the literature does indeed often
use memory as a type of shared object (for historical reasons it is often
termed "register").


Maybe there isn't a program possible that illustrates the non-composability
of memory as SC.
Regardless, my example certainly doesn't





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13583.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From thurston at nomagicsoftware.com  Thu Jun 23 11:37:00 2016
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 23 Jun 2016 08:37:00 -0700 (MST)
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
 <1466689475423-13581.post@n7.nabble.com>
 <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
Message-ID: <1466696220943-13584.post@n7.nabble.com>

Alex Otenko wrote
>> On 23 Jun 2016, at 14:44, thurstonn &lt;

> thurston@

> &gt; wrote:
>> 
>> "The second outcome is certainly allowed by JMM - there is a total order
>> of
>> synchronization actions that will yield that outcome. "
>> 
>> No.  That is incorrect.
>> The program contains no data-races.  Why?  because *it only contains SAs*
>> (viz. volatile reads and writes).
>> There is no SO that produces the execution.
> 
> I don’t know what reading of JMM produces that conclusion.

Which "conclusion" are you referring to?

"The program contains no data-races.  Why?  because *it only contains SAs*"

Are you suggesting that isn't deducible from the JMM?  
I don't see how a SA-only program can ever be incorrectly synchronized (it
can be incorrect for other reasons, but not improper synchronization).



"There is no SO that produces the execution" - yes that is incorrect (as I
conceded)




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13584.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at gmail.com  Thu Jun 23 13:38:01 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 23 Jun 2016 18:38:01 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1466696220943-13584.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
 <1466689475423-13581.post@n7.nabble.com>
 <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
 <1466696220943-13584.post@n7.nabble.com>
Message-ID: <6366339E-2C01-4EED-8B24-32479F8CB233@gmail.com>

I only referred to “no SO that produces the execution”. I am not sure of the importance of the statement that there are no data races.

Then when reasoning about correctness, one need to fix either the order (and work out what can be observed), or the observations (and work out what order can produce that). In your original post it looked like you fixed both.


Alex

> On 23 Jun 2016, at 16:37, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> Alex Otenko wrote
>>> On 23 Jun 2016, at 14:44, thurstonn &lt;
> 
>> thurston@
> 
>> &gt; wrote:
>>> 
>>> "The second outcome is certainly allowed by JMM - there is a total order
>>> of
>>> synchronization actions that will yield that outcome. "
>>> 
>>> No.  That is incorrect.
>>> The program contains no data-races.  Why?  because *it only contains SAs*
>>> (viz. volatile reads and writes).
>>> There is no SO that produces the execution.
>> 
>> I don’t know what reading of JMM produces that conclusion.
> 
> Which "conclusion" are you referring to?
> 
> "The program contains no data-races.  Why?  because *it only contains SAs*"
> 
> Are you suggesting that isn't deducible from the JMM?  
> I don't see how a SA-only program can ever be incorrectly synchronized (it
> can be incorrect for other reasons, but not improper synchronization).
> 
> 
> 
> "There is no SO that produces the execution" - yes that is incorrect (as I
> conceded)
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13584.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at gmail.com  Thu Jun 23 13:43:48 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Thu, 23 Jun 2016 18:43:48 +0100
Subject: [concurrency-interest] Is Synchronization Order A *Strict*
	Total Order?
In-Reply-To: <1466693930412-13583.post@n7.nabble.com>
References: <1464619256580-13457.post@n7.nabble.com>
 <1466683808391-13578.post@n7.nabble.com>
 <396FA22C-523C-46C3-BA61-762FDF5F62AA@gmail.com>
 <1466689475423-13581.post@n7.nabble.com>
 <E67EBF3B-F470-4577-8F50-C65E3B406CB3@gmail.com>
 <1466693930412-13583.post@n7.nabble.com>
Message-ID: <52953EF7-1643-4483-823F-882400832CAA@gmail.com>

A sequential specification of memory is not yet sequential consistency of software at an arbitrary level of abstraction.

Alex

> On 23 Jun 2016, at 15:58, thurstonn <thurston at nomagicsoftware.com> wrote:
> 
> Ah, yes.  My bad
> 
> (a, b) (b, c), (c,d), (d, e) (e, f) is one obvious SC execution that
> produces the result; my example does not illustrate the problem (brain
> cramp).
> 
> On your other points:
> Memory has a  "sequential specification" as well, and it is specifically
> mentioned in the JLS (as I explained); and the literature does indeed often
> use memory as a type of shared object (for historical reasons it is often
> termed "register").
> 
> 
> Maybe there isn't a program possible that illustrates the non-composability
> of memory as SC.
> Regardless, my example certainly doesn't
> 
> 
> 
> 
> 
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Is-Synchronization-Order-A-Strict-Total-Order-tp13457p13583.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From ivan.gerasimov at oracle.com  Thu Jun 30 09:42:42 2016
From: ivan.gerasimov at oracle.com (Ivan Gerasimov)
Date: Thu, 30 Jun 2016 16:42:42 +0300
Subject: [concurrency-interest] javadoc typos
Message-ID: <9dfb2ad7-f12b-48e7-b227-9c02d3aabcbc@oracle.com>

Hello!

A couple of minor javadoc corrections:


diff --git 
a/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java 
b/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
--- a/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
+++ b/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
@@ -92,7 +92,7 @@
   * encountering the exception; minimally only the latter.
   *
   * <p>It is possible to define and use ForkJoinTasks that may block,
- * but doing do requires three further considerations: (1) Completion
+ * but doing so requires three further considerations: (1) Completion
   * of few if any <em>other</em> tasks should be dependent on a task
   * that blocks on external synchronization or I/O. Event-style async
   * tasks that are never joined (for example, those subclassing {@link
diff --git 
a/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java 
b/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
--- 
a/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
+++ 
b/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
@@ -68,7 +68,7 @@
   * <p>Class {@link LongAdder} provides analogs of the functionality of
   * this class for the common special case of maintaining counts and
   * sums.  The call {@code new LongAdder()} is equivalent to {@code new
- * LongAccumulator((x, y) -> x + y, 0L}.
+ * LongAccumulator((x, y) -> x + y, 0L)}.
   *
   * <p>This class extends {@link Number}, but does <em>not</em> define
   * methods such as {@code equals}, {@code hashCode} and {@code


With kind regards,
Ivan


From martinrb at google.com  Thu Jun 30 10:23:28 2016
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 30 Jun 2016 07:23:28 -0700
Subject: [concurrency-interest] javadoc typos
In-Reply-To: <9dfb2ad7-f12b-48e7-b227-9c02d3aabcbc@oracle.com>
References: <9dfb2ad7-f12b-48e7-b227-9c02d3aabcbc@oracle.com>
Message-ID: <CA+kOe0_N5XFcwH5v4mw6y8DegXiv4vea0UdJGmaOFHc3jy1fVg@mail.gmail.com>

Thanks!

These will be fixed in our upcoming jdk9 integration.

On Thu, Jun 30, 2016 at 6:42 AM, Ivan Gerasimov <ivan.gerasimov at oracle.com>
wrote:

> Hello!
>
> A couple of minor javadoc corrections:
>
>
> diff --git
> a/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
> b/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
> --- a/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
> +++ b/src/java.base/share/classes/java/util/concurrent/ForkJoinTask.java
> @@ -92,7 +92,7 @@
>   * encountering the exception; minimally only the latter.
>   *
>   * <p>It is possible to define and use ForkJoinTasks that may block,
> - * but doing do requires three further considerations: (1) Completion
> + * but doing so requires three further considerations: (1) Completion
>   * of few if any <em>other</em> tasks should be dependent on a task
>   * that blocks on external synchronization or I/O. Event-style async
>   * tasks that are never joined (for example, those subclassing {@link
> diff --git
> a/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
> b/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
> ---
> a/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
> +++
> b/src/java.base/share/classes/java/util/concurrent/atomic/LongAccumulator.java
> @@ -68,7 +68,7 @@
>   * <p>Class {@link LongAdder} provides analogs of the functionality of
>   * this class for the common special case of maintaining counts and
>   * sums.  The call {@code new LongAdder()} is equivalent to {@code new
> - * LongAccumulator((x, y) -> x + y, 0L}.
> + * LongAccumulator((x, y) -> x + y, 0L)}.
>   *
>   * <p>This class extends {@link Number}, but does <em>not</em> define
>   * methods such as {@code equals}, {@code hashCode} and {@code
>
>
> With kind regards,
> Ivan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20160630/a583b43a/attachment.html>

