From heinz at javaspecialists.eu  Tue Nov  1 08:17:35 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 14:17:35 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
Message-ID: <581887DF.80501@javaspecialists.eu>

Hi,

in my latest newsletter, I showed an IntList that was protected with a 
StampedLock.  http://www.javaspecialists.eu/archive/Issue242.html  It 
was more an educational exercise than a class you'd want to use to store 
ints :-)

In my first version of this class, I made size volatile, but Alex Snaps 
suggested that I could simply call tryOptimisticRead() before reading 
the size field.  Since inside tryOptimisticRead() we are doing a 
volatile read of "state", I believe that we are correct with the code 
below.  However, there were some objections to this code on Twitter 
""Volatile read == synchronized" will keep biting us forever. The 
comment is broken." 
https://twitter.com/VladimirSitnikv/status/793364258658607104 "

I based my comment on JCiP and the JMM FAQ, so I think it is correct.  
There was a follow-on comment that there was no happens-before guarantee 
with tryOptimisticRead(), but IMHO there would have to be, otherwise it 
would be possible to read stale fields into local variables.  
StampedLock (and similar constructs) would be fundamentally broken if 
this does not work.

import java.util.*;
import java.util.concurrent.locks.*;

public class IntList {
  private static final int OPTIMISTIC_SPIN = 3;
  private final StampedLock sl = new StampedLock();
  private int[] arr = new int[10];
  private int size = 0;

  public int size() {
    // volatile read, same as entering a synchronized block
    sl.tryOptimisticRead();
    return this.size;
  }

  public int get(int index) {
    for (int i = 0; i < OPTIMISTIC_SPIN; i++) {
      long stamp = sl.tryOptimisticRead();
      int size = this.size;
      int[] arr = this.arr;
      if (index < arr.length) {
        int r = arr[index];
        if (sl.validate(stamp)) {
          rangeCheck(index, size);
          return r;
        }
      }
    }
    long stamp = sl.readLock();
    try {
      rangeCheck(index, this.size);
      return this.arr[index];
    } finally {
      sl.unlockRead(stamp);
    }
  }

  public void trimToSize() {
    long stamp = sl.tryOptimisticRead();
    int currentSize = size;
    int[] currentArr = arr;
    if (sl.validate(stamp)) {
      // fast optimistic read to accelerate trimToSize() when
      // there is no work to do
      if (currentSize == currentArr.length) return;
    }
    stamp = sl.writeLock();
    try {
      if (size < arr.length) {
        arr = Arrays.copyOf(arr, size);
      }
    } finally {
      sl.unlockWrite(stamp);
    }
  }

  public boolean add(int e) {
    long stamp = sl.writeLock();
    try {
      if (size + 1 > arr.length)
        arr = Arrays.copyOf(arr, size + 10);

      arr[size++] = e;
      return true;
    } finally {
      sl.unlockWrite(stamp);
    }
  }

  // just to illustrate how an upgrade could be coded
  public int removeWithUpgrade(int index) {
    long stamp = sl.readLock();
    try {
      while (true) {
        rangeCheck(index, size);
        long writeStamp = sl.tryConvertToWriteLock(stamp);
        if (writeStamp == 0) {
          sl.unlockRead(stamp);
          stamp = sl.writeLock();
        } else {
          stamp = writeStamp;
          return doActualRemove(index);
        }
      }
    } finally {
      sl.unlock(stamp);
    }
  }

  public int remove(int index) {
    long stamp = sl.writeLock();
    try {
      rangeCheck(index, size);
      return doActualRemove(index);
    } finally {
      sl.unlock(stamp);
    }
  }

  private int doActualRemove(int index) {
    int oldValue = arr[index];

    int numMoved = size - index - 1;
    if (numMoved > 0)
      System.arraycopy(arr, index + 1, arr, index, numMoved);
    arr[--size] = 0;

    return oldValue;
  }

  private static void rangeCheck(int index, int size) {
    if (index >= size)
      throw new IndexOutOfBoundsException(
          "Index: " + index + ", Size: " + size);
  }
}

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion since 2005
JavaOne Rock Star Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz


From nitsanw at azulsystems.com  Tue Nov  1 10:37:30 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Tue, 1 Nov 2016 16:37:30 +0200
Subject: [concurrency-interest] StampedLock happens-before with
 tryOptimisticRead()
In-Reply-To: <581887DF.80501@javaspecialists.eu>
References: <581887DF.80501@javaspecialists.eu>
Message-ID: <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>

Hi,
Even if the code is correct the comment is misleading. Volatile loads 
are not the same as syncronized block entry. They may work in particular 
usecases as a cheap replacement, but that is context sensitive. The 
"volatile read of a field written in a synchronized block" idiom is 
appropriate in this case if size was volatile, but that cannot be 
generalized as the comment suggests. (E.g. Size read is not volatile, 
this implies for instance that if I were to change it to a long this 
code will be broken. If I were to add writes to the mix they can get 
reordered etc)

But I don't believe the code is correct. Because the value returned by 
tryOptimisticRead is never used it can be eliminated after inlining, at 
that point the fence/barrier is also notionally gone. See here:

https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles

Cheers,

Nitsan



From Maciej.Bobrowski at morganstanley.com  Tue Nov  1 10:53:21 2016
From: Maciej.Bobrowski at morganstanley.com (Bobrowski, Maciej)
Date: Tue, 1 Nov 2016 14:53:21 +0000
Subject: [concurrency-interest] Unsynchronized read guarantees
Message-ID: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>

Hi,

Given a very simple class:

class Counter {
  private int val = 0;

  public int unsynchronizedGet() {
    return val;
  }
  public synchronized void increment() {
    val++;
  }
}

If I have one thread periodically calling increment() on an instance of Counter, and the other thread periodically reading the value (using unsynchronized get), my question is: are there any guarantees about what value the unsynchronizedGet returns? Can it be e.g. 0 for a first few reads, then e.g. 20 (because the new value was loaded from memory)? Or will it either always be 0 (cached in a register) or will it always keep on increasing (being read from memory)?

Any pointers would be greatly appreciated



________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/01e4ef50/attachment.html>

From oleksandr.otenko at gmail.com  Tue Nov  1 11:07:00 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 1 Nov 2016 15:07:00 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
Message-ID: <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>

Graal’s behaviour in that link is not correct. The comment there simply says that it is likely to cause you problems, even though formally the barriers should have stayed.

Alex

> On 1 Nov 2016, at 14:37, Nitsan Wakart <nitsanw at azulsystems.com> wrote:
> 
> Hi,
> Even if the code is correct the comment is misleading. Volatile loads are not the same as syncronized block entry. They may work in particular usecases as a cheap replacement, but that is context sensitive. The "volatile read of a field written in a synchronized block" idiom is appropriate in this case if size was volatile, but that cannot be generalized as the comment suggests. (E.g. Size read is not volatile, this implies for instance that if I were to change it to a long this code will be broken. If I were to add writes to the mix they can get reordered etc)
> 
> But I don't believe the code is correct. Because the value returned by tryOptimisticRead is never used it can be eliminated after inlining, at that point the fence/barrier is also notionally gone. See here:
> 
> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles
> 
> Cheers,
> 
> Nitsan
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From nathanila at gmail.com  Tue Nov  1 11:41:14 2016
From: nathanila at gmail.com (Nathan & Ila Reynolds)
Date: Tue, 1 Nov 2016 09:41:14 -0600
Subject: [concurrency-interest] Unsynchronized read guarantees
In-Reply-To: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
References: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
Message-ID: <01a401d23456$62146020$263d2060$@gmail.com>

Let's take the following code.

 

while (unsynchronizedGet() == 0)

{

   // spin

}

 

JIT is allowed to inline unsynchronizedGet().  JIT is allowed to skip
loading the field "val" and simply use it as a constant.  In fact, the
entire loop could be optimized down to something like this.

 

Label:

   Goto Label;

 

As soon as you put a break point on the loop, the execute will switch back
to bytecode and "val" will be loaded from memory.

 

To "fix" the code, you could declare "val" to be volatile.  This will force
"val" to be loaded from cache/RAM every time.

 

-Nathan

 

From: Concurrency-interest
[mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Bobrowski,
Maciej
Sent: Tuesday, November 01, 2016 8:53 AM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] Unsynchronized read guarantees

 

Hi,

 

Given a very simple class:

 

class Counter {

  private int val = 0;

 

  public int unsynchronizedGet() {

    return val;

  }

  public synchronized void increment() {

    val++;

  }

}

 

If I have one thread periodically calling increment() on an instance of
Counter, and the other thread periodically reading the value (using
unsynchronized get), my question is: are there any guarantees about what
value the unsynchronizedGet returns? Can it be e.g. 0 for a first few reads,
then e.g. 20 (because the new value was loaded from memory)? Or will it
either always be 0 (cached in a register) or will it always keep on
increasing (being read from memory)?

 

Any pointers would be greatly appreciated

 

 

  _____  


NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions
or views contained herein are not intended to be, and do not constitute,
advice within the meaning of Section 975 of the Dodd-Frank Wall Street
Reform and Consumer Protection Act. If you have received this communication
in error, please destroy all electronic and paper copies and notify the
sender immediately. Mistransmission is not intended to waive confidentiality
or privilege. Morgan Stanley reserves the right, to the extent permitted
under applicable law, to monitor electronic communications. This message is
subject to terms available at the following link:
http://www.morganstanley.com/disclaimers  If you cannot access these links,
please notify us by reply message and we will send the contents to you. By
communicating with Morgan Stanley you consent to the foregoing and to the
voice recording of conversations with personnel of Morgan Stanley.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/1872ba49/attachment-0001.html>

From nitsanw at azulsystems.com  Tue Nov  1 11:42:56 2016
From: nitsanw at azulsystems.com (Nitsan Wakart)
Date: Tue, 1 Nov 2016 17:42:56 +0200
Subject: [concurrency-interest] StampedLock happens-before with
 tryOptimisticRead()
In-Reply-To: <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
Message-ID: <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>

Can you provide a pointer to the JMM or other sources? I'm not sure 
which comment you refer to.

I can't see clear rules around the behaviour of eliminated volatile 
accesses expanded on in JMM or cookbook, but I could easily have missed 
something.

Thanks


On 01/11/2016 17:07, Alex Otenko wrote:
> Graal’s behaviour in that link is not correct. The comment there simply says that it is likely to cause you problems, even though formally the barriers should have stayed.
>
> Alex

From oleksandr.otenko at gmail.com  Tue Nov  1 12:02:35 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 1 Nov 2016 16:02:35 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
Message-ID: <860F11DB-14CE-4499-A523-98AA354CCF99@gmail.com>

I am referring to “But we certainly would not like to throw compilers under the bus and say this is forbidden.”

If elimination of barriers destroys synchronizes-with edges, it should not be allowed. The write itself is futile, so it’s ok to eliminate it, but the h-b edges are not.

That particular idiom, 

        x = 1;
        h.GREAT_BARRIER_REEF = h.GREAT_BARRIER_REEF;
        y = 1;

is broken for other reasons, and would permit to reorder the reads/writes like so:

        tmp =  h.GREAT_BARRIER_REEF;
        x = 1;
        y = 1;
        h.GREAT_BARRIER_REEF = tmp;

But complete elimination of the barriers would be wrong.


Alex


> On 1 Nov 2016, at 15:42, Nitsan Wakart <nitsanw at azulsystems.com> wrote:
> 
> Can you provide a pointer to the JMM or other sources? I'm not sure which comment you refer to.
> 
> I can't see clear rules around the behaviour of eliminated volatile accesses expanded on in JMM or cookbook, but I could easily have missed something.
> 
> Thanks
> 
> 
> On 01/11/2016 17:07, Alex Otenko wrote:
>> Graal’s behaviour in that link is not correct. The comment there simply says that it is likely to cause you problems, even though formally the barriers should have stayed.
>> 
>> Alex


From henri.tremblay at gmail.com  Tue Nov  1 12:02:39 2016
From: henri.tremblay at gmail.com (Henri Tremblay)
Date: Tue, 1 Nov 2016 12:02:39 -0400
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
Message-ID: <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>

Alex is referring to Alexsey's post
https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles

There is a comment about Graal. Telling that nothing prevents a volatile
assignment from disappearing if unused (and so the barrier). Even though C2
doesn't do it right now.

On 1 November 2016 at 11:42, Nitsan Wakart <nitsanw at azulsystems.com> wrote:

> Can you provide a pointer to the JMM or other sources? I'm not sure which
> comment you refer to.
>
> I can't see clear rules around the behaviour of eliminated volatile
> accesses expanded on in JMM or cookbook, but I could easily have missed
> something.
>
> Thanks
>
>
> On 01/11/2016 17:07, Alex Otenko wrote:
>
>> Graal’s behaviour in that link is not correct. The comment there simply
>> says that it is likely to cause you problems, even though formally the
>> barriers should have stayed.
>>
>> Alex
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/b84da73a/attachment.html>

From Maciej.Bobrowski at morganstanley.com  Tue Nov  1 12:18:06 2016
From: Maciej.Bobrowski at morganstanley.com (Bobrowski, Maciej)
Date: Tue, 1 Nov 2016 16:18:06 +0000
Subject: [concurrency-interest] Unsynchronized read guarantees
In-Reply-To: <01a401d23456$62146020$263d2060$@gmail.com>
References: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
 <01a401d23456$62146020$263d2060$@gmail.com>
Message-ID: <9838003A10254741BC6FFADE2ED02B4581D08E26@OZWEX0205N1.msad.ms.com>

So basically, I could put a breakpoint at any time, causing unpredictable behaviour for the value of counter, is that right?

From: Nathan & Ila Reynolds [mailto:nathanila at gmail.com]
Sent: 01 November 2016 15:41
To: Bobrowski, Maciej (IST); concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] Unsynchronized read guarantees

Let's take the following code.

while (unsynchronizedGet() == 0)
{
   // spin
}

JIT is allowed to inline unsynchronizedGet().  JIT is allowed to skip loading the field "val" and simply use it as a constant.  In fact, the entire loop could be optimized down to something like this.

Label:
   Goto Label;

As soon as you put a break point on the loop, the execute will switch back to bytecode and "val" will be loaded from memory.

To "fix" the code, you could declare "val" to be volatile.  This will force "val" to be loaded from cache/RAM every time.

-Nathan

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Bobrowski, Maciej
Sent: Tuesday, November 01, 2016 8:53 AM
To: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] Unsynchronized read guarantees

Hi,

Given a very simple class:

class Counter {
  private int val = 0;

  public int unsynchronizedGet() {
    return val;
  }
  public synchronized void increment() {
    val++;
  }
}

If I have one thread periodically calling increment() on an instance of Counter, and the other thread periodically reading the value (using unsynchronized get), my question is: are there any guarantees about what value the unsynchronizedGet returns? Can it be e.g. 0 for a first few reads, then e.g. 20 (because the new value was loaded from memory)? Or will it either always be 0 (cached in a register) or will it always keep on increasing (being read from memory)?

Any pointers would be greatly appreciated


________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.


________________________________

NOTICE: Morgan Stanley is not acting as a municipal advisor and the opinions or views contained herein are not intended to be, and do not constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall Street Reform and Consumer Protection Act. If you have received this communication in error, please destroy all electronic and paper copies and notify the sender immediately. Mistransmission is not intended to waive confidentiality or privilege. Morgan Stanley reserves the right, to the extent permitted under applicable law, to monitor electronic communications. This message is subject to terms available at the following link: http://www.morganstanley.com/disclaimers  If you cannot access these links, please notify us by reply message and we will send the contents to you. By communicating with Morgan Stanley you consent to the foregoing and to the voice recording of conversations with personnel of Morgan Stanley.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/33c7e651/attachment-0001.html>

From alex.snaps at gmail.com  Tue Nov  1 12:18:00 2016
From: alex.snaps at gmail.com (Alex Snaps)
Date: Tue, 01 Nov 2016 16:18:00 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
 <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>
Message-ID: <CAKux6pbA2J3hsA8k9zazpV1-9LFQ+w2bFDSdfYyNcdg_BzrcXw@mail.gmail.com>

Am I getting you right Alex, that since we don't `StampedLock.validate()`
(or otherwise _use_) the status returned by `.tryOptimisticRead()` the
barrier may disappear as the code gets optimized? So that the read/write to
the volatile status within StampedLock doesn't provide enough guarantees...

On Tue, Nov 1, 2016 at 12:10 PM Henri Tremblay <henri.tremblay at gmail.com>
wrote:

> Alex is referring to Alexsey's post
> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles
>
> There is a comment about Graal. Telling that nothing prevents a volatile
> assignment from disappearing if unused (and so the barrier). Even though C2
> doesn't do it right now.
>
> On 1 November 2016 at 11:42, Nitsan Wakart <nitsanw at azulsystems.com>
> wrote:
>
> Can you provide a pointer to the JMM or other sources? I'm not sure which
> comment you refer to.
>
> I can't see clear rules around the behaviour of eliminated volatile
> accesses expanded on in JMM or cookbook, but I could easily have missed
> something.
>
> Thanks
>
>
> On 01/11/2016 17:07, Alex Otenko wrote:
>
> Graal’s behaviour in that link is not correct. The comment there simply
> says that it is likely to cause you problems, even though formally the
> barriers should have stayed.
>
> Alex
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-- 
Alex Snaps
Twitter: @alexsnaps
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/10ef2744/attachment.html>

From oleksandr.otenko at gmail.com  Tue Nov  1 12:36:41 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 1 Nov 2016 16:36:41 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CAKux6pbA2J3hsA8k9zazpV1-9LFQ+w2bFDSdfYyNcdg_BzrcXw@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
 <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>
 <CAKux6pbA2J3hsA8k9zazpV1-9LFQ+w2bFDSdfYyNcdg_BzrcXw@mail.gmail.com>
Message-ID: <7C78324C-986A-42A2-A6BB-91B75EB5CCFF@gmail.com>

In my opinion Nitsan’s comment about the validity of the algorithm and the caveats is correct. But I disagree with the suggestion that the barriers could/should be optimized for unused reads. The article Aleksey posted in his blog is talking about a different case, where there may be room for optimization - eg through reordering of reads.

But if Graal completely removes the barriers (rather than move them around) in cases like these, it is not correct.

Alex

> On 1 Nov 2016, at 16:18, Alex Snaps <alex.snaps at gmail.com> wrote:
> 
> Am I getting you right Alex, that since we don't `StampedLock.validate()` (or otherwise _use_) the status returned by `.tryOptimisticRead()` the barrier may disappear as the code gets optimized? So that the read/write to the volatile status within StampedLock doesn't provide enough guarantees... 
> 
> On Tue, Nov 1, 2016 at 12:10 PM Henri Tremblay <henri.tremblay at gmail.com <mailto:henri.tremblay at gmail.com>> wrote:
> Alex is referring to Alexsey's post https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles <https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles>
> 
> There is a comment about Graal. Telling that nothing prevents a volatile assignment from disappearing if unused (and so the barrier). Even though C2 doesn't do it right now.
> 
> On 1 November 2016 at 11:42, Nitsan Wakart <nitsanw at azulsystems.com <mailto:nitsanw at azulsystems.com>> wrote:
> Can you provide a pointer to the JMM or other sources? I'm not sure which comment you refer to.
> 
> I can't see clear rules around the behaviour of eliminated volatile accesses expanded on in JMM or cookbook, but I could easily have missed something.
> 
> Thanks
> 
> 
> On 01/11/2016 17:07, Alex Otenko wrote:
> Graal’s behaviour in that link is not correct. The comment there simply says that it is likely to cause you problems, even though formally the barriers should have stayed.
> 
> Alex
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> -- 
> Alex Snaps
> Twitter: @alexsnaps
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/cdd70f9b/attachment.html>

From alex.snaps at gmail.com  Tue Nov  1 12:46:24 2016
From: alex.snaps at gmail.com (Alex Snaps)
Date: Tue, 01 Nov 2016 16:46:24 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <7C78324C-986A-42A2-A6BB-91B75EB5CCFF@gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
 <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>
 <CAKux6pbA2J3hsA8k9zazpV1-9LFQ+w2bFDSdfYyNcdg_BzrcXw@mail.gmail.com>
 <7C78324C-986A-42A2-A6BB-91B75EB5CCFF@gmail.com>
Message-ID: <CAKux6pY0ZQKQ9VasXqpk49fWFogpFHANbs2KdZ+9cyZO9sVytw@mail.gmail.com>

So as per my understanding this code is currently (still) correct... But I
also I can see how people get confused by synchronized == volatile.
Now, forgive my ignorance, but with Graal, we're talking about a new
revision of the JMM then, is that right?
Piggybacking on volatile read/write, afaik, has been widely used out
there... This would loosen the contract, right?



On Tue, Nov 1, 2016 at 12:36 PM Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> In my opinion Nitsan’s comment about the validity of the algorithm and the
> caveats is correct. But I disagree with the suggestion that the barriers
> could/should be optimized for unused reads. The article Aleksey posted in
> his blog is talking about a different case, where there may be room for
> optimization - eg through reordering of reads.
>
> But if Graal completely removes the barriers (rather than move them
> around) in cases like these, it is not correct.
>
>
> Alex
>
> On 1 Nov 2016, at 16:18, Alex Snaps <alex.snaps at gmail.com> wrote:
>
> Am I getting you right Alex, that since we don't `StampedLock.validate()`
> (or otherwise _use_) the status returned by `.tryOptimisticRead()` the
> barrier may disappear as the code gets optimized? So that the read/write to
> the volatile status within StampedLock doesn't provide enough
> guarantees...
>
> On Tue, Nov 1, 2016 at 12:10 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
> Alex is referring to Alexsey's post
> https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles
>
> There is a comment about Graal. Telling that nothing prevents a volatile
> assignment from disappearing if unused (and so the barrier). Even though C2
> doesn't do it right now.
>
> On 1 November 2016 at 11:42, Nitsan Wakart <nitsanw at azulsystems.com>
> wrote:
>
> Can you provide a pointer to the JMM or other sources? I'm not sure which
> comment you refer to.
>
> I can't see clear rules around the behaviour of eliminated volatile
> accesses expanded on in JMM or cookbook, but I could easily have missed
> something.
>
> Thanks
>
>
> On 01/11/2016 17:07, Alex Otenko wrote:
>
> Graal’s behaviour in that link is not correct. The comment there simply
> says that it is likely to cause you problems, even though formally the
> barriers should have stayed.
>
> Alex
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Alex Snaps
> Twitter: @alexsnaps
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
Alex Snaps
Twitter: @alexsnaps
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/dc5a0686/attachment-0001.html>

From martinrb at google.com  Tue Nov  1 12:55:13 2016
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 1 Nov 2016 09:55:13 -0700
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <7C78324C-986A-42A2-A6BB-91B75EB5CCFF@gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <aaa03a17-35ef-1952-de86-d8fee85eb37a@azulsystems.com>
 <1E881566-FA62-4A39-A4EC-CE200A73A773@gmail.com>
 <433a3414-56b4-c955-edf9-68747033747e@azulsystems.com>
 <CADZL2=t5X20xZEA1cQJCAh4-Zo615FG4nO6pQzHM5wS1fzmjXg@mail.gmail.com>
 <CAKux6pbA2J3hsA8k9zazpV1-9LFQ+w2bFDSdfYyNcdg_BzrcXw@mail.gmail.com>
 <7C78324C-986A-42A2-A6BB-91B75EB5CCFF@gmail.com>
Message-ID: <CA+kOe08mGK2+iGmKVKwiynLn14hTq9LZq9KSJCH5w4xEwcA0sw@mail.gmail.com>

Using and implementing StampedLock is tricky.  There were implementation
changes in jdk9.
See previous discussion
Does StampedLock need a releaseFence in theory?
http://markmail.org/message/smysl447nbgnl2ud?q=StampedLock+list:edu%2Eoswego%2Ecs%2Econcurrency-interest#query:StampedLock%20list%3Aedu.oswego.cs.concurrency-interest+page:1+mid:cuinojiqrnb7rkhx+state:results
which did lead to an implementation change.

I keep thinking of adding "No one is smart enough to use StampedLock
correctly." to the javadoc, but we haven't quite gone that far.

 *  <li><b>Optimistic Reading.</b> Method {@link #tryOptimisticRead}
 *   returns a non-zero stamp only if the lock is not currently held
 *   in write mode. Method {@link #validate} returns true if the lock
 *   has not been acquired in write mode since obtaining a given
 *   stamp.  This mode can be thought of as an extremely weak version
 *   of a read-lock, that can be broken by a writer at any time.  The
 *   use of optimistic mode for short read-only code segments often
 *   reduces contention and improves throughput.  However, its use is
 *   inherently fragile.  Optimistic read sections should only read
 *   fields and hold them in local variables for later use after
 *   validation. Fields read while in optimistic mode may be wildly
 *   inconsistent, so usage applies only when you are familiar enough
 *   with data representations to check consistency and/or repeatedly
 *   invoke method {@code validate()}.  For example, such steps are
 *   typically required when first reading an object or array
 *   reference, and then accessing one of its fields, elements or
 *   methods.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/82ede6ce/attachment.html>

From martinrb at google.com  Tue Nov  1 13:01:50 2016
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 1 Nov 2016 10:01:50 -0700
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <581887DF.80501@javaspecialists.eu>
References: <581887DF.80501@javaspecialists.eu>
Message-ID: <CA+kOe08H__b1XZOp-HoXqTfh7mXVdMMo9XMi6=EsHnvAMJeWcA@mail.gmail.com>

On Tue, Nov 1, 2016 at 5:17 AM, Dr Heinz M. Kabutz <heinz at javaspecialists.eu
> wrote:

>
> In my first version of this class, I made size volatile, but Alex Snaps
> suggested that I could simply call tryOptimisticRead() before reading the
> size field.


It is the intent that fields protected by a StampedLock need not be
volatile, but then for example out-of-thin-air reads of longs becomes
possible.  You should be OK if you always validate the stamp before using
any of the values read.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/f25897c0/attachment.html>

From dahankzter at gmail.com  Tue Nov  1 13:30:16 2016
From: dahankzter at gmail.com (Henrik Johansson)
Date: Tue, 01 Nov 2016 17:30:16 +0000
Subject: [concurrency-interest] Unsynchronized read guarantees
In-Reply-To: <9838003A10254741BC6FFADE2ED02B4581D08E26@OZWEX0205N1.msad.ms.com>
References: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
 <01a401d23456$62146020$263d2060$@gmail.com>
 <9838003A10254741BC6FFADE2ED02B4581D08E26@OZWEX0205N1.msad.ms.com>
Message-ID: <CAKOF6953JRSMd-PMF4ojuYH_6RzGXOG7q_peZbROFFxoffG7mA@mail.gmail.com>

We actually saw this break in some old code between updates of the jvm. Not
sure exactly when but stuff broke spectacularly and fortunately making the
one affected variable volatile fixed it. It was a little bit funny and a
very enlightening teaching moment.

On Tue, Nov 1, 2016, 17:21 Bobrowski, Maciej <
Maciej.Bobrowski at morganstanley.com> wrote:

> So basically, I could put a breakpoint at any time, causing unpredictable
> behaviour for the value of counter, is that right?
>
>
>
> *From:* Nathan & Ila Reynolds [mailto:nathanila at gmail.com]
> *Sent:* 01 November 2016 15:41
> *To:* Bobrowski, Maciej (IST); concurrency-interest at cs.oswego.edu
> *Subject:* RE: [concurrency-interest] Unsynchronized read guarantees
>
>
>
> Let’s take the following code.
>
>
>
> while (unsynchronizedGet() == 0)
>
> {
>
>    // spin
>
> }
>
>
>
> JIT is allowed to inline unsynchronizedGet().  JIT is allowed to skip
> loading the field “val” and simply use it as a constant.  In fact, the
> entire loop could be optimized down to something like this.
>
>
>
> Label:
>
>    Goto Label;
>
>
>
> As soon as you put a break point on the loop, the execute will switch back
> to bytecode and “val” will be loaded from memory.
>
>
>
> To “fix” the code, you could declare “val” to be volatile.  This will
> force “val” to be loaded from cache/RAM every time.
>
>
>
> -Nathan
>
>
>
> *From:* Concurrency-interest [
> mailto:concurrency-interest-bounces at cs.oswego.edu
> <concurrency-interest-bounces at cs.oswego.edu>] *On Behalf Of *Bobrowski,
> Maciej
> *Sent:* Tuesday, November 01, 2016 8:53 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Unsynchronized read guarantees
>
>
>
> Hi,
>
>
>
> Given a very simple class:
>
>
>
> class Counter {
>
>   private int val = 0;
>
>
>
>   public int unsynchronizedGet() {
>
>     return val;
>
>   }
>
>   public synchronized void increment() {
>
>     val++;
>
>   }
>
> }
>
>
>
> If I have one thread periodically calling increment() on an instance of
> Counter, and the other thread periodically reading the value (using
> unsynchronized get), my question is: are there any guarantees about what
> value the unsynchronizedGet returns? Can it be e.g. 0 for a first few
> reads, then e.g. 20 (because the new value was loaded from memory)? Or will
> it either always be 0 (cached in a register) or will it always keep on
> increasing (being read from memory)?
>
>
>
> Any pointers would be greatly appreciated
>
>
>
>
> ------------------------------
>
>
> NOTICE: Morgan Stanley is not acting as a municipal advisor and the
> opinions or views contained herein are not intended to be, and do not
> constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall
> Street Reform and Consumer Protection Act. If you have received this
> communication in error, please destroy all electronic and paper copies and
> notify the sender immediately. Mistransmission is not intended to waive
> confidentiality or privilege. Morgan Stanley reserves the right, to the
> extent permitted under applicable law, to monitor electronic
> communications. This message is subject to terms available at the following
> link: http://www.morganstanley.com/disclaimers  If you cannot access
> these links, please notify us by reply message and we will send the
> contents to you. By communicating with Morgan Stanley you consent to the
> foregoing and to the voice recording of conversations with personnel of
> Morgan Stanley.
>
>
>
> ------------------------------
>
> NOTICE: Morgan Stanley is not acting as a municipal advisor and the
> opinions or views contained herein are not intended to be, and do not
> constitute, advice within the meaning of Section 975 of the Dodd-Frank Wall
> Street Reform and Consumer Protection Act. If you have received this
> communication in error, please destroy all electronic and paper copies and
> notify the sender immediately. Mistransmission is not intended to waive
> confidentiality or privilege. Morgan Stanley reserves the right, to the
> extent permitted under applicable law, to monitor electronic
> communications. This message is subject to terms available at the following
> link: http://www.morganstanley.com/disclaimers  If you cannot access
> these links, please notify us by reply message and we will send the
> contents to you. By communicating with Morgan Stanley you consent to the
> foregoing and to the voice recording of conversations with personnel of
> Morgan Stanley.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/65d079d8/attachment-0001.html>

From shade at redhat.com  Tue Nov  1 13:38:40 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Tue, 1 Nov 2016 18:38:40 +0100
Subject: [concurrency-interest] Unsynchronized read guarantees
In-Reply-To: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
References: <9838003A10254741BC6FFADE2ED02B4581D08D65@OZWEX0205N1.msad.ms.com>
Message-ID: <efa4aa83-d91f-0644-b793-9913d419cf18@redhat.com>

On 11/01/2016 03:53 PM, Bobrowski, Maciej wrote:
> Given a very simple class:
> 
> class Counter {
>   private int val = 0;
>   public int unsynchronizedGet() {
>     return val;
>   }
> 
>   public synchronized void increment() {
>     val++;
>   }
> }
> 
> If I have one thread periodically calling increment() on an instance of
> Counter, and the other thread periodically reading the value (using
> unsynchronized get), my question is: are there any guarantees about what
> value the unsynchronizedGet returns? Can it be e.g. 0 for a first few
> reads, then e.g. 20 (because the new value was loaded from memory)? Or
> will it either always be 0 (cached in a register) or will it always keep
> on increasing (being read from memory)?

This is the data race. With the naked example as above, JMM gives
(almost) no promises about the values you can read. Your explanation
hinges on the mental model of cache coherent memory, and once you hit
the hardware, most hardware would indeed work like you describe. But the
model itself is weaker, and if you do e.g.:

Counter cnt = ...;
int r1 = cnt.unsynchronizedGet();
int r2 = cnt.unsynchronizedGet();

...then model allows observing (r2 < r1).

Do not rely on data races!

More discussion:
https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-hb-actual

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/7dc50fda/attachment.sig>

From heinz at javaspecialists.eu  Tue Nov  1 14:08:17 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 20:08:17 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CA+kOe08H__b1XZOp-HoXqTfh7mXVdMMo9XMi6=EsHnvAMJeWcA@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <CA+kOe08H__b1XZOp-HoXqTfh7mXVdMMo9XMi6=EsHnvAMJeWcA@mail.gmail.com>
Message-ID: <5818DA11.5070308@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/60755796/attachment.html>

From shade at redhat.com  Tue Nov  1 14:14:52 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Tue, 1 Nov 2016 19:14:52 +0100
Subject: [concurrency-interest] StampedLock happens-before with
 tryOptimisticRead()
In-Reply-To: <581887DF.80501@javaspecialists.eu>
References: <581887DF.80501@javaspecialists.eu>
Message-ID: <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>

On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
> In my first version of this class, I made size volatile, but Alex Snaps
> suggested that I could simply call tryOptimisticRead() before reading
> the size field.  Since inside tryOptimisticRead() we are doing a
> volatile read of "state", I believe that we are correct with the code
> below. 

Trick question: can a correct (yet not very efficient) implementation of
StampedLock.tryOptimisticRead() simply return zero (e.g. "lock is busy,
no optimistic reads for you, go away")? If so, the whole argument goes
down the sink, as there are obviously no memory effects whatsoever, and
this is a data race.

I do think that a correct size() method should validate the stamp, e.g.:

public int size() {
  long stamp = sl.tryOptimisticRead();
  int size = this.size;
  if (sl.validate(stamp)) {
    return size;
  } else {
    ... go full read lock ...
  }
}

...which plays nicely into suggested StampedLock use patterns. If
StampedLock wanted to advertise memory consistency properties for
optimistic reads, it would have to talk about stamp validation, IMO --
because otherwise it has to mandate the particular implementation of
never-a-noop tryOptimisticRead().

Having that in mind, peeking into StampedLock implementation is cheating
;) This does not rely on specification, but only on the actual
implementation -- which is fragile, as we all know.

> However, there were some objections to this code on Twitter
> ""Volatile read == synchronized" will keep biting us forever. The
> comment is broken."
> https://twitter.com/VladimirSitnikv/status/793364258658607104 "

Well... kinda. As far as the memory effects are concerned, volatile and
synchronized are pretty symmetrical. But, synchronized also gives you
mutual exclusion, which makes it different enough to break "==".

How many times you have seen people blindly removing synchronized for
piggybacking on volatiles, and then realizing their mutex guarantees are
gone? That's usually fun to watch. If you are doing the educational
example, I would try to not to perpetuate this easy to make mistake.

> I based my comment on JCiP and the JMM FAQ, so I think it is
> correct. There was a follow-on comment that there was no
> happens-before guarantee with tryOptimisticRead(), but IMHO there
> would have to be, otherwise it would be possible to read stale fields
> into local variables. StampedLock (and similar constructs) would be 
> fundamentally broken if this does not work.

This is why you *need* to call validate(stamp) -- in the case of no-op
tryOptimisticRead, it would tell you have read garbage. Coding to the
actual implementation, not to the spec, would always be fundamentally
broken.

Now, to the original question: does volatile read of SL.state suffices
to provide happens-before-s? Yes, it does, by accident.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/63af04f7/attachment.sig>

From heinz at javaspecialists.eu  Tue Nov  1 14:31:58 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 20:31:58 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
Message-ID: <5818DF9E.3010602@javaspecialists.eu>

Aleksey Shipilev wrote:
> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
>
> Now, to the original question: does volatile read of SL.state suffices
> to provide happens-before-s? Yes, it does, by accident.
>
> Thanks,
> -Aleksey
Phew, glad I'm not mad :-)

And yes, you are 100% correct.  I did peep into the StampedLock 
implementation and used that to make my decision.

From shade at redhat.com  Tue Nov  1 15:18:03 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Tue, 1 Nov 2016 20:18:03 +0100
Subject: [concurrency-interest] StampedLock happens-before with
 tryOptimisticRead()
In-Reply-To: <5818DF9E.3010602@javaspecialists.eu>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
Message-ID: <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>

On 11/01/2016 07:31 PM, Dr Heinz M. Kabutz wrote:
> Aleksey Shipilev wrote:
>> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
>>
>> Now, to the original question: does volatile read of SL.state suffices
>> to provide happens-before-s? Yes, it does, by accident.
>>
>> Thanks,
>> -Aleksey
> Phew, glad I'm not mad :-)

Teaching people to peek into the implementation and making the
correctness arguments based on the implementation alone *is* mad in my
book ;)

But I really should expand. Should read:

-Yes, it does, by accident.
+Yes, it does, sometimes, by accident.

The trouble with the "happens-before" reasoning here is that you can
certainly argue there are executions where SL.unlock() --hb-->
SL.tryOptimisticRead(), but you *also* have to prove there are no other
interesting executions where the read of some $size *in progress* is racy.

The deal with optimistic reads is that they *are* in some linear order
with the rest of the updates under the lock. stamps provide that linear
versioning, and validate(stamp) enforces that you have actually seen the
"latest" version of the state protected by the lock, and advises you to
ignore the garbage you read, if you got in the unlucky spot.

Motivating example requires a stupidly excessive thing during the add():
not the "size++", but "size += 2; size--;" plus hand-wrist compiler into
not collapsing the updates" [1]. Then, this happens:

IntList il = ..;

Thread 1:
  il.add(1); // happens under write lock

Thread 2:
  // Naked half-optimisic read; we don't validate the stamp, and
  // so return with untrusted $size in hands, and it can be the write
  // in progress...
  int r1 = il.size(); // reads 2; <-- WTF, a transient result?
  int r2 = il.size(); // reads 1;

Notice that formally the second read can be justified by the execution
where "volatile write in SL.writeLock()" --sw/hb--> "volatile read in
tryOptimisticRead()". But the first read is completely racy!

Notice two things:
 a) this would not happen if size() validated the stamp. It would then
figure the update under writeLock is happening, and dealt with it
accordingly;
 b) volatile size would not help either -- so much for "volatile ==
synchronized" again, mutual exclusion is not something to ignore;

The goal for a good lock is to provide mutual exclusion with readers, so
that no transient result is ever visible. In other words, you want a
linearizable primitive...

Thanks,
-Aleksey

[1] http://cr.openjdk.java.net/~shade/scratch/UnwarrantedOptimismRead.java

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/349863c9/attachment.sig>

From heinz at javaspecialists.eu  Tue Nov  1 16:01:07 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 22:01:07 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
Message-ID: <5818F483.8030707@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/9a3e2ef3/attachment-0001.html>

From alex.snaps at gmail.com  Tue Nov  1 16:02:55 2016
From: alex.snaps at gmail.com (Alex Snaps)
Date: Tue, 01 Nov 2016 20:02:55 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
Message-ID: <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>

Actually there is somewhat of a "similar" race in this current
implementation, if I'm not mistaken:
Since we increment size prior to inserting the entry in the backing array,
it could well be we observe inconsistent state of `IntList`.
If for some reason size gets incremented and say the mutative thread gets
unscheduled "for longer", the entry wouldn't yet be installed, and it
wouldn't be "get"able (as the SL protects that ok, yet size() would reflect
the "to be size" once the mutation eventually completes). I guess this
would be fixable by incrementing size only after the element was
inserted...


On Tue, Nov 1, 2016 at 3:22 PM Aleksey Shipilev <shade at redhat.com> wrote:

> On 11/01/2016 07:31 PM, Dr Heinz M. Kabutz wrote:
> > Aleksey Shipilev wrote:
> >> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
> >>
> >> Now, to the original question: does volatile read of SL.state suffices
> >> to provide happens-before-s? Yes, it does, by accident.
> >>
> >> Thanks,
> >> -Aleksey
> > Phew, glad I'm not mad :-)
>
> Teaching people to peek into the implementation and making the
> correctness arguments based on the implementation alone *is* mad in my
> book ;)
>
> But I really should expand. Should read:
>
> -Yes, it does, by accident.
> +Yes, it does, sometimes, by accident.
>
> The trouble with the "happens-before" reasoning here is that you can
> certainly argue there are executions where SL.unlock() --hb-->
> SL.tryOptimisticRead(), but you *also* have to prove there are no other
> interesting executions where the read of some $size *in progress* is racy.
>
> The deal with optimistic reads is that they *are* in some linear order
> with the rest of the updates under the lock. stamps provide that linear
> versioning, and validate(stamp) enforces that you have actually seen the
> "latest" version of the state protected by the lock, and advises you to
> ignore the garbage you read, if you got in the unlucky spot.
>
> Motivating example requires a stupidly excessive thing during the add():
> not the "size++", but "size += 2; size--;" plus hand-wrist compiler into
> not collapsing the updates" [1]. Then, this happens:
>
> IntList il = ..;
>
> Thread 1:
>   il.add(1); // happens under write lock
>
> Thread 2:
>   // Naked half-optimisic read; we don't validate the stamp, and
>   // so return with untrusted $size in hands, and it can be the write
>   // in progress...
>   int r1 = il.size(); // reads 2; <-- WTF, a transient result?
>   int r2 = il.size(); // reads 1;
>
> Notice that formally the second read can be justified by the execution
> where "volatile write in SL.writeLock()" --sw/hb--> "volatile read in
> tryOptimisticRead()". But the first read is completely racy!
>
> Notice two things:
>  a) this would not happen if size() validated the stamp. It would then
> figure the update under writeLock is happening, and dealt with it
> accordingly;
>  b) volatile size would not help either -- so much for "volatile ==
> synchronized" again, mutual exclusion is not something to ignore;
>
> The goal for a good lock is to provide mutual exclusion with readers, so
> that no transient result is ever visible. In other words, you want a
> linearizable primitive...
>
> Thanks,
> -Aleksey
>
> [1] http://cr.openjdk.java.net/~shade/scratch/UnwarrantedOptimismRead.java
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-- 
Alex Snaps
Twitter: @alexsnaps
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/877ecde1/attachment.html>

From heinz at javaspecialists.eu  Tue Nov  1 16:29:25 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 22:29:25 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
Message-ID: <5818FB25.4090200@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/fedfe975/attachment.html>

From heinz at javaspecialists.eu  Tue Nov  1 16:32:21 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Tue, 01 Nov 2016 22:32:21 +0200
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <5818FB25.4090200@javaspecialists.eu>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
 <5818FB25.4090200@javaspecialists.eu>
Message-ID: <5818FBD5.4090107@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/78ddad2a/attachment-0001.html>

From oleksandr.otenko at gmail.com  Tue Nov  1 16:34:24 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Tue, 1 Nov 2016 20:34:24 +0000
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
Message-ID: <CAB2D15D-4369-4B5F-BD66-2FC016C200D6@gmail.com>

You need a definition of a “consistent” size().

Like, “if the list only grows, then a value at location size()-1 can always be read”. If so, then size() must have the validate() call, etc, like in get().

Alex

> On 1 Nov 2016, at 20:02, Alex Snaps <alex.snaps at gmail.com> wrote:
> 
> Actually there is somewhat of a "similar" race in this current implementation, if I'm not mistaken:
> Since we increment size prior to inserting the entry in the backing array, it could well be we observe inconsistent state of `IntList`.
> If for some reason size gets incremented and say the mutative thread gets unscheduled "for longer", the entry wouldn't yet be installed, and it wouldn't be "get"able (as the SL protects that ok, yet size() would reflect the "to be size" once the mutation eventually completes). I guess this would be fixable by incrementing size only after the element was inserted... 
> 
> 
> On Tue, Nov 1, 2016 at 3:22 PM Aleksey Shipilev <shade at redhat.com <mailto:shade at redhat.com>> wrote:
> On 11/01/2016 07:31 PM, Dr Heinz M. Kabutz wrote:
> > Aleksey Shipilev wrote:
> >> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
> >>
> >> Now, to the original question: does volatile read of SL.state suffices
> >> to provide happens-before-s? Yes, it does, by accident.
> >>
> >> Thanks,
> >> -Aleksey
> > Phew, glad I'm not mad :-)
> 
> Teaching people to peek into the implementation and making the
> correctness arguments based on the implementation alone *is* mad in my
> book ;)
> 
> But I really should expand. Should read:
> 
> -Yes, it does, by accident.
> +Yes, it does, sometimes, by accident.
> 
> The trouble with the "happens-before" reasoning here is that you can
> certainly argue there are executions where SL.unlock() --hb-->
> SL.tryOptimisticRead(), but you *also* have to prove there are no other
> interesting executions where the read of some $size *in progress* is racy.
> 
> The deal with optimistic reads is that they *are* in some linear order
> with the rest of the updates under the lock. stamps provide that linear
> versioning, and validate(stamp) enforces that you have actually seen the
> "latest" version of the state protected by the lock, and advises you to
> ignore the garbage you read, if you got in the unlucky spot.
> 
> Motivating example requires a stupidly excessive thing during the add():
> not the "size++", but "size += 2; size--;" plus hand-wrist compiler into
> not collapsing the updates" [1]. Then, this happens:
> 
> IntList il = ..;
> 
> Thread 1:
>   il.add(1); // happens under write lock
> 
> Thread 2:
>   // Naked half-optimisic read; we don't validate the stamp, and
>   // so return with untrusted $size in hands, and it can be the write
>   // in progress...
>   int r1 = il.size(); // reads 2; <-- WTF, a transient result?
>   int r2 = il.size(); // reads 1;
> 
> Notice that formally the second read can be justified by the execution
> where "volatile write in SL.writeLock()" --sw/hb--> "volatile read in
> tryOptimisticRead()". But the first read is completely racy!
> 
> Notice two things:
>  a) this would not happen if size() validated the stamp. It would then
> figure the update under writeLock is happening, and dealt with it
> accordingly;
>  b) volatile size would not help either -- so much for "volatile ==
> synchronized" again, mutual exclusion is not something to ignore;
> 
> The goal for a good lock is to provide mutual exclusion with readers, so
> that no transient result is ever visible. In other words, you want a
> linearizable primitive...
> 
> Thanks,
> -Aleksey
> 
> [1] http://cr.openjdk.java.net/~shade/scratch/UnwarrantedOptimismRead.java <http://cr.openjdk.java.net/~shade/scratch/UnwarrantedOptimismRead.java>
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> -- 
> Alex Snaps
> Twitter: @alexsnaps
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/aac86f61/attachment.html>

From jh at squareup.com  Tue Nov  1 16:35:10 2016
From: jh at squareup.com (Josh Humphries)
Date: Tue, 1 Nov 2016 16:35:10 -0400
Subject: [concurrency-interest] StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
Message-ID: <CAHJZN-uSZmq2RG-hNsvj4OU5A3KwdjjOvzh1ZqXJq-+yuc+g8w@mail.gmail.com>

Surely the method that fetches the element uses the full optimistic read ->
validate -> maybe pessimistic read lock idiom. If so, a call to get() after
observing the increased size will either successfully see the new element
(writer concurrently finished and optimistic read succeeds) or block until
the writer finishes (optimistic read fails).

So the fetch will successfully yield the element whose addition was "in
flight" during the call to size().

This of course doesn't work if there are threads concurrently removing
elements, but that has nothing to do with size() not validating the
optimistic read but is instead due to size() and get() not being atomic.


----
*Josh Humphries*
Payments Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Tue, Nov 1, 2016 at 4:02 PM, Alex Snaps <alex.snaps at gmail.com> wrote:

> Actually there is somewhat of a "similar" race in this current
> implementation, if I'm not mistaken:
> Since we increment size prior to inserting the entry in the backing array,
> it could well be we observe inconsistent state of `IntList`.
> If for some reason size gets incremented and say the mutative thread gets
> unscheduled "for longer", the entry wouldn't yet be installed, and it
> wouldn't be "get"able (as the SL protects that ok, yet size() would reflect
> the "to be size" once the mutation eventually completes). I guess this
> would be fixable by incrementing size only after the element was
> inserted...
>
>
> On Tue, Nov 1, 2016 at 3:22 PM Aleksey Shipilev <shade at redhat.com> wrote:
>
>> On 11/01/2016 07:31 PM, Dr Heinz M. Kabutz wrote:
>> > Aleksey Shipilev wrote:
>> >> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
>> >>
>> >> Now, to the original question: does volatile read of SL.state suffices
>> >> to provide happens-before-s? Yes, it does, by accident.
>> >>
>> >> Thanks,
>> >> -Aleksey
>> > Phew, glad I'm not mad :-)
>>
>> Teaching people to peek into the implementation and making the
>> correctness arguments based on the implementation alone *is* mad in my
>> book ;)
>>
>> But I really should expand. Should read:
>>
>> -Yes, it does, by accident.
>> +Yes, it does, sometimes, by accident.
>>
>> The trouble with the "happens-before" reasoning here is that you can
>> certainly argue there are executions where SL.unlock() --hb-->
>> SL.tryOptimisticRead(), but you *also* have to prove there are no other
>> interesting executions where the read of some $size *in progress* is racy.
>>
>> The deal with optimistic reads is that they *are* in some linear order
>> with the rest of the updates under the lock. stamps provide that linear
>> versioning, and validate(stamp) enforces that you have actually seen the
>> "latest" version of the state protected by the lock, and advises you to
>> ignore the garbage you read, if you got in the unlucky spot.
>>
>> Motivating example requires a stupidly excessive thing during the add():
>> not the "size++", but "size += 2; size--;" plus hand-wrist compiler into
>> not collapsing the updates" [1]. Then, this happens:
>>
>> IntList il = ..;
>>
>> Thread 1:
>>   il.add(1); // happens under write lock
>>
>> Thread 2:
>>   // Naked half-optimisic read; we don't validate the stamp, and
>>   // so return with untrusted $size in hands, and it can be the write
>>   // in progress...
>>   int r1 = il.size(); // reads 2; <-- WTF, a transient result?
>>   int r2 = il.size(); // reads 1;
>>
>> Notice that formally the second read can be justified by the execution
>> where "volatile write in SL.writeLock()" --sw/hb--> "volatile read in
>> tryOptimisticRead()". But the first read is completely racy!
>>
>> Notice two things:
>>  a) this would not happen if size() validated the stamp. It would then
>> figure the update under writeLock is happening, and dealt with it
>> accordingly;
>>  b) volatile size would not help either -- so much for "volatile ==
>> synchronized" again, mutual exclusion is not something to ignore;
>>
>> The goal for a good lock is to provide mutual exclusion with readers, so
>> that no transient result is ever visible. In other words, you want a
>> linearizable primitive...
>>
>> Thanks,
>> -Aleksey
>>
>> [1] http://cr.openjdk.java.net/~shade/scratch/
>> UnwarrantedOptimismRead.java
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> --
> Alex Snaps
> Twitter: @alexsnaps
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/5ef2cbd2/attachment-0001.html>

From alex.snaps at gmail.com  Tue Nov  1 16:54:50 2016
From: alex.snaps at gmail.com (Alex Snaps)
Date: Tue, 01 Nov 2016 20:54:50 +0000
Subject: [concurrency-interest] Fwd: StampedLock happens-before with
	tryOptimisticRead()
In-Reply-To: <CAKux6pb_tV9jbDdJMZjcgpCz9FcukegUec_ZU2tRerLe2xw2aA@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
 <CAHJZN-uSZmq2RG-hNsvj4OU5A3KwdjjOvzh1ZqXJq-+yuc+g8w@mail.gmail.com>
 <CAKux6pb_tV9jbDdJMZjcgpCz9FcukegUec_ZU2tRerLe2xw2aA@mail.gmail.com>
Message-ID: <CAKux6pbnmh0vNNY-yj2aOq=Qi6ZaCKkKUro0kGGQzVGvUTS7xQ@mail.gmail.com>

That was meant to be a reply all!

---------- Forwarded message ---------
From: Alex Snaps <alex.snaps at gmail.com>
Date: Tue, Nov 1, 2016 at 4:54 PM
Subject: Re: [concurrency-interest] StampedLock happens-before with
tryOptimisticRead()
To: Josh Humphries <jh at squareup.com>


Yes. Actually you are right, if the get doesn't settle on the optimistic
read but upgrades to pessimistic read it'd block. My bad!

On Tue, Nov 1, 2016 at 4:35 PM Josh Humphries <jh at squareup.com> wrote:

Surely the method that fetches the element uses the full optimistic read ->
validate -> maybe pessimistic read lock idiom. If so, a call to get() after
observing the increased size will either successfully see the new element
(writer concurrently finished and optimistic read succeeds) or block until
the writer finishes (optimistic read fails).

So the fetch will successfully yield the element whose addition was "in
flight" during the call to size().

This of course doesn't work if there are threads concurrently removing
elements, but that has nothing to do with size() not validating the
optimistic read but is instead due to size() and get() not being atomic.


----
*Josh Humphries*
Payments Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Tue, Nov 1, 2016 at 4:02 PM, Alex Snaps <alex.snaps at gmail.com> wrote:

Actually there is somewhat of a "similar" race in this current
implementation, if I'm not mistaken:
Since we increment size prior to inserting the entry in the backing array,
it could well be we observe inconsistent state of `IntList`.
If for some reason size gets incremented and say the mutative thread gets
unscheduled "for longer", the entry wouldn't yet be installed, and it
wouldn't be "get"able (as the SL protects that ok, yet size() would reflect
the "to be size" once the mutation eventually completes). I guess this
would be fixable by incrementing size only after the element was
inserted...


On Tue, Nov 1, 2016 at 3:22 PM Aleksey Shipilev <shade at redhat.com> wrote:

On 11/01/2016 07:31 PM, Dr Heinz M. Kabutz wrote:
> Aleksey Shipilev wrote:
>> On 11/01/2016 01:17 PM, Dr Heinz M. Kabutz wrote:
>>
>> Now, to the original question: does volatile read of SL.state suffices
>> to provide happens-before-s? Yes, it does, by accident.
>>
>> Thanks,
>> -Aleksey
> Phew, glad I'm not mad :-)

Teaching people to peek into the implementation and making the
correctness arguments based on the implementation alone *is* mad in my
book ;)

But I really should expand. Should read:

-Yes, it does, by accident.
+Yes, it does, sometimes, by accident.

The trouble with the "happens-before" reasoning here is that you can
certainly argue there are executions where SL.unlock() --hb-->
SL.tryOptimisticRead(), but you *also* have to prove there are no other
interesting executions where the read of some $size *in progress* is racy.

The deal with optimistic reads is that they *are* in some linear order
with the rest of the updates under the lock. stamps provide that linear
versioning, and validate(stamp) enforces that you have actually seen the
"latest" version of the state protected by the lock, and advises you to
ignore the garbage you read, if you got in the unlucky spot.

Motivating example requires a stupidly excessive thing during the add():
not the "size++", but "size += 2; size--;" plus hand-wrist compiler into
not collapsing the updates" [1]. Then, this happens:

IntList il = ..;

Thread 1:
  il.add(1); // happens under write lock

Thread 2:
  // Naked half-optimisic read; we don't validate the stamp, and
  // so return with untrusted $size in hands, and it can be the write
  // in progress...
  int r1 = il.size(); // reads 2; <-- WTF, a transient result?
  int r2 = il.size(); // reads 1;

Notice that formally the second read can be justified by the execution
where "volatile write in SL.writeLock()" --sw/hb--> "volatile read in
tryOptimisticRead()". But the first read is completely racy!

Notice two things:
 a) this would not happen if size() validated the stamp. It would then
figure the update under writeLock is happening, and dealt with it
accordingly;
 b) volatile size would not help either -- so much for "volatile ==
synchronized" again, mutual exclusion is not something to ignore;

The goal for a good lock is to provide mutual exclusion with readers, so
that no transient result is ever visible. In other words, you want a
linearizable primitive...

Thanks,
-Aleksey

[1] http://cr.openjdk.java.net/~shade/scratch/UnwarrantedOptimismRead.java

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Alex Snaps
Twitter: @alexsnaps

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-- 
Alex Snaps
Twitter: @alexsnaps
-- 
Alex Snaps
Twitter: @alexsnaps
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161101/27be481f/attachment.html>

From heinz at javaspecialists.eu  Wed Nov  2 04:28:33 2016
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 02 Nov 2016 10:28:33 +0200
Subject: [concurrency-interest] Fwd: StampedLock happens-before
	with	tryOptimisticRead()
In-Reply-To: <CAKux6pbnmh0vNNY-yj2aOq=Qi6ZaCKkKUro0kGGQzVGvUTS7xQ@mail.gmail.com>
References: <581887DF.80501@javaspecialists.eu>
 <78a6d479-07ae-2364-a6ea-b158195c7c50@redhat.com>
 <5818DF9E.3010602@javaspecialists.eu>
 <6527da69-278d-27c3-62fd-d9e4b20e5ed6@redhat.com>
 <CAKux6pbRJ5+WV+JUE2WtXEiNfKV9U1w1tnEyAJaB793_5+FAoA@mail.gmail.com>
 <CAHJZN-uSZmq2RG-hNsvj4OU5A3KwdjjOvzh1ZqXJq-+yuc+g8w@mail.gmail.com>
 <CAKux6pb_tV9jbDdJMZjcgpCz9FcukegUec_ZU2tRerLe2xw2aA@mail.gmail.com>
 <CAKux6pbnmh0vNNY-yj2aOq=Qi6ZaCKkKUro0kGGQzVGvUTS7xQ@mail.gmail.com>
Message-ID: <5819A3B1.9020609@javaspecialists.eu>

An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161102/26885d63/attachment-0001.html>

From aph at redhat.com  Wed Nov  9 05:03:14 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 9 Nov 2016 10:03:14 +0000
Subject: [concurrency-interest] Does StampedLock need a releaseFence in
 theory?
In-Reply-To: <CAPUmR1bfDEG68S-Kcs+DtZB-khQ0WKoYnaG5ZRjGP=oXe9jMYg@mail.gmail.com>
References: <CA+kOe0-yucV7zTgb6hWythLg0Qg=L+sLHXDDj2jxvFm=v1gUEQ@mail.gmail.com>
 <CAPUmR1bfDEG68S-Kcs+DtZB-khQ0WKoYnaG5ZRjGP=oXe9jMYg@mail.gmail.com>
Message-ID: <27e9226d-55fb-38e3-1a16-77671abc46c1@redhat.com>

On 14/07/16 01:53, Hans Boehm wrote:
> An ARMv8 compareAndSet operation (using only acquire and release
> operations, not dmb, as it should be implemented) will behave like the
> lock-based one in this respect. 

Following up after a long delay:

What exactly do you mean by "as it should be implemented" here?  is it
that the implementation of compareAndSet in Java should not use
ldaxr; cmp; stlxr ?  Is the mapping in
https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html incorrect?

Or are you only referring to StampedLock?

Andrew.

From boehm at acm.org  Wed Nov  9 21:15:24 2016
From: boehm at acm.org (Hans Boehm)
Date: Wed, 9 Nov 2016 18:15:24 -0800
Subject: [concurrency-interest] Does StampedLock need a releaseFence in
	theory?
In-Reply-To: <27e9226d-55fb-38e3-1a16-77671abc46c1@redhat.com>
References: <CA+kOe0-yucV7zTgb6hWythLg0Qg=L+sLHXDDj2jxvFm=v1gUEQ@mail.gmail.com>
 <CAPUmR1bfDEG68S-Kcs+DtZB-khQ0WKoYnaG5ZRjGP=oXe9jMYg@mail.gmail.com>
 <27e9226d-55fb-38e3-1a16-77671abc46c1@redhat.com>
Message-ID: <CAPUmR1a2ZvTfQBVaP0SokND9Gbm5L47eNran3QYn_APjf7VOAA@mail.gmail.com>

Sorry. My message was syntactically ambiguous. I believe compareAndSet
should be implemented using acquire/release operations on ARMv8. It should
not be implemented using dmb. The mappings on the Cambridge page for
Aarch64 look correct to me.

On Wed, Nov 9, 2016 at 2:03 AM, Andrew Haley <aph at redhat.com> wrote:

> On 14/07/16 01:53, Hans Boehm wrote:
> > An ARMv8 compareAndSet operation (using only acquire and release
> > operations, not dmb, as it should be implemented) will behave like the
> > lock-based one in this respect.
>
> Following up after a long delay:
>
> What exactly do you mean by "as it should be implemented" here?  is it
> that the implementation of compareAndSet in Java should not use
> ldaxr; cmp; stlxr ?  Is the mapping in
> https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html incorrect?
>
> Or are you only referring to StampedLock?
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161109/406d1e67/attachment.html>

From maire.anthony at gmail.com  Tue Nov 15 09:34:02 2016
From: maire.anthony at gmail.com (Anthony Maire)
Date: Tue, 15 Nov 2016 15:34:02 +0100
Subject: [concurrency-interest] LockSupport park/unpark and memory visibility
Message-ID: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>

Hi
Sorry if this has been already asked, but I didn't succeed to find a clear
answer
I'm wondering on how memory consistency can be achieved when using
LockSupport park() and unpark() methods
As far as I know, there is no explicit guarantee either in LS javadoc or in
the JLS, so I assume that the condition to be check should come with its
own visibility mechanism since we don't have any HB relationship.


Let's consider the following idiom, "ready" being a volatile boolean field:
- threadA: while(!ready){ LockSupport.park(this); }
- threadB: ready = true; LockSupport.unpark(threadA);

What does make this example work ? Is it possible for threadA to un²park,
check the "ready" field without seeing threadB's store and park again ? If
I understand the JMM correctly I would say that it is a valid execution
order.
On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile
store force the store buffer to drain so that threadA will always see the
store when the permit is made available by threadB, but is there any
guarantee that is not implementation or architecture dependent ?

Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
instead of volatile read/write ? I assume that it should be correct on x86
+ openJDK, but is there any strong guarantee on this ?

Kind regards.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161115/fca9a015/attachment.html>

From henri.tremblay at gmail.com  Tue Nov 15 14:45:08 2016
From: henri.tremblay at gmail.com (Henri Tremblay)
Date: Tue, 15 Nov 2016 14:45:08 -0500
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
Message-ID: <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>

I'm not a subject matter expert on this but:

Yes. It works if "ready" is volatile. Because for any implementation, the
JMM says a write to a volatile field can be seen by all threads.

An AtomicBoolean would also work on a set().

Two things I don't know:
1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
after threadA check its value and is back to park
2- Do "ready" need to be volatile? Or is pack/unpark providing some kind of
memory fence.

Henri


On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com> wrote:

> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161115/2433358d/attachment.html>

From vitalyd at gmail.com  Tue Nov 15 14:55:13 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 15 Nov 2016 19:55:13 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
Message-ID: <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>

The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
Anthony's example. That would imply the idiom is intended to work.

LS on its own isn't spec'd to provide any happens-before/ordering, so user
needs to provide that themselves. I recall Dave Dice mentioning that a good
litmus test for proper LS usage is to pretend park/unpark are empty methods
that degenerate into spinning.

On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
wrote:

> I'm not a subject matter expert on this but:
>
> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>
> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161115/09f000dc/attachment.html>

From concurrency-interest at tomlee.co  Tue Nov 15 15:23:29 2016
From: concurrency-interest at tomlee.co (Tom Lee)
Date: Tue, 15 Nov 2016 12:23:29 -0800
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
Message-ID: <CAKwFPQ9EO062Z098xJ8pFkPFEC8ZuzWLKZWFi45kUiHzg87Bbw@mail.gmail.com>

On Tue, Nov 15, 2016 at 11:45 AM, Henri Tremblay <henri.tremblay at gmail.com>
wrote:

> I'm not a subject matter expert on this but:
>

Nor am I, so forgive any lies that follow. :)


> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
>

To the best of my knowledge, lazySet on x86 is roughly equivalent to

*mov [dest], src*

(i.e. write to memory location *dest *the value from some register *src*)

That write eventually makes its way into the executing CPU's store
buffer. *Eventually
*that write will become visible as the store buffer flushes out, but
exactly *when* may not be entirely deterministic.

"Normal" volatile writes (e.g. writing to a volatile instance variable, or
calling AtomicBoolean.set()) will add an additional instruction after the
*mov* (iirc usually a *lock xchg*) that forces the CPU to wait until the
store buffer has completely flushed before proceeding. So you "know" that
the value written will be visible to all CPUs performing a read before
anything else happens on the CPU doing the write. Put another way, the *lock
xchg* enforces a *happens-before *relationship on a CPU.


> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>

I don't believe unpark is a fence, but I could be wrong -- I'll leave that
to others. On the assumption that it isn't, either ready needs to be
volatile or it needs to be an AtomicBoolean written with set() (N.B.
lazySet is not what you want, because the store buffer may not flush before
you unpark threadA & it will go right back to sleep again).

And of course, all this ignores JIT/compiler reordering, and other
archetectures may be more or less strict with respect to lazySet. To be
safe, I imagine you'd typically want to combine lazySets with volatile
writes to give yourself some better guarantees. Personally I've never had
cause to worry about that sort of thing, but it's fun to think about how it
all works.

Hope all this makes sense and/or is free from gross misinformation. :)

Cheers,
Tom


> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a
>> clear answer
>> I'm wondering on how memory consistency can be achieved when using
>> LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or
>> in the JLS, so I assume that the condition to be check should come with its
>> own visibility mechanism since we don't have any HB relationship.
>>
>>
>> Let's consider the following idiom, "ready" being a volatile boolean
>> field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>>
>> What does make this example work ? Is it possible for threadA to un²park,
>> check the "ready" field without seeing threadB's store and park again ? If
>> I understand the JMM correctly I would say that it is a valid execution
>> order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>> volatile store force the store buffer to drain so that threadA will always
>> see the store when the permit is made available by threadB, but is there
>> any guarantee that is not implementation or architecture dependent ?
>>
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>> instead of volatile read/write ? I assume that it should be correct on x86
>> + openJDK, but is there any strong guarantee on this ?
>>
>> Kind regards.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161115/3a7ea114/attachment-0001.html>

From vitalyd at gmail.com  Tue Nov 15 17:43:41 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 15 Nov 2016 17:43:41 -0500
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAKwFPQ9EO062Z098xJ8pFkPFEC8ZuzWLKZWFi45kUiHzg87Bbw@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAKwFPQ9EO062Z098xJ8pFkPFEC8ZuzWLKZWFi45kUiHzg87Bbw@mail.gmail.com>
Message-ID: <CAHjP37Grjb59LsHB050bLoXp7fw+vSNHPkKjs_8j9biZ8NRTxg@mail.gmail.com>

On Tue, Nov 15, 2016 at 3:23 PM, Tom Lee <concurrency-interest at tomlee.co>
wrote:

> On Tue, Nov 15, 2016 at 11:45 AM, Henri Tremblay <henri.tremblay at gmail.com
> > wrote:
>
>> I'm not a subject matter expert on this but:
>>
>
> Nor am I, so forgive any lies that follow. :)
>
>
>> Yes. It works if "ready" is volatile. Because for any implementation, the
>> JMM says a write to a volatile field can be seen by all threads.
>>
>> An AtomicBoolean would also work on a set().
>>
>> Two things I don't know:
>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set
>> ready after threadA check its value and is back to park
>>
>
> To the best of my knowledge, lazySet on x86 is roughly equivalent to
>
> *mov [dest], src*
>
> (i.e. write to memory location *dest *the value from some register *src*)
>
> That write eventually makes its way into the executing CPU's store buffer. *Eventually
> *that write will become visible as the store buffer flushes out, but
> exactly *when* may not be entirely deterministic.
>
> "Normal" volatile writes (e.g. writing to a volatile instance variable, or
> calling AtomicBoolean.set()) will add an additional instruction after the
> *mov* (iirc usually a *lock xchg*) that forces the CPU to wait until the
> store buffer has completely flushed before proceeding. So you "know" that
> the value written will be visible to all CPUs performing a read before
> anything else happens on the CPU doing the write. Put another way, the *lock
> xchg* enforces a *happens-before *relationship on a CPU.
>
Current (Hotspot in Java 8) incarnation does a lock'd add of 0 to %rsp.
Aleksey Shipilev did some study of various flavors of the fence:
https://shipilev.net/blog/2014/on-the-fence-with-dependencies/

And yes, for x86, lazySet is just a mov (+ compiler barrier preventing
prior stores from moving after the lazySet store).

>
>
>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
>> of memory fence.
>>
>
> I don't believe unpark is a fence, but I could be wrong -- I'll leave that
> to others. On the assumption that it isn't, either ready needs to be
> volatile or it needs to be an AtomicBoolean written with set() (N.B.
> lazySet is not what you want, because the store buffer may not flush before
> you unpark threadA & it will go right back to sleep again).
>
AFAICT, LockSupport park or unpark have no memory order definition.  As
mentioned in my earlier reply, Dave Dice said you should pretend
park/unpark are empty methods and then consider whether your
synchronization is still sound.  In Oracle/OpenJDK, the native parker impl
will have instructions causing fences, but that's an implementation detail.

With lazySet then, if you pretend that unpark() does a normal store, then
there's nothing preventing that store in unpark() from floating above the
store to 'ready'.

>
> And of course, all this ignores JIT/compiler reordering, and other
> archetectures may be more or less strict with respect to lazySet. To be
> safe, I imagine you'd typically want to combine lazySets with volatile
> writes to give yourself some better guarantees. Personally I've never had
> cause to worry about that sort of thing, but it's fun to think about how it
> all works.
>
> Hope all this makes sense and/or is free from gross misinformation. :)
>
> Cheers,
> Tom
>
>
>> Henri
>>
>>
>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>>> Hi
>>> Sorry if this has been already asked, but I didn't succeed to find a
>>> clear answer
>>> I'm wondering on how memory consistency can be achieved when using
>>> LockSupport park() and unpark() methods
>>> As far as I know, there is no explicit guarantee either in LS javadoc or
>>> in the JLS, so I assume that the condition to be check should come with its
>>> own visibility mechanism since we don't have any HB relationship.
>>>
>>>
>>> Let's consider the following idiom, "ready" being a volatile boolean
>>> field:
>>> - threadA: while(!ready){ LockSupport.park(this); }
>>> - threadB: ready = true; LockSupport.unpark(threadA);
>>>
>>> What does make this example work ? Is it possible for threadA to
>>> un²park, check the "ready" field without seeing threadB's store and park
>>> again ? If I understand the JMM correctly I would say that it is a valid
>>> execution order.
>>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>>> volatile store force the store buffer to drain so that threadA will always
>>> see the store when the permit is made available by threadB, but is there
>>> any guarantee that is not implementation or architecture dependent ?
>>>
>>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>>> instead of volatile read/write ? I assume that it should be correct on x86
>>> + openJDK, but is there any strong guarantee on this ?
>>>
>>> Kind regards.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161115/4d1dc711/attachment.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 02:51:27 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 07:51:27 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
Message-ID: <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>

Normally there will be more - you’d need to tell who is waiting. For illustrative purposes (not that this makes sense in the presence of multiple waiters):

threadA: if (!ready) {waiter = Thread.currentThread; while(!ready) {LockSupport.park();}}

threadB: ready = true; if (waiter != null) {waiter = null; LockSupport.unpark(w);}

The parts in bold need to be a Dekker idiom, otherwise you won’t wake up some waiters. So waiter and ready need to be volatile.

In the presence of multiple waiters waiter will be a list with thread-safe add, poll, remove, but the essence is the same - adding or removing a waiter to/from the list is at least a volatile-store.

Then the barriers inside park/unpark do not matter.


Alex


> On 15 Nov 2016, at 14:34, Anthony Maire <maire.anthony at gmail.com> wrote:
> 
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear answer
> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
> 
> 
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
> 
> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
> 
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
> 
> Kind regards.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/8f17de9b/attachment.html>

From maire.anthony at gmail.com  Wed Nov 16 05:18:53 2016
From: maire.anthony at gmail.com (Anthony Maire)
Date: Wed, 16 Nov 2016 11:18:53 +0100
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
Message-ID: <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>

Thank you for that first feedback

for the "the JMM says a write to a volatile field can be seen by all
threads." by Henri, the issue is not that the waiter will not see the
write, but that it may not see at the precise moment that it has been
unparked, so it is parked again and wait possibly forever. As far as I
understand it, the JMM is based on HB edges, so to make sure the write is
visible at that precise moment, there should be an HB edge between unpark
and park, which is not guaranteed
Of course, it will work because the volatile write implies waiting for the
store buffer to drain so that write will always be visible for threadA, but
it seems an implementation detail to me, so it is not something I want to
rely on.

For the FIFOMutex example, I agree that is intended to work (LS will be
pretty useless if it doesn't) and it works by experience. But I would like
to know why it is guaranteed to works so I can reason about LS easier :)

For the Dave Dice test, I assume that Vitaly is referring to this blog post
: https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
That's basically where my second question about lazySet comes from : it
will work with empty methods, but if we assume that park/unpark can have no
fences (since the current fences are an implementation detail) it seems
even more fragile


2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:

> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
> Anthony's example. That would imply the idiom is intended to work.
>
> LS on its own isn't spec'd to provide any happens-before/ordering, so user
> needs to provide that themselves. I recall Dave Dice mentioning that a good
> litmus test for proper LS usage is to pretend park/unpark are empty methods
> that degenerate into spinning.
>
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
>> I'm not a subject matter expert on this but:
>>
>> Yes. It works if "ready" is volatile. Because for any implementation, the
>> JMM says a write to a volatile field can be seen by all threads.
>>
>> An AtomicBoolean would also work on a set().
>>
>> Two things I don't know:
>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set
>> ready after threadA check its value and is back to park
>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
>> of memory fence.
>>
>> Henri
>>
>>
>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a
>> clear answer
>> I'm wondering on how memory consistency can be achieved when using
>> LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or
>> in the JLS, so I assume that the condition to be check should come with its
>> own visibility mechanism since we don't have any HB relationship.
>>
>>
>> Let's consider the following idiom, "ready" being a volatile boolean
>> field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>>
>> What does make this example work ? Is it possible for threadA to un²park,
>> check the "ready" field without seeing threadB's store and park again ? If
>> I understand the JMM correctly I would say that it is a valid execution
>> order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>> volatile store force the store buffer to drain so that threadA will always
>> see the store when the permit is made available by threadB, but is there
>> any guarantee that is not implementation or architecture dependent ?
>>
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>> instead of volatile read/write ? I assume that it should be correct on x86
>> + openJDK, but is there any strong guarantee on this ?
>>
>> Kind regards.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> --
> Sent from my phone
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/05447486/attachment-0001.html>

From maire.anthony at gmail.com  Wed Nov 16 05:27:42 2016
From: maire.anthony at gmail.com (Anthony Maire)
Date: Wed, 16 Nov 2016 11:27:42 +0100
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>
Message-ID: <CAJpyQ4pTJ0OLA6NKJmAd2C2Coxjs=_cX8hRpJ2NPbeOZy59zRA@mail.gmail.com>

In fact, the structure I'm working on is guaranteed to have at most a
single waiter so the full code is more like this one (although in the
current version I have, setting the waiter to null was done in the waiter
code after exiting the wait loop, but that is only temporary code until I
have more information on LS usage)

However, I don't understand how this guarantee that we have no possible
issue, even when the volatile write to the waiter field in threadB. This
write is not observed so it is not guaranteed to have any memory effect
according to
https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles
.

Could you please explain further ?

Regards,
Anthony

2016-11-16 8:51 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com>:

> Normally there will be more - you’d need to tell who is waiting. For
> illustrative purposes (not that this makes sense in the presence of
> multiple waiters):
>
> threadA: if (!ready) {*waiter = Thread.currentThread; while(!ready*)
> {LockSupport.park();}}
>
> threadB: *ready = true; if (waiter != null)* {waiter = null;
> LockSupport.unpark(w);}
>
> The parts in bold need to be a Dekker idiom, otherwise you won’t wake up
> some waiters. So waiter and ready need to be volatile.
>
> In the presence of multiple waiters waiter will be a list with thread-safe
> add, poll, remove, but the essence is the same - adding or removing a
> waiter to/from the list is at least a volatile-store.
>
> Then the barriers inside park/unpark do not matter.
>
>
> Alex
>
>
> On 15 Nov 2016, at 14:34, Anthony Maire <maire.anthony at gmail.com> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/572dbc87/attachment.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 07:17:06 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 12:17:06 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAJpyQ4pTJ0OLA6NKJmAd2C2Coxjs=_cX8hRpJ2NPbeOZy59zRA@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>
 <CAJpyQ4pTJ0OLA6NKJmAd2C2Coxjs=_cX8hRpJ2NPbeOZy59zRA@mail.gmail.com>
Message-ID: <A105CFD4-3E79-437C-BC81-EE15AA2A7E05@gmail.com>

There are several issues with that example. One of them is that even if the barriers were not eliminated, they could still be reordered with the reads and the writes to x and y - that code isn’t an example of correct synchronization even if the same Holder instance were used by both actors.

I am not sure what your question is about waiter store and load. The important aspect is that threadA must indicate that it decided to enter park, and for threadB to check that indicator - be it a volatile boolean flag or a volatile reference to a Thread. If your code isn’t doing that, you are likely losing out more by always calling park/unpark than you win by eliminating that volatile indicator, unless you want to bet on a particular JVM that perhaps reduces the cost of park/unpark to less than a volatile store/load.


Besides, after return from park you better check whether the Thread was interrupted. I think there are implementations that will not “park” until you checked (thus cleared) that flag, if it happens to be set - so the interrupted threadA will burn CPU by ignoring the interrupted flag check.


Alex

> On 16 Nov 2016, at 10:27, Anthony Maire <maire.anthony at gmail.com> wrote:
> 
> In fact, the structure I'm working on is guaranteed to have at most a single waiter so the full code is more like this one (although in the current version I have, setting the waiter to null was done in the waiter code after exiting the wait loop, but that is only temporary code until I have more information on LS usage)
> 
> However, I don't understand how this guarantee that we have no possible issue, even when the volatile write to the waiter field in threadB. This write is not observed so it is not guaranteed to have any memory effect according to https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles <https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles>.
> 
> Could you please explain further ?
> 
> Regards,
> Anthony
> 
> 2016-11-16 8:51 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>>:
> Normally there will be more - you’d need to tell who is waiting. For illustrative purposes (not that this makes sense in the presence of multiple waiters):
> 
> threadA: if (!ready) {waiter = Thread.currentThread; while(!ready) {LockSupport.park();}}
> 
> threadB: ready = true; if (waiter != null) {waiter = null; LockSupport.unpark(w);}
> 
> The parts in bold need to be a Dekker idiom, otherwise you won’t wake up some waiters. So waiter and ready need to be volatile.
> 
> In the presence of multiple waiters waiter will be a list with thread-safe add, poll, remove, but the essence is the same - adding or removing a waiter to/from the list is at least a volatile-store.
> 
> Then the barriers inside park/unpark do not matter.
> 
> 
> Alex
> 
> 
>> On 15 Nov 2016, at 14:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>> 
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a clear answer
>> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
>> 
>> 
>> Let's consider the following idiom, "ready" being a volatile boolean field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>> 
>> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
>> 
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
>> 
>> Kind regards.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/92dea4d3/attachment.html>

From vitalyd at gmail.com  Wed Nov 16 07:20:16 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 12:20:16 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
Message-ID: <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>

Yes, that's the blog entry I was referring to.

So I agree with you that LS ordering is underspecified. Specifically,
unpark() must have release semantics to preclude it from moving above the
volatile store. Otherwise, as you say, threadA can be unparked before the
volatile write is visible and then it'll go back to sleep indefinitely.

Note that a volatile write doesn't change this - if unpark() isn't ordered,
it can be moved above the volatile write (theoretically).

On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
wrote:

> Thank you for that first feedback
>
> for the "the JMM says a write to a volatile field can be seen by all
> threads." by Henri, the issue is not that the waiter will not see the
> write, but that it may not see at the precise moment that it has been
> unparked, so it is parked again and wait possibly forever. As far as I
> understand it, the JMM is based on HB edges, so to make sure the write is
> visible at that precise moment, there should be an HB edge between unpark
> and park, which is not guaranteed
> Of course, it will work because the volatile write implies waiting for the
> store buffer to drain so that write will always be visible for threadA, but
> it seems an implementation detail to me, so it is not something I want to
> rely on.
>
> For the FIFOMutex example, I agree that is intended to work (LS will be
> pretty useless if it doesn't) and it works by experience. But I would like
> to know why it is guaranteed to works so I can reason about LS easier :)
>
> For the Dave Dice test, I assume that Vitaly is referring to this blog
> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
> That's basically where my second question about lazySet comes from : it
> will work with empty methods, but if we assume that park/unpark can have no
> fences (since the current fences are an implementation detail) it seems
> even more fragile
>
>
> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>
> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
> Anthony's example. That would imply the idiom is intended to work.
>
> LS on its own isn't spec'd to provide any happens-before/ordering, so user
> needs to provide that themselves. I recall Dave Dice mentioning that a good
> litmus test for proper LS usage is to pretend park/unpark are empty methods
> that degenerate into spinning.
>
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
> I'm not a subject matter expert on this but:
>
> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>
> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Sent from my phone
>
>
> --
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/a1607836/attachment-0001.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 07:35:56 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 12:35:56 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
Message-ID: <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>

Not necessarily.

unpark can end up just checking a flag to see that the thread has not been parked after some preceding call to unpark. This can make it behave like a volatile load, which will have to be totally ordered with a store in park that clears that flag. So there may be no stores in a particular execution of unpark.

Alex

> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
> Yes, that's the blog entry I was referring to.
> 
> So I agree with you that LS ordering is underspecified.  Specifically, unpark() must have release semantics to preclude it from moving above the volatile store.  Otherwise, as you say, threadA can be unparked before the volatile write is visible and then it'll go back to sleep indefinitely.
> 
> Note that a volatile write doesn't change this - if unpark() isn't ordered, it can be moved above the volatile write (theoretically).
> 
> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
> Thank you for that first feedback
> 
> for the "the JMM says a write to a volatile field can be seen by all threads." by Henri, the issue is not that the waiter will not see the write, but that it may not see at the precise moment that it has been unparked, so it is parked again and wait possibly forever. As far as I understand it, the JMM is based on HB edges, so to make sure the write is visible at that precise moment, there should be an HB edge between unpark and park, which is not guaranteed
> Of course, it will work because the volatile write implies waiting for the store buffer to drain so that write will always be visible for threadA, but it seems an implementation detail to me, so it is not something I want to rely on.
> 
> For the FIFOMutex example, I agree that is intended to work (LS will be pretty useless if it doesn't) and it works by experience. But I would like to know why it is guaranteed to works so I can reason about LS easier :)
> 
> For the Dave Dice test, I assume that Vitaly is referring to this blog post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park <https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park>
> That's basically where my second question about lazySet comes from : it will work with empty methods, but if we assume that park/unpark can have no fences (since the current fences are an implementation detail) it seems even more fragile
> 
> 
> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>>:
> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to Anthony's example.  That would imply the idiom is intended to work.
> 
> LS on its own isn't spec'd to provide any happens-before/ordering, so user needs to provide that themselves.  I recall Dave Dice mentioning that a good litmus test for proper LS usage is to pretend park/unpark are empty methods that degenerate into spinning.
> 
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com <mailto:henri.tremblay at gmail.com>> wrote:
> I'm not a subject matter expert on this but:
> 
> Yes. It works if "ready" is volatile. Because for any implementation, the JMM says a write to a volatile field can be seen by all threads.
> 
> An AtomicBoolean would also work on a set().
> 
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind of memory fence.
> 
> Henri
> 
> 
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear answer
> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
> 
> 
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
> 
> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
> 
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
> 
> Kind regards.
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> -- 
> Sent from my phone
> 
> -- 
> Sent from my phone
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/25b5d138/attachment.html>

From vitalyd at gmail.com  Wed Nov 16 07:45:27 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 12:45:27 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
Message-ID: <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>

This would be a particular implementation detail. Since we're
hypothesizing, it's possible that unpark() can use an unordered read (i.e.
relaxed) as an optimization, and then we're back to same situation.

Either way though, this thread and previous ones on LS are indication that
more documentation is needed.

On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Not necessarily.
>
> unpark can end up just checking a flag to see that the thread has not been
> parked after some preceding call to unpark. This can make it behave like a
> volatile load, which will have to be totally ordered with a store in park
> that clears that flag. So there may be no stores in a particular execution
> of unpark.
>
>
> Alex
>
> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> Yes, that's the blog entry I was referring to.
>
> So I agree with you that LS ordering is underspecified. Specifically,
> unpark() must have release semantics to preclude it from moving above the
> volatile store. Otherwise, as you say, threadA can be unparked before the
> volatile write is visible and then it'll go back to sleep indefinitely.
>
> Note that a volatile write doesn't change this - if unpark() isn't
> ordered, it can be moved above the volatile write (theoretically).
>
> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Thank you for that first feedback
>
> for the "the JMM says a write to a volatile field can be seen by all
> threads." by Henri, the issue is not that the waiter will not see the
> write, but that it may not see at the precise moment that it has been
> unparked, so it is parked again and wait possibly forever. As far as I
> understand it, the JMM is based on HB edges, so to make sure the write is
> visible at that precise moment, there should be an HB edge between unpark
> and park, which is not guaranteed
> Of course, it will work because the volatile write implies waiting for the
> store buffer to drain so that write will always be visible for threadA, but
> it seems an implementation detail to me, so it is not something I want to
> rely on.
>
> For the FIFOMutex example, I agree that is intended to work (LS will be
> pretty useless if it doesn't) and it works by experience. But I would like
> to know why it is guaranteed to works so I can reason about LS easier :)
>
> For the Dave Dice test, I assume that Vitaly is referring to this blog
> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
> That's basically where my second question about lazySet comes from : it
> will work with empty methods, but if we assume that park/unpark can have no
> fences (since the current fences are an implementation detail) it seems
> even more fragile
>
>
> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>
> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
> Anthony's example. That would imply the idiom is intended to work.
>
> LS on its own isn't spec'd to provide any happens-before/ordering, so user
> needs to provide that themselves. I recall Dave Dice mentioning that a good
> litmus test for proper LS usage is to pretend park/unpark are empty methods
> that degenerate into spinning.
>
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
> I'm not a subject matter expert on this but:
>
> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>
> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Sent from my phone
>
>
> --
> Sent from my phone
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/89aa3084/attachment-0001.html>

From viktor.klang at gmail.com  Wed Nov 16 07:57:27 2016
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 16 Nov 2016 13:57:27 +0100
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
Message-ID: <CANPzfU_xCpEK=TOdL1eEmCYd5igTeZ001A4Rrt_cjCw8kJPeOw@mail.gmail.com>

Since we're talking about something which is un(der)-specced, what use is
it to speculate in what horrors could happen? ^^

The only way to make sure is to add the appropriate belt-and-suspenders
around it to make sure that there's something which provides the desired
semantics :)

On Wed, Nov 16, 2016 at 1:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> This would be a particular implementation detail. Since we're
> hypothesizing, it's possible that unpark() can use an unordered read (i.e.
> relaxed) as an optimization, and then we're back to same situation.
>
> Either way though, this thread and previous ones on LS are indication that
> more documentation is needed.
>
> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> Not necessarily.
>>
>> unpark can end up just checking a flag to see that the thread has not
>> been parked after some preceding call to unpark. This can make it behave
>> like a volatile load, which will have to be totally ordered with a store in
>> park that clears that flag. So there may be no stores in a particular
>> execution of unpark.
>>
>>
>> Alex
>>
>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>> Yes, that's the blog entry I was referring to.
>>
>> So I agree with you that LS ordering is underspecified. Specifically,
>> unpark() must have release semantics to preclude it from moving above the
>> volatile store. Otherwise, as you say, threadA can be unparked before the
>> volatile write is visible and then it'll go back to sleep indefinitely.
>>
>> Note that a volatile write doesn't change this - if unpark() isn't
>> ordered, it can be moved above the volatile write (theoretically).
>>
>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>> Thank you for that first feedback
>>
>> for the "the JMM says a write to a volatile field can be seen by all
>> threads." by Henri, the issue is not that the waiter will not see the
>> write, but that it may not see at the precise moment that it has been
>> unparked, so it is parked again and wait possibly forever. As far as I
>> understand it, the JMM is based on HB edges, so to make sure the write is
>> visible at that precise moment, there should be an HB edge between unpark
>> and park, which is not guaranteed
>> Of course, it will work because the volatile write implies waiting for
>> the store buffer to drain so that write will always be visible for threadA,
>> but it seems an implementation detail to me, so it is not something I want
>> to rely on.
>>
>> For the FIFOMutex example, I agree that is intended to work (LS will be
>> pretty useless if it doesn't) and it works by experience. But I would like
>> to know why it is guaranteed to works so I can reason about LS easier :)
>>
>> For the Dave Dice test, I assume that Vitaly is referring to this blog
>> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
>> That's basically where my second question about lazySet comes from : it
>> will work with empty methods, but if we assume that park/unpark can have no
>> fences (since the current fences are an implementation detail) it seems
>> even more fragile
>>
>>
>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>>
>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
>> Anthony's example. That would imply the idiom is intended to work.
>>
>> LS on its own isn't spec'd to provide any happens-before/ordering, so
>> user needs to provide that themselves. I recall Dave Dice mentioning that a
>> good litmus test for proper LS usage is to pretend park/unpark are empty
>> methods that degenerate into spinning.
>>
>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
>> wrote:
>>
>> I'm not a subject matter expert on this but:
>>
>> Yes. It works if "ready" is volatile. Because for any implementation, the
>> JMM says a write to a volatile field can be seen by all threads.
>>
>> An AtomicBoolean would also work on a set().
>>
>> Two things I don't know:
>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set
>> ready after threadA check its value and is back to park
>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
>> of memory fence.
>>
>> Henri
>>
>>
>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a
>> clear answer
>> I'm wondering on how memory consistency can be achieved when using
>> LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or
>> in the JLS, so I assume that the condition to be check should come with its
>> own visibility mechanism since we don't have any HB relationship.
>>
>>
>> Let's consider the following idiom, "ready" being a volatile boolean
>> field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>>
>> What does make this example work ? Is it possible for threadA to un²park,
>> check the "ready" field without seeing threadB's store and park again ? If
>> I understand the JMM correctly I would say that it is a valid execution
>> order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>> volatile store force the store buffer to drain so that threadA will always
>> see the store when the permit is made available by threadB, but is there
>> any guarantee that is not implementation or architecture dependent ?
>>
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>> instead of volatile read/write ? I assume that it should be correct on x86
>> + openJDK, but is there any strong guarantee on this ?
>>
>> Kind regards.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> --
>> Sent from my phone
>>
>>
>> --
>> Sent from my phone
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> --
> Sent from my phone
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/b75ad8b7/attachment-0001.html>

From dl at cs.oswego.edu  Wed Nov 16 08:11:37 2016
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 16 Nov 2016 08:11:37 -0500
Subject: [concurrency-interest] LockSupport park/unpark and memory
 visibility
In-Reply-To: <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
Message-ID: <e3fd9c39-ea3c-58ec-04fd-1d3b1226b5d2@cs.oswego.edu>


On 11/16/2016 07:45 AM, Vitaly Davidovich wrote:
> This would be a particular implementation detail. Since we're
> hypothesizing, it's possible that unpark() can use an unordered read
> (i.e. relaxed) as an optimization, and then we're back to same situation.
>
> Either way though, this thread and previous ones on LS are indication
> that more documentation is needed.

We do already say in the javadocs:

"Reliable usage requires the use of volatile (or atomic) variables to 
control when to park or unpark."

Some people familiar with the implementation on some JVM might wish we
would promise more, but (1) the specs are designed to allow other
implementations, even including park() as no-op; (2) as nicely
explained by Alex Otenko, a stronger spec shouldn't impact how you
use it anyway.

-Doug





From vitalyd at gmail.com  Wed Nov 16 08:15:20 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 13:15:20 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CANPzfU_xCpEK=TOdL1eEmCYd5igTeZ001A4Rrt_cjCw8kJPeOw@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <CANPzfU_xCpEK=TOdL1eEmCYd5igTeZ001A4Rrt_cjCw8kJPeOw@mail.gmail.com>
Message-ID: <CAHjP37EpAfnfN7Qqx=GKGTb9_g1Uj4EKw4LbKu2PdAXhUdmXUQ@mail.gmail.com>

Right, nobody really wants to speculate but lacking specs/docs, that's all
that's left :).

IMO, a component meant as a low-level concurrency building block deserves
extensive/sufficient specs. Knowing what properties are provided by the
building block (vs what user needs to add themselves) may have performance
implications. There's really no room for guessing/speculation here.

On Wed, Nov 16, 2016 at 7:57 AM Viktor Klang <viktor.klang at gmail.com> wrote:

> Since we're talking about something which is un(der)-specced, what use is
> it to speculate in what horrors could happen? ^^
>
> The only way to make sure is to add the appropriate belt-and-suspenders
> around it to make sure that there's something which provides the desired
> semantics :)
>
> On Wed, Nov 16, 2016 at 1:45 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
> This would be a particular implementation detail. Since we're
> hypothesizing, it's possible that unpark() can use an unordered read (i.e.
> relaxed) as an optimization, and then we're back to same situation.
>
> Either way though, this thread and previous ones on LS are indication that
> more documentation is needed.
>
> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
> Not necessarily.
>
> unpark can end up just checking a flag to see that the thread has not been
> parked after some preceding call to unpark. This can make it behave like a
> volatile load, which will have to be totally ordered with a store in park
> that clears that flag. So there may be no stores in a particular execution
> of unpark.
>
>
> Alex
>
> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> Yes, that's the blog entry I was referring to.
>
> So I agree with you that LS ordering is underspecified. Specifically,
> unpark() must have release semantics to preclude it from moving above the
> volatile store. Otherwise, as you say, threadA can be unparked before the
> volatile write is visible and then it'll go back to sleep indefinitely.
>
> Note that a volatile write doesn't change this - if unpark() isn't
> ordered, it can be moved above the volatile write (theoretically).
>
> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Thank you for that first feedback
>
> for the "the JMM says a write to a volatile field can be seen by all
> threads." by Henri, the issue is not that the waiter will not see the
> write, but that it may not see at the precise moment that it has been
> unparked, so it is parked again and wait possibly forever. As far as I
> understand it, the JMM is based on HB edges, so to make sure the write is
> visible at that precise moment, there should be an HB edge between unpark
> and park, which is not guaranteed
> Of course, it will work because the volatile write implies waiting for the
> store buffer to drain so that write will always be visible for threadA, but
> it seems an implementation detail to me, so it is not something I want to
> rely on.
>
> For the FIFOMutex example, I agree that is intended to work (LS will be
> pretty useless if it doesn't) and it works by experience. But I would like
> to know why it is guaranteed to works so I can reason about LS easier :)
>
> For the Dave Dice test, I assume that Vitaly is referring to this blog
> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
> That's basically where my second question about lazySet comes from : it
> will work with empty methods, but if we assume that park/unpark can have no
> fences (since the current fences are an implementation detail) it seems
> even more fragile
>
>
> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>
> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
> Anthony's example. That would imply the idiom is intended to work.
>
> LS on its own isn't spec'd to provide any happens-before/ordering, so user
> needs to provide that themselves. I recall Dave Dice mentioning that a good
> litmus test for proper LS usage is to pretend park/unpark are empty methods
> that degenerate into spinning.
>
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
> I'm not a subject matter expert on this but:
>
> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>
> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Sent from my phone
>
>
> --
> Sent from my phone
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
> Sent from my phone
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> --
> Cheers,
> √
>
-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/364de1ca/attachment-0001.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 08:15:34 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 13:15:34 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
Message-ID: <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>

It is not just an “implementation detail”. It is also a minimal sensible implementation.

How about this: park enter/exit and unpark appear in synchronization order, but only guarantee park exit occurs for every park enter, if there are unpark events after the preceding park exit in that order - no new synchronizes-with edges. This is pretty much all that park-unpark can be reasonably expected to do, and the minimal requirement.


Alex


> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
> This would be a particular implementation detail.  Since we're hypothesizing, it's possible that unpark() can use an unordered read (i.e. relaxed) as an optimization, and then we're back to same situation.
> 
> Either way though, this thread and previous ones on LS are indication that more documentation is needed.
> 
> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> Not necessarily.
> 
> unpark can end up just checking a flag to see that the thread has not been parked after some preceding call to unpark. This can make it behave like a volatile load, which will have to be totally ordered with a store in park that clears that flag. So there may be no stores in a particular execution of unpark.
> 
> 
> Alex
> 
>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>> 
>> Yes, that's the blog entry I was referring to.
>> 
>> So I agree with you that LS ordering is underspecified.  Specifically, unpark() must have release semantics to preclude it from moving above the volatile store.  Otherwise, as you say, threadA can be unparked before the volatile write is visible and then it'll go back to sleep indefinitely.
>> 
>> Note that a volatile write doesn't change this - if unpark() isn't ordered, it can be moved above the volatile write (theoretically).
>> 
>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>> Thank you for that first feedback
>> 
>> for the "the JMM says a write to a volatile field can be seen by all threads." by Henri, the issue is not that the waiter will not see the write, but that it may not see at the precise moment that it has been unparked, so it is parked again and wait possibly forever. As far as I understand it, the JMM is based on HB edges, so to make sure the write is visible at that precise moment, there should be an HB edge between unpark and park, which is not guaranteed
>> Of course, it will work because the volatile write implies waiting for the store buffer to drain so that write will always be visible for threadA, but it seems an implementation detail to me, so it is not something I want to rely on.
>> 
>> For the FIFOMutex example, I agree that is intended to work (LS will be pretty useless if it doesn't) and it works by experience. But I would like to know why it is guaranteed to works so I can reason about LS easier :)
>> 
>> For the Dave Dice test, I assume that Vitaly is referring to this blog post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park <https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park>
>> That's basically where my second question about lazySet comes from : it will work with empty methods, but if we assume that park/unpark can have no fences (since the current fences are an implementation detail) it seems even more fragile
>> 
>> 
>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>>:
>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to Anthony's example.  That would imply the idiom is intended to work.
>> 
>> LS on its own isn't spec'd to provide any happens-before/ordering, so user needs to provide that themselves.  I recall Dave Dice mentioning that a good litmus test for proper LS usage is to pretend park/unpark are empty methods that degenerate into spinning.
>> 
>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com <mailto:henri.tremblay at gmail.com>> wrote:
>> I'm not a subject matter expert on this but:
>> 
>> Yes. It works if "ready" is volatile. Because for any implementation, the JMM says a write to a volatile field can be seen by all threads.
>> 
>> An AtomicBoolean would also work on a set().
>> 
>> Two things I don't know:
>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready after threadA check its value and is back to park
>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind of memory fence.
>> 
>> Henri
>> 
>> 
>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a clear answer
>> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
>> 
>> 
>> Let's consider the following idiom, "ready" being a volatile boolean field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>> 
>> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
>> 
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
>> 
>> Kind regards.
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> -- 
>> Sent from my phone
>> 
>> -- 
>> Sent from my phone
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
> 
> -- 
> Sent from my phone

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/cdeabe45/attachment.html>

From vitalyd at gmail.com  Wed Nov 16 08:23:22 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 13:23:22 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <e3fd9c39-ea3c-58ec-04fd-1d3b1226b5d2@cs.oswego.edu>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <e3fd9c39-ea3c-58ec-04fd-1d3b1226b5d2@cs.oswego.edu>
Message-ID: <CAHjP37E+Vb_zHVCKWrAqRX=Gi8DQvZSu4EjP1t-whb8WHT3hDA@mail.gmail.com>

Doug,

It's still not clear. Can you explain what in the current docs states that
unpark() cannot move above a volatile store? How about a lazySet/ordered
put? Is that considered "atomically" or not?

On Wed, Nov 16, 2016 at 8:15 AM Doug Lea <dl at cs.oswego.edu> wrote:

>
> On 11/16/2016 07:45 AM, Vitaly Davidovich wrote:
> > This would be a particular implementation detail. Since we're
> > hypothesizing, it's possible that unpark() can use an unordered read
> > (i.e. relaxed) as an optimization, and then we're back to same situation.
> >
> > Either way though, this thread and previous ones on LS are indication
> > that more documentation is needed.
>
> We do already say in the javadocs:
>
> "Reliable usage requires the use of volatile (or atomic) variables to
> control when to park or unpark."
>
> Some people familiar with the implementation on some JVM might wish we
> would promise more, but (1) the specs are designed to allow other
> implementations, even including park() as no-op; (2) as nicely
> explained by Alex Otenko, a stronger spec shouldn't impact how you
> use it anyway.
>
> -Doug
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-- 
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/c2ede6ad/attachment-0001.html>

From vitalyd at gmail.com  Wed Nov 16 08:35:32 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 13:35:32 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
Message-ID: <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>

unpark can also unconditionally set a flag, with a relaxed store, with no
ordered load. Since park is allowed to return spuriously "for no reason at
all", I don't see how that isn't allowed (whether it's sensible or not is a
separate topic).

And as I mentioned to Doug, is lazySet considered using "atomics" or not?

On Wed, Nov 16, 2016 at 8:15 AM Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> It is not just an “implementation detail”. It is also a minimal sensible
> implementation.
>
> How about this: park enter/exit and unpark appear in synchronization
> order, but only guarantee park exit occurs for every park enter, if there
> are unpark events after the preceding park exit in that order - no new
> synchronizes-with edges. This is pretty much all that park-unpark can be
> reasonably expected to do, and the minimal requirement.
>
>
> Alex
>
>
> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> This would be a particular implementation detail. Since we're
> hypothesizing, it's possible that unpark() can use an unordered read (i.e.
> relaxed) as an optimization, and then we're back to same situation.
>
> Either way though, this thread and previous ones on LS are indication that
> more documentation is needed.
>
> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
> Not necessarily.
>
> unpark can end up just checking a flag to see that the thread has not been
> parked after some preceding call to unpark. This can make it behave like a
> volatile load, which will have to be totally ordered with a store in park
> that clears that flag. So there may be no stores in a particular execution
> of unpark.
>
>
> Alex
>
> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> Yes, that's the blog entry I was referring to.
>
> So I agree with you that LS ordering is underspecified. Specifically,
> unpark() must have release semantics to preclude it from moving above the
> volatile store. Otherwise, as you say, threadA can be unparked before the
> volatile write is visible and then it'll go back to sleep indefinitely.
>
> Note that a volatile write doesn't change this - if unpark() isn't
> ordered, it can be moved above the volatile write (theoretically).
>
> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Thank you for that first feedback
>
> for the "the JMM says a write to a volatile field can be seen by all
> threads." by Henri, the issue is not that the waiter will not see the
> write, but that it may not see at the precise moment that it has been
> unparked, so it is parked again and wait possibly forever. As far as I
> understand it, the JMM is based on HB edges, so to make sure the write is
> visible at that precise moment, there should be an HB edge between unpark
> and park, which is not guaranteed
> Of course, it will work because the volatile write implies waiting for the
> store buffer to drain so that write will always be visible for threadA, but
> it seems an implementation detail to me, so it is not something I want to
> rely on.
>
> For the FIFOMutex example, I agree that is intended to work (LS will be
> pretty useless if it doesn't) and it works by experience. But I would like
> to know why it is guaranteed to works so I can reason about LS easier :)
>
> For the Dave Dice test, I assume that Vitaly is referring to this blog
> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
> That's basically where my second question about lazySet comes from : it
> will work with empty methods, but if we assume that park/unpark can have no
> fences (since the current fences are an implementation detail) it seems
> even more fragile
>
>
> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>
> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
> Anthony's example. That would imply the idiom is intended to work.
>
> LS on its own isn't spec'd to provide any happens-before/ordering, so user
> needs to provide that themselves. I recall Dave Dice mentioning that a good
> litmus test for proper LS usage is to pretend park/unpark are empty methods
> that degenerate into spinning.
>
> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
> wrote:
>
> I'm not a subject matter expert on this but:
>
> Yes. It works if "ready" is volatile. Because for any implementation, the
> JMM says a write to a volatile field can be seen by all threads.
>
> An AtomicBoolean would also work on a set().
>
> Two things I don't know:
> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready
> after threadA check its value and is back to park
> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
> of memory fence.
>
> Henri
>
>
> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
> wrote:
>
> Hi
> Sorry if this has been already asked, but I didn't succeed to find a clear
> answer
> I'm wondering on how memory consistency can be achieved when using
> LockSupport park() and unpark() methods
> As far as I know, there is no explicit guarantee either in LS javadoc or
> in the JLS, so I assume that the condition to be check should come with its
> own visibility mechanism since we don't have any HB relationship.
>
>
> Let's consider the following idiom, "ready" being a volatile boolean field:
> - threadA: while(!ready){ LockSupport.park(this); }
> - threadB: ready = true; LockSupport.unpark(threadA);
>
> What does make this example work ? Is it possible for threadA to un²park,
> check the "ready" field without seeing threadB's store and park again ? If
> I understand the JMM correctly I would say that it is a valid execution
> order.
> On x86 with OpenJDK or Oracle JVM, I assume this works because the
> volatile store force the store buffer to drain so that threadA will always
> see the store when the permit is made available by threadB, but is there
> any guarantee that is not implementation or architecture dependent ?
>
> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
> instead of volatile read/write ? I assume that it should be correct on x86
> + openJDK, but is there any strong guarantee on this ?
>
> Kind regards.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> --
> Sent from my phone
>
>
> --
> Sent from my phone
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
> Sent from my phone
>
>
> --
Sent from my phone
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/b74c917c/attachment-0001.html>

From maire.anthony at gmail.com  Wed Nov 16 08:40:46 2016
From: maire.anthony at gmail.com (Anthony Maire)
Date: Wed, 16 Nov 2016 14:40:46 +0100
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <A105CFD4-3E79-437C-BC81-EE15AA2A7E05@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>
 <CAJpyQ4pTJ0OLA6NKJmAd2C2Coxjs=_cX8hRpJ2NPbeOZy59zRA@mail.gmail.com>
 <A105CFD4-3E79-437C-BC81-EE15AA2A7E05@gmail.com>
Message-ID: <CAJpyQ4rjS9c-nF5=QSWbErZuqSyjJSAGwTEfbLcqTNpT1ee7PA@mail.gmail.com>

for threadA indicating that it is entering the park loop and threadB
collecting that information to know which thread to unpark, it is something
that I already have in my code, but I didn't include it in my original
question ...
My point is not on how to make sure that threadB will actually call unpark
on the right thread, but on how to be sure that when unparked threadA will
correctly see the write that will allow to exit from the park loop.

For the interruption part, for the moment, I have a big "//TODO: handle
interruption" at the beginning of the class :) Thank you for the additional
information, it will be helpful.

2016-11-16 13:17 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com>:

> There are several issues with that example. One of them is that even if
> the barriers were not eliminated, they could still be reordered with the
> reads and the writes to x and y - that code isn’t an example of correct
> synchronization even if the same Holder instance were used by both actors.
>
> I am not sure what your question is about waiter store and load. The
> important aspect is that threadA must indicate that it decided to enter
> park, and for threadB to check that indicator - be it a volatile boolean
> flag or a volatile reference to a Thread. If your code isn’t doing that,
> you are likely losing out more by always calling park/unpark than you win
> by eliminating that volatile indicator, unless you want to bet on a
> particular JVM that perhaps reduces the cost of park/unpark to less than a
> volatile store/load.
>
>
> Besides, after return from park you better check whether the Thread was
> interrupted. I think there are implementations that will not “park” until
> you checked (thus cleared) that flag, if it happens to be set - so the
> interrupted threadA will burn CPU by ignoring the interrupted flag check.
>
>
> Alex
>
> On 16 Nov 2016, at 10:27, Anthony Maire <maire.anthony at gmail.com> wrote:
>
> In fact, the structure I'm working on is guaranteed to have at most a
> single waiter so the full code is more like this one (although in the
> current version I have, setting the waiter to null was done in the waiter
> code after exiting the wait loop, but that is only temporary code until I
> have more information on LS usage)
>
> However, I don't understand how this guarantee that we have no possible
> issue, even when the volatile write to the waiter field in threadB. This
> write is not observed so it is not guaranteed to have any memory effect
> according to https://shipilev.net/blog/2016/close-encounters-of-jmm-
> kind/#wishful-unobserved-volatiles.
>
> Could you please explain further ?
>
> Regards,
> Anthony
>
> 2016-11-16 8:51 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com>:
>
>> Normally there will be more - you’d need to tell who is waiting. For
>> illustrative purposes (not that this makes sense in the presence of
>> multiple waiters):
>>
>> threadA: if (!ready) {*waiter = Thread.currentThread; while(!ready*)
>> {LockSupport.park();}}
>>
>> threadB: *ready = true; if (waiter != null)* {waiter = null;
>> LockSupport.unpark(w);}
>>
>> The parts in bold need to be a Dekker idiom, otherwise you won’t wake up
>> some waiters. So waiter and ready need to be volatile.
>>
>> In the presence of multiple waiters waiter will be a list with
>> thread-safe add, poll, remove, but the essence is the same - adding or
>> removing a waiter to/from the list is at least a volatile-store.
>>
>> Then the barriers inside park/unpark do not matter.
>>
>>
>> Alex
>>
>>
>> On 15 Nov 2016, at 14:34, Anthony Maire <maire.anthony at gmail.com> wrote:
>>
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a
>> clear answer
>> I'm wondering on how memory consistency can be achieved when using
>> LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or
>> in the JLS, so I assume that the condition to be check should come with its
>> own visibility mechanism since we don't have any HB relationship.
>>
>>
>> Let's consider the following idiom, "ready" being a volatile boolean
>> field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>>
>> What does make this example work ? Is it possible for threadA to un²park,
>> check the "ready" field without seeing threadB's store and park again ? If
>> I understand the JMM correctly I would say that it is a valid execution
>> order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>> volatile store force the store buffer to drain so that threadA will always
>> see the store when the permit is made available by threadB, but is there
>> any guarantee that is not implementation or architecture dependent ?
>>
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>> instead of volatile read/write ? I assume that it should be correct on x86
>> + openJDK, but is there any strong guarantee on this ?
>>
>> Kind regards.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/4eb9c9d6/attachment.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 08:46:50 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 13:46:50 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
 <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>
Message-ID: <7B4172C7-7207-4DE0-B992-DDA0B8358E5C@gmail.com>

Perfectly good example of why park/unpark should be disconnected from synchronizes-with edges. Plugging them in synchronization order gives you that guarantee about "reordering" park/unpark with preceding/following volatile accesses.

Alex

> On 16 Nov 2016, at 13:35, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
> unpark can also unconditionally set a flag, with a relaxed store, with no ordered load.  Since park is allowed to return spuriously "for no reason at all", I don't see how that isn't allowed (whether it's sensible or not is a separate topic).
> 
> And as I mentioned to Doug, is lazySet considered using "atomics" or not?
> 
> On Wed, Nov 16, 2016 at 8:15 AM Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> It is not just an “implementation detail”. It is also a minimal sensible implementation.
> 
> How about this: park enter/exit and unpark appear in synchronization order, but only guarantee park exit occurs for every park enter, if there are unpark events after the preceding park exit in that order - no new synchronizes-with edges. This is pretty much all that park-unpark can be reasonably expected to do, and the minimal requirement.
> 
> 
> Alex
> 
> 
>> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>> 
>> This would be a particular implementation detail.  Since we're hypothesizing, it's possible that unpark() can use an unordered read (i.e. relaxed) as an optimization, and then we're back to same situation.
>> 
>> Either way though, this thread and previous ones on LS are indication that more documentation is needed.
>> 
>> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> Not necessarily.
>> 
>> unpark can end up just checking a flag to see that the thread has not been parked after some preceding call to unpark. This can make it behave like a volatile load, which will have to be totally ordered with a store in park that clears that flag. So there may be no stores in a particular execution of unpark.
>> 
>> 
>> Alex
>> 
>>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>> 
>>> Yes, that's the blog entry I was referring to.
>>> 
>>> So I agree with you that LS ordering is underspecified.  Specifically, unpark() must have release semantics to preclude it from moving above the volatile store.  Otherwise, as you say, threadA can be unparked before the volatile write is visible and then it'll go back to sleep indefinitely.
>>> 
>>> Note that a volatile write doesn't change this - if unpark() isn't ordered, it can be moved above the volatile write (theoretically).
>>> 
>>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>>> Thank you for that first feedback
>>> 
>>> for the "the JMM says a write to a volatile field can be seen by all threads." by Henri, the issue is not that the waiter will not see the write, but that it may not see at the precise moment that it has been unparked, so it is parked again and wait possibly forever. As far as I understand it, the JMM is based on HB edges, so to make sure the write is visible at that precise moment, there should be an HB edge between unpark and park, which is not guaranteed
>>> Of course, it will work because the volatile write implies waiting for the store buffer to drain so that write will always be visible for threadA, but it seems an implementation detail to me, so it is not something I want to rely on.
>>> 
>>> For the FIFOMutex example, I agree that is intended to work (LS will be pretty useless if it doesn't) and it works by experience. But I would like to know why it is guaranteed to works so I can reason about LS easier :)
>>> 
>>> For the Dave Dice test, I assume that Vitaly is referring to this blog post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park <https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park>
>>> That's basically where my second question about lazySet comes from : it will work with empty methods, but if we assume that park/unpark can have no fences (since the current fences are an implementation detail) it seems even more fragile
>>> 
>>> 
>>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>>:
>>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to Anthony's example.  That would imply the idiom is intended to work.
>>> 
>>> LS on its own isn't spec'd to provide any happens-before/ordering, so user needs to provide that themselves.  I recall Dave Dice mentioning that a good litmus test for proper LS usage is to pretend park/unpark are empty methods that degenerate into spinning.
>>> 
>>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com <mailto:henri.tremblay at gmail.com>> wrote:
>>> I'm not a subject matter expert on this but:
>>> 
>>> Yes. It works if "ready" is volatile. Because for any implementation, the JMM says a write to a volatile field can be seen by all threads.
>>> 
>>> An AtomicBoolean would also work on a set().
>>> 
>>> Two things I don't know:
>>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready after threadA check its value and is back to park
>>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind of memory fence.
>>> 
>>> Henri
>>> 
>>> 
>>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>>> Hi
>>> Sorry if this has been already asked, but I didn't succeed to find a clear answer
>>> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
>>> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
>>> 
>>> 
>>> Let's consider the following idiom, "ready" being a volatile boolean field:
>>> - threadA: while(!ready){ LockSupport.park(this); }
>>> - threadB: ready = true; LockSupport.unpark(threadA);
>>> 
>>> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
>>> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
>>> 
>>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
>>> 
>>> Kind regards.
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> -- 
>>> Sent from my phone
>>> 
>>> -- 
>>> Sent from my phone
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> -- 
>> Sent from my phone
> 
> -- 
> Sent from my phone

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/8bd0722e/attachment-0001.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 08:55:28 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 13:55:28 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAJpyQ4rjS9c-nF5=QSWbErZuqSyjJSAGwTEfbLcqTNpT1ee7PA@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <1A1AF6E1-D138-4F50-B730-1086D6262843@gmail.com>
 <CAJpyQ4pTJ0OLA6NKJmAd2C2Coxjs=_cX8hRpJ2NPbeOZy59zRA@mail.gmail.com>
 <A105CFD4-3E79-437C-BC81-EE15AA2A7E05@gmail.com>
 <CAJpyQ4rjS9c-nF5=QSWbErZuqSyjJSAGwTEfbLcqTNpT1ee7PA@mail.gmail.com>
Message-ID: <DDB0524F-7EE2-4693-8239-296A40281C6E@gmail.com>

I don’t know what is not satisfactory about a Dekker idiom.

If you want to eliminate one or both volatiles from that idiom, then the question should be about a particular implementation of park/unpark, not about park/unpark in general. Relying on one implementation is something that would require a strong justification, though.

Alex

> On 16 Nov 2016, at 13:40, Anthony Maire <maire.anthony at gmail.com> wrote:
> 
> for threadA indicating that it is entering the park loop and threadB collecting that information to know which thread to unpark, it is something that I already have in my code, but I didn't include it in my original question ...
> My point is not on how to make sure that threadB will actually call unpark on the right thread, but on how to be sure that when unparked threadA will correctly see the write that will allow to exit from the park loop.
> 
> For the interruption part, for the moment, I have a big "//TODO: handle interruption" at the beginning of the class :) Thank you for the additional information, it will be helpful.
> 
> 2016-11-16 13:17 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>>:
> There are several issues with that example. One of them is that even if the barriers were not eliminated, they could still be reordered with the reads and the writes to x and y - that code isn’t an example of correct synchronization even if the same Holder instance were used by both actors.
> 
> I am not sure what your question is about waiter store and load. The important aspect is that threadA must indicate that it decided to enter park, and for threadB to check that indicator - be it a volatile boolean flag or a volatile reference to a Thread. If your code isn’t doing that, you are likely losing out more by always calling park/unpark than you win by eliminating that volatile indicator, unless you want to bet on a particular JVM that perhaps reduces the cost of park/unpark to less than a volatile store/load.
> 
> 
> Besides, after return from park you better check whether the Thread was interrupted. I think there are implementations that will not “park” until you checked (thus cleared) that flag, if it happens to be set - so the interrupted threadA will burn CPU by ignoring the interrupted flag check.
> 
> 
> Alex
> 
>> On 16 Nov 2016, at 10:27, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>> 
>> In fact, the structure I'm working on is guaranteed to have at most a single waiter so the full code is more like this one (although in the current version I have, setting the waiter to null was done in the waiter code after exiting the wait loop, but that is only temporary code until I have more information on LS usage)
>> 
>> However, I don't understand how this guarantee that we have no possible issue, even when the volatile write to the waiter field in threadB. This write is not observed so it is not guaranteed to have any memory effect according to https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles <https://shipilev.net/blog/2016/close-encounters-of-jmm-kind/#wishful-unobserved-volatiles>.
>> 
>> Could you please explain further ?
>> 
>> Regards,
>> Anthony
>> 
>> 2016-11-16 8:51 GMT+01:00 Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>>:
>> Normally there will be more - you’d need to tell who is waiting. For illustrative purposes (not that this makes sense in the presence of multiple waiters):
>> 
>> threadA: if (!ready) {waiter = Thread.currentThread; while(!ready) {LockSupport.park();}}
>> 
>> threadB: ready = true; if (waiter != null) {waiter = null; LockSupport.unpark(w);}
>> 
>> The parts in bold need to be a Dekker idiom, otherwise you won’t wake up some waiters. So waiter and ready need to be volatile.
>> 
>> In the presence of multiple waiters waiter will be a list with thread-safe add, poll, remove, but the essence is the same - adding or removing a waiter to/from the list is at least a volatile-store.
>> 
>> Then the barriers inside park/unpark do not matter.
>> 
>> 
>> Alex
>> 
>> 
>>> On 15 Nov 2016, at 14:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>>> 
>>> Hi
>>> Sorry if this has been already asked, but I didn't succeed to find a clear answer
>>> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
>>> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
>>> 
>>> 
>>> Let's consider the following idiom, "ready" being a volatile boolean field:
>>> - threadA: while(!ready){ LockSupport.park(this); }
>>> - threadB: ready = true; LockSupport.unpark(threadA);
>>> 
>>> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
>>> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
>>> 
>>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
>>> 
>>> Kind regards.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> 
>> 
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/8ff5fcb5/attachment.html>

From vitalyd at gmail.com  Wed Nov 16 08:57:51 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 08:57:51 -0500
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <7B4172C7-7207-4DE0-B992-DDA0B8358E5C@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
 <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>
 <7B4172C7-7207-4DE0-B992-DDA0B8358E5C@gmail.com>
Message-ID: <CAHjP37GmgLQOW4+Uwp7X-b3p9bp-QSpAYoLSauYOTFnyzN2DzQ@mail.gmail.com>

On Wed, Nov 16, 2016 at 8:46 AM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> Perfectly good example of why park/unpark should be disconnected from
> synchronizes-with edges. Plugging them in synchronization order gives you
> that guarantee about "reordering" park/unpark with preceding/following
> volatile accesses.
>
Right, which is the release/acquire between unpark/park that I alluded to
earlier.  That would also allow lazySet to work.

Assuming this is an agreed upon clarification, should that be added to the
javadoc? I say yes.

>
> Alex
>
> On 16 Nov 2016, at 13:35, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
> unpark can also unconditionally set a flag, with a relaxed store, with no
> ordered load. Since park is allowed to return spuriously "for no reason at
> all", I don't see how that isn't allowed (whether it's sensible or not is a
> separate topic).
>
> And as I mentioned to Doug, is lazySet considered using "atomics" or not?
>
> On Wed, Nov 16, 2016 at 8:15 AM Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> It is not just an “implementation detail”. It is also a minimal sensible
>> implementation.
>>
>> How about this: park enter/exit and unpark appear in synchronization
>> order, but only guarantee park exit occurs for every park enter, if there
>> are unpark events after the preceding park exit in that order - no new
>> synchronizes-with edges. This is pretty much all that park-unpark can be
>> reasonably expected to do, and the minimal requirement.
>>
>>
>> Alex
>>
>>
>> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>> This would be a particular implementation detail. Since we're
>> hypothesizing, it's possible that unpark() can use an unordered read (i.e.
>> relaxed) as an optimization, and then we're back to same situation.
>>
>> Either way though, this thread and previous ones on LS are indication
>> that more documentation is needed.
>>
>> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
>> wrote:
>>
>> Not necessarily.
>>
>> unpark can end up just checking a flag to see that the thread has not
>> been parked after some preceding call to unpark. This can make it behave
>> like a volatile load, which will have to be totally ordered with a store in
>> park that clears that flag. So there may be no stores in a particular
>> execution of unpark.
>>
>>
>> Alex
>>
>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>> Yes, that's the blog entry I was referring to.
>>
>> So I agree with you that LS ordering is underspecified. Specifically,
>> unpark() must have release semantics to preclude it from moving above the
>> volatile store. Otherwise, as you say, threadA can be unparked before the
>> volatile write is visible and then it'll go back to sleep indefinitely.
>>
>> Note that a volatile write doesn't change this - if unpark() isn't
>> ordered, it can be moved above the volatile write (theoretically).
>>
>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>> Thank you for that first feedback
>>
>> for the "the JMM says a write to a volatile field can be seen by all
>> threads." by Henri, the issue is not that the waiter will not see the
>> write, but that it may not see at the precise moment that it has been
>> unparked, so it is parked again and wait possibly forever. As far as I
>> understand it, the JMM is based on HB edges, so to make sure the write is
>> visible at that precise moment, there should be an HB edge between unpark
>> and park, which is not guaranteed
>> Of course, it will work because the volatile write implies waiting for
>> the store buffer to drain so that write will always be visible for threadA,
>> but it seems an implementation detail to me, so it is not something I want
>> to rely on.
>>
>> For the FIFOMutex example, I agree that is intended to work (LS will be
>> pretty useless if it doesn't) and it works by experience. But I would like
>> to know why it is guaranteed to works so I can reason about LS easier :)
>>
>> For the Dave Dice test, I assume that Vitaly is referring to this blog
>> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
>> That's basically where my second question about lazySet comes from : it
>> will work with empty methods, but if we assume that park/unpark can have no
>> fences (since the current fences are an implementation detail) it seems
>> even more fragile
>>
>>
>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>>
>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to
>> Anthony's example. That would imply the idiom is intended to work.
>>
>> LS on its own isn't spec'd to provide any happens-before/ordering, so
>> user needs to provide that themselves. I recall Dave Dice mentioning that a
>> good litmus test for proper LS usage is to pretend park/unpark are empty
>> methods that degenerate into spinning.
>>
>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
>> wrote:
>>
>> I'm not a subject matter expert on this but:
>>
>> Yes. It works if "ready" is volatile. Because for any implementation, the
>> JMM says a write to a volatile field can be seen by all threads.
>>
>> An AtomicBoolean would also work on a set().
>>
>> Two things I don't know:
>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set
>> ready after threadA check its value and is back to park
>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
>> of memory fence.
>>
>> Henri
>>
>>
>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
>> wrote:
>>
>> Hi
>> Sorry if this has been already asked, but I didn't succeed to find a
>> clear answer
>> I'm wondering on how memory consistency can be achieved when using
>> LockSupport park() and unpark() methods
>> As far as I know, there is no explicit guarantee either in LS javadoc or
>> in the JLS, so I assume that the condition to be check should come with its
>> own visibility mechanism since we don't have any HB relationship.
>>
>>
>> Let's consider the following idiom, "ready" being a volatile boolean
>> field:
>> - threadA: while(!ready){ LockSupport.park(this); }
>> - threadB: ready = true; LockSupport.unpark(threadA);
>>
>> What does make this example work ? Is it possible for threadA to un²park,
>> check the "ready" field without seeing threadB's store and park again ? If
>> I understand the JMM correctly I would say that it is a valid execution
>> order.
>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>> volatile store force the store buffer to drain so that threadA will always
>> see the store when the permit is made available by threadB, but is there
>> any guarantee that is not implementation or architecture dependent ?
>>
>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>> instead of volatile read/write ? I assume that it should be correct on x86
>> + openJDK, but is there any strong guarantee on this ?
>>
>> Kind regards.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> --
>> Sent from my phone
>>
>>
>> --
>> Sent from my phone
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> --
>> Sent from my phone
>>
>>
>> --
> Sent from my phone
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/a5157cff/attachment-0001.html>

From oleksandr.otenko at gmail.com  Wed Nov 16 09:07:21 2016
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 16 Nov 2016 14:07:21 +0000
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <CAHjP37GmgLQOW4+Uwp7X-b3p9bp-QSpAYoLSauYOTFnyzN2DzQ@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
 <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>
 <7B4172C7-7207-4DE0-B992-DDA0B8358E5C@gmail.com>
 <CAHjP37GmgLQOW4+Uwp7X-b3p9bp-QSpAYoLSauYOTFnyzN2DzQ@mail.gmail.com>
Message-ID: <B954D93D-6B74-497A-BDA7-F0F96B2C00CC@gmail.com>

release/acquire is used in chapter 17 only a couple of times, and only in conjunction with synchronizes-with edges, so I’d say no, since others may conclude park/unpark forms edges that are part of happens-before - but the conclusion is this is exactly what is not promised.


Alex


> On 16 Nov 2016, at 13:57, Vitaly Davidovich <vitalyd at gmail.com> wrote:
> 
> 
> 
> On Wed, Nov 16, 2016 at 8:46 AM, Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
> Perfectly good example of why park/unpark should be disconnected from synchronizes-with edges. Plugging them in synchronization order gives you that guarantee about "reordering" park/unpark with preceding/following volatile accesses.
> Right, which is the release/acquire between unpark/park that I alluded to earlier.  That would also allow lazySet to work.
> 
> Assuming this is an agreed upon clarification, should that be added to the javadoc? I say yes.
> 
> Alex
> 
>> On 16 Nov 2016, at 13:35, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>> 
>> unpark can also unconditionally set a flag, with a relaxed store, with no ordered load.  Since park is allowed to return spuriously "for no reason at all", I don't see how that isn't allowed (whether it's sensible or not is a separate topic).
>> 
>> And as I mentioned to Doug, is lazySet considered using "atomics" or not?
>> 
>> On Wed, Nov 16, 2016 at 8:15 AM Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>> It is not just an “implementation detail”. It is also a minimal sensible implementation.
>> 
>> How about this: park enter/exit and unpark appear in synchronization order, but only guarantee park exit occurs for every park enter, if there are unpark events after the preceding park exit in that order - no new synchronizes-with edges. This is pretty much all that park-unpark can be reasonably expected to do, and the minimal requirement.
>> 
>> 
>> Alex
>> 
>> 
>>> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>> 
>>> This would be a particular implementation detail.  Since we're hypothesizing, it's possible that unpark() can use an unordered read (i.e. relaxed) as an optimization, and then we're back to same situation.
>>> 
>>> Either way though, this thread and previous ones on LS are indication that more documentation is needed.
>>> 
>>> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com <mailto:oleksandr.otenko at gmail.com>> wrote:
>>> Not necessarily.
>>> 
>>> unpark can end up just checking a flag to see that the thread has not been parked after some preceding call to unpark. This can make it behave like a volatile load, which will have to be totally ordered with a store in park that clears that flag. So there may be no stores in a particular execution of unpark.
>>> 
>>> 
>>> Alex
>>> 
>>>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>>> 
>>>> Yes, that's the blog entry I was referring to.
>>>> 
>>>> So I agree with you that LS ordering is underspecified.  Specifically, unpark() must have release semantics to preclude it from moving above the volatile store.  Otherwise, as you say, threadA can be unparked before the volatile write is visible and then it'll go back to sleep indefinitely.
>>>> 
>>>> Note that a volatile write doesn't change this - if unpark() isn't ordered, it can be moved above the volatile write (theoretically).
>>>> 
>>>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>>>> Thank you for that first feedback
>>>> 
>>>> for the "the JMM says a write to a volatile field can be seen by all threads." by Henri, the issue is not that the waiter will not see the write, but that it may not see at the precise moment that it has been unparked, so it is parked again and wait possibly forever. As far as I understand it, the JMM is based on HB edges, so to make sure the write is visible at that precise moment, there should be an HB edge between unpark and park, which is not guaranteed
>>>> Of course, it will work because the volatile write implies waiting for the store buffer to drain so that write will always be visible for threadA, but it seems an implementation detail to me, so it is not something I want to rely on.
>>>> 
>>>> For the FIFOMutex example, I agree that is intended to work (LS will be pretty useless if it doesn't) and it works by experience. But I would like to know why it is guaranteed to works so I can reason about LS easier :)
>>>> 
>>>> For the Dave Dice test, I assume that Vitaly is referring to this blog post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park <https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park>
>>>> That's basically where my second question about lazySet comes from : it will work with empty methods, but if we assume that park/unpark can have no fences (since the current fences are an implementation detail) it seems even more fragile
>>>> 
>>>> 
>>>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com <mailto:vitalyd at gmail.com>>:
>>>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom to Anthony's example.  That would imply the idiom is intended to work.
>>>> 
>>>> LS on its own isn't spec'd to provide any happens-before/ordering, so user needs to provide that themselves.  I recall Dave Dice mentioning that a good litmus test for proper LS usage is to pretend park/unpark are empty methods that degenerate into spinning.
>>>> 
>>>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com <mailto:henri.tremblay at gmail.com>> wrote:
>>>> I'm not a subject matter expert on this but:
>>>> 
>>>> Yes. It works if "ready" is volatile. Because for any implementation, the JMM says a write to a volatile field can be seen by all threads.
>>>> 
>>>> An AtomicBoolean would also work on a set().
>>>> 
>>>> Two things I don't know:
>>>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set ready after threadA check its value and is back to park
>>>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind of memory fence.
>>>> 
>>>> Henri
>>>> 
>>>> 
>>>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com <mailto:maire.anthony at gmail.com>> wrote:
>>>> Hi
>>>> Sorry if this has been already asked, but I didn't succeed to find a clear answer
>>>> I'm wondering on how memory consistency can be achieved when using LockSupport park() and unpark() methods
>>>> As far as I know, there is no explicit guarantee either in LS javadoc or in the JLS, so I assume that the condition to be check should come with its own visibility mechanism since we don't have any HB relationship.
>>>> 
>>>> 
>>>> Let's consider the following idiom, "ready" being a volatile boolean field:
>>>> - threadA: while(!ready){ LockSupport.park(this); }
>>>> - threadB: ready = true; LockSupport.unpark(threadA);
>>>> 
>>>> What does make this example work ? Is it possible for threadA to un²park, check the "ready" field without seeing threadB's store and park again ? If I understand the JMM correctly I would say that it is a valid execution order.
>>>> On x86 with OpenJDK or Oracle JVM, I assume this works because the volatile store force the store buffer to drain so that threadA will always see the store when the permit is made available by threadB, but is there any guarantee that is not implementation or architecture dependent ?
>>>> 
>>>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet() instead of volatile read/write ? I assume that it should be correct on x86 + openJDK, but is there any strong guarantee on this ?
>>>> 
>>>> Kind regards.
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> 
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>> -- 
>>>> Sent from my phone
>>>> 
>>>> -- 
>>>> Sent from my phone
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>> 
>>> -- 
>>> Sent from my phone
>> 
>> -- 
>> Sent from my phone
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/7a6192fb/attachment-0001.html>

From vitalyd at gmail.com  Wed Nov 16 09:16:17 2016
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 16 Nov 2016 09:16:17 -0500
Subject: [concurrency-interest] LockSupport park/unpark and memory
	visibility
In-Reply-To: <B954D93D-6B74-497A-BDA7-F0F96B2C00CC@gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <EA6AFC7B-7934-453E-A7A7-83DCE8B46931@gmail.com>
 <CAHjP37G6DwBLG1iv-Fj76u=YdxFNtZ3M3bLzOYCXJ9swSso05g@mail.gmail.com>
 <7B4172C7-7207-4DE0-B992-DDA0B8358E5C@gmail.com>
 <CAHjP37GmgLQOW4+Uwp7X-b3p9bp-QSpAYoLSauYOTFnyzN2DzQ@mail.gmail.com>
 <B954D93D-6B74-497A-BDA7-F0F96B2C00CC@gmail.com>
Message-ID: <CAHjP37HSxQ6NgHSNSCsdOUHg0usMFROpdckH2N_bOqjTwi2vig@mail.gmail.com>

I'm not too interested in lang/spec lawyering, but chapter 17 explicitly
splits program order, synchronization order, and happens-before order.  So
while someone may conclude that happens-before order is implied, it's a
better state of affairs than the current documentation.  Moreover, I'd be
just as happy spec'ing LS without explicit use of JMM terms - similar to
how Unsafe.XXXFence, VarHandle, etc are specified - on their own.

On Wed, Nov 16, 2016 at 9:07 AM, Alex Otenko <oleksandr.otenko at gmail.com>
wrote:

> release/acquire is used in chapter 17 only a couple of times, and only in
> conjunction with synchronizes-with edges, so I’d say no, since others may
> conclude park/unpark forms edges that are part of happens-before - but the
> conclusion is this is exactly what is not promised.
>
>
> Alex
>
>
> On 16 Nov 2016, at 13:57, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>
>
>
> On Wed, Nov 16, 2016 at 8:46 AM, Alex Otenko <oleksandr.otenko at gmail.com>
> wrote:
>
>> Perfectly good example of why park/unpark should be disconnected from
>> synchronizes-with edges. Plugging them in synchronization order gives you
>> that guarantee about "reordering" park/unpark with preceding/following
>> volatile accesses.
>>
> Right, which is the release/acquire between unpark/park that I alluded to
> earlier.  That would also allow lazySet to work.
>
> Assuming this is an agreed upon clarification, should that be added to the
> javadoc? I say yes.
>
>>
>> Alex
>>
>> On 16 Nov 2016, at 13:35, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>
>> unpark can also unconditionally set a flag, with a relaxed store, with no
>> ordered load. Since park is allowed to return spuriously "for no reason at
>> all", I don't see how that isn't allowed (whether it's sensible or not is a
>> separate topic).
>>
>> And as I mentioned to Doug, is lazySet considered using "atomics" or not?
>>
>> On Wed, Nov 16, 2016 at 8:15 AM Alex Otenko <oleksandr.otenko at gmail.com>
>> wrote:
>>
>>> It is not just an “implementation detail”. It is also a minimal sensible
>>> implementation.
>>>
>>> How about this: park enter/exit and unpark appear in synchronization
>>> order, but only guarantee park exit occurs for every park enter, if there
>>> are unpark events after the preceding park exit in that order - no new
>>> synchronizes-with edges. This is pretty much all that park-unpark can be
>>> reasonably expected to do, and the minimal requirement.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 16 Nov 2016, at 12:45, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>>
>>> This would be a particular implementation detail. Since we're
>>> hypothesizing, it's possible that unpark() can use an unordered read (i.e.
>>> relaxed) as an optimization, and then we're back to same situation.
>>>
>>> Either way though, this thread and previous ones on LS are indication
>>> that more documentation is needed.
>>>
>>> On Wed, Nov 16, 2016 at 7:35 AM Alex Otenko <oleksandr.otenko at gmail.com>
>>> wrote:
>>>
>>> Not necessarily.
>>>
>>> unpark can end up just checking a flag to see that the thread has not
>>> been parked after some preceding call to unpark. This can make it behave
>>> like a volatile load, which will have to be totally ordered with a store in
>>> park that clears that flag. So there may be no stores in a particular
>>> execution of unpark.
>>>
>>>
>>> Alex
>>>
>>> On 16 Nov 2016, at 12:20, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>>>
>>> Yes, that's the blog entry I was referring to.
>>>
>>> So I agree with you that LS ordering is underspecified. Specifically,
>>> unpark() must have release semantics to preclude it from moving above the
>>> volatile store. Otherwise, as you say, threadA can be unparked before the
>>> volatile write is visible and then it'll go back to sleep indefinitely.
>>>
>>> Note that a volatile write doesn't change this - if unpark() isn't
>>> ordered, it can be moved above the volatile write (theoretically).
>>>
>>> On Wed, Nov 16, 2016 at 5:18 AM Anthony Maire <maire.anthony at gmail.com>
>>> wrote:
>>>
>>> Thank you for that first feedback
>>>
>>> for the "the JMM says a write to a volatile field can be seen by all
>>> threads." by Henri, the issue is not that the waiter will not see the
>>> write, but that it may not see at the precise moment that it has been
>>> unparked, so it is parked again and wait possibly forever. As far as I
>>> understand it, the JMM is based on HB edges, so to make sure the write is
>>> visible at that precise moment, there should be an HB edge between unpark
>>> and park, which is not guaranteed
>>> Of course, it will work because the volatile write implies waiting for
>>> the store buffer to drain so that write will always be visible for threadA,
>>> but it seems an implementation detail to me, so it is not something I want
>>> to rely on.
>>>
>>> For the FIFOMutex example, I agree that is intended to work (LS will be
>>> pretty useless if it doesn't) and it works by experience. But I would like
>>> to know why it is guaranteed to works so I can reason about LS easier :)
>>>
>>> For the Dave Dice test, I assume that Vitaly is referring to this blog
>>> post : https://blogs.oracle.com/dave/entry/a_race_in_locksupport_park
>>> That's basically where my second question about lazySet comes from : it
>>> will work with empty methods, but if we assume that park/unpark can have no
>>> fences (since the current fences are an implementation detail) it seems
>>> even more fragile
>>>
>>>
>>> 2016-11-15 20:55 GMT+01:00 Vitaly Davidovich <vitalyd at gmail.com>:
>>>
>>> The LS javadoc has an example usage, FIFOMutex, that's a similar idiom
>>> to Anthony's example. That would imply the idiom is intended to work.
>>>
>>> LS on its own isn't spec'd to provide any happens-before/ordering, so
>>> user needs to provide that themselves. I recall Dave Dice mentioning that a
>>> good litmus test for proper LS usage is to pretend park/unpark are empty
>>> methods that degenerate into spinning.
>>>
>>> On Tue, Nov 15, 2016 at 2:49 PM Henri Tremblay <henri.tremblay at gmail.com>
>>> wrote:
>>>
>>> I'm not a subject matter expert on this but:
>>>
>>> Yes. It works if "ready" is volatile. Because for any implementation,
>>> the JMM says a write to a volatile field can be seen by all threads.
>>>
>>> An AtomicBoolean would also work on a set().
>>>
>>> Two things I don't know:
>>> 1- How lazy is lazySet()? I would be afraid that lazySet() might set
>>> ready after threadA check its value and is back to park
>>> 2- Do "ready" need to be volatile? Or is pack/unpark providing some kind
>>> of memory fence.
>>>
>>> Henri
>>>
>>>
>>> On 15 November 2016 at 09:34, Anthony Maire <maire.anthony at gmail.com>
>>> wrote:
>>>
>>> Hi
>>> Sorry if this has been already asked, but I didn't succeed to find a
>>> clear answer
>>> I'm wondering on how memory consistency can be achieved when using
>>> LockSupport park() and unpark() methods
>>> As far as I know, there is no explicit guarantee either in LS javadoc or
>>> in the JLS, so I assume that the condition to be check should come with its
>>> own visibility mechanism since we don't have any HB relationship.
>>>
>>>
>>> Let's consider the following idiom, "ready" being a volatile boolean
>>> field:
>>> - threadA: while(!ready){ LockSupport.park(this); }
>>> - threadB: ready = true; LockSupport.unpark(threadA);
>>>
>>> What does make this example work ? Is it possible for threadA to
>>> un²park, check the "ready" field without seeing threadB's store and park
>>> again ? If I understand the JMM correctly I would say that it is a valid
>>> execution order.
>>> On x86 with OpenJDK or Oracle JVM, I assume this works because the
>>> volatile store force the store buffer to drain so that threadA will always
>>> see the store when the permit is made available by threadB, but is there
>>> any guarantee that is not implementation or architecture dependent ?
>>>
>>> Same question is we use AtomicBoolean.get() / AtomicBoolean.lazySet()
>>> instead of volatile read/write ? I assume that it should be correct on x86
>>> + openJDK, but is there any strong guarantee on this ?
>>>
>>> Kind regards.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> --
>>> Sent from my phone
>>>
>>>
>>> --
>>> Sent from my phone
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>> --
>>> Sent from my phone
>>>
>>>
>>> --
>> Sent from my phone
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/afa31fca/attachment-0001.html>

From david.lloyd at redhat.com  Wed Nov 16 09:33:07 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 16 Nov 2016 08:33:07 -0600
Subject: [concurrency-interest] Lightweight mutual exclusion without locking
Message-ID: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>

Sometimes there is a case where there is a resource which cannot safely 
*or meaningfully* be accessed concurrently by multiple threads.  For 
such resources, there is a clear expectation that only one thread would 
ever access it, and in many cases this expectation is necessary in order 
to gain maximum performance.  However, it is possible that this 
constraint might be accidentally be violated, causing potentially weird 
and/or serious problems.

In such cases, one can use synchronization or locking to ensure safe 
access, however this has been observed to cause potentially major 
performance degradation in real-world cases, and is theoretically much 
stronger/heavier than necessary, because it is not a requirement to 
allow safe concurrent access; rather, the best behavior would be to 
reject any latecomers with an exception.

One good example of a resource which cannot usually be meaningfully 
shared among concurrently running tasks is a stream-oriented pipe.  The 
pipe itself may be 100% safe for concurrent access, but since reads and 
writes of stream pipes are not generally guaranteed to be atomic, a 
typical stream protocol cannot usually be guaranteed to have correct 
output in the presence of concurrent writes because stream protocols are 
not usually resilient against arbitrary interleaving of content.

In cases like this, a good programmer may wish to take measures to 
ensure that accidental concurrent access is deterred with an exception, 
so that problems can be detected more easily.  However, despite that it 
seems to me like it should be possible to do, I have not yet been able 
to derive a provable scheme that would allow for this.

The requirements are:
* Access to the object from one thread should be maximally performant, 
i.e. the impact of adding the extra check should be very small
* Access to the object from a thread when another thread is accessing it 
should be detectable as reliably as possible
* Access to the object from two threads at different times, where the 
object's state was not safely published from one thread to another by 
some other mechanism (like a concurrent queue), can be disallowed, but 
does not have to be

The simplest non-lock approach is to use an atomic boolean of some sort, 
and use a get/CAS loop to determine if the object has been entered, 
throwing an exception if it fails.  However the performance 
characteristics of this approach seem to be very poor; nearly as bad as 
locks when I last tested it.

What I imagine happening in a better solution is something like this, 
given mutex field "mx": Atomically determine if "mx" is either "true" or 
has a pending unflushed write; if so, fail with exception; if not, set 
to "true" and indicate that the field has been written.

This requires the existence of the idea that one has the ability to 
write to a field and globally signal that the field and/or cache line is 
now "dirty", while also atomically failing if the field and/or cache 
line is already considered "dirty" by some other processor (without 
necessarily needing to know what the "dirty" value is or incur a cost 
therefor).  Thus this solution would rely on such objects being safely 
published before being usable by a different thread: but this is an OK 
expectation, since this is already required for non-thread-safe objects 
anyway, and this technique would merely be a safeguard of sorts.

Note that this hypothetical approach would completely crap out in the 
presence of false sharing, so the object with this kind of guard in it 
would have to be aligned to, and fill, a cache line, and you'd probably 
want the mutex field to be early or first in case the object dribbles 
out to a fraction of another line.

So, getting to my question (finally!): does VarHandle give us a CAS 
variation which will *only* fail in exactly the circumstances I've 
listed above (i.e. if the field is definitely not equal to the expected 
value, *or* the field is subject to a pending unpublished write from 
another CPU, while at the same time not causing other CPUs to reload the 
value if the write succeeds)?  Is there another approach that could work 
here?

--
- DML

From shade at redhat.com  Wed Nov 16 11:29:06 2016
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 16 Nov 2016 17:29:06 +0100
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
Message-ID: <75182199-8059-51dd-d7bb-04342c8c4acc@redhat.com>

Hi,

On 11/16/2016 03:33 PM, David M. Lloyd wrote:
> The requirements are:
> * Access to the object from one thread should be maximally performant,
> i.e. the impact of adding the extra check should be very small
> * Access to the object from a thread when another thread is accessing it
> should be detectable as reliably as possible
> * Access to the object from two threads at different times, where the
> object's state was not safely published from one thread to another by
> some other mechanism (like a concurrent queue), can be disallowed, but
> does not have to be
> 
> The simplest non-lock approach is to use an atomic boolean of some sort,
> and use a get/CAS loop to determine if the object has been entered,
> throwing an exception if it fails.  However the performance
> characteristics of this approach seem to be very poor; nearly as bad as
> locks when I last tested it.

This is surprising, and probably warrants looking into. Care to share
the experimental results?


> What I imagine happening in a better solution is something like this,
> given mutex field "mx": Atomically determine if "mx" is either "true" or
> has a pending unflushed write; if so, fail with exception; if not, set
> to "true" and indicate that the field has been written.

Um, I am confused. I think this is a wishful thinking to be able to
detect a (remote) "pending unflushed write". That's the job for hardware
cache coherency protocols, and you would not be able to tap into that.
There are no ISA instructions that I know of that can poll the status
for a cache line (not to say, store buffers / invalidation queues, or
other machine-implementation specific things).

But even if we had the visibility into that, it would still be not
enough. For example, in MESI...

> This requires the existence of the idea that one has the ability to
> write to a field and globally signal that the field and/or cache line is
> now "dirty", 

...there might be no communication that the cache line is dirty, e.g.
when transiting from E(xclusive) to M(odified) state.

> while also atomically failing if the field and/or cache
> line is already considered "dirty" by some other processor (without
> necessarily needing to know what the "dirty" value is or incur a cost
> therefor). 

...there is no way to ask other processors if they have a given cache
line in M(odified) state, except for doing the actual
Request-For-Ownership (RFO) request for its S(hared) cache line. Which
is the hard part of the actual write/CAS as well.


> So, getting to my question (finally!): does VarHandle give us a CAS
> variation which will *only* fail in exactly the circumstances I've
> listed above (i.e. if the field is definitely not equal to the expected
> value, *or* the field is subject to a pending unpublished write from
> another CPU, while at the same time not causing other CPUs to reload the
> value if the write succeeds)? 

This "pending unpublished write" business is from the layer below,
invisible to the software. But VarHandle CASes are not really different
from the CASes that Atomics have: strong CAS only fails when the actual
!= expected, and weak CAS is allowed to spuriously fail (at hardware's
discretion).

VarHandles is not a magic implementation that can peek into cache
coherency. It can help you to solve the indirection costs associated
with standalone Atomics (pretty much like Atomic*FieldUpdaters would),
provide some interesting access modes (which are irrelevant here,
because you are fighting the single-location cache coherency), but it
cannot hack into the CPU.

> Is there another approach that could work here?

No, I don't think so. You cannot to something better than hardware
allows for.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161116/3725f644/attachment.sig>

From david.lloyd at redhat.com  Wed Nov 16 11:42:59 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 16 Nov 2016 10:42:59 -0600
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <75182199-8059-51dd-d7bb-04342c8c4acc@redhat.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
 <75182199-8059-51dd-d7bb-04342c8c4acc@redhat.com>
Message-ID: <b875c2b7-7aa1-2a7d-6fb5-668f9f3a83f0@redhat.com>

On 11/16/2016 10:29 AM, Aleksey Shipilev wrote:
> Hi,
>
> On 11/16/2016 03:33 PM, David M. Lloyd wrote:
>> The requirements are:
>> * Access to the object from one thread should be maximally performant,
>> i.e. the impact of adding the extra check should be very small
>> * Access to the object from a thread when another thread is accessing it
>> should be detectable as reliably as possible
>> * Access to the object from two threads at different times, where the
>> object's state was not safely published from one thread to another by
>> some other mechanism (like a concurrent queue), can be disallowed, but
>> does not have to be
>>
>> The simplest non-lock approach is to use an atomic boolean of some sort,
>> and use a get/CAS loop to determine if the object has been entered,
>> throwing an exception if it fails.  However the performance
>> characteristics of this approach seem to be very poor; nearly as bad as
>> locks when I last tested it.
>
> This is surprising, and probably warrants looking into. Care to share
> the experimental results?

Sure, next time I swing around to the project in question I'll follow up 
with some test results.

>> What I imagine happening in a better solution is something like this,
>> given mutex field "mx": Atomically determine if "mx" is either "true" or
>> has a pending unflushed write; if so, fail with exception; if not, set
>> to "true" and indicate that the field has been written.
>
> Um, I am confused. I think this is a wishful thinking to be able to
> detect a (remote) "pending unflushed write". That's the job for hardware
> cache coherency protocols, and you would not be able to tap into that.
> There are no ISA instructions that I know of that can poll the status
> for a cache line (not to say, store buffers / invalidation queues, or
> other machine-implementation specific things).
>
> But even if we had the visibility into that, it would still be not
> enough. For example, in MESI...
>
>> This requires the existence of the idea that one has the ability to
>> write to a field and globally signal that the field and/or cache line is
>> now "dirty",
>
> ...there might be no communication that the cache line is dirty, e.g.
> when transiting from E(xclusive) to M(odified) state.
>
>> while also atomically failing if the field and/or cache
>> line is already considered "dirty" by some other processor (without
>> necessarily needing to know what the "dirty" value is or incur a cost
>> therefor).
>
> ...there is no way to ask other processors if they have a given cache
> line in M(odified) state, except for doing the actual
> Request-For-Ownership (RFO) request for its S(hared) cache line. Which
> is the hard part of the actual write/CAS as well.
>
>> So, getting to my question (finally!): does VarHandle give us a CAS
>> variation which will *only* fail in exactly the circumstances I've
>> listed above (i.e. if the field is definitely not equal to the expected
>> value, *or* the field is subject to a pending unpublished write from
>> another CPU, while at the same time not causing other CPUs to reload the
>> value if the write succeeds)?
>
> This "pending unpublished write" business is from the layer below,
> invisible to the software. But VarHandle CASes are not really different
> from the CASes that Atomics have: strong CAS only fails when the actual
> != expected, and weak CAS is allowed to spuriously fail (at hardware's
> discretion).

The CASes that Atomics have are limited to "compareAndSet" and 
"weakCompareAndSet", whereas VarHandle (according to the current 
JavaDoc) has four different modes each of normal and weak CAS.  So 
already there is some difference there, and it's still not super-clear 
to me what the tradeoffs are.  More on this below...

> VarHandles is not a magic implementation that can peek into cache
> coherency. It can help you to solve the indirection costs associated
> with standalone Atomics (pretty much like Atomic*FieldUpdaters would),
> provide some interesting access modes (which are irrelevant here,
> because you are fighting the single-location cache coherency), but it
> cannot hack into the CPU.
>
>> Is there another approach that could work here?
>
> No, I don't think so.

That's a shame.  Thanks for the detailed answer.

In lieu of using the normal "strong" CAS flavor I described above, does 
it make more sense in this use case to use an "acquire" CAS on entry and 
a "release" set on exit?  Is there likely to be any performance 
difference in the common architectures?

-- 
- DML

From peter.levart at gmail.com  Wed Nov 16 12:12:06 2016
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 16 Nov 2016 18:12:06 +0100
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
Message-ID: <5b3c6051-341a-6408-d3b5-6bc894ea25e8@gmail.com>

Hi David,

On 11/16/2016 03:33 PM, David M. Lloyd wrote:
> The requirements are:
> * Access to the object from one thread should be maximally performant, 
> i.e. the impact of adding the extra check should be very small
> * Access to the object from a thread when another thread is accessing 
> it should be detectable as reliably as possible
> * Access to the object from two threads at different times, where the 
> object's state was not safely published from one thread to another by 
> some other mechanism (like a concurrent queue), can be disallowed, but 
> does not have to be 

You don't say if your requirements include granting access to the object 
from different threads in any circumstance and in which circumstance 
access is to be granted.

If you only need allowing access from a single thread for the lifetime 
of the resource, then you can simply "pin" the resource to the thread 
that "allocates" it. For example:

public class Resource {
     private final Thread constructingThread;

     public Resource(Thread constructingThread) {
         this.constructingThread = Thread.currentThread();
     }

     public void access() {
         if (constructingThread != Thread.currentThread()) {
             throw new IllegalStateException("Access by non-constructing 
thread rejected");
         }
         // ... access the resource ...
     }
}



Regards, Peter


From david.lloyd at redhat.com  Wed Nov 16 12:26:45 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 16 Nov 2016 11:26:45 -0600
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <5b3c6051-341a-6408-d3b5-6bc894ea25e8@gmail.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
 <5b3c6051-341a-6408-d3b5-6bc894ea25e8@gmail.com>
Message-ID: <dd8904ac-436d-6f56-c9ba-9b333bb85e54@redhat.com>

On 11/16/2016 11:12 AM, Peter Levart wrote:
> Hi David,
>
> On 11/16/2016 03:33 PM, David M. Lloyd wrote:
>> The requirements are:
>> * Access to the object from one thread should be maximally performant,
>> i.e. the impact of adding the extra check should be very small
>> * Access to the object from a thread when another thread is accessing
>> it should be detectable as reliably as possible
>> * Access to the object from two threads at different times, where the
>> object's state was not safely published from one thread to another by
>> some other mechanism (like a concurrent queue), can be disallowed, but
>> does not have to be
>
> You don't say if your requirements include granting access to the object
> from different threads in any circumstance and in which circumstance
> access is to be granted.
>
> If you only need allowing access from a single thread for the lifetime
> of the resource, then you can simply "pin" the resource to the thread
> that "allocates" it. For example:

I guess I didn't explain well; I meant "one thread at a time, as 
mediated by safe publication".  In other words, from a high level, I'd 
like to be able to treat the object exactly the same as any other 
thread-unsafe object, with the exception that there is some kind of 
lightweight check to ensure that the rules aren't broken.

-- 
- DML

From aph at redhat.com  Wed Nov 16 12:47:29 2016
From: aph at redhat.com (Andrew Haley)
Date: Wed, 16 Nov 2016 17:47:29 +0000
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <b875c2b7-7aa1-2a7d-6fb5-668f9f3a83f0@redhat.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
 <75182199-8059-51dd-d7bb-04342c8c4acc@redhat.com>
 <b875c2b7-7aa1-2a7d-6fb5-668f9f3a83f0@redhat.com>
Message-ID: <5a9a8f40-365d-967c-25db-df85cb1d3995@redhat.com>

On 16/11/16 16:42, David M. Lloyd wrote:
>> This "pending unpublished write" business is from the layer below,
>> > invisible to the software. But VarHandle CASes are not really different
>> > from the CASes that Atomics have: strong CAS only fails when the actual
>> > != expected, and weak CAS is allowed to spuriously fail (at hardware's
>> > discretion).
> The CASes that Atomics have are limited to "compareAndSet" and 
> "weakCompareAndSet", whereas VarHandle (according to the current 
> JavaDoc) has four different modes each of normal and weak CAS.  So 
> already there is some difference there, and it's still not super-clear 
> to me what the tradeoffs are.  More on this below...

This is all to do with ordering.  On x86 it won't make any difference
at all, and on other architectures it might make a small difference.

>>> >> Is there another approach that could work here?
>> >
>> > No, I don't think so.
> That's a shame.  Thanks for the detailed answer.
> 
> In lieu of using the normal "strong" CAS flavor I described above, does 
> it make more sense in this use case to use an "acquire" CAS on entry and 
> a "release" set on exit?  Is there likely to be any performance 
> difference in the common architectures?

That's unlikely.  But I think we are all very surprised that you are
seeing poor performance.  Unless the lock/CAS/whatever is highly
contended the overhead should be very minor.

Andrew.


From david.lloyd at redhat.com  Wed Nov 16 13:57:59 2016
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 16 Nov 2016 12:57:59 -0600
Subject: [concurrency-interest] Lightweight mutual exclusion without
 locking
In-Reply-To: <5a9a8f40-365d-967c-25db-df85cb1d3995@redhat.com>
References: <c01b96cb-c274-3fde-b038-59387db9a887@redhat.com>
 <75182199-8059-51dd-d7bb-04342c8c4acc@redhat.com>
 <b875c2b7-7aa1-2a7d-6fb5-668f9f3a83f0@redhat.com>
 <5a9a8f40-365d-967c-25db-df85cb1d3995@redhat.com>
Message-ID: <1b6428bf-8cf7-f429-c0df-084976fafb95@redhat.com>

On 11/16/2016 11:47 AM, Andrew Haley wrote:
> On 16/11/16 16:42, David M. Lloyd wrote:
>>> This "pending unpublished write" business is from the layer below,
>>>> invisible to the software. But VarHandle CASes are not really different
>>>> from the CASes that Atomics have: strong CAS only fails when the actual
>>>> != expected, and weak CAS is allowed to spuriously fail (at hardware's
>>>> discretion).
>> The CASes that Atomics have are limited to "compareAndSet" and
>> "weakCompareAndSet", whereas VarHandle (according to the current
>> JavaDoc) has four different modes each of normal and weak CAS.  So
>> already there is some difference there, and it's still not super-clear
>> to me what the tradeoffs are.  More on this below...
>
> This is all to do with ordering.  On x86 it won't make any difference
> at all, and on other architectures it might make a small difference.
>
>>>>>> Is there another approach that could work here?
>>>>
>>>> No, I don't think so.
>> That's a shame.  Thanks for the detailed answer.
>>
>> In lieu of using the normal "strong" CAS flavor I described above, does
>> it make more sense in this use case to use an "acquire" CAS on entry and
>> a "release" set on exit?  Is there likely to be any performance
>> difference in the common architectures?
>
> That's unlikely.  But I think we are all very surprised that you are
> seeing poor performance.  Unless the lock/CAS/whatever is highly
> contended the overhead should be very minor.

OK, next time I come 'round to it I'll try and come up with some kind of 
cogent benchmark.  It's been long enough that I can't promise there 
isn't some ancillary problem like false sharing going on.

-- 
- DML

From davidcholmes at aapt.net.au  Wed Nov 16 17:12:18 2016
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 17 Nov 2016 08:12:18 +1000
Subject: [concurrency-interest] LockSupport park/unpark and
	memory	visibility
In-Reply-To: <CAHjP37E+Vb_zHVCKWrAqRX=Gi8DQvZSu4EjP1t-whb8WHT3hDA@mail.gmail.com>
References: <CAJpyQ4rjo5753f86=1fP_WkN0aXOGORGsWbELe49H9Y8paHkbQ@mail.gmail.com>
 <CADZL2=tFi-EU8AK7PXr-==OsfYgJHjtFrNx3igQWio4BsvknLg@mail.gmail.com>
 <CAHjP37EcvLX41XTvX_Z_vniNwwAGxRtChpVehq1idQq9GHCmwg@mail.gmail.com>
 <CAJpyQ4qOZimBjq7MePp2kEY6H5WooiUQQxQwo0_kXDgNFf7sLQ@mail.gmail.com>
 <CAHjP37Fyf9j0kgRbEqRDf+XpCfstvXnzi6nqnd8-s-W4u7qGYA@mail.gmail.com>
 <1B38D61A-7D1E-42A1-BEF0-1E6DC14F3753@gmail.com>
 <CAHjP37FhR4FGy4Z8yC2h7+-r1FOhCOSeTtL3xPYC5EwKjufLHQ@mail.gmail.com>
 <e3fd9c39-ea3c-58ec-04fd-1d3b1226b5d2@cs.oswego.edu>
 <CAHjP37E+Vb_zHVCKWrAqRX=Gi8DQvZSu4EjP1t-whb8WHT3hDA@mail.gmail.com>
Message-ID: <02b801d24056$7f4d4060$7de7c120$@aapt.net.au>

This is the same discussion that was had in January 2015 where it was done to death! Please everyone go and refresh their memories on that extremely long discussion. I think I can summarise it as “Doug agreed to add a statement about ordering, although it did not really help anything, but was a marginal improvement that did no harm; then others showed it could do harm, so we returned to current state of affairs.”

 

Cheers,

David

 

From: Concurrency-interest [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Vitaly Davidovich
Sent: Wednesday, November 16, 2016 11:23 PM
To: Doug Lea <dl at cs.oswego.edu>; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] LockSupport park/unpark and memory visibility

 

Doug,

It's still not clear. Can you explain what in the current docs states that unpark() cannot move above a volatile store? How about a lazySet/ordered put? Is that considered "atomically" or not?

 

On Wed, Nov 16, 2016 at 8:15 AM Doug Lea <dl at cs.oswego.edu <mailto:dl at cs.oswego.edu> > wrote:


On 11/16/2016 07:45 AM, Vitaly Davidovich wrote:
> This would be a particular implementation detail. Since we're
> hypothesizing, it's possible that unpark() can use an unordered read
> (i.e. relaxed) as an optimization, and then we're back to same situation.
>
> Either way though, this thread and previous ones on LS are indication
> that more documentation is needed.

We do already say in the javadocs:

"Reliable usage requires the use of volatile (or atomic) variables to
control when to park or unpark."

Some people familiar with the implementation on some JVM might wish we
would promise more, but (1) the specs are designed to allow other
implementations, even including park() as no-op; (2) as nicely
explained by Alex Otenko, a stronger spec shouldn't impact how you
use it anyway.

-Doug




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 

Sent from my phone

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161117/1bb68fcf/attachment.html>

From eregontp at gmail.com  Tue Nov 22 18:03:12 2016
From: eregontp at gmail.com (Benoit Daloze)
Date: Wed, 23 Nov 2016 00:03:12 +0100
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
Message-ID: <CANLTa0Dk+SBjtQ+YFD9AqmvmPZ119my8iHcVkMCmfEC-dc0x-g@mail.gmail.com>

FWIW, if you are willing to use a patched JVM, there is a patch in mlvm
from Lukas Stadler to add native coroutines on HotSpot:
http://ssw.jku.at/General/Staff/LS/coro/
http://hg.openjdk.java.net/mlvm/mlvm/hotspot/file/4cd7d914b0e3/coro-simple.patch

I rebased that patch on JDK8: https://github.com/eregon/jvmci
and integrated the Coroutine API with Truffle (it would be easy to split it
out as well):
https://github.com/eregon/truffle/tree/coro

On Fri, Oct 7, 2016 at 7:47 PM, Andrew Trumper <andrew at intelerad.com> wrote:

> Hi All,
>
> I've been meaning to ask for some time. Is there anyone working on adding
> continuations to the jvm with the idea of using them to allow the writing
> of single-threaded threaded Actors more naturally? ie: instead of callbacks
> just blocking? like:
>
> Typical modern style:
> someAsyncTask()
>   .mapAsync(result -> someOtherAsyncTask(result))
>   .mapAsync(otherResult -> yetAnotherAsyncTask(otherResult))
>   .onSuccess(yetAnotherResult -> System.out.println( "" +
> yetAnotherResult))
>   .setInvoker(thisThread);
>
> converts to:
>
> // doesn't block the current (actor? EDT?) thread
> Object result = yetAnotherAsyncTask(someOtherAsyncTask(someAsyncTask()));
> System.out.println( "" + result );
>
> - Andrew
>
> --
>
> This email or any attachments may contain confidential or legally
> privileged information intended for the sole use of the addressees. Any
> use, redistribution, disclosure, or reproduction of this information,
> except as intended, is prohibited. If you received this email in error,
> please notify the sender and remove all copies of the message, including
> any attachments.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161123/d86b5006/attachment.html>

From godmar at gmail.com  Fri Nov 25 09:41:46 2016
From: godmar at gmail.com (Godmar Back)
Date: Fri, 25 Nov 2016 09:41:46 -0500
Subject: [concurrency-interest] Continuations / Fibers
In-Reply-To: <83BD5DBB-D7C6-493F-91D2-C71637C8498F@kodewerk.com>
References: <104ab306-3b24-e826-c9db-5d45ae1f5831@intelerad.com>
 <00a601d220c3$f0ea8aa0$d2bf9fe0$@eu>
 <CANPzfU9gkX4daMhOU-1iaQahZ_DGk9piiv_8H8nHBi-_bnxcCg@mail.gmail.com>
 <f1483d8c-ed22-9098-d82c-525e20301b15@intelerad.com>
 <CANPzfU_Zy+JnS1qvGhGajcg=nnb1OQeVUGCscttYS74Sobc2wQ@mail.gmail.com>
 <CAE-f1xQg1ut94D83nOJW_kW=2tnvMh5FHFyMA3cK8HLL3HrH+Q@mail.gmail.com>
 <00fd01d220e3$7674b230$635e1690$@gmail.com>
 <83BD5DBB-D7C6-493F-91D2-C71637C8498F@kodewerk.com>
Message-ID: <CAB4+JYK9dyGqt4+=GeS1WhVjD1c0wEMXHsXAAwteQdOr_p+QEA@mail.gmail.com>

On Sat, Oct 8, 2016 at 4:50 AM, kirk at kodewerk.com <kirk at kodewerk.com> wrote:

> Hi Nathan,
>
> Nice overview. Sun already tried this with LWPs in Solaris. An LWP was
> tied to a thread. The OS scheduled the thread but was also involved with
> which LWP the thread was running. Sound familiar? (
> https://docs.oracle.com/cd/E19455-01/806-5257/mtintro-72944/index.html)
>
>
There is a whitepaper that describes Sun's experience when moving to
Solaris 9 from Solaris 8 about 15 years ago. This is when they retired
their M:N implementation in favor of an 1:1 implementation.
I couldn't find it on the Oracle website anymore, but here's a link:
http://home.mit.bme.hu/~meszaros/edu/oprendszerek/segedlet/unix/2_folyamatok_es_utemezes/solaris_multithread.pdf

I quote a relevant paragraph from page P8, my *emphasis* added.

"The first advantage of a two-tiered architecture is that application
thread synchronization can be done either via the kernel (using system
calls), or at the user level (leveraging atomic test-and-set facilities in
the multiprocessor hardware). A system call may take many hundreds or even
thousands of instructions, but a simple compare-and-swap operation takes
just one (although this may involve a considerable number of clock cycles
to complete).
The other advantage of a two-tiered implementation is that the cost of
context switching in a user-level scheduler is considerably less expensive
than with the kernel’s scheduler. In the early days, it was expected that
user-level scheduling would be orders of magnitude faster than kernel
scheduling.
The first advantage has proven to be significant, *but the second advantage
has not. Building a user-level scheduler that works well in tandem with the
kernel’s scheduler is a significant challenge. *However, the kernel’s
ability to efficiently schedule many threads has improved."

The issue of how to manage concurrency in applications and systems has of
course been around since Lauer/Needham. About 15-20 years ago, OS designers
to not support large number of execution contexts in the kernel (as said
above, Solaris tried and reversed the effort, and Linux never adopted the
NGPT <https://en.wikipedia.org/wiki/Native_POSIX_Thread_Library> which used
a similar approach, instead focusing on scalability improvements of their
1:1 implementation.)  But underneath, I think there was a belief that
hundreds of thousands of threads simply is not a good way to design highly
concurrent applications, and thus that trying to provide OS support for it
is not a good idea. There was work to efficiently supports large number of
user-level threads and avoid the cost of abstraction that prevented the
underlying scheduler from making suboptimal resource decisions, e.g.
Capriccio <http://capriccio.cs.berkeley.edu/>.

I'm not quite certain how to place the developments in the last decade
here. On the one hand, there are highly scalable event-based designs (e.g.
nginx, redis) which do not try to use a thread-like abstraction and instead
manage concurrency explicitly. There are also event-based designs in
higher-level languages (say JavaScript + node.js) which - in my opinion -
appear now to be moving to a programming model (via async/await) that more
resembles threads with their linear control flow, albeit provided by
continuation-like facilities at the language level.  This seems to me a
repudiation of their original idea of managing concurrency explicitly (via
callbacks or promises).  Then there is Go, which seems to be designed for
large number of concurrent goroutines, using a M:N approach at the
user-level, along with an event-based approach for managing I/O. But this
requires a lot of engineering, e.g. segmented stacks.

It is not clear to me how these newer approaches have overcome the lessons
from 15 years ago, i.e., why people now believe that providing system or
language-level support for managing large numbers of execution contexts in
a thread-like fashion is a worthwhile idea whose benefits justify the
engineering effort.

I'm very curious to learn about what has changed!

 - Godmar
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20161125/6f4617c0/attachment.html>

